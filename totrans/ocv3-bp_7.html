<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Efficient Person Identification Using Biometric Properties</h1></div></div></div><p class="calibre8">The rise of digital media is greater than ever. People are placing more and more of their personal information on digital carriers like laptops, smartphones, and tablets. However, many of these systems do not provide efficient identification and authentication systems to ensure that strangers cannot access your personal data. This is where biometrics-based identification systems come into play and try to make your data more secure and less vulnerable to malicious people.</p><p class="calibre8">These identification systems can be used to lock down your computer, avoid people getting into a secure room, and so on, but, with technology improving each day, we are only one step away from further digitalizing our personal lives. How about using your facial expressions to unlock your door? How about opening your car with your fingerprint? The possibilities are endless.</p><p class="calibre8">Many techniques and algorithms are already available in open source computer vision and machine learning packages like OpenCV to efficiently use these personal identification properties. Of course, this opens up the possibility for enthusiastic computer vision programmers to create many different applications based on these techniques.</p><p class="calibre8">In this chapter, we will focus on techniques that use individual biometrics in order to create personal authentication systems that outperform standard available login systems based on passwords. We will take a deeper look at iris and fingerprint recognition, face detection, and face recognition.</p><p class="calibre8">We will first discuss the main principles behind each biometric technique, and then we'll show an implementation based on the OpenCV 3 library. For some of the biometrics, we will make use of the available open source frameworks out there. All datasets used to demonstrate the techniques are available for free online for research purposes. However, if you want to apply them to a commercial application, be sure to check their licenses!</p><p class="calibre8">Finally, we will illustrate how you can combine several biometric classifications to increase the chance of successfully identifying a specific person based on the probability of the individual biometrics.</p><p class="calibre8">At the end of this chapter, you will be able to create a fully functional identification system that will help you to avoid your personal details being stolen by any malicious party out there.</p></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec46" class="calibre1"/>Biometrics, a general approach</h1></div></div></div><p class="calibre8">The <a id="id574" class="calibre1"/>general idea behind identifying a person using a biometric property is the same for all biometrics out there. There are several steps that we should follow in the correct order if we want to achieve decent results. Moreover, we will point out some major points inside these general steps that will help you improve your recognition rate with extreme measures.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec60" class="calibre1"/>Step 1 – getting a good training dataset and applying application-specific normalization</h2></div></div></div><p class="calibre8">The key to most <a id="id575" class="calibre1"/>biometric identification systems is to <a id="id576" class="calibre1"/>collect a system training dataset that is representative of the problem for which you will actually use the system. Research has proven that there is something called <a id="id577" class="calibre1"/><strong class="calibre9">dataset bias</strong>, which means that if you train a system on a training set with a specific setup, environmental factors, and recording devices, and then apply that system to a test set which has been taken from a completely different setup with different environmental factors (like lighting sources) and different recording devices, then this will produce a decrease in performance of up to 25%. This is a very large setback in performance, since you want to make sure that your identification system runs with top performance.</p><p class="calibre8">Therefore, there are several things to consider when creating your training set for your identification system:</p><div><ul class="itemizedlist"><li class="listitem">You should only collect training data with the <strong class="calibre9">known setup</strong> that will be used when applying the biometric recognition. This means that you need to decide on hardware before you start training models and classifiers.</li><li class="listitem">For biometric login systems, it is important to <strong class="calibre9">constrain your data</strong> as much as possible. If you can eliminate lighting changes, different background setups, movement, non-equal positioning, and so on, then you can drastically improve the performance of your application.</li><li class="listitem">Try to <strong class="calibre9">normalize your data</strong> orientation-wise. If you align all your training data to the same position, you avoid introducing undesired variance in a single person's description of the biometric. Research in this field has proven that this can increase recognition rates by more than 15%!</li><li class="listitem">Use <strong class="calibre9">multiple training instances</strong> of a single biometric and use the average biometric description for authenticating a person. Single-shot training systems have the downside that slight differences between two biometric recordings have a large influence on the classification rate. Single-shot learning is still a very active research topic, and there is yet to be found a very stable solution to this problem.</li></ul></div><p class="calibre8">How to <a id="id578" class="calibre1"/>apply this normalization <a id="id579" class="calibre1"/>for specific techniques will be discussed in the corresponding subtopics; for example, in the case of face recognition, since it can actually depend a lot on the techniques used. Once you get a good training set, with sufficient samples, you are ready to move to the second step.</p><div><h3 class="title2"><a id="note89" class="calibre1"/>Note</h3><p class="calibre8">Keep in mind that there will be cases where applying constraints is not always a good way to go. Consider a laptop login system based on biometric features that only works with the lights on like face detection and recognition. That system would not work when somebody was working in a dark room. In that case, you would reconsider your application and ensure that there were enough biometric checks irrelevant to the changing light. You could even check the light intensity yourself through the webcam and disable the face check if you could predict that it would fail.</p><p class="calibre8">The simplification of the application and circumstances involves simplifying the algorithm discussed in this chapter, leading to better performance in these constrained scenarios.</p></div></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec61" class="calibre1"/>Step 2 – creating a descriptor of the recorded biometric</h2></div></div></div><p class="calibre8">Once you get the<a id="id580" class="calibre1"/> required training data to build your biometric identification system, it is important to find a way to uniquely describe each biometric parameter for each individual. This description is called a "unique feature vector" and it has several benefits compared to the original recorded image:</p><div><ul class="itemizedlist"><li class="listitem">A full scale RGB image with high resolution (which is used a lot in biometrics recording) contains a lot of data. If we applied the classification to the complete image it would be:<div><ul class="itemizedlist1"><li class="listitem">Computationally very expensive.</li><li class="listitem">Not as unique as desired, since regions over different persons can be identical or very similar.</li></ul></div></li><li class="listitem">It reduces the important and unique information in an input image to a sparse representation based on keypoints which are unique features of each image.</li></ul></div><p class="calibre8">Again, how <a id="id581" class="calibre1"/>you construct the feature descriptor depends on which biometric you want to use to authenticate. Some approaches are based on Gabor filter banks, local binary pattern descriptions, and keypoint descriptors such as SIFT, SURF, and ORB. The possibilities are, again, endless. It all depends on getting the best description for your application. We will make suggestions for each biometric, but a more exhaustive search will need to be done to find the best solution for your application.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec62" class="calibre1"/>Step 3 – using machine learning to match the retrieved feature vector</h2></div></div></div><p class="calibre8">Each feature <a id="id582" class="calibre1"/>vector created from step 2 needs to be unique to ensure that a machine learning technique based on these feature vectors can differentiate between the biometrics of different test subjects. Therefore, it is important to have a descriptor with enough dimensions. Machine learning techniques are way better at separating data in high dimensional spaces than humans are, while they fail at separating data at low dimension feature spaces, for which a human brain outperforms the system.</p><p class="calibre8">Selecting the best machine learning approach is very cumbersome. In principle, different techniques offer similar results, but getting the best one is a game of trial and error. You can apply parameter optimization inside each machine learning approach to get even better results. This optimization would be too detailed for this chapter. People interested in this should take a deeper look at <a id="id583" class="calibre1"/><strong class="calibre9">hyper parameter optimization</strong> techniques.</p><div><h3 class="title2"><a id="note90" class="calibre1"/>Note</h3><p class="calibre8">Some interesting publications about this hyper parameter optimization problem can be found below:</p><div><ul class="itemizedlist"><li class="listitem">Bergstra J. S., Bardenet R., Bengio Y., and Kégl B. (2011), <em class="calibre10">Algorithms for hyper-parameter optimization</em>, in Advances in Neural Information Processing Systems (pp. 2546-2554).</li><li class="listitem">Bergstra J. and Bengio Y. (2012), <em class="calibre10">Random search for hyper-parameter optimization</em>, The Journal of Machine Learning Research, 13(1), 281-305.</li><li class="listitem">Snoek J., Larochelle H., and Adams R. P. (2012), <em class="calibre10">Practical Bayesian optimization of machine learning algorithms</em>, in Advances in Neural Information Processing Systems (pp. 2951-2959).</li></ul></div></div><p class="calibre8">There are many machine learning techniques in OpenCV 3. Some of the most frequently used techniques can be found below in order of complexity:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Similarity matching</strong> using<a id="id584" class="calibre1"/> distance metrics.</li><li class="listitem"><strong class="calibre9">K-Nearest neighbors search</strong>: A <a id="id585" class="calibre1"/>multi (K) class classification based on distance (Euclidean, Hamming, and so on) calculations between feature vectors in a high dimensional space.</li><li class="listitem"><strong class="calibre9">Naïve Bayes classifier</strong>: A <a id="id586" class="calibre1"/>binary classifier that uses Bayesian learning to differentiate between different classes.</li><li class="listitem"><strong class="calibre9">Support vector machines</strong>: Mostly <a id="id587" class="calibre1"/>used as a binary classifier learning approach, but can be adapted to a multi-class classifier system. This approach depends on the separation of data in high dimensional spaces by looking for optimal separation plains between training data clouds and separating margins.</li><li class="listitem"><strong class="calibre9">Boosting and random forests</strong>: Techniques<a id="id588" class="calibre1"/> that combine several weak classifiers or learners into a complex model able to separate binary and multi-class problems.</li><li class="listitem"><strong class="calibre9">Artificial neural networks</strong>: A <a id="id589" class="calibre1"/>group of techniques that use the power of combining huge amounts of neurons (like the small cells in the brain) that learn connections and decisions based on examples. Due to their steeper learning curve <a id="id590" class="calibre1"/>and complex optimization steps, we will discard their use in this chapter.</li></ul></div><div><h3 class="title2"><a id="note91" class="calibre1"/>Note</h3><p class="calibre8">If you are interested in using neural networks for your classification problems, then take a look at this OpenCV documentation page:</p><p class="calibre8"><a class="calibre1" href="http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html">http://docs.opencv.org/master/d0/dce/classcv_1_1ml_1_1ANN__MLP.html</a></p></div></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch06lvl2sec63" class="calibre1"/>Step 4 – think about your authentication process</h2></div></div></div><p class="calibre8">Once you have a <a id="id591" class="calibre1"/>machine learning technique that outputs a classification for your input feature vector, you need to retrieve a certainty. This certainty is needed to be sure how certain a classification result is. For example, if a certain output has a match for both entry 2 and entry 5 in a database, then you will need to use the certainty to be sure of which of the two matches you should continue with.</p><p class="calibre8">Here, it is also important to think about how your authentication system will operate. It can either be a one-versus-one approach, where you match each database entry with your test sample until you get a high enough matching score, or a one-versus-all approach, where you match the complete database, then look at the retrieved score for each match and take the best match possible.</p><p class="calibre8">One-versus-one can be seen as an iterative version of one-versus-all. They usually use the same logic; the difference is in the data structure used during the comparison. The one-versus-all approach requires a more complex way of storing and indexing the data, while one-versus-one uses a more brute-force approach.</p><div><h3 class="title2"><a id="note92" class="calibre1"/>Note</h3><p class="calibre8">Keep in mind that both techniques can yield different results due to the fact that a false positive match is always possible in machine learning.</p></div><p class="calibre8">Imagine an<a id="id592" class="calibre1"/> input test query for your system. Using one-versus-one matching, you would stop analyzing the database when you had a high enough match. However, if further down the road there was a match yielding an even higher score, then this one would be discarded. With the one-versus-all approach, this could be avoided, so in many cases it is better to apply this one-versus-all approach.</p><p class="calibre8">To give an example of which approach to use in a given case, imagine a door to a secret lab. If you want to check if a person is allowed to enter the lab, then a one-versus-all approach is required to make sure that you match all database entries and that the highest matching score has a certainty above a certain threshold. However, if this final secret lab door is only used to select who is already allowed to enter the room, then a one-versus-one approach is sufficient.</p><p class="calibre8">In order to avoid problems with two individuals who have very similar descriptors for a single biometric, multiple biometrics are combined to reduce the occurrence of false positive detections. This will be discussed further at the end of this chapter, when we combine multiple biometrics in an effective authentication system.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec47" class="calibre1"/>Face detection and recognition</h1></div></div></div><p class="calibre8">Most existing <a id="id593" class="calibre1"/>authentication systems start by detecting a face and trying to recognize it by matching it to a database of known people who use the system. This subsection will take a closer look at that. We will not dive into every single parameter of the software.</p><div><h3 class="title2"><a id="note93" class="calibre1"/>Note</h3><p class="calibre8">If you want more information about complete face detection and the recognition pipeline for both people and cats, then take a look at one of the PacktPub books called <em class="calibre10">OpenCV for Secret Agents</em>. It looks at the complete process in more detail.</p><p class="calibre8">If you want a very detailed explanation of the parameters used for the face detection interface in OpenCV based on the cascade classification pipeline from Viola and Jones, then I suggest going to <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <em class="calibre10">Generic Object Detection for Industrial Applications</em>, which discusses the interface generalized for generic object detection.</p></div><p class="calibre8">Whenever <a id="id594" class="calibre1"/>you are focusing on an authentication system, you want to make sure that you are familiar with the different sub-tasks that need to be applied, as seen in the figure <em class="calibre10">An example of face detection software and the cut-out face region</em> in the section <em class="calibre10">Face detection using the Viola and Jones boosted cascade classifier algorithm</em>.</p><div><ol class="orderedlist"><li class="listitem" value="1">You should start by using a <a id="id595" class="calibre1"/><strong class="calibre9">general face detector</strong>. This is used to find faces in any given input; for example, from your webcam. We will use the Viola and Jones face detector inside OpenCV, trained with a cascade classifier based on AdaBoost.</li><li class="listitem" value="2">Secondly, you should perform some normalization on the image. In our case, we will apply some grayscaling, histogram equalization, and some alignment based on eye and mouth detection.</li><li class="listitem" value="3">Finally, the data needs to be passed to a face recognizer interface. We will discuss the different options briefly (LBPH, Eigenfaces, and Fisherfaces) and walk you through it. This will return the selected user from the database we use to match to.</li></ol><div></div><p class="calibre8">We will discuss the possible advantages, disadvantages, and risks of possible approaches at all stages. We will also suggest several open source packages that give you the chance to further optimize the approach if you want to.</p><div><h3 class="title2"><a id="note94" class="calibre1"/>Note</h3><p class="calibre8">Software for this subsection can be found at the following location:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/</a></p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec64" class="calibre1"/>Face detection using the Viola and Jones boosted cascade classifier algorithm</h2></div></div></div><p class="calibre8">Most webcam setups <a id="id596" class="calibre1"/>nowadays provide a high resolution RGB image as input. However, keep in mind that, for all OpenCV based operations, OpenCV formats the input as a BGR image. Therefore, we should apply some pre-processing steps to the output image <a id="id597" class="calibre1"/>before applying a face detector.</p><div><ul class="itemizedlist"><li class="listitem">Start by converting the image to a grayscale image. The Viola and Jones approach uses a HAAR wavelet or local binary pattern-based features, which are both independent of color. Both feature types look for regions of changing pixel intensities. Therefore, we can omit this extra color information and reduce the amount of data that needs to be processed.</li><li class="listitem">Reduce the resolution of the image. This depends on the webcam output format but, keeping in mind that processing time increases exponentially with increasing resolution, a ratio of 640x360 is more than enough for face detection.</li><li class="listitem">Apply <a id="id598" class="calibre1"/><strong class="calibre9">histogram equalization</strong> to the image to cover invariance under different illuminations. Basically, this operation tries to flatten the intensity histogram of the complete image. The same was done when training the detection models in OpenCV 3 and doing the same works here.</li></ul></div><div><h3 class="title2"><a id="note95" class="calibre1"/>Note</h3><p class="calibre8">It is always good to use different input and output containers for algorithms, since inline operations tend to do very nasty things to the output. Avoid problems by declaring an extra variable if you are not sure that inline replacement is supported.</p></div><p class="calibre8">The following code snippet illustrates this behavior:</p><div><pre class="programlisting">Mat image, image_gray, image_hist;
VideoCapture webcam(0);
Webcam &gt;&gt; image;
resize(image, image, Size(640,360));
cvtColor(image, image_gray, COLOR_BGR2GRAY);
equalizeHist(image_gray, image_hist);</pre></div><p class="calibre8">Once you have done all the preprocessing, you can apply the following code to have an operational face detector on your input image:</p><div><pre class="programlisting">CascadeClassifier cascade('path/to/face/model/');
vector&lt;Rect&gt; faces;
cascade.detectMultiScale(image_hist, faces, 1.1, 3);</pre></div><p class="calibre8">You can now draw the retrieved rectangles on top of the image to visualize the detections.</p><div><h3 class="title2"><a id="note96" class="calibre1"/>Note</h3><p class="calibre8">If you want to know more about used parameters or retrieved detections, have a look at <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <em class="calibre10">Generic Object Detection for Industrial Applications</em>, which discusses this interface in much more detail.</p></div><p class="calibre8">The face <a id="id599" class="calibre1"/>detections will look like the figure below:</p><div><img src="img/00097.jpeg" alt="Face detection using the Viola and Jones boosted cascade classifier algorithm" class="calibre11"/><div><p class="calibre28">An example of face detection software and the cut-out face region</p></div></div><p class="calibre12"> </p><p class="calibre8">Finally, you should cut out the detected face regions so that they can be passed to the interfaces that will process the image. The best approach is to grab these face regions from the original resized image, as seen in the preceding figure, and not from the visualization matrix, to avoid the red border being cut out and polluting the face image.</p><div><pre class="programlisting">for(int i=0; i&lt;faces.size(); i++){
   Rect current_face = faces[i];
   Mat face_region = image( current_face ).clone();
   // do something with this image here
}</pre></div><div><h3 class="title2"><a id="note97" class="calibre1"/>Note</h3><p class="calibre8">The software for executing <a id="id600" class="calibre1"/>this face detection can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_detection/</a>.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec65" class="calibre1"/>Data normalization on the detected face regions</h2></div></div></div><p class="calibre8">If you are<a id="id601" class="calibre1"/> only interested in a basic test setup, then face normalization steps are not really necessary. They are mainly used for improving the quality of your face recognition software.</p><p class="calibre8">A good way to start is to <a id="id602" class="calibre1"/>reduce the amount of variation in the image. You can already apply conversion to grayscale and histogram equalization to remove information from the image, as described in the previous subtopic. This would be enough if you wanted a simple test setup, but it would require the person to keep their head positioned in the same way as the training data was grabbed for that person. If not, then the slight variation in the data due to a different head position would be enough to trigger a false positive match with another person in the database.</p><p class="calibre8">To avoid this, and to increase the quality of the following face recognition system, we propose applying face alignment. This can be done in several ways.</p><div><ul class="itemizedlist"><li class="listitem">As a basic approach, one could run an eye and mouth detector based on the existing OpenCV detectors, and use the centers of the detections as a way to align faces.<div><h3 class="title2"><a id="note98" class="calibre1"/>Note</h3><p class="calibre8">For a very detailed explanation, refer to chapter 8 of <em class="calibre10">Mastering OpenCV</em> by Shervan Emami (<a class="calibre1" href="https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition">https://github.com/MasteringOpenCV/code/tree/master/Chapter8_FaceRecognition</a>). He discusses several ways to align faces using eye detection.</p><p class="calibre8">Also, have a look at the section <em class="calibre10">Finding the face region in the image</em> in <a class="calibre1" title="Chapter 3. Recognizing Facial Expressions with Machine Learning" href="part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69">Chapter 3</a>, <em class="calibre10">Recognizing Facial Expressions with Machine Learning</em>.</p></div></li><li class="listitem">A more advanced approach would be to apply a facial landmark detector and use all those points to normalize and align the faces.<div><h3 class="title2"><a id="note99" class="calibre1"/>Note</h3><p class="calibre8">If you are interested in more advanced techniques, take a look at the flandmark library (<a class="calibre1" href="http://cmp.felk.cvut.cz/~uricamic/flandmark/">http://cmp.felk.cvut.cz/~uricamic/flandmark/</a>). More information about using the facial landmark techniques can be found in <a class="calibre1" title="Chapter 3. Recognizing Facial Expressions with Machine Learning" href="part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69">Chapter 3</a>, <em class="calibre10">Recognizing Facial Expressions with Machine Learning</em>, which discusses how to install this library, configure the software, and then run it on any given face image.</p></div></li></ul></div><p class="calibre8">A good discussion about face alignment can be found at the following OpenCV Q&amp;A forum: <a class="calibre1" href="http://answers.opencv.org/question/24670/how-can-i-align-face-images/">http://answers.opencv.org/question/24670/how-can-i-align-face-images/</a>. Multiple active forum users have gathered their OpenCV knowledge to come up with a very <a id="id603" class="calibre1"/>promising alignment technique, based on basic facial landmark techniques.</p><p class="calibre8">The most basic<a id="id604" class="calibre1"/> alignment can be carried out by using the following approach:</p><div><ol class="orderedlist"><li class="listitem" value="1">Start by detecting the two eyes using the provided eye cascades.</li><li class="listitem" value="2">Find the center points of both eye detections.</li><li class="listitem" value="3">Calculate the angle between both eyes.</li><li class="listitem" value="4">Rotate the image around its own center.</li></ol><div></div><p class="calibre8">The following code does this:</p><div><pre class="programlisting">CascadeClassifier eye('../haarcascades/haarcascade_eye.xml');
vector&lt;Rect&gt; eyes_found;
eye.detectMultiScale(face_region, eyesfound, 1.1, 3);
// Now let us assume only two eyes (both eyes and no FP) are found
double angle = atan( double(eyes_found[0].y - eyes_found[1].y) / double(eyes_found[0].x - eyes_found[1].x) ) * 180 / CV_PI;
Point2f pt(image.cols/2, image.rows/2);
Mat rotation = getRotationMatrix2D(pt, angle, 1.0);
Mat rotated_face;
warpAffine(face_region, rotated_face, rotation, Size(face_region.cols, face_region.rows));</pre></div><div><h3 class="title2"><a id="note100" class="calibre1"/>Note</h3><pre>Ptr&lt;BasicFaceRecognizer&gt; face_model = createEigenFaceRecognizer();</pre></div><p class="calibre8">This will generate an Eigenface-based model ready for training, which will use all eigenvectors (which could be slow) and without a certainty threshold. To be able to use them, you need to use an overloaded interface of the face recognizer.</pre><div><ul class="itemizedlist"><li class="listitem"><code class="email">Ptr&lt;</code> <code class="email">BasicFaceRecognizer &gt; face_model = createEigenFaceRecognizer(20);</code></li><li class="listitem"><code class="email">Ptr&lt;</code> <code class="email">BasicFaceRecognizer &gt; face_model = createEigenFaceRecognizer(20, 100.0);</code></li></ul></div><p class="calibre8">Here, you need to make a decision on what you actually want to achieve. The training will be fast with a low number of eigenvectors, but the accuracy will be lower. To increase the accuracy, increase the number of eigenvectors used. Getting the correct number of eigenvectors is quite cumbersome since it depends a lot on the training data used. As a heuristic, you could train a recognizer with a low number of eigenvectors, test the recognition rate on a test set, and then increase the number of eigenvectors as long as you do not reach the recognition rate goal.</p><p class="calibre8">Then, the model can be learned with the following code:</p><div><pre class="programlisting">// train a face recognition model
vector&lt;Mat&gt; faces;
vector&lt;int&gt; labels;
Mat test_image = imread("/path/to/test/image.png");
// do not forget to fill the data before training
face_model.train(faces, labels);
// when you want to predict on a new image given, using the model
int predict = modelàpredict(test_image);</pre></div><p class="calibre8">If you want to have a bit more information on the prediction, like the prediction confidence, then you can replace the last line with:</p><div><pre class="programlisting">int predict = -1; // a label that is unexisting for starters
double confidence = 0.0;
modelàpredict(test_image, predict, confidence);</pre></div><p class="calibre8">This is a basic<a id="id611" class="calibre1"/> setup. The things that you need to remember to improve the quality of your model are:</p><div><ul class="itemizedlist"><li class="listitem">Generally the more training faces of a person you have, the better a new sample of that person will be recognized. However, keep in mind that your training samples should contain as many different situations as possible, regarding lighting conditions, facial hair, attributes, and so on.</li><li class="listitem">Increasing the number of eigenvectors used for projecting increases accuracy, but it also makes the algorithm slower. Finding a good trade-off is very important for your application.</li><li class="listitem">To avoid fraud getting in by the best match from the database principle, you can use the confidence scores to threshold out matches that are not secure enough</li></ul></div><div><h3 class="title2"><a id="note105" class="calibre1"/>Note</h3><p class="calibre8">If you want to do further research on the algorithm specifics, I suggest reading a paper that describes the technique in more detail:</p><p class="calibre8">Turk, Matthew, and Alex P. Pentland. "Face recognition using eigenfaces" Computer Vision and Pattern Recognition, 1991. Proceedings CVPR'91., IEEE Computer Society Conference on. IEEE, 1991.</p></div><p class="calibre8">If you want to play along with the internal data of the Eigenface-based model, you can retrieve interesting information using the following code:</p><div><pre class="programlisting">// Getting the actual eigenvalues (reprojection values on the eigenvectors for each sample)
Mat eigenvalues = face_model-&gt;getEigenValues();
// Get the actual eigenvectors used for projection and dimensionality reduction
Mat eigenvectors = face_model-&gt;getEigenVectors();
// Get the mean eigenface
Mat mean = face_model-&gt;getMean();</pre></div><p class="calibre8">Some output results on the samples that we used for testing can be seen in the figure below. Remember that, if<a id="id612" class="calibre1"/> you want to show these images, you will need to transform them to the [0 255] range. The OpenCV 3 FaceRecognizer guide shows clearly how you should do this. The jet color space is often used to visualize Eigenfaces data.</p><div><h3 class="title2"><a id="note106" class="calibre1"/>Note</h3><p class="calibre8">The complete and detailed OpenCV 3 FaceRecognizer interface guide can be found at the following web page, and discusses further use of these parameters in more depth than this chapter:</p><p class="calibre8"><a class="calibre1" href="http://docs.opencv.org/master/da/d60/tutorial_face_main.html">http://docs.opencv.org/master/da/d60/tutorial_face_main.html</a></p></div><div><img src="img/00099.jpeg" alt="Eigenface decomposition through PCA" class="calibre11"/><div><p class="calibre28">The first ten Eigenfaces visualized in their most common color spaces, grayscale and JET. Note the influence of the background.</p></div></div><p class="calibre12"> </p></div><div><div><div><div><h3 class="title2"><a id="ch06lvl3sec35" class="calibre1"/>Linear discriminant analysis using the Fisher criterion</h3></div></div></div><p class="calibre8">The downside of using the <a id="id613" class="calibre1"/>Eigenface decomposition is that the transformation is optimal if you think about the pure reconstruction of the given data, however, the technique does not take into account class labels. This could lead to a case where the axes of maximal variance are actually created by external sources rather than the faces themselves. In order to cope with this problem, the technique of using LDA, or linear discriminant analysis, was introduced, based on the Fisher criterion. This minimizes variance within a single class, while maximizing variance between classes at the same time, which makes the technique more robust in the long run.</p><p class="calibre8">The software for executing this face detection can be found at:</p><div><h3 class="title2"><a id="note107" class="calibre1"/>Note</h3><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_fisher/</a></p></div><p class="calibre8">To build a LDA face recognizer interface using the Fisher criteria, you should use the following code snippet in OpenCV 3:</p><div><pre class="programlisting">// Again make sure that the data is available
vector&lt;Mat&gt; faces;
vector&lt;int&gt; labels;
Mat test_image = imread("/path/to/test/image.png");
// Now train the model, again overload functions are available
Ptr&lt;BasicFaceRecognizer&gt; face_model = createFisherFaceRecognizer();
face_modelàtrain(faces, labels);
// Now predict the outcome of a sample test image
int predict = face_modelàpredict(test_image);</pre></div><p class="calibre8">If you want to get more specific properties of the model, this can be achieved with property-specific functions, as depicted below. Remember that, if you want to show these images, you will need to transform them to the [0 255] range. The bone color space is often used to visualize Fisherfaces data.</p><div><img src="img/00100.jpeg" alt="Linear discriminant analysis using the Fisher criterion" class="calibre11"/><div><p class="calibre28">The first 10 Fisherface dimensions, visualized in their most common color spaces, grayscale and BONE.</p></div></div><p class="calibre12"> </p><div><h3 class="title2"><a id="note108" class="calibre1"/>Note</h3><p class="calibre8">Note that the background influence is minimal for these Fisherfaces compared to the previous Eigenfaces technique. This is the main advantage of Fisherfaces over Eigenfaces.</p></div><p class="calibre8">It is nice to <a id="id614" class="calibre1"/>know that both Eigenfaces and Fisherfaces support the reconstruction of any given input inside the Eigenspace or Fisherspace at a certain point in mapping onto the dimensions selected. This is done by applying the following code:</p><div><pre class="programlisting">// Get the eigenvectors or fishervectors and the mean face
Mat mean = face_modelàgetMean();
Mat vectors = face_modelàgetEigenValues();
// Then apply the partial reconstruction
// Do specify at which stage you want to look
int component_index = 5;
Mat current_slice = vectors.col(component_index);
// Images[0] is the first image and used for reshape properties
Mat projection = cv::LDA::subspaceProject(current_slice, mean, images[0].reshape(1,1));
Mat reconstruction = cv::LDA::subspaceReconstruct(current_slice, mean, projection);
// Then normalize and reshape the result if you want to visualize, as explained on the web page which I referred to.</pre></div><div><h3 class="title2"><a id="note109" class="calibre1"/>Note</h3><p class="calibre8">The software for executing this face detection can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_projection/</a></p></div><p class="calibre8">This will result in<a id="id615" class="calibre1"/> the output shown in the figure below. We re-project one of the test subjects at different stages in the Eigenspace, subsequently adding 25 eigenvectors to the representation. Here, you can clearly see that we have succeeded in reconstructing the individual in 12 steps. We can apply a similar procedure to the Fisherfaces. However, due to the fact that Fisherfaces have lower dimensionality, and the fact that we only look for features to distinguish between labels, we cannot expect a reconstruction that is as pure as it is with Eigenfaces.</p><div><img src="img/00101.jpeg" alt="Linear discriminant analysis using the Fisher criterion" class="calibre11"/><div><p class="calibre28">Reprojection result for both Eigenfaces and Fisherfaces</p></div></div><p class="calibre12"> </p><div><h3 class="title2"><a id="note110" class="calibre1"/>Note</h3><p class="calibre8">If you want to do further research on the algorithm specifics, I suggest reading a paper that describes the technique in more detail:</p><p class="calibre8">Belhumeur Peter N., João P. Hespanha, and David J. Kriegman, <em class="calibre10">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</em>, Pattern Analysis and Machine Intelligence, IEEE Transactions on 19.7 (1997): 711-720.</p></div></div><div><div><div><div><h3 class="title2"><a id="ch06lvl3sec36" class="calibre1"/>Local binary pattern histograms</h3></div></div></div><p class="calibre8">Instead of <a id="id616" class="calibre1"/>simply reducing the dimensionality to universal axes, another approach is the use of local feature extraction. By looking at local features rather than a complete global description of the feature, researchers have tried to cope with problems like partial occlusion, illumination, and small sample size. The use of local binary pattern intensity histograms is a technique that looks at local face information rather than looking at global face information for a single individual. Local binary patterns have their origin in texture analysis and have proven to be efficient at face recognition by focusing on very specific local textured areas. This measure is more prone to changing lighting conditions than the previous techniques.</p><div><h3 class="title2"><a id="note111" class="calibre1"/>Note</h3><p class="calibre8">The software for executing this face detection can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_recognition_LBPH/</a></p></div><p class="calibre8">The LBPH <a id="id617" class="calibre1"/>features are illustrated below. They clearly show a more local feature description than Eigenfaces or Fisherfaces.</p><div><img src="img/00102.jpeg" alt="Local binary pattern histograms" class="calibre11"/><div><p class="calibre28">Example face image and its ELBP projection</p></div></div><p class="calibre12"> </p><div><h3 class="title2"><a id="note112" class="calibre1"/>Note</h3><p class="calibre8">The software for executing this LBPH face projection can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/face/face_to_ELBP/</a></p></div><p class="calibre8">To build a LBP face recognizer interface using histograms of local binary patterns, you should use the following code snippet in OpenCV 3:</p><div><pre class="programlisting">// Again make sure that the data is available
vector&lt;Mat&gt; faces;
vector&lt;int&gt; labels;
Mat test_image = imread("/path/to/test/image.png");
// Now train the model, again overload functions are available
Ptr&lt;LBPHFaceRecognizer&gt; face_model = createLBPHFaceRecognizer();
face_modelàtrain(faces, labels);
// Now predict the outcome of a sample test image
int predict = face_modelàpredict(test_image);</pre></div><p class="calibre8">The LBPH interface<a id="id618" class="calibre1"/> also has an overload function, but this time related to the structure of the LBPH pattern and not the projection axes. This can be seen below:</p><div><pre class="programlisting">// functionality createLBPHFaceRecognizer(radius, neighbors, grid_X, grid_Y, treshold)
cv::createLBPHFaceRecognizer(1,8,8,8,123.0);
// Getting the properties can be done using the getInt function.
int radius = model-&gt;getRadius();
int neighbors = model-&gt;getNeighbors();
int grid_x = model-&gt;getGridX();
int grid_y = model-&gt;getGridY();
double threshold = model-&gt;getThreshold();</pre></div><p class="calibre8">Again, the function can operate with or without a threshold being set in advance. Getting or setting the parameters of the model can also be done using the specific getter and setter functions.</p><div><h3 class="title2"><a id="note113" class="calibre1"/>Note</h3><p class="calibre8">If you want to do further research on the algorithm specifics, I suggest reading a paper that describes the technique in more detail:</p><p class="calibre8">Ahonen Timo, Abdenour Hadid, and Matti Pietikäinen, <em class="calibre10">Face recognition with local binary patterns</em>, Computer vision-eccv 2004, Springer Berlin Heidelberg, 2004. 469-481.</p></div><p class="calibre8">We provided functionality for each of the above three interfaces, also calculating the number of test samples classified correctly and the ones classified incorrectly, as shown below. In the case of LBPH, this means that we have a correct classification rate on the test samples of 96.25%, which is quite amazing with the very limited training data of only eight samples per person.</p><div><img src="img/00103.jpeg" alt="Local binary pattern histograms" class="calibre11"/><div><p class="calibre28">The number of correctly classified samples is outputted after each run.</p></div></div><p class="calibre12"> </p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec66" class="calibre1"/>The problems with facial recognition in its current OpenCV 3 based implementation</h2></div></div></div><p class="calibre8">The techniques discussed <a id="id619" class="calibre1"/>enable us to recognize a face and link it to a person in the dataset. However, there are still some problems with this system that should be addressed:</p><div><ul class="itemizedlist"><li class="listitem">When using the Eigenfaces system, it is a general rule that the more Eigenvectors you use, the better the system will become, and the higher the accuracy will be. Defining how many dimensions you need to get a decent recognition result is frustrating, since it depends on how the data is presented to the system. The more variation there is in the original data, the more challenging the task will be, and thus the more dimensions you will need. The experiments of Philipp Wagner have shown that, in the AT&amp;T database, about 300 Eigenvectors should be enough.</li><li class="listitem">You can apply thresholding to both Eigenfaces and Fisherfaces. This is a must if you want to be certain of classification accuracy. If you do not apply this, then the system will basically return the best match. If a given person is not part of the dataset, then you want to avoid this, and that can be done by calling the interface with a threshold value!</li><li class="listitem">Keep in mind that, with all face recognition systems, if you train them with data in one setup and test them with data containing completely different situations and setups, then the drop in accuracy will be huge.</li><li class="listitem">If you build a recognition system based on 2D image information, then frauds will be able to hack it by simply printing a 2D image of the person and presenting it to the system. In order to avoid this, either include 3D knowledge or add extra biometrics.</li></ul></div><div><h3 class="title2"><a id="note114" class="calibre1"/>Note</h3><p class="calibre8">More information on adding 3D information to avoid fraud attempts can be found in the following publications:</p><p class="calibre8">Akarun Lale, B. Gokberk, and Albert Ali Salah, <em class="calibre10">3D face recognition for biometric applications</em>, Signal Processing Conference, 2005 13th European. IEEE, 2005.</p><p class="calibre8">Abate Andrea F., et al, <em class="calibre10">2D and 3D face recognition: A survey</em>, Pattern Recognition Letters 28.14 (2007): 1885-1906.</p><p class="calibre8">However, this topic is too specific and complex for the scope of this chapter, and will thus not be discussed further.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec48" class="calibre1"/>Fingerprint identification, how is it done?</h1></div></div></div><p class="calibre8">In the previous <a id="id620" class="calibre1"/>section, we discussed the use of the first biometric, which is the face of the person trying to log in to the system. However, since we mentioned that using a single biometric is risky, it is better to add secondary biometric checks to the system, like a fingerprint. There are several off-the-shelf fingerprint scanners that are quite cheap and return you a scanned image. However, you will still have to write your own registration software for these scanners, and this can be done with OpenCV. Examples of such fingerprint images can be found below:</p><div><img src="img/00104.jpeg" alt="Fingerprint identification, how is it done?" class="calibre11"/><div><p class="calibre28">Examples of single individual thumbprints from different scanners</p></div></div><p class="calibre12"> </p><p class="calibre8">This dataset can <a id="id621" class="calibre1"/>be downloaded from the FVC2002 competition website released by the University of Bologna. The website (<a class="calibre1" href="http://bias.csr.unibo.it/fvc2002/databases.asp">http://bias.csr.unibo.it/fvc2002/databases.asp</a>) contains four databases of fingerprints available for public download in the following format:</p><div><ul class="itemizedlist"><li class="listitem">Four fingerprint capturing devices, DB1 - DB4</li><li class="listitem">For each device, the prints of 10 individuals are available</li><li class="listitem">For each person, eight different positions of prints were recorded</li></ul></div><p class="calibre8">We will use this publicly available dataset to build our system. We will focus on the first capturing device, using up to four fingerprints from each individual for training the system and making an average descriptor of the fingerprint. Then, we will use the other four fingerprints to evaluate our system and make sure that the person is still recognized by our system.</p><p class="calibre8">You can apply the same approach to the data grabbed from the other devices if you want to investigate the difference between a system that captures binary images and one that captures grayscale images. However, we will provide techniques for doing the binarization yourself.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec67" class="calibre1"/>Implementing the approach in OpenCV 3</h2></div></div></div><div><h3 class="title2"><a id="note115" class="calibre1"/>Note</h3><p class="calibre8">The complete <a id="id622" class="calibre1"/>fingerprint software for processing fingerprints obtained from a fingerprint scanner can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/fingerprint/fingerprint_process/</a></p></div><p class="calibre8">In this subsection, we<a id="id623" class="calibre1"/> will describe how you can implement this approach in the OpenCV interface. We start by grabbing the image from the fingerprint system and applying binarization. This enables us to remove any noise from the image, as well as helping us to make the contrast better between the skin and the wrinkled surface of the finger:</p><div><pre class="programlisting">// Start by reading in an image
Mat input = imread("/data/fingerprints/image1.png", IMREAD_GRAYSCALE);
// Binarize the image, through local thresholding
Mat input_binary;
threshold(input, input_binary, 0, 255, THRESH_BINARY | THRESH_OTSU);</pre></div><p class="calibre8">The Otsu thresholding will automatically choose the best generic threshold for the image to obtain a good contrast between foreground and background information. This is because the image contains a bimodal distribution (which means that we have an image with two peak histograms) of pixel values. For that image, we can take an approximate value in the middle of those peaks as the threshold value (for images that are not bimodal, binarization won't be accurate). Otsu allows us to avoid using a fixed threshold value, making the system more compatible with capturing devices. However, we do acknowledge that, if you only have one capturing device, then playing around with a fixed threshold value may result in a better image for that specific setup. The result of the thresholding can be seen below.</p><p class="calibre8">In order to make the thinning from the next skeletization step as effective as possible, we need to invert the binary image.</p><div><img src="img/00105.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">Comparison of grayscale and binarized fingerprint images</p></div></div><p class="calibre12"> </p><p class="calibre8">Once we <a id="id624" class="calibre1"/>have a binary image, we are ready to calculate our feature points and feature point descriptors. However, in order to improve the process a bit more, it is better to skeletize the image. This will create more unique and stronger interest points. The following piece of code can apply the skeletization on top of the binary image. The skeletization is based on the Zhang-Suen line-thinning approach.</p><div><h3 class="title2"><a id="note116" class="calibre1"/>Note</h3><p class="calibre8">Special thanks to <code class="email">@bsdNoobz</code> of the OpenCV Q&amp;A forum, who supplied this iteration approach.</p></div><div><pre class="programlisting">#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;

using namespace std;
using namespace cv;

// Perform a single thinning iteration, which is repeated until the skeletization is finalized
void thinningIteration(Mat&amp; im, int iter)
{
    Mat marker = Mat::zeros(im.size(), CV_8UC1);
    for (int i = 1; i &lt; im.rows-1; i++)
    {
        for (int j = 1; j &lt; im.cols-1; j++)
        {
            uchar p2 = im.at&lt;uchar&gt;(i-1, j);
            uchar p3 = im.at&lt;uchar&gt;(i-1, j+1);
            uchar p4 = im.at&lt;uchar&gt;(i, j+1);
            uchar p5 = im.at&lt;uchar&gt;(i+1, j+1);
            uchar p6 = im.at&lt;uchar&gt;(i+1, j);
            uchar p7 = im.at&lt;uchar&gt;(i+1, j-1);
            uchar p8 = im.at&lt;uchar&gt;(i, j-1);
            uchar p9 = im.at&lt;uchar&gt;(i-1, j-1);

            int A  = (p2 == 0 &amp;&amp; p3 == 1) + (p3 == 0 &amp;&amp; p4 == 1) + 
                     (p4 == 0 &amp;&amp; p5 == 1) + (p5 == 0 &amp;&amp; p6 == 1) + 
                     (p6 == 0 &amp;&amp; p7 == 1) + (p7 == 0 &amp;&amp; p8 == 1) +
                     (p8 == 0 &amp;&amp; p9 == 1) + (p9 == 0 &amp;&amp; p2 == 1);
            int B  = p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9;
            int m1 = iter == 0 ? (p2 * p4 * p6) : (p2 * p4 * p8);
            int m2 = iter == 0 ? (p4 * p6 * p8) : (p2 * p6 * p8);

            if (A == 1 &amp;&amp; (B &gt;= 2 &amp;&amp; B &lt;= 6) &amp;&amp; m1 == 0 &amp;&amp; m2 == 0)
                marker.at&lt;uchar&gt;(i,j) = 1;
        }
    }

    im &amp;= ~marker;
}

// Function for thinning any given binary image within the range of 0-255. If not you should first make sure that your image has this range preset and configured!
void thinning(Mat&amp; im)
{
    // Enforce the range to be in between 0 - 255  
    im /= 255;

    Mat prev = Mat::zeros(im.size(), CV_8UC1);
    Mat diff;

    do {
        thinningIteration(im, 0);
        thinningIteration(im, 1);
        absdiff(im, prev, diff);
        im.copyTo(prev);
    } 
    while (countNonZero(diff) &gt; 0);

    im *= 255;
}</pre></div><p class="calibre8">The code above <a id="id625" class="calibre1"/>can then simply be applied to our previous steps by calling the thinning function on top of our previous binary-generated image. The code for this is:</p><div><pre class="programlisting">// Apply thinning algorithm
Mat input_thinned = input_binary.clone();
thinning(input_thinned);</pre></div><p class="calibre8">This will result in the following output:</p><div><img src="img/00106.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">Comparison of binarized and thinned fingerprint images using skeletization techniques</p></div></div><p class="calibre12"> </p><p class="calibre8">When we get this skeleton image, the next step is to look for crossing points on the ridges of the fingerprint, called minutiae points. We can do this with a keypoint detector that looks for large changes in local contrast, like the Harris corner detector. Since the Harris corner detector is able to detect strong corners and edges, it is ideal for the fingerprint problem, where the most important minutiae are short edges and bifurcations—the positions where edges come together.</p><div><h3 class="title2"><a id="note117" class="calibre1"/>Note</h3><p class="calibre8">More information about minutiae points and Harris corner detection can be found in the following publications:</p><p class="calibre8">Ross Arun A., Jidnya Shah, and Anil K. Jain, <em class="calibre10">Toward reconstructing fingerprints from minutiae points</em>, Defense and Security. International Society for Optics and Photonics, 2005.</p><p class="calibre8">Harris Chris and Mike Stephens, <em class="calibre10">A combined corner and edge detector</em>, Alvey vision conference, Vol. 15, 1988.</p></div><p class="calibre8">Calling the <a id="id626" class="calibre1"/>Harris Corner operation on a skeletonized and binarized image in OpenCV is quite straightforward. The Harris corners are stored as positions corresponding with their cornerness response value in the image. If we want to detect points with a certain cornerness, then we should simply threshold the image.</p><div><pre class="programlisting">Mat harris_corners, harris_normalised;
harris_corners = Mat::zeros(input_thinned.size(), CV_32FC1);
cornerHarris(input_thinned, harris_corners, 2, 3, 0.04, BORDER_DEFAULT);
normalize(harris_corners, harris_normalised, 0, 255, NORM_MINMAX, CV_32FC1, Mat());</pre></div><p class="calibre8">We now have a map with all the available corner responses rescaled to the range of [0 255] and stored as float values. We can now manually define a threshold which will generate a good number of keypoints for our application. Playing around with this parameter could improve performance in other cases. This can be done by using the following code snippet:</p><div><pre class="programlisting">float threshold = 125.0;
vector&lt;KeyPoint&gt; keypoints;
Mat rescaled;
convertScaleAbs(harris_normalised, rescaled);
Mat harris_c(rescaled.rows, rescaled.cols, CV_8UC3);
Mat in[] = { rescaled, rescaled, rescaled };
int from_to[] = { 0,0, 1,1, 2,2 };
mixChannels( in, 3, &amp;harris_c, 1, from_to, 3 );
for(int x=0; x&lt;harris_normalised.cols; x++){
   for(int y=0; y&lt;harris_normalised.rows; y++){
          if ( (int)harris_normalised.at&lt;float&gt;(y, x) &gt; threshold ){
             // Draw or store the keypoint location here, just like
             //you decide. In our case we will store the location of 
             // the keypoint
             circle(harris_c, Point(x, y), 5, Scalar(0,255,0), 1);
             circle(harris_c, Point(x, y), 1, Scalar(0,0,255), 1);
             keypoints.push_back( KeyPoint (x, y, 1) );
          }
       }
    }</pre></div><div><img src="img/00107.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">Comparison of thinned fingerprints and the Harris corner response, as well as the selected Harris corners</p></div></div><p class="calibre12"> </p><p class="calibre8">Now that we <a id="id627" class="calibre1"/>have a list of keypoints, we need to create some sort of formal descriptor of the local region around each keypoint to be able to uniquely identify it from other keypoints.</p><div><h3 class="title2"><a id="note118" class="calibre1"/>Note</h3><p class="calibre8"><a class="calibre1" title="Chapter 3. Recognizing Facial Expressions with Machine Learning" href="part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69">Chapter 3</a>, <em class="calibre10">Recognizing Facial Expressions with Machine Learning</em>, discusses in more detail the wide range of keypoints out there. In this chapter, we will mainly focus on the process. Feel free to adapt the interface to other keypoint detectors and descriptors out there, for better or for worse performance.</p></div><p class="calibre8">Since we have an application where the orientation of the thumb can differ (since it is not in a fixed position), we want a keypoint descriptor that is good at handling these slight differences. One of the most common descriptors for this is the SIFT descriptor, which stands for <strong class="calibre9">scale invariant feature transform</strong>. However, SIFT is not under a BSD license, which can pose problems when used in commercial software. A good alternative in OpenCV is the ORB descriptor. You can implement it in the following way:</p><div><pre class="programlisting">Ptr&lt;Feature2D&gt; orb_descriptor = ORB::create();
Mat descriptors;
orb_descriptor-&gt;compute(input_thinned, keypoints, descriptors);</pre></div><p class="calibre8">This enables us to calculate only the descriptors using the ORB approach, since we already retrieved the location of the keypoints using the Harris corner approach.</p><p class="calibre8">At this point, we can retrieve a descriptor for each detected keypoint of any given fingerprint. The descriptors matrix contains a row for each keypoint containing the representation.</p><p class="calibre8">Let's start with <a id="id628" class="calibre1"/>the example in which we have just one reference image for each fingerprint. We then have a database containing a set of feature descriptors for the training persons in the database. We have a single new entry, consisting of multiple descriptors for the keypoints found at registration time. We now have to match these descriptors to the descriptors stored in the database, to see which one has the best match.</p><p class="calibre8">The simplest way to do this is to perform brute-force matching using the hamming distance criteria between descriptors of different keypoints.</p><div><pre class="programlisting">// Imagine we have a vector of single entry descriptors as a database
// We will still need to fill those once we compare everything, by using the code snippets above
vector&lt;Mat&gt; database_descriptors;
Mat current_descriptors;
// Create the matcher interface
Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create("BruteForce-Hamming");
// Now loop over the database and start the matching
vector&lt; vector&lt; DMatch &gt; &gt; all_matches;
for(int entry=0; i&lt;database_descriptors.size();entry++){
   vector&lt; DMatch &gt; matches;
   matcheràmatch(database_descriptors[entry], current_descriptors, matches);
   all_matches.push_back(matches);
}</pre></div><p class="calibre8">We now have all the matches stored as DMatch objects. This means that, for each matching couple, we will have the original keypoint, the matched keypoint, and a floating point score between both matches, representing the distance between the matched points.</p><p class="calibre8">This seems pretty straightforward. We take a look at the number of matches that have been returned by the matching process and weigh them by their Euclidean distance in order to add some certainty. We then look for the matching process that yielded the biggest score. This will be our best match, and the match we want to return as the selected one from the database.</p><p class="calibre8">If you want to avoid an imposter getting assigned to the best matching score, you can add a manual threshold on top of the scoring to avoid matches and ignore those that are not good enough. However, it is possible that, if you increase the score too much, people with little change will be rejected from the system, if, for example, someone cuts their finger and thus changes their pattern drastically.</p><div><img src="img/00108.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">The fingerprint matching process visualized</p></div></div><p class="calibre12"> </p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec49" class="calibre1"/>Iris identification, how is it done?</h1></div></div></div><p class="calibre8">The<a id="id629" class="calibre1"/> last biometric that we will use is the output of an iris scan. Considering <a id="id630" class="calibre1"/>our setup, there might be several ways to grab iris data:</p><div><ul class="itemizedlist"><li class="listitem">We can separate the face and apply an eye detector using face detection, which can be done with a high-resolution camera. We can use the resulting regions to perform iris segmentation and classification.</li><li class="listitem">We can use a specific eye camera, which grabs an eye image to be classified. This can be done either with RGB or NIR.</li></ul></div><p class="calibre8">Since the first approach is prone to a lot of problems, such as the resulting eye image having a low resolution, a more common approach is to use a separate eye camera that grabs the eye. This is the method that we will use in this chapter. An example of a captured eye in both the RGB (visible colors) and NIR (near infra-red) spectrums is visualized below:</p><div><img src="img/00109.jpeg" alt="Iris identification, how is it done?" class="calibre11"/><div><p class="calibre28">An example of both a RGB and a NIR iris-based image</p></div></div><p class="calibre12"> </p><p class="calibre8">Using NIR<a id="id631" class="calibre1"/> images helps us in several ways:</p><div><ul class="itemizedlist"><li class="listitem">First of all, color information is omitted, since a lot of conditions like external sources of light can influence color information when grabbing the iris image.</li><li class="listitem">Secondly, the pupil center becomes clearer and fully black, which allows us to use techniques that depend on this for segmenting the pupil center.</li><li class="listitem">Thirdly, the available structure is maintained, under different lighting conditions, due to the NIR spectrum.</li><li class="listitem">Finally, the outer border of the iris region is clearer, and thus more easily separable.</li></ul></div><p class="calibre8">We will use data from the CASIA eye dataset<a id="id632" class="calibre1"/> for the iris recognition, which can be found at <a class="calibre1" href="http://biometrics.idealtest.org/">http://biometrics.idealtest.org/</a>. This dataset is publicly available for research and non-commercial purposes, and access can be requested through the site. A small part of it can be found in our software repository, where we have a right and a left eye from one individual, which we can now treat as two people since no two irises are identical. We have 10 samples for each eye, of which we will use eight to train and two to test.</p><p class="calibre8">The approach that we will implement for iris recognition is based on the technique suggested by John Daugman. The technique is widely accepted and used in commercial systems, and has thus proven its quality.</p><div><h3 class="title2"><a id="note119" class="calibre1"/>Note</h3><p class="calibre8">The original paper written by John Daugman can be found at: <a class="calibre1" href="http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf">http://www.cl.cam.ac.uk/~jgd1000/irisrecog.pdf</a></p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec68" class="calibre1"/>Implementing the approach in OpenCV 3</h2></div></div></div><p class="calibre8">The first step in <a id="id633" class="calibre1"/>getting the iris information is segmenting out the actual eye region, containing both the iris and the pupil. We apply a series of operations on top of our data to achieve the desired result. This process is necessary to keep only the desired data and remove all the redundant eye data that is still around.</p><p class="calibre8">We first try to get the pupil. The pupil is the darkest area in NIR images, and this information can be used to our advantage. The following steps will lead us to the pupil area in an eye image:</p><div><ul class="itemizedlist"><li class="listitem">First, we need to apply segmentation to the darkest regions. We can use the <code class="email">inRange()</code> image, since the values in which the pupil lie are specific to the capturing system. However, due to the fact that they all use NIR, the end result will be identical for each separate system.</li><li class="listitem">Then, we apply contour detection to get the outer border of the pupil. We make sure that we get the biggest contour from just the outer contours so that we only keep one region.</li></ul></div><div><h3 class="title2"><a id="note120" class="calibre1"/>Note</h3><p class="calibre8">If you want to improve performance, you can also look for the bright spots of the IR LED first, remove them from the region, and then run the contour detection. This will improve robustness when IR LED spots are close to the pupil border.</p><p class="calibre8">The code for the complete process of a single iris can be found at: <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing/</a></p></div><p class="calibre8">This behavior can be achieved by using the following code snippet:</p><div><pre class="programlisting">// Read in image and perform contour detection
Mat original = imread("path/to/eye/image.png", IMREAD_GRAYSCALE);
Mat mask_pupil;
inRange(original, Scalar(30,30,30), Scalar(80,80,80), mask_pupil);
vector&lt; vector&lt;Point&gt; &gt; contours;
findContours(mask_pupil.clone(), contours, RETR_EXTERNAL, CHAIN_APPROX_NONE);
// Calculate all the corresponding areas which are larger than
vector&lt; vector&lt;Point&gt; &gt; filtered;
for(int i=0; i&lt;contours.size(); i++){
   double area = contourArea(contours[i]);
   // Remove noisy regions
   if(area &gt; 50.0){
      filtered.push_back(contours[i]);
   }
}
// Now make a last check, if there are still multiple contours left, take the one that has a center closest to the image center
vector&lt;Point&gt; final_contour=filtered[0];
if(filtered.size() &gt; 1){
   double distance = 5000;
   int index = -1;
   Point2f orig_center(original.cols/2, original.rows/2); 
   for(int i=0; i&lt;filtered.size(); i++){
      Moments temp = moments(filtered[i]);
      Point2f current_center((temp.m10/temp.m00), (temp.m01/temp.m00));
      // Find the Euclidean distance between both positions
      double dist = norm(Mat(orig_center), Mat(current_center));
      if(dist &lt; distance){
         distance = dist;
         index = i;
      }   
   }
   final_contour = filtered[index];
}
// Now finally make the black contoured image;
vector&lt; vector&lt;Point&gt; &gt; draw;
draw.push_back(final_contour);
Mat blacked_pupil = original.clone();
drawContours(blacked_pupil, draw, -1, Scalar(0,0,0), FILLED);</pre></div><div><img src="img/00110.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><pre><a id="note121" class="calibre1"/>Note</h3><p class="calibre8">Sometimes, the Hough circle detection will not yield a single circle. This is not the case with<a id="id635" class="calibre1"/> the proposed database, but if you encounter this, then looking at other techniques like the Laplacian of Gaussians should help you to find and reconstruct the iris region.</pre></div><div><pre class="programlisting">// Make sure that the input image is gray, we took care of that while reading in the original image and making sure that the blacked pupil image is a clone of that.
// Apply a canny edge filter to look for borders
// Then clean it a bit by adding a smoothing filter, reducing noise
Mat preprocessed;
Canny(blacked_pupil, blacked_pupil, 5, 70);
GaussianBlur(blacked_pupil, preprocessed, Size(7,7));
// Now run a set of HoughCircle detections with different parameters
// We increase the second accumulator value until a single circle is left and take that one for granted
int i = 80;
vector&lt;Point3f&gt; found_circle;
while (i &lt; 151){
   vector&lt; vector&lt;Point3f&gt; &gt; storage;
   // If you use other data than the database provided, tweaking of these parameters will be necessary
   HoughCircles(preprocessed, storage, HOUGH_GRADIENT, 2, 100.0, 30, i, 100, 140);
   if(storage.size() == 1){
      found_circle = storage[0];
      break;
   }
   i++;
}
// Now draw the outer circle of the iris
int radius = found_circle[2];
Mat mask = Mat::zeros(blacked_pupil.rows, blacked_pupil.cols, CV_8UC1);
// The centroid value here must be the same as the one of the inner pupil so we reuse it back here
Moments temp = Moments(final_contour);
Point2f centroid((temp.m10/temp.m00), (temp.m01/temp.m00));
Circle(mask, centroid, radius, Scalar(255,255,255), FILLED);
bitwise_not(mask, mask);
Mat final_result;
subtract(blacked_pupil, blacked_pupil.clone(), final_result, mask);
// Visualize the final result
imshow("final blacked iris region", final_result);</pre></div><div><img src="img/00111.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">An example of the retrieved Hough Circle result, which gives us the outer border of the iris region, for both the left and right eyes.</p></div></div><p class="calibre12"> </p><p class="calibre8">Once we <a id="id636" class="calibre1"/>have succeeded in finding the outer contour, it is pretty straightforward to mask the iris region from the original input, as shown in the figure below:</p><div><img src="img/00112.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">An example of the masked iris image</p></div></div><p class="calibre12"> </p><p class="calibre8">We now <a id="id637" class="calibre1"/>have our region of interest, meaning only the iris region, as shown in the above figure. We acknowledge that there could still be some partial whiskers inside the region, but for now we will simply ignore them. Now, we want to encode this iris image into a feature vector for comparison. There are two steps still to take to reach that level:</p><div><ul class="itemizedlist"><li class="listitem">Unwrapping of the iris pattern from a polar coordinate system to a Cartesian coordinate system for further processing</li><li class="listitem">Applying encoding to the iris image and matching it against a database of known representations</li></ul></div><p class="calibre8">We start by providing a code snippet that will unwrap the desired iris region from the retrieved final result:</p><div><pre class="programlisting">// Lets first crop the final iris region from the image
int x = int(centroid[0] - radius);
int y = int(centroid[1] - radius);
int w = int(radius * 2);
int h = w;
Mat cropped_region = final_result( Rect(x,y,w,h) ).clone();
// Now perform the unwrapping
// This is done by the logpolar function who does Logpolar to Cartesian coordinates, so that it can get unwrapped properly
Mat unwrapped;
int center = (float(cropped_region.cols/2.0), float(cropped_region.cols /2.0));
LogPolar(image, unwrapped, c, 60.0, INTER_LINEAR +  WARP_FILL_OUTLIERS);
imshow("unwrapped image", unwrapped); waitKey(0);</pre></div><p class="calibre8">This will result in the following conversion, which gives us the radial unwrapping of the iris region, as shown in the figure below:</p><div><img src="img/00113.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">An example of the radially unwrapped iris image for both the left and right eyes</p></div></div><p class="calibre12"> </p><p class="calibre8">This radial <a id="id638" class="calibre1"/>unwrapping is done for all the eight training images that we have for each eye and for the two testing images that we also have for each eye. The Daugman approach applies phase quadrant modulation to encode the iris pattern. However, this is not yet implemented in OpenCV and is too complex for this chapter. Therefore, we decided to look for an available OpenCV implementation that could be used to match the irises. A good approach is to use the local binary pattern histogram comparison, since we are looking for something that can identify local textures, and this was also used for face recognition.</p><div><h3 class="title2"><a id="note122" class="calibre1"/>Note</h3><p class="calibre8">Software for unwrapping a complete set of iris images can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_processing_batch/</a></p><p class="calibre8">Software for creating the matching interface can be found at:</p><p class="calibre8"><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_6/source_code/iris/iris_recognition/</a></p></div><p class="calibre8">Finally, encoding<a id="id639" class="calibre1"/> works as follows in OpenCV 3:</p><div><pre class="programlisting">// Try using the facerecognizer interface for these irises
// Choice of using LBPH --&gt; local and good for finding texture
Ptr&lt;LBPHFaceRecognizer&gt; iris_model = createLBPHFaceRecognizer();
// Train the facerecognizer
iris_model-&gt;train(train_iris, train_labels);
// Loop over test images and match their labels
int total_correct = 0, total_wrong = 0;
for(int i=0; i&lt;test_iris.size(); i ++){
       int predict = iris_model-&gt;predict(test_iris[i]);
       if(predict == test_labels[i]){
            total_correct++;
       }else{
            total_wrong++;
       }
}</pre></div><p class="calibre8">We count the testing results again, which yields the result shown in the figure below:</p><div><img src="img/00114.jpeg" alt="Implementing the approach in OpenCV 3" class="calibre11"/><div><p class="calibre28">Encoded iris image and the corresponding iris code visualized.</p></div></div><p class="calibre12"> </p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec50" class="calibre1"/>Combining the techniques to create an efficient people-registration system</h1></div></div></div><p class="calibre8">The previous sections <a id="id640" class="calibre1"/>each discussed a specific biometric property. Now, let's combine all this information to create an efficient identification system. The approach that we will implement follows the structure described in the figure below:</p><div><img src="img/00115.jpeg" alt="Combining the techniques to create an efficient people-registration system" class="calibre11"/><div><p class="calibre28">People authentication pipeline</p></div></div><p class="calibre12"> </p><p class="calibre8">As shown above, the <a id="id641" class="calibre1"/>first step is to use a camera interface to check if there actually is a person in front of the camera. This is done by performing face detection on the input image. We also test to see if the other biometric systems are active. This leaves us two checks that need to be performed:</p><div><ul class="itemizedlist"><li class="listitem">Check if the iris scanner is in use. This, of course, depends on the system used. If it depends on the eye retrieved from the face detection, this check should be ignored. If the eye is retrieved using an actual eye scanner, then there should at least be an eye detected to give a positive signal.</li><li class="listitem">Check if the fingerprint scanner is active. Do we actually have a finger available for taking a fingerprint picture? This is checked by applying background subtraction to the empty scene. If a finger is in place, then there should be a response to the background-foreground subtraction.</li></ul></div><p class="calibre8">Of course, we are aware that some of these systems use pressure-based detection to find a hand or finger. In such cases, you do not have to perform this check yourself, but let the system decide whether to proceed or not.</p><p class="calibre8">Once we have all the systems, we can start the individual recognition systems described in previous sections. They will all output the identity of a known person from the common database that was constructed for this purpose. Then, all these outcomes are given to the smart majority voting. This system checks for several things:</p><div><ul class="itemizedlist"><li class="listitem">It checks if the biometric system checks actually succeeded, by returning their match from the database. If not, a person is not granted access and the system asks to reconfirm the failing biometrics.</li><li class="listitem">If the system has to measure biometrics more than three times in a row, the system jams and doesn't work until the owner of the system unlocks it. This is to avoid a bug in the current interface that exploits the system and tries to get in.</li><li class="listitem">If the <a id="id642" class="calibre1"/>biometric checks work, a smart majority voting is applied to the results. This means that if two biometrics identify person A but one biometric identifies person B, then the output result will still be person A. If that person is marked as the owner, then the system will allow access.</li><li class="listitem">Based on the individual software provided with the separate subtopics, it should be quite straightforward to combine them into a single interface.</li><li class="listitem">If the system still fails (this is a case study, not a 100% failproof system), there are several things that can be done to achieve the desired results.<div><ul class="itemizedlist1"><li class="listitem">You should try to improve the detection and matching quality of each separate biometric. This can be done by supplying better training data, experimenting with different feature extraction methods or different feature comparison methods, as discussed in the introduction to the chapter. The variety of combinations is endless, so go ahead and give it a try.</li><li class="listitem">You should try to give each biometric a certainty score on its output. Since we have multiple systems voting for the identity of a person, we could take into account their certainty on single classifications. For example, when running a database, matching the distance to the best match can be wrapped to a scale range of [0 100] to give a certainty percentage. We can then multiply the vote of each biometric by its weight and do a smart-weighted majority voting.</li></ul></div></li></ul></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec51" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, you learned that an authentication system can be more than a simple face recognition interface by using multiple biometric properties of the person trying to authenticate. We showed you how to perform iris and fingerprint recognition using the OpenCV library to make a multi-biometric authentication system. One can add even more biometrics to the system, since the possibilities are endless.</p><p class="calibre8">The focus of the chapter was to get people interested in the power of biometrics and the endless possibilities of the OpenCV library. If you feel inspired by this, do experiment further and share your thoughts with the community.</p><div><h3 class="title2"><a id="note123" class="calibre1"/>Note</h3><p class="calibre8">I would like to thank the users of the OpenCV Q&amp;A discussion forum who helped me to push the limits when I hit brick walls. I would explicitly like to thank the following users for their directions: Berak, Guanta, Theodore, and GilLevi.</p></div></div></body></html>