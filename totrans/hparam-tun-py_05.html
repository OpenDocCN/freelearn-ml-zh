<html><head></head><body>
		<div id="_idContainer156">
			<h1 id="_idParaDest-36"><a id="_idTextAnchor036"/><em class="italic">Chapter 4</em>: Exploring Bayesian Optimization</h1>
			<p><strong class="bold">Bayesian optimization</strong> (<strong class="bold">BO</strong>) is the second out of four groups<a id="_idIndexMarker090"/> of hyperparameter tuning methods. Unlike grid search and random search, which are categorized as uninformed search methods, all of the methods that belong<a id="_idIndexMarker091"/> to the BO group are categorized as <strong class="bold">informed search</strong> methods, meaning they are learning from previous iterations to (hopefully) provide a better search space in the future.</p>
			<p>In this chapter, we will discuss several methods that belong to the BO group, including <strong class="bold">Gaussian process</strong> (<strong class="bold">GP</strong>), <strong class="bold">sequential model-based algorithm configuration</strong> (<strong class="bold">SMAC</strong>), <strong class="bold">Tree-structured Parzen Estimators</strong> (<strong class="bold">TPE</strong>), and Metis. Similar to <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring Exhaustive Search</em>, we will discuss the definition of each method, the differences between them, how they work, and the pros and cons of each method.</p>
			<p>By the end of this chapter, you will be able to explain BO and its variations when someone asks you. You will not only be able to explain what they are, but also how they work, in a high-level and technical way. You will also be able to tell the differences between them, along with the pros and cons of each of the methods. Furthermore, you will experience a crucial benefit once you understand the ins and outs of each method; that is, you will be able to understand what’s happening if there are errors or unexpected results and understand how to set up the method configuration to match your specific problem.</p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>Introducing BO</li>
				<li>Understanding BO GP</li>
				<li>Understanding SMAC</li>
				<li>Understanding TPE</li>
				<li>Understanding Metis</li>
			</ul>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Introducing BO</h1>
			<p>BO is categorized as an informed search<a id="_idIndexMarker092"/> hyperparameter tuning method, meaning the search is learning from previous iterations to have a (hopefully) better subspace<a id="_idIndexMarker093"/> in the next iterations. It is also categorized as the <strong class="bold">sequential model-based optimization</strong> (<strong class="bold">SMBO</strong>) group. All SMBO methods work by sequentially updating probability models to estimate the effect of a set of hyperparameters on their performance based on historical observed data, as well as suggesting new hyperparameters to be tested in the following trials.</p>
			<p>BO is a popular hyperparameter tuning method due to its <em class="italic">data-efficient</em> property, meaning it needs a relatively small number of samples to get to the optimal solution. You may be wondering, how exactly does BO get this ground-breaking data-efficient property? This property exists thanks to BO’s ability to learn from previous iterations. BO can learn and predict<a id="_idIndexMarker094"/> which subspace is worth visiting in the future by utilizing a <strong class="bold">probabilistic regression model</strong>, which acts as the <em class="italic">cheap cloned version of the expensive objective function</em>, and an <strong class="bold">acquisition function</strong>, which governs which<em class="italic"> set of hyperparameters should be tested</em> in the next<a id="_idIndexMarker095"/> iteration.</p>
			<p>The objective function is just a function that takes hyperparameter values as input and returns the cross-validation score (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Evaluating Machine Learning Models</em>). We do not know what the output of the objective function for all possible hyperparameter values is. If we did, there would be no need to perform hyperparameter tuning. We could just use that function to get the hyperparameter values, which results in the highest cross-validation score. That’s why we need a probabilistic regression model, to approximate the objective function by fitting a set of known hyperparameter and cross-validation score value pairs (see <em class="italic">Figure 4.1</em>). The approximation concept is <em class="italic">similar to the concept of ML-based regressor</em> models, such as random forest, linear regression, and many more. First, we fit the regressor to the samples of independent and dependent variables; then, the model will try to <em class="italic">learn</em> from the data, which in the end can be<a id="_idIndexMarker096"/> used to predict new given data. The probabilistic regression model is also often called the <strong class="bold">surrogate model</strong>:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18753_04_001.jpg" alt="Figure 4.1 – Illustration of the probabilistic regression model, M&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Illustration of the probabilistic regression model, M</p>
			<p>The acquisition function <em class="italic">governs which subspace we should search in the next iteration</em>. Thanks<a id="_idIndexMarker097"/> to this function, BO enables us to learn from past experiences and have fewer hyperparameter tuning iterations compared to random search, in general. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Remember that, to get the cross-validation score, we need to perform multiple training and evaluation processes (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">,</em> <em class="italic">Evaluating Machine Learning Models</em>). This is an <em class="italic">expensive process</em> when you have a big, complex model with a large amount of training data. That’s why the acquisition function plays a big role here.</p>
			<p>In general, BO works<a id="_idIndexMarker098"/> as follows:</p>
			<ol>
				<li>Split the original full data into train and test sets. (See <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">,</em> <em class="italic">Evaluating Machine Learning Models</em>).</li>
				<li>Define the hyperparameter space, <em class="italic">H</em>, with the accompanied distributions.</li>
				<li>Define the objective function, <em class="italic">f</em>, based on the train set.</li>
				<li>Define the stopping criterion. Usually, the number of trials is used. However, it is also possible to use the time taken or convergence as the stopping criterion.</li>
				<li>Initializes the empty set, <em class="italic">D</em>, which will be used to store the initial pairs of hyperparameter values and cross-validation scores, as well as the resulting pairs suggested by the acquisition function, <em class="italic">A</em>.</li>
				<li>Initialize several pairs of hyperparameter<a id="_idIndexMarker099"/> values and cross-validation scores and store them in <em class="italic">D</em>.</li>
				<li>Fit the probabilistic regression model/surrogate model, <em class="italic">M</em>, using the value pairs in <em class="italic">D</em>.</li>
				<li>Sample the next set of hyperparameters by utilizing the acquisition function, <em class="italic">A</em>:<ol><li>Perform optimization on the acquisition function, <em class="italic">A</em>, with the help of the surrogate model, <em class="italic">M</em>, to sample which hyperparameters are to be passed to the acquisition function.</li><li>Get the expected optimal set of hyperparameters based on the acquisition function, <em class="italic">A</em>.</li></ol></li>
				<li>Compute the cross-validation score using the objective function, <em class="italic">f</em>, based on the output from <em class="italic">Step 8</em>.</li>
				<li>Add the hyperparameters and cross-validation score pair from <em class="italic">Step 8</em> and <em class="italic">Step 9</em> to set <em class="italic">D</em>.</li>
				<li>Repeat <em class="italic">Steps 7</em> to <em class="italic">10</em> until the stopping criterion is met.</li>
				<li>Trains on the full training set using the final hyperparameter values.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>You can initialize the hyperparameter values and cross-validation scores, as shown in <em class="italic">Step 6</em>, using several sampling strategies. The most <a id="_idIndexMarker100"/>straightforward and go-to way, in practice, is to just perform <strong class="bold">random sampling</strong>. However, there are also other methods<a id="_idIndexMarker101"/> that you may consider<a id="_idIndexMarker102"/> during your experiments, such as the <strong class="bold">quasi-random</strong> or <strong class="bold">Latin hypercube</strong> sampling methods. </p>
			<p>Similar to random search, we also need<a id="_idIndexMarker103"/> to define the distribution of each hyperparameter in BO. You may wonder if BO can also work on a non-numerical type of hyperparameter. The answer is <em class="italic">based on the probabilistic regression model</em> you are using. There are several surrogate models you can choose from. Those options<a id="_idIndexMarker104"/> will be discussed in the next<a id="_idIndexMarker105"/> three sections of this chapter, and they include <strong class="bold">GP</strong>, <strong class="bold">Tree-structured Parzen Estimator</strong> (<strong class="bold">TPE</strong>), random forest, extra trees, or other ML-based regressors. In this book, we will discuss the random forest regressor that’s implemented in the SMAC model.</p>
			<p>It is also worth noting that the optimization process in <em class="italic">Step 8</em> can be <em class="italic">replaced with a random search</em>. So, instead of performing some kind of second-order optimization method, we can randomly sample sets of hyperparameters from the search space and pass them onto the acquisition function. Then, we can get the<a id="_idTextAnchor038"/> optimal set of hyperparameters based on the output from the acquisition function. When using random search in this step, we still utilize the acquisition function to govern which subspace we should search for in the next iteration, but we add some random behavior to it, with the hope that we can escape the local optimum and converge toward the global optimum.</p>
			<p>The first and the most<a id="_idIndexMarker106"/> popular acquisition function is <strong class="bold">expected improvement</strong> (<strong class="bold">EI</strong>), which is defined as follows:</p>
			<p><img src="image/Formula_B18753_04_001.png" alt=""/> when <img src="image/Formula_B18753_04_002.png" alt=""/></p>
			<p><img src="image/Formula_B18753_04_003.png" alt=""/> when <img src="image/Formula_B18753_04_004.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_005.png" alt=""/>, <img src="image/Formula_B18753_04_006.png" alt=""/> and <img src="image/Formula_B18753_04_007.png" alt=""/> are the cumulative distribution and probability density functions of the standard normal distribution, respectively. <img src="image/Formula_B18753_04_008.png" alt=""/> and <img src="image/Formula_B18753_04_009.png" alt=""/> represent the expected performance and the uncertainty, respectively, that are captured by the surrogate model. Fi<a id="_idTextAnchor039"/>nally, <img src="image/Formula_B18753_04_010.png" alt=""/> represents the current best value of the objective function. </p>
			<p>Implicitly, the EI acquisition function<a id="_idIndexMarker107"/> enables BO methods to have the <em class="italic">exploration versus exploitation trade-off property</em>. This property can be achieved by two terms competing within the formula. When the value of the first term is high, meaning the expected performance, <img src="image/Formula_B18753_04_011.png" alt=""/>, is higher than the current best value, <img src="image/Formula_B18753_04_012.png" alt=""/>, EI will favor the exploitation process. On the other hand, when the uncertainty is very high, meaning we have a high value of <img src="image/Formula_B18753_04_013.png" alt=""/>, EI will favor the exploration process. By exploitation, this means that the acquisition function will recommend the set of hyperparameters that possibly get a higher value of the objective function, <em class="italic">f</em>. In terms of exploration, this means that the acquisition function will recommend the set of hyperparameters from the subspace that we haven’t explored yet.</p>
			<p>You can imagine this exploration and exploitation trade-off as when you are craving some food. Let’s say you want to have lunch with your brother today. Imagine the following two scenarios:</p>
			<ul>
				<li>“Hey bro, let’s have lunch at our favorite restaurant today!”</li>
				<li>“Hey bro, have you heard of the new restaurant up there? Why don’t we try it for lunch?”</li>
			</ul>
			<p>In the first scenario, you choose<a id="_idIndexMarker108"/> to eat at your favorite restaurant since you are confident that there is nothing wrong with the food and, more importantly, you are <em class="italic">confident about the taste of the food and the overall experience</em> of eating at that restaurant. This first scenario best explains what we call the exploitation process. In the second scenario, you <em class="italic">don’t have any idea what the overall experience</em> of eating at that new restaurant is. It may be worse than your favorite restaurant, but it may also potentially be your new favorite restaurant! This is what we call the exploration process.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In some<a id="_idIndexMarker109"/> implementations, such as in the <strong class="bold">Scikit-optimize</strong> package, there is a hyperparameter that enables us to <em class="italic">control how much we are leaning toward exploitation</em> compared to exploration. In Scikit-optimize, the sign of the EI function is negative. This is because the package <em class="italic">treats the optimization problem as the minimization problem</em> by default.</p>
			<p>In our previous explanation, we treated the optimization problem as the maximization problem since we wanted to get the highest cross-validation score possible. Don’t confuse this with the minimization versus maximization problem – just choose what best describes the problem you will be facing in practice!</p>
			<p>The following is the EI acquisition function that’s implemented in the Scikit-optimize package:</p>
			<p><img src="image/Formula_B18753_04_014.png" alt=""/></p>
			<p>As you can see in the first term, the value of <img src="image/Formula_B18753_04_015.png" alt=""/> will control how big our tendency is toward exploitation compared to exploration. The smaller the <img src="image/Formula_B18753_04_016.png" alt=""/> value is, the more we lean toward exploitation. We will learn more about the implementation part of BO using Scikit or other packages from <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em> to <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>.</p>
			<p>To get a better understanding of how the exploration and exploitation trade-off happens during the hyperparameter<a id="_idIndexMarker110"/> tuning phase, let’s look at an example. Let’s say, for instance, we are using the GP surrogate model to estimate the following objective function. There’s no need to worry about what and how GP works for now; we will discuss it in more detail in the next section:</p>
			<p><img src="image/Formula_B18753_04_017.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_018.png" alt=""/> is a noise that follows the standard normal distribution. The following is a plot of this function within the range of <img src="image/Formula_B18753_04_019.png" alt=""/>. Note that, in this example, we are assuming that we know what the true objective function is. However, in practice, this function is unknown:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18753_04_002.jpg" alt="Figure 4.2 – Plot of the objective function, f(x)&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Plot of the objective function, f(x)</p>
			<p>Let’s say we are using the EI as the acquisition function, setting the number of trials as <strong class="source-inline">15</strong>, setting the initial number of points as <strong class="source-inline">5</strong>, and setting the <img src="image/Formula_B18753_04_020.png" alt=""/> value to <strong class="source-inline">0.01</strong>. You can see how the fitting process works for the first five trials in the following figure:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18753_04_003.jpg" alt="Figure 4.3 – GP and EI illustration, δ = 0.01&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – GP and EI illustration, δ = 0.01</p>
			<p>Each row in the preceding figure corresponds to the first until the fifth trial. The left column contains information on the objective function (<em class="italic">red dashed line</em>), the GP surrogate model approximation of the objective function (<em class="italic">green dashed line</em>), how sure the approximation is (<em class="italic">green transparent area</em>), and the observed points up to each trial (<em class="italic">red dots</em>). The right column<a id="_idIndexMarker111"/> contains information on the EI acquisition function values (<em class="italic">blue line</em>) and the next point (<em class="italic">blue dot</em>) to be included in the next trials. </p>
			<p>Let’s run through each of the rows in <em class="italic">Figure 4.3</em> so that you understand how it works. In the first trial (<em class="italic">see the first row from the top in the left column</em>), we initialize five random sample points – or hyperparameter values, in the context of hyperparameter tuning – and fit the GP model based on those five points. Remember that the GP model doesn’t know the actual objective function; the only information it has is just those five random points. Then (<em class="italic">see the first row from the top in the right column</em>), based on the fitted GP model, we get the value of the EI acquisition function across the space. In this case, the space is just a range – that is, <img src="image/Formula_B18753_04_022.png" alt=""/>. We also get the point to be included in the next trials, which in this case is around point <strong class="source-inline">0.5</strong>. </p>
			<p>In the second trial, we utilize<a id="_idIndexMarker112"/> the point suggested by the EI acquisition function and fit the GP model again based on the six sample points we have (<em class="italic">see the second row from the top in the left column</em>). If you compare the GP approximation of the second trial with the first trial, you will see that it is closer to the true objective function. Next (<em class="italic">see the second row from the top in the right column</em>), we repeat the same process, which is to generate the EI function value across the space and the point to be included in the next trial. The suggested point in this step is around <strong class="source-inline">0.7</strong>.</p>
			<p>We keep repeating the same process until the stopping criteria are met, which in this case is 15 trials. The following plot shows the result after 15 trials. It is much better than the approximation in the first trial (<em class="italic">see the green dashed line</em>)! You can also see that there are some ranges of <img src="image/Formula_B18753_04_023.png" alt=""/> where the confidence of the GP approximation is high, such as around points <strong class="source-inline">–1.5</strong> and <strong class="source-inline">1.6</strong>:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B18753_04_004.jpg" alt="Figure 4.4 – Result after 15 trials, δ = 0.01&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Result after 15 trials, δ = 0.01</p>
			<p>Based on the preceding plot, the final suggested<a id="_idIndexMarker113"/> point, or the hyperparameter value, is <strong class="source-inline">–1.5218</strong>, which results in the value of the objective function being equal to <strong class="source-inline">–1.9765</strong>. Let’s also look at the convergence plot from the first until the last trial. From the following convergence plot, we can see how our surrogate model and acquisition function help us get the minimum value of the objective function based on all the trials:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B18753_04_005.jpg" alt="Figure 4.5 – Convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Convergence plot</p>
			<p>Now, let’s try to change the value of <img src="image/Formula_B18753_04_025.png" alt=""/> to a lower value than what we had previously to see how the EI acquisition function<a id="_idIndexMarker114"/> will favor exploitation more than exploration. Let’s set the <img src="image/Formula_B18753_04_026.png" alt=""/> value to be 1,000 times lower than the previous value. Note that we only change the <img src="image/Formula_B18753_04_027.png" alt=""/>value and leave the other setups as-is:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18753_04_006.jpg" alt="Figure 4.6 – GP and EI illustration, δ = 0.00001&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – GP and EI illustration, δ = 0.00001</p>
			<p>As you can see, the EI acquisition function suggested most of the points in a range between <strong class="bold">0.5</strong> and <strong class="bold">1.4</strong>. The acquisition function<a id="_idIndexMarker115"/> doesn’t suggest exploring the <img src="image/Formula_B18753_04_029.png" alt=""/> range, although we can get a much lower objective function value in that range. This happens because there are no initial random points in that range, and we favor exploitation a lot in this example. The following plot shows the final results after 15 trials. In this case, we get a worse result when we favor more exploitation over exploration. However, this is not always the case. <em class="italic">You have to experiment</em> since different data, different objective functions, a different hyperparameter space, and different implementations may result in different conclusions:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18753_04_007.jpg" alt="Figure 4.7 – Result after 15 trials, δ = 0.00001&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Result after 15 trials, δ = 0.00001</p>
			<p>Now, let’s see what the impact is if we set the <img src="image/Formula_B18753_04_031.png" alt=""/> value to <strong class="source-inline">100</strong>, which in this case means that we favor exploration<a id="_idIndexMarker116"/> more than exploitation. Similar to the previous trial, after running 15 trials, we got the following results:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18753_04_008.jpg" alt="Figure 4.8 – Result after 15 trials, δ = 100&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Result after 15 trials, δ = 100</p>
			<p>As you can see, the points that are suggested by the acquisition function (<em class="italic">the red dots</em>) are all over the place. This is because<a id="_idIndexMarker117"/> we set such a high <img src="image/Formula_B18753_04_033.png" alt=""/> value. This means that the acquisition function’s outputs will suggest points in the space that haven’t been observed yet. We will learn how to produce the plots shown here in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, <em class="italic">Hyperparameter Tuning via Scikit</em>.</p>
			<p>Besides the EI acquisition<a id="_idIndexMarker118"/> function, there are also other popular acquisition<a id="_idIndexMarker119"/> functions that you may consider using, including <strong class="bold">Probability of Improvement</strong> (<strong class="bold">PI</strong>) and <strong class="bold">Upper Confidence Bound</strong> (<strong class="bold">UCB</strong>). </p>
			<p>PI is the acquisition function that existed before EI. It is simpler than EI – in fact, the formula of <img src="image/Formula_B18753_04_034.png" alt=""/> is derived based on the following simple definition of <em class="italic">improvement</em>:</p>
			<p><img src="image/Formula_B18753_04_035.png" alt=""/></p>
			<p>The idea of <img src="image/Formula_B18753_04_036.png" alt=""/> is to return the size of improvement, if there is improvement between the expected performance<a id="_idIndexMarker120"/> and the current best performance, or just return zero if there is no improvement. Based on <img src="image/Formula_B18753_04_037.png" alt=""/>, we can define PI as follows:</p>
			<p><img src="image/Formula_B18753_04_038.png" alt=""/> when <img src="image/Formula_B18753_04_039.png" alt=""/></p>
			<p><img src="image/Formula_B18753_04_040.png" alt=""/> when <img src="image/Formula_B18753_04_041.png" alt=""/></p>
			<p>The problem with PI is that it will <em class="italic">give the same reward for all sets of hyperparameters</em>, so long as there’s an improvement compared to the current best value, <img src="image/Formula_B18753_04_042.png" alt=""/>, no matter how big the improvement is. This behavior is not very preferable in practice since it can <em class="italic">guide us to the local minima and get us stuck in there</em>. If you are familiar with calculus and statistics, you will realize that EI is just the expectation over <img src="image/Formula_B18753_04_037.png" alt=""/>, as shown here:</p>
			<p><img src="image/Formula_B18753_04_044.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_045.png" alt=""/> is the probability density function of the standard normal distribution. Unlike PI, the <em class="italic">EI acquisition function will take the size of improvement into account</em>.</p>
			<p>As for the UCB, it is very straightforward<a id="_idIndexMarker121"/> compared to others. We have the power to control the trade-off between exploration and exploitation by ourselves via the <img src="image/Formula_B18753_04_046.png" alt=""/>parameter. This acquisition function can be defined as follows:</p>
			<p><img src="image/Formula_B18753_04_047.png" alt=""/></p>
			<p>As you can see, UCB <em class="italic">doesn’t take into account the current best value</em> of the objective function. It only considers the expected performance and the uncertainty captured by the surrogate model. You can control the exploration and exploitation trade-off by changing the <img src="image/Formula_B18753_04_048.png" alt=""/>value. If you want to lean toward exploring the search space, then you can increase the value of <img src="image/Formula_B18753_04_049.png" alt=""/>. However, if you want to focus more on the set of hyperparameters that are expected to perform well, then you can decrease the value of <img src="image/Formula_B18753_04_050.png" alt=""/>.</p>
			<p>Apart from the variations of surrogate model and acquisition functions, there are also other variations of BO methods based on modifying<a id="_idIndexMarker122"/> the algorithm itself, including Metis and <strong class="bold">Bayesian optimization and HyperBand</strong> (<strong class="bold">BOHB</strong>). We will discuss Metis in the <em class="italic">Understanding Metis</em> section and BOHB in <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>, <em class="italic">Exploring </em><em class="italic">Multi-Fidelity Optimization</em>.</p>
			<p>The following are the pros and cons<a id="_idIndexMarker123"/> of BO hyperparameter tuning, in general, compared<a id="_idIndexMarker124"/> to other hyperparameter tuning methods:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18753_04_009.jpg" alt="Figure 4.9 – Pros and cons of BO&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Pros and cons of BO</p>
			<p>BO can handle expensive objective functions and is more data-efficient and arguably better than random search when it has good initial points. You can utilize the set of hyperparameters we used for the initial points up to <em class="italic">Step 6</em> from the procedure mentioned at the beginning of this section. However, if you don’t have that privileged access, BO still can outperform random search if you give the method some more time since it has to build a good surrogate model first from scratch, especially if you have a huge hyperparameter space. Once BO has built a good surrogate model, it tends to work faster than random search to find the optimal set of hyperparameters. </p>
			<p>There is also another way to speed up the relatively slow warm-up process of BO. The idea is to adopt a <strong class="bold">meta-learning</strong> procedure to initialize the initial set of hyperparameters by learning from meta-features in other, similar datasets. </p>
			<p class="callout-heading">Speeding Up BO’s Warm-Up</p>
			<p class="callout">See the following paper for more information: <em class="italic">Efficient and Robust Automated Machine Learning</em>, by Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter (<a href="https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html">https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html</a>).</p>
			<p>BO also has a nice feature that random search doesn’t have – the ability to control the exploration and exploitation trade-off, as explained previously in this section. This feature enables BO to do more than just constantly explore, as random search does.</p>
			<p>Now that you are aware of what BO is, how it works, what its important components are, and the pros and cons of this method, we will dive deeper into the variations of BO in the following sections.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor040"/>Understanding BO GP </h1>
			<p><strong class="bold">Bayesian optimization Gaussian process</strong> (<strong class="bold">BOGP</strong>) is one of the variants of the BO hyperparameter tuning method. It is well-known for its good capability in describing the objective function. This variant is very popular due to the unique <em class="italic">analytically tractable</em> nature of the surrogate model and its ability to produce relatively accurate approximation, even with only a few observed points.</p>
			<p>However, BOGP has limitations. It <em class="italic">only works on continuous hyperparameters</em>, not on the discrete or categorical types of hyperparameters. It is not recommended to use BOGP when you need a lot of iterations to get the optimal set of hyperparameters, especially when you have a large number of samples. This is BOGP has a <img src="image/Formula_B18753_04_051.png" alt=""/> runtime, where <img src="image/Formula_B18753_04_052.png" alt=""/> is the number of samples. If you have <em class="italic">more than 10 hyperparameters</em> to be optimized, the common belief is that BOGP is not the right hyperparameter tuning method for you.</p>
			<p>Having GP as the surrogate model means that we utilize GP as the <em class="italic">prior</em> for our objective function. Then, we can utilize the prior along with a <em class="italic">likelihood model</em> to compute the <em class="italic">posterior</em> that we care about. All of these nerdy terms can easily be understood if we are familiar with the famous <strong class="bold">Bayes Theorem</strong>.</p>
			<p>Bayes Theorem allows us to calculate the probability of an event, given a specific condition, by utilizing our previous knowledge or common belief that we have. Formally, Bayes Theorem is defined as follows:</p>
			<p><img src="image/Formula_B18753_04_053.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_054.png" alt=""/>is the event we want to know the probability of, and <img src="image/Formula_B18753_04_116.png" alt=""/> refers to the specific condition we mentioned previously. The left-hand side of the equation, <img src="image/Formula_B18753_04_055.png" alt=""/>, is what we called as the posterior. <img src="image/Formula_B18753_04_056.png" alt=""/> is the prior and <img src="image/Formula_B18753_04_057.png" alt=""/> is what we call the likelihood model. Finally, <img src="image/Formula_B18753_04_058.png" alt=""/> is <a id="_idTextAnchor041"/>just a constant to ensure that the resulting value of this formula is bound in the range of <img src="image/Formula_B18753_04_059.png" alt=""/>.</p>
			<p>To understand Bayes Theorem, let’s walk through an example. Let’s say we want to know the probability of you eating at your favorite restaurant, given that today’s weather is sunny. In this example, you eating at your favorite restaurant is the event we are interested in. This is <img src="image/Formula_B18753_04_060.png" alt=""/> in the equation. The information that today is sunny refers to <img src="image/Formula_B18753_04_117.png" alt=""/> in the equation. </p>
			<p>Let’s say you are eating at your favorite restaurant for 40 out of 100 days. This means that before knowing what today’s weather is, your <img src="image/Formula_B18753_04_061.png" alt=""/> is equal to <img src="image/Formula_B18753_04_062.png" alt=""/>. Let’s also assume that out of 100 days, there are 30 sunny days. Then, the <img src="image/Formula_B18753_04_063.png" alt=""/> value is equal to <img src="image/Formula_B18753_04_064.png" alt=""/>. Based on your experience of eating at your favorite restaurant, you have realized that you ate in the sunny weather condition 20 out of 40 times. Thus, the likelihood, <img src="image/Formula_B18753_04_065.png" alt=""/>, is equal to <img src="image/Formula_B18753_04_066.png" alt=""/>. Using all of this information, we can calculate the probability of you eating at your restaurant, given that today’s weather is sunny, as <img src="image/Formula_B18753_04_067.png" alt=""/></p>
			<p>Now, we are ready to revisit the GP. BOGP utilizes GP as the surrogate model. GP as the surrogate model means that we utilize it as the prior of our objective function, which implies that the <em class="italic">posterior distribution is also a GP</em>. You can think of GP as a generalization of a Gaussian distribution that you are familiar with. Unlike Gaussian distribution, which describes the distribution of a random variable, <em class="italic">GP describes the distribution over functions</em>. Similar to the Gaussian distribution that is accompanied by the mean and variance of the random variable, GP is also accompanied by the <em class="italic">mean and covariance</em> of the function. As for the <em class="italic">likelihood</em>, we assume that the objective function, <em class="italic">f</em>, follows a normal likelihood with noise:</p>
			<p><img src="image/Formula_B18753_04_068.png" alt=""/></p>
			<p><img src="image/Formula_B18753_04_069.png" alt=""/></p>
			<p>Then, we can describe <img src="image/Formula_B18753_04_070.png" alt=""/>, or the values of our objective function for all <em class="italic">n</em> samples. as a GP with a mean function of <img src="image/Formula_B18753_04_071.png" alt=""/> and a covariance kernel, <img src="image/Formula_B18753_04_072.png" alt=""/>, sized <em class="italic">n x n</em>, which is defined as follows:</p>
			<p><img src="image/Formula_B18753_04_073.png" alt=""/></p>
			<p>The distribution of prediction from GP also follows the Gaussian distribution, which can be defined as follows:</p>
			<p><img src="image/Formula_B18753_04_074.png" alt=""/></p>
			<p>Here, the value of <img src="image/Formula_B18753_04_075.png" alt=""/>and <img src="image/Formula_B18753_04_076.png" alt=""/> can be <em class="italic">analytically derived from the kernel</em>, <img src="image/Formula_B18753_04_077.png" alt=""/>.</p>
			<p>To summarize, <em class="italic">GP approximates the objective function by following a normal distribution assumption</em>. In practice, GP can also be utilized when we don’t have zero mean processes, as per our previous<a id="_idIndexMarker125"/> assumption. However, we need to do some preprocessing on the values of the objective function to center them to zero. Choosing the <em class="italic">right covariance kernel</em>, <img src="image/Formula_B18753_04_078.png" alt=""/>, is also crucial. It highly impacts the performance of our hyperparameter tuning process. The most popular kernel that’s used in practice is the <em class="italic">Matern kernel</em>. However, we must choose the right kernel for our case, since each kernel has a characteristic that may or may not be suitable for our objective function. We will discuss the kernels that are available in the Scikit package in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, <em class="italic">Hyperparameter Tuning via Scikit</em>.</p>
			<p>The following table shows the list<a id="_idIndexMarker126"/> of pros and cons of BOGP compared to other variants of the BO hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18753_04_010.jpg" alt="Figure 4.10 – Pros and cons of BOGP&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Pros and cons of BOGP</p>
			<p>In the previous section, we saw how GP works in practice, where we discussed the exploration and exploitation trade-off. You can revisit that example to get a better understanding of how GP works in practice through the help of visualizations. </p>
			<p>In this section, we learned about utilizing GP as the surrogate model in BO, along with the pros and cons compared to other variants of BO. In the next section, we will learn about another variant of BO<a id="_idIndexMarker127"/> that utilizes random forest as the surrogate model. </p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor042"/>Understanding SMAC </h1>
			<p><strong class="bold">SMAC</strong> is part of the BO hyperparameter tuning<a id="_idIndexMarker128"/> method group and utilizes random forest as the surrogate model. This method is optimized to handle discrete or categorical hyperparameters. If your hyperparameter space is huge and is dominated by discrete hyperparameters, then SMAC is a good choice for you. </p>
			<p>Similar to BOGP, SMAC also works by modeling the objective function. Specifically, it utilizes random forest as the surrogate model to create an estimation of the real objective function, which can then be passed to the acquisition function (see the <em class="italic">Introducing BO</em> section for more details).</p>
			<p>Random forest is a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) algorithm that can be utilized<a id="_idIndexMarker129"/> in classification<a id="_idIndexMarker130"/> or regression tasks. It is built upon a collection of decision trees, which is known to perform well with categorical types of features. The name random forest comes from the fact that it is built from several decision trees. We will discuss random forest, along with its hyperparameters, in more detail in <a href="B18753_11_ePub.xhtml#_idTextAnchor110"><em class="italic">Chapter 11</em></a>, <em class="italic">Understanding Hyperparameters of Popular Algorithms</em>.</p>
			<p>The main difference between SMAC and BOGP lies in the type of surrogate model that’s used in each method. While BOGP utilizes GP as the surrogate model, SMAC utilizes random forest as the surrogate model. The acquisition function that was used in the original paper on SMAC is the <em class="italic">EI function with some modifications</em> on how the optimization process in <em class="italic">Step 8</em> in the <em class="italic">Introducing BO</em> section is done, which also can be seen in the following screenshot:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18753_04_011.jpg" alt="Figure 4.11 – Optimization process of the acquisition function &#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Optimization process of the acquisition function </p>
			<p>In SMAC, similar to BOGP, we are also assuming<a id="_idIndexMarker131"/> that the distribution of our surrogate model’s <em class="italic">prediction follows the Gaussian distribution</em>, as shown here:</p>
			<p><img src="image/Formula_B18753_04_079.png" alt=""/> </p>
			<p>Here, the <img src="image/Formula_B18753_04_080.png" alt=""/>and <img src="image/Formula_B18753_04_081.png" alt=""/> values are derived from the <em class="italic">random forest prediction’s mean and variance</em>, respectively.</p>
			<p>We can also <em class="italic">utilize random forest to perform hyperparameter tuning on a random forest model</em>! How is this possible? How can a model be used to improve the performance of another model of the same type?</p>
			<p>It is possible because we are treating one model as the surrogate model while the other one is the actual model that is fitted to the independent variables to predict the dependent variable. As the surrogate model, random forest will act as the regressor, which has the goal of learning the relationship between the hyperparameter space and the corresponding objective function. So, when we said that we are utilizing random forest to perform hyperparameter tuning on a random forest model, there are two random forest models with different goals and different input-output pairs! </p>
			<p>Take a look at the following <a id="_idIndexMarker132"/>steps to get a better understanding of this concept. Note that the following procedure replaces <em class="italic">Steps 7</em> to <em class="italic">11</em> in the <em class="italic">Introducing BO</em> section:</p>
			<p>6. (The first few steps are the same as we saw earlier).</p>
			<p>7. Fit the <em class="italic">first random forest model</em>, which acts as a surrogate model, <em class="italic">M</em>, using the value pairs in <em class="italic">D</em>. Remember that <em class="italic">D</em> consists of pairs of hyperparameter values and the cross-validation score.</p>
			<p>8. Sample the next set of hyperparameters by utilizing the acquisition function, <em class="italic">A</em>:</p>
			<ol>
				<li>Perform optimization on the acquisition function with the help of the surrogate model, <em class="italic">M</em>, to sample which hyperparameters are to be passed to the acquisition function.</li>
				<li>Get the optimal set of hyperparameters based on the acquisition function.</li>
			</ol>
			<p>9. Compute the cross-validation score using the objective function, <em class="italic">f</em>, based on the output from <em class="italic">Step 8</em>. Note that the cross-validation score is computed based on the <em class="italic">second random forest model</em>, whose goal is to learn the relationship between the dependent and independent variables from our original problem.</p>
			<p>10. Add the hyperparameters and cross-validation score pair from <em class="italic">Step 8</em> and <em class="italic">Step 9</em> to set <em class="italic">D</em>.</p>
			<p>11. Repeat <em class="italic">Steps 7</em> to <em class="italic">10</em> until the stopping criteria are met.</p>
			<p>12. (The last few steps are the same as we saw earlier).</p>
			<p>You may be wondering, why bother utilizing the same ML algorithm as the surrogate model? Why don’t we just perform a grid search or random search instead? Remember that the surrogate model is just one piece of the full BO algorithm. There is also the acquisition function and other optimization steps that can help us get the optimal set of hyperparameters faster. It is worth noting that <em class="italic">we can utilize any ML model</em> other than random forest. When it comes to tree-based ML models, XGBoost, CatBoost, and LightGBM are also popular among data scientists since they work well in practice.</p>
			<p>In the <em class="italic">Introducing BO</em> section, we saw how GP<a id="_idIndexMarker133"/> works with the EI acquisition function to estimate a dummy objective function. Let’s use the same dummy objective function, as defined here, and see the result of utilizing random forest (not necessarily the SMAC algorithm) as the surrogate model instead of GP. We will still use EI as the acquisition function in this example and the Scikit-optimize package as the implementation:</p>
			<p><img src="image/Formula_B18753_04_082.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_083.png" alt=""/> is a noise that follows the standard normal distribution. Please see <em class="italic">Figure 4.2</em> for a visualization of this dummy objective function.</p>
			<p>Let’s set the number of trials and the exploitation versus exploration trade-off controller, <img src="image/Formula_B18753_04_084.png" alt=""/>, using the default values given by the Scikit-optimize package for the random forest surrogate model, which are <strong class="source-inline">100</strong> and <strong class="source-inline">0.01</strong>, respectively. You can see how the random forest surrogate model fitting process works for the first five trials in the following figure:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B18753_04_012.jpg" alt="Figure 4.12 – Random forest and EI illustration; δ = 0.01; trials 1 – 5&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Random forest and EI illustration; δ = 0.01; trials 1 – 5</p>
			<p>As you can see, not many things happened<a id="_idIndexMarker134"/> in the first five trials. Even the approximation of the objective function that’s given by the random forest (<em class="italic">see the green-dashed line</em>) is still very bad since it is just a straight line! Let’s see what the condition is during trials 71 until 75:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B18753_04_013.jpg" alt="Figure 4.13 – Random forest and EI illustration; δ = 0.01; trials 71- 75&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Random forest and EI illustration; δ = 0.01; trials 71- 75</p>
			<p>Here, we can see that our random<a id="_idIndexMarker135"/> forest surrogate model has improved a lot in estimating the true objective function. One interesting point is that the acquisition function curve looks very different from the one we saw when utilizing GP as the surrogate model. Here, the acquisition function looks edgier, just like the one we usually see from visualizing random forest. Finally, let’s see what the final form of the approximated function is:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B18753_04_014.jpg" alt="Figure 4.14 – Result after 100 trials; δ = 0.01&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Result after 100 trials; δ = 0.01</p>
			<p>Here, we can see that random forest<a id="_idIndexMarker136"/> fails to fit the true objective function in general, but it succeeds to focus on the local minima of the objective function. This happens because <em class="italic">random forest needs a lot of data</em>, or in this case, the observed points (<em class="italic">see red dots</em>), to have a good approximation of the objective function. You can also see the convergence plot of the fitting process, starting from the first until the last trial, in the following plot. If we compare <em class="italic">Figure 4.15</em> to <em class="italic">Figure 4.5</em>, we can easily see that, in this example, random forest, when supported by the EI acquisition function, learns much slower than GP supported by EI:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B18753_04_015.jpg" alt="Figure 4.15 – Convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Convergence plot</p>
			<p>From <em class="italic">Figure 4.14</em>, we can also<a id="_idIndexMarker137"/> see that, currently, we are only focusing on several ranges and missing the global minima of the dummy objective function, which is located around the <img src="image/Formula_B18753_04_088.png" alt=""/> range. Let’s see if changing the value of <img src="image/Formula_B18753_04_089.png" alt=""/> to <strong class="source-inline">100</strong> can solve this issue. The expectation is that the EI acquisition function can help the random forest surrogate model <em class="italic">explore more</em> in other ranges of values as well. You can see the result of the first five trials in the following figure:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B18753_04_016.jpg" alt="Figure 4.16 – Random forest and EI illustration; δ = 100; trials 1 – 5&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Random forest and EI illustration; δ = 100; trials 1 – 5</p>
			<p>Similar to the first five trials <a id="_idIndexMarker138"/>of the default <img src="image/Formula_B18753_04_091.png" alt=""/> value, we still can’t see much of the learning process. Let’s see what the condition is during trials 71 until 75:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B18753_04_017.jpg" alt="Figure 4.17 – Random forest and EI illustration; δ = 100; trials 71 – 75&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Random forest and EI illustration; δ = 100; trials 71 – 75</p>
			<p>Here, we can see a very<a id="_idIndexMarker139"/> big difference between <em class="italic">Figure 4.17</em> and <em class="italic">Figure 4.13</em>. Finally, let’s see what the final form of the approximated function is:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B18753_04_018.jpg" alt="Figure 4.18 – Result after 100 trials; δ = 100&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – Result after 100 trials; δ = 100</p>
			<p>By changing the value of <img src="image/Formula_B18753_04_094.png" alt=""/> to <strong class="source-inline">100</strong>, it seems that our expectation has been achieved. The approximation from<a id="_idIndexMarker140"/> the random forest surrogate model (<em class="italic">see the green-dashed line</em>) is now focusing on more than specific ranges. Moreover, we even get a better result compared to GP (see <em class="italic">Figure 4.4</em>). Again, it is worth noting that this is not always the case – you must experiment a lot on your own since different data, different objective functions, a different hyperparameter space, and different implementations may result in different conclusions. We will learn how to implement random forest as the surrogate model and how to produce these figures in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, <em class="italic">Hyperparameter Tuning via Scikit</em>.</p>
			<p>There is another method, called <strong class="bold">Bayesian optimization inside a Grove</strong> (<strong class="bold">BOinG</strong>), whose goal is to get the best<a id="_idIndexMarker141"/> of both worlds by utilizing random forest and GP as surrogate models. </p>
			<p class="callout-heading">Bayesian Optimization Inside a Grove</p>
			<p class="callout">See the following paper for more information: <em class="italic">Searching in the Forest for Local Bayesian Optimization</em>, by Difan Deng and Marius Lindauer (<a href="https://arxiv.org/abs/2111.05834">https://arxiv.org/abs/2111.05834</a>).</p>
			<p>BOinG<a id="_idIndexMarker142"/> works by using <em class="italic">two-stage optimization</em> by using global and local models to cut down the computational cost<a id="_idIndexMarker143"/> and focus more on the promising subspace, respectively. In BOinG, random forest is utilized as the global model and GP as the local model. The global model is responsible for searching the promising subspace of the local model. Thus, a global model should be flexible enough to handle complex problems with different types of hyperparameters. Since the local model only searches in a promising subspace, it is possible to utilize a more accurate but expensive model, such as GP.</p>
			<p>The following table lists the pros and cons of utilizing random forest as a surrogate model compared to other variants of the BO hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B18753_04_019.jpg" alt="Figure 4.19 – Pros and cons of utilizing random forest as a surrogate model&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – Pros and cons of utilizing random forest as a surrogate model</p>
			<p>A conditional hyperparameter is a hyperparameter that will only be utilized when a certain condition is met. The tree structure of random forest is very suitable for this kind of situation since it can just add another branch of the tree to check whether the condition is met or not. The condition is usually just a specific value or range of other hyperparameters in the space.</p>
			<p>Now that you are aware of SMAC<a id="_idIndexMarker144"/> and utilizing random forest as a surrogate model in general, in the next section, we will discuss another variant of BO that has a different approach in terms of approximating the objective function.</p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor043"/>Understanding TPE</h1>
			<p><strong class="bold">TPE</strong> is another variant of BO that performs<a id="_idIndexMarker145"/> well in general and can be utilized for both categorical and continuous types of hyperparameters. Unlike BOGP, which has cubical time complexity, TPE runs in linear time. TPE is suggested if you have a huge hyperparameter space and have a very tight budget for evaluating the cross-validation score. </p>
			<p>The main difference between TPE and BOGP or SMAC is in the way that it models the relationship between hyperparameters and the cross-validation score. Unlike BOGP or SMAC, which approximate the value of the objective function, or the posterior probability, <img src="image/Formula_B18753_04_095.png" alt=""/>, <em class="italic">TPE works the other way around</em>. It tries to get the optimal hyperparameters based on the condition of the objective function, or the likelihood probability, <img src="image/Formula_B18753_04_096.png" alt=""/> (see the explanation of Bayes Theorem in the <em class="italic">Understanding BO GP</em> section). </p>
			<p>In other words, unlike BOGP or SMAC, which construct a predictive distribution over the objective function, TPE tries to utilize the information of the objective function to <em class="italic">model the hyperparameter distributions</em>. To be more precise, when the optimization problem is in the form of a <em class="italic">minimization problem</em>, <img src="image/Formula_B18753_04_097.png" alt=""/> is defined as follows:</p>
			<p><img src="image/Formula_B18753_04_098.png" alt=""/></p>
			<p>Here, <img src="image/Formula_B18753_04_099.png" alt=""/> and <img src="image/Formula_B18753_04_100.png" alt=""/> are utilized when the value<a id="_idIndexMarker146"/> of the objective function<a id="_idIndexMarker147"/> is lower or higher than the threshold, <img src="image/Formula_B18753_04_101.png" alt=""/>, respectively. There is no specific rule on how to choose the threshold, <img src="image/Formula_B18753_04_102.png" alt=""/>. However, in the <strong class="bold">Hyperopt</strong> and <strong class="bold">Microsoft NNI</strong> implementations, this threshold is chosen based on the TPE’s hyperparameter, <img src="image/Formula_B18753_04_103.png" alt=""/>, and the number of observed points in <em class="italic">D</em> up to the current trial. The definition of <img src="image/Formula_B18753_04_104.png" alt=""/> tells us that TPE has two models that act as the learning algorithm<a id="_idIndexMarker148"/> based on the value of the objective function, which is ruled by the threshold, <img src="image/Formula_B18753_04_105.png" alt=""/>.</p>
			<p>When the <em class="italic">distribution of hyperparameters is continuous</em>, TPE will utilize <strong class="bold">Gaussian mixture models</strong> (<strong class="bold">GMMs</strong>), along with the EI acquisition function, to suggest<a id="_idIndexMarker149"/> the next set of hyperparameters to be tested. If the continuous distribution is not a Gaussian distribution, then TPE will convert it to mimic the Gaussian distribution. For example, if the specified hyperparameter distribution is the uniform distribution, then it will be converted into a truncated Gaussian distribution. </p>
			<p>The probabilities of the different possible outcomes for the multinomial distribution within the GMM, and the mean and variance<a id="_idIndexMarker150"/> values for the normal distribution within the GMM, are generated by the <strong class="bold">adaptive Parzen estimator</strong>. This estimator is responsible for constructing the two probability distributions, <img src="image/Formula_B18753_04_106.png" alt=""/> and <img src="image/Formula_B18753_04_107.png" alt=""/>, based on the mean and variance of the normal hyperparameter distribution, as well as the hyperparameter value of all observed points in <em class="italic">D</em> up to the current trial.</p>
			<p>When the <em class="italic">distribution is categorical or discrete</em>, TPE will convert the categorical distribution into a re-weighted categorical and use <em class="italic">weighted random sampling</em>, along with the EI acquisition function, to suggest the expected best set of hyperparameters. The weights in the random sampling procedure are generated based on the historical counts of the hyperparameter value. </p>
			<p>The EI acquisition function definition in TPE is a bit different from the definition we learned about in the <em class="italic">Introducing BO</em> section. In TPE, we are using Bayes Theorem when deriving the EI formula. The simple formulation of the EI acquisition function in TPE is defined as follows: </p>
			<p> <img src="image/Formula_B18753_04_108.png" alt=""/></p>
			<p>The proportionality defined here tells<a id="_idIndexMarker151"/> us that to get a high value of EI, we need to get a high <img src="image/Formula_B18753_04_109.png" alt=""/> ratio. In other words, when the optimization problem is in the form of a <em class="italic">minimization problem</em>, the EI acquisition function must suggest more hyperparameters from <img src="image/Formula_B18753_04_110.png" alt=""/> over <img src="image/Formula_B18753_04_111.png" alt=""/>. It is the other way around when the optimization problem is in the form of a <em class="italic">maximization problem</em>. For example, when we use accuracy to measure the performance of our classification model, then we should sample more hyperparameters from <img src="image/Formula_B18753_04_112.png" alt=""/> over <img src="image/Formula_B18753_04_113.png" alt=""/>.</p>
			<p>To summarize, TPE works as follows. Note that the following procedure describes how TPE works for the <em class="italic">minimization problem</em>. This procedure replaces <em class="italic">Steps 7</em> to <em class="italic">11</em> in the <em class="italic">Introducing BO</em> section:</p>
			<p>6.  (The first few steps are the same as we saw earlier).</p>
			<p>7.  Divide pairs of hyperparameter values and cross-validation scores in <em class="italic">D</em> into two groups based on the threshold, <img src="image/Formula_B18753_04_114.png" alt=""/>, namely <em class="italic">below</em> and <em class="italic">above</em> groups (see <em class="italic">Figure 4.19</em>). </p>
			<p>8.  Sample the next set of hyperparameters<a id="_idIndexMarker152"/> by utilizing the EI acquisition function:</p>
			<ol>
				<li value="1">For each group, calculate the probabilities, means, and variances for the GMM using the adaptive Parzen estimator (if it’s a continuous type) or weights for random sampling (if it’s a categorical type).</li>
				<li>For each group, fit the GMM (if it’s a continuous type), or perform random sampling (if it’s a categorical type), to sample which hyperparameters will be passed to the EI acquisition function.</li>
				<li>For each group, calculate the probability of those samples being good samples (for the below group), or the probability of those samples being bad samples (for the above group).</li>
				<li>Get the expected optimal set of hyperparameters based on the EI acquisition function.</li>
			</ol>
			<p>9.  Compute the cross-validation score using the objective function, <em class="italic">f</em>, based on the output from <em class="italic">Step 8</em>. </p>
			<p>10.  Add the hyperparameters and cross-validation score pair from <em class="italic">Step 8</em> and <em class="italic">Step 9</em> to set <em class="italic">D</em>.</p>
			<p>11.  Repeat <em class="italic">Steps 7</em> to <em class="italic">10</em> until the stopping criteria have been met.</p>
			<p>12.  (The last few steps are the same as we saw earlier):</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B18753_04_020.jpg" alt="Figure 4.20 – Illustration of groups division in TPE&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20 – Illustration of groups division in TPE</p>
			<p>Based on the stated procedure and the preceding plot, we can see that, unlike BOGP or SMAC, which constructs a predictive<a id="_idIndexMarker153"/> distribution over the objective function, TPE tries to utilize the information of the objective function to model the hyperparameter distributions. This way, we are not only focusing on the best-observed points during the trials – we are focusing on the <em class="italic">distribution of the best-observed points</em> instead.</p>
			<p>You may be wondering why the <em class="italic">Tree-structured</em> term is within the TPE method’s name. This term refers to the conditional hyperparameters<a id="_idIndexMarker154"/> that we discussed in the previous section. This means that there are hyperparameters in the space that will only be utilized when a certain condition is met. We will see what a tree-structured or conditional hyperparameter space looks like in <a href="B18753_08_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 8</em></a>, <em class="italic">Hyperparameter Tuning via Hyperopt</em>, and <a href="B18753_09_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 9</em></a>, <em class="italic">Hyperparameter Tuning via Optuna</em>.</p>
			<p>One of the drawbacks that TPE has is that it may <em class="italic">overlook the interdependencies among hyperparameters</em> in a certain space since the Parzen estimators work univariately. However, this is not the case for BOGP or SMAC, since the surrogate model is constructed based on the configurations in the hyperparameter space. Thus, they can take into account the interdependencies among hyperparameters. Fortunately, there<a id="_idIndexMarker155"/> is an implementation of TPE that overcomes this drawback. The <strong class="bold">Optuna</strong> package provides the <strong class="bold">multivariate TPE</strong> implementation, which can take<a id="_idIndexMarker156"/> into account the interdependencies among hyperparameters. </p>
			<p>The following table lists of pros and cons of utilizing TPE compared to other variants of the BO hyperparameter<a id="_idIndexMarker157"/> tuning method:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B18753_04_021.jpg" alt="Figure 4.21 – Pros and cons of TPE&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.21 – Pros and cons of TPE</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Some implementations support parallel tuning, but with a trade-off between the suggested hyperparameter quality and the wall time. The Microsoft NNI package supports this feature via the <strong class="source-inline">constant_liar_type</strong> argument, which will be discussed in more detail in <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>.</p>
			<p>In this section, we learned about<a id="_idIndexMarker158"/> TPE, along with its pros and cons compared to other variants of BO. In the next section, we will learn about another variant of BO that has a slightly modified algorithm compared to the BO method in general.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor044"/>Understanding Metis</h1>
			<p>Metis is one of the variants<a id="_idIndexMarker159"/> of BO that has several algorithm modifications compared to the BO method in general. Metis utilizes GP and GMM in its algorithm. GP is used as the surrogate model and outliers detector, while GMM is used as part of the acquisition function, similar to TPE. </p>
			<p>What makes Metis different from other BO methods, in general, is that it can <em class="italic">balance exploration and exploitation more data-efficiently</em> than the EI acquisition function. It can also <em class="italic">handle noise in the data that doesn’t follow the Gaussian</em> distribution, and this is the case most of the time. Unlike most of the methods that perform random sampling to initialize the set of hyperparameters<a id="_idIndexMarker160"/> and cross-validation score, <em class="italic">D</em>, Metis utilizes <strong class="bold">Latin Hypercube Sampling</strong> (<strong class="bold">LHS</strong>), which is a stratified sampling procedure based on the equal interval of each hyperparameter. This sampling method is believed to be more data-efficient compared to random sampling to achieve the same exploration coverage.</p>
			<p>So, how can Metis balance exploration and exploitation more efficiently than the EI acquisition function, in terms of the needs of the observed points? This is achieved through the <em class="italic">custom acquisition function</em> that Metis has, which consists of three sub-acquisition functions, as shown here:</p>
			<ul>
				<li><strong class="bold">Lowest confidence</strong> (<strong class="bold">LC</strong>): This sub-acquisition function’s goal<a id="_idIndexMarker161"/> is to sample hyperparameters <a id="_idIndexMarker162"/>with the highest uncertainty. In other words, the goal of this sub-acquisition function is to <em class="italic">maximize exploration</em>. This function is defined as follows:</li>
			</ul>
			<p><img src="image/Formula_B18753_04_115.png" alt=""/></p>
			<ul>
				<li><strong class="bold">Parzen estimator</strong>: This sub-acquisition function<a id="_idIndexMarker163"/> is inspired by the TPE method, which utilizes<a id="_idIndexMarker164"/> GMM to estimate how likely the sampled hyperparameter is part of the <em class="italic">below</em> or <em class="italic">above</em> group (see the <em class="italic">Understanding TPE</em> section for more details). The goal of this sub-acquisition function is to sample hyperparameters with the highest probability to be the optimum<a id="_idIndexMarker165"/> hyperparameters. In other<a id="_idIndexMarker166"/> words, it is <em class="italic">optimized for exploitation</em>.</li>
				<li><strong class="bold">Outliers detector</strong>: As its name suggests, the goal of this<a id="_idIndexMarker167"/> sub-acquisition function is to <em class="italic">detect outliers within D</em>. The detected outlier will then be suggested<a id="_idIndexMarker168"/> as the candidate to be resampled in the next trial. Metis utilized GP to build the outliers detector or the <strong class="bold">diagnostic model</strong>. This diagnostic model works by comparing<a id="_idIndexMarker169"/> each of the cross-validation scores in <em class="italic">D</em> with the mean and standard deviation estimated by the GP. If the absolute difference between the cross-validation score and the estimated mean is greater than some constant multiplied by the estimated standard deviation, then it is flagged as an outlier. In other words, the diagnostic model will mark the hyperparameter as an outlier if it <em class="italic">lies outside the confidence interval of the GP estimation</em>. The constant for the 98% confidence interval is <strong class="source-inline">2.326</strong>.</li>
			</ul>
			<p>Based on candidates suggested by these three sub-acquisition functions, Metis will then compute their <em class="italic">information gain</em> to select the final candidate to be included in the next trial. This selection process is done by utilizing the lower bound of the GP estimation confidence interval. Metis will measure the difference between the lower bound of the interval and the expected mean from GP. The candidate that has the highest improvement will be selected as the final candidate. </p>
			<p>It is worth noting that Metis can handle non-Gaussian noise in the data because of the diagnostic model. The detected outliers made it possible for Metis to resample the previously tested hyperparameters so that it is robust to non-Gaussian noise as well. This way, Metis can <em class="italic">balance exploration, exploitation, and re-sampling</em> during the hyperparameter tuning process.</p>
			<p>To have a better understanding<a id="_idIndexMarker170"/> of how Metis works, take a look at the following procedure. Note that the following procedure replaces <em class="italic">Steps 6</em> to <em class="italic">11</em> in the <em class="italic">Introducing BO</em> section.</p>
			<p>5.  (The first few steps are the same as we saw earlier).</p>
			<p>6.  Initialize several pairs of hyperparameter values and cross-validations scores using the LHS method, and store them in <em class="italic">D</em>. </p>
			<p>7.  Fit a GP that acts as a surrogate model, <em class="italic">M</em>, using the value pairs in <em class="italic">D</em>.</p>
			<p>8.  Sample the next set of hyperparameters by utilizing the <em class="italic">custom acquisition function</em>, which consists of three sub-acquisition functions:</p>
			<ol>
				<li value="1">Get the current best optimum set of hyperparameters</li>
				<li>Get the suggested<a id="_idIndexMarker171"/> hyperparameters for <em class="italic">exploration</em> via the LC sub-acquisition function</li>
				<li>Get the suggested hyperparameters for <em class="italic">exploitation</em> via the Parzen estimator</li>
				<li>Get the suggested hyperparameters to be resampled based on the <em class="italic">detected outliers</em> by the diagnostic model.</li>
				<li>Calculate the <em class="italic">information gain</em> from each suggested candidate.</li>
				<li>Select the candidate that has the highest information gain.</li>
				<li>If no candidate is suggested, then pick one random candidate.</li>
			</ol>
			<p>9.  Compute the cross-validation score using the objective function, <em class="italic">f</em>, based on the output from <em class="italic">Step 8</em>. Note that the cross-validation score is computed based on the <em class="italic">second random forest model</em>, whose goal is to learn the relationship between the dependent and independent variables from our original problem.</p>
			<p>10.  Add the hyperparameters and cross-validation score pair from <em class="italic">Step 8</em> and <em class="italic">Step 9</em> to set <em class="italic">D</em>.</p>
			<p>11.  Repeat <em class="italic">Steps 7</em> to <em class="italic">10</em> until the stopping criteria are met.</p>
			<p>12.  (<em class="italic">The last few steps are the same as we saw earlier</em>).</p>
			<p>The following table lists the pros and cons of utilizing Metis compared to other variants of the BO hyperpara<a id="_idTextAnchor045"/>meter<a id="_idIndexMarker172"/> tuning method:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B18753_04_022.jpg" alt="Figure 4.22 – Pros and cons of Metis&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.22 – Pros and cons of Metis</p>
			<p>It is also worth<a id="_idIndexMarker173"/> noting that, unlike other BO variants, there is only one package that implements Metis<a id="_idIndexMarker174"/> for the hyperparameter tuning method, which is <strong class="bold">Microsoft NNI</strong>. As you may have noticed, all the variants of BO that were discussed in this chapter have the drawback of not being able to exploit parallel computing resources. So, why didn’t we put that drawback in the first section instead? Because there is a variant of BO, namely BOHB, that can exploit the parallel computing resources. We will discuss BOHB in more detail in <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>, <em class="italic">Exploring Multi-Fidelity Optimization</em>.</p>
			<p>In this section, we covered Metis in detail, including, what it is, how it works, what makes it different from other BO variants, and its pros and cons.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor046"/>Summary</h1>
			<p>In this chapter, we discussed the second out of four groups of hyperparameter tuning methods, called the BO group. We not only discussed BO in general but also several of its variants, including BOGP, SMAC, TPE, and Metis. We saw what makes each of the variants differ from each other, along with the pros and cons of each. At this point, you should be able to explain BO with confidence when someone asks you and apply hyperparameter tuning methods in this group with ease.</p>
			<p>In the next chapter, we will start discussing heuristic search, the third group of hyperparameter tuning methods. The goal of the next chapter is similar to this chapter: to provide a better understanding of the methods that belong to the heuristic search group.</p>
		</div>
	</body></html>