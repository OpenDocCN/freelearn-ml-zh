- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Designing Databricks: Day One'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Design is not just what it looks like and feels like. Design is how it works.”
  prefs: []
  type: TYPE_NORMAL
- en: '- Steve Jobs'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will introduce concepts and topics that engineers, data scientists,
    and people in similar roles should know to set themselves up for success in the
    Databricks Data Intelligence Platform. When setting up your data and AI platform,
    in our case, Databricks, there are always best practices to follow. We share those
    in this chapter to give you a better understanding of the setup options and their
    impacts; these can be strategic decisions that impact the entire data product
    workflow, as well as simply matters of preference. We start by explaining Databrick’s
    general architecture and key terminology, then cover the most important decisions
    to be made during platform setup, and conclude with code examples and configurations
    to download the data for our example projects. We also introduce a variety of
    platform features and components throughout the chapter, which we will cover in
    more detail throughout the rest of this book. Here is what you will learn as part
    of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn about as part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning your platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the metastore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning to create features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling in Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning your platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers topics for discussion before and during the DI Platform
    setup process. The role of the data team often determines the platform setup.
    One of Databricks’ ideal attributes is that the technology stack is unified, making
    the setup and collaboration between teams more straightforward. The data team
    reporting structure frequently determines the border where one role ends and another
    begins, rather than the actual data product workflow. Luckily, we do not have
    to worry because the DI Platform serves data engineers, scientists, and analysts
    alike.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2**.1*, you can see an end-to-end lakehouse architecture and the
    components in Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform
    fits this paradigm](img/B16865_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform
    fits this paradigm
  prefs: []
  type: TYPE_NORMAL
- en: The DI Platform consists of one or more Databricks accounts. Most of the time,
    companies only have one. However, there are situations where companies require
    extra environment isolation, and having separate accounts for development, staging,
    and production is an option. Discussion about multiple accounts for levels of
    isolation is outside of this book's scope, but if you have questions or want to
    know more, please check out the resources in *Further reading*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Visual representation of the environment isolation options](img/B16865_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Visual representation of the environment isolation options
  prefs: []
  type: TYPE_NORMAL
- en: We separate our environments using different catalogs. Most of this book’s project
    work occurs using the `ml_in_action` catalog. For the production version of some
    models, we use the `ml_in_prod` catalog. Setting up multiple workspaces is another
    way to separate environments. We recommend using documentation and your company
    policies to guide your isolation setup. Let’s move on to what precisely a workspace
    is in the context of Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a workspace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s important to know that Databricks uses the word **workspace** to refer
    to two distinct components: an instance of Databricks (meaning your hosted Databricks
    deployment that you access via your unique URL address) and the folder environment
    for accessing your work products, like notebooks, queries, and dashboards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Workspace as an instance**: A Databricks account can have multiple workspaces
    attached to it, meaning instances of the DI Platform are deployed and often accessible
    from a browser, as mentioned previously, but are also accessible via an SDK or
    a REST API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Workspace` folder to store their MLFlow experiments or Terraform states for
    pipeline deployment. You can also create and store notebooks outside source control
    in your home and project folders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now have a clearer understanding of a workspace. Now let’s discuss why we
    choose **Unity Catalog** (**UC**) as our preferred metastore.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the metastore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A metastore is a system that stores metadata for a data platform and can be
    thought of as the top-level container of objects. It registers a variety of information
    about databases, tables, views, **User-Defined Functions** (**UDFs**), and other
    data assets. Metadata includes details such as storage location and the permissions
    that govern access to each asset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of metastores are natively available in the DI Platform: **Unity
    Catalog** (**UC**) and the **Hive Metastore** (**HMS**). UC has a three-level
    namespace consisting of a catalog, a database (also called a schema), and a table
    name. In contrast, the HMS only uses a two-level namespace containing just a database
    and table name. A metastore is required for your Databricks Workspace instance,
    as this is the component that organizes and governs data access. Deciding on the
    right metastore is an early decision in your DI Platform journey, and we recommend
    Unity Catalog. Let’s talk about why.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice in *Figure 2**.3* that you can have multiple workspaces assigned to the
    same metastore. **Access Controls** and **User Management** are scoped to the
    account level, as shown in the figure. A UC **Metastore**, a group of catalogs,
    is scoped to a region with precisely one metastore per region. Within the region,
    you can easily share **Data**, **Features**, **Volumes** access, **Functions**,
    and **Models**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The Design of Unity Catalog with multiple workspaces](img/B16865_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The Design of Unity Catalog with multiple workspaces
  prefs: []
  type: TYPE_NORMAL
- en: Unity Catalog is more than a group of data assets. UC also tracks who has accessed
    assets, which makes auditing a simple exercise. Using UC allows companies to administer
    privileges and secure data and objects easily while being able to share them between
    various workspaces. Securely sharing between environments is one of the reasons
    why we recommend using the UC metastore.
  prefs: []
  type: TYPE_NORMAL
- en: The HMS design is less centralized than that of UC. For example, historically,
    workspaces have been created as data and code isolation, meaning there is a separate
    workspace for separate isolation levels. This design often required a centralized
    model registry workspace in addition to development, staging, and production workspaces.
    If not using UC, each workspace requires its own HMS and user and group management.
    In contrast, UC governs all assets at the account level rather than the individual
    workspace level; see *Figure 2**.3*, *Figure 2**.4*, and *Further reading*. The
    centralized governance model provides the ability to integrate multiple workspaces
    more seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – UC governs all assets under catalogs, including databases, tables,
    volumes, functions, and models](img/B16865_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – UC governs all assets under catalogs, including databases, tables,
    volumes, functions, and models
  prefs: []
  type: TYPE_NORMAL
- en: Deciding on your metastore does not have to be a permanent choice. However,
    migrating later could become a headache. UC is continually improving and integrating
    with new Databricks features. The list of reasons to choose UC over HMS continues
    to grow, and our recommendation is to begin with and stick with UC.
  prefs: []
  type: TYPE_NORMAL
- en: To determine whether Unity Catalog is the right choice for you and your company,
    you can check out the shrinking list of limitations for choosing UC in *Further
    reading*. As UC continues to expand in capability, it is the path of the future.
    Specifically, for machine learning, there is an integration with the Feature Engineering
    client and the new Model Registry. Using the UC Model Registry for model sharing
    and governing is simpler. We will cover more about the Model Registry in Unity
    Catalog and Databricks Feature Engineering Client in *Chapters 5, 6*, and *7*,
    but if you’re curious and eager to learn more now, you can check out Manage model
    lifecycle in Unity Catalog in the Further reading section. Given the ever-growing
    number of reasons to use UC, all project code in this book will use UC.
  prefs: []
  type: TYPE_NORMAL
- en: Defining where the data lives, and cloud object storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All data products start with data, and so how we make the data accessible for
    data teams is another important early design choice. Databricks is a cloud-based
    platform that connects to cloud object storage – **Azure Data Lake Storage** (**ADLS**),
    Amazon **Simple Storage Service** (**S3**), or **Google Cloud Storage** (**GCS**).
    There is a separation of compute and storage. Databricks orchestrates compute;
    the data is in cloud object storage.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, data had to be in cloud object storage before being utilized on
    the DI Platform. Now, **Query Federation** allows customers to query their data
    no matter where it resides (see the documentation for any possible limitations)
    without first worrying about ingestion and data engineering from that remote system.
    However, that is data in memory, not persistent data. You can land your data in
    cloud storage in various ways. There are many documentation sources and external
    tools for the actual landing of data in cloud storage. These may depend on your
    cloud service provider of choice. Despite best practices of storing data in your
    cloud storage, using the **Databricks File System** (**DBFS**) to store the data
    for this book’s example projects is also possible.
  prefs: []
  type: TYPE_NORMAL
- en: DBFS is a shared filesystem provided by Databricks that all users of a given
    workspace can access. Any data stored in DBFS is potentially accessible to all
    users, regardless of their group, role, or permissions. Therefore, only non-sensitive
    and non-production data you are willing to share openly across your organization
    should be in DBFS. An example of non-sensitive data would be the publicly available
    *Kaggle* datasets. This lack of governance is why we recommend storing data in
    Databricks volumes, where you can apply governance. We will cover more on volumes
    in the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to cloud storage, the optimal format for structured data is almost
    always Delta, which we talked about in detail in [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016).
    When a table is stored in the Delta format, we refer to it as a Delta table. You
    can choose tables to be “managed” or “external” tables. We use both types of tables
    in this book (the choice is justified when required). Please see the resources
    in *Further reading* for more information on the two types of tables.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing source control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether or not to use a source control is usually not the question. The question
    is how should someone use it? Databricks has a few features that can aid in source
    control.
  prefs: []
  type: TYPE_NORMAL
- en: The first is the version history that is built into notebooks. The history of
    changes for each notebook is tracked even before submitting it to a remote repository
    using Git. Version tracking is beneficial as we are not always ready to make a
    Git commit but still want to track progress and collaborate. It’s also a game
    changer if you accidentally pull someone’s code into your working remote branch
    and forget to push your code before it. The notebook’s history will keep your
    edited copy so you can simply roll back in time and restore all your work!
  prefs: []
  type: TYPE_NORMAL
- en: The second feature is the ease of connecting notebooks and files in your workspace
    to a remote Git repository. Historically, saving Jupyter notebooks to remote repositories
    was a technical nightmare for code reviews, sharing, and diffs. The Databricks
    code repository integration allows Databricks notebooks to contain multiple languages
    (Python, Scala, SQL) and track them as nicely as a typical Python file. This ability
    to track notebooks in source as a standard file is an improvement for data engineers
    and scientists wanting to review notebooks compared to previously converting files
    to Python and losing all output and images. The days of setting up hooks to automatically
    save your notebook as a regular Python file every time you save your notebook
    are over.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can store standard file formats such as markdown, delimiter separated,
    JSON, or YML for a whole reproducibility approach. Note that we do not recommend
    keeping data under repos unless it’s a data sample for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Within a repository, how a team defines the expected folder structure for each
    project is generally less important than the consistent use of that structure.
    However, defining your project structure is still important. We recommend reading
    through *Big Book of MLOps* (Joseph Bradley, Rafi Kurlansik, Matthew Thomson,
    and Niall Turbitt, 2023, *Big Book of MLOps, second edition*, [https://www.databricks.com/resources/ebook/the-big-book-of-mlops](https://www.databricks.com/resources/ebook/the-big-book-of-mlops))
    to determine the best structure for your team or organization. As we will see
    in future chapters, MLflow and repositories are essential for **reproducible research**.
    In the world of data science and machine learning specifically, we want to ensure
    the reproduction of models and experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, the first step for any data science project is to explore and prepare
    the data. We will refer to this process as moving the data from “Bronze” to “Silver”
    layers in reference to the Medallion architecture methodology. You might think
    of this type of data transformation exclusively as a data engineering task, but
    it’s also essential for data science and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you aren’t familiar with this architecture terminology, the **Medallion
    architecture** is a data design pattern used to organize data logically in a warehouse.
    This architecture is also commonly called “multi-hop” architecture. It aims to
    incrementally and progressively improve the structure and quality of data as it
    flows through each layer. The Medallion architecture has three layers: Bronze,
    Silver, and Gold, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Bronze** layer is the raw data layer. It contains all the raw, unprocessed
    data ingested from the source systems. This data still needs to be cleaned or
    transformed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Silver** layer is the validated data layer. It contains data that has
    been cleaned and is subject to various validation and transformation steps. This
    data is ready to be used for analysis and modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Gold** layer is the enriched data layer. It is the highest level and contains
    data enriched with additional information, such as business intelligence metrics
    and key performance indicators, to meet the requirements of the business users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Medallion architecture is a flexible and customizable architecture that
    can meet the specific needs of each organization. The Medallion architecture is
    compatible with the concept of Data Mesh. Data Mesh is an architectural and organizational
    paradigm to ensure value from data. Lakehouse and Data Mesh are complementary,
    paradigms. See *Further reading* for blog posts on leveraging a data mesh with
    the DI Platform. This distributed data architecture enables organizations to unlock
    the value of their data by making it accessible and usable by everyone in the
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Communication and collaboration are vital for the data preparation process.
    This step involves identifying and correcting errors, filling in missing values,
    and resolving inconsistencies in the data. The actions you take should be discussed
    as a team and documented. This is especially important when working collaboratively
    across data teams because data engineers and data scientists often have different
    perspectives on how data should be prepared. For example, we have seen situations
    where an engineer imputed all the missing values in a column with a zero. The
    rationalization made sense; many zeros were already in the column, making the
    KPIs’ values come out correctly. However, from a modeling perspective, missing
    data differs from zeros, especially if there are already zeros in the dataset.
    The approach of replacing nulls with zeros is not necessarily incorrect; it simply
    needs to be discussed with the downstream consumers of the data. One helpful communication
    tool is the column tagging functionality from the Databricks Catalog UI. See *Figure
    2**.5* for an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Example of how to use tagging in a catalog to communicate the
    transformation performed on a column to all table users](img/B16865_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Example of how to use tagging in a catalog to communicate the transformation
    performed on a column to all table users
  prefs: []
  type: TYPE_NORMAL
- en: This implementation of an incorrect imputation method also serves as an example
    of wanting to go back and reprocess history. Luckily, using the Medallion architecture
    methodology is a saving grace. In the situation mentioned previously, the chosen
    imputation would only be present in the Silver and Gold data layers. Meanwhile,
    the Bronze layer still contains the original raw data, so the source of truth
    is not lost, and reprocessing is possible.
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways that the Databricks Platform boosts productivity and collaboration
    is the feature of real-time collaboration support for notebooks. This feature
    allows two or more people to simultaneously see and edit a notebook. The ability
    to pair-program virtually during the pandemic was a lifesaver for many. We’re
    big fans of people who have worked remotely for much of our careers. Collaborative
    editing of a notebook is much easier than sharing code via video call. While there
    are many options for reviewing code, historically, reviewing code in notebooks
    has been difficult, particularly when committing notebooks to source control.
  prefs: []
  type: TYPE_NORMAL
- en: After completing transformations, documentation is easy using markdown in notebooks.
    Even if the resulting notebook is not not itself designated for production ETL,
    documenting the how and why of your data transformations is important for all
    downstream users. To read more about Databricks notebooks, see the documentation
    in *Further reading*.
  prefs: []
  type: TYPE_NORMAL
- en: Planning to create features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data engineer may build Gold tables from Silver tables for consumption by
    the business. At the same time, a data scientist is building features from the
    same Silver tables for models. If we aren’t careful, two people working separately
    without communication can create different versions of the same metrics. When
    architecting your unified DI Platform, be sure to think about reusability and
    maintainability. For this reason, with features specifically, the features-as-code
    approach is our recommendation. Features-as-code refers to the software development
    practice *everything is code*, with a focus on creating a repository of reusable
    code to define features rather than features stored in tables.
  prefs: []
  type: TYPE_NORMAL
- en: You can implement features-as-code in various ways. Initially, we mainly focus
    on function reusability. You can place functions you execute in multiple notebooks
    or scripts in a folder within the repository root directory. In the *Applying
    our learning* section, you will see this is where we store functions even when
    not calculating a feature per se. We call these the utils. You will be referencing
    the `mlia_utils` notebook throughout the example projects.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the `mlia_utils` functions in the root folder of the GitHub repository
    ([https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action](https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action)).
    We walk through pulling the GitHub repository into Databricks in the *Applying
    our learning* section. In it, you will find Python files containing useful functions
    we will use in the projects. It is best practice to save, share, and track functions
    so that the metrics and features we calculate are consistent. Note the empty `__init__.py`
    file is also in the `utils` folder. Having an `__init__.py` file is required.
    With this structure, we can use all functions as imports, for example, from `mlia_utils.rag_funcs`
    import `extract_doc_text`.
  prefs: []
  type: TYPE_NORMAL
- en: Features-as-code is not only a way to reduce duplicative work by reusing functions.
    It is also a great way to ensure consistent business metrics. For example, online
    advertising often has complex calculations for the different types of revenue.
    Therefore, if other people or teams calculate business-critical metrics differently,
    it will be hard to establish the true metric value. Instead, you can often avoid
    this confusion by providing executive-approved functions for use. We will talk
    about this again in *Chapters 5* and *6*. In addition to features being centrally
    located and thus easier to find, Databricks offers easy ways to document your
    data assets.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a business process requiring the team to document tables and functions
    with descriptions makes current and previous efforts more discoverable. In the
    Databricks Catalog UI, you should see *AI-generated* suggestions to fill in your
    table and column descriptions, so you don’t have to start from scratch. Another
    great way to document transformations performed on a data table would be using
    tags. Tags can help with documentation and communication. Recall the example of
    missing data being imputed (*Figure 2**.5*).
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on ML features more specifically, you will learn how to store and serve
    feature functions to simplify your final deployment process. You will create an
    on-demand feature function and use it in your model. We also will show you how
    to leverage saved feature tables to create training sets. If you want to jump
    ahead right away, see [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), where we
    cover topics such as on-demand feature functions, feature lookups, syncing to
    the online store, and the Databricks Feature Engineering client.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After features have been created and stored as feature tables, we create training
    sets and focus on model training. We will cover modeling in terms of leveraging
    Databricks to facilitate the model lifecycle in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    In [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), we’ll discuss the Unity Catalog
    Registry and how to use it to track an enormous amount of information from the
    associated experiments, in addition to details such as model lineage. You can
    register multiple versions of a model at every stage and can give these different
    versions aliases, such as **champion** and **challenger**, or a more specific
    alias referring to versions A and B in an A/B test. See aliasing in *Figure 2**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – A user can alias a model with specific names for A/B testing
    or multi-armed bandits](img/B16865_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – A user can alias a model with specific names for A/B testing or
    multi-armed bandits
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), we demonstrate how to trigger
    a testing script to test every model before having a human review it. Testing
    models is an efficient practice to reduce the time to production when used consistently
    and with intention. We suggest defining the criteria for successfully transitioning
    models/code through isolation environments (from development to stage to production).
    Clearly defined environments are one of the practices that enable you to create
    clear and consistent model quality expectations across all models. Be sure to
    consult *The Big Book of MLOps* on best practices for isolation and model promotion.
    No matter where your environment is, it is beneficial to incorporate logging into
    model experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discuss logging in the ML context rather than the software development sense.
    Logging for ML is focused on reproducible research and is also known as experiment
    tracking. It is common practice to track the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data used to train a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters used to train a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy and speed performance of a model during training and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The errors that occur during training and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The runtime environment of a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using MLflow, you have access to a powerful feature called automatic logging,
    or autologging. Autologging is excellent because it makes it easy to track the
    parameters, metrics, and artifacts of your machine learning experiments without
    explicit instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Auto logging only tracks flavors supported by MLflow. Custom *pyfunc* models
    are not supported. For more information, check *Further reading*.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow auto logging logs parameter values and models for each run in a single
    experiment. Every time you train and evaluate your model, MLflow logs your standard
    metrics and parameters. If you have custom metrics to track with your models,
    you can also easily add them. We demonstrate tracking custom metrics in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297), when we log parameters for the streaming
    transactions model.
  prefs: []
  type: TYPE_NORMAL
- en: An admin can enable auto logging for all notebooks attached to interactive clusters
    at the workspace level. At the cluster level, you can add `spark.databricks.mlflow.autologging.enabled=true`
    to the advanced section of your cluster configuration to turn on auto logging
    with a cluster scope. It is less common but possible to enable auto logging within
    a notebook scope by adding `mlflow.autolog()` to a Python cell in a notebook.
    Be sure to check the list of modeling flavors supported by autolog.
  prefs: []
  type: TYPE_NORMAL
- en: By default, MLflow saves the tracked items in the managed folder in DBFS (which
    will be in UC in the future). You can also set `artifact_location` to point to
    a volume path, which is what we do in the example projects. You also have the
    option to set the location to another cloud storage location, although doing so
    eliminates the ability to see your experiments in the MLflow UI.
  prefs: []
  type: TYPE_NORMAL
- en: The MLflow UI makes it incredibly easy to compare each trail; see *Figures 2.7*
    and *2.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Comparison of experiment runs using the MLflow UI](img/B16865_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Comparison of experiment runs using the MLflow UI
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous options for visualizing the results of experiments tracked
    using MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Graphically comparing parameters and model performance in the
    MLflow UI](img/B16865_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Graphically comparing parameters and model performance in the MLflow
    UI
  prefs: []
  type: TYPE_NORMAL
- en: We’ve examined how to compare parameters and model performance in your experiment
    using the MLflow UI. Next, we’ll look at how to use model monitoring (Lakehouse
    monitoring) to keep track of your model’s performance over time.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data and models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we think about model monitoring and how to implement it, it becomes less
    about the actual model itself and more about the model’s input and output. For
    this reason, Databricks Lakehouse Monitoring focuses on monitoring a model’s input
    and output, which is simply data. The computation of table metrics occurs in the
    background using serverless compute, or as we like to call it, managed compute.
    Fully managed compute abstracts away the complexities and optimization so users
    focus on which tables to monitor, known as primary tables, rather than how. Lakehouse
    Monitoring is currently in public preview, meaning not all information is ready
    for release. For the latest on this feature, check out the Lakehouse Monitoring
    product page. We demonstrate how to use Lakehouse Monitoring in *Chapters 4* *and
    7*.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve touched on a wide variety of topics so far, from the earliest design decisions
    when setting up your Databricks Data Intelligence Platform to the key topics we’ll
    cover throughout the rest of this book. Now let’s dive into the example projects.
    Get ready to follow along in your own Databricks workspace as you work through
    setting up your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s *Applying our learning* section focuses on getting your Databricks
    workspace set up and ready for each project we’ll be working through. We’ll also
    go over getting set up in Kaggle so that you can download the datasets we will
    use throughout the rest of this book. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we begin setting up a workspace, please review the technical requirements
    needed to complete the hands-on work in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We utilize a Python package, `opendatasets`, to download the data we need from
    the Kaggle API easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the Databricks Labs Python library, `dbldatagen`, to generate synthetic
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use the Kaggle API, you must download your credential file, `kaggle.json`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GitHub account is beneficial for connecting Databricks and the code repository
    for the book ([https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)).
    In addition to a GitHub account, it is ideal to fork the book repository into
    your GitHub account. You will see that each chapter has a folder, and each project
    has a folder under the chapters. We will refer to the notebooks by name throughout
    the project work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the Databricks Secrets API to save both Kaggle and OpenAI credentials.
    The Secrets API requires the Databricks CLI. We will walk through this setup.
    However, you will need to create a **personal access token** (**PAT**) on your
    own for the configuration step: [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The compute clusters we use are as follows (they vary slightly depending on
    your data cloud):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-node CPU configuration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2](img/B16865_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2
  prefs: []
  type: TYPE_NORMAL
- en: This will work for most workloads in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your workspace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As defined previously, the workspace discussed in this section refers to the
    deployment instance. Here, we will discuss workspace setup recommendations, project
    setup files, and download instructions for each dataset used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: There is comprehensive documentation on deploying your workspace for the first
    time. If you do not already have a Databricks account and deployed workspace,
    then you have a couple of places to start from. One method is going into your
    cloud account and activating Databricks through the marketplaces. Another method
    is to begin on the Databricks website. For more advanced users, consider using
    Terraform. Given the amount of documentation and the ever-changing world of technology,
    we leave the exercise of activating a workspace up to you.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a workspace deployed, we can begin setting it up. Generally, we
    start with user groups and governance. The experience of setting up Unity Catalog
    is frequently updated for simplicity. Therefore, we recommend you watch the latest
    video documentation on how to do so (see *Further reading*). The process is the
    same, regardless of the data persona using the platform. Please be sure to complete
    metastore and governance setup before going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will need a Kaggle account to download the Kaggle datasets we’ll be using,
    which require an API token for authentication. There is an official Kaggle API,
    but, there are numerous other ways to connect to Kaggle to download data and interact
    with the Kaggle site as well. All methods require downloading your API credentials
    file, `kaggle.json`, from the Kaggle website. Before downloading data, you need
    to make your credentials accessible. Here are three methods for accomplishing
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kaggle.json` file to your project folder. If you choose to do this, be aware
    that your credentials are viewable to others, even if only admins. Also, add `kaggle.json`
    to your `.gitignore` file to prevent committing your credentials to the repository
    and ensure you do not commit your credentials to a Git repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.gitignore` file to prevent committing your credentials to the repository.
    However, the ability to remove other users’ access may not be in your control,
    depending on your role. Furthermore, in general, admins can see all files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Passing user credentials to the notebook](img/B16865_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Passing user credentials to the notebook
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 3**: Use Databricks secrets to store and retrieve your username and
    token, for optimal security, as shown in *Figure 2**.11*. This is the method we
    use for downloading images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Using secrets to store and retrieve your username and token](img/B16865_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Using secrets to store and retrieve your username and token
  prefs: []
  type: TYPE_NORMAL
- en: This code is in global_setup.py, but you could also put it in the notebook itself
  prefs: []
  type: TYPE_NORMAL
- en: '`o``pendatasets` library to paste your credentials in at the time of download.
    This is a safe way to download data, so we demonstrate this with the Favorita
    Sales data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll will walk through `global-setup.py` later. The last section of the file
    is setting your Kaggle credentials. We recommend setting up a secret scope with
    your credentials. We will show you how to set this up once you have a cluster
    running, so there is no need to jump around. Simply download your Kaggle credentials
    for now.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our GitHub repository
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing is to pull the code you will work with throughout the book from
    the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Setting up a Git repository: Workspace > Repos > Home folder
    > Add > Repo'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16865_02_12.jpg)![Figure 2.12 – Setting up a Git repository: Workspace
    > Repos > Home folder > Add > Repo](img/B16865_02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12 – Setting up a Git repository: Workspace > Repos > Home folder
    > Add > Repo'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to your fork of the Book’s GitHub repository, as mentioned in the *Technical
    requirements* section under the *Applying our learning* section. You can copy
    and paste the HTTPS link into the **Add Repo** screen’s URL section, shown in
    *Figure 2**.12*. Next, you will link your GitHub account. If you are unsure how
    to do this, follow the documentation linked in *Further reading* titled *About
    personal access tokens* and *Set up Databricks Repos*. Once your repository is
    ready, you can create a cluster if no one has done so yet.
  prefs: []
  type: TYPE_NORMAL
- en: Creating compute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We provide the cluster configurations we use for this project in the *Technical
    requirements* section. You can use the same configuration if you like. There are
    several options to choose from when creating a new cluster configuration. It might
    seem complicated for new users, but stay calm when trying to choose the right
    one. We highly recommend the *Best Practices for Cluster Configuration* linked
    in *Further reading* for guidance, especially if you are responsible for setting
    up compute for one or more teams. Let’s talk about some of the compute options
    as they relate to ML and this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-node versus single-node**: Multi-node is excellent for distributed
    projects (think Spark). Single-node is suitable for projects or workloads that
    are performed on the driver (think scikit-learn or pandas).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access mode**: Some cluster configurations support Unity Catalog, and some
    don’t. Choose a cluster that supports UC for the projects in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyenv`). You will install libraries both at the cluster level and in a couple
    of project notebooks. You can see the **Libraries** tab in *Figure 2**.13*. Simply
    click **Install new** to install a new library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster
    configuration](img/B16865_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster
    configuration
  prefs: []
  type: TYPE_NORMAL
- en: This is the ideal time to install the libraries you will need. Install via `PyPI
    opendatasets`, `dbldatagen`, `databricks-feature-engineering`, and `mlflow-skinny[databricks]>=2.5.0`.
    These libraries are used across multiple notebooks throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Photon acceleration**: Photon is an acceleration engine that speeds up ETL
    and SQL workloads. Photon currently is not advantageous for standard ML modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VM types**: There are many VMs to pick from. You can choose VMs by family
    from the drop-down list. You can start with a VM in the *General Purpose* group
    if you need more clarification or are just starting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Min and max workers**: The rule of thumb is to start with a few max workers
    and increase the number of workers as your workload increases. Keep in mind that
    your cluster will autoscale for you. However, we still recommend starting smaller
    and growing out for only the more compute-heavy examples, such as certain notebooks
    in the Multilabel Image Classification deep learning project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have your development environment set up. You are ready to lock down
    your credentials for safe use in your code.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Databricks CLI and secrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Databricks CLI is the command line interface for Databricks. We recommend
    using the web terminal from your cluster to install Databricks CLI as well as
    create Databricks secrets. As we mentioned earlier in the chapter, there are other
    options to get access to Kaggle datasets, but we walk you through the steps to
    set up secrets here. Please see the documentation in *Further reading* for more
    details on the CLI, installation, usage, and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Apps** tab of the compute cluster you set up in the last section.
    You can refer to *Figure 2**.13* to see the location of the **Apps** tab. Apps
    are only available while the cluster is, so it may be greyed out initially. You
    will have to start your cluster to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the web terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the latest version of the CLI. `curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh
    |` `sudo sh`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check your Databricks version to be sure it’s greater than `0.2`. We had to
    point to the updated version in the location installed by curl. `/usr/local/bin/databricks-v`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you need to configure the connection. You need your PAT for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a secret scope for storing credentials related to this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a secret for storing your Kaggle username:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a secret for storing your Kaggle API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Last, list your secrets to make sure everything works as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), we’ll create another scope
    to hold an OpenAI API key, but for now we just need the Kaggle credentials. Now
    that we have our secrets set up, let's get our codebase ready!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your code base
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use a setup file to help keep variables consistent across multiple project
    notebooks. You will run the setup file each time you run the project notebooks
    using a magic command, `%run`. This command brings everything into the memory
    of your notebook session. The `global-setup.py` file has numerous components to
    it. Let’s walk through each section. Feel free to edit the file to fit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s possible you’ll receive an error message: `py4j.security.Py4JSecurityException:
    Method public scala.collection.immutable.Map com.databricks.backend.common.rpc.CommandContext.tags()
    is not whitelisted on class` `class com.databricks.backend.common.rpc.CommandContext`'
  prefs: []
  type: TYPE_NORMAL
- en: This is because you are on a shared compute cluster. You can simply hardcode
    `current_user` to your username.
  prefs: []
  type: TYPE_NORMAL
- en: Passing variables via widgets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Widgets pass variables to notebooks similarly to how command-line arguments
    pass variables to Python scripts. The code block in *Figure 2**.14* creates the
    widgets needed to pass variables from the `Run` command to the `global-setup.py`
    file using Databricks Utilities or `dbutils`. You can read more about the `dbutils`
    capabilities in the Databricks Utilities documentation in *Further reading*. These
    widgets create, pass, and access parameters. The arguments are the variable name,
    default value, and verbose name in respective order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16865_02_15.jpg)![](img/B16865_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Creating widgets for accepting notebook-specific variables
  prefs: []
  type: TYPE_NORMAL
- en: You can pass each variable while running the file by adding a single line cell
    with appropriate parameters at the top of the notebook, as shown in *Figure 2**.1**5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Running project-specific variables in our global setup file](img/B16865_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Running project-specific variables in our global setup file
  prefs: []
  type: TYPE_NORMAL
- en: Running `global-setup.py` saves all the variables defined in the script in memory
    for easy reference.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for compatibility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, in `global-setup.py`, we run checks for compatibility between the code
    base and the cluster attached to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compatibility code block checks the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A project name was submitted as a variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster is configured with an ML runtime and meets the minimum version.
    To be sure all features in the code are available in the runtime used, we set
    a minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once all checks pass, we assign a user and paths.
  prefs: []
  type: TYPE_NORMAL
- en: Setting a default catalog and project-specific database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book provides code that uses the Unity Catalog catalog. Your default catalog
    is set based on your environment. If you do not set the environment or you set
    it to `dev`, then the catalog is named `ml_in_action`. When the environment is
    `prod`, the catalog is `ml_in_prod`. The default name for the database is always
    the project name. However, you can provide a different name if you desire by entering
    a project variable for the database name.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Using the defined variables to set the default with retries](img/B16865_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Using the defined variables to set the default with retries
  prefs: []
  type: TYPE_NORMAL
- en: We want to be sure that the catalog and database are set to the notebooks’ defaults.
    Occasionally, with parallel execution, this command can fail during initialization;
    therefore, we add a few retries to work around this issue, as shown in *Figure
    2**.16*.
  prefs: []
  type: TYPE_NORMAL
- en: Granting permissions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve set our catalog and database defaults, we can grant permissions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16865_02_19.jpg)![](img/B16865_02_19_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Granting permissions to the catalog and database
  prefs: []
  type: TYPE_NORMAL
- en: We grant the group `account users` permission. If you do not want to make your
    assets available to others, remove this or comment it out.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to use tick marks around your group name or email address when granting
    permission. If you use single quotes instead, you will get an error message.
  prefs: []
  type: TYPE_NORMAL
- en: The catalog and database are ready for tables. However, not all data we use
    in machine learning goes into a table. For other data, files, and objects, we
    have volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up volumes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Volumes are views of cloud object storage. We create project-specific volumes.
    Use them for path-based access to structured or unstructured data. Volumes sit
    under a database in a catalog and are used to manage and provide access to data
    files. You can govern access to volumes using `GRANT` statements. Volumes provide
    scalable file-based storage without sacrificing governance. Often, we use unstructured,
    semi-structured, or non-tabular data in machine learning. Images are a good example
    of unstructured, non-tabular data that we will use for the Multilabel Image Classification
    project. To work with these images, the Multilabel Image Classification project
    uses volumes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16865_02_20.jpg)![Figure 2.18 – EndpointApiClient class](img/B16865_02_20_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – EndpointApiClient class
  prefs: []
  type: TYPE_NORMAL
- en: Starting the projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have planned our platform and set up our workspace environment. Next, let’s
    work through each project. In GitHub, you will see that each chapter has a folder
    containing folders corresponding to each project. When we refer to the notebooks
    by name, we assume you are in the appropriate chapter and project folder. For
    example, this chapter has the first notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We refer to the notebook by only the filename itself, `CH2-01-Downloading_Sales_Forecast_Data`.
    Let’s jump into the first project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: Favorita store sales – time series forecasting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall from [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016) that we use a Kaggle-provided
    dataset to forecast sales. In this chapter, we download our data from the Kaggle
    website. To follow along in your workspace, please open the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH2-01-Downloading_Sales_Forecast_Data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the notebook, and as well as the code here in *Figures 2.19 and 2.20*, we
    set our path and download our data from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: First, we designate `raw_data_path` to store the files.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Setting the path for our volume](img/B16865_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Setting the path for our volume
  prefs: []
  type: TYPE_NORMAL
- en: In the following code block (*Figure 2**.20*), we use the Python package `opendatasets`,
    a library specifically created to download data from the Kaggle API. You can find
    more information in the *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Downloading Favorita data from opendatasets](img/B16865_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Downloading Favorita data from opendatasets
  prefs: []
  type: TYPE_NORMAL
- en: That is all for the *Favorita Store Sales* project in this chapter! Now, we
    can focus on generating data for our `Streaming` `Transactions` project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: Streaming Transactions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your goal with the Streaming Transactions project is to build a model to classify
    transactions. The dataset consists of JSON-formatted transactions with `Transaction`,
    `timestamp`, `Label`, `Amount`, and `CustomerID`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In later chapters, you will add a product column to demonstrate schema evolution.
    In this chapter, you’ll create the first version of transaction data used throughout
    the rest of the book. To follow along in your workspace, please open the following
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH2-01-Generating_Records_Using_DBKS_Labs_Datagen`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can run each cell of the notebook as we work through them or run them all
    at once. After the setup commands, we set notebook variables to establish the
    number of rows generated per batch of transactions (`nRows`), the number of positively
    labeled rows per batch (`nPositiveRows`), the path to the volume where you will
    store the JSON dataset (`destination_path`), a temporary path (`temp_path`), and
    the number of seconds between each batch of data you generate (`sleepIntervalSeconds`).
  prefs: []
  type: TYPE_NORMAL
- en: The following code block accesses the value of the `Reset` widget. Any data
    already written to the volume will be deleted if the widget is set to `True` (its
    default value).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Checking the Reset widget](img/B16865_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – Checking the Reset widget
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set the parameter values used in the data generator to create the transactions.
    We set the minimum and maximum values for each `CustomerID`. We also create a
    dictionary of product types and set `min`, `max`, `mean`, `alpha`, and `beta`
    variables, which you use to generate random transaction amounts according to a
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16865_02_24.jpg)![](img/B16865_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – Dictionaries to hold variables for use within the define_specs
    function
  prefs: []
  type: TYPE_NORMAL
- en: With the variables set, we build out the functions to create the transaction
    data, starting with the `define_specs` function. The function accepts as input
    a product type (defined in the dictionary in *Figure 2**.22*), a positive or negative
    label, and a timestamp; it returns a dollar amount for the transaction. *Figure
    2**.23* shows a portion of the code; the rest is in the accompanying notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 2.23 – Defining the define_specs function to generate transaction
    records](img/B16865_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 – Defining the define_specs function to generate transaction records
  prefs: []
  type: TYPE_NORMAL
- en: Next, we write a function to generate a single record by calling `define_specs`
    and including the current timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24 – Defining a function to generate a single transaction record](img/B16865_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.24 – Defining a function to generate a single transaction record
  prefs: []
  type: TYPE_NORMAL
- en: 'We then build `generateRecordSet` to generate the `recordCount` number of records
    in each batch. Notice that in this notebook, we’re using the `None` product type,
    so the records generated will only have four features: `CustomerID`, `TransactionTimestamp`,
    `Amount`, and `Label` (this will be important in the next chapter!).'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 2.25 – The generateRecordSet function creates a record for each product
    and each label. Each record contains nRows transactions](img/B16865_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.25 – The generateRecordSet function creates a record for each product
    and each label. Each record contains nRows transactions
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we write a function to generate a set of data, convert the data to
    a DataFrame, and write it out as one JSON file to a temporary path. Then, we move
    that file to the final volume destination.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26 – The writeJsonFile function generates a set of records](img/B16865_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.26 – The writeJsonFile function generates a set of records
  prefs: []
  type: TYPE_NORMAL
- en: The set contains amounts generated as integers, so we divide by 100 to turn
    the amounts into dollars and type float. The function writes out the JSON file
    to a `temp` directory and then moves the single file to the final directory.
  prefs: []
  type: TYPE_NORMAL
- en: With everything set up, create the dataset with the code provided. Feel free
    to increase the iterations to build a larger dataset. Then, move on to the next
    project!
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: Retrieval-Augmented Generation Chatbot'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The RAG Chatbot project will ingest PDF documents to build the knowledge base
    for the chatbot. We use a volume to store the PDFs. To follow along in your workspace,
    please open the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH2-01-Downloading_PDF_Documents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files can be uploaded to a volume directly in the Databricks console via the
    user interface, as shown in *Figure 2**.27*; however, this project uses the code
    provided in the notebook to download and save the data to a volume programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27 – Manually uploading documents into a volume](img/B16865_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.27 – Manually uploading documents into a volume
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter begins with setup cells and helper functions, and
    in *Figure* *2**.28* we designate `library_folder` where we will save the PDFs
    we download.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28 – Designating the library folder to hold the files for this project](img/B16865_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.28 – Designating the library folder to hold the files for this project
  prefs: []
  type: TYPE_NORMAL
- en: We are using open articles published on the **Arxiv** page that relate to **Generative
    AI** (**GenAI**) and how it can impact human labor markets and economics. We pass
    the URLs to be used as documents for our chatbot and load these files into our
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29 – Download PDF files and save them to our volume](img/B16865_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.29 – Download PDF files and save them to our volume
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the documents downloaded and, they are ready to be processed
    for our chatbot. With that completed, we can move on to our final project: **Multilabel**
    **Image Classification**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: Multilabel Image Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The MIC project ingests images into Delta tables to fine-tune a pre-trained
    model from the *Restnet* family to improve its accuracy. We will programmatically
    download the images from Kaggle and save the data to a volume. To follow along
    in your workspace, please open the `CH2-01-Downloading_Images` notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16865_02_33.jpg)![](img/B16865_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.30 – Downloading data from Kaggle using Databricks magic commands
  prefs: []
  type: TYPE_NORMAL
- en: Now we create the volume folder and unzip the images for our classification
    project into our volumes. It will take around one hour (as it contains 80K images!)
    to extract the images from ZIP to `Volumes`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31 – Unzipping images into the volumes for this project](img/B16865_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.31 – Unzipping images into the volumes for this project
  prefs: []
  type: TYPE_NORMAL
- en: We have downloaded or generated all four datasets, and they are ready to be
    brought into our Bronze layer in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered a wide range of setup decisions, options, and processes
    for planning your Data Intelligence Platform. We took you through an overview
    of the main components of the DI Platform, from early design choices to important
    features that we will dive into further in upcoming chapters. You also learned
    how to set up your workspace and project code base. We hope you feel more comfortable
    with the basics of the platform. With Databricks ready and the project data downloaded,
    we are now ready to get into what it means to build out the Bronze data layer.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123), we cover the essentials
    of building out the Bronze data layer within the Databricks Intelligence Platform.
    We will format our data into the most optimized format, learn about schema evolution,
    change data capture using Delta, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following questions are meant to solidify key points to remember and tie
    the content back to your own experience.
  prefs: []
  type: TYPE_NORMAL
- en: How do Databricks runtimes enable stability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we make our data more discoverable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some common steps needed to set up a Databricks workspace?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After putting thought into the questions, compare your answers to ours.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks runtimes enable stability by providing a consistent set of libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilizing the built-in functionality for metadata, such as table and column
    descriptions, makes our data more discoverable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some common steps for setting up a workspace are activating Databricks through
    the marketplace and setting up user groups and governance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we identified specific libraries, technical features, and
    options. Please take a look at these resources to delve deeper into the areas
    that interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is Unity* *Catalog?*: [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lakehouse Monitoring* *demo*: [https://youtu.be/3TLBZSKeYTk?t=560](https://youtu.be/3TLBZSKeYTk?t=560)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*UC has a more centralized method of managing the model lifecycle than* *HMS*:
    [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Share Models across* *workspaces*: [https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*In-depth UC setup on* *Azure*: [https://youtu.be/itGKRVHdNPo](https://youtu.be/itGKRVHdNPo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Connecting external HMS to* *UC*: [https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unity Catalog* *limitations*: [https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations](https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Best practices: Cluster configuration | Select Cloud in the* *dropdown*: [https://docs.databricks.com/clusters/cluster-config-best-practices.html](https://docs.databricks.com/clusters/cluster-config-best-practices.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks* *Notebooks*: [https://docs.databricks.com/en/notebooks/index.html](https://docs.databricks.com/en/notebooks/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Autologging | Select Cloud in the* *dropdown*: [https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management](https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lakehouse Monitoring product* *page*: [https://www.databricks.com/product/machine-learning/lakehouse-monitoring](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System* *Tables*: [https://www.databricks.com/resources/demos/tutorials/governance/system-tables](https://www.databricks.com/resources/demos/tutorials/governance/system-tables)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Opendatasets Python* *package*: [https://pypi.org/project/opendatasets/](https://pypi.org/project/opendatasets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle* *API*: [https://www.kaggle.com/docs/api](https://www.kaggle.com/docs/api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GitHub*: [https://github.com/](https://github.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks ML in Action GitHub* *Repository*: [https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks Secrets* *API*: [https://docs.databricks.com/en/security/secrets/secrets.html](https://docs.databricks.com/en/security/secrets/secrets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks* *CLI*: [https://docs.databricks.com/en/dev-tools/cli/index.html](https://docs.databricks.com/en/dev-tools/cli/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Databricks* *Utilities*: [https://docs.databricks.com/en/dev-tools/databricks-utils.html](https://docs.databricks.com/en/dev-tools/databricks-utils.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Workspace* *libraries*: [https://docs.databricks.com/en/libraries/workspace-libraries.html](https://docs.databricks.com/en/libraries/workspace-libraries.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Mesh and the DI Platforms Blog Posts*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html),
    [https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html](https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Short YouTube video on managed vs external tables in* *UC*: [https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA](https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Query* *Federation*: [https://docs.databricks.com/en/query-federation/index.html](https://docs.databricks.com/en/query-federation/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Centralized model registry workspace for* *HMS*: [https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manage model lifecycle in Unity* *Catalog*: [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Terraform* *https*: [https://github.com/databricks/terraform-provider-databricks](https://github.com/databricks/terraform-provider-databricks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Widgets*: [https://docs.databricks.com/notebooks/widgets.html](https://docs.databricks.com/notebooks/widgets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
