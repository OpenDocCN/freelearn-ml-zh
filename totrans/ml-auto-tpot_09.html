<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer143">
			<h1 id="_idParaDest-72"><em class="italic"><a id="_idTextAnchor073"/>Chapter 6</em>: Getting Started with Deep Learning: Crash Course in Neural Networks</h1>
			<p>In this chapter, you'll learn the basics of deep learning and artificial neural networks. You'll discover the basic idea and theory behind these topics and how to train simple neural network models with Python. The chapter will serve as an excellent primer for the upcoming chapters, where the ideas of pipeline optimization and neural networks are combined.</p>
			<p>We'll cover the essential topics and ideas behind deep learning, why it has gained popularity in the last few years, and the cases in which neural networks work better than traditional machine learning algorithms. You'll also get hands-on experience in coding your own neural networks, both from scratch and through pre-made libraries.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>An overview of deep learning</li>
				<li>Introducing artificial neural networks</li>
				<li>Using neural networks to classify handwritten digits</li>
				<li>Comparing neural networks in regression and classification</li>
			</ul>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor074"/>Technical requirements</h1>
			<p>No prior experience with deep learning and neural networks is necessary. You should be able to understand the basics from this chapter alone. Previous experience is helpful, as deep learning isn't something you can learn in one sitting.</p>
			<p>You can download the source code and dataset for this chapter here: <a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06">https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06</a>.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor075"/>Overview of deep learning</h1>
			<p>Deep learning is a <a id="_idIndexMarker340"/>subfield of machine learning that focuses on neural networks. Neural networks aren't that new as a concept – they were introduced back in the 1940s but didn't gain much in popularity until they started winning data science competitions (somewhere around 2010).</p>
			<p>Potentially the biggest year for deep learning and AI was 2016, all due to a single event. <em class="italic">AlphaGo</em>, a <a id="_idIndexMarker341"/>computer program that plays the board game Go, defeated the highest-ranking player in the world. Before this event, Go was considered to be a game that computers couldn't master, as there are so many potential board configurations.</p>
			<p>As mentioned before, deep learning is based on neural networks. You can think of neural networks as <strong class="bold">directed acyclic graphs</strong> – a graph consisting <a id="_idIndexMarker342"/>of vertices (nodes) and edges (connections). The input layer (the first layer, on the far left side) takes in the raw data from your datasets, passes it through one or multiple hidden layers, and constructs an output.</p>
			<p>You can see an example architecture of a neural network in the following diagram:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="Images/B16954_06_1.jpg" alt="Figure 6.1 – An example neural network architecture&#13;&#10;" width="999" height="489"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – An example neural network architecture</p>
			<p>The small black nodes on the far left side represent the input data – data that comes directly from your dataset. These values are then connected with the hidden layers, with their respective weights and biases. A common way to refer to these weights and biases is by <a id="_idIndexMarker343"/>using the term <strong class="bold">tunable parameters</strong>. We'll address this term and show how to calculate them in the next section.</p>
			<p>Every node of a neural network is <a id="_idIndexMarker344"/>called a <strong class="bold">neuron</strong>. Let's take a look at the architecture of an individual neuron:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="Images/B16954_06_2.jpg" alt="Figure 6.2 – Individual neuron&#13;&#10;" width="999" height="696"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Individual neuron</p>
			<p>The X's correspond to <a id="_idIndexMarker345"/>the values either from the input layer or from the previous hidden layer. These values are multiplied together (<em class="italic">x1 * w1</em>, <em class="italic">x2 * w2</em>) and then added together (<em class="italic">x1w1 + x2w2</em>). After the summation, a bias term is added, and finally, everything is passed <a id="_idIndexMarker346"/>through an <strong class="bold">activation function</strong>. This function determines if the neuron will "fire" or not. It's something like an on-off switch, in the simplest terms.</p>
			<p>A brief explanation of weights and biases is shown here:</p>
			<ul>
				<li>Weights:<p>a) Multiplied with <a id="_idIndexMarker347"/>values from the previous layer</p><p>b) Can change the magnitude or entirely flip the value from positive to negative</p><p>c) In function terms – adjusting the weight changes the slope of the function</p></li>
				<li>Biases:<p>a) Interpreted as an <a id="_idIndexMarker348"/>offset of the function</p><p>b) An increase in bias leads to an upward shift of a function</p><p>c) A decrease in bias leads to a downward shift of a function</p></li>
			</ul>
			<p>There are many types of neural network architectures<a id="_idIndexMarker349"/> besides artificial neural networks, and they are discussed here:</p>
			<ul>
				<li><strong class="bold">Convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) – a type of <a id="_idIndexMarker350"/>neural network most commonly applied to analyzing images. They <a id="_idIndexMarker351"/>are based on the convolution operation – an operation between two matrices in which the <a id="_idIndexMarker352"/>second one slides (convolves) over the first one and computes element-wise multiplication. The goal of this operation is to find a sliding matrix (kernel) that can extract the correct features from the input image and hence make image classification tasks easy.</li>
				<li><strong class="bold">Recurrent neural networks </strong>(<strong class="bold">RNNs</strong>) – a type of <a id="_idIndexMarker353"/>neural network most commonly used on sequence data. Today <a id="_idIndexMarker354"/>these networks are applied in many tasks, such as handwriting recognition, speech recognition, machine translation, and time series forecasting. The RNN model processes a single element in the sequence at a time. After processing, the new updated unit's state is passed down to the next time step. Imagine predicting a single character based on the previous <em class="italic">n</em> characters; that's the general gist.</li>
				<li><strong class="bold">Generative adversarial networks</strong> (<strong class="bold">GANs</strong>) – a type of <a id="_idIndexMarker355"/>neural network most commonly used to create new <a id="_idIndexMarker356"/>samples after learning from real data. The GAN architecture comprises two separate models – generators and discriminators. The job of a generator model is to make fake images and send them to the discriminator. The discriminator works like a judge and tries to tell whether an image is fake or not.</li>
				<li><strong class="bold">Autoencoders</strong> – unsupervised <a id="_idIndexMarker357"/>learning <a id="_idIndexMarker358"/>techniques, designed to learn a low-dimensional representation of a high-dimensional dataset. In a way, they <a id="_idIndexMarker359"/>work similarly to <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>).</li>
			</ul>
			<p>These four deep learning concepts won't be covered in this book. We'll focus only on artificial neural networks, but it's good to know they exist in case you want to dive deeper on your own.</p>
			<p>The next section looks at artificial neural networks and shows you how to implement them in Python, both from scratch and with data science libraries.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>Introducing artificial neural networks</h1>
			<p>The fundamental building block of an <a id="_idIndexMarker360"/>artificial neural network is the neuron. By itself, a single neuron is useless, but it can have strong predictive power when combined into a more complex network.</p>
			<p>If you can't reason why, think about your brain and how it works for a minute. Just like artificial neural networks, it is also made from millions of neurons, which function only when there's communication between them. Since artificial neural networks try to imitate the human brain, they need to somehow replicate neurons in the brain and connections between them (weights). This association will be made less abstract throughout this section.</p>
			<p>Today, artificial neural networks can be used to tackle any problem that regular machine learning algorithms can. In a nutshell, if you can solve a problem with linear or logistic regression, you can solve it with neural networks.</p>
			<p>Before we can explore the complexity and inner workings of an entire network, we have to start simple – with the theory of a single neuron.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor077"/>Theory of a single neuron</h2>
			<p>Modeling a <a id="_idIndexMarker361"/>single neuron is easy with Python. For example, let's say a <a id="_idIndexMarker362"/>neuron receives values from five other neurons (inputs, or X's). Let's examine this behavior visually before implementing it in code. The following diagram shows how a single neuron looks when receiving values from five neurons in the previous layers (we're modeling the neuron on the right):</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="Images/B16954_06_3.jpg" alt="Figure 6.3 – Modeling a single neuron&#13;&#10;" width="682" height="587"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Modeling a single neuron</p>
			<p>The X's represent input features, either from the raw data or from the previous hidden layer. Each input feature has a weight assigned to it, denoted with W's. Corresponding input values and weights are multiplied and summed, and then the bias term (b) is added on top of the result.</p>
			<p>The formula for calculating the output value of our neuron is as follows:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="Images/Formula_06_001.jpg" alt="" width="1158" height="67"/>
				</div>
			</div>
			<p>Let's work with concrete values to get this concept a bit clearer. The following diagram looks identical to Figure 6.3, but has actual numbers instead of variables:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="Images/B16954_06_4.jpg" alt="Figure 6.4 – Neuron value calculation&#13;&#10;" width="539" height="464"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Neuron value calculation</p>
			<p>We can plug the values <a id="_idIndexMarker363"/>directly into the preceding <a id="_idIndexMarker364"/>formula to calculate the value:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="Images/Formula_06_002.jpg" alt="" width="962" height="55"/>
				</div>
			</div>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="Images/Formula_06_003.jpg" alt="" width="1299" height="52"/>
				</div>
			</div>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="Images/Formula_06_004.jpg" alt="" width="775" height="45"/>
				</div>
			</div>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="Images/Formula_06_005.jpg" alt="" width="270" height="47"/>
				</div>
			</div>
			<p>In reality, single neurons get their value from potentially thousands of neurons in the previous layers, so calculating values manually and expressing visually isn't practical. </p>
			<p>Even if you decide to do so, that's only a single forward pass. Neural networks learn during the backward pass, which is much more complicated to calculate by hand.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor078"/>Coding a single neuron</h2>
			<p>Next, let's see how you can <a id="_idIndexMarker365"/>semi-automate neuron value calculation with Python:</p>
			<ol>
				<li>To start, let's declare <a id="_idIndexMarker366"/>input values, their respective weights, and a value for the bias term. The first two are lists, and the bias is just a number:<p class="source-code">inputs = [5, 4, 2, 1, 6]</p><p class="source-code">weights = [0.1, 0.3, 0.05, 0.4, 0.9]</p><p class="source-code">bias = 4</p><p>That's all you need to calculate the output value. Let's examine what your options are next.</p></li>
				<li>There are three simple methods for calculating neuron output values. The first one is the most manual, and that is to explicitly multiply corresponding inputs and weights, adding them together with the bias. <p>Here's a Python implementation:</p><p class="source-code">output = (inputs[0] * weights[0] + </p><p class="source-code">          inputs[1] * weights[1] + </p><p class="source-code">          inputs[2] * weights[2] +</p><p class="source-code">          inputs[3] * weights[3] +</p><p class="source-code">          inputs[4] * weights[4] + </p><p class="source-code">          bias)</p><p class="source-code">output</p><p>You should see a value of <strong class="source-inline">11.6</strong> printed out after executing this code. To be more precise, the value should be <strong class="source-inline">11.600000000000001</strong>, but don't worry about this calculation error.</p><p>The next method is a bit more scalable, and it boils down to iterating through inputs and weights at the same time and incrementing the variable declared earlier for the output. After the loop finishes, the bias term is added. Here's how to implement this calculation method:</p><p class="source-code">output = 0</p><p class="source-code">for x, w in zip(inputs, weights):</p><p class="source-code">    output += x * w</p><p class="source-code">output += bias</p><p class="source-code">output</p><p>The output is still identical, but <a id="_idIndexMarker367"/>you can immediately see how much more scalable this option is. Just imagine using the first option if the previous network layer had 1,000 neurons – it's not even remotely convenient.</p><p>The third and preferred method <a id="_idIndexMarker368"/>is to use a scientific computing library, such as NumPy. With it, you can calculate the vector dot product and add the bias term. Here's how:</p><p class="source-code">import numpy as np</p><p class="source-code">output = np.dot(inputs, weights) + bias</p><p class="source-code">output</p><p>This option is the fastest, both to write and to execute, so it's the preferred one.</p></li>
			</ol>
			<p>You now know how to code a single neuron – but neural networks employ layers of neurons. You'll learn more about layers next.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>Theory of a single layer</h2>
			<p>To make <a id="_idIndexMarker369"/>things simpler, think of layers <a id="_idIndexMarker370"/>as vectors or simple groups. Layers aren't some complicated or abstract data structure. In code terms, you can think of them as lists. They contain a number of neurons.</p>
			<p>Coding a single layer of neurons is quite similar to coding a single neuron. We still have the same inputs, as they are coming either from a previous hidden layer or an input layer. What changes are weights and biases. In code terms, weights aren't treated as a list anymore, but as a list of lists instead (or a matrix). Similarly, bias is now a list instead of a scalar value.</p>
			<p>Put simply, your matrix of weights will <a id="_idIndexMarker371"/>have as many rows as there are neurons in the new layer and as many columns as there are neurons in the previous layer. Let's take a look at a sample diagram to make this concept a bit less abstract:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="Images/B16954_06_5.jpg" alt="Figure 6.5 – Layer of neurons&#13;&#10;" width="503" height="494"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Layer of neurons</p>
			<p>Weight values <a id="_idIndexMarker372"/>deliberately weren't placed on the previous diagram, as it would look messy. To implement this layer in code, you'll need to have the following structures:</p>
			<ul>
				<li>Vector of inputs (1 row, 5 columns)</li>
				<li>Matrix of weights (2 rows, 5 columns)</li>
				<li>Vector of biases (1 row, 2 columns)</li>
			</ul>
			<p>A matrix multiplication rule from linear algebra states that two matrices need to be of shapes (m, n) and (n, p) in order to produce an (m, p) matrix after multiplication. Bearing that in mind, you could easily perform matrix multiplication by transposing the matrix of weights.</p>
			<p>Mathematically, here's the formula you <a id="_idIndexMarker373"/>can use to calculate the values of the output layer:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="Images/Formula_06_006.jpg" alt="" width="473" height="66"/>
				</div>
			</div>
			<p>Here, the following applies:</p>
			<ul>
				<li><img src="Images/Formula_06_007.png" alt="" width="13" height="13"/> is the vector of inputs.</li>
				<li><img src="Images/Formula_06_008.png" alt="" width="17" height="14"/> is the matrix of weights.</li>
				<li><img src="Images/Formula_06_009.png" alt="" width="12" height="17"/> is the vector of biases.</li>
			</ul>
			<p>Let's declare values for <a id="_idIndexMarker374"/>all of these and see how to calculate the values for an output layer:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="Images/Formula_06_010.jpg" alt="" width="1174" height="256"/>
				</div>
			</div>
			<p>The previously mentioned formula can now be used to calculate the values of the output layer:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/Formula_06_011.jpg" alt="" width="453" height="62"/>
				</div>
			</div>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="Images/Formula_06_012.jpg" alt="" width="972" height="239"/>
				</div>
			</div>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/Formula_06_013.jpg" alt="" width="660" height="249"/>
				</div>
			</div>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="Images/Formula_06_014.jpg" alt="" width="1357" height="106"/>
				</div>
			</div>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/Formula_06_015.jpg" alt="" width="510" height="111"/>
				</div>
			</div>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="Images/Formula_06_016.jpg" alt="" width="367" height="107"/>
				</div>
			</div>
			<p>And that's essentially how you <a id="_idIndexMarker375"/>can calculate outputs for an entire layer. The <a id="_idIndexMarker376"/>calculations will grow in size for actual neural networks, as there are thousands of neurons per layer, but the logic behind the math is identical. </p>
			<p>You can see how tedious it is to calculate layer outputs manually. You'll learn how to calculate the values in Python next.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Coding a single layer</h2>
			<p>Let's now examine three <a id="_idIndexMarker377"/>ways in which you could calculate <a id="_idIndexMarker378"/>the output values for a single layer. As with single neurons, we'll start with the manual approach and finish with a NumPy one-liner.</p>
			<p>You'll have to declare values for inputs, weights, and biases first, so here's how to do that:</p>
			<p class="source-code">inputs = [5, 4, 2, 1, 6]</p>
			<p class="source-code">weights = [</p>
			<p class="source-code">    [0.1, 0.3, 0.05, 0.4, 0.9],</p>
			<p class="source-code">    [0.3, 0.15, 0.4, 0.7, 0.2]</p>
			<p class="source-code">]</p>
			<p class="source-code">biases = [4, 2]</p>
			<p>Let's proceed with calculating the values of the output layer:</p>
			<ol>
				<li value="1">Let's start with the manual approach. No, we won't do the same procedure as with neurons. You could, of course, but it would look too messy and impractical. Instead, we'll immediately use the <strong class="source-inline">zip()</strong> function to iterate over the <strong class="source-inline">weights</strong> matrix and <strong class="source-inline">biases</strong> array and calculate the value of a single output neuron.<p>This procedure is repeated for <a id="_idIndexMarker379"/>however many neurons there are, and each output neuron is appended to a list that represents the output layer.</p><p>Here's the entire code snippet:</p><p class="source-code">layer = []</p><p class="source-code">for n_w, n_b in zip(weights, biases):</p><p class="source-code">    output = 0</p><p class="source-code">    for x, w in zip(inputs, n_w):</p><p class="source-code">        output += x * w</p><p class="source-code">    output += n_b</p><p class="source-code">    layer.append(output)</p><p class="source-code">    </p><p class="source-code">layer</p><p>The result is a list with the value <strong class="source-inline">[11.6, 6.8]</strong>, which are the same results we got from the manual <a id="_idIndexMarker380"/>calculation earlier.</p><p>While this approach works, it's still not optimal. Let's see how to improve next.</p></li>
				<li>You'll now calculate the values of the output layer by taking the vector dot product between input values and every row of the <strong class="source-inline">weights</strong> matrix. The bias term will be added after this operation is completed.<p>Let's see how it works in action:</p><p class="source-code">import numpy as np</p><p class="source-code">layer = []</p><p class="source-code">for n_w, n_b in zip(weights, biases):</p><p class="source-code">    layer.append(np.dot(inputs, n_w) + n_b)</p><p class="source-code">layer</p><p>The layer values are still identical – <strong class="source-inline">[11.6, 6.8]</strong>, and this approach is a bit more scalable than the <a id="_idIndexMarker381"/>previous one. It can still be improved upon. Let's see how next.</p></li>
				<li>You can <a id="_idIndexMarker382"/>perform a matrix multiplication between inputs and transposed weights and add the corresponding biases with a single line of Python code. Here's how:<p class="source-code">layer = np.dot(inputs, np.transpose(weights)) + biases</p><p class="source-code">layer</p><p>That's the recommended way if, for some reason, you want to calculate outputs manually. NumPy handles it completely, so it's the fastest one at the same time.</p></li>
			</ol>
			<p>You now know how to calculate output values both for a single neuron and for a single layer of a neural network. So far, we haven't covered a crucial idea in neural networks that decides whether a neuron will "fire" or "activate" or not. These are called activation functions, and we'll cover them next.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Activation functions</h2>
			<p>Activation functions <a id="_idIndexMarker383"/>are essential for the <a id="_idIndexMarker384"/>output of neural networks, and hence to the output of the deep learning model. They are nothing but mathematical equations, and relatively simple ones to be precise. Activation functions are those that determine whether the neuron should be "activated" or not.</p>
			<p>Another way to think about the activation function is as a sort of gate that stands between the input coming into the current neuron and its output, which goes to the next layer. Activation function can be as simple as a step function (turns neurons on or off), or a bit more complicated and non-linear. It's the non-linear functions that prove useful in learning complex data and providing accurate predictions.</p>
			<p>We'll go over a couple of the most common activation functions next.</p>
			<h3>Step function</h3>
			<p>The step function is <a id="_idIndexMarker385"/>based on a <a id="_idIndexMarker386"/>threshold. If the value coming in is above the threshold, the neuron is activated. That's why we can say the step function serves as an on-off switch – there are no values in between.</p>
			<p>You can easily use Python and NumPy to declare and visualize a basic step function. The procedure is shown here:</p>
			<ol>
				<li value="1">To start, you'll have to define a step function. The typical threshold value is 0, so the neuron will activate if and only if the value passed in to the function is greater than 0 (the input value being the sum of the previous inputs multiplied by the weights and added bias).<p>This kind of logic is trivial to implement in Python:</p><p class="source-code">def step_function(x):</p><p class="source-code">    return 1 if x &gt; 0 else 0</p></li>
				<li>You can now declare a list of values that will serve as an input to this function, and then apply <strong class="source-inline">step_function()</strong> to this list. Here's an example:<p class="source-code">xs = np.arange(-10, 10, step=0.1)</p><p class="source-code">ys = [step_function(x) for x in xs]</p></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<p class="source-code">plt.plot(xs, ys, color='#000000', lw=3)</p><p class="source-code">plt.title('Step activation function', fontsize=20)</p><p>You can see how the function works visually in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="Images/B16954_06_6.jpg" alt="Figure 6.6 – Step activation function&#13;&#10;" width="1389" height="724"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Step activation function</p>
			<p>The biggest problem of the <a id="_idIndexMarker387"/>step function is that it <a id="_idIndexMarker388"/>doesn't allow multiple outputs – only two. We'll dive into a set of non-linear functions next, and you'll see what makes them different.</p>
			<h3>Sigmoid function</h3>
			<p>The sigmoid <a id="_idIndexMarker389"/>activation function is frequently referred to as the <a id="_idIndexMarker390"/>logistic function. It is a very popular function in the realm of neural networks and deep learning. It essentially transforms the input into a value between 0 and 1.</p>
			<p>You'll see how the function works later, and you'll immediately notice an advantage over the step function – the gradient is smooth, so there are no jumps in the output values. For example, you wouldn't get a jump from 0 to 1 if the value changed slightly (for example, from -0.000001 to 0.0000001).</p>
			<p>The sigmoid function does suffer from a common problem in deep learning – <strong class="bold">vanishing gradient</strong>. It is a <a id="_idIndexMarker391"/>problem that often occurs during backpropagation (a process of learning in neural networks, way beyond this chapter's scope). Put simply, the gradient "vanishes" during the backward pass, making it impossible for the network to learn (tweak weights and biases), as the suggested tweaks are too close to zero.</p>
			<p>You can use Python and NumPy to <a id="_idIndexMarker392"/>easily declare and visualize the sigmoid function. The procedure is shown here:</p>
			<ol>
				<li value="1">To start, you'll have to define the sigmoid function. Its formula is pretty well established: <em class="italic">(1 / (1 + exp(-x)))</em>, where <em class="italic">x</em> is the input value.<p>Here's how to implement this formula in Python:</p><p class="source-code">def sigmoid_function(x):</p><p class="source-code">     return 1 / (1 + np.exp(-x))</p></li>
				<li>You can now <a id="_idIndexMarker393"/>declare a list of values that will serve as an input to this function, and then apply <strong class="source-inline">sigmoid_function()</strong> to this list. Here's an example:<p class="source-code">xs = np.arange(-10, 10, step=0.1)</p><p class="source-code">ys = [step_function(x) for x in xs]</p></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<p class="source-code">plt.plot(xs, ys, color='#000000', lw=3)</p><p class="source-code">plt.title(Sigmoid activation function', fontsize=20)</p><p>You can see how the function works visually in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="Images/B16954_06_7.jpg" alt="Figure 6.7 – Sigmoid activation function&#13;&#10;" width="1389" height="724"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Sigmoid activation function</p>
			<p>One big disadvantage is that <a id="_idIndexMarker394"/>the values returned by the <a id="_idIndexMarker395"/>sigmoid function are not centered around zero. This is a problem because modeling inputs that are highly negative or highly positive gets harder. The hyperbolic tangent function fixes this problem.</p>
			<h3>Hyperbolic tangent function</h3>
			<p>Hyperbolic tangent function (or TanH) is closely <a id="_idIndexMarker396"/>related to the sigmoid function. It's also a type of activation function that <a id="_idIndexMarker397"/>suffers from the vanishing gradient issue, but its outputs are centered around zero – as the function ranges from -1 to +1. </p>
			<p>This makes it much easier to model inputs that are highly negative or highly positive. You can use Python and NumPy to easily declare and visualize the hyperbolic tangent function. The procedure is shown here:</p>
			<ol>
				<li value="1">To start, you'll have to define the hyperbolic tangent function. You can use the <strong class="source-inline">tanh()</strong> function from NumPy for the implementation.<p>Here's how to implement it in Python:</p><p class="source-code">def tanh_function(x):</p><p class="source-code">    return np.tanh(x)</p></li>
				<li>You can now <a id="_idIndexMarker398"/>declare a list of values that will serve as an input to this function, and then apply the <strong class="source-inline">tanh_function()</strong> to this list. Here's an example:<p class="source-code">xs = np.arange(-10, 10, step=0.1)</p><p class="source-code">ys = [step_function(x) for x in xs]</p></li>
				<li>Finally, you can visualize the <a id="_idIndexMarker399"/>function with the help of the Matplotlib library in just two lines of code:<p class="source-code">plt.plot(xs, ys, color='#000000', lw=3)</p><p class="source-code">plt.title(Tanh activation function', fontsize=20)</p><p>You can see how the function works visually in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="Images/B16954_06_8.jpg" alt="Figure 6.8 – Hyperbolic tangent activation function&#13;&#10;" width="1388" height="712"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Hyperbolic tangent activation function</p>
			<p>To train and <a id="_idIndexMarker400"/>optimize neural networks efficiently, you need an activation function that acts as a linear function but is <a id="_idIndexMarker401"/>non-linear in nature, allowing the network to learn the complex relationships in the data. That's where the last activation function in this section comes in.</p>
			<h3>Rectified linear unit function</h3>
			<p>The <a id="_idIndexMarker402"/>rectified linear unit (or ReLU) function is an activation function you can see in most modern-day deep learning <a id="_idIndexMarker403"/>architectures. Put simply, it returns the larger of two values between 0 and x, where x is the input value. </p>
			<p>ReLU is one of the most computationally efficient functions, and it allows the relatively quick finding of the convergence point. You'll see how to implement it in Python next:</p>
			<ol>
				<li value="1">To start, you'll have to define the ReLU function. This can be done entirely from scratch or with NumPy, as you only have to find the larger values of the two (0 and x).<p>Here's how to implement ReLU in Python:</p><p class="source-code">def relu_function(x):</p><p class="source-code">    return np.maximum(0, x)</p></li>
				<li>You can now declare a list of values that will serve as an input to this function, and then apply <strong class="source-inline">relu_function()</strong> to this list. Here's an example:<p class="source-code">xs = np.arange(-10, 10, step=0.1)</p><p class="source-code">ys = [step_function(x) for x in xs]</p></li>
				<li>Finally, you can visualize the function with the help of the Matplotlib library in just two lines of code:<p class="source-code">plt.plot(xs, ys, color='#000000', lw=3)</p><p class="source-code">plt.title(ReLU activation function', fontsize=20)</p><p>You can see how the function works visually in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="Images/B16954_06_9.jpg" alt="Figure 6.9 – ReLU activation function&#13;&#10;" width="1388" height="728"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – ReLU activation function</p>
			<p>And that's ReLU in a nutshell. You can use the default version or any of the variations (for example, leaky ReLU or parametric ReLU), depending on the use case.</p>
			<p>You now know enough of the <a id="_idIndexMarker404"/>theory to code a basic neural network with Python. We haven't covered all of the <a id="_idIndexMarker405"/>theoretical topics, so terms such as loss, gradient descent, backpropagation, and others may still feel abstract. We'll try to demystify them in the hands-on example that's coming up next.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor082"/>Using neural networks to classify handwritten digits</h1>
			<p>The "hello world" of deep learning is training a model that can classify handwritten digits. That's just what <a id="_idIndexMarker406"/>you'll do in this section. It will only require a couple of lines of code to implement with the TensorFlow library.</p>
			<p>Before you can proceed, you'll have to install TensorFlow. The process is a bit different depending on whether you're on Windows, macOS, or Linux, and whether you have a CUDA-compatible GPU or not. You can <a id="_idIndexMarker407"/>refer to the official installation instructions: <a href="https://www.tensorflow.org/install">https://www.tensorflow.org/install</a>. The rest of this section assumes you have TensorFlow 2.x installed. Here are the steps to follow:</p>
			<ol>
				<li value="1">To start, you'll have to import the <a id="_idIndexMarker408"/>TensorFlow library along with some additional modules. The <strong class="source-inline">datasets</strong> module enables you to download data straight from the notebook. The <strong class="source-inline">layers</strong> and <strong class="source-inline">models</strong> modules will be used later to design the architecture of the neural network.<p>Here's the code snippet for the imports:</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import datasets, layers, models</p></li>
				<li>You can now proceed with data gathering and preparation. A call to <strong class="source-inline">datasets.mnist.load_data()</strong> will download train and test images alongside the train and test labels. The images are grayscale and 28x28 pixels in size. This means you'll have a bunch of 28x28 matrices with values ranging from 0 (black) to 255 (white).<p>You can then further prepare the dataset by rescaling the images – dividing the values by 255 to bring everything into a zero-to-one range:</p><p class="source-code">(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()</p><p class="source-code">train_images, test_images = train_images / 255.0, test_images / 255.0</p><p>Here's what you should see in your notebook:</p><div id="_idContainer138" class="IMG---Figure"><img src="Images/B16954_06_10.jpg" alt="Figure 6.10 – Downloading the MNIST dataset&#13;&#10;" width="1462" height="88"/></div><p class="figure-caption">Figure 6.10 – Downloading the MNIST dataset</p></li>
				<li>Furthermore, you can inspect the matrix values for one of the images to see if you can spot the pattern inside. <p>The following line of code makes it <a id="_idIndexMarker409"/>easy to inspect matrices – it prints them and rounds all floating-point numbers to a single decimal point:</p><p class="source-code">print('\n'.join([''.join(['{:4}'.format(round(item, 1)) for item in row]) for row in train_images[0]]))</p><p>The results are shown in the following screenshot:</p><div id="_idContainer139" class="IMG---Figure"><img src="Images/B16954_06_11.jpg" alt="Figure 6.11 – Inspecting a single image matrix&#13;&#10;" width="1644" height="902"/></div><p class="figure-caption">Figure 6.11 – Inspecting a single image matrix</p><p>Do you notice how easy it is to spot a 5 in the image? You can execute <strong class="source-inline">train_labels[0]</strong> to verify.</p></li>
				<li>You can continue with laying out the neural network architecture next. As mentioned earlier, the input images are 28x28 pixels in size. Artificial neural networks can't process a matrix directly, so you'll have to convert this matrix to a vector. This process is <a id="_idIndexMarker410"/>known as <strong class="bold">flattening</strong>. As a result, you'll end up with a single vector of size (1, (28x28)), or (1, 784). <p>This input data can be passed to our first and only hidden layer, with 128 neurons. You know how to code out neurons and layers manually, but that's inconvenient in practice. Instead, you can use <strong class="source-inline">layers.Dense()</strong> to construct a layer.</p><p>This hidden layer will also <a id="_idIndexMarker411"/>need an activation function, so you can use ReLU.</p><p>Finally, you can add the final (output) layer, which needs to have as many neurons as there are distinct classes – 10 in this case.</p><p>Here's the entire code snippet for the network architecture:</p><p class="source-code">model = models.Sequential([</p><p class="source-code">  layers.Flatten(input_shape=(28, 28)),</p><p class="source-code">  layers.Dense(128, activation='relu'),</p><p class="source-code">  layers.Dense(10)</p><p class="source-code">])</p><p>The <strong class="source-inline">models.Sequential</strong> function allows you to stack layers one after the other, and, well, to make a network out of the individual layers.</p><p>You can view the architecture of your model by calling the <strong class="source-inline">summary()</strong> method on it:</p><p class="source-code">model.summary()</p><p>The results are shown in the following screenshot:</p><div id="_idContainer140" class="IMG---Figure"><img src="Images/B16954_06_12.jpg" alt="Figure 6.12 – Neural network architecture&#13;&#10;" width="1044" height="490"/></div><p class="figure-caption">Figure 6.12 – Neural network architecture</p></li>
				<li>There's still one thing you need to do before model training, and that is to compile the model. During the <a id="_idIndexMarker412"/>compilation, you'll have to specify values for the optimizer, loss, and the optimization metrics. <p>These haven't been covered in this chapter, but a brief explanation of each follows:</p><ul><li><em class="italic">Optimizers</em> – algorithms used to change the attributes of the neural networks to reduce the loss. These attributes include weights, learning rates, and so on.</li><li><em class="italic">Loss</em> – a method used to calculate gradients, which are then used to update the weights in the neural network.</li><li><em class="italic">Metrics</em> – the metric(s) you're optimizing for (for example, accuracy).<p>Going deeper into any of these topics is beyond the scope of this book. There are plenty of resources for discovering the theory behind deep learning. This chapter only aims to cover the essential basics.</p><p>You can compile your neural network by executing the following code:</p><p class="source-code">model.compile(</p><p class="source-code">    optimizer='adam',</p><p class="source-code">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),</p><p class="source-code">    metrics=['accuracy']</p><p class="source-code">)</p></li></ul></li>
				<li>Now you're ready to train the model. The training subset will be used to train the network, and the testing <a id="_idIndexMarker413"/>subset will be used for the evaluation. The network will be trained for 10 epochs (10 complete passes through the entire training data).<p>You can use the following code snippet to train the model:</p><p class="source-code">history = model.fit(</p><p class="source-code">    train_images, </p><p class="source-code">    train_labels, </p><p class="source-code">    epochs=10, </p><p class="source-code">    validation_data=(test_images, test_labels)</p><p class="source-code">)</p><p>Executing the preceding code will start the training process. How long it will take depends on the hardware you have and whether you're using a GPU or CPU. You should see something similar to the following screenshot:</p><div id="_idContainer141" class="IMG---Figure"><img src="Images/B16954_06_13.jpg" alt="Figure 6.13 – MNIST model training&#13;&#10;" width="1387" height="460"/></div><p class="figure-caption">Figure 6.13 – MNIST model training</p><p>After 10 epochs, the accuracy on the validation set was 97.7% – excellent if we consider that regular neural networks don't work too well with images.</p></li>
				<li>To test your model on a <a id="_idIndexMarker414"/>new instance, you can use the <strong class="source-inline">predict()</strong> method. It returns an array that tells you how likely it is that the prediction for a given class is correct. There will be 10 items in this array, as there were 10 classes. <p>You can then call <strong class="source-inline">np.argmax()</strong> to get the item with the highest value:</p><p class="source-code">import numpy as np</p><p class="source-code">prediction = model.predict(test_images[0].reshape(-1, 784))</p><p class="source-code">print(f'True digit = {test_labels[0]}')</p><p class="source-code">print(f'Predicted digit = {np.argmax(prediction)}')</p><p>The results are shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="Images/B16954_06_14.jpg" alt="Figure 6.14 – Testing the MNIST model&#13;&#10;" width="317" height="74"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Testing the MNIST model</p>
			<p>As you can see, the prediction is correct.</p>
			<p>And that's how easy it is to train neural networks with libraries such as TensorFlow. Keep in mind that this way of handling image classification isn't recommended in the real world, as we've flattened a 28x28 image and immediately lost all two-dimensional information. CNNs would be a better approach for image classification, as they can extract useful features from <a id="_idIndexMarker415"/>two-dimensional data. Our artifical neural network worked well here because MNIST is a simple and clean dataset – not something you'll get a whole lot of in your job.</p>
			<p>In the next section, you'll learn the differences in approaching classification and regression tasks with neural networks.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/>Neural networks in regression versus classification</h1>
			<p>If you've done any machine learning with scikit-learn, you know there are dedicated classes and models for regression and classification datasets. For example, if you would like to apply a decision tree <a id="_idIndexMarker416"/>algorithm to a classification dataset, you would use the <strong class="source-inline">DecisionTreeClassifier</strong> class. Likewise, you would use the <strong class="source-inline">DecisionTreeRegressor</strong> class for regression tasks.</p>
			<p>But what do you do with <a id="_idIndexMarker417"/>neural networks? There are no dedicated classes or layers for classification and regression tasks.</p>
			<p>Instead, you can <a id="_idIndexMarker418"/>accommodate by tweaking the number of neurons in the output layer. Put simply, if you're dealing with regression tasks, there has to be a single neuron in the output layer. If you're dealing with classification tasks, there will be as many neurons in the output layer as there are distinct classes in your target variable.</p>
			<p>For example, you saw how the <a id="_idIndexMarker419"/>neural network in the previous section had 10 neurons in the output layer. The reason is that there are 10 distinct digits, from zero to nine. If you were instead predicting the price of something (regression), there would be only a single neuron in the output layer.</p>
			<p>The task of the neural network is to learn the adequate parameter values (weights and biases) to produce the best output value, irrespective of the type of problem you're solving.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>Summary</h1>
			<p>This chapter might be hard to process if this was your first encounter with deep learning and neural networks. Going over the materials a couple of times could help, but it won't be enough to understand the topic fully. Entire books have been written on deep learning, and even on small subsets of deep learning. Hence, covering everything in a single chapter isn't possible.</p>
			<p>Still, you should have the basic theory behind the concepts of neurons, layers, and activation functions, and you can always learn more on your own. The following chapter, <a href="B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a><em class="italic">, Neural Network Classifier with TPOT</em>, will show you how to connect neural networks and pipeline optimization, so you can build state-of-the-art models in a completely automated fashion.</p>
			<p>As always, please feel free to explore the theory and practice of deep learning and neural networks on your own. It is definitely a field of study worth exploring further.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Q&amp;A</h1>
			<ol>
				<li value="1">How would you define the term "deep learning"?</li>
				<li>What is the difference between traditional machine learning algorithms and algorithms used in deep learning?</li>
				<li>List and briefly describe five types of neural networks.</li>
				<li>Can you figure out how to calculate the number of trainable parameters in a network given the number of neurons per layer? For example, a neural network with the architecture [10, 8, 8, 2] has in total 178 trainable parameters (160 weights and 18 biases).</li>
				<li>Name four different activation functions and briefly explain them.</li>
				<li>In your own words, describe <em class="italic">loss</em> in neural networks.</li>
				<li>Explain why modeling imagine classification models with regular artificial neural networks isn't a good idea.</li>
			</ol>
		</div>
	</div></body></html>