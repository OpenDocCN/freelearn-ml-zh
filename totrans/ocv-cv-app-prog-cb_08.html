<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Detecting Interest Points"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Detecting Interest Points</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Detecting corners in an image</li><li class="listitem">Detecting features quickly</li><li class="listitem">Detecting scale-invariant features</li><li class="listitem">Detecting FAST features at multiple scales</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Detecting Interest Points">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec52" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">In computer vision, the concept of interest points—also called<a id="id664" class="calibre1"/> <span class="strong"><strong class="calibre2">keypoints</strong></span> or <a id="id665" class="calibre1"/>
<span class="strong"><strong class="calibre2">feature points</strong></span>—has been largely used to solve many problems in object recognition, image registration, visual tracking, 3D reconstruction, and more. This concept relies on the idea that instead of looking at the image as a whole, it could be advantageous to select some special points in the image and perform a local analysis on them. This approach works well as long as a sufficient number of such points are detected in the images of interest and these points are distinguishing and stable features that can be accurately localized.</p><p class="calibre8">Because they are used for analyzing image content, feature points should ideally be detected at the same scene or object location no matter from which viewpoint, scale, or orientation the image was taken. View invariance is a very desirable property in image analysis and has been the object of numerous studies. As we will see, different detectors have different invariance properties. This chapter focuses on the keypoint extraction process itself. The next two chapters will then show you how interest points can be put to work in different contexts such as image matching or image geometry estimation.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Detecting corners in an image"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec53" class="calibre1"/>Detecting corners in an image</h1></div></div></div><p class="calibre8">When <a id="id666" class="calibre1"/>searching<a id="id667" class="calibre1"/> for interesting feature points in images, corners come out as an interesting solution. They are indeed local features that can be easily localized in an image, and in addition, they should abound in scenes of man-made objects (where they are produced by walls, doors, windows, tables, and so on). Corners are also interesting because they are two-dimensional features that can be accurately localized (even at sub-pixel accuracy), as they are at the junction <a id="id668" class="calibre1"/>of two edges. This is in<a id="id669" class="calibre1"/> contrast to points located on a uniform area or on the contour of an object and points that would be difficult to repeatedly localize precisely on other images of the same object. The Harris feature detector is a classical approach to detecting corners in an image. We will explore this operator in this recipe.</p></div>

<div class="book" title="Detecting corners in an image">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec151" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The basic OpenCV function that is used to detect Harris corners is called <code class="email">cv::cornerHarris</code><a id="id670" class="calibre1"/> and is straightforward to use. You call it on an input image, and the result is an image of floats that gives you the corner strength at each pixel location. A threshold is then applied on this output image in order to obtain a set of detected corners. This is accomplished with the following code:</p><div class="informalexample"><pre class="programlisting">   // Detect Harris Corners
   cv::Mat cornerStrength;
   cv::cornerHarris(image,      // input image
                cornerStrength, // image of cornerness
                3,              // neighborhood size
                3,              // aperture size
                0.01);          // Harris parameter

   // threshold the corner strengths
   cv::Mat harrisCorners;
   double threshold= 0.0001; 
   cv::threshold(cornerStrength,harrisCorners,
                 threshold,255,cv::THRESH_BINARY);</pre></div><p class="calibre8">Here is the original image:</p><div class="mediaobject"><img src="../images/00125.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The result is a <a id="id671" class="calibre1"/>binary map image shown in the following screenshot, which<a id="id672" class="calibre1"/> is inverted for better viewing (that is, we used <code class="email">cv::THRESH_BINARY_INV</code> instead of <code class="email">cv::THRESH_BINARY</code> to get the detected corners in black):</p><div class="mediaobject"><img src="../images/00126.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From the <a id="id673" class="calibre1"/>preceding function call, we observe that this interest point detector requires several parameters (these will be explained in the next section) that might <a id="id674" class="calibre1"/>make it difficult to tune. In addition, the corner map that is obtained contains many clusters of corner pixels that contradict the fact that we would like to detect well-localized points. Therefore, we will try to improve the corner-detection method by defining our own class to detect Harris corners.</p><p class="calibre8">The class encapsulates the Harris parameters with their default values and corresponding getter and setter methods (which are not shown here):</p><div class="informalexample"><pre class="programlisting">class HarrisDetector {

  private:

     // 32-bit float image of corner strength
     cv::Mat cornerStrength;
     // 32-bit float image of thresholded corners
     cv::Mat cornerTh;
     // image of local maxima (internal)
     cv::Mat localMax;
     // size of neighborhood for derivatives smoothing
     int neighbourhood; 
     // aperture for gradient computation
     int aperture; 
     // Harris parameter
     double k;
     // maximum strength for threshold computation
     double maxStrength;
     // calculated threshold (internal)
     double threshold;
     // size of neighborhood for non-max suppression
     int nonMaxSize; 
     // kernel for non-max suppression
     cv::Mat kernel;

  public:

     HarrisDetector() : neighbourhood(3), aperture(3), 
                        k(0.01), maxStrength(0.0), 
                        threshold(0.01), nonMaxSize(3) {
     
        // create kernel used in non-maxima suppression
        setLocalMaxWindowSize(nonMaxSize);
     }</pre></div><p class="calibre8">To detect<a id="id675" class="calibre1"/> the <a id="id676" class="calibre1"/>Harris corners on an image, we proceed with two steps. First, the Harris values at each pixel are computed:</p><div class="informalexample"><pre class="programlisting">     // Compute Harris corners
     void detect(const cv::Mat&amp; image) {
   
        // Harris computation
        cv::cornerHarris(image,cornerStrength,
                neighbourhood,// neighborhood size
                aperture,     // aperture size
                k);           // Harris parameter
   
        // internal threshold computation 
        cv::minMaxLoc(cornerStrength,
             0&amp;maxStrength);

        // local maxima detection
        cv::Mat dilated;  // temporary image
        cv::dilate(cornerStrength,dilated,cv::Mat());
        cv::compare(cornerStrength,dilated,
                    localMax,cv::CMP_EQ);
     }</pre></div><p class="calibre8">Next, the <a id="id677" class="calibre1"/>feature points are obtained based on a specified threshold <a id="id678" class="calibre1"/>value. Since the range of possible values for Harris depends on the particular choices of its parameters, the threshold is specified as a quality level that is defined as a fraction of the maximal Harris value computed in the image:</p><div class="informalexample"><pre class="programlisting">     // Get the corner map from the computed Harris values
     cv::Mat getCornerMap(double qualityLevel) {

        cv::Mat cornerMap;

        // thresholding the corner strength
        threshold= qualityLevel*maxStrength;
        cv::threshold(cornerStrength,cornerTh,
                      threshold,255,cv::THRESH_BINARY);

        // convert to 8-bit image
        cornerTh.convertTo(cornerMap,CV_8U);
   
        // non-maxima suppression
        cv::bitwise_and(cornerMap,localMax,cornerMap);

        return cornerMap;
     }</pre></div><p class="calibre8">This method returns a binary corner map of the detected features. The fact that the detection of the Harris features has been split into two methods allows us to test the detection with a different threshold (until an appropriate number of feature points are obtained) without the need to repeat costly computations. It is also possible to obtain the Harris features in the form of a <code class="email">std::vector</code> of <code class="email">cv::Point</code>:</p><div class="informalexample"><pre class="programlisting">     // Get the feature points from the computed Harris values
     void getCorners(std::vector&lt;cv::Point&gt; &amp;points, 
                     double qualityLevel) {

        // Get the corner map
        cv::Mat cornerMap= getCornerMap(qualityLevel);
        // Get the corners
        getCorners(points, cornerMap);
     }

     // Get the feature points from the computed corner map
     void getCorners(std::vector&lt;cv::Point&gt; &amp;points, 
                     const cv::Mat&amp; cornerMap) {
           
        // Iterate over the pixels to obtain all features
        for( int y = 0; y &lt; cornerMap.rows; y++ ) {
    
           const uchar* rowPtr = cornerMap.ptr&lt;uchar&gt;(y);
    
           for( int x = 0; x &lt; cornerMap.cols; x++ ) {

              // if it is a feature point
              if (rowPtr[x]) {

                 points.push_back(cv::Point(x,y));
              }
           } 
        }
     }</pre></div><p class="calibre8">This class also<a id="id679" class="calibre1"/> improves the detection of the Harris corners by<a id="id680" class="calibre1"/> adding a non-maxima suppression step, which will be explained in the next section. The detected points can now be drawn on an image using the <code class="email">cv::circle</code> function, as demonstrated by the following method:</p><div class="informalexample"><pre class="programlisting">     // Draw circles at feature point locations on an image
     void drawOnImage(cv::Mat &amp;image, 
        const std::vector&lt;cv::Point&gt; &amp;points, 
        cv::Scalar color= cv::Scalar(255,255,255), 
        int radius=3, int thickness=1) {
        std::vector&lt;cv::Point&gt;::const_iterator it= 
                                       points.begin();

        // for all corners
        while (it!=points.end()) {

           // draw a circle at each corner location
           cv::circle(image,*it,radius,color,thickness);
           ++it;
        }
     }</pre></div><p class="calibre8">Using this class, the detection of the Harris points is accomplished as follows:</p><div class="informalexample"><pre class="programlisting">   // Create Harris detector instance
   HarrisDetector harris;
   // Compute Harris values
   harris.detect(image);
   // Detect Harris corners
   std::vector&lt;cv::Point&gt; pts;
   harris.getCorners(pts,0.02);
   // Draw Harris corners
   harris.drawOnImage(image,pts);</pre></div><p class="calibre8">This <a id="id681" class="calibre1"/>results <a id="id682" class="calibre1"/>in the following image:</p><div class="mediaobject"><img src="../images/00127.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Detecting corners in an image">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec152" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">To define the notion of corners in images, the Harris feature detector looks at the average change in directional intensity in a small window around a putative interest point. If we consider a displacement vector, <code class="email">(u,v)</code>, the average intensity change is given by the following:</p><div class="mediaobject"><img src="../images/00128.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The summation<a id="id683" class="calibre1"/> is over a defined neighborhood around the <a id="id684" class="calibre1"/>considered pixel (the size of this neighborhood corresponds to the third parameter in the <code class="email">cv::cornerHarris</code> function). This average intensity change can then be computed in all possible directions, which leads to the definition of a corner as a point for which the average change is high in more than one direction. From this definition, the Harris test is performed as follows. We first obtain the direction of the maximal average intensity change. Next, we check whether the average intensity change in the orthogonal direction is high as well. If this is the case, then we have a corner.</p><p class="calibre8">Mathematically, this condition can be tested by using an approximation of the preceding formula using the Taylor expansion:</p><div class="mediaobject"><img src="../images/00129.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This is then rewritten in the matrix form:</p><div class="mediaobject"><img src="../images/00130.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This matrix is a covariance matrix that characterizes the rate of intensity change in all directions. This definition involves the image's first derivatives that are often computed using the Sobel operator. This is the case with the OpenCV implementation, which is the fourth parameter of the function that corresponds to the aperture used for the computation of the Sobel filters. It <a id="id685" class="calibre1"/>can be shown that the two eigenvalues of the covariance <a id="id686" class="calibre1"/>matrix give you the maximal average intensity change and the average intensity change for the orthogonal direction. Then, if these two eigenvalues are low, we are in a relatively homogenous region. If one eigenvalue is high and the other is low, we must be on an edge. Finally, if both eigenvalues are high, then we are at a corner location. Therefore, the condition for a point to be accepted as a corner is that it must have the smallest eigenvalue of the covariance matrix at a higher point than a given threshold.</p><p class="calibre8">The original definition of the Harris corner algorithm uses some properties of the eigen decomposition theory in order to avoid the cost of explicitly computing the eigenvalues. These properties are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The product of the eigenvalues of a matrix is equal to its determinant</li><li class="listitem">The sum of the eigenvalues of a matrix is equal to the sum of the diagonal of the matrix (also known as the <span class="strong"><strong class="calibre2">trace</strong></span><a id="id687" class="calibre1"/> of the matrix)</li></ul></div><p class="calibre8">It then follows that we can verify whether the eigenvalues of a matrix are high by computing the following score:</p><div class="mediaobject"><img src="../images/00131.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">One can easily verify that this score will indeed be high only if both eigenvalues are high too. This is the score that is computed by the <code class="email">cv::cornerHarris</code> function at each pixel location. The value of <code class="email">k</code> is specified as the fifth parameter of the function. It could be difficult to determine what would be the best value for this parameter. However, in practice, it has been seen that a value in the range of <code class="email">0.05</code> and <code class="email">0.5</code> generally gives good results.</p><p class="calibre8">To improve the result of the detection, the class described in the previous section adds an additional non-maxima suppression step. The goal here is to exclude Harris corners that are adjacent to others. Therefore, to be accepted, the Harris corner must not only have a score higher than the specified threshold, but it must also be a local maximum. This condition is tested by using a simple trick that consists of dilating the image of the Harris score in our <code class="email">detect</code> method:</p><div class="informalexample"><pre class="programlisting">        cv::dilate(cornerStrength,dilated,cv::Mat());</pre></div><p class="calibre8">Since the dilation replaces each pixel value with the maximum in the defined neighborhood, the only points that will not be modified are the local maxima. This is what is verified by the following equality test:</p><div class="informalexample"><pre class="programlisting">        cv::compare(cornerStrength,dilated, 
                    localMax,cv::CMP_EQ);</pre></div><p class="calibre8">The <code class="email">localMax</code> matrix will therefore be true (that is, non-zero) only at local maxima locations. We then use it in our <code class="email">getCornerMap</code> method to suppress all non-maximal features (using the <code class="email">cv::bitwise_and</code> function).</p></div></div>

<div class="book" title="Detecting corners in an image">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec153" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Additional improvements can be made to the original Harris corner algorithm. This section describes another corner detector found in OpenCV, which expands the Harris detector to make its corners more uniformly distributed across the image. As we will see, this operator has an implementation for the feature detector in the OpenCV 2 common interface.</p><div class="book" title="Good features to track"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec35" class="calibre1"/>Good features to track</h3></div></div></div><p class="calibre8">With the advent of floating-point processors, the mathematical simplification introduced to avoid eigenvalue decomposition has become negligible, and consequently, the detection of Harris corners can be made based on the explicitly computed eigenvalues. In principle, this modification should not significantly affect the result of the detection, but it avoids the use of the arbitrary <code class="email">k</code> parameter. Note that two functions exist that allow you to explicitly get the eigenvalues (and eignevectors) of the Harris covariance matrix; these are <code class="email">cv::cornerEigenValsAndVecs</code> and <code class="email">cv::cornerMinEigenVal</code>.</p><p class="calibre8">A second modification addresses the problem of feature point clustering. Indeed, in spite of the introduction of the local maxima condition, interest points tend to be unevenly distributed across an image, showing concentrations at highly textured locations. A solution to this problem is to impose a minimum distance between two interest points. This can be achieved using the following algorithm. Starting from the point with the strongest Harris score (that is, with the largest minimum eigenvalue), only accept interest points if they are located at, at least, a given distance from the already accepted points. This solution is implemented in OpenCV in the <a id="id688" class="calibre1"/>
<code class="email">cv::goodFeaturesToTrack</code> function, which is thus named because the features it detects can be used as a good <a id="id689" class="calibre1"/>starting set in visual tracking applications. This is called as follows:</p><div class="informalexample"><pre class="programlisting">   // Compute good features to track
   std::vector&lt;cv::Point2f&gt; corners;
   cv::goodFeaturesToTrack(image,  // input image
      corners, // corner image
      500,     // maximum number of corners to be returned
      0.01,    // quality level
      10);     // minimum allowed distance between points</pre></div><p class="calibre8">In addition to the quality-level threshold value and the minimum tolerated distance between interest points, the<a id="id690" class="calibre1"/> function also uses a maximum number of points that can be returned (this is possible since points are accepted in the order of strength). The preceding function call produces the following result:</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="Good features to track" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This approach increases the complexity of the detection, since it requires the interest points to be sorted by their Harris score, but it also clearly improves the distribution of the points across the image. Note that this function also includes an optional flag that requests Harris corners to be detected using the classical corner score definition (using the covariance matrix determinant and trace).</p></div><div class="book" title="The feature detector's common interface"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec36" class="calibre1"/>The feature detector's common interface</h3></div></div></div><p class="calibre8">OpenCV 2 has<a id="id691" class="calibre1"/> introduced a common interface for its different interest point detectors. This interface allows easy testing of different interest point detectors within the same application.</p><p class="calibre8">The interface defines a<a id="id692" class="calibre1"/> <code class="email">cv::Keypoint</code> class that encapsulates the properties of each detected feature point. For the Harris corners, only the position of the keypoints and its response strength is relevant. The <span class="strong"><em class="calibre9">Detecting scale-invariant features</em></span> recipe will discuss the other properties that can be associated with a keypoint.</p><p class="calibre8">The <code class="email">cv::FeatureDetector</code> abstract class<a id="id693" class="calibre1"/> basically imposes the existence of a <code class="email">detect</code> operation with the following signatures:</p><div class="informalexample"><pre class="programlisting">   void detect( const Mat&amp; image, vector&lt;KeyPoint&gt;&amp; keypoints,
                const Mat&amp; mask=Mat() ) const;

   void detect( const vector&lt;Mat&gt;&amp; images,
                vector&lt;vector&lt;KeyPoint&gt; &gt;&amp; keypoints,
                const vector&lt;Mat&gt;&amp; masks=
                                   vector&lt;Mat&gt;() ) const;</pre></div><p class="calibre8">The second method allows interest points to be detected in a vector of images. The class also includes other methods that can read and write the detected points in a file.</p><p class="calibre8">The <code class="email">cv::goodFeaturesToTrack</code> function has a wrapper<a id="id694" class="calibre1"/> class called <code class="email">cv::GoodFeaturesToTrackDetector</code>, which inherits from the <code class="email">cv::FeatureDetector</code> class. It can be used in a way that is similar to what we did with our Harris corners class, as follows:</p><div class="informalexample"><pre class="programlisting">   // vector of keypoints
   std::vector&lt;cv::KeyPoint&gt; keypoints;
   // Construction of the Good Feature to Track detector 
  cv::Ptr&lt;cv::FeatureDetector&gt; gftt= 
     new cv::GoodFeaturesToTrackDetector(
     500,  // maximum number of corners to be returned
     0.01, // quality level
     10);  // minimum allowed distance between points
  // point detection using FeatureDetector method
  gftt-&gt;detect(image,keypoints);</pre></div><p class="calibre8">The result is the same as the one obtained previously, since the same function is ultimately called by the wrapper. Note how we used the OpenCV 2 smart pointer class <code class="email">(cv::Ptr)</code> that, as explained in <a class="calibre1" title="Chapter 1. Playing with Images" href="part0014_split_000.html#page">Chapter 1</a>, <span class="strong"><em class="calibre9">Playing with Images</em></span>, automatically releases the pointed object when the reference count drops to zero.</p></div></div></div>

<div class="book" title="Detecting corners in an image">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec154" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The classic article that describes the Harris operator by C. Harris and M.J. Stephens<span class="strong"><em class="calibre9">, A combined corner and edge detector, Alvey Vision Conference, pp. 147–152, 1988</em></span></li><li class="listitem">The article by J. Shi and C. Tomasi<span class="strong"><em class="calibre9">, Good features to track, Int. Conference on Computer Vision and Pattern Recognition, pp. 593-600, 1994</em></span>, introduces these features</li><li class="listitem">The article by K. Mikolajczyk and C. Schmid,<span class="strong"><em class="calibre9"> Scale and Affine invariant interest point detectors, International Journal of Computer Vision, vol 60, no 1, pp. 63-86, 2004</em></span>, proposes a multi-scale and affine-invariant Harris operator</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Detecting features quickly"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec54" class="calibre1"/>Detecting features quickly</h1></div></div></div><p class="calibre8">The Harris operator<a id="id695" class="calibre1"/> proposed a formal mathematical definition for corners (or more generally, interest points) based on the rate of intensity changes in two perpendicular directions. Although this constitutes a sound definition, it requires the computation of the image derivatives, which is a costly operation, especially considering the fact that interest point detection is often just the first step in a more complex algorithm.</p><p class="calibre8">In this recipe, we present another feature point operator, called<a id="id696" class="calibre1"/> <span class="strong"><strong class="calibre2">FAST</strong></span> (<span class="strong"><strong class="calibre2">Features from Accelerated Segment Test</strong></span>). This one has been specifically designed to allow quick detection of interest points in an image; the decision to accept or not to accept a keypoint is based on only a few pixel comparisons.</p></div>

<div class="book" title="Detecting features quickly">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec155" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Using the OpenCV 2 common interface for feature point detection makes the deployment of any feature point detectors easy. The detector presented in this recipe is the FAST detector. As the name suggests, it has been designed to be quick in order to compute the following:</p><div class="informalexample"><pre class="programlisting">   // vector of keypoints
   std::vector&lt;cv::KeyPoint&gt; keypoints;
   // Construction of the Fast feature detector object 
   cv::Ptr&lt;cv::FeatureDetector&gt; fast= 
   new cv::FastFeatureDetector(
     40); // threshold for detection
   // feature point detection 
   fast-&gt;detect(image,keypoints);</pre></div><p class="calibre8">Note that OpenCV also proposes a generic function to draw keypoints on an image:</p><div class="informalexample"><pre class="programlisting">   cv::drawKeypoints(image,    // original image
      keypoints,               // vector of keypoints
      image,                   // the output image
      cv::Scalar(255,255,255), // keypoint color
      cv::DrawMatchesFlags::DRAW_OVER_OUTIMG); //drawing flag</pre></div><p class="calibre8">By specifying the chosen drawing flag, the keypoints are drawn over the input image, thus producing the following output result:</p><div class="mediaobject"><img src="../images/00133.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">An interesting option is to specify a negative value for the keypoint color. In this case, a different random color will be selected for each drawn circle.</p></div></div>

<div class="book" title="Detecting features quickly">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec156" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">As in the case <a id="id697" class="calibre1"/>with the Harris point detector, the FAST feature algorithm derives from the definition of what constitutes a <span class="strong"><em class="calibre9">corner</em></span>. This time, this definition is based on the image intensity around a putative feature point. The decision to accept a keypoint is taken by examining a circle of pixels centered at a candidate point. If an arc of contiguous points of a length greater than 3/4 of the circle perimeter in which all pixels significantly differ from the intensity of the center point (being all darker or all brighter) is found, then a keypoint is declared.</p><p class="calibre8">This is a simple test that can be computed quickly. Moreover, in its original formulation, the algorithm uses an additional trick to further speed up the process. Indeed, if we first test four points separated by 90 degrees on the circle (for example, top, bottom, right, and left points), it can be easily shown that in order to satisfy the condition expressed previously, at least three of these points must all be brighter or darker than the central pixel.</p><p class="calibre8">If this is not the case, the point can be rejected immediately, without inspecting additional points on the circumference. This is a very effective test, since in practice, most of the image points will be rejected by this simple 4-comparison test.</p><p class="calibre8">In principle, the radius of the circle of examined pixels could have been a parameter of the method. However, it has been found that in practice, a radius of <code class="email">3</code> gives you both good results and high efficiency. There are, then, <code class="email">16</code> pixels that need to be considered on the circumference of the circle, shown as follows:</p><div class="mediaobject"><img src="../images/00134.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The four points used for the pretest are the <span class="strong"><strong class="calibre2">1</strong></span>, <span class="strong"><strong class="calibre2">5</strong></span>, <span class="strong"><strong class="calibre2">9</strong></span>, and <span class="strong"><strong class="calibre2">13</strong></span> pixels, and the required number of contiguous darker or brighter points is <span class="strong"><strong class="calibre2">12</strong></span>. However, it has been observed that by reducing the length of the contiguous segment to <span class="strong"><strong class="calibre2">9</strong></span>, better repeatability of the detected corners across images is obtained. This variant is often designated as the <a id="id698" class="calibre1"/>
<span class="strong"><strong class="calibre2">FAST-9</strong></span> corner detector, and this is the one that is used by OpenCV. Note that there exists a <code class="email">cv::FASTX</code> function that proposes another variant of the FAST detector.</p><p class="calibre8">To be considered as being significantly darker or brighter, the intensity of a point must differ from the intensity of the central pixel by at least a given amount; this value corresponds to the threshold parameter specified in the function call. The larger this threshold is, the fewer corner points will be detected.</p><p class="calibre8">As for Harris features, it is often better to perform non-maxima suppression on the corners that have been found. Therefore, a corner strength measure needs to be defined. Several alternatives measures to this can considered, and the one that has been retained is the following. The <a id="id699" class="calibre1"/>strength of a corner is given by the sum of the absolute difference between the central pixel and the pixels on the identified contiguous arc. Note that the algorithm is also available through a direct function call:</p><div class="informalexample"><pre class="programlisting">  cv::FAST(image,     // input image 
         keypoints,   // output vector of keypoints
         40,          // threshold
         false);      // non-max suppression? (or not)</pre></div><p class="calibre8">However, because of its flexibility, the use of the <code class="email">cv::FeatureDetector</code> interface is recommended.</p><p class="calibre8">This algorithm results in very fast interest point detection and is therefore the feature of choice when speed is a concern. This is the case, for example, in real-time visual tracking or object-recognition applications where several points must be tracked or matched in a live video stream.</p></div></div>

<div class="book" title="Detecting features quickly">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec157" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">To improve the detection of feature points, additional tools are offered by OpenCV. Indeed, a number of class adapters are available in order to better control the way the keypoints are extracted.</p><div class="book" title="Adapted feature detection"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec37" class="calibre1"/>Adapted feature detection</h3></div></div></div><p class="calibre8">If you<a id="id700" class="calibre1"/> wish to better control the number of detected points, a special subclass of the <code class="email">cv::FeatureDetector</code> class, called <code class="email">cv::DynamicAdaptedFeatureDetector</code>, is available. This allows you to specify the number of interest points that can be detected as an interval. In the case of the FAST feature detector, this is used as follows:</p><div class="informalexample"><pre class="programlisting">  cv::DynamicAdaptedFeatureDetector fastD(
    new cv::FastAdjuster(40), // the feature detector
    150,   // min number of features
    200,   // max number of features
    50);   // max number of iterations
  fastD.detect(image,keypoints); // detect points</pre></div><p class="calibre8">The interest points will then be iteratively detected. After each iteration, the number of detected points is checked and the detector threshold is adjusted accordingly in order to produce more or less points; this process is repeated until the number of detected points fit into the specified interval. A maximum number of iterations is specified in order to avoid that the method spends too much time on multiple detections. For this method to be implemented in a generic way, the used <code class="email">cv::FeatureDetector</code> class must implement the <code class="email">cv::AdjusterAdapter</code> interface. This class includes a <code class="email">tooFew</code> method and a <code class="email">tooMany</code> method, both of which modify the internal threshold of the detector in order to produce more or less keypoints. There is also a <code class="email">good</code> predicate method that returns <code class="email">true</code> if the detector threshold can still be adjusted. Using a <code class="email">cv::DynamicAdaptedFeatureDetector</code> class can be a good strategy to obtain an appropriate number of feature points; however, you must understand that there is a performance price that you will have to to pay for this benefit. Moreover, there is no guarantee that you will indeed obtain the requested number of features within the specified number of iterations.</p><p class="calibre8">You probably <a id="id701" class="calibre1"/>noticed that we passed an argument, which is the address of a dynamically allocated object, to specify the feature detector that will be used by the adapter class. You might wonder whether you have to release the allocated memory at some point in order to avoid memory leaks. The answer is no, and this is because the pointer is transferred to a <code class="email">cv::Ptr&lt;FeatureDetector&gt;</code> parameter that automatically releases the pointed object.</p></div><div class="book" title="Grid adapted feature detection"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec38" class="calibre1"/>Grid adapted feature detection</h3></div></div></div><p class="calibre8">A second useful class adapter is the <code class="email">cv::GridAdaptedFeatureDetector</code> class. As the name suggests, it allows you to define a grid over the image. Each cell of this grid is then constrained to contain a maximum number of elements. The idea here is to spread the set of detected keypoints over the image in a better manner. When detecting keypoints in an image, it is indeed common to see a concentration of interest points in a specific textured area. This is the case, for example, of the two towers of the church image on which a very dense set of FAST points have been detected. This class adapter is used as follows:</p><div class="informalexample"><pre class="programlisting">  cv::GridAdaptedFeatureDetector fastG(
    new cv::FastFeatureDetector(10), // the feature detector
    1200,  // max total number of keypoints
    5,     // number of rows in grid
    2);    // number of cols in grid
  fastG.detect(image,keypoints);</pre></div><p class="calibre8">The class adapter simply proceeds by detecting feature points on each individual cell using the provided <code class="email">cv::FeatureDetector</code> object. A maximum total number of points is also specified. Only the strongest points in each cell are kept in order to not exceed the specified maximum.</p></div><div class="book" title="Pyramid adapted feature detection"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec39" class="calibre1"/>Pyramid adapted feature detection</h3></div></div></div><p class="calibre8">The <code class="email">cv::PyramidAdaptedFeatureDetector</code> adapter proceeds by applying the feature detector on an image pyramid. The results are combined in the output vector of keypoints. This is called as follows:</p><div class="informalexample"><pre class="programlisting">  cv::PyramidAdaptedFeatureDetector fastP(
    new cv::FastFeatureDetector(60), // the feature detector
    3);    // number of levels in the pyramid
  fastP.detect(image,keypoints);</pre></div><p class="calibre8">The coordinates of each point are specified in the original image coordinates. In addition, the special <code class="email">size</code> attribute of the <code class="email">cv::Keypoint</code> class is set such that points detected at half the original resolution are attributed a size that is twice the size of  the detected points in the original image. There is a special flag in the <code class="email">cv::drawKeypoints</code> function that will draw the keypoints with a radius that is equal to the keypoint's <code class="email">size</code> attribute.</p><div class="mediaobject"><img src="../images/00135.jpeg" alt="Pyramid adapted feature detection" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Detecting features quickly">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec158" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The article by E. Rosten and T. Drummond<span class="strong"><em class="calibre9">, Machine learning for high-speed corner detection, In European Conference on Computer Vision, pp. 430-443, 2006</em></span>, describes the FAST feature algorithm and its variants in detail</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Detecting scale-invariant features"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec55" class="calibre1"/>Detecting scale-invariant features</h1></div></div></div><p class="calibre8">The view<a id="id702" class="calibre1"/> invariance of feature detection was presented as an important concept in the introduction of this chapter. While orientation invariance, which is the ability to detect the same points even if an image is rotated, has been relatively well handled by the simple feature point detectors that have been presented so far, the invariance to scale changes is more difficult to achieve. To address this problem, the concept of scale-invariant features has been introduced in computer vision. The idea here is to not only have a consistent detection of keypoints no matter at which scale an object is pictured, but to also have a scale factor associated with each of the detected feature points. Ideally, for the same object point featured at two different scales on two different images, the ratio of the two computed scale factors should correspond to the ratio of their respective scales. In recent years, several scale-invariant features have been proposed, and this recipe presents one of them, the <a id="id703" class="calibre1"/>
<span class="strong"><strong class="calibre2">SURF</strong></span> features. SURF stands for Speeded Up Robust Features, and as we will see, they are not only scale-invariant features, but they also offer the advantage of being computed very efficiently.</p></div>

<div class="book" title="Detecting scale-invariant features">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec159" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The SURF feature detector<a id="id704" class="calibre1"/> is implemented in OpenCV in the <code class="email">cv::SURF</code> function. It is also possible to use this through <code class="email">cv::FeatureDetector</code> as follows:</p><div class="informalexample"><pre class="programlisting">  // Construct the SURF feature detector object
  cv::Ptr&lt;cv::FeatureDetector&gt; detector = new cv::SURF(2000.); // threshold
  // Detect the SURF features
  detector-&gt;detect(image,keypoints);</pre></div><p class="calibre8">To draw these features, we again use the <code class="email">cv::drawKeypoints</code> OpenCV function with the <code class="email">DRAW_RICH_KEYPOINTS</code> flag such that we can visualize the associated scale factor:</p><div class="informalexample"><pre class="programlisting">   // Draw the keypoints with scale and orientation information
   cv::drawKeypoints(image,      // original image
      keypoints,                 // vector of keypoints
      featureImage,              // the resulting image
      cv::Scalar(255,255,255),   // color of the points
      cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS); //flag</pre></div><p class="calibre8">The resulting image with the detected features is then as follows:</p><div class="mediaobject"><img src="../images/00136.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As explained in the previous recipe, the size of the keypoint circles resulting from the use of the <code class="email">DRAW_RICH_KEYPOINTS</code> flag is proportional to the computed scale of each feature. The SURF algorithm<a id="id705" class="calibre1"/> also associates an orientation with each feature to make them invariant to rotations. This orientation is illustrated by a radial line inside each drawn circle.</p><p class="calibre8">If we take another picture of the same object but at a different scale, the feature-detection result is as follows:</p><div class="mediaobject"><img src="../images/00137.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">By carefully observing the detected keypoints on the two images, it can be seen that the change in the<a id="id706" class="calibre1"/> size of corresponding circles is often proportional to the change in scale. As an example, consider the bottom part of the upper-right window of the church. In both images, a SURF feature has been detected at that location, and the two corresponding circles (of different sizes) contain the same visual elements. Of course, this is not the case for all features, but as we will discover in the next chapter, the repeatability rate is sufficiently high to allow good matching between the two images.</p></div></div>

<div class="book" title="Detecting scale-invariant features">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec160" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 6. Filtering the Images" href="part0047_split_000.html#page">Chapter 6</a>, <span class="strong"><em class="calibre9">Filtering the Images</em></span>, we learned that the derivatives of an image can be estimated using Gaussian filters. These filters make use of a <code class="email">σ</code> parameter, which defines the aperture (size) of the kernel. As we saw, this <code class="email">σ</code> parameter corresponds to the variance of the Gaussian function used to construct the filter, and it then implicitly defines a scale at which the derivative is evaluated. Indeed, a filter that has a larger <code class="email">σ</code> value smoothes out the finer details of the image. This is why we can say that it operates at a coarser scale.</p><p class="calibre8">Now, if we <a id="id707" class="calibre1"/>compute, for instance, the Laplacian of a given image point using Gaussian filters at different scales, then different values are obtained. Looking at the evolution of the filter response for different scale factors, we obtain a curve that eventually reaches a maximum value at a <code class="email">σ</code> value. If we extract this maximum value for two images of the same object taken at two different scales, the ratio of these two <code class="email">σ</code> maxima will correspond to the ratio of the scales at which the images were taken. This important observation is at the core of the scale-invariant feature extraction process. That is, scale-invariant features should be detected as the local maxima in both the spatial space (in the image) and the scale space (as obtained from the derivative filters applied at different scales).</p><p class="calibre8">SURF implements this idea by proceeding as follows. First, to detect the features, the Hessian matrix is computed at each pixel. This matrix measures the local curvature of a function and is defined as follows:</p><div class="mediaobject"><img src="../images/00138.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The determinant of this matrix gives you the strength of this curvature. The idea, therefore, is to define corners as image points with high local curvature (that is, high variation in more than one direction). Since it is composed of second-order derivatives, this matrix can be computed using Laplacian of Gaussian kernels of a different scale, such as <code class="email">σ</code>. This Hessian then becomes a function of three variables, which are <code class="email">H(x,y,σ)</code>. Therefore, a scale-invariant feature is declared when the determinant of this Hessian reaches a local maximum in both spatial and scale space (that is, <code class="email">3x3x3</code> non-maxima suppression needs to be performed). Note that in order to be considered as a valid point, this determinant must have a minimum value as specified by the first parameter in the constructor of the <code class="email">cv::SURF</code> class.</p><p class="calibre8">However, the calculation of all of these derivatives at different scales is computationally costly. The objective of the SURF algorithm is to make this process as efficient as possible. This is achieved by using approximated Gaussian kernels that involve only few integer additions. These have the following structure:</p><div class="mediaobject"><img src="../images/00139.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The kernel <a id="id708" class="calibre1"/>on the left-hand side is used to estimate the mixed second derivatives, while the one on the right-hand side estimates the second derivative in the vertical direction. A rotated version of this second kernel estimates the second derivative in the horizontal direction. The smallest kernels have a size of <code class="email">9x9</code> pixels, corresponding to <code class="email">σ≈1.2</code>. To obtain a scale-space representation, kernels of increasing size are successively applied. The exact number of filters that are applied can be specified by additional parameters of the SURF class. By default, 12 different sizes of kernels are used (going up to size <code class="email">99x99</code>). Note that the fact that integral images are used guarantees that the sum inside each lobe of each filter can be computed by using only three additions independent of the size of the filter.</p><p class="calibre8">Once the local maxima are identified, the precise position of each detected interest point is obtained through interpolation in both scale and image space. The result is then a set of feature points that are localized at sub-pixel accuracy and to which a scale value is associated.</p></div></div>

<div class="book" title="Detecting scale-invariant features">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec161" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The SURF algorithm has been developed as an efficient variant of another well-known scale-invariant feature detector called<a id="id709" class="calibre1"/> <span class="strong"><strong class="calibre2">SIFT</strong></span> (<span class="strong"><strong class="calibre2">Scale-Invariant Feature Transform</strong></span>).</p><div class="book" title="The SIFT feature-detection algorithm"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec40" class="calibre1"/>The SIFT feature-detection algorithm</h3></div></div></div><p class="calibre8">SIFT also<a id="id710" class="calibre1"/> detects features as local maxima in the image and scale space but uses the Laplacian filter response instead of the Hessian determinant. This Laplacian is computed at different scales (that is, increasing values of <code class="email">σ</code>) using the difference of Gaussian filters, as explained in <a class="calibre1" title="Chapter 6. Filtering the Images" href="part0047_split_000.html#page">Chapter 6</a>, <span class="strong"><em class="calibre9">Filtering the Images</em></span>. To improve efficiency, each time the value of <code class="email">σ</code> is doubled, the size of the image is reduced by two. Each pyramid level corresponds to an <a id="id711" class="calibre1"/>
<span class="strong"><strong class="calibre2">octave</strong></span>, and each scale is a <span class="strong"><em class="calibre9">layer</em></span>. There are typically three layers per octave.</p><p class="calibre8">The following figure illustrates a pyramid of two octaves in which the four Gaussian-filtered images of the first octave produce three DoG layers:</p><div class="mediaobject"><img src="../images/00140.jpeg" alt="The SIFT feature-detection algorithm" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">OpenCV has a class that detects these features, and it is called in a way that is similar to the SURF one:</p><div class="informalexample"><pre class="programlisting">  // Construct the SIFT feature detector object
  detector = new cv::SIFT();
  // Detect the SIFT features
  detector-&gt;detect(image,keypoints);</pre></div><p class="calibre8">Here, we use all<a id="id712" class="calibre1"/> the default arguments to construct the detector, but you can specify the number of desired SIFT points (the strongest ones are kept), the number of layers per octave, and the initial value for <code class="email">σ</code>. The result is similar to the one obtained with SURF:</p><div class="mediaobject"><img src="../images/00141.jpeg" alt="The SIFT feature-detection algorithm" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">However, since the computation of the feature point is based on floating-point kernels, SIFT is generally <a id="id713" class="calibre1"/>considered to be more accurate in terms of feature localization in regards to space and scale. For the same reason, it is also more computationally expensive, although this relative efficiency depends on each particular implementation.</p><p class="calibre8">As a final remark, you might have noticed that the SURF and SIFT classes have been placed in a nonfree package of the OpenCV distribution. This is because these algorithms have been patented, and as such, their use in commercial applications might be subject to licensing agreements.</p></div></div></div>

<div class="book" title="Detecting scale-invariant features">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec162" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <span class="strong"><em class="calibre9">Computing the Laplacian of an image</em></span> recipe in <a class="calibre1" title="Chapter 6. Filtering the Images" href="part0047_split_000.html#page">Chapter 6</a>, <span class="strong"><em class="calibre9">Filtering the Images</em></span>, gives you more details on the Laplacian-of-Gaussian operator and the use of the difference of Gaussians</li><li class="listitem">The <span class="strong"><em class="calibre9">Describing local intensity patterns</em></span> recipe in <a class="calibre1" title="Chapter 9. Describing and Matching Interest Points" href="part0063_split_000.html#page">Chapter 9</a>, <span class="strong"><em class="calibre9">Describing and Matching Interest Points</em></span>, explains how these scale-invariant features can be described for robust image matching</li><li class="listitem">The article <span class="strong"><em class="calibre9">SURF: Speeded Up Robust Features </em></span>by H. Bay, A. Ess, T. Tuytelaars and L. Van Gool in <span class="strong"><em class="calibre9">Computer Vision and Image Understanding, vol. 110, No. 3, pp. 346-359, 2008</em></span>, describes the SURF feature algorithm</li><li class="listitem">The pioneering work by D. Lowe<span class="strong"><em class="calibre9">, Distinctive Image Features from Scale Invariant Features </em></span>in<span class="strong"><em class="calibre9"> International Journal of Computer Vision, Vol. 60, No. 2, 2004, pp. 91-110</em></span>, describes the SIFT algorithm</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Detecting FAST features at multiple scales"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec56" class="calibre1"/>Detecting FAST features at multiple scales</h1></div></div></div><p class="calibre8">FAST has been <a id="id714" class="calibre1"/>introduced as a quick way to detect keypoints in an image. With SURF and SIFT, the emphasis was on designing scale-invariant features. More recently, new interest point detectors have been introduced with the objective of achieving both fast detection and invariance to scale changes. This recipe presents the <a id="id715" class="calibre1"/>
<span class="strong"><strong class="calibre2">Binary Robust Invariant Scalable Keypoints</strong></span> (<span class="strong"><strong class="calibre2">BRISK</strong></span>) detector. It is based on the FAST feature detector that we described in a previous recipe of this chapter. Another detector, called<a id="id716" class="calibre1"/> <span class="strong"><strong class="calibre2">ORB</strong></span> (<span class="strong"><strong class="calibre2">Oriented FAST and Rotated BRIEF</strong></span>), will also be discussed at the end of this recipe. These two feature point detectors constitute an excellent solution when fast and reliable image matching is required. They are especially efficient when they are used in conjunction with their associated binary descriptors, as will be discussed in <a class="calibre1" title="Chapter 9. Describing and Matching Interest Points" href="part0063_split_000.html#page">Chapter 9</a>, <span class="strong"><em class="calibre9">Describing and Matching Interest Points</em></span>.</p></div>

<div class="book" title="Detecting FAST features at multiple scales">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec163" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Following what we did in the previous recipes, the detection of keypoints with BRISK uses the <code class="email">cv::FeatureDetector</code> abstract class. We first create an instance of the detector, and then the <code class="email">detect</code> method is called on an image:</p><div class="informalexample"><pre class="programlisting">  // Construct the BRISK feature detector object
  detector = new cv::BRISK();
  // Detect the BRISK features
  detector-&gt;detect(image,keypoints);</pre></div><p class="calibre8">The image result shows you the keypoints that are detected at multiple scales:</p><div class="mediaobject"><img src="../images/00142.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Detecting FAST features at multiple scales">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec164" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">BRISK is not<a id="id717" class="calibre1"/> only a feature point detector; the <a id="id718" class="calibre1"/>method also includes a procedure that describes  the neighborhood of each detected keypoint. This second aspect will be the subject of the next chapter. We describe here how the quick detection of keypoints at multiple scales is performed using BRISK.</p><p class="calibre8">In order to detect interest points at different scales, the method first builds an image pyramid through two down-sampling processes. The first process starts from the original image size and downscales it by half at each layer (or octave). Secondly, in-between layers are created by down-sampling the original image by a factor of 1.5, and from this reduced image, additional layers are generated through successive half-sampling.</p><div class="mediaobject"><img src="../images/00143.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The FAST feature detector<a id="id719" class="calibre1"/> is then applied on all the images of this pyramid. Keypoint extraction is based on a criterion that is similar to the one used by SIFT. First, an acceptable interest point must be a local maximum when comparing its strength with one of its eight spatial neighbors. If this is the case, the point is then compared with the scores of the neighboring points<a id="id720" class="calibre1"/> in the layers above and below; if its score is higher in scale as well, then it is accepted as an interest point. A key aspect of BRISK resides in the fact that the different layers of the pyramid have different resolutions. The method requires interpolation in both scale and space in order to locate each keypoint precisely. This interpolation is based on the FAST keypoint scores. In space, the interpolation is performed on a 3 x 3 neighborhood. In scale, it is computed by fitting a 1D parabola along the scale axis through the current point and its two neighboring local keypoints in the layers above and below; this keypoint localization in scale is illustrated in the preceding figure. As a result, even if the FAST keypoint detection is performed at discrete image scales, the resulting detected scales associated with each keypoint are continuous values.</p><p class="calibre8">The <code class="email">cv::BRISK</code> class<a id="id721" class="calibre1"/> proposes two optional parameters to control the detection of the keypoints. The first parameter is a threshold value that accepts FAST keypoints, and the second parameter is the number of octaves that will be generated in the image pyramid:</p><div class="informalexample"><pre class="programlisting">  // Construct another BRISK feature detector object
  detector = new cv::BRISK(
    20,  // threshold for FAST points to be accepted
    5);  // number of octaves</pre></div></div></div>

<div class="book" title="Detecting FAST features at multiple scales">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec165" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">BRISK is not the only multiscale, fast detector that is proposed in OpenCV. The ORB feature detector can also perform efficient keypoint detection.</p><div class="book" title="The ORB feature-detection algorithm"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl3sec41" class="calibre1"/>The ORB feature-detection algorithm</h3></div></div></div><p class="calibre8">ORB stands<a id="id722" class="calibre1"/> for <a id="id723" class="calibre1"/>
<span class="strong"><strong class="calibre2">Oriented FAST and Rotated BRIEF</strong></span>. The first part of this acronym refers to the keypoint detection part, while the second part refers to the descriptor that is proposed by ORB. Here, we focus here on the detection method; the descriptor will be presented in the next chapter.</p><p class="calibre8">As with BRISK, ORB first creates an image pyramid. This one is made of a number of layers in which each layer is a down-sampled version of the previous one by a certain scale factor (typically, <code class="email">8</code> scales and <code class="email">1.2</code> scale factor reduction; these are parameters in the <code class="email">cv::ORB</code> function). The strongest <code class="email">N</code> keypoints are then accepted where the keypoint score is defined by the Harris <span class="strong"><em class="calibre9">cornerness</em></span> measure that was defined in the first recipe of this chapter (authors of this method found the Harris score to be a more reliable measure).</p><p class="calibre8">An original aspect of the ORB detector<a id="id724" class="calibre1"/> resides in the fact that an orientation is associated with each detected interest point. As we will see in the next chapter, this information will be useful to align the descriptors of  keypoints detected in different images. In the <span class="strong"><em class="calibre9">Computing components' shape descriptors</em></span> recipe of <a class="calibre1" title="Chapter 7. Extracting Lines, Contours, and Components" href="part0052_split_000.html#page">Chapter 7</a>, <span class="strong"><em class="calibre9">Extracting Lines, Contours, and Components</em></span>, we introduced the concept of image moments and in particular, we showed you how the centroid of a component can be computed from its first three moments. ORB proposes that we use the orientation of the centroid of a circular neighborhood around the keypoint. Since, FAST keypoints, by definition, always have a decentered centroid, the angle of the line that joins the central point and the centroid will always be well defined.</p><p class="calibre8">The ORB features are detected as follows:</p><div class="informalexample"><pre class="programlisting">  // Construct the ORB feature detector object
  detector = new cv::ORB(200, // total number of keypoints
                        1.2, // scale factor between layers
                   8);  // number of layers in pyramid
  // Detect the ORB features
  detector-&gt;detect(image,keypoints);</pre></div><p class="calibre8">This call produces the following result:</p><div class="mediaobject"><img src="../images/00144.jpeg" alt="The ORB feature-detection algorithm" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As can be seen, since<a id="id725" class="calibre1"/> the keypoints are independently detected on each pyramid layer, the detector tends to repeatedly detect the same feature point at different scales.</p></div></div></div>

<div class="book" title="Detecting FAST features at multiple scales">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec166" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <span class="strong"><em class="calibre9">Describing keypoints with binary features</em></span> recipe in <a class="calibre1" title="Chapter 9. Describing and Matching Interest Points" href="part0063_split_000.html#page">Chapter 9</a>, <span class="strong"><em class="calibre9">Describing and Matching Interest Points</em></span>, explains how simple binary descriptors can be used for efficient robust matching of these features</li><li class="listitem">The article <span class="strong"><em class="calibre9">BRISK: Binary Robust Invariant Scalable Keypoint by </em></span>S. Leutenegger, M. Chli and R. Y. Siegwart in <span class="strong"><em class="calibre9">IEEE International Conference on Computer Vision, pp. 2448--2555, 2011</em></span>, describes the BRISK feature algorithm</li><li class="listitem">The article <span class="strong"><em class="calibre9">ORB: an efficient alternative to SIFT or SURF </em></span>by E. Rublee, V. Rabaud, K. Konolige and G. Bradski in <span class="strong"><em class="calibre9">IEEE International Conference on Computer Vision, pp.2564-2571, 2011</em></span>, describes the ORB feature algorithm</li></ul></div></div></div></body></html>