<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Segmentation and Tracking</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Segmentation and Tracking</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we studied different methods for feature extraction and image classification&#160;using <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>)&#160;to detect objects in an image. Those methods work well in creating a bounding box around the target object. However, if our application requires a precise boundary, called an&#160;<strong>instance</strong>, around the object, we need to apply a different approach.</p>
<p>In this chapter, we will be focusing on object instance detection, which is also termed image segmentation. In the second part of the chapter, we will first see MOSSE tracker with OpenCV see various approaches to tracking objects in a sequence of image&#160;</p>
<p>Segmentation and tracking are, however, not quite interlinked problems, but they depend heavily on the previous approaches of feature extraction and object detection. The application's range is&#160;quite vast, including image editing, image denoising, surveillance, motion capture, and so on.&#160;<span>The chosen methods for segmentation and tracking are suitable for specific applications.</span>&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets and libraries</h1>
                </header>
            
            <article>
                
<p>We will be continuing the use of OpenCV and NumPy for image processing. For deep learning, we will use Keras with the TensorFlow backend. For segmentation, we will be using the&#160;<kbd>Pascal VOC</kbd> dataset. This has annotations for object detection, as well as segmentation. For tracking, we will use the <kbd>MOT16</kbd> dataset, which consists of an annotated sequence of images from video. We will mention how to use the code in the sections where it is used.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Segmentation</h1>
                </header>
            
            <article>
                
<p>Segmentation is often referred to as the clustering of pixels of a similar category. An example is as shown in the following screenshot. Here, we see that inputs are on the left and the segmentation results are on the right. The colors of an object are according to pre-defined object categories. These examples are taken from the&#160;<kbd>Pascal VOC</kbd> dataset:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="302" width="537" src="images/69476ba8-0d38-46cf-83cd-6c583e42c4a1.png"/></div>
<p><span>In the top picture on the left, there are several small aeroplanes in the background and, therefore, we see small pixels colored accordingly in the corresponding image on the right. In the bottom-left picture, there are two pets laying together, therefore, their segmented image on the right has different colors for the pixels belonging to the cat and dog respectively. In this figure, the boundary is differently colored for convenience and does not imply a different category.</span></p>
<p>In traditional segmentation techniques, the key property used is image intensity levels. First, different smaller regions of similar intensity values are found, and later they are merged into larger regions. To get the best performance, an initial point is chosen by the user for algorithms. Recent approaches using deep learning have shown better performance without the need for initialization. In further sections, we will see an extension of previously seen CNNs for image segmentation.</p>
<p><span>Before starting our discussion on segmentation methods, let's look at the challenges.&#160;</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Challenges in segmentation&#160;</h1>
                </header>
            
            <article>
                
<p>The challenges in a segmentation task are greater than the previous object detection task, as the complexity of detection is increased:</p>
<ul>
<li><strong>Noisy boundaries</strong>: Grouping pixels that belong to a category may not be as accurate due to the fuzzy edges of an object. As a result, objects from different categories are clustered together.&#160;</li>
<li><strong>Cluttered scene</strong>: With several objects in the image frame, it becomes harder to classify pixels correctly. With more clutter, the chances of false positive classification also increase.&#160;</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNNs for segmentation</h1>
                </header>
            
            <article>
                
<p>Deep learning based segmentation approaches have recently grown, both in terms of accuracy as well as effectiveness, in more complex domains. One of the popular models using CNN for segmentation is a&#160;<strong>fully convolutional network</strong> (<strong>FCN</strong>)[5], which we will explore in this section. This method has the advantage of training an end-to-end CNN to perform pixel-wise semantic segmentation. The output is an image with each pixel classified as either background or into one of the predefined categories of objects. The overall architecture is shown in the following screenshot:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="240" width="457" class="alignnone size-full wp-image-337 image-border" src="images/49f5436a-4b06-4495-bee9-349d2a6c25cc.png"/></div>
<p>As the layers are stacked hierarchically, the output from each layer gets downsampled yet is feature rich. In the last layer, as shown in the figure, the downsampled output is upsampled using a deconvolutional layer, resulting in the final output being the same size as that of the input.&#160;</p>
<p>The deconvolutional layer is used to transform the input feature to the upsampled feature, however, the name is a bit misleading, as the operation is not exactly the inverse of convolution. This acts as transposed convolution, where the input is convolved after a transpose, as compared to a regular convolution operation.</p>
<p>In the previous model, the upsampling of the feature layer was done with a single layer. This can, however, be extended over to a hierarchical structure, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="254" width="460" class="alignnone size-full wp-image-338 image-border" src="images/29bbcdc6-0f64-4016-9e24-9288c303e85b.png"/></div>
<p>In the preceding screenshot, the feature extractor is kept the same, while upsampling is updated with more deconvolutional layers where each of these layers&#160;upsamples features from the previous layer&#160;and generates an overall richer prediction.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of FCN</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn to model one of the basic segmentation models in Keras.</p>
<p>Let's begin by importing Keras required modules:</p>
<pre><strong>from keras.models import *</strong><br/><strong>from keras.layers import *</strong><br/><strong>from keras.applications.vgg16 import VGG16</strong></pre>
<p>The following code will create an FCN model, which takes in VGG16 features as input and adds further layers for fine tuning them. These are then upsampled to give resulting output:</p>
<pre><strong>def create_model_fcn32(nb_class, input_w=256):</strong><br/><strong>    """</strong><br/><strong>    Create FCN-32s model for segmentaiton. </strong><br/><strong>    Input:</strong><br/><strong>        nb_class: number of detection categories</strong><br/><strong>        input_w: input width, using square image</strong><br/><br/><strong>    Returns model created for training. </strong><br/><strong>    """</strong><br/><strong>    input = Input(shape=(input_w, input_w, 3))</strong><br/><br/><strong>    # initialize feature extractor excuding fully connected layers</strong><br/><strong>    # here we use VGG model, with pre-trained weights. </strong><br/><strong>    vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input)</strong><br/><strong>    # create further network</strong><br/><strong>    x = Conv2D(4096, kernel_size=(7,7), use_bias=False,</strong><br/><strong>               activation='relu', padding="same")(vgg.output)</strong><br/><strong>    x = Dropout(0.5)(x)</strong><br/><strong>    x = Conv2D(4096, kernel_size=(1,1), use_bias=False,</strong><br/><strong>               activation='relu', padding="same")(x)</strong><br/><strong>    x = Dropout(0.5)(x)</strong><br/><strong>    x = Conv2D(nb_class, kernel_size=(1,1), use_bias=False, </strong><br/><strong>               padding="same")(x)</strong><br/><strong>    # upsampling to image size using transposed convolution layer</strong><br/><strong>    x = Conv2DTranspose(nb_class , </strong><br/><strong>                        kernel_size=(64,64), </strong><br/><strong>                        strides=(32,32), </strong><br/><strong>                        use_bias=False, padding='same')(x)</strong><br/><strong>    x = Activation('softmax')(x)</strong><br/><strong>    model = Model(input, x)</strong><br/><strong>    model.summary()</strong><br/><strong>    return model</strong><br/><br/><strong># Create model for pascal voc image segmentation for 21 classes</strong><br/><strong>model = create_model_fcn32(21)</strong></pre>
<p>In this section, we saw segmentation methods to compute object precise region in an image. The FCN method shown here uses only convolutional layers to compute these regions. The <kbd>upsampling</kbd> method is key to compute pixel-wise categories and hence different choices of upsampling methods will result in a different quality of results.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tracking</h1>
                </header>
            
            <article>
                
<p>Tracking is the problem of estimating the position of an object over consecutive image sequences. This is also further divided into single object tracking and multiple object tracking, however, both single and multi-object tracking require slightly different approaches. In this section, we will see the methods for multi-object tracking, as well as single-object tracking.&#160;</p>
<p>The methods for image-based tracking are used in several applications, such as action recognition, self-driving cars, security and surveillance, augmented reality apps, motion capture systems, and video compression techniques. In <strong>Augmented Reality</strong> (<strong>AR</strong>) apps, for example, if we want to draw a virtual three-dimensional object on a planar surface, we would want to keep track of the planar surface for a feasible output.</p>
<p>In surveillance or traffic monitoring, tracking vehicles and keeping records of number plates helps to manage traffic and keeps security in check. Also, in video compression applications, if we already know that a single object is the only thing changing in frames, we can perform better compression by using only those pixels that change, thereby optimizing video transmission and receiving.&#160;</p>
<p>In the setup of tracking, we will first see challenges in the next section.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Challenges in tracking</h1>
                </header>
            
            <article>
                
<p>It is always crucial to know which challenges we need to take care of before building apps. As a standard computer vision method, a lot of the challenges here are common:</p>
<ul>
<li><strong>Object occlusion</strong>: If the target object is hidden behind other objects in a sequence of images, then it becomes not only hard to detect the object but also to update future images if it becomes visible again.&#160;</li>
<li><strong>Fast movement</strong>: Cameras, such as on smartphones, often suffers from jittery movement. This causes a blurring effect and, sometimes, the complete absence of an object from the frame. Therefore, sudden changes in the motion of cameras also lead to problems in tracking applications.</li>
<li><strong>Change of shape</strong>: If we are targeting non-rigid objects, changes in shape or the complete deformation of an object will often lead to being unable to detect the object and also tracking failure.</li>
<li><strong>False positives</strong>: In a scene with multiple similar objects, it is hard to match which object is targeted in subsequent images. The tracker may lose the current object in terms of detection and start tracking a similar object.&#160;</li>
</ul>
<p>These challenges can make our applications crash suddenly or give a completely incorrect estimate of an object's location.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Methods for object tracking</h1>
                </header>
            
            <article>
                
<p>An intuitive method for tracking is to use the object detection method from the previous chapter and compute detection in each frame. This will result in a bounding box detection for every frame, but we would also like to know if a particular object stays in the image sequence and for how many frames, that is, to keep track of K-frames for the object in the scene. We would also need a matching strategy to say that the object found in the previous image is the same as the one in the current image frame.&#160;</p>
<p>Continuing with this intuition, we add a predictor for the bounding box motion. We assume a state for the bounding box, which consists of coordinates for the box center as well as its velocities. This state changes as we see more boxes in the sequence.</p>
<p>Given the current state of the box, we can predict a possible region for where it will be in the next frame by assuming some noise in our measurement. The object detector can search <span>for an object similar to the previous object</span> in the next possible region. The location of the newly found object box and the previous box state will help us to update the new state of the box. This will be used for the next frame. As a result, iterating this process over all of the frames will result in not only the tracking of the object bounding box but keeping a location check on particular objects over the whole sequence. This method of tracking is also termed as <strong>tracking by detection.&#160;</strong></p>
<p>In tracking by detection, each frame uses an object detector to find possible instances of objects and matches those detections with corresponding objects in the previous frame.&#160;</p>
<p>On the other hand, if no object detector is to be used, we can initialize the target object and track it by matching it and finding&#160;a similar object in each frame.&#160;</p>
<p>In the following section, we will see two popular methods for tracking. The first method is quite fast, yet simple, while the latter is quite accurate, even in the case of multiple-object tracking.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MOSSE tracker</h1>
                </header>
            
            <article>
                
<p>This is proposed by for fast object tracking using correlation filter methods. Correlation filter-based tracking comprises the following steps:</p>
<ol>
<li>Assuming a template of a target object <em>T</em> and an input image <em>I</em>,&#160;we first take the&#160;<strong>Fast Fourier Transform</strong> (<strong>FFT</strong>) of both the template (<em>T</em>) and the image (<em>I</em>).</li>
<li>A convolution operation is performed between template <em>T</em> and image <em>I</em>.&#160;</li>
<li>The result from step 2 is inverted to the spatial domain using <strong>Inverse Fast Fourier&#160;Transform</strong> (<strong>IFFT</strong>). The position of the template object in the image <em>I</em> is the max value of the IFFT response we get.&#160;</li>
</ol>
<p>This correlation filter-based technique has limitations in the choice of <em>T</em>. As a single template image match may not observe all the variations of an object, such as rotation in the image sequence, Bolme, and its co-authors[1] proposed a more robust tracker-based correlation filter, termed as&#160;<strong>Minimum Output Sum of Squared Error</strong> (<strong>MOSSE</strong>) filter. In this method, the template <em>T</em> for matching is first learned by minimizing a sum of squared error as:</p>
<p style="padding-left: 150px"><img height="40" width="169" class="alignnone size-full wp-image-339 image-border" src="images/a0ceec79-6109-42d4-b7e0-e1f27ddcebaa.png"/></p>
<p>Here, <em>i</em> is the training samples and the resulting learned template is <em>T*</em>.</p>
<div class="packt_infobox">We will see the implementation of MOSSE tracker from OpenCV, as it already has good implementation here:&#160;<a href="https://github.com/opencv/opencv/blob/master/samples/python/mosse.py">https://github.com/opencv/opencv/blob/master/samples/python/mosse.py</a></div>
<p>We will look at the key parts of the following code:</p>
<pre><strong>    def correlate(self, img):</strong><br/><strong>        """</strong><br/><strong>        Correlation of input image with the kernel</strong><br/><strong>        """</strong><br/><br/><strong>        # get response in fourier domain</strong><br/><strong>        C = cv2.mulSpectrums(cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT), </strong><br/><strong>                            self.H, 0, conjB=True)</strong><br/><br/><strong>        # compute inverse to get image domain output</strong><br/><strong>        resp = cv2.idft(C, flags=cv2.DFT_SCALE | cv2.DFT_REAL_OUTPUT)</strong><br/>       <br/><strong>        # max location of the response</strong><br/><strong>        h, w = resp.shape</strong><br/><strong>        _, mval, _, (mx, my) = cv2.minMaxLoc(resp)</strong><br/><strong>        side_resp = resp.copy()</strong><br/><strong>        cv2.rectangle(side_resp, (mx-5, my-5), (mx+5, my+5), 0, -1)</strong><br/><strong>        smean, sstd = side_resp.mean(), side_resp.std()</strong><br/><strong>        psr = (mval-smean) / (sstd+eps)</strong><br/><br/><strong>        # displacement of max location from center is displacement for  <br/>        tracker</strong><br/><strong>        return resp, (mx-w//2, my-h//2), psr</strong></pre>
<p>The <kbd>update</kbd> function gets a frame from video or image sequence iteratively and updates the state of the tracker:</p>
<pre><strong>def update(self, frame, rate = 0.125):</strong><br/><strong>        # compute current state and window size</strong><br/><strong>        (x, y), (w, h) = self.pos, self.size</strong><br/>        <br/><strong>        # compute and update rectangular area from new frame</strong><br/><strong>        self.last_img = img = cv2.getRectSubPix(frame, (w, h), (x, y))</strong><br/>        <br/><strong>        # pre-process it by normalization</strong><br/><strong>        img = self.preprocess(img)</strong><br/>        <br/><strong>        # apply correlation and compute displacement</strong><br/><strong>        self.last_resp, (dx, dy), self.psr = self.correlate(img)</strong><br/>        <br/>  <br/><strong>        self.good = self.psr &gt; 8.0</strong><br/><strong>        if not self.good:</strong><br/><strong>            return</strong><br/><br/><strong>        # update pos</strong><br/><strong>        self.pos = x+dx, y+dy</strong><br/><strong>        self.last_img = img = cv2.getRectSubPix(frame, (w, h), self.pos)</strong><br/><strong>        img = self.preprocess(img)</strong><br/><br/>        <br/><strong>        A = cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT)</strong><br/><strong>        H1 = cv2.mulSpectrums(self.G, A, 0, conjB=True)</strong><br/><strong>        H2 = cv2.mulSpectrums( A, A, 0, conjB=True)</strong><br/>        <br/><strong>        self.H1 = self.H1 * (1.0-rate) + H1 * rate</strong><br/><strong>        self.H2 = self.H2 * (1.0-rate) + H2 * rate</strong><br/><strong>        self.update_kernel()</strong></pre>
<p>A major advantage of using the MOSSE filter is that it is quite fast for real-time tracking systems. The overall algorithm is simple to implement and can be used in the hardware without special image processing libraries, such as embedded platforms. There have been several modifications to this filter and, as such, readers are requested to explore more about these filters.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep SORT</h1>
                </header>
            
            <article>
                
<p>Previously, we looked at one of the simplest trackers. In this section, we will use richer features from CNNs to perform tracking. <strong>Deep SORT</strong>[2] is a recent algorithm for tracking that extends <strong>Simple Online and Real-time Tracking</strong>[3] and has shown remarkable results in the&#160;<strong>Multiple Object Tracking</strong> (<strong>MOT</strong>) problem.</p>
<p>In the problem setting of MOT, each frame has more than one object to track. A generic method to solve this has two steps:</p>
<ul>
<li><strong>Detection</strong>: First, all the objects are detected in the frame. There can be single or multiple detections.&#160;</li>
<li><strong>Association</strong>: Once we have detections for the frame, a matching is performed for similar detections with respect to the previous frame. The matched frames are followed through the sequence to get the tracking for an object.&#160;</li>
</ul>
<p>In Deep SORT, this generic method is further divided into three steps:</p>
<ol>
<li>To compute detections, a popular CNN-based object detection method is used. In the paper[2], Faster-RCNN[4] is used to perform the initial detection per frame. As explained in the previous chapter, this method is two-stage object detection, which performs well for object detection, even in cases of object transformations and occlusions.</li>
<li>The intermediate step before data association consists of an estimation model. This uses the state of each track as a vector of eight quantities, that is, box center&#160;(<em>x</em>,&#160;<em>y</em>), box scale (<em>s</em>), box aspect ratio (<em>a</em>), and their derivatives with time as velocities. The Kalman filter is used to model these states as a dynamical system. If there is no detection of a tracking object for a threshold of consecutive frames, it is considered to be out of frame or lost. For a newly detected box, the new track is started.&#160;</li>
</ol>
<ol start="3">
<li>In the final step, given the predicted states from Kalman filtering using the previous information and the newly detected box in the current frame, an association is made for the new detection with old object tracks in the previous&#160;frame. This is computed using Hungarian algorithm on bipartite graph matching. This is made even more robust by setting the weights of the matching with distance formulation.&#160;</li>
</ol>
<p>This is further explained in the following diagram. The tracker uses a vector of states to store the historical information for previous detections. If a new frame comes, we can either use pre-stores bounding box detections or compute them using object detection methods discussed in<span>&#160;</span><em>chapter 6.</em><span>&#160; Finally, using current observation of bounding box detections and previous states, the current tracking is estimated:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="255" width="455" src="images/52fec427-9359-4a8c-92ca-d2e9ea01434e.png"/></div>
<p>We will see an effective demo of Deep SORT using its official repository at&#160;&#160;<a href="https://github.com/nwojke/deep_sort">https://github.com/nwojke/deep_sort</a></p>
<p>At first, clone the following repository:</p>
<pre><strong>git clone https://github.com/nwojke/deep_sort.git</strong></pre>
<p>Since we already have TensorFlow and Keras installed, we will not be going through their installation. As we saw previously, it uses CNN-based object detection for initial detection. We can run the network and get detection or use pre-generated detections. To do so, let's get pre-trained models here in the&#160;<kbd>deep_sort</kbd> folder:</p>
<ul>
<li>On macOS (if <kbd>wget</kbd> is not available):</li>
</ul>
<pre style="padding-left: 60px"><strong>curl -O https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8/download</strong></pre>
<ul>
<li>On Linux:</li>
</ul>
<pre style="padding-left: 60px"><strong>wget https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8/download</strong></pre>
<p>These downloaded files consist of pre-detected boxes using CNN-based models for the <kbd>MOT challenge</kbd> dataset CC BY-NC-SA 3.0. We need one more thing to use the downloaded model, that is, a dataset on which these detections were created. Let's get the dataset from&#160;<a href="https://motchallenge.net/data/MOT16.zip" target="_blank">https://motchallenge.net/data/MOT16.zip</a>:<a href="https://motchallenge.net/data/MOT16.zip" target="_blank"></a></p>
<ul>
<li>On macOS:</li>
</ul>
<pre style="padding-left: 60px"><strong>curl -O https://motchallenge.net/data/MOT16.zip</strong></pre>
<ul>
<li>On Linux:</li>
</ul>
<pre style="padding-left: 60px"><strong>wget https://motchallenge.net/data/MOT16.zip</strong></pre>
<p>Now that we have finished setting up the code structure, we can run a demo:&#160;</p>
<pre><strong>python deep_sort_app.py \</strong><br/><strong>    --sequence_dir=./MOT16/test/MOT16-06 \</strong><br/><strong>    --detection_file=./deep_sort_data/resources/detections/MOT16_POI_test/MOT16-06.npy \</strong><br/><strong>    --min_confidence=0.3 \</strong><br/><strong>    --display=True</strong></pre>
<p>In this case:</p>
<ul>
<li><kbd>--sequence_dir</kbd> is the path to the MOT challenge test image sequence</li>
<li><kbd>--detection_file</kbd> is our downloaded pre-generated detection corresponding to the sequence directory&#160;we chose previously</li>
<li><kbd>--min_confidence</kbd> is the threshold to filter any detection less than this value</li>
</ul>
<p>For test sequence MOT16-06, we can see the window which shows video output frame-by-frame. Each frame consists of the bounding box around person tracked and the number is the ID of the person being tracked. The number updates if a new person is detected and follows until the tracking stops.&#160; In the following figure, a sample output is explained from the tracking window. For ease of explanation, background image is not shown and only tracking boxes are shown:</p>
<div class="CDPAlignCenter CDPAlign"><img height="328" width="582" src="images/1e15452e-1e3e-4fa2-a7bc-d508958426fb.png"/></div>
<p>Readers are encourages to run other test sequences too, like&#160;MOT16-07, to further understand effectiveness of the model with varying environments.&#160;</p>
<p>In this section, we saw a demo of the Deep SORT method for MOT. One of the crucial parts of this method is detection and the use of Faster RCNN as a good detector. However, to increase the speed of the overall algorithm, Faster RCNN can also be replaced by other fast object detectors such as the Single Shot detector, because the rest of the method uses&#160;detected box states and not on the feature extraction method and features itself.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, two different computer vision problems were shown. In segmentation, both the pixel level as well as convolutional neural net-based methods were shown. FCN shows the effectiveness of segmenting an image using the feature extraction method and, as a result, several current applications can be based on it. In track, two different approaches were discussed. Tracking by detection and tracking by matching can both be used for applications to track objects in the video. MOSSE tracker is a simple tracker for fast-paced applications and can be implemented on small computing devices. The Deep SORT method explained in this chapter can be used for multi-object tracking that uses deep CNN object detectors.</p>
<p>In the next chapter, we will begin with another branch of computer vision that focuses on understanding geometry of the scene explicitly. We will see methods to compute camera position and track its trajectory using only images.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Bolme David S. J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. <em>Visual object tracking using adaptive correlation filters</em>. In&#160;<span>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</span><span>, pp. 2544-2550. IEEE, 2010.</span></li>
<li><span>Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. <em>Simple Online and Realtime Tracking with a Deep Association Metric</em>.&#160;</span><span>arXiv preprint arXiv:1703.07402</span><span><span>&#160;(2017).</span></span></li>
<li><span>Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. <em>Simple online and realtime tracking</em>. In&#160;</span>Image Processing (ICIP), 2016 IEEE International Conference on<span>, pp. 3464-3468. IEEE, 2016.</span></li>
<li><span>Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. <em>Faster R-CNN: Towards real-time object detection with region proposal networks</em>. In&#160;</span>Advances in neural information processing systems<span>, pp. 91-99. 2015.</span></li>
<li><span>Long, Jonathan, Evan Shelhamer, and Trevor Darrell. <em>Fully convolutional networks for semantic segmentation</em>. In&#160;</span>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition<span>, pp. 3431-3440. 2015.</span></li>
</ul>
<p>&#160;</p>
<p>&#160;</p>


            </article>

            
        </section>
    </div>
</body>
</html>