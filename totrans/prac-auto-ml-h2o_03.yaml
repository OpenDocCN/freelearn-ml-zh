- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Understanding Data Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据处理
- en: A **Machine Learning** (**ML**) model is the output we get once data is fitted
    into an ML algorithm. It represents the underlying relationship between various
    features and how that relationship impacts the target variable. This relationship
    depends entirely on the contents of the dataset. What makes every ML model unique,
    despite using the same ML algorithm, is the dataset that is used to train said
    model. Data can be collected from various sources and can have different schemas
    and structures, which need not be structurally compatible among themselves but
    may in fact be related to each other. This relationship can be very valuable and
    can also potentially be the differentiator between a good and a bad model. Thus,
    it is important to transform this data to meet the requirements of the ML algorithm
    to eventually train a good model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）模型是在数据拟合到ML算法后得到的输出。它代表了各种特征之间的潜在关系以及这种关系如何影响目标变量。这种关系完全取决于数据集的内容。尽管使用相同的ML算法，但每个ML模型都是独特的，这是因为用于训练该模型的特定数据集。数据可以从各种来源收集，并且可以具有不同的模式和结构，它们之间可能不需要结构上兼容，但实际上可能相互关联。这种关系可能非常有价值，也可能潜在地成为好模型和坏模型之间的区别。因此，将数据转换为满足ML算法的要求，最终训练出一个好模型是很重要的。'
- en: '**Data processing**, data preparation, and data preprocessing are all steps
    in the ML pipeline that focus on best exposing the underlying relationship between
    the features by transforming the structure of the data. Data processing may be
    the most challenging step in the ML pipeline, as there are no set steps to the
    transformation process. Data processing depends entirely on the problem you wish
    to solve; however, there are some similarities among all datasets that can help
    us define certain processes that we can perform to optimize our ML pipeline.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据处理**、数据准备和数据预处理都是ML管道中的步骤，它们通过转换数据结构来最佳地暴露特征之间的潜在关系。数据处理可能是ML管道中最具挑战性的步骤，因为转换过程没有固定的步骤。数据处理完全取决于您希望解决的问题；然而，所有数据集之间都有一些相似之处，这可以帮助我们定义可以执行以优化ML管道的某些过程。'
- en: In this chapter, we will learn about some of the common functionalities that
    are often used in data processing and how H2O has in-built operations that can
    help us easily perform them. We will understand some of the H2O operations that
    can reframe the structure of our dataframe. We will understand how to handle missing
    values and the importance of the imputation of values. We will then investigate
    how we can manipulate the various feature columns in the dataframe, as well as
    how to slice the dataframe for different needs. We shall also investigate what
    encoding is and what the different types of encoding are.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解一些在数据处理中经常使用的常见功能，以及H2O内置的操作如何帮助我们轻松地执行它们。我们将了解一些可以重构我们数据框结构的H2O操作。我们将了解如何处理缺失值以及值插补的重要性。然后，我们将研究如何操作数据框中的各种特征列，以及如何根据不同的需求切片数据框。我们还将研究编码是什么以及不同的编码类型。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Reframing your dataframe
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构你的数据框
- en: Handling missing values in the dataframe
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据框中的缺失值
- en: Manipulation of feature columns of the dataframe
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作数据框的特征列
- en: Tokenization of textual data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据的分词
- en: Encoding of data using target encoding
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目标编码对数据进行编码
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All code examples in this chapter are run on **Jupyter Notebook** for an easy
    understanding of what each line in the code block does. You can run the whole
    block of code via a Python or R script executor and observe the output results,
    or you can follow along by installing Jupyter Notebook and observing the execution
    results of every line in the code blocks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例都是在**Jupyter Notebook**上运行的，以便于理解代码块中每一行的操作。您可以通过Python或R脚本执行器运行整个代码块并观察输出结果，或者您可以通过安装Jupyter
    Notebook并观察代码块中每一行的执行结果来跟随操作。
- en: 'To install Jupyter Notebook, make sure you have the latest version of Python
    and `pip` installed on your system and execute the following command:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Jupyter Notebook，请确保您的系统上安装了最新版本的Python和`pip`，并执行以下命令：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once JupyterLab has successfully installed, you can start your Jupyter Notebook
    locally by executing the following command in your terminal:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦JupyterLab成功安装，您可以通过在终端执行以下命令来在本地启动Jupyter Notebook：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will open the **Jupyter Notebook** page on your default browser. You can
    then select which language you want to use and start executing the lines in the
    code step by step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: All code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%203](https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%203).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s begin processing our data by first creating a dataframe and reframing
    it so that it meets our model training requirement.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Reframing your dataframe
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data collected from various sources is often termed **raw data**. It is called
    raw in the sense that there might be a lot of unnecessary or stale data, which
    might not necessarily benefit our model training. The structure of the data collected
    also might not be consistent among all the sources. Hence, it becomes very important
    to first reframe the data from various sources into a consistent format.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that once we import the dataset into H2O, H2O converts
    the dataset into a `.hex` file, also called a dataframe. You have the option to
    import multiple datasets as well. Assuming you are importing multiple datasets
    from various sources, each with its own format and structure, then you will need
    a certain functionality that helps you reframe the contents of the dataset and
    merge them to form a single dataframe that you can feed to your ML pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: H2O provides several functionalities that you can use to perform the required
    manipulations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the dataframe manipulation functionalities that help you reframe
    your dataframe:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Combining columns from two dataframes
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining rows from two dataframes
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging two dataframes
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how we can combine columns from different dataframes in H2O.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Combining columns from two dataframes
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common dataframe manipulation functionalities is combining different
    columns from different dataframes. Sometimes, the columns of one dataframe may
    be related to those of another. This could prove beneficial during model training.
    Thus, it is quite useful to have a functionality that can help us manipulate these
    columns and combine them together to form a single dataframe for model training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: H2O has a function called `cbind()` that combines the columns from one dataset
    into another.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this function out in our Jupyter Notebook using Python. Execute the
    following steps in sequence:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Import the `numpy` library; we will use this to create a sample dataframe for
    our study:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the `h2o` server:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s create a dataframe called `important_dataframe_1`; this is a dataframe
    whose columns are important. To ensure that you generate the same values in the
    dataset as in this example, set the random seed value for `numpy` to `123`. We
    will set the number of rows to `15` and the number of columns to `5`. You can
    name the columns anything you like:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s check out the content of the dataset by executing the following code:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows you the contents of the dataset:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – important_dataframe_1 data content ](img/B17298_03_001.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – important_dataframe_1 data content
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create another dataframe called `important_dataframe_2`, as before but
    with different column names, but an equal number of rows and only `2` columns:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s check out the content of this dataframe as well:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – important_dataframe_2 data content ](img/B17298_03_002.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – important_dataframe_2 data content
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s combine the columns of both the dataframes and store them in another
    variable called `final_dataframe`, using the `cbind()` function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s now observe `final_dataframe`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should see the contents of **final_dataframe** as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – final_dataframe data content after cbind() ](img/B17298_03_003.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – final_dataframe data content after cbind()
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Here, you will notice that we have successfully combined the columns from `important_dataframe_2`
    with the columns of **important_dataframe_1**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: This is how you can use the `cbind()` function to combine the columns of two
    different datasets into a single dataframe. The only thing to bear in mind while
    using the `cbind()` function is that it is necessary to ensure that both the datasets
    to be combined have the same number of rows. Also, if you have dataframes with
    the same column name, then H2O will append a **0** in front of the column from
    dataframe.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to combine the columns of different dataframes, let’s see
    how we can combine the column values of multiple dataframes with the same column
    structure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Combining rows from two dataframes
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The majority of big corporations often handle tremendous amounts of data. This
    data is often partitioned into multiple chunks to make storing and reading it
    faster and more efficient. However, during model training, we will often need
    to access all these partitioned datasets. These datasets have the same structure
    but the data contents are distributed. In other words, the dataframes have the
    same columns; however, the data values or rows are split among them. We will often
    need a function that combines all these dataframes together so that we have all
    the data values available for model training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: H2O has a function called `rbind()` that combines the rows from one dataset
    into another.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this function out in the following example:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Import the `numpy` library; we will use this to create a random dataframe for
    our study:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize the `h2o` server:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let’s create a random dataframe called `important_dataframe_1`. To ensure
    that you generate the same values in the dataset as in this example, set the random
    seed value for `numpy` to `123`. We will set the number of rows to `15` and the
    number of columns to `5`. You can name the columns anything you like:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s check out the number of rows of the dataframe, which should be `15`:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s create another dataframe called `important_dataframe_2`, as with the
    previous one, with the same column names and any number of rows. In the example,
    I have used `10` rows:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s check out the number of rows for `important_dataframe_2`, which should
    be `10`:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s combine the rows of both the dataframes and store them in another
    variable called `final_dataframe`, using the `rbind()` function:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s now observe `final_dataframe`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should see the contents of **final_dataframe** as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – final_dataframe data contents after rbind() ](img/B17298_03_004.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – final_dataframe data contents after rbind()
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out the number of rows in **final_dataframe**:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output of the last operation should show you the value of the number of
    rows in the final dataset. You will see that the value is **25** and the contents
    of the dataframe are the combined row values of both the previous datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how to combine the rows of two dataframes in H2O
    using the `rbind()` function, let’s see how we can fully combine two datasets.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Merging two dataframes
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can directly merge two dataframes, combining their rows and columns into
    a single dataframe. H2O provides a `merge()` function that combines two datasets
    that share a common column or common columns. During merging, columns that the
    two datasets have in common are used as the **merge key**. If they only have one
    column in common, then that column forms the singular primary key for the merge.
    If there are multiple common columns, then H2O will form a complex key of all
    these columns based on their data values and use that as the merge key. If there
    are multiple common columns between the two datasets and you only wish to merge
    a specific subset of them, then you will need to rename the other common columns
    to remove the corresponding commonality.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this function out in the following example in Python:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Import the `numpy` library; we will use this to create a random dataframe for
    our study:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Initialize the `h2o` server:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let’s create a dataframe called `dataframe_1`. The dataframe has `3` columns:
    `words`, `numerical_representation`, and `letters`. Now, let’s fill in the data
    content as follows:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s check out the content of the dataset:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will notice the contents of the dataset as follows:![Figure 3.5 – dataframe_1
    data content ](img/B17298_03_005.jpg)
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.5 – dataframe_1 data content
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create another dataframe called `dataframe_2`. This dataframe also contains
    `3` columns: the `numerical_representation` column, the `letters` column (both
    of which it has in common with `dataframe_1`), and an uncommon column. Let’s call
    it `other_words`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s check out the content of this dataframe as well:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'On executing the code, you should see the following output in your notebook:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – dataframe_2 data contents ](img/B17298_03_006.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – dataframe_2 data contents
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s merge `dataframe_1` into `dataframe_2`, using the `merge()` operation:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s now observe `final_dataframe`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You should see the contents of **final_dataframe** as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – final_dataframe contents after merge() ](img/B17298_03_007.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – final_dataframe contents after merge()
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that H2O used the combination of the `numerical_representation`
    column with the appropriate values in the other columns.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you may be wondering why there is no row for **4**. That is because while
    merging, we have two common columns: **numerical_representation** and **letters**.
    So, H2O used a complex merging key that uses both these columns: **(0, a)**, **(1,
    b)**, **(2, c)**, and so on.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Now the next question you might have is *What about the row with the value 5?
    It has no value in the letters column.* That is because even an empty value is
    treated as a unique value in ML. Thus, during merging, the complex key that was
    generated treated **(5, )** as a valid merge key.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: H2O drops all the remaining values since **dataframe_1** does not have any more
    numerical representation values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'You can enforce H2O to not drop any of the values from the merge key column
    by setting the `all_x` parameter to `True` as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let’s observe the contents of `describe` attribute:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – final_dataframe data content after enforcing merge() ](img/B17298_03_008.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – final_dataframe data content after enforcing merge()
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that we now have all the values from both dataframes merged
    into a single dataframe. We have all the numerical representations from **0 to
    9** and all letters from **a to e** from **dataframe_2** that were missing in
    the previous step, along with the correct values from the **other_words** column
    and the **words** column.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we learned how to combine dataframe columns and rows. We also learned
    how to combine entire dataframes together using the `merge()` function. However,
    we noticed that if we enforced the merging of dataframes despite them not having
    common data values in their key columns, we ended up with missing values in the
    dataframe.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the different methods we can use to handle missing values
    using H2O.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values in the dataframe
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing values in datasets are the most common issue in the real world. It is
    often expected to have at least a few instances of missing data in huge chunks
    of datasets collected from various sources. Data can be missing for several reasons,
    which can range from anything from data not being generated at the source all
    the way to downtimes in data collectors. Handling missing data is very important
    for model training, as many ML algorithms don’t support missing data. Those that
    do may end up giving more importance to looking for patterns in the missing data,
    rather than the actual data that is present, which distracts the machine from
    learning.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Missing data is often referred to as **Not Available** (**NA**) or **nan**.
    Before we can send a dataframe for model training, we need to handle these types
    of values first. You can either drop the entire row that contains any missing
    values or you can fill them with any default value either default or common for
    that data column. How you handle missing values depends entirely on which data
    is missing and how important it is for the overall model training.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O provides some functionalities that you can use to handle missing values
    in a dataframe. These are some of them:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The `fillna()` function
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing values in a frame
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputation
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s see how we can fill missing values in a dataframe using H2O.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Filling NA values
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`fillna()` is a function in H2O that you can use to fill missing data values
    in a sequential manner. This is especially handy if you have certain data values
    in a column that are sequential in nature, for example, time series or any metric
    that increases or decreases sequentially and can be sorted. The smaller the difference
    between the values in the sequence, the more applicable this function becomes.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fillna()` function has the following parameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '`method`: This can either be *forward* or *backward*. It indicates the direction
    in which H2O should start filling the NA values in the dataframe.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`axis`: `0` for column-wise fill or `1` for row-wise fill.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxlen`: The maximum number of consecutive NAs to fill.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see an example in Python of how we can use this function to fill missing
    values:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Import the `numpy` library; we will use this to create a random dataframe for
    our study:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Initialize the `h2o` server:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a random dataframe with `1000` rows, `3` columns, and some NA values:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s observe the contents of this dataframe. Execute the following code and
    you will see certain missing values in the dataframe:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should see the contents of the dataframe as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Dataframe contents ](img/B17298_03_009.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Dataframe contents
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now use the `fillna()` function to forward fill the NA values. Execute
    the following code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s observe the filled contents of the dataframe. Execute the following code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should see the contents of the dataframe as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – filled_dataframe contents ](img/B17298_03_010.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – filled_dataframe contents
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The `fillna()` function has filled most of the NA values in the dataframe sequentially.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: However, you will notice that we still have some `NA`. Since this is the very
    first column, H2O does not have any previous value in the record to fill it, thus
    it skips over it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how we can sequentially fill data in a dataframe using
    the `fillna()` function in H2O, let’s see how we can replace certain values in
    the dataframe.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Replacing values in a frame
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common functionality often needed for data processing is replacing certain
    values in the dataframe. There can be plenty of reasons why you might want to
    do this. This is especially common for numerical data where some of the most common
    transformations include rounding off values, normalizing numerical ranges, or
    just correcting a data value. In this section, we will explore some of the functions
    that we can use in H2O to replace values in the dataframe.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create a dataframe that we can use to test out such functions.
    Execute the following code so that we have a dataframe ready for manipulation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The dataframe should look as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Dataframe data contents ](img/B17298_03_011.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Dataframe data contents
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have a dataframe with three columns: **C1**, **C2**, and **C3**. Each
    column has a few negative numbers and some **nan** values. Let’s see how we can
    play around with this dataframe.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with something simple. Let’s update the value of a single data
    value, also called a `99`. You can update the value of a single data value based
    on its position in the dataframe as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Note that the columns and rows in the dataframe all start with `0`. Hence,
    we set the value in the dataframe with the row number of `3` and the column number
    of `1` as `99`. You should see the results in the dataframe by executing `dataframe.describe`
    as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The dataframe should look as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Dataframe contents after the datum update ](img/B17298_03_012.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Dataframe contents after the datum update
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the dataframe, we replaced the **nan** value that was previously
    in the third row of the **C2** column with **99**.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a manipulation of just one data value. Let’s see how we can replace
    the values of an entire column. Let’s increase the data values in the **C3** column
    to three times their original value. You can do so by executing the following
    code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You should see the results in the dataframe by executing `dataframe.describe`
    as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The dataframe should look as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Dataframe contents after column value updates ](img/B17298_03_013.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Dataframe contents after column value updates
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the output that the values in the **C3** column have now been
    increased to three times the original values in the column.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'All these replacements we performed till now are straightforward. Let’s try
    some conditional updates on the dataframe. Let’s round off all the negative numbers
    in the dataframe to `0`. So, the condition is that we only update the negative
    numbers to `0` and don’t change any of the positive numbers. You can do conditional
    updates as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You should see the results in the dataframe by executing `dataframe.describe`
    as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The dataframe should look as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Dataframe contents after conditional updates ](img/B17298_03_014.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Dataframe contents after conditional updates
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the dataframe, all the negative values have been rounded up/replaced
    by **0**.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what if instead of rounding the negative numbers up to **0** we wished
    to just inverse the negative numbers? We could do so by combining the conditional
    updates with arithmetic updates. Refer to the following example:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let’s try to see whether we can replace the remaining `fillna()` function,
    but what if the **nan** values are nothing but some missing values that don’t
    exactly fall into any incremental or decremental pattern, and we just want to
    set it to 0? Let’s do that now. Run the following code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You should see the results in the dataframe by executing `dataframe.describe`
    as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The dataframe should look as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Dataframe contents after replacing nan values with 0 ](img/B17298_03_015.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Dataframe contents after replacing nan values with 0
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The `isna()` function is a function that checks whether the value in the datum
    is **nan** or not and returns either **True** or **False**. We use this condition
    to replace the values in the dataframe.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'There are plenty of ways to manipulate and replace the values in a dataframe
    and H2O provides plenty of functionality to make implementation easy. Feel free
    to explore and experiment more with manipulating the values in the dataframe.
    You can find more details here: [https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/frame.xhtml).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned various methods to replace values in the dataframe,
    let’s look into a more advanced approach to doing so that data scientists and
    engineers often take.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Imputation
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we have seen how we can replace nan values in the dataset using
    `fillna()`, which sequentially replaces the nan data in the dataframe. The `fillna()`
    function fills data in a sequential manner; however, data need not always be sequential
    in nature. For example, consider a dataset of people buying gaming laptops. The
    dataset will mostly contain data about people in the age demographic of 13-28,
    with a few outliers. In such a scenario, if there are any nan values in the `fillna()`
    function to fill the nan values, as any nan value after any outlier value will
    introduce a bias in the dataframe. We need to replace the nan value with a value
    that is common among the standard distribution of the age group for that product,
    something that is between 13 and 28, rather than say 59, which is less likely.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Imputation is the process of replacing certain values in the dataframe with
    an appropriate substitute that does not introduce any bias or outliers that may
    impact model training. The method or formulas used to calculate the substitute
    value are termed the **imputation strategy**. Imputation is one of the most important
    methods of data processing, which handles missing and nan values and tries to
    replace them with a value that will potentially introduce the least bias into
    the model training process.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O has a function called `impute()` that specifically provides this functionality.
    It has the following parameters:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`column`: This parameter accepts the column number that sets the columns to
    `impute()`. The value `1` imputes the entire dataframe.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method`: This parameter sets which method of imputation to use. The methods
    can be either `mean`, `median`, or `mode`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combine_method`: This parameter dictates how to combine the quantiles for
    even samples when the imputation method chosen is `median`. The combination methods
    are either `interpolate`, `average`, `low`, or `high`.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group_by_frame`: This parameter imputes the values of the selected precomputed
    grouped frame.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`by`: This parameter groups the imputation results by the selected columns.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values`: This parameter accepts a list of values that are imputed per column.
    Having the `None` value in the list skips the column.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see an example in Python of how we can use this function to fill missing
    values.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: For this, we shall use the **high school student sprint** dataset. The high
    school student sprint dataset is a dataset that consists of recordings of the
    age of high school students, their weight, maximum recorded speed, and their performance
    in a 100-meter sprint. The dataset is used to predict how the age, weight, and
    sprint speed affect the performance of students in a 100-meter sprint race.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset looks as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – A high school student sprint dataset ](img/B17298_03_016.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – A high school student sprint dataset
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The features of the dataset are as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '**age**: Age of the student'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weight**: Weight of the student in kilograms'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_speed**: The maximum sprint speed of the student in kilometers per hour'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**100_meter_time**: The time taken by the student to finish a 100-meter sprint
    in seconds'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there are plenty of missing values in the **100_meter_time**
    column.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We cannot simply use the `fillna()` function, as that will introduce bias into
    the data if the missing values happen to be right after the fastest or slowest
    time. We can’t simply replace the values with a constant number either.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: What would actually make sense is to replace these missing values with whatever
    is normal for an average teenager doing a 100-meter dash. We already have values
    for the majority of students, so we can use their results to calculate a general
    average 100-meter sprint time and use that as a baseline to replace all the missing
    values without introducing any bias.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what imputation is used for. Let’s use the imputation function
    to fill in these missing values:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h20` module and start the `h20` server:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We then import the `high school student sprint` dataset by using `h2o.import_file()`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Using the `impute()` function, let’s impute the missing values in the `100_meter_time`
    column by `mean` and display the data:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You will see the output of the imputed dataframe as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – 100_meter_time column imputed by its mean ](img/B17298_03_017.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – 100_meter_time column imputed by its mean
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: H2O calculated the `mean` value of all the values in the **100_meter_time**
    column as **23.5558** and replaced the missing values with it.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, instead of `mean`, you can use `median` values as well. However,
    note that if a column has categorical values, then the method must be `mode`.
    The decision is up to you to make, depending on the dataset that is most useful
    when replacing the missing values:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s increase the complexity a bit. What if the average 100-meter sprint time
    is not truly comparable between all students? What if the performances are more
    comparable age-wise? For example, students of age 16 are faster than the ones
    who are 13 since they are more physically developed. In that case, it won’t make
    sense considering a 13-year-old’s sprint time when imputing the missing value
    of a 16-year-old. This is where we can use the `group` parameter of the `impute()`
    function:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You will see the output as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – 100_meter_sprint imputed by its mean and grouped by age ](img/B17298_03_018.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – 100_meter_sprint imputed by its mean and grouped by age
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that now H2O has calculated the `mean` values by age and replaced
    the respective missing values for that age in the `mean` value of all the `impute()`
    function to flexibly impute the correct values.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The `impute()` function is extremely powerful to impute the correct values in
    a dataframe. The additional parameters for grouping via columns as well as frames
    make it very flexible for use in handling all sorts of missing values.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to use and explore all these functions on different datasets. At the
    end of the day, all these functions are just tools used by data scientists and
    engineers to improve the quality of the data; the real skill is understanding
    when and how to use these tools to get the most out of your data, and that requires
    experimentation and practice.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the different ways in which we can handle missing
    data, let’s move on to the next part of data processing, which is how to manipulate
    the feature columns of the dataframe.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating feature columns of the dataframe
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of the time, your data processing activities will mostly involve
    manipulating the columns of the dataframes. Most importantly, the type of values
    in the column and the ordering of the values in the column will play a major role
    in model training.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O provides some functionalities that help you do so. The following are some
    of the functionalities that help you handle missing values in your dataframe:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Sorting of columns
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the type of the column
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first understand how we can sort a column using H2O.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Sorting columns
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ideally, you want the data in a dataframe to be shuffled before passing it off
    to model training. However, there may be certain scenarios where you might want
    to re-order the dataframe based on the values in a column.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O has a functionality called `sort()` to sort dataframes based on the values
    in a column. It has the following parameters:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '`by`: The column to sort by. You can pass multiple column names as a list as
    well.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ascending`: A `boolean` array that denotes the direction in which H2O should
    sort the columns. If `True`, H2O will sort the column in ascending order. If `False`,
    then H2O will sort it in descending order. If neither of the flags is passed,
    then H2O defaults to sorting in ascending order.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way H2O will sort the dataframe depends on whether one column name is passed
    to the `sort()` function or multiple column names. If only a single column name
    is passed, then H2O will return a frame that is sorted by that column.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if multiple columns are passed, then H2O will return a dataframe that
    is sorted as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: H2O will first sort the dataframe on the first column that is passed in the
    parameter.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O will then sort the dataframe on the next column passed in the parameter,
    but only those rows will be sorted that have the same values as in the first sorted
    column. If there are no duplicate values in the previous columns, then no sorting
    will be done on subsequent columns.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see an example in Python of how we can use this function to sort columns:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and initialize it:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Create a dataframe by executing the following code and observe the dataset:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The contents of the dataset should be as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – dataframe_1 data contents ](img/B17298_03_019.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – dataframe_1 data contents
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'So, at the moment, the values in columns `sort()` function to sort the dataframe
    by column `0` into the `by` parameter, indicating the first column of the dataframe,
    or by passing **[‘C1’]**, which is a list containing column names to sequentially
    sort the dataset:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You should get an output of the code as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – dataframe_1 sorted by the C1 column ](img/B17298_03_020.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – dataframe_1 sorted by the C1 column
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: You will see that the dataframe is now sorted in ascending order by the **C1**
    column.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we shall get if we pass multiple columns in the `by` parameter
    to sort on multiple columns. Run the following code line:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You should get an output as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – dataframe_1 sorted by columns C1 and C2 ](img/B17298_03_021.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – dataframe_1 sorted by columns C1 and C2
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, H2O first sorted the columns by the `sort` function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also reverse the sorting order by passing `False` in the `ascending`
    parameter. Let’s test this out by running the following code line:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You should see an output as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17298_03_022.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – dataframe_1 sorted by the C1 column in ascending order and the
    C2 column in descending order
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In this case, H2O first sorted the columns by the **C1** column. Then, it sorted
    the rows by the **C2** column for those rows that had the same value in the **C1**
    column. However, this time it sorted the values in descending order.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned how to sort the dataframe by a single column as well
    as by multiple columns, let’s move on to another column manipulation function
    that changes the type of the column.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Changing column types
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 2*](B17298_02.xhtml#_idTextAnchor038), *Working with
    H2O Flow (H2O’s Web UI)*, we changed the type of the `Heart Disease` column to
    `enum` from `numerical`. The reason we did this is that the type of column plays
    a major role in model training. During model training, the type of column decides
    whether the ML problem is a classification problem or a regression problem. Despite
    the fact that the data in both cases is numerical in nature, how a ML algorithm
    will treat the column depends entirely on its type. Thus, it becomes very important
    to correct the types of columns that might not be correctly set during the initial
    stages of data collection.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: H2O has several functions that not only help you change the type of the columns
    but also run initial checks on the column types.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the functions are as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '`.isnumeric()`: Checks whether the column in the dataframe is of the numeric
    type. Returns `True` or `False` accordingly'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.asnumeric()`: Creates a new frame with all the values converted to numeric
    for the specified column'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.isfactor()`: Checks whether the column in the dataframe is of categorical
    type. Returns `True` or `False` accordingly'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.asfactor()`: Creates a new frame with all the values converted to the categorical
    type for the specified column'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.isstring()`: Checks whether the column in the dataframe is of the string
    type. Returns `True` or `False` accordingly'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ascharacter()`: Creates a new frame with all the values converted to the
    string type for the specified column'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see an example in Python of how we can use these functions to change
    the column types:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and initialize H2O:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Create a dataframe by executing the following code line and observe the dataset:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The contents of the dataset should be as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Dataframe data contents ](img/B17298_03_023.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Dataframe data contents
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s confirm whether the `numerical` column by using the `isnumeric()` function
    as follows:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You should get an output of `True`.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we get if we check whether the `categorical` column using the
    `asfactor()` function as follows:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You should get an output of `False`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s convert the `categorical` column using the `asfactor()` function
    and then check whether `isfactor()` returns `True`:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You should now get an output of `True`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'You can convert the `numerical` column by using the `asnumeric()` function:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: You should now get an output of `True`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to sort the columns of a dataframe and change
    column types, let’s move on to another important topic in data processing, which
    is tokenization and encoding.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization of textual data
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all **Machine Learning Algorithms** (**MLAs**) are focused on mathematical
    problem-solving. **Natural Language Processing** (**NLP**) is a branch of ML that
    specializes in analyzing meaning out of textual data, though it will try to derive
    meaning and understand the contents of a document or any text for that matter.
    Training an NLP model can be very tricky, as every language has its own grammatical
    rules and the interpretation of certain words depends heavily on context. Nevertheless,
    an NLP algorithm often tries its best to train a model that can predict the meaning
    and sentiments of a textual document.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The way to train an NLP algorithm is to first break down the chunk of textual
    data into smaller units called **tokens**. Tokens can be words, characters, or
    even letters. It depends on what the requirements of the MLA are and how it uses
    these tokens to train a model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: H2O has a function called `tokenize()` that helps break down string data in
    a dataframe into tokens and creates a separate column containing all the tokens
    for further processing.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following parameter: `split`: We pass a regular expression in this
    parameter that will be used by the function to split the text data into tokens.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of how we can use this function to tokenize string data
    in a dataframe:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `h2o` library and initialize it:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create a dataframe by executing the following code line and observe the dataset:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The dataset should look as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Dataframe data contents ](img/B17298_03_024.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Dataframe data contents
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: This type of textual data is usually collected in systems that generate a lot
    of log text or conversational data. To solve such NLP tasks, we need to break
    down the sentences into individual tokens so that we can eventually build the
    context and meaning of these texts that will help the ML algorithm to make semantic
    predictions. However, before diving into the complexities of NLP, data scientists
    and engineers will process this data by tokenizing it first.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s tokenize our dataframe using this function to split the text with blank
    spaces and observe the tokenized column:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You should see the dataframe as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Tokenized dataframe data contents ](img/B17298_03_025.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Tokenized dataframe data contents
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the `tokenize()` function splits the text data into tokens
    and appends the tokens as rows into a single column. You will also notice that
    all tokenized sentences are separated by empty rows. You can cross-check this
    by comparing the number of words in all the sentences in the dataframe, plus the
    empty spaces between the sentences against the number of rows in the tokenized
    dataset, using `nrows`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the most used data processing methods that are used to process
    your data before you feed it to your ML pipeline for training. There are still
    plenty of methods and techniques that you can use to further clean and polish
    your dataframes. So much so that you could dedicate an entire book to discussing
    them. Data processing happens to be the most difficult part of the entire ML life
    cycle. The quality of the data used for training depends on the context of the
    problem statement. It also depends on the creativity and ingenuity of the data
    scientists and engineers in processing that data. The end goal of data processing
    is to extract as much information as we can from the dataset and remove noise
    and bias from the data to allow for a more efficient analysis of data during training.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Encoding data using target encoding
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, machines are only capable of understanding numbers. However, plenty
    of real-world ML problems revolve around objects and information that are not
    necessarily numerical in nature. Things such as states, names, and classes, in
    general, are represented as categories rather than numbers. This kind of data
    is called **categorical data**. Categorical data will often play a big part in
    analysis and prediction. Hence, there is a need to convert these categorical values
    to a numerical format so that machines can understand them. The conversion should
    also be in such a way that we do not lose the inherent meaning of those categories,
    nor do we introduce new information into the data, such as the incremental nature
    of numbers, for example.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: This is where encoding is used. **Encoding** is a process where categorical
    values are transformed, in other words, *encoded*, into numerical values. There
    are plenty of encoding methods that can perform this transformation. One of the
    most commonly used ones is **target encoding**.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding is an encoding process that transforms categorical values into
    numerical values by calculating the average probability of the target variable
    occurring for a given category. H2O also has methods that help users implement
    target encoding on their data.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand this method, consider the following sample `Mythical creatures`
    dataset:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Our mythical creatures dataset ](img/B17298_03_026.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Our mythical creatures dataset
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset has the following content:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '**Animals**: This column contains categorical values of the names of animals.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mythical**: This column contains the **0** binary value and the **1** binary
    value. **1** indicates that the creature is mythical, while **0** indicates that
    the creature is not mythical.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s encode the `categorical` column using target encoding. Target encoding
    will perform the following steps:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Group the categorical values and record the number of times the target value,
    **Mythical**, was **1** and when it was **0** for a given category as follows:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.27 – The mythical creatures dataset with a target count ](img/B17298_03_027.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – The mythical creatures dataset with a target count
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the probability that the **1** target value will occur, as compared
    to the **0** target value within each specific group. This would look as follows:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.28 – The mythical creatures dataset with a Probability of Target
    1 Occurring column ](img/B17298_03_028.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – The mythical creatures dataset with a Probability of Target 1
    Occurring column
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'Drop the **Animals** column and use the **Probability of Target 1 Occurring**
    column as the encoded representation of the **Animals** column. The new encoded
    dataset will look as follows:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.29 – A target-encoded mythical creatures dataset ](img/B17298_03_029.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – A target-encoded mythical creatures dataset
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: In the encoded dataset, the **Animals** feature is encoded using target encoding
    and we have a dataset that is entirely numerical in nature. This dataset will
    be easy for an ML algorithm to interpret and learn from, providing high-quality
    models.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now see how we can perform target encoding using H2O. The dataset we
    will use for this example is the `Automobile price prediction` dataset. You can
    find the details of this dataset at [https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)
    (*Dua, D. and Graff, C. (2019). UCI Machine Learning Repository* [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    *Irvine, CA: University of California, School of Information and Computer Science*).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is fairly straightforward. It contains various details about cars,
    such as the **make of the car**, **engine size**, **fuel system**, **compression
    ratio**, and **price**. The aim of the ML algorithm is to predict the price of
    a car based on these features.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: For our experiment, we shall encode the `categorical` columns **make**, **fuel
    type**, and **body style** using target encoding where the **price** column is
    the target.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s perform target encoding by following this example:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `h2o` and H2O’s `H2OTargetEncoderEstimator`, and initialize your H2O
    server. Execute the following code:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Import the `Automobile price prediction` dataset and print the contents of
    the dataset. Execute the following code:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let’s observe the contents of the dataframe; it should look as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – An automobile price prediction dataframe ](img/B17298_03_030.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 – An automobile price prediction dataframe
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, the dataframe consists of a large number
    of columns containing the details of cars. For the sake of understanding target
    encoding, let’s filter out the columns that we want to experiment with while dropping
    the rest. Since we plan on encoding the `make` column, the `fuel-type` column,
    and the `body-style` column, let’s use only those columns along with the `price`
    response column. Execute the following code:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The filtered dataframe will look as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31 – The automobile price prediction dataframe with filtered columns
    ](img/B17298_03_031.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 – The automobile price prediction dataframe with filtered columns
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now split this dataframe into training and testing dataframes. Execute
    the following code:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s now train our target encoder model using `H2OTargetEncoderEstimator`.
    Execute the following code:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Once the target encoder has finished its training, you will see the following
    output:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – The result of target encoder training  ](img/B17298_03_032.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 – The result of target encoder training
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, you can see that the H2O target encoder will
    generate the target-encoded values for the `make` column, the `fuel-type` column,
    and the `body-style` column and store them in different columns named `make_te`,
    `fuel-type_te`, and `body-style_te`, respectively. These new columns will contain
    the encoded values.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now use this trained target encoder to encode the training dataset and
    print the encoded dataframe:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The encoded training frame should look as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – An encoded automobile price prediction training dataframe ](img/B17298_03_033.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – An encoded automobile price prediction training dataframe
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the figure, our training frame now has three additional
    columns, `make_te`, `fuel-type_te`, and `body-style_te`, with numerical values.
    These are the target-encoded columns for the `make` column, the `fuel-type` column,
    and the `body-style` column.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, let’s now use the trained target encoder to encode the test dataframe
    and print the encoded dataframe. Execute the following code:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The encoded test frame should look as follows:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – An encoded automobile price prediction test dataframe ](img/B17298_03_034.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – An encoded automobile price prediction test dataframe
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the figure, our test frame also has three additional columns,
    which are the encoded columns. You can now use these dataframes to train your
    ML models.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your next actions, you can use the encoded dataframes however you
    see fit. If you want to use the dataframe to train ML models, then you can drop
    the `categorical` columns from the dataframe and use the respective encoded columns
    as training features to train your models. If you wish to perform any further
    analytics on the dataset, then you can keep both types of columns and perform
    any comparative study.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O’s target encoder has several parameters that you can set to tweak the encoding
    process. Selecting the correct settings for target encoding your dataset can get
    very complex, depending on the type of data with which you are working. So, feel
    free to experiment with this function, as the better you understand this feature
    and target encoding in general, the better you can encode your dataframe and further
    improve your model training. You can find more details about H2O’s target encoder
    here: [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/target-encoding.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/target-encoding.xhtml).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have just understood how you can encode categorical values
    using H2O’s target encoder.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first explored the various techniques and some of the common
    functions we use to preprocess our dataframe before it is sent to model training.
    We looked into how we can reframe our raw dataframe into a suitable consistent
    format that meets the requirement for model training. We learned how to manipulate
    the columns of dataframes by combining them with different columns of different
    dataframes. We learned how to combine rows from partitioned dataframes, as well
    as how to directly merge dataframes into a single dataframe.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Once we knew how to reframe our dataframes, we learned how to handle the missing
    values that are often present in freshly collected data. We learned how to fill
    NA values, replace certain incorrect values, as well as how to use different imputation
    strategies to avoid adding noise and bias when filling missing values.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: We then investigated how we can manipulate the feature columns by sorting the
    dataframes by column, as well as changing the types of columns. We also learned
    how to tokenize strings to handle textual data, as well as how to encode categorical
    values using H2O’s target encoder.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will open the black box of AutoML, explore its training,
    and what happens internally during the AutoML process. This will help us to better
    understand how H2O does its magic and efficiently automates the model training
    process.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
