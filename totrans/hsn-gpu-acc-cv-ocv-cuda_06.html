<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic Computer Vision Operations Using OpenCV and CUDA</h1>
                </header>
            
            <article>
                
<p><span>The last chapter described the process of working with images and videos using OpenCV and CUDA. We looked at the code for some basic image and video processing applications and compared the performance of OpenCV code with and without CUDA acceleration. In this chapter, we will build on this knowledge and try to develop some more computer vision and image processing applications using OpenCV and CUDA. This chapter describes the method for accessing individual pixel intensities in color and grayscale images. A histogram is a very useful concept for image processing. This chapter describes the method for calculating histograms and how histogram equalization can improve the visual quality of images. This chapter will also describe how different geometric transformations can be performed using OpenCV and CUDA. Image filtering is a very important concept, which is useful in image preprocessing and feature extraction. This is described in detail in this chapter. The last part of this chapter describes different morphological operations, such as erosion, dilation, and opening and closing images.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Accessing individual pixel intensities in OpenCV</li>
<li>Histogram calculation and histogram equalization</li>
<li>Image transformation</li>
<li>Filtering operations on images</li>
<li>Morphological operations on images</li>
</ul>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>This chapter requires a basic understanding of image processing and computer vision. It needs familiarity with basic C or C++ programming language, CUDA, and all of the codes sample explained in previous chapters. All of the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. The code can be executed on any operating system, though it has only been tested on Ubuntu 16.04. </span></p>
<p>Check out the following video to see the code in action:<br/>
<a href="http://bit.ly/2xERUDL">http://bit.ly/2xERUDL</a></p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Accessing the individual pixel intensities of an image</h1>
                </header>
            
            <article>
                
<p>Sometimes there is a need to access pixel intensity value at a particular location when we are working with images. This is very useful when we want to change the brightness or contrast of a group of pixels or we want to perform some other pixel-level operations. For an 8-bit grayscale image, this intensity value at a point will be in a range of 0 to 255, while for a color image there will be three different intensity values for the blue, green, and red channels with all having values between 0 to 255. </p>
<p>OpenCV provides a <kbd>cv::Mat::at&lt;&gt;</kbd> method for accessing intensity values at a particular location for any channel images. It needs one argument, which is the location of the point at which the intensity is to be accessed. The point is passed using the <kbd>Point</kbd> class with row and column values as arguments. For a grayscale image, the method will return a scalar object while, for a color image, it will return a vector of three intensities.  The code for accessing pixel intensities at a particular location for a grayscale as well as a color image is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  //Gray Scale Image<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::Scalar intensity = h_img1.at&lt;uchar&gt;(cv::Point(100, 50));<br/>  std::cout&lt;&lt;"Pixel Intensity of gray scale Image at (100,50) is:"  &lt;&lt;intensity.val[0]&lt;&lt;std::endl;<br/>  //Color Image<br/>  cv::Mat h_img2 = cv::imread("images/autumn.tif",1);<br/>  cv::Vec3b intensity1 = h_img1.at&lt;cv::Vec3b&gt;(cv::Point(100, 50));<br/>  std::cout&lt;&lt;"Pixel Intensity of color Image at (100,50) is:"&lt;&lt;intensity1&lt;&lt;std::endl;<br/>  return 0;<br/>}</pre>
<p>The grayscale image is read first and the <kbd>at</kbd> method is called on this image object. The intensity value is measured at the <kbd>(100,50)</kbd> point, which indicates the pixel at the 100<sup>th</sup> row and 50<sup>th</sup> column. It returns a scalar, which is stored in the intensity variable. The value is printed on the console. The same procedure is followed for a color image but the return value for it will be a vector of three intensities, which is stored in the <kbd>Vec3b</kbd> object. The intensity values are printed on the console. The output of the above program is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-376 image-border" src="assets/e89e363b-0180-4b19-8817-08d900820e44.png" style="" width="738" height="99"/></div>
<p>As can be seen, pixel intensity for a grayscale image at <kbd>(100,50)</kbd> is <kbd>9</kbd> while for a color image it is <kbd>[175,179,177]</kbd>, which indicates the blue intensity is <kbd>175</kbd>, the green intensity is <kbd>179</kbd>, and red intensity is <kbd>177</kbd>.  The same method is used to modify pixel intensity at a particular location. Suppose, you want to change the pixel intensity at the <kbd>(100,50)</kbd> location to <kbd>128</kbd>, then you can write:</p>
<pre><span class="n">h_img1</span><span class="p">.</span><span class="n">at</span><span class="o">&lt;</span><span class="n">uchar</span><span class="o">&gt;</span><span class="p">(100</span><span class="p">,</span> 50<span class="p">)</span> <span class="o">=</span> <span class="mi">128</span><span class="p">;</span></pre>
<p>To summarize, in this section we have seen a method to access and change intensity values at a particular location. In the next section, we will see the method to calculate the histogram in OpenCV. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Histogram calculation and equalization in OpenCV</h1>
                </header>
            
            <article>
                
<p>A histogram is a very important property of an image as it provides a global description of the appearance of that image. An enormous amount of information can be obtained from the histogram. It represents the relative frequency of occurrence of gray levels in an image. It is basically a plot of gray levels on the <em>X</em>-axis and the number of pixels in each gray level on the <em>Y</em>-axis. If the histogram is concentrated on the left side then the image will be very dark and if it is concentrated on the right side, then the image will be very bright. It should be evenly distributed for good visual quality of an image.</p>
<p>The following image demonstrates histograms for dark, bright, and normal images: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-377 image-border" src="assets/57923152-d000-474c-85e8-2fc0c884e9cc.png" style="" width="768" height="589"/></div>
<p>OpenCV provides a function to calculate the histogram of an Image. The syntax of the function is as follows:</p>
<pre>void cv::cuda::calcHist ( InputArray src, OutputArray hist)</pre>
<p>The function needs two arrays as an argument. The first array is the input image for which the histogram needs to be calculated. The second argument is an output array in which the histogram will be stored. The output can be plotted to get a histogram like what is shown in the preceding screenshot. As described earlier, a flat histogram improves the visual quality of an Image. OpenCV and CUDA provide a function to flatten the histogram, which is described in the following section.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Histogram equalization</h1>
                </header>
            
            <article>
                
<p>A perfect image has an equal number of pixels in all its gray levels. So a histogram should have a large dynamic range and an equal number of pixels in the entire range. This can be accomplished by a technique called histogram equalization. It is a very important preprocessing step in any computer vision application. In this section, we will see how histogram equalization can be performed for grayscale and color images using OpenCV and CUDA.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Grayscale images</h1>
                </header>
            
            <article>
                
<p>Grayscale images are normally 8-bit single channel images that have 256 different gray levels. If the histogram is not evenly distributed, the image is too dark or the image is too light, and histogram equalization should be performed to improve the visual quality of the image. The following code describes the process for histogram equalization on a grayscale image:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::cuda::GpuMat d_img1,d_result1;<br/>  d_img1.upload(h_img1);<br/>  cv::cuda::equalizeHist(d_img1, d_result1);<br/>  cv::Mat h_result1;<br/>  d_result1.download(h_result1);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Histogram Equalized Image", h_result1);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>The image read is uploaded to the device memory for histogram equalization. It is a mathematically intensive step so CUDA acceleration will help in improving the performance of the program. OpenCV provides the <kbd>equalizeHist</kbd> function for histogram equalization. It needs two arguments. The first argument is the source image and the second argument is the destination image. The destination image is downloaded back to the host and displayed on the console. The output after histogram equalization is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-378 image-border" src="assets/33934253-36ec-4673-9d4e-33569c4d1aa6.png" style="" width="510" height="280"/></div>
<p>As can be seen, the image after histogram equalization has a better visual quality than the original image. The same operation for color images is described next.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Color image</h1>
                </header>
            
            <article>
                
<p>Histogram equalization can also be done on color images. It has to be performed on separate channels. So the color image has to be split into three channels. The histogram of each channel is equalized independently and then channels are merged to reconstruct the image. The code for histogram equalization on a color image is as follows: </p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/autumn.tif");<br/>  cv::Mat h_img2,h_result1;<br/>  cvtColor(h_img1, h_img2, cv::COLOR_BGR2HSV);<br/>  //Split the image into 3 channels; H, S and V channels respectively and store it in a std::vector<br/>  std::vector&lt; cv::Mat &gt; vec_channels;<br/>  cv::split(h_img2, vec_channels); <br/>  //Equalize the histogram of only the V channel <br/>  cv::equalizeHist(vec_channels[2], vec_channels[2]);<br/>  //Merge 3 channels in the vector to form the color image in HSV color space.<br/>  cv::merge(vec_channels, h_img2); <br/>  //Convert the histogram equalized image from HSV to BGR color space again<br/>  cv::cvtColor(h_img2,h_result1, cv::COLOR_HSV2BGR);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Histogram Equalized Image", h_result1);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>A histogram is not normally equalized in the BGR color space; HSV and YCrCb color spaces are used for it. So, in the code, the BGR color space is converted in to the HSV color space. Then, it is split into three separate channels using the <kbd>split</kbd> function. Now, hue and saturation channels contain the color information so there is no point in equalizing those channels. Histogram equalization is performed only on the value channel.  Three channels are merged back to reconstruct the color image using the <kbd>merge</kbd> function. The HSV color image is converted back into the BGR color space for display using <kbd>imshow</kbd>. The output of the program is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-379 image-border" src="assets/caded0d4-03be-471c-ae00-ea7b724a6b5d.png" style="" width="704" height="274"/></div>
<p>To summarize, histogram equalization improves the visual quality of an image so it is a very important preprocessing step for any computer vision application. The next section describes the geometric transformation of images.   </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Geometric transformation on images</h1>
                </header>
            
            <article>
                
<p>Sometimes, there is a need for resizing an image, the translation of images, and the rotation of images for larger computer vision applications. These kinds of geometric transformation is explained in this section.  </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image resizing</h1>
                </header>
            
            <article>
                
<p>images need to be of specific sizes in some computer vision applications. So there is a need to convert the image of arbitrary size into the specific size. OpenCV provides a function to resize an image. The code for image resizing is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::cuda::GpuMat d_img1,d_result1,d_result2;<br/>  d_img1.upload(h_img1);<br/>  int width= d_img1.cols;<br/>  int height = d_img1.size().height;<br/>  cv::cuda::resize(d_img1,d_result1,cv::Size(200, 200),   cv::INTER_CUBIC);<br/>  cv::cuda::resize(d_img1,d_result2,cv::Size(0.5*width, 0.5*height),   cv::INTER_LINEAR); <br/>  cv::Mat h_result1,h_result2;<br/>  d_result1.download(h_result1);<br/>  d_result2.download(h_result2);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Resized Image", h_result1);<br/>  cv::imshow("Resized Image 2", h_result2);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>The height and width of an image can be obtained using two different functions, as shown in the code. The <kbd>rows</kbd> and <kbd>cols</kbd> properties of the <kbd>Mat</kbd> object describes the <kbd>height</kbd> and <kbd>width</kbd> of an image respectively. The <kbd>Mat</kbd> object also has the <kbd>size()</kbd> method, which has <kbd>height</kbd> and <kbd>width</kbd> properties which are used to find the size of an image. The image is resized in two ways. In the first way, the image is resized to a specific size of <kbd>(200,200)</kbd> and, in the second, it is resized to half of its original dimensions. OpenCV provides the <kbd>resize</kbd> function for this operation. It has four arguments.</p>
<p>The first two arguments are the source and destination images respectively. The third argument is the size of the destination image. It is defined using the <kbd>Size</kbd> object. When images are resized, then pixel values have to be interpolated on the destination image from the source image. There are various interpolation methods such as, bilinear interpolation, bicubic interpolation, area interpolation which are available for interpolating pixel values. This interpolation method is provided as the fourth argument to the <kbd>resize</kbd> function. It can be <kbd>cv::INTER_LINEAR (bilinear)</kbd>, <kbd>cv::INTER_CUBIC (bicubic)</kbd>, or <kbd>cv::INTER_AREA (Area)</kbd>. The output of the image resizing code is as follows:  </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-380 image-border" src="assets/a7179ed4-cbc3-4d76-b0e2-345089955041.png" style="" width="673" height="329"/></div>
<p> </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image translation and rotation</h1>
                </header>
            
            <article>
                
<p>Image translation and rotation are important geometric transformations that are needed in some computer vision applications. OpenCV provides an easy API to perform these transformations on images. The code to perform translation and rotation is as follows: </p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::cuda::GpuMat d_img1,d_result1,d_result2;<br/>  d_img1.upload(h_img1);<br/>  int cols= d_img1.cols;<br/>  int rows = d_img1.size().height;<br/>  //Translation<br/>  cv::Mat trans_mat = (cv::Mat_&lt;double&gt;(2,3) &lt;&lt; 1, 0, 70, 0, 1, 50);<br/>  cv::cuda::warpAffine(d_img1,d_result1,trans_mat,d_img1.size());<br/>  //Rotation<br/>  cv::Point2f pt(d_img1.cols/2., d_img1.rows/2.); <br/>  cv::Mat rot_mat = cv::getRotationMatrix2D(pt, 45, 1.0);<br/>  cv::cuda::warpAffine(d_img1, d_result2, rot_mat, cv::Size(d_img1.cols, d_img1.rows));<br/>  cv::Mat h_result1,h_result2;<br/>  d_result1.download(h_result1);<br/>  d_result2.download(h_result2);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Translated Image", h_result1);<br/>  cv::imshow("Rotated Image", h_result2);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>A translation matrix needs to be created, which specifies the image translation in horizontal and vertical directions. It is a 2 x 3 matrix as shown here:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e728715d-0b8d-44f4-85e8-8da1dbdefd1a.png" style="width:6.67em;height:3.33em;" width="990" height="480"/></div>
<p><kbd>tx</kbd> and <kbd>ty</kbd> are translation offset in the <em>x</em> and <em>y </em>directions. In the code, this matrix is created using the <kbd>Mat</kbd> object with 70 as an offsets in the <em>X</em>-direction and 50 as an offset in the <em>Y</em>-direction.  This matrix is passed as an argument to the <kbd>warpAffine</kbd> function for Image translation. The other arguments for the <kbd>warpAffine</kbd> function are the source image, destination image, and size of the output image respectively. </p>
<p>A rotation matrix should be created for Image rotation at a particular degree centered at a particular point. OpenCV provides the <kbd>cv::getRotationMatrix2D</kbd> function to construct this rotation matrix. It needs three arguments. The first argument is the point for rotation; the center of the image is used for this. The second argument is the angle of rotation in degrees, which is specified as 45 degrees. The last argument is the scale, which is specified as 1. The constructed rotation matrix is again passed as an argument to the <kbd>warpAffine</kbd> function for Image rotation.</p>
<p>The output of the Image translation and Image rotation code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-381 image-border" src="assets/00396820-7bf0-4da4-84cb-7c198c24b00c.png" style="" width="815" height="292"/></div>
<p>To summarize, this section described various geometric transformations such as Image resizing, image translation, and Image rotation using OpenCV and CUDA.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Filtering operations on images</h1>
                </header>
            
            <article>
                
<p class="mce-root">The methods described till this point worked on a single pixel intensity, and are called point processing methods. Sometimes it is helpful to look at the neighborhood of a pixel rather than only single pixel intensity. This are called neighborhood processing techniques. The neighborhood can be 3 x 3, 5 x 5, 7 x 7, and so on and are matrix-centered at a particular pixel. Image filtering is an important neighborhood processing technique. </p>
<p>Filtering is an important concept in signal processing where we reject a certain band of frequencies and allow a certain band of frequency to pass. How is frequency measured in images? If gray levels change slowly over a region, then it is a low-frequency region. If gray levels changes drastically, then it is a high-frequency region.  Normally the background of an image is considered a low-frequency region and the edges are high-frequency regions. Convolution is a very important mathematical concept for neighborhood processing and Image filtering in particular. It is explained in the following section.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolution operations on an image</h1>
                </header>
            
            <article>
                
<p>The basic idea of convolutions evolved from a similar idea in biology called receptive fields, where it is sensitive to some parts in an image and insensitive to other parts. It can be mathematically represented as the following:</p>
<p>                  <strong> </strong> <em>g(x,y)=f(x,y)*h(x,y)= ∑∑f(n,m)h(x-n,y-m) </em>         </p>
<p>In simplified form, this equation is a dot product between a filter, <em>h,</em> and a sub-image of the image, <em>f,</em> centered around the <em>(x,y)</em> point. The answer to this product is equal to the <em>(x,y)</em> point in the image, <em>g</em>. To illustrate the working of the convolution operation on an image, an example of a 3 x 3 filter applied to an image of a size of 6 x 6 is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-382 image-border" src="assets/a9321074-4028-4de5-a4bd-9ad4aa78c0b8.png" style="" width="409" height="381"/></div>
<p><span>A dot product is taken between the leftmost window shown in red with the filter to find a point in the destination image. The answer of the dot product will be <em>2 ((1*1 + 1*1 + 1*0 + 1*5 + 1*1 +1*7 +1*0 +1*1 + 1*1)/9)</em>. The same operation is repeated after moving this window by 1 pixel to the right and answer the will be <em>3</em>. This is repeated for all windows in an image to construct the destination image. Different low pass and high-pass filters can be constructed by changing the values of the 3 x 3 filter matrix. This is explained in the next two sections.</span></p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Low pass filtering on an image</h1>
                </header>
            
            <article>
                
<p>A low pass filter removes high-frequency content from an Image. Generally, noise is considered high-frequency content so a low pass filter removes noise from an image. There are many types of noise, such as Gaussian noise, uniform noise, exponential noise, and salt-and-pepper noise, which can effect an image. Low pass filters are used to eliminate this kind of noise. There are many types of low pass filters available:</p>
<ul>
<li>Averaging or box filters</li>
<li>Gaussian filters</li>
</ul>
<p class="mce-root"/>
<ul>
<li>Median filters</li>
</ul>
<p>These filters and implementation of them using OpenCV is explained in this section.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Averaging filters</h1>
                </header>
            
            <article>
                
<p>An averaging filters as the name suggests, performs averaging operations on neighborhood pixels. If a Gaussian noise is present in an image then the low pass averaging filter can be used to remove the noise. It will also blur the edges of an Image, because of the averaging operation. The neighborhood can be 3 x 3, 5 x 5, 7 x 7, and so on. The bigger the size of the filter window, the more blurring of the image will take place. The 3 x 3 and 5 x 5 averaging mask is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-383 image-border" src="assets/394559ad-38f0-4218-bd97-5273f0a1bc4e.png" style="" width="421" height="139"/></div>
<p>OpenCV provides a simple interface to apply many kinds of filters on an image. The code to apply an averaging filter with a different mask is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::cuda::GpuMat d_img1,d_result3x3,d_result5x5,d_result7x7;<br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filter3x3,filter5x5,filter7x7;<br/>  filter3x3 = cv::cuda::createBoxFilter(CV_8UC1,CV_8UC1,cv::Size(3,3));<br/>  filter3x3-&gt;apply(d_img1, d_result3x3);<br/>  filter5x5 = cv::cuda::createBoxFilter(CV_8UC1,CV_8UC1,cv::Size(5,5));<br/>  filter5x5-&gt;apply(d_img1, d_result5x5);<br/>  filter7x7 = cv::cuda::createBoxFilter(CV_8UC1,CV_8UC1,cv::Size(7,7));<br/>  filter7x7-&gt;apply(d_img1, d_result7x7);<br/><br/>  cv::Mat h_result3x3,h_result5x5,h_result7x7;<br/>  d_result3x3.download(h_result3x3);<br/>  d_result5x5.download(h_result5x5);<br/>  d_result7x7.download(h_result7x7);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Blurred with kernel size 3x3", h_result3x3);<br/>  cv::imshow("Blurred with kernel size 5x5", h_result5x5);<br/>  cv::imshow("Blurred with kernel size 7x7", h_result7x7);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p><kbd>cv::Ptr</kbd>, which is a template class for smart pointers, is used to store a filter of the <kbd>cv::cuda::Filter</kbd> type. Then, the <kbd>createBoxFilter</kbd> function is used to create an averaging filter of different window sizes. It requires three mandatory and three optional arguments.  The first and second arguments are datatypes for the source and destination images. They are taken as <kbd>CV_8UC1</kbd>, which indicates an 8-bit unsigned grayscale image. The third argument defines the size of the filter window. It can be 3 x 3, 5 x 5, 7 x 7, and so on. The fourth argument is the anchor point, which has a default value of (-1,-1) which indicates the anchor is at the center point of the kernel. The final two optional arguments are related to the pixel interpolation method and border value, which are omitted here. </p>
<p>The created filter pointer has an apply method, which is used to apply the created filter on any image. It has three arguments. The first argument is the source image, the second argument is the destination image, and the third optional argument is, CUDA stream, which is used for multitasking, as explained earlier in this book. In the code, three averaging filters of different sizes are applied on an Image. The result is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-384 image-border" src="assets/0d5c6cf0-9d19-4774-be37-edbcc634877a.png" style="" width="575" height="568"/></div>
<p>As can be seen from the output, as the size of the filter increases, more pixels are used for averaging, which introduces more blurring on an image. Though a large filter will eliminate more noise. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gaussian filters</h1>
                </header>
            
            <article>
                
<p>A Gaussian filter uses a mask that has a Gaussian distribution to filter an Image instead of a simple averaging mask. This filter also introduces smooth blurring on an Image and is widely used to eliminate noise from an image. A 5 x 5 Gaussian filter with an approximate standard deviation of 1 is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-385 image-border" src="assets/111f7e02-3cb6-4702-919d-3f170e573560.png" style="" width="245" height="164"/></div>
<p>OpenCV provides a function to implement the Gaussian filter. The code for it is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/cameraman.tif",0);<br/>  cv::cuda::GpuMat d_img1,d_result3x3,d_result5x5,d_result7x7;<br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filter3x3,filter5x5,filter7x7;<br/>  filter3x3 = cv::cuda::createGaussianFilter(CV_8UC1,CV_8UC1,cv::Size(3,3),1);<br/>  filter3x3-&gt;apply(d_img1, d_result3x3);<br/>  filter5x5 = cv::cuda::createGaussianFilter(CV_8UC1,CV_8UC1,cv::Size(5,5),1);<br/>  filter5x5-&gt;apply(d_img1, d_result5x5);<br/>  filter7x7 = cv::cuda::createGaussianFilter(CV_8UC1,CV_8UC1,cv::Size(7,7),1);<br/>  filter7x7-&gt;apply(d_img1, d_result7x7);<br/><br/>  cv::Mat h_result3x3,h_result5x5,h_result7x7;<br/>  d_result3x3.download(h_result3x3);<br/>  d_result5x5.download(h_result5x5);<br/>  d_result7x7.download(h_result7x7);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Blurred with kernel size 3x3", h_result3x3);<br/>  cv::imshow("Blurred with kernel size 5x5", h_result5x5);<br/>  cv::imshow("Blurred with kernel size 7x7", h_result7x7);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>The <kbd>createGaussianFilter</kbd> function is used to create a mask for the Gaussian filter. The datatype of the source and destination images, size of the filter, and standard deviation in the horizontal direction are provided as arguments to the function. We can also provide a standard deviation in the vertical direction as an argument; if it is not provided, then its default value is equal to standard deviation in the horizontal direction. The created Gaussian mask of a different size is applied to the image using an <kbd>apply</kbd> method. The output of the program is as follows:  </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-386 image-border" src="assets/3dcba18f-b470-4ce8-8cdc-a4e555c4a06c.png" style="" width="571" height="569"/></div>
<p>Again as the size of the Gaussian filter increases, more blurring is introduced in the image. The Gaussian filter is used to eliminate noise and introduce smooth blurring on an image.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Median filtering</h1>
                </header>
            
            <article>
                
<p>When an image is affected by salt and pepper noise, it will not be eliminated by the averaging or Gaussian filter. It needs a nonlinear filter. Median operations on a neighborhood instead of averaging can help in eliminating salt and pepper noise. In this filter, the median of 9-pixel values in the neighborhood is placed at the center pixel. It will eliminate extreme high or low values introduced by salt and pepper noise. Though OpenCV and CUDA provide a function for median filtering, it is slower than regular functions in OpenCV, so this function is used to implement median filter as shown in the following code:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/saltpepper.png",0);<br/>  cv::Mat h_result;<br/>  cv::medianBlur(h_img1,h_result,3);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Median Blur Result", h_result);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>The <kbd>medianBlur</kbd> function in OpenCV is used to implement a median filter. It needs three arguments. The first argument is the source image, the second argument is the destination image, and the third argument is the window size for the median operation. The output of the median filtering is as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-387 image-border" src="assets/f692e545-d0a8-457b-8ade-e6b45c278b16.png" style="" width="512" height="284"/></div>
<p>The source image is affected by salt and pepper noise, as can be seen in the screenshot. This noise is eliminated completely by the median filter of a size of 3x3 without introducing extreme blurring. So median filtering is a very important preprocessing step when images applications are affected by salt and pepper noise.</p>
<p>To summarize, we have seen three types of low pass filter, which are widely used in various computer vision applications. Averaging and Gaussian filters are used to eliminate Gaussian noise, but they will also blur the edges of an Image. A median filter is used to remove salt-and-pepper noise.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">High-pass filtering on an image</h1>
                </header>
            
            <article>
                
<p>High-pass filters remove low-frequency components from images and enhance high-frequency components. So, when a high-pass filter is applied to an Image, it will remove the background, as it is a low-frequency region, and enhances the edges, which are high-frequency components. So, high-pass filters can also be called edge detectors. The coefficients of the filter will change, otherwise it is similar to the filters seen in the last section. There are many high-pass filters available such as following:</p>
<ul>
<li>Sobel filters</li>
<li>Scharr filters</li>
<li>Laplacian filters</li>
</ul>
<p>We will see each one of them separately in this section.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sobel filters</h1>
                </header>
            
            <article>
                
<p>The Sobel operator or Sobel filter is a widely used image processing and computer vision algorithm for edge detection applications. It is a 3 x 3 filter that approximates the gradient of the image intensity function. It provides a separate filter to compute the gradient in a horizontal and vertical direction. The filter is convolved with the image in a similar way as described earlier in this chapter.  The horizontal and vertical 3 x 3 Sobel filter is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-388 image-border" src="assets/08b07763-8193-4132-9968-3c8b55ffa033.png" style="" width="473" height="98"/></div>
<p>The code for implementing this Sobel filter is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/blobs.png",0);<br/>  cv::cuda::GpuMat d_img1,d_resultx,d_resulty,d_resultxy;<br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filterx,filtery,filterxy;<br/>  filterx = cv::cuda::createSobelFilter(CV_8UC1,CV_8UC1,1,0);<br/>  filterx-&gt;apply(d_img1, d_resultx);<br/>  filtery = cv::cuda::createSobelFilter(CV_8UC1,CV_8UC1,0,1);<br/>  filtery-&gt;apply(d_img1, d_resulty);<br/>  cv::cuda::add(d_resultx,d_resulty,d_resultxy); <br/>  cv::Mat h_resultx,h_resulty,h_resultxy;<br/>  d_resultx.download(h_resultx);<br/>  d_resulty.download(h_resulty);<br/>  d_resultxy.download(h_resultxy);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Sobel-x derivative", h_resultx);<br/>  cv::imshow("Sobel-y derivative", h_resulty);<br/>  cv::imshow("Sobel-xy derivative", h_resultxy);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>OpenCV provides the <kbd>createSobelFilter</kbd> function for implementing a Sobel filter.  It requires many arguments. The first two arguments are data types of the source and destination images. The third and fourth arguments are the order of the <kbd>x</kbd> and <kbd>y</kbd> derivatives respectively. For computing the <kbd>x</kbd> derivative or vertical edges, 1 and 0 are provided, and for computing the <kbd>y</kbd> derivative or horizontal edges, 0 and 1 are provided. The fifth argument which indicates the size of the kernel, is optional. The default value is 3. The scale for derivatives can also be provided.</p>
<p>To see both horizontal and vertical edges simultaneously, the result of the <kbd>x</kbd>-derivative and the <kbd>y</kbd>-derivative are summed. The result is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-389 image-border" src="assets/6a632427-fd9a-4d72-a303-501284f8c1ba.png" style="" width="688" height="604"/></div>
<p>The Sobel operator provides a very inaccurate approximation of derivative but still, it is quite useful in computer vision applications for edge detection. It does not have rotation symmetry; to overcome that, the Scharr operator is used.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scharr filters</h1>
                </header>
            
            <article>
                
<p>As Sobel does not provide rotation symmetry, a Scharr operator is used to overcome that by using different filter masks as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-390 image-border" src="assets/6ebcaa19-69d1-49fb-94fa-40e6cf520ee8.png" style="" width="473" height="100"/></div>
<p>As can be seen from the mask, the Scharr operator gives more weight to central rows or central columns to find edges. The program to implement a Scharr filter is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/blobs.png",0);<br/>  cv::cuda::GpuMat d_img1,d_resultx,d_resulty,d_resultxy;<br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filterx,filtery;<br/>  filterx = cv::cuda::createScharrFilter(CV_8UC1,CV_8UC1,1,0);<br/>  filterx-&gt;apply(d_img1, d_resultx);<br/>  filtery = cv::cuda::createScharrFilter(CV_8UC1,CV_8UC1,0,1);<br/>  filtery-&gt;apply(d_img1, d_resulty);<br/>  cv::cuda::add(d_resultx,d_resulty,d_resultxy); <br/>  cv::Mat h_resultx,h_resulty,h_resultxy;<br/>  d_resultx.download(h_resultx);<br/>  d_resulty.download(h_resulty);<br/>  d_resultxy.download(h_resultxy);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Scharr-x derivative", h_resultx);<br/>  cv::imshow("Scharr-y derivative", h_resulty);<br/>  cv::imshow("Scharr-xy derivative", h_resultxy);<br/>  cv::waitKey();<br/>    return 0;<br/>}</pre>
<p><span>OpenCV provides the </span><kbd>createScharrFilter</kbd><span> function for implementing the Scharr filter.  It requires many arguments. The first two arguments are datatypes of the source and destination images. The third and fourth arguments are the order of the <kbd>x</kbd> and <kbd>y</kbd> derivatives respectively. For computing the <kbd>x</kbd> derivative or vertical edges, 1 and 0 are provided, and for computing the <kbd>y</kbd> derivative or horizontal edges, 0 and 1 are provided. The fifth argument, which indicate the size of the kernel, is optional. The default value is 3. </span></p>
<p><span>To see both horizontal and vertical edges simultaneously, the result of the <kbd>x</kbd>-derivative and the <kbd>y</kbd>-derivative are summed. The result is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-391 image-border" src="assets/681f64de-2f79-41eb-9d92-01acbfc9d973.png" style="" width="684" height="592"/></div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Laplacian filters</h1>
                </header>
            
            <article>
                
<p>The Laplacian filters is also a derivative operator to find out edges in an Image. The difference is that Sobel and Scharr are first-order derivative operators, while Laplacian is a second-order derivative operator. It also finds out edges in both horizontal and vertical directions simultaneously, which is different than Sobel and Scharr operators. The Laplacian filter is computes the second derivative, so it is very sensitive to noise in an image, it is desirable to blur the image and remove noise before applying the Laplacian filter. The code for implementing a Laplacian filter is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/blobs.png",0);<br/>  cv::cuda::GpuMat d_img1,d_result1,d_result3;<br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filter1,filter3;<br/>  filter1 = cv::cuda::createLaplacianFilter(CV_8UC1,CV_8UC1,1);<br/>  filter1-&gt;apply(d_img1, d_result1);<br/>  filter3 = cv::cuda::createLaplacianFilter(CV_8UC1,CV_8UC1,3);<br/>  filter3-&gt;apply(d_img1, d_result3);<br/>  cv::Mat h_result1,h_result3;<br/>  d_result1.download(h_result1);<br/>  d_result3.download(h_result3);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Laplacian filter 1", h_result1);<br/>  cv::imshow("Laplacian filter 3", h_result3);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>Two Laplacian filters with kernel sizes of 1 and 3 are applied on an image using the <kbd>createLaplacianFilter</kbd> function. Along with the size of the kernel, the function also requires datatypes of the source and destination images as arguments. The created Laplacian filter is applied to an image using the <kbd>apply</kbd> method. The output of the Laplacian filter is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-392 image-border" src="assets/c9f8842c-0a03-4a11-9c52-2b3fd23a180d.png" style="" width="1042" height="311"/></div>
<p>To summarize, in this section, we have described different high-pass filters such as Sobel, Scharr, and Laplacian filters. Sobel and Scharr are first-order derivative operators used to compute edges and they are less sensitive to noise. Laplacian is a second-order derivative operator used to compute edges and it is very sensitive to noise. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Morphological operations on images</h1>
                </header>
            
            <article>
                
<p>Image morphology deals with the regions and shapes of an image. It is used to extract image components that are useful to represent shapes and regions. Image morphology treats the image as an ensemble of sets unlike, other Image processing operations seen earlier. The image interacts with a small template, which is called a structuring element, and which defines the region of interest or neighborhood in the image morphology. There are various morphological operations that can be performed on images, which are explained one by one in this section:</p>
<ul>
<li><strong>Erosion</strong>: Erosion sets a center pixel to the minimum over all pixels in the neighborhood. The neighborhood is defined by the structuring element, which is a matrix of 1s and 0s. Erosion is used to enlarge holes in the object, shrink the boundary, eliminate the island, and get rid of narrow peninsulas that might exist on the image boundary.</li>
<li><strong>Dilation</strong>: Dilation sets a center pixel to the maximum over all pixels in the neighborhood. The dilation increases the size of a white block and reduces the size of the black region. It is used to fill holes in the object and expand the boundary of the object.</li>
<li><strong>Opening</strong>: Image opening is basically a combination of erosion and dilation. Image opening is defined as erosion followed by dilation. Both operations are performed using the same structuring elements. It is used to smooth the contours of the image, break down narrow bridges and isolate objects that are touching one another. It is used in the analysis of wear particles in engine oils, ink particles in recycled paper, and so on.</li>
<li><strong>Closing</strong>: Image closing is defined as dilation followed by erosion. Both operations are performed using the same structuring elements. It is used to fuse narrow breaks and eliminate small holes.</li>
</ul>
<p>The morphological operators can be easily understood by applying them on binary images that only contain black and white. OpenCV and CUDA provide an easy API to apply a morphological transformation on images. The code for it is as follows: </p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>int main ()<br/>{<br/>  cv::Mat h_img1 = cv::imread("images/blobs.png",0);<br/>  cv::cuda::GpuMat d_img1,d_resulte,d_resultd,d_resulto, d_resultc;<br/>  cv::Mat element = cv::getStructuringElement(cv::MORPH_RECT,cv::Size(5,5)); <br/>  d_img1.upload(h_img1);<br/>  cv::Ptr&lt;cv::cuda::Filter&gt; filtere,filterd,filtero,filterc;<br/>  filtere = cv::cuda::createMorphologyFilter(cv::MORPH_ERODE,CV_8UC1,element);<br/>  filtere-&gt;apply(d_img1, d_resulte);<br/>  filterd = cv::cuda::createMorphologyFilter(cv::MORPH_DILATE,CV_8UC1,element);<br/>  filterd-&gt;apply(d_img1, d_resultd);<br/>  filtero = cv::cuda::createMorphologyFilter(cv::MORPH_OPEN,CV_8UC1,element);<br/>  filtero-&gt;apply(d_img1, d_resulto);<br/>  filterc = cv::cuda::createMorphologyFilter(cv::MORPH_CLOSE,CV_8UC1,element);<br/>  filterc-&gt;apply(d_img1, d_resultc);<br/>  <br/>  cv::Mat h_resulte,h_resultd,h_resulto,h_resultc;<br/>  d_resulte.download(h_resulte);<br/>  d_resultd.download(h_resultd);<br/>  d_resulto.download(h_resulto);<br/>  d_resultc.download(h_resultc);<br/>  cv::imshow("Original Image ", h_img1);<br/>  cv::imshow("Erosion", h_resulte);<br/>  cv::imshow("Dilation", h_resultd);<br/>  cv::imshow("Opening", h_resulto);<br/>  cv::imshow("closing", h_resultc);<br/>  cv::waitKey();<br/>  return 0;<br/>}</pre>
<p>A structuring element that defines the neighborhood for the morphological operation needs to be created first. This can be done by using the <kbd>getStructuringElement</kbd> function in OpenCV. The shape and size of the structuring element need to be provided as an argument to this function. In the code, a rectangular structuring element of a size of 5 x 5 is defined. </p>
<p>The filter for morphological operations is created using the <kbd>createMorphologyFilter</kbd> function. It needs three mandatory arguments. The first argument defines the operation to be performed. <kbd>cv::MORPH_ERODE</kbd> is used for erosion, <kbd>cv::MORPH_DILATE</kbd> for dilation, <kbd>cv::MORPH_OPEN</kbd> for opening, and <kbd>cv::MORPH_CLOSE</kbd> for closing. The second argument is the datatype of an image, and the third argument is the structuring element created earlier. The <kbd>apply</kbd> method is used to apply these filters on an image.</p>
<p>The output of morphological operations on an image is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-393 image-border" src="assets/e5158d17-32b8-4f56-9c57-29242cedfb1a.png" style="" width="1033" height="601"/></div>
<div>
<p>As can be seen from the output, erosion reduces the boundary of an object while dilation thickens it. We consider the white part an object and the black part is the background. Opening smooths the contours of the image. Closing eliminates small holes in an image.  If the size of the structuring element is increased to 7 x 7 from 5 x 5, then the erosion of the boundary will be more pronounced in an erosion operation and the boundary will get thicker in a dilation operation. The small circles on the left side, which were visible in an eroded image of 5 x 5, are removed when it is eroded with a size of 7 x 7.</p>
<p>The output of morphological operations using a 7 x 7 structuring element is as follows:</p>
</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-394 image-border" src="assets/4e8edd25-34ff-4742-8aaf-1ca95014aaa1.png" style="" width="1036" height="597"/></div>
<p>To summarize, morphological operations are important to find out components used to define the shape and regions of an image. It can be used to fill holes in an image and smoothen the contours of an image. </p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter described the method to access pixel intensities at a particular location in an image. It is very useful when we are performing a pointwise operation on an image. A histogram is a very important global feature used to describe an image. This chapter described the method to compute a histogram and the process of histogram equalization, which improves the visual quality of an image. Various geometric transformations such as image resizing, rotation, and translation were explained in detail. Image filtering is a useful neighborhood processing technique used to eliminate noise and extract edge features of an image and was described in detail. A low pass filter is used to remove noise but it will also blur out the edges of an image. A high-pass filter removes the background, which is a low-frequency region while enhancing edges, which are high-frequency regions. The last part of this chapter described different morphological operations such as erosion, dilation, opening, and closing, which can be used to describe the shape of an image and fill holes in an image. In the next chapter, we will use these concepts to build some useful computer vision applications using OpenCV and CUDA.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Write an OpenCV function to print pixel intensity at the location (200,200) of any color image on the console.</li>
<li>Write an OpenCV function to resize an image to (300,200) pixels. Use the bilinear interpolation method.</li>
<li>Write an OpenCV function to upsample an Image by 2. Use the area interpolation method.</li>
<li>State true or false: blurring decreases as we increase the size of the averaging filter.</li>
<li><span>State true or false: median filters can remove gaussian noise.</span></li>
<li>What steps can be taken to reduce noise sensitivity of Laplacian operator?</li>
<li>Write an OpenCV function to implement top hat and black hat morphological operation.</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>