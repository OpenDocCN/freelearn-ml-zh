<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Tree-Based Methods" id="1R42S1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Tree-Based Methods</h1></div></div></div><p class="calibre8">In this chapter, we are going to present one of the most intuitive ways to create a predictive model—using the concept of a tree. Tree-based models, often also known as decision tree models, are successfully used to handle both regression and classification type problems. We'll explore both scenarios in this chapter, and we'll be looking at a range of different algorithms that are effective in training these models. We will also learn about a number of useful properties that these models possess, such as their ability to handle missing data and the fact that they are highly interpretable.</p></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Tree-Based Methods" id="1R42S1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="The intuition for tree models"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec55" class="calibre1"/>The intuition for tree models</h1></div></div></div><p class="calibre8">A <span class="strong"><strong class="calibre2">decision tree</strong></span> is a model with a very straightforward structure that allows us to make a prediction <a id="id531" class="calibre1"/>on an output variable, based on a series of rules arranged in a tree-like structure. The output variable that we can model can be categorical, allowing us to <a id="id532" class="calibre1"/>use a decision tree to handle classification problems. Equally, we can use decision trees to predict a numerical output, and in this way we'll also be able to tackle problems where the predictive task is a regression task.</p><p class="calibre8">Decision trees <a id="id533" class="calibre1"/>consist of a series of split points, often referred to as <span class="strong"><strong class="calibre2">nodes</strong></span>. In order to make a prediction using a decision tree, we start at the top of the tree at a single node <a id="id534" class="calibre1"/>known as the <span class="strong"><strong class="calibre2">root node</strong></span>. The root node is a decision or split point, because it imposes a condition in terms of the value of one of the input features, and based on this decision we know whether to continue on with the left part of the tree or with the right part <a id="id535" class="calibre1"/>of the tree. We repeat this process of choosing to go left or right at each <span class="strong"><strong class="calibre2">inner node</strong></span> that we encounter until we reach one of the <span class="strong"><strong class="calibre2">leaf nodes</strong></span>. These are the nodes at <a id="id536" class="calibre1"/>the base of the tree, which give us a specific value of the output to use as our prediction.</p><p class="calibre8">To illustrate this, let's look at a very simple decision tree in terms of two features, <span class="strong"><em class="calibre9">x1</em></span> and <span class="strong"><em class="calibre9">x2</em></span>.</p><div class="mediaobject"><img src="../images/00125.jpeg" alt="The intuition for tree models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that <a id="id537" class="calibre1"/>the tree is a recursive structure, in that the left and right parts of the tree that lie beneath a particular node are trees themselves. They are referred <a id="id538" class="calibre1"/>to as the <span class="strong"><strong class="calibre2">left subtree</strong></span> and the <span class="strong"><strong class="calibre2">right subtree</strong></span> respectively and the <a id="id539" class="calibre1"/>nodes that they lead to are the <span class="strong"><strong class="calibre2">left child</strong></span> and <span class="strong"><strong class="calibre2">right child</strong></span>. To understand how we go about using a decision tree in practice, we can try a simple example. Suppose <a id="id540" class="calibre1"/>we want to use our tree to predict the output for an observation where <a id="id541" class="calibre1"/>the value of <span class="strong"><em class="calibre9">x1</em></span> is 96.0 and the value of <span class="strong"><em class="calibre9">x2</em></span> is 79.9. We start at the root and make a decision as to which subtree to follow. Our value of <span class="strong"><em class="calibre9">x2</em></span> is larger than 23, so we follow the right branch and come to a new node with a new condition to check. Our value of <span class="strong"><em class="calibre9">x1</em></span> is larger than 46, so we once again take the right branch and arrive at a leaf node. Thus, we output the value indicated by the leaf node, which is -3.7. This is the value that our model predicts given the pair of inputs that we specified.</p><p class="calibre8">One way of thinking about decision trees is that they are in fact encoding a series of if-then rules leading to distinct outputs. For every leaf node, we can write a single rule (using the boolean <code class="email">AND</code> operator if necessary to join together multiple conditions) that must hold true for the tree to output that node's value. We can extract all of these if-then rules by starting at the root node and following every path down the tree that leads to a leaf node. For example, our small regression tree leads to the following three rules, one for each of its leaf nodes:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">If (x2 &lt; 23) Then Output 2.1</code></li><li class="listitem"><code class="email">If (x2 &gt; 23) AND (x1 &lt; 46) Then Output 1.2</code></li><li class="listitem"><code class="email">If (x2 &gt; 23) AND (x1 &gt; 46) Then Output -3.7</code></li></ul></div><p class="calibre8">Note that we had to join together two conditions for each one of the last two rules using the <code class="email">AND</code> operator, as the corresponding paths leading down to a leaf node included more than one decision node (counting the root node).</p><p class="calibre8">Another way to think about decision trees is that they partition the feature space into a series of rectangular regions in two dimensions, cubes in three dimensions, and hypercubes in higher dimensions. Remember that the number of dimensions in the feature space is just the number of features. The feature space for our example regression tree has two dimensions and <a id="id542" class="calibre1"/>we can visualize how this space is split up into rectangular regions as follows:</p><div class="mediaobject"><img src="../images/00126.jpeg" alt="The intuition for tree models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The rule-based interpretation and the space partitioning interpretation are equivalent views of the same model. The space partitioning interpretation in particular is very useful in helping us appreciate one particular characteristic of decision trees: they must completely cover all possible combinations of input features. Put differently, there should be no particular input for which there is no path to a leaf node in the decision tree. Every time we are given a value for our input features, we should always be able to return an answer. Our feature space partitioning interpretation of a decision tree essentially tells us that there is no point or space of points that doesn't belong to a particular partition with an assigned value. Similarly, with our if-then ruleset view of a decision tree, we are saying that there is always one rule that can be used for any input feature combination, and therefore <a id="id543" class="calibre1"/>we can reorganize our rules into an equivalent <code class="email">if-then-else</code> structure where the last rule is an <code class="email">else</code> statement.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Algorithms for training decision trees"><div class="book" id="1S2JE2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec56" class="calibre1"/>Algorithms for training decision trees</h1></div></div></div><p class="calibre8">Now that <a id="id544" class="calibre1"/>we have understood how a decision tree works, we'll want to address the issue of how we can train one using some data. There are several algorithms that have been proposed to build decision trees, and in this section we will present a few of the most well-known. One thing we should bear in mind is that, whatever tree-building algorithm we choose, we will have to answer four fundamental questions:</p><div class="book"><ul class="itemizedlist"><li class="listitem">For every node (including the root node), how should we choose the input feature to split on and, given this feature, what is the value of the split point?</li><li class="listitem">How do we decide whether a node should become a leaf node or if we should make another split point?</li><li class="listitem">How deep should our tree be allowed to become?</li><li class="listitem">Once we arrive at a leaf node, what value should we predict?</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre8">A great introduction to decision trees is <span class="strong"><em class="calibre9">Chapter 3</em></span> of <span class="strong"><em class="calibre9">Machine Learning</em></span>, <span class="strong"><em class="calibre9">Tom Mitchell</em></span>. This book was probably the first comprehensive introduction to machine learning and is well worth reading. Although published in 1997, much of the material in the book remains relevant today. Furthermore, according to the book's website at <a class="calibre1" href="http://www.cs.cmu.edu/~tom/mlbook.html">http://www.cs.cmu.edu/~tom/mlbook.html</a>, there is a planned second edition in the works.</p></div></div>

<div class="book" title="Algorithms for training decision trees">
<div class="book" title="Classification and regression trees"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec58" class="calibre1"/>Classification and regression trees</h2></div></div></div><p class="calibre8">The <span class="strong"><strong class="calibre2">Classification and regression tree</strong></span> (<span class="strong"><strong class="calibre2">CART</strong></span>) methodology, which we will henceforth <a id="id545" class="calibre1"/>refer to simply as CART, is one of the earliest proposed approaches to building tree-based models. As the name <a id="id546" class="calibre1"/>implies, the methodology encompasses both an approach to building regression trees and an approach to building classification trees.</p><div class="book" title="CART regression trees"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec10" class="calibre1"/>CART regression trees</h3></div></div></div><p class="calibre8">For <a id="id547" class="calibre1"/>regression trees, the key intuition with the CART approach is that, at any given point in the tree, we choose <a id="id548" class="calibre1"/>both the input feature to split on and the value of the split point within that feature, by finding which combination of these maximizes the reduction in the <span class="strong"><strong class="calibre2">sum of squared error</strong></span> (<span class="strong"><strong class="calibre2">SSE</strong></span>). For every leaf node in a regression tree built using CART, the predicted value is simply the average value of the output predicted by all the data points that are assigned to that particular leaf node. To determine whether a new split point should be made or whether the tree should grow a leaf node, we simply count the number of data points that are currently assigned to a node; if this value is less than a predetermined threshold, we create a new leaf node.</p><p class="calibre8">For any given node in the tree, including the root node, we begin by having some data points assigned to that node. At the root node, all the data points are assigned, but once we make a split, some of the data points are assigned to the left child and the remaining points are assigned to the right child. The starting value of the SSE is just the sum of squared error computed using the average value <span class="strong"><img src="../images/00127.jpeg" alt="CART regression trees" class="calibre26"/></span> of the output variable <span class="strong"><img src="../images/00128.jpeg" alt="CART regression trees" class="calibre26"/></span> for the <span class="strong"><em class="calibre9">n</em></span> data points assigned to the current node:</p><div class="mediaobject"><img src="../images/00129.jpeg" alt="CART regression trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If we split these data points into two groups of size <span class="strong"><em class="calibre9">n<sub class="calibre14">1</sub></em></span> and <span class="strong"><em class="calibre9">n<sub class="calibre14">2</sub></em></span> so that <span class="strong"><em class="calibre9">n<sub class="calibre14">1</sub> + n<sub class="calibre14">2</sub> = n</em></span>, and we compute the new SSE for all the data points as the sum of the SSE values for each of the two new groups, we have:</p><div class="mediaobject"><img src="../images/00130.jpeg" alt="CART regression trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the first sum iterates over <span class="strong"><em class="calibre9">j</em></span>, which is the new indices of the data points in the first group corresponding to the left child, and the second sum iterates over <span class="strong"><em class="calibre9">k</em></span>, which is the new indices of the data points inside the second group belonging to the right child. The idea behind CART is that we find a way to form these two groups of data points by considering every possible feature and every possible split point within that feature so that this new quantity is minimized. Thus, we can think of our error function in CART as the SSE.</p><p class="calibre8">One of the natural advantages of CART, and tree-based models in general, is that they are capable of handling various input types, from numerical inputs (both discrete and continuous) to binary inputs as well as categorical inputs. Numerical inputs can be ordered in a natural way by sorting them in ascending order, for example. When we do this, we can see that, if we have <span class="strong"><em class="calibre9">k</em></span> distinct numbers, there are <span class="strong"><em class="calibre9">k-1</em></span> distinct ways to split these into two groups so that <a id="id549" class="calibre1"/>all the numbers in one group are smaller than all the numbers in the second group, and both groups have at least one element. This is simply done by picking the numbers themselves as split points and not counting the smallest number as a split point (which would produce an empty group). So if we have a feature vector <span class="strong"><em class="calibre9">x</em></span> with the numbers <code class="email">{5.6, 2.8, 9.0}</code>, we first sort these into <code class="email">{2.8, 5.6, 9.0}</code>. </p><p class="calibre8">Then, we take each number except the smallest <code class="email">{2.8}</code>to form a split point and a corresponding rule that checks whether the input value is smaller than the split point. In this way, we produce the only two possible groupings for our feature vector:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">Group1 = {2.8}, Group2 = {5.6, 9.0} IF x &lt; 5.6 THEN Group1 ELSE Group2</code></li><li class="listitem"><code class="email">Group1 = {2.8, 5.6}, Group2 = {9.0} IF x &lt; 9.0 THEN Group1 ELSE Group2</code></li></ul></div><p class="calibre8">Note that it is important to have at least one element in each group, otherwise we haven't actually split our data. Binary input features can also be handled by simply using the split point that corresponds to putting all data points that have this feature take the first value in the first group, and the remaining data points that have the second value of this feature in the second group.</p><p class="calibre8">Handling unordered categorical input features (factors) is substantially harder because there is no natural order. As a result, any combination of levels can be assigned to the first group and the remainder to the second group. If we are dealing with a factor that has <span class="strong"><em class="calibre9">k</em></span> distinct levels, then there are <span class="strong"><em class="calibre9">2<sup class="calibre15">k-1</sup>-1</em></span> possible ways to form two groups with at least one level assigned to each group.</p><p class="calibre8">So, a binary-valued feature has one possible split, as we know, and a three-valued feature has three possible splits. With the numerical feature vector containing the numbers <code class="email">{5.6, 2.8, 9.0}</code>, we've already seen two possible splits. The third possible split that could arise if these numbers were labels is the one in which one group has data points with this feature taking the value <code class="email">5.6</code>, and another group with the two values <code class="email">2.8</code> and <code class="email">9.0</code>. Clearly, this is not a valid split when we treat the feature as numerical.</p><p class="calibre8">As a final note, we always have the option of a one-versus-all approach for categorical input features, which is essentially the same as considering splits in which one group always consists of a single element. This is not always a good idea as it may turn out that a particular subset of the levels when taken together is more predictive of an output compared to a single level. If this is the case, the resulting tree will probably be more complex, having a greater number of node splits.</p><p class="calibre8">There are various ways to deal with the large increase in complexity associated with finding and evaluating all the different split points for categorical input features, but we won't go into further detail right now. Instead, let's write some R code to see how we might find the split point of a numerical input feature using the SSE criterion that CART uses:</p><div class="informalexample"><pre class="programlisting">compute_SSE_split &lt;- function(v, y, split_point) {
  index &lt;- v &lt; split_point
  y1 &lt;- y[index]
  y2 &lt;- y[!index]
  SSE &lt;- sum((y1 - mean(y1)) ^ 2) + sum((y2 - mean(y2)) ^ 2)
  return(SSE)
}

compute_all_SSE_splits &lt;- function(v, y) {
  sapply(unique(v), function(sp) compute_SSE_split(v, y, sp))
}</pre></div><p class="calibre8">The first function, <code class="email">compute_SSE_split()</code>, takes in a feature vector <code class="email">v</code>, an output vector <code class="email">y</code>, and a <a id="id550" class="calibre1"/>specific value for the feature vector that we want to split on, namely <code class="email">split_point</code>. It uses these to compute the SSE value for that split. To do this, it first creates an indexing vector, identifying all the elements of the feature vector whose value is less than the <code class="email">split_point</code> value. This is then used to split the output vector into two groups, <code class="email">y1</code> and <code class="email">y2</code>; the former contains elements identified by the indexing vector and the latter contains the remaining elements. Finally, the SSE is computed as the sum of the SSE of the two groups using the familiar formula of the sum of squared distances from the group mean.</p><p class="calibre8">The second function, <code class="email">compute_all_SSE_splits()</code>, takes in a feature vector <code class="email">v</code>, and an output vector <code class="email">y</code>, and uses all the unique elements of the feature vector as potential split points (for simplicity, we are ignoring the fact that we should not be splitting on the smallest value of the feature vector).</p><p class="calibre8">In order to demonstrate how splitting works in CART, we will generate a small artificial dataset. The following code snippet generates 20 random observations of two input features and stores the result in a data frame, <code class="email">rcart_df</code>:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(99)
&gt; x1 &lt;- rbinom(20, 1, 0.5)
&gt; set.seed(100)
&gt; x2 &lt;- round(10 + rnorm(20, 5, 5), 2)
&gt; set.seed(101)
&gt; y &lt;- round((1 + (x2 * 2 / 5) + x1 - rnorm(20, 0, 3)), 2)
&gt; rcart_df &lt;- data.frame(x1, x2, y)
&gt; rcart_df
   x1    x2     y
1   1 12.49  7.97
2   0 15.66  5.61
3   1 14.61  9.87
4   1 19.43  9.13
5   1 15.58  7.30
6   1 16.59  5.11
7   1 12.09  4.98
8   0 18.57  8.77
9   0 10.87  2.60
10  0 13.20  6.95
11  1 15.45  6.60
12  1 15.48 10.58
13  0 13.99  2.31
14  1 18.70 13.88
15  1 15.62  8.96
16  1 14.85  8.52
17  0 13.06  8.77
18  0 17.55  7.84
19  0 10.43  7.63
20  0 26.55 17.77</pre></div><p class="calibre8">In practice, 20 data points might be a suitable number to use as a threshold for building a leaf node, but for this example we will simply suppose that we wanted to make a new split using <a id="id551" class="calibre1"/>this data. We have two input features, <code class="email">x1</code> and <code class="email">x2</code>. The former is a binary input feature that we have coded using the numerical labels 0 and 1. This allows us to reuse the functions we just wrote to compute the possible splits. The latter is a numerical input feature. By applying our <code class="email">compute_all_SSE_splits()</code> function on each feature separately, we can compute all the possible split points for each feature and their SSE. The following two plots show these SSE values for each feature in turn:</p><div class="mediaobject"><img src="../images/00131.jpeg" alt="CART regression trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Looking at both plots, we can see that the best possible split produces an SSE value of <code class="email">124.05</code> and <a id="id552" class="calibre1"/>this can be achieved by splitting on the feature <code class="email">x2</code> at the value <code class="email">18.7</code>. Consequently, our regression tree would contain a node with the following splitting rule:</p><div class="informalexample"><pre class="programlisting"> If x2 &lt; 18.7</pre></div><p class="calibre8">The CART <a id="id553" class="calibre1"/>methodology always applies the same logic to determine whether to make a new split at each node as well as how to pick which feature and value to split on. This recursive approach of splitting up the data points at each node to build the regression tree is why this process is also known as <span class="strong"><strong class="calibre2">recursive partitioning</strong></span>.</p></div><div class="book" title="Tree pruning"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec11" class="calibre1"/>Tree pruning</h3></div></div></div><p class="calibre8">If we were <a id="id554" class="calibre1"/>to allow the recursive partitioning process to repeat indefinitely, we would eventually terminate by having leaf nodes with a single data point each, because that is when we cannot split the data any further. This model would fit the training data perfectly, but it is highly unlikely that its performance would generalize on unseen data. Thus, tree-based models are susceptible to overfitting. To combat this, we need to control the depth of our final decision tree.</p><p class="calibre8">The process <a id="id555" class="calibre1"/>of removing nodes from the tree to limit its size and complexity is known as <span class="strong"><strong class="calibre2">pruning</strong></span>. One possible pruning method is to impose a threshold for the smallest number of data points that can be used in order to create a new split in the tree instead of creating a leaf node. This will create leaf nodes earlier on in the procedure and the data points that are assigned to them may not all have the same output. In this case, we can simply <a id="id556" class="calibre1"/>predict the average value for regression (and the most popular class for classification). This is an example of <span class="strong"><strong class="calibre2">pre-pruning</strong></span>, as we are pruning the tree while building it and before it is fully constructed.</p><p class="calibre8">Intuitively, we should be able to see that, the larger the depth of the tree and the smaller the average number of data points assigned to leaf nodes, the greater the degree of overfitting. Of course, if we have fewer nodes in the tree, we probably aren't being granular enough in our modeling of the underlying data.</p><p class="calibre8">The question <a id="id557" class="calibre1"/>of how large a tree should be allowed to grow is thus effectively a question of how to model our data as closely as possible while controlling the degree of overfitting. In practice, using pre-pruning is tricky as it is difficult to find an appropriate threshold.</p><p class="calibre8">Another <a id="id558" class="calibre1"/>regularization process that is used by the CART methodology to prune trees is known as <span class="strong"><strong class="calibre2">cost-complexity tuning</strong></span>. In effect, trees are often allowed to grow fully using the recursive partitioning approach described in the previous section. Once this completes, we prune the resulting tree, that is to say, we start removing split points and merging leaf nodes to shrink the tree according to a certain criterion. This is known as <span class="strong"><strong class="calibre2">post-pruning</strong></span>, as we prune the tree after it has been built. When we construct the original tree, the error function that we use is the SSE. To prune the tree, we use a penalized version of the SSE for minimizing:</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="Tree pruning" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre9">α</em></span> is a complexity parameter controlling the degree of regularization and <span class="strong"><em class="calibre9">Tp</em></span> is the number of nodes in the tree, which is a way to model the size of the tree. Similar to the way in which lasso limits the size of regression coefficients in generalized linear models, this regularization procedure limits the size of the resulting tree. A very small value of <span class="strong"><em class="calibre9">α</em></span> results in a small degree of pruning, which in the limit of <span class="strong"><em class="calibre9">α</em></span> taking the value of 0 corresponds <a id="id559" class="calibre1"/>to no pruning at all. On the other hand, using a high value for this parameter results in trees that are much shorter, which, at its limit, can result in a zero-sized tree with no splits at all that predicts a single average value of the output for all possible inputs.</p><p class="calibre8">It turns out that every particular value of <span class="strong"><em class="calibre9">α</em></span> corresponds to a unique tree structure that minimizes this penalized form of the SSE for that particular value. Put differently, given a particular value for <span class="strong"><em class="calibre9">α</em></span>, there is a unique and predictable way to prune a tree in order to minimize the penalized SSE, but the details of this procedure are beyond the scope of this book. For now, we can simply assume that every value of <span class="strong"><em class="calibre9">α</em></span> is associated with a single tree.</p><p class="calibre8">This particular feature is very useful, as we don't have any ambiguity in picking a tree once we settle on a value for the complexity parameter <span class="strong"><em class="calibre9">α</em></span>. It does not, however, give us a way to determine what actual value we should use. Cross-validation, which we saw in <a class="calibre1" title="Chapter 5. Neural Networks" href="part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7">Chapter 5</a>, <span class="strong"><em class="calibre9">Support Vector Machines</em></span>, is a commonly used approach designed to estimate an appropriate value of this parameter. Cross-validation applied to this problem would involve partitioning the data into <span class="strong"><em class="calibre9">k</em></span> folds. We then train and prune <span class="strong"><em class="calibre9">k</em></span> trees by using all the data excluding a single fold and repeating this for each of the <span class="strong"><em class="calibre9">k</em></span> folds. Finally, we measure the SSE on the folds held out for testing and average the results. We can repeat our cross-validation procedure for different values of <span class="strong"><em class="calibre9">a</em></span>. Another approach when more data is available is to use a validation dataset for evaluating models that have been trained on the same training dataset, but with different values of <span class="strong"><em class="calibre9">α</em></span>.</p></div><div class="book" title="Missing data"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec12" class="calibre1"/>Missing data</h3></div></div></div><p class="calibre8">One <a id="id560" class="calibre1"/>characteristic of decision trees is that they have a natural way of handling missing data during training. For example, when we consider which feature to split on at a particular node, we can ignore data points that have a missing value for a particular feature and compute the potential reduction in our error function (deviance, SSE, and so on) using the remaining data points. Note that while this approach is handy, it could potentially increase the bias of the model substantially, especially if we are ignoring a large portion of our available training data because of missing values.</p><p class="calibre8">One <a id="id561" class="calibre1"/>might wonder whether we are able to handle missing values during prediction for unseen data points. If we are <a id="id562" class="calibre1"/>at a particular node in the tree that is splitting on a feature, and for which our test data point has a missing value, we are seemingly stuck. In practice, this situation can be dealt with via the use of <span class="strong"><strong class="calibre2">surrogate splits</strong></span>. The key notion behind these is that, for every node in the tree, apart from the feature that was optimally chosen to split on, we keep track of a list of other features that produce splits in the data similar to the feature we actually chose. In this way, when our testing data point has a missing value for a feature that we need in order to make a prediction, we can refer to a node's surrogate splits instead, and use a different feature for this node.</p></div></div></div>

<div class="book" title="Algorithms for training decision trees">
<div class="book" title="Regression model trees"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec59" class="calibre1"/>Regression model trees</h2></div></div></div><p class="calibre8">One of <a id="id563" class="calibre1"/>the potential drawbacks of regression trees built with CART is that, even though we limit the number of data points that are assigned to a particular leaf node, these may still have significant variations in the output variable <a id="id564" class="calibre1"/>among themselves. When this happens, taking the average value and using this as a single prediction for that leaf node may not be the best idea.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Regression model trees</strong></span> attempt to overcome this limitation by using the data points at the leaf nodes to construct a linear model to predict the output. The original regression model tree algorithm was developed by <span class="strong"><em class="calibre9">J. Ross Quinlan</em></span> and is known as <span class="strong"><strong class="calibre2">M5</strong></span>. The M5 algorithm computes a <a id="id565" class="calibre1"/>linear model at each node in the tree. For a test data point, we first compute the decision path traversed from the root node to the leaf node. The prediction that is then made is the output of the linear model associated with that leaf node.</p><p class="calibre8">M5 also differs from the algorithm used in CART in that it employs a different criterion to determine which feature to split on. This criterion is the weighted reduction in standard deviation:</p><div class="mediaobject"><img src="../images/00133.jpeg" alt="Regression model trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This general equation assumes that we split the data into <span class="strong"><em class="calibre9">p</em></span> partitions (as we have seen for trees, <span class="strong"><em class="calibre9">p</em></span> is typically 2). For each partition <span class="strong"><em class="calibre9">i</em></span>, we compute the standard deviation <span class="strong"><em class="calibre9">σ<sub class="calibre14">i</sub></em></span>. Then we compute the weighted average of these standard deviations using the relative size of each partition (<span class="strong"><em class="calibre9">n<sub class="calibre14">i</sub>/n</em></span>) as the weights. This is subtracted from the initial standard deviation of the unpartitioned data.</p><p class="calibre8">The <a id="id566" class="calibre1"/>idea behind this criterion is that splitting a node should produce groups of data points that within each group display less variability with <a id="id567" class="calibre1"/>respect to the output variable than all the data points when grouped together. We'll have a chance to see <span class="strong"><em class="calibre9">M5 trees</em></span> as well as CART trees in action later on in this chapter.</p></div></div>

<div class="book" title="Algorithms for training decision trees">
<div class="book" title="CART classification trees"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec60" class="calibre1"/>CART classification trees</h2></div></div></div><p class="calibre8">Building <a id="id568" class="calibre1"/>classification trees using the CART methodology continues the notion of recursively splitting up groups of data points in order <a id="id569" class="calibre1"/>to minimize some error function. Our first guess for an appropriate error function is the classification accuracy. It turns out that this is not a particularly good measure to use to build a classification tree.</p><p class="calibre8">What we <a id="id570" class="calibre1"/>would actually like to use is a measure for node purity that would score nodes based on whether they contain data points primarily belonging to one of the output classes. This is a very intuitive idea because what we are effectively aiming for in a classification tree is to eventually be able to group our training data points into sets of data points at the leaf nodes, so that each leaf node contains data points belonging to only one of the classes. This will mean that we can confidently predict this class if we arrive at that leaf node during prediction.</p><p class="calibre8">One possible measure of node purity, frequently used with CART for classification trees, is the <span class="strong"><strong class="calibre2">Gini index</strong></span>. For an output variable with <span class="strong"><em class="calibre9">K</em></span> different classes, the Gini index <span class="strong"><em class="calibre9">G</em></span> is defined as follows:</p><div class="mediaobject"><img src="../images/00134.jpeg" alt="CART classification trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To calculate the Gini index, we compute an estimate of the probability of every class and multiply this with the probability of not being that class. We then add up all these products. For a binary classification problem, it should be easy to see that the Gini index evaluates to <span class="strong"><em class="calibre9">2</em></span>
<span class="strong"><img src="../images/00135.jpeg" alt="CART classification trees" class="calibre26"/></span> (<span class="strong"><em class="calibre9">1- </em></span>
<span class="strong"><img src="../images/00135.jpeg" alt="CART classification trees" class="calibre26"/></span>), where <span class="strong"><img src="../images/00135.jpeg" alt="CART classification trees" class="calibre26"/></span> is the estimated probability of one of the classes.</p><p class="calibre8">To compute the Gini index at a particular node in a tree, we can simply use the ratio of the number of data points labeled as class <span class="strong"><em class="calibre9">k</em></span> over the total number of data points as an estimate for the probability of a data point belonging to class <span class="strong"><em class="calibre9">k</em></span> at the node in question. Here is a simple R function to compute the Gini index:</p><div class="informalexample"><pre class="programlisting">gini_index &lt;- function(v) {
  t &lt;- table(v)
  probs &lt;- t / sum(t)
  terms &lt;- sapply(probs, function(p) p * (1 - p) )
  return(sum(terms))
}</pre></div><p class="calibre8">To compute <a id="id571" class="calibre1"/>the Gini index, our <code class="email">gini_index()</code> function first tabulates all the entries in a vector. It divides each of these frequency counts with the total number of counts to transform them into probability estimates. Finally, it computes the product (<span class="strong"><em class="calibre9">1-</em></span>) for each of these and sums up all the terms. Let's try a few examples:</p><div class="informalexample"><pre class="programlisting">&gt; gini_index(v = c(0, 0, 0, 1, 1, 1))
[1] 0.5
&gt; gini_index(v = c(0, 0, 0, 1, 1, 1, 1, 1, 1))
[1] 0.4444444
&gt; gini_index(v = c(0, 0, 0, 1, 1, 1, 2, 2, 2))
[1] 0.6666667
&gt; gini_index(v = c(1, 1, 1, 1, 1, 1))
[1] 0</pre></div><p class="calibre8">Note how <a id="id572" class="calibre1"/>the Gini index for a completely pure node (a node with only one class) is 0. For a binary output with equal proportions of the two classes, the Gini index is 0.5. Similar to the standard deviation in regression trees, we use the weighted reduction in the Gini index, where we weigh each partition by its relative size, to determine appropriate split points:</p><div class="mediaobject"><img src="../images/00136.jpeg" alt="CART classification trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Another commonly used criterion is deviance. When we studied logistic regression, we saw that this is just the constant <span class="strong"><em class="calibre9">-2</em></span> multiplied by the log-likelihood of the data. In a classification tree setting, we compute the deviance of a node in a classification tree as:</p><div class="mediaobject"><img src="../images/00137.jpeg" alt="CART classification trees" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Unlike the Gini index, the total number of observations <span class="strong"><em class="calibre9">n<sub class="calibre14">k</sub></em></span> at a node affects the value of deviance. All nodes that have the same proportion of data points across different classes will <a id="id573" class="calibre1"/>have the same value of the Gini index, but if they have different numbers of observations, they will have different values of deviance. In both splitting <a id="id574" class="calibre1"/>criteria, however, a completely pure node will have a value of 0 and a positive value otherwise.</p><p class="calibre8">Aside from using a different splitting criterion, the logic to build a classification tree using the CART methodology is exactly parallel to that of building a regression tree. Missing values are handled in the same way and the tree is pre-pruned in the same way using a threshold on the number of data points left to build leaf nodes. The tree is also post-pruned using the same cost-complexity approach outlined for regression trees, but after replacing the SSE as the error function with either the Gini index or the deviance.</p></div></div>

<div class="book" title="Algorithms for training decision trees">
<div class="book" title="C5.0"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec61" class="calibre1"/>C5.0</h2></div></div></div><p class="calibre8">The <span class="strong"><strong class="calibre2">C5.0</strong></span> algorithm developed by <span class="strong"><em class="calibre9">Ross Quinlan</em></span> is an algorithm to build a decision tree for classification. This algorithm is the latest in a chain of successively improved versions starting <a id="id575" class="calibre1"/>from an algorithm known as <span class="strong"><strong class="calibre2">ID3</strong></span>, which developed into <span class="strong"><strong class="calibre2">C4.5</strong></span> (and an open source implementation in the Java programming language known as <span class="strong"><strong class="calibre2">J48</strong></span>) before culminating in C5.0. There are many good acronyms used for decision trees, but thankfully <a id="id576" class="calibre1"/>many of them are related to each other. The C5.0 chain of algorithms <a id="id577" class="calibre1"/>has several differences from the CART methodology, most notably in the choice of the splitting criterion as well as in the pruning procedure.</p><p class="calibre8">The <a id="id578" class="calibre1"/>splitting criterion used with C5.0 is known as <span class="strong"><strong class="calibre2">entropy</strong></span> or the <span class="strong"><strong class="calibre2">information statistic</strong></span>, and has its roots in information theory. Entropy is defined as the average <a id="id579" class="calibre1"/>number of binary digits (bits) needed to communicate information via a message as a function of the probabilities of the different symbols used. Entropy <a id="id580" class="calibre1"/>also has roots in statistical physics, where it is used to represent the degree of chaos and uncertainty in a system. When the symbols or components of a system have equal probabilities, there is a high degree of uncertainty, but entropy is lower when one symbol is far likelier than the others. This observation renders the definition of entropy very useful in measuring node purity. The formal definition of entropy in bits for the multiclass scenario with <span class="strong"><em class="calibre9">K</em></span> classes is:</p><div class="mediaobject"><img src="../images/00138.jpeg" alt="C5.0" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the binary case, the equation simplifies to (where <span class="strong"><em class="calibre9">p</em></span> arbitrarily refers to the probability of one of the two classes):</p><div class="mediaobject"><img src="../images/00139.jpeg" alt="C5.0" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We <a id="id581" class="calibre1"/>can compare entropy to the Gini index for binary classification in the following plot:</p><div class="mediaobject"><img src="../images/00140.jpeg" alt="C5.0" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From the plot, we can see that both functions have the same general shape for the binary class <a id="id582" class="calibre1"/>problem. Recall that the lower the entropy, the lower the uncertainty we have about the distribution of our classes and hence we have higher node purity. Consequently, we want to minimize the entropy as we build our tree. In ID3, the splitting criterion that is used is the weighted entropy reduction, which is also known as <span class="strong"><strong class="calibre2">information gain</strong></span>:</p><div class="mediaobject"><img src="../images/00141.jpeg" alt="C5.0" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It turns out that this criterion suffers from <span class="strong"><strong class="calibre2">selection bias</strong></span>, in that it tends to favor categorical variables <a id="id583" class="calibre1"/>because of the large number of possible groupings compared to the linear range of splits we find with continuous features. To combat this, from C4.5 onwards the criterion was refined into the <span class="strong"><strong class="calibre2">information gain ratio</strong></span>. This is a normalized <a id="id584" class="calibre1"/>version of information gain, where we normalize with <a id="id585" class="calibre1"/>respect to a quantity known as the <span class="strong"><strong class="calibre2">split information value</strong></span>.</p><p class="calibre8">This in <a id="id586" class="calibre1"/>turn represents the potential increase in information that we can get just by the size of the partitions themselves. A high split information value occurs when we have evenly sized partitions and a low value occurs when most of the data points are concentrated in a small number of the partitions. In summary, we have this:</p><div class="mediaobject"><img src="../images/00142.jpeg" alt="C5.0" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The C5.0 chain of algorithms also incorporate alternative methods to prune a tree that go beyond the simple elimination of nodes and sub-trees. For example, inner nodes may be removed before leaf nodes so that the nodes beneath the removed node (the sub-tree) are pushed up (raised) to replace the removed node. C5.0, in particular, is a very powerful algorithm that also contains improvements to speed, memory usage, native boosting (covered in the next chapter) capabilities, as well as the ability to specify a cost matrix so that the algorithm can avoid making certain types of misclassifications over others, just as we saw with support vector machines in the previous chapter.</p><p class="calibre8">We'll demonstrate how to build trees with C5.0 in R in a subsequent section.</p></div></div>
<div class="book" title="Predicting class membership on synthetic 2D data" id="1T1401-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec57" class="calibre1"/>Predicting class membership on synthetic 2D data</h1></div></div></div><p class="calibre8">Our first <a id="id587" class="calibre1"/>example showcasing tree-based methods in R will operate on a synthetic dataset that we have created. The dataset can be generated using commands in the companion R file for this chapter, available from the publisher. The data consists of 287 observations of two input features, <code class="email">x1</code> and <code class="email">x2</code>.</p><p class="calibre8">The output variable is a categorical variable with three possible classes: <code class="email">a</code>, <code class="email">b</code>, and <code class="email">c</code>. If we follow the commands in the code file, we will end up with a data frame in R, <code class="email">mcdf</code>:</p><div class="informalexample"><pre class="programlisting">&gt; head(mcdf, n = 5)
          x1       x2 class
1 18.58213 12.03106     a
2 22.09922 12.36358     a
3 11.78412 12.75122     a
4 23.41888 13.89088     a
5 16.37667 10.32308     a</pre></div><p class="calibre8">This problem is actually very simple because, on the one hand, we have a very small dataset with only two features, and on the other the classes happen to be quite well separated in the feature space, something that is very rare. Nonetheless, our objective in this section is to demonstrate the construction of a classification tree on <span class="strong"><em class="calibre9">well-behaved</em></span> data before we get our hands (or keyboards) dirty on a real-world dataset in the next section.</p><p class="calibre8">To build a classification tree for this dataset, we will use the <code class="email">tree</code> package, which provides us with the <code class="email">tree()</code> function that trains a model using the CART methodology. As is the norm, the first parameter to be provided is a formula and the second parameter is the data frame. The function also has a <code class="email">split</code> parameter that identifies the criterion to be used for splitting. By default, this is set to <code class="email">deviance</code> for the deviance criterion, for which we observed better performance on this dataset. We encourage readers to repeat these experiments by setting the <code class="email">split</code> parameter to <code class="email">gini</code> for splitting on the Gini index.</p><p class="calibre8">Without further ado, let us train our first decision tree:</p><div class="informalexample"><pre class="programlisting">&gt; library(tree)
&gt; d2tree &lt;- tree(class ~ ., data = mcdf)
&gt; summary(d2tree)

Classification tree:
tree(formula = class ~ ., data = mcdf)
Number of leaf nodes:  5 
Residual mean deviance:  0.03491 = 9.844 / 282 
Misclassification error rate: 0.003484 = 1 / 287</pre></div><p class="calibre8">We invoke the <code class="email">summary()</code> function on our trained model to get some useful information about the tree we built. Note that for this example, we won't be splitting our data into a training and test set, as our goal is to discuss the quality of the model fit first. From the provided summary, we seem to have only misclassified a single example in our entire dataset. Ordinarily, this would raise suspicion that we are overfitting; however, we already know that our classes are well separated in the feature space. We can use the <code class="email">plot()</code> function <a id="id588" class="calibre1"/>to plot the shape of our tree as well as the <code class="email">text()</code> function to display all the relevant labels so we can fully visualize the classifier we have built:</p><div class="informalexample"><pre class="programlisting">&gt; plot(d2tree)
&gt; text(d2tree, all = T)</pre></div><p class="calibre8">This is the plot that is produced:</p><div class="mediaobject"><img src="../images/00143.jpeg" alt="Predicting class membership on synthetic 2D data" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that our plot shows a predicted class for every node, including non-leaf nodes. This simply allows us to see which class is predominant at every step of the tree. For example, at the root node, we see that the predominant class is class <code class="email">b</code>, simply because this is the most commonly represented class in our dataset. It is instructive to be able to see the partitioning of our 2D space that our decision tree represents.</p><p class="calibre8">For one and two features, the <code class="email">tree</code> package allows us to use the <code class="email">partition.tree()</code> function to visualize our decision tree. We have done this and superimposed our original data over it in order to see how the classifier has partitioned the space:</p><div class="mediaobject"><img src="../images/00144.jpeg" alt="Predicting class membership on synthetic 2D data" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Most of <a id="id589" class="calibre1"/>us would probably identify six clusters in our data; however, the clusters on the top-right of the plot are both assigned to class <code class="email">b</code> and so the tree classifier has identified this entire region of space as a single leaf node. Finally, we can spot the misclassified point belonging to class <code class="email">b</code> that has been assigned to class <code class="email">c</code> (it is the triangle in the middle of the top part of the graph).</p><p class="calibre8">Another interesting observation to make is how efficiently the space has been partitioned into rectangles in this particular case (only five rectangles for a dataset with six clusters). On the other hand, we can expect this model to have some instabilities because several of the boundaries of the rectangles are very close to data points in the dataset (and thus close to the edges of a cluster). Consequently, we should also expect to obtain a lower accuracy with unseen data that is generated from the same process that generated our training data.</p><p class="calibre8">In the next section, we will build a tree model for a real-world classification problem.</p></div>
<div class="book" title="Predicting the authenticity of banknotes" id="1TVKI1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec58" class="calibre1"/>Predicting the authenticity of banknotes</h1></div></div></div><p class="calibre8">In this <a id="id590" class="calibre1"/>section, we will study the problem of predicting whether a particular banknote is genuine or whether it has been forged. The <span class="strong"><em class="calibre9">banknote authentication dataset</em></span> is hosted at <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication">https://archive.ics.uci.edu/ml/datasets/banknote+authentication</a>. The creators <a id="id591" class="calibre1"/>of the dataset have taken specimens of both genuine and forged banknotes and photographed them with an industrial camera. The resulting grayscale image was processed using a type of time-frequency transformation <a id="id592" class="calibre1"/>known as a <span class="strong"><strong class="calibre2">wavelet transform</strong></span>. Three features of this transform are constructed, and, along with the image entropy, they make up the four features in total for this binary classification task.</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">waveletVar</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Variance of the wavelet-transformed image</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">waveletSkew</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Skewness of the wavelet-transformed image</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">waveletCurt</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Kurtosis of the wavelet-transformed image</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">entropy</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Entropy of the image</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">class</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Authenticity (a 0 output means genuine and a 1 output means forged)</p>
</td></tr></tbody></table></div><p class="calibre8">First, we will split our 1,372 observations into training and test sets:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; set.seed(266)
&gt; bnote_sampling_vector &lt;- createDataPartition(bnote$class, p = 
                           0.80, list = FALSE)
&gt; bnote_train &lt;- bnote[bnote_sampling_vector,]
&gt; bnote_test &lt;- bnote[-bnote_sampling_vector,]</pre></div><p class="calibre8">Next, we will introduce the <code class="email">C50</code> R package that contains an implementation of the C5.0 algorithm for classification. The <code class="email">C5.0()</code> function that belongs to this package also takes in a formula and a data frame as its minimum required input. Just as before, we can use the <code class="email">summary()</code> function to examine the resulting model. Instead of reproducing the entire output of the latter, we'll focus on just the tree that is built:</p><div class="informalexample"><pre class="programlisting">&gt; bnote_tree &lt;- C5.0(class ~ ., data = bnote_train)
&gt; summary(bnote_tree)
waveletVar &gt; 0.75896:
:...waveletCurt &gt; -1.9702: 0 (342)
:   waveletCurt &lt;= -1.9702:
:   :...waveletSkew &gt; 4.9228: 0 (128)
:       waveletSkew &lt;= 4.9228:
:       :...waveletVar &lt;= 3.4776: 1 (34)
:           waveletVar &gt; 3.4776: 0 (2)
waveletVar &lt;= 0.75896:
:...waveletSkew &gt; 5.1401:
    :...waveletVar &lt;= -3.3604: 1 (31)
    :   waveletVar &gt; -3.3604: 0 (93/1)
    waveletSkew &lt;= 5.1401:
    :...waveletVar &gt; 0.30081:
        :...waveletCurt &lt;= 0.35273: 1 (25)
        :   waveletCurt &gt; 0.35273:
        :   :...entropy &lt;= 0.71808: 0 (24)
        :       entropy &gt; 0.71808: 1 (3)
        waveletVar &lt;= 0.30081:
        :...waveletCurt &lt;= 3.0423: 1 (241)
            waveletCurt &gt; 3.0423:
            :...waveletSkew &gt; -1.8624: 0 (21/1)
                waveletSkew &lt;= -1.8624:
                :...waveletVar &lt;= -0.69572: 1 (146)
                    waveletVar &gt; -0.69572:
                    :...entropy &lt;= -0.73535: 0 (2)
                        entropy &gt; -0.73535: 1 (6)</pre></div><p class="calibre8">As we <a id="id593" class="calibre1"/>can see, it is perfectly acceptable to use a feature more than once in the tree in order to make a new split. The numbers in brackets to the right of the leaf nodes in the tree indicate the number of observations from each class that are assigned to that node. As we can see, the vast majority of the leaf nodes in the tree are pure nodes, so that only observations from one class are assigned to them.</p><p class="calibre8">Only two leaf nodes have a single observation each from the minority class for that node, and with this we can infer that we only made two mistakes in our training data using this model. To see if our model has overfitted the data or whether it really can generalize well, we'll test it on our test set:</p><div class="informalexample"><pre class="programlisting">&gt; bnote_predictions &lt;- predict(bnote_tree, bnote_test)
&gt; mean(bnote_test$class == bnote_predictions)
[1] 0.9890511</pre></div><p class="calibre8">The test accuracy is near perfect, a rare sight and the last time in this chapter that we'll be finished so easily! As a final note, <code class="email">C50()</code> also has a <code class="email">costs</code> parameter, which is useful for dealing with asymmetric error costs.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting complex skill learning"><div class="book" id="1UU542-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec59" class="calibre1"/>Predicting complex skill learning</h1></div></div></div><p class="calibre8">In this section, we'll have a chance to explore data from an innovative and recent project known as <span class="strong"><em class="calibre9">SkillCraft</em></span>. The <a id="id594" class="calibre1"/>interested reader can find out more about this project on the web by going to <a class="calibre1" href="http://skillcraft.ca/">http://skillcraft.ca/</a>. The key premise behind the project is that, by studying <a id="id595" class="calibre1"/>the performance of players in a <span class="strong"><strong class="calibre2">real-time strategy</strong></span> (<span class="strong"><strong class="calibre2">RTS</strong></span>) game that involves complex resource management and strategic decisions, we can study how humans learn complex skills and develop speed and competence in <a id="id596" class="calibre1"/>dynamic resource allocation scenarios. To achieve this, data has been collected from players playing the popular real-time strategy game, <span class="strong"><em class="calibre9">Starcraft 2</em></span>, developed by <span class="strong"><em class="calibre9">Blizzard</em></span>.</p><p class="calibre8">In this game, players compete against each other on one of many fixed maps and starting locations. Each player must choose a fictional race from three available choices and start with six worker units, which are used to collect one of two game resources. These resources are needed in order to build military and production buildings, military units unique to each race, research technologies, and to build more worker units. The game involves a mix of economic advancement, military growth, and military strategy in real-time engagements.</p><p class="calibre8">Players are pitted against each other via an online matching algorithm that groups players into leagues according to their perceived level of skill. The algorithm's perception of a player's skill changes over time on the basis of that player's performance across the games in which the player participates. There are eight leagues in total, which are uneven in population in that the lower leagues tend to have more players and the upper leagues have fewer players.</p><p class="calibre8">Having a basic understanding of the game, we can download the SkillCraft1 Master Table dataset from the UCI Machine Learning repository by going to <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset">https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset</a>. The rows <a id="id597" class="calibre1"/>of this dataset are individual games that are played and the features of the games are metrics of a player's playing speed, competence, and decision-making. The authors of the dataset have used <a id="id598" class="calibre1"/>both standard performance metrics familiar to players of the game, as well as other metrics such as <span class="strong"><strong class="calibre2">Perception Action Cycles</strong></span> (<span class="strong"><strong class="calibre2">PACs</strong></span>), which attempt to quantify a player's actions at the fixed location on the map at which a player is looking during a particular time window.</p><p class="calibre8">The task at hand is to predict which of the eight leagues a player is currently assigned to on the basis of these performance metrics. Our output variable is an ordered categorical variable because we have eight distinct leagues ordered from 1 to 8, where the latter corresponds to the league with players of the highest skill.</p><p class="calibre8">One possible way to deal with ordinal outputs is to treat them as a numeric variable, modeling this as a regression task, and build a regression tree. The following table describes <a id="id599" class="calibre1"/>the features and output variables that we have in our dataset:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Feature name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Description</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Age</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Player's age</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">HoursPerWeek</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Reported hours spent playing per week</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">TotalHours</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Reported total hours ever spent playing</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">APM</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Game actions per minute</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">SelectByHotkeys</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of unit or building selections made using hotkeys per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">AssignToHotkeys</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of units or buildings assigned to hotkeys per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">UniqueHotkeys</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of unique hotkeys used per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MinimapAttacks</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of attack actions on minimap per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MinimapRightClicks</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of right-clicks on minimap per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">NumberOfPACs</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of PACs per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">GapBetweenPACs</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Mean duration in milliseconds between PACs</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ActionLatency</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Mean latency from the onset of a PAC to their first action in milliseconds</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ActionsInPAC</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Mean number of actions within each PAC</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">TotalMapExplored</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of 24x24 game coordinate grids viewed by the player per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">WorkersMade</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of worker units trained per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">UniqueUnitsMade</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Unique units made per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ComplexUnitsMade</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Number of complex units trained per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ComplexAbilitiesUsed</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Abilities requiring specific targeting instructions used per timestamp</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">LeagueIndex</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numeric</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Bronze, Silver, Gold, Platinum, Diamond, Master, GrandMaster, and Professional leagues coded 1-8 (output)</p>
</td></tr></tbody></table></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip16" class="calibre1"/>Tip</h3><p class="calibre8">If the reader has never played a real-time strategy game like <span class="strong"><em class="calibre9">Starcraft 2</em></span> on a computer before, it is likely that many of the features used by the dataset will sound arcane. If one simply takes on board that these features represent various aspects of a player's level of performance in the game, it will still be possible to follow all the discussion surrounding the training and testing of our regression tree without any difficulty.</p></div><p class="calibre8">To start with, we load this dataset onto the data frame <code class="email">skillcraft</code>. Before beginning to work <a id="id600" class="calibre1"/>with the data, we will have to do some preprocessing. Firstly, we'll drop the first column. This simply has a unique game identifier that we don't need and won't use. Secondly, a quick inspection of the imported data frame will show that three columns have been interpreted as factors because the input dataset contains a question mark to denote a missing value. To deal with this, we first need to convert these columns to numeric columns, a process that will introduce missing values in our dataset.</p><p class="calibre8">Next, although we've seen that trees are quite capable of handling these missing values, we are going to remove the few rows that have them. We will do this because we want to be able to compare the performance of several different models in this chapter and in the next, not all of which support missing values.</p><p class="calibre8">Here is the code for the preprocessing steps just described:</p><div class="informalexample"><pre class="programlisting">&gt; skillcraft &lt;- read.csv("SkillCraft1_Dataset.csv")
&gt; skillcraft &lt;- skillcraft[-1]
&gt; skillcraft$TotalHours &lt;- as.numeric(
  levels(skillcraft$TotalHours))[skillcraft$TotalHours]
Warning message:
NAs introduced by coercion 
&gt; skillcraft$HoursPerWeek &lt;- as.numeric(
  levels(skillcraft$HoursPerWeek))[skillcraft$HoursPerWeek]
Warning message:
NAs introduced by coercion 
&gt; skillcraft$Age &lt;- as.numeric(
  levels(skillcraft$Age))[skillcraft$Age]
Warning message:
NAs introduced by coercion 
&gt; skillcraft &lt;- skillcraft[complete.cases(skillcraft),]</pre></div><p class="calibre8">As usual, the next step will be to split our data into training and test sets:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; set.seed(133)
&gt; skillcraft_sampling_vector &lt;- createDataPartition( 
  skillcraft$LeagueIndex, p = 0.80, list = FALSE)
&gt; skillcraft_train &lt;- skillcraft[skillcraft_sampling_vector,]
&gt; skillcraft_test &lt;- skillcraft[-skillcraft_sampling_vector,]</pre></div><p class="calibre8">This time, we <a id="id601" class="calibre1"/>will use the <code class="email">rpart</code> package in order to build our decision tree (along with the <code class="email">tree</code> package, these two are the most commonly used packages for building tree-based models in R). This package provides us with an <code class="email">rpart()</code> function to build our tree. Just as with the <code class="email">tree()</code> function, we can build a regression tree using the default behavior by simply providing a formula and our data frame:</p><div class="informalexample"><pre class="programlisting">&gt; library(rpart)
&gt; regtree &lt;- rpart(LeagueIndex ~ ., data = skillcraft_train)</pre></div><p class="calibre8">We can plot our regression tree to see what it looks like:</p><div class="informalexample"><pre class="programlisting">&gt; plot(regtree, uniform = TRUE)
&gt; text(regtree, use.n = FALSE, all = TRUE, cex = .8)</pre></div><p class="calibre8">This is the plot that is produced:</p><div class="mediaobject"><img src="../images/00145.jpeg" alt="Predicting complex skill learning" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To get a <a id="id602" class="calibre1"/>sense of the accuracy of our regression tree, we will compute predictions on the test data and then measure the SSE. This can be done with the help of a simple function that we will define, <code class="email">compute_SSE()</code>, which calculates the sum of squared error when given a vector of target values and a vector of predicted values:</p><div class="informalexample"><pre class="programlisting"> compute_SSE &lt;- function(correct, predictions) {
     return(sum((correct - predictions) ^ 2))
 }
 
&gt; regtree_predictions &lt;- predict(regtree, skillcraft_test)
&gt; (regtree_SSE &lt;- compute_SSE(regtree_predictions, skillcraft_test$LeagueIndex))
[1] 740.0874</pre></div></div>

<div class="book" title="Predicting complex skill learning">
<div class="book" title="Tuning model parameters in CART trees"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec62" class="calibre1"/>Tuning model parameters in CART trees</h2></div></div></div><p class="calibre8">So far, all we have done is use default values for all the parameters of the recursive partitioning <a id="id603" class="calibre1"/>algorithm for building the tree. The <code class="email">rpart()</code> function has a special <code class="email">control</code> parameter to which we can provide an object containing the values of any parameters we wish to override. To build this object, we must use the special <code class="email">rpart.control()</code> function. There are a number of different parameters that we could tweak, and it is worth studying the help file for this function to learn more about them.</p><p class="calibre8">Here we will focus on three important parameters that affect the size and complexity of our tree. The <code class="email">minsplit</code> parameter holds the minimum number of data points that are needed in order for the algorithm to attempt a split before it is forced to create a leaf node. The default value is 30. The <code class="email">cp</code> parameter is the complexity parameter we have seen before and the default value of this is 0.01. Finally, the <code class="email">maxdepth</code> parameter limits the maximum number of nodes between a leaf node and the root node. The default value of 30 is quite liberal here, allowing for fairly large trees to be built. We can try out a different regression tree by specifying some values for these that are different from their default. We'll do this, and see if this affects the SSE performance on our test set:</p><div class="informalexample"><pre class="programlisting">&gt; regtree.random &lt;- rpart(LeagueIndex ~ ., data = skillcraft_train, 
  control = rpart.control(minsplit = 20, cp = 0.001, maxdepth = 10))
&gt; regtree.random_predictions &lt;- predict(regtree.random, 
  skillcraft_test)
&gt; (regtree.random_SSE &lt;- compute_SSE(regtree.random_predictions, 
   skillcraft_test$LeagueIndex))
[1] 748.6157</pre></div><p class="calibre8">Using these values we are trying to limit the tree to a depth of 10, while making it easier to force a split by needing 20 or more data points at a node. We are also lowering the effect of regularization by setting the complexity parameter to 0.001. This is a completely random choice that happens to give us a worse SSE value on our test set. In practice, what is needed is a systematic way to find appropriate values of these parameters for our tree by trying out a number of different combinations and using cross-validation as a way to estimate their performance on unseen data.</p><p class="calibre8">Essentially, we would like to tune our regression tree training and in <a class="calibre1" title="Chapter 5. Neural Networks" href="part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7">Chapter 5</a>, <span class="strong"><em class="calibre9">Support Vector Machines</em></span>, we met the <code class="email">tune()</code> function inside the <code class="email">e1071</code> package, which can help us do just that. We will use this function with <code class="email">rpart()</code> and provide it with ranges for the three parameters we just discussed:</p><div class="informalexample"><pre class="programlisting">&gt; library(e1071)
&gt; rpart.ranges &lt;- list(minsplit = seq(5, 50, by = 5), cp = c(0,  
  0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2,0.5), maxdepth = 1:10)
&gt; (regtree.tune &lt;- tune(rpart,LeagueIndex ~ ., 
   data = skillcraft_train, ranges = rpart.ranges))

Parameter tuning of 'rpart':

- sampling method: 10-fold cross validation 

- best parameters:
 minsplit    cp maxdepth
       35 0.002        6

- best performance: 1.046638</pre></div><p class="calibre8">Running the preceding tasks will likely take several minutes to complete, as there are many combinations of parameters. Once the procedure completes, we can train a tree with the suggested values:</p><div class="informalexample"><pre class="programlisting">&gt; regtree.tuned &lt;- rpart(LeagueIndex ~ ., data = skillcraft_train,  
  control = rpart.control(minsplit = 35, cp = 0.002, maxdepth = 6))
&gt; regtree.tuned_predictions &lt;- predict(regtree.tuned, 
  skillcraft_test)
&gt; (regtree.tuned_SSE &lt;- compute_SSE(regtree.tuned_predictions, 
   skillcraft_test$LeagueIndex))
[1] 701.3386</pre></div><p class="calibre8">Indeed, we <a id="id604" class="calibre1"/>have a lower SSE value with these settings on our test set. If we type in the name of our new regression tree model, <code class="email">regree.tuned</code>, we'll see that we have many more nodes in our tree, which is now substantially more complex.</p></div></div>

<div class="book" title="Predicting complex skill learning">
<div class="book" title="Variable importance in tree models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec63" class="calibre1"/>Variable importance in tree models</h2></div></div></div><p class="calibre8">For large <a id="id605" class="calibre1"/>trees such as this, plotting is less useful as it is very hard to make the plot readable. One interesting plot that we <a id="id606" class="calibre1"/>can obtain is a plot of <span class="strong"><strong class="calibre2">variable importance</strong></span>. For every input feature, we keep track of the reduction in the optimization criterion (for example, deviance or SSE) that occurs every time it is used anywhere in the tree. We can then tally up this quantity for all the splits in the tree and thus obtain relative amounts of variable importance.</p><p class="calibre8">Intuitively, features that are highly important will tend to have been used early to split the data (and hence appear higher up in the tree, closer to the root node) as well as more often. If a feature is never used, then it is not important and in this way we can see that we have a built-in feature selection.</p><p class="calibre8">Note that this approach is sensitive to correlation in the features. When trying to determine what feature to split on, we may randomly end up picking between two highly correlated features resulting in the model using more features than necessary and as a result, the importance of these features is lower than if either had been chosen on its own. It turns out that variable importance is automatically computed by <code class="email">rpart()</code> and stored in the <code class="email">variable.importance</code> attribute on the tree model that is returned. Plotting this using <code class="email">barplot()</code> produces the following:</p><div class="mediaobject"><img src="../images/00146.jpeg" alt="Variable importance in tree models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To an <a id="id607" class="calibre1"/>experienced player of the RTS genre, this graph looks quite reasonable and intuitive. The biggest separator of skill according to this graph is the average number of game actions that a player makes in a minute (<code class="email">APM</code>). Experienced and effective players are capable of making many actions, whereas less experienced players will make fewer.</p><p class="calibre8">At first glance, this may seem to be simply a matter of acquiring so-called muscle memory and developing faster reflexes, but in actuality it is knowing which actions to carry out, and playing with strategy and planning during the game (a characteristic of better players), which also significantly increases this metric.</p><p class="calibre8">Another speed-related attribute is the <code class="email">ActionLatency</code> feature, which essentially measures the time between choosing to focus the map on a particular location on the battlefield and executing the first action at that location. Better players will spend less time looking at a map <a id="id608" class="calibre1"/>location and will be faster at selecting units, giving orders, and deciding what to do given an image of a situation in the game.</p></div></div>

<div class="book" title="Predicting complex skill learning">
<div class="book" title="Regression model trees in action"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec64" class="calibre1"/>Regression model trees in action</h2></div></div></div><p class="calibre8">We'll wrap <a id="id609" class="calibre1"/>up the experiments in this chapter with a very short demonstration of how to run a regression model tree in R, followed by some information around the notion of improving the M5 model.</p><p class="calibre8">We can do this very easily using the <code class="email">RWeka</code> package, which contains the <code class="email">M5P()</code> function. This follows the typical convention of requiring a formula and a data frame with the training data:</p><div class="informalexample"><pre class="programlisting">&gt; library("RWeka")
&gt; m5tree &lt;- M5P(LeagueIndex ~ ., data = skillcraft_train)
&gt; m5tree_predictions &lt;- predict(m5tree, skillcraft_test)
&gt; m5tree_SSE &lt;- compute_SSE(m5tree_predictions, 
                            skillcraft_test$LeagueIndex)
&gt; m5tree_SSE
[1] 714.8785</pre></div><p class="calibre8">Note that we get almost comparable performance to our tuned CART tree using the default settings. We'll leave the readers to explore this function further, but we will be revisiting this dataset once again in <a class="calibre1" title="Chapter 9. Ensemble Methods" href="part0071_split_000.html#23MNU2-c6198d576bbb4f42b630392bd61137d7">Chapter 9</a>, <span class="strong"><em class="calibre9">Ensemble Methods</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre8">A good reference on regression model trees containing several case studies is the original paper by <span class="strong"><em class="calibre9">Quinlan</em></span>, entitled <span class="strong"><em class="calibre9">Learning with continuous cases</em></span>, from the proceedings of the <span class="strong"><em class="calibre9">Australian Joint Conference on Artificial Intelligence</em></span> (1992).</p></div></div></div>
<div class="book" title="Improvements to the M5 model" id="1VSLM1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec60" class="calibre1"/>Improvements to the M5 model</h1></div></div></div><p class="calibre8">The standard <a id="id610" class="calibre1"/>M5 algorithm tree currently has been received as the most state-of-the-art model among decision trees for completing complex regression tasks. This is mainly because of the accurate results it yields as well as its ability to handle tasks with a very large number of dimensions with upwards of hundreds of attributes.</p><p class="calibre8">In an attempt to improve on or otherwise optimize the standard M5 algorithm, <span class="strong"><em class="calibre9">M5Flex</em></span> has recently been introduced as perhaps the most viable option. The M5Flex algorithm approach will attempt to <span class="strong"><em class="calibre9">augment</em></span> a standard M5 tree model with <span class="strong"><em class="calibre9">domain knowledge</em></span>. In other words, M5Flex empowers someone who has familiarity with the data population to review and choose the <span class="strong"><em class="calibre9">split attributes</em></span> and <span class="strong"><em class="calibre9">split values</em></span> for those important nodes (within the model tree) with the assumption that, since they may "know best," the resulting model will be even more accurate, consistent, and appropriate for practical applications than it would be by relying exclusively on the standard M5. One drawback or criticism to using M5Flex is that, in most cases, a domain expert may not always be available.</p><p class="calibre8">Still <a id="id611" class="calibre1"/>another attempt at improving M5 is <span class="strong"><strong class="calibre2">M5opt</strong></span>. M5opt is a semi-non-greedy algorithm utilizing the unassuming approach of not trying to solve <a id="id612" class="calibre1"/>your globally complex optimization problem holistically, or as "one entity", but rather by splitting the procedure of generating the tree layers into two distinct steps, each using a different type or nature of algorithm, based upon the layer of the tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre2">Global optimization</strong></span>: Generate <a id="id613" class="calibre1"/>upper layers of the tree (from the first layer) by using a global (multi-extremum) optimization algorithm (or a better-than-greedy approach algorithm).</li><li class="listitem" value="2"><span class="strong"><strong class="calibre2">Greedy searching</strong></span>: Generate <a id="id614" class="calibre1"/>the rest of the tree (the tree's lower layers) by using a faster "greedy algorithm" like the standard M5.</li></ol><div class="calibre13"/></div><p class="calibre8">Additionally, the layer up to which global optimization is applied could be different in different branches. However, it would be reasonable to fix it at some value for all branches; this allows for a flexible trade-off between speed and optimization. Although using the M5opt algorithm to optimize the process of constructing tree models has been shown to be successful in yielding models more accurate than those created using standard M5, the computational costs will be increased due to the nature of how "non-greedy" algorithms work. </p><p class="calibre8">To address this, one can control the cost by reviewing what tree level is "most appropriate," or which level would yield the most accuracy with the least possible cost required, and then performing the more exhaustive, non-greedy search at that level of the tree.</p><p class="calibre8">Further <a id="id615" class="calibre1"/>attempts to optimize standard M5 have been to try to combine both the M5opt and the M5flex approaches.</p><p class="calibre8">Finally, <span class="strong"><strong class="calibre2">artificial neural networks</strong></span> (<span class="strong"><strong class="calibre2">ANN's</strong></span>) discussed in <a class="calibre1" title="Chapter 5. Neural Networks" href="part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7">Chapter 5</a>, <span class="strong"><em class="calibre9">Neural Networks</em></span> have been offered as an alternative to standard M5, but only in such scenarios where the tree model is presumed to be less complex. In complex models, M5 almost always outperforms ANN.</p></div>
<div class="book" title="Summary" id="20R681-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec61" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we learned how to build decision trees for regression and classification tasks. We saw that, although the idea is simple, there are several decisions that we have to make in order to construct our tree model, such as what splitting criterion to use, as well as when and how to prune our final tree.</p><p class="calibre8">In each case, we considered a number of viable options and it turns out that there are several algorithms that are used to build decision tree models. Some of the best qualities of decision trees are the fact that they are typically easy to implement and very easy to interpret, while making no assumptions about the underlying model of the data. Decision trees have native options for performing feature selection and handling missing data, and are very capable of handling a wide range of feature types.</p><p class="calibre8">Having said that, we saw that, from a computational perspective, finding a split for categorical variables is quite expensive due to the exponential growth of the number of possible splits. In addition, we saw that categorical features can often tend to impose selection bias in splitting criteria such as information gain, because of this large number of potential splits.</p><p class="calibre8">Another drawback to using decision tree models is the fact that they can be unstable in the sense that small changes in the data can potentially alter a splitting decision high up in the tree, and consequently we can end up with a very different tree after that. Additionally, and this is particularly relevant to regression problems, because of the finite number of leaf nodes, our model may not be sufficiently granular in its output. Finally, although there are several different approaches to pruning, we should note that decision trees can be vulnerable to overfitting.</p><p class="calibre8">In the next chapter, we are not going to focus on a new type of model. Instead, we are going to look at different techniques to combine multiple models together, such as bagging and boosting. Collectively, these are known as ensemble methods. These methods have been demonstrated to be quite effective in improving the performance of simpler models, and overcoming some of the limitations just discussed for tree-based models, such as model instability and susceptibility to overfitting.</p><p class="calibre8">We'll present a well-known algorithm, AdaBoost, which can be used with a number of models that we've seen so far. In addition, we will also introduce random forests, as a special type of ensemble model specifically designed for decision trees. Ensemble methods in general are typically not easy to interpret, but for random forests we can still use the notion of variable importance that we saw in this chapter in order to get an overall idea of which features our model relies upon the most.</p></div></body></html>