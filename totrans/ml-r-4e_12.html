<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer563">
    <h1 class="chapterNumber">12</h1>
    <h1 class="chapterTitle" id="_idParaDest-269">Advanced Data Preparation</h1>
    <p class="normal">The truism that 80 percent of the time invested in real-world machine learning projects is spent on data preparation is so widely cited that it is mostly accepted without question. Earlier chapters of this book helped perpetuate the cliché by stating it as a matter of fact without qualification, and although it is certainly a common experience and perception, it is also an oversimplification, as tends to be the case when generalizing from a statistic. In reality, there is no single, uniform experience for data preparation. Yet, it is indeed true that data prep work almost always involves more effort than anticipated.</p>
    <p class="normal">Rare is the case in which you will be provided a single CSV formatted text file, which can be easily read into R and processed with just a few lines of R code, as was the case in previous chapters. Instead, necessary data elements are often distributed across databases, which must then be gathered, filtered, reformatted, and combined before the features can be used with machine learning. This can require significant effort even before considering the time expended gaining access to the data from stakeholders, as well as exploring and understanding the data.</p>
    <p class="normal">This chapter is intended to prepare you (pun intended!) for the larger and more complex datasets that you’ll be preparing in the real world. You will learn:</p>
    <ul>
      <li class="bulletList">Why data preparation is crucial to building better models</li>
      <li class="bulletList">Tips and tricks for transforming data into more useful predictors </li>
      <li class="bulletList">Specialized R packages for efficiently preparing data</li>
    </ul>
    <p class="normal">Different teams and different projects require their data scientists to invest different amounts of time preparing data for the machine learning process, and thus, the 80 percent statistic may overstate or understate the effort needed for any given project or from any single contributor. </p>
    <p class="normal">Still, whether it is you or someone else performing this work, you will soon discover the undeniable fact that advanced data preparation is a necessary step in the process of building strong machine learning projects.</p>
    <h1 class="heading-1" id="_idParaDest-270">Performing feature engineering</h1>
    <p class="normal">Time, effort, and imagination are central to the process of <strong class="keyWord">feature engineering</strong>, which involves applying subject-matter expertise to create new features for prediction. In simple terms, it might be described as the art of making data more useful. In more complex terms, it involves a combination of domain expertise and data transformations. One needs to know not just what data will be useful to gather for the machine learning project, but also how to merge, code, and clean the data to conform to the algorithm’s expectations.</p>
    <p class="normal">Feature engineering is<a id="_idIndexMarker1335"/> closely interrelated with data exploration, as described in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>. Both involve interrogating data through the generation and testing of hypotheses. Exploring and brainstorming are likely to lead to insights about which features will be useful for prediction, and the act of engineering the features may lead to new questions to explore.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_12_01.png"/></figure>
    <p class="packt_figref">Figure 12.1: Feature engineering is part of a cycle that helps the model and data work together</p>
    <p class="normal">Feature engineering is part of a cycle within a cycle in which effort is invested to help the model and data work better together. A round of data exploration and feature engineering leads to improvements to the data, which leads to iterations of training better models, which then informs another round of potential improvements to the data. These potential improvements are not only the bare minimum cleaning and preparation tasks needed to address simple data issues and allow the algorithm to run in R, but also the <a id="_idIndexMarker1336"/>steps that lead an algorithm to learn more effectively. These may include:</p>
    <ul>
      <li class="bulletList">Performing complex data transformations that help the algorithm to learn faster or to learn a simpler representation of the data</li>
      <li class="bulletList">Creating features that are easier to interpret or better represent the underlying theoretical concepts</li>
      <li class="bulletList">Utilizing unstructured data or merging additional features onto the main source</li>
    </ul>
    <p class="normal">All three of these require both intense thought and creativity, and are improvisational and domain-specific rather than formulaic. This being said, the computer and the practitioner can share this work using complementary strengths. What the computer lacks in creativity and ability to improvise, it may be able to address with computational horsepower, brute force, and unwavering persistence.</p>
    <h2 class="heading-2" id="_idParaDest-271">The role of human and machine</h2>
    <p class="normal">Feature<a id="_idIndexMarker1337"/> engineering can be viewed as a collaboration between the human and the machine during the learning process stage of abstraction. Recall that in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introducing Machine Learning</em>, the abstraction step was defined as the translation of stored data into broader concepts and representations. In other words, during abstraction, connections are made between elements of raw data, which will represent important concepts for the learning objective. These relationships are generally defined by a model, which links the learned concepts to an outcome of interest. During feature engineering, the human gently guides or nudges the abstraction process in a specific direction, with the goal of producing a better-performing model.</p>
    <p class="normal">Imagine it this way: recall an instance in your past where you attempted to learn a difficult concept—possibly even while reading this very textbook! Reading and later re-reading the text proves to be of no help to understanding the concept, and frustrated, you contact a friend or colleague for help. Perhaps this friend explains the concept in a different way, using analogies or examples that help connect the concept to your prior experience, and in doing so, it leads you to a moment of enlightenment: “Eureka!” All is suddenly clear, and you wonder how you couldn’t understand the concept in the first place. Such is the power of abstractions, which can be transferred from one learner to another to aid the learning process. The process of feature engineering allows the human to transfer their intuitive knowledge or subject-matter expertise to the machine through intentionally and purposefully designed input data.</p>
    <p class="normal">Given the fact that abstraction is the cornerstone of the learning process, it can be argued that machine learning is fundamentally feature engineering. The renowned computer scientist and artificial intelligence pioneer Andrew Ng said, “<em class="italic">Coming up with features is difficult, time-consuming, and requires expert knowledge. Applied machine learning is basically feature engineering</em>.” Pedro Domingos, a professor of computer science and author of the machine learning book <em class="italic">The Master Algorithm</em>, said that “<em class="italic">some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used</em>.”</p>
    <div class="note">
      <p class="normal">Andrew Ng’s quote appears in a lecture titled <em class="italic">Machine Learning and AI via Brain simulations</em>, which is available online via web search. In addition to Pedro Domingos’ book <em class="italic">The Master Algorithm</em> (2015), see also his excellent paper “A few useful things to know about machine learning” in <em class="italic">Communications of the ACM</em> (2012). <a href="https://doi.org/10.1145/2347736.2347755"><span class="url">https://doi.org/10.1145/2347736.2347755</span></a>.</p>
    </div>
    <p class="normal">Feature <a id="_idIndexMarker1338"/>engineering performed well can turn weaker learners into much stronger learners. Many machine learning and artificial intelligence problems can be solved with simple linear regression methods, assuming that the data has been sufficiently cleaned. Even very complex machine learning methods can be replicated in standard linear regression given sufficient feature engineering. Linear regression can be adapted to model nonlinear patterns, using splines and quadratic terms, and can approach the performance of even the most complex neural networks, given a sufficient number of new features that have been engineered as carefully designed interactions or transformations of the original input data.</p>
    <p class="normal">The idea that simple learning algorithms can be adapted to more complex problems is not limited to regression. For example, decision trees can work around their axis-parallel decision boundaries by rotations of the input data, while hyperplane-based support vector machines can model complex nonlinear patterns with a well-chosen kernel trick. A method as simple as k-NN could be used to mimic regression or perhaps even more complex methods, given enough effort and sufficient understanding of the input data and learning problem, but herein lies the catch. Why invest large amounts of time performing feature engineering to employ a simple method when a more complex algorithm will perform just as well or better, while also performing the feature engineering for us automatically?</p>
    <p class="normal">Indeed, it is probably best to match the complexity of the data’s underlying patterns with a learning algorithm capable of handling them readily. Performing feature engineering by hand when a computer can do it automatically is not only wasted effort but also prone to mistakes and missing important patterns. Algorithms like decision trees and neural networks with a sufficiently large number of hidden nodes—and especially, deep learning neural networks—are particularly capable of doing their own form of feature engineering, which is likely to be more rigorous and thorough than what can be done by hand. Unfortunately, this does not mean we can blindly apply these same methods to every task—after all, there is no free lunch in machine learning!</p>
    <p class="normal">Applying the same algorithm to every problem suggests that there is a one-size-fits-all approach to feature engineering, when we know that it is as much an art as it is a science. Consequently, if all practitioners apply the same method to all tasks, they will have no way of knowing whether better performance is possible. Perhaps a slightly different feature engineering approach could have resulted in a model that more accurately predicted churn or cancer, and would have led to greater profits or more lives saved. This is clearly a problem in the real world, where even a small performance boost can mean a substantial edge over the competition.</p>
    <p class="normal">In a <a id="_idIndexMarker1339"/>high-stakes competition environment, such as the machine learning competitions on Kaggle, each team has access to the same learning algorithms and is readily capable of rapidly applying each of them to identify which one performs best. It is no surprise, then, that a theme emerges while reading interviews with Kaggle champions: they often invest significant effort into feature engineering. Xavier Conort, who was the top-rated data scientist on Kaggle in 2012–2013, said in an interview that:</p>
    <blockquote class="packt_quote">
      <p class="quote">”The algorithms we used are very standard for Kagglers… We spent most of our efforts on feature engineering… We were also very careful to discard features likely to expose us to the risk of overfitting.”</p>
    </blockquote>
    <p class="normal">Because feature engineering is one of the few proprietary aspects of machine learning, it is one of the few points of distinction across teams. In other words, teams that perform feature engineering well tend to outperform the competition.</p>
    <div class="note">
      <p class="normal">To read the full interview with Xavier Conort, which was originally posted on the Kaggle “No Free Hunch” blog, visit <a href="https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/"><span class="url">https://web.archive.org/web/20190609154949/http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/</span></a>. Interviews with other Kaggle champions are available at <a href="https://medium.com/kaggle-blog/tagged/kaggle-competition"><span class="url">https://medium.com/kaggle-blog/tagged/kaggle-competition</span></a>.</p>
    </div>
    <p class="normal">Based on Conort’s statement, it would be easy to assume that the need to invest in feature engineering necessitates greater investment in human intelligence and the application of subject-matter expertise, but this is not always true. Jeremy Achin, a member of a top-performing “DataRobot” team on Kaggle, remarked on the surprisingly limited utility of human expertise. Commenting on his team’s time spent on feature engineering, he said in an interview that:</p>
    <blockquote class="packt_quote">
      <p class="quote">”The most surprising thing was that almost all attempts to use subject matter knowledge or insights drawn from data visualization led to drastically worse results. We actually arranged a 2-hour whiteboard lecture from a very talented biochemist and came up with some ideas based on what we learned, but none of them worked out.”</p>
    </blockquote>
    <p class="normal">Jeremy <a id="_idIndexMarker1340"/>Achin, along with Xavier Conort and several other high-profile Kaggle Grand Masters, bootstrapped their Kaggle competition successes into an artificial intelligence company called DataRobot, which is now worth billions of dollars. Their software performs machine learning automatically, suggesting that a key lesson learned from their Kaggle work was that computers can perform many steps in the machine learning process just as well as humans, if not better.</p>
    <div class="note">
      <p class="normal">To read the full interview with Jeremy Achin, which was originally posted on the Kaggle “No Free Hunch” blog, visit <a href="https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/"><span class="url">https://web.archive.org/web/20190914030000/http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/</span></a>. The DataRobot company is found on the web at <a href="https://www.datarobot.com"><span class="url">https://www.datarobot.com</span></a>.</p>
    </div>
    <p class="normal">Of course, there is a balance between building models piece by piece using subject-matter expertise and throwing everything at the machine to see what sticks. Although feature engineering today is largely still a manual process, the future of the field seems to be headed toward the scattershot “see what sticks” approach, as <strong class="keyWord">automated feature engineering</strong> is <a id="_idIndexMarker1341"/>a rapidly growing area of research. The foundation of automated feature engineering tools is the idea that a computer can make up for its lack of creativity and domain knowledge by testing many more combinations of features than a human would ever have time to attempt. Automated feature engineering exchanges narrow-but-guided human thought for broad-and-systematic computer thought, with the potential upside of finding a more optimal solution and potential downsides including loss of interpretability and greater likelihood of overfitting.</p>
    <p class="normal">Before getting too<a id="_idIndexMarker1342"/> excited about the potential for automation, it is worth noting that while such tools may allow a human to outsource certain parts of <em class="italic">thinking</em> about feature engineering, effort must still be invested in the <em class="italic">coding</em> part of the task. That is to say, time that was once spent hand-coding features one by one is instead spent coding functions that systematically find or construct useful features. </p>
    <p class="normal">There are promising algorithms in development, such as the Python-based <code class="inlineCode">Featuretools</code> package (and corresponding R package <code class="inlineCode">featuretoolsR</code>, which interacts with the Python code), that may help automate the feature-building process, but the use of such tools is not yet widespread. Additionally, such methods must be fed by data and computing time, both of which may be limiting factors in many machine learning projects.</p>
    <div class="note">
      <p class="normal">For more information on <a id="_idIndexMarker1343"/>Featuretools, visit: <a href="https://www.featuretools.com"><span class="url">https://www.featuretools.com</span></a>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-272">The impact of big data and deep learning</h2>
    <p class="normal">Whether feature <a id="_idIndexMarker1344"/>engineering is performed by a human or by automated machine methods, a point is inevitably reached at which additional invested effort leads to little or no boost to the learning algorithm’s performance. The application of more sophisticated learning algorithms may also improve the model’s performance somewhat, but this is also subject to diminishing returns, as there exists only a finite number of potential methods to apply and their performance differences tend to be relatively minor. Consequently, if additional performance gains are truly necessary, we are left with one remaining option: increasing the size of the training dataset with additional features or examples. Moreover, because adding additional columns would require revising data generated by past business processes, in many cases, collecting more rows is the easier option of the two.</p>
    <p class="normal">In practice, there is a relatively low ceiling on the performance gains achievable via the inclusion of more rows of data. Most algorithms described in this book plateau quickly and will perform little better on a dataset of 1 million rows than on a dataset containing a few thousand. You may have already observed this firsthand if you’ve applied machine learning methods to real-world projects in your own areas of interest. Once a dataset is big enough—often just a few thousand rows for many real-world applications—additional examples merely cause additional problems, such as extended computation time and running out of memory. If more data causes more problems, then the natural follow-up question is, why is there so much hype around the so-called “big data” era?</p>
    <p class="normal">To answer<a id="_idIndexMarker1345"/> this question, we must first begin by making a philosophical distinction between datasets of various sizes. To be clear, “big data” does not merely imply a large number of rows or a large amount of storage consumed in a database or filesystem. In fact, it comprises both of these and more, as size is just one of four elements that may indicate the presence of big data. </p>
    <p class="normal">These are the so-called <strong class="keyWord">four V’s of big data</strong>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Volume</strong>: The <a id="_idIndexMarker1346"/>literal size of the data, whether it be more rows, more columns, or more storage</li>
      <li class="bulletList"><strong class="keyWord">Velocity</strong>: The <a id="_idIndexMarker1347"/>speed at which data accumulates, which impacts not only the volume but also the complexity of data processing</li>
      <li class="bulletList"><strong class="keyWord">Variety</strong>: The differences<a id="_idIndexMarker1348"/> in types or definitions of data across different systems, particularly the addition of unstructured sources such as text, images, and audio data</li>
      <li class="bulletList"><strong class="keyWord">Veracity</strong>: The<a id="_idIndexMarker1349"/> trustworthiness of the input data and the ability to match data across sources</li>
    </ul>
    <p class="normal">Reading this list from top to bottom, the elements become less intuitively obvious, yet are more challenging to handle when encountered. The first two elements, volume and velocity, are the basis of what might be dubbed the <strong class="keyWord">medium data</strong> space. While this is not to say that there aren’t challenges working with high-volume, high-velocity data, these challenges can often be solved by scaling up what we are already doing. For example, it may be possible to use faster computers with more memory or apply a more computationally efficient algorithm. The presence of a greater variety and reduced veracity of data requires a completely different approach for use in machine learning projects, especially at high-velocity and high-volume scales. The following table lists some of the distinctions between the small, medium, and big data spaces:</p>
    <p class="normal"><img alt="Table  Description automatically generated" src="../Images/B17290_12_02.png"/></p>
    <p class="packt_figref">Figure 12.2: Most machine learning projects are on the scale of “medium data” while additional skills and tools are required to make use of “big data” </p>
    <p class="normal">Moving from<a id="_idIndexMarker1350"/> small to medium and then from medium to big data requires exponential investment. As datasets increase in size and complexity, the required infrastructure becomes much more complex, adding increasingly specialized databases, computing hardware, and analysis tools, some of which will be covered in <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Use of Big Data</em>. These tools are rapidly changing, which necessitates constant training and re-skilling. With the increased scale of data, time becomes a more significant constraint; not only are the projects more complex with many more moving pieces, requiring more cycles of iteration and refinement, but the work simply takes longer to complete—literally! A machine learning algorithm that runs in minutes on a medium-sized dataset may take hours or days on a much larger dataset, even with the benefit of cloud computing power.</p>
    <p class="normal">Given the high stakes of big data, there is often an order of magnitude difference in how such projects are staffed and resourced—it is simply considered part of “the cost of doing business.” There may be dozens of data scientists, with matching numbers of IT professionals supporting the required infrastructure and data processing pipeline. Typical big data solutions require numerous tools and technologies to work together. This<a id="_idIndexMarker1351"/> creates an opportunity for <strong class="keyWord">data architects</strong> to <a id="_idIndexMarker1352"/>plan and structure the various computing resources and to monitor their security, performance, and cloud hosting costs. Similarly, data scientists are often matched by an equal or greater number of <strong class="keyWord">data engineers</strong>, who are responsible for piping <a id="_idIndexMarker1353"/>data between sources and doing the most complex programming work. Their efforts in processing large datasets allow data scientists to focus on analysis and machine learning model building.</p>
    <p class="normal">From the perspective of those working on the largest and most challenging machine learning projects today, many everyday projects, including nearly all the examples covered in this book, fall squarely into what has been<a id="_idIndexMarker1354"/> called a <strong class="keyWord">small data regime</strong>. In this paradigm, datasets can grow to be “large” in terms of the number of rows or in sheer storage volume, but they will never truly be “big data.” Computer science and machine learning expert Andrew Ng has noted that in the realm of small data, the role of the human is still impactful; the human can greatly impact a project’s performance via hand-engineering features or by the selection of the most performant learning algorithm. However, as a dataset grows beyond “large” and into “huge” sizes and into <a id="_idIndexMarker1355"/>the <strong class="keyWord">big data regime</strong>, a different class of algorithms breaks through the performance plateau to surpass the small gains of manual tweaks. </p>
    <p class="normal"><em class="italic">Figure 12.3</em>, which is adapted from Ng’s work, illustrates this phenomenon:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_12_03.png"/></figure>
    <p class="packt_figref">Figure 12.3: In the small data regime, traditional machine learning algorithms are competitive with and may even perform better than more complex methods, which perform much better as the size of data increases</p>
    <p class="normal">Within the <a id="_idIndexMarker1356"/>confines of the small data regime, no single algorithm or class of algorithms performs predictably better than the others. Here, clever feature engineering including subject-matter expertise and hand-coded features may allow simpler algorithms to outperform much more sophisticated approaches or deep learning neural networks.</p>
    <p class="normal">As the size of data increases to the medium data regime, ensemble approaches (described in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>) tend to perform better than even a carefully handcrafted model that uses traditional machine learning algorithms. For the largest datasets found in the big data regime, only deep learning neural networks (introduced in <em class="chapterRef">Chapter 7</em>, <em class="italic">Black-Box Methods – Neural Networks and Support Vector Machines</em>, and to be covered in more detail in <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Use of Big Data</em>) appear to be capable of the utmost performance, as their capability to learn from additional data practically never plateaus. Does this imply that the “no free lunch” theorem is incorrect and there truly is one learning algorithm to rule them all?</p>
    <div class="note">
      <p class="normal">The visualization of the performance of different learning algorithms under the small and big data regimes can be found described by Andrew Ng in his own words. To find these, simply search YouTube for “Nuts and Bolts of Applying Deep Learning” (appears 3 minutes into the video) or “Artificial Intelligence is the New Electricity” (appears 20 minutes into the video).</p>
    </div>
    <p class="normal">To understand why certain algorithms perform better than others under the big data regime and why the “no free lunch” principle still applies, we must first consider the relationship between the size and complexity of the data, the capability of a model to learn a complex pattern, and the risk of overfitting. Let’s begin by considering a case in which the size and complexity of the data is held constant, but we increase the complexity of the learning algorithm to more closely model what is observed in the training data. For example, we may grow a decision tree to an overly large size, increase the number of predictors in a regression model, or add hidden nodes in a neural network. This relationship is closely linked to the idea of the bias-variance trade-off; by increasing the model complexity, we allow the model to conform more closely to the training data and, therefore, reduce its inherent bias and increase its variance.</p>
    <p class="normal"><em class="italic">Figure 12.4</em> illustrates the typical pattern that occurs as models increase in complexity. Initially, when the model is underfitted to the training dataset, increases in model complexity lead to reductions in model error and increases in model performance. However, there is a point at which increases in model complexity contribute to overfitting the training dataset. Beyond this point, although the model’s error rate on the training dataset continues to be reduced, the test set error rate increases, as the model’s ability to generalize beyond training is dramatically hindered. Again, this assumes a limit to the dataset’s ability to support the model’s increasing complexity.</p>
    <figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" src="../Images/B17290_12_04.png"/></figure>
    <p class="packt_figref">Figure 12.4: For many training datasets, increasing the complexity of the learning algorithm runs the risk of overfitting and increased test set error</p>
    <p class="normal">If we can<a id="_idIndexMarker1357"/> increase the size and scope of the training dataset, the big data regime may unlock a second tier of machine learning performance, but only if the learning algorithm is likewise capable of increasing its complexity to make use of the additional data. Many traditional algorithms, such as those covered so far in this book, are incapable of making such an evolutionary leap—at least not without some extra help.</p>
    <p class="normal">The missing link between the traditional machine learning algorithms and those capable of making this leap has to do with the number of parameters that the algorithms attempt to learn about the data. Recall that in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>, parameters were described as the learner’s internal values that represent its abstraction of the data. Traditionally, for a variety of reasons, including the bias-variance trade-off depicted above, as well as the belief that simpler, more parsimonious models should be favored over more complex ones, models with fewer parameters have been favored. It was assumed that increasing the number of parameters too high would allow the dataset to simply memorize the training data, leading to severe overfitting.</p>
    <p class="normal">Interestingly, this is<a id="_idIndexMarker1358"/> true, but only to a point, as <em class="italic">Figure 12.5</em> depicts. As model complexity—that is, the number of parameters—increases, the test set error follows the same U-shaped pattern as before. However, a new pattern emerges once complexity and parameterization have <a id="_idIndexMarker1359"/>reached the <strong class="keyWord">interpolation threshold</strong>, or the point at which there are enough parameters to memorize and accurately classify virtually all the training set examples. At this threshold, generalization error is at its maximum, as the model has been greatly overfitted to the training data. However, as model complexity increases even further, test set error once again begins to drop. With sufficient additional complexity, a heavily overfitted model may even surpass the performance of a well-tuned traditional model, at least according to our existing notion of “overfitted.”</p>
    <figure class="mediaobject"><img alt="Chart  Description automatically generated with low confidence" src="../Images/B17290_12_05.png"/></figure>
    <p class="packt_figref">Figure 12.5: Some algorithms are able to make use of big data to generalize well even after they seemingly overfit the training data</p>
    <div class="note">
      <p class="normal">For more information on the apparent contradiction of the “double descent” curve depicted here, see this groundbreaking paper: <em class="italic">Reconciling modern machine-learning practice and the classical bias–variance trade-off, Belkin M, Hsu D, Ma S, and Mandal S, 2019, Proceedings of the National Academy of Sciences, Vol. 116(32), pp. 15,849-15,854</em>.</p>
    </div>
    <p class="normal">The mechanism that explains this unexpected result has to do with an interesting and perhaps even magical transformation that occurs in models capable of additional parameterization beyond the interpolation threshold. Once a learner has sufficient parameters to interpolate (to sufficiently conform to) the training data, additional parameters lead to a <a id="_idIndexMarker1360"/>state of <strong class="keyWord">overparameterization</strong>, in which the additional complexity enables higher levels of thinking and abstraction. In essence, an overparameterized learner is capable of learning higher-order concepts; in practice, this means it is capable of learning how to engineer features or learning how to learn. A significant jump in model complexity beyond the interpolation threshold is likely to lead to a significant leap in the way the algorithm approaches the problem, but of course, not every algorithm is capable of this leap.</p>
    <p class="normal">Deep neural networks, which can add additional complexity endlessly and trivially via the addition of hidden nodes arranged in layers, are the ideal candidate for consuming big data. As you will learn in <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Use of Big Data</em>, a cleverly designed neural network can engineer its own features out of unstructured data such as images, text, or audio. Similarly, its designation as a universal function approximator implies that it can identify the best functional form to model any pattern it identifies in the data. Thus, we must once again revisit the early question of how exactly this doesn’t violate the principle of “no free lunch.” It would appear that for datasets of sufficient size, deep learning neural networks are the single best approach.</p>
    <p class="normal">Putting a couple of practical issues aside—notably, the fact that most real-world projects reside in the small data regime and the fact that deep neural networks are computationally expensive and difficult to train—a key reason that deep learning doesn’t violate the “no free lunch” principle is based on the fact that once the neural network becomes large and substantially overparameterized, and assuming it has access to a sufficiently large and complex training dataset, it ceases to be a single learning <em class="italic">algorithm</em> and instead becomes a generalized learning <em class="italic">process</em>. If this seems like a distinction without a difference, perhaps a metaphor will help: rather than providing us with a free lunch, the<a id="_idIndexMarker1361"/> process of deep learning provides an opportunity to teach the algorithm how to make its own lunch. Given the limited availability of truly big data and the limited applicability of deep learning to most business tasks, to produce the strongest models, it is still necessary for the machine learning practitioner to assist in the feature engineering process.</p>
    <h1 class="heading-1" id="_idParaDest-273">Feature engineering in practice</h1>
    <p class="normal">Depending on the project <a id="_idIndexMarker1362"/>or circumstances, the practice of feature engineering may look very different. Some large, technology-focused companies employ one or more data engineers per data scientist, which allows machine learning practitioners to focus less on data preparation and more on model building and iteration. Certain projects may rely on very small or very massive quantities of data, which may preclude or necessitate the use of deep learning methods or automated feature engineering techniques. Even projects requiring little initial feature engineering effort may suffer from the so-called “last mile problem,” which describes the tendency for costs and complexity to be disproportionally high for the small distances to be traveled for the “last mile” of distribution. Relating this concept to feature engineering implies that even if most of the work is taken care of by other teams or automation, a surprising amount of effort may still be required for the final steps of preparing the data for the model.</p>
    <p class="normal">It is likely that the bulk of real-world machine learning projects today require a substantial amount of feature engineering. Most companies have yet to achieve the level of analytics maturity at the organizational level needed to allow data scientists to focus solely on model building. Many companies and projects will never achieve this level due to their small size or limited scope. For many small-to-mid-sized companies and small-to-mid-sized projects, data scientists must take the lead on all aspects of the project from start to finish. Consequently, it is necessary for data scientists to understand the role of the feature engineer and prepare to perform this role if needed.</p>
    <p class="normal">As stated previously, feature engineering is more art than science and requires as much imagination as it does programming skills. In a nutshell, the three main goals of feature engineering might be described as:</p>
    <ul>
      <li class="bulletList">Supplementing<a id="_idIndexMarker1363"/> what data is already available with additional external sources of information</li>
      <li class="bulletList">Transforming the data to conform to the machine learning algorithm’s requirements and to assist the model with its learning</li>
      <li class="bulletList">Eliminating the noise while minimizing the loss of useful information— conversely, maximizing the use of available information</li>
    </ul>
    <p class="normal">An overall mantra to keep in mind when practicing feature engineering is “be clever.” One should strive to be a clever, frugal data miner and try to think about the subtle insights that you might find in every single feature, working systematically, and avoiding letting any data go to waste. Applying this rule serves as a reminder of the requisite creativity and helps to inspire the competitive spirit needed to build the strongest-performing learners. </p>
    <p class="normal">Although each project will require you to apply these skills in a unique way, experience will reveal certain patterns that emerge in many types of projects. The sections that follow, which provide seven “hints” for the art of feature engineering, are not intended to be exhaustive but, rather, provide a spark of inspiration on how to think <a id="_idIndexMarker1364"/>creatively about making data more useful.</p>
    <div class="note">
      <p class="normal">There has been an unfortunate dearth of feature engineering books on the market, until recently, when a number have been published. Two of the earliest books on this subject are Packt Publishing’s <em class="italic">Feature Engineering Made Easy</em> (Ozdemir &amp; Susara, 2018) and O’Reilly’s <em class="italic">Feature Engineering for Machine Learning</em> (Zheng &amp; Casari, 2018). The book <em class="italic">Feature Engineering and Selection</em> (Kuhn &amp; Johnson, 2019) is also a standout and even has a free version, available on the web at: <a href="http://www.feat.engineering"><span class="url">http://www.feat.engineering</span></a>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-274">Hint 1: Brainstorm new features</h2>
    <p class="normal">The choice of topic for<a id="_idIndexMarker1365"/> a new machine learning project is typically motivated by an unfulfilled need. It may be motivated by a desire for more profit, to save lives, or even simple curiosity, but in any case, the topic is almost surely not selected at random. Instead, it relates to an issue at the core of the company, or a topic held dear by the curious, both of which suggest a fundamental interest in the work. The company or individual pursuing the project is likely to already know a great deal about the subject and the important factors that contribute to the outcome of interest. With this domain experience and subject-matter expertise, the company, team, or individual that commissioned the project is likely to hold proprietary insights about the task that they alone can bring.</p>
    <p class="normal">To capitalize on these insights, at the beginning of a machine learning project, just prior to feature engineering, it can be helpful to conduct a brainstorming session in which stakeholders are gathered and ideas are generated about the potential factors that are associated with the outcome of interest. During this process, it is important to avoid limiting yourself to what is readily available in existing datasets. Instead, consider the process of cause-and-effect at a more abstract level, imagining the various metaphorical “levers” that can be pulled in order to impact the outcome in a positive or negative direction. Be as thorough as possible and exhaust all ideas during this session. If you could have literally anything you wanted in the model, what would be most useful?</p>
    <p class="normal">The culmination of a <a id="_idIndexMarker1366"/>brainstorming session may be a <strong class="keyWord">mind map</strong>, which is a method of diagramming ideas around a central topic. Placing the <a id="_idIndexMarker1367"/>outcome of interest at the center of the mind map, the various potential predictors radiate out from the central theme, as shown in the following example of a mind mapping session designing a model to predict heart disease mortality. </p>
    <p class="normal">A mind map diagram may use a hierarchy to link associated concepts or group factors that are related in a similar data source:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_12_06.png"/></figure>
    <p class="packt_figref">Figure 12.6: Mind maps can be useful to help imagine the factors that contribute to an outcome</p>
    <p class="normal">While constructing the mind map, you may determine that some of the desired features are unavailable in the existing data sources. Perhaps the brainstorming group can help identify alternative sources of these data elements or find someone willing to help gather them. Alternatively, it may be possible to develop <a id="_idIndexMarker1368"/>a <strong class="keyWord">proxy measure</strong> that effectively measures the same concept using a different method. For example, it may be impossible or practically infeasible to directly measure someone’s diet, but it may be possible to use their social media activity as a proxy by counting the number of fast-food restaurants they follow. This is not perfect, but it is something, and is certainly better than nothing.</p>
    <p class="normal">A mind <a id="_idIndexMarker1369"/>mapping session can also help reveal potential interactions between features in which two or more factors have a disproportionate impact on the outcome; a joint effect may be greater (or lesser) than the sum of its parts. In the heart disease example, one might hypothesize that the combined effect of stress and obesity is substantially more likely to cause heart disease than the sum of their separate effects. Algorithms such as decision trees and neural networks can find these interaction effects automatically, but many others cannot, and in either case, it may <a id="_idIndexMarker1370"/>benefit the learning process or result in a simpler, more interpretable model if these combinations are coded explicitly in the data.</p>
    <h2 class="heading-2" id="_idParaDest-275">Hint 2: Find insights hidden in text</h2>
    <p class="normal">One<a id="_idIndexMarker1371"/> of the richest sources of hidden data and, therefore, one of the most fruitful areas for feature engineering is text data. Machine learning algorithms are generally not very good at realizing the full value of text data because they lack the external knowledge of semantic meaning that a human has gained over a lifetime of language use. </p>
    <p class="normal">Of course, given a tremendous amount of text data, a computer may be able to learn the same thing, but this is not feasible for many projects, and would greatly add to a project’s complexity. Furthermore, text data cannot be used as-is, as it suffers from the curse of dimensionality; each block of text is unique and, therefore, serves as a form of fingerprint linking the text to an outcome. If used in the learning process, the algorithm will severely overfit or ignore the text data altogether.</p>
    <div class="note">
      <p class="normal">The curse of dimensionality applies to unstructured “big” data more generally as image and audio data are likewise difficult to use directly in machine learning models. <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Use of Big Data</em>, covers some methods that allow these types of data sources to be used with traditional machine learning approaches.</p>
    </div>
    <p class="normal">The humans in charge of constructing features for the learning algorithm can add insight to the text data by coding reduced-dimensionality features derived from the interpretation of the text. In selecting a small number of categories, the implicit meaning is made explicit. For example, in a customer churn analysis, suppose a company has access to the public Twitter timeline for its customers. Each customer’s tweets are unique, but a human may be able to code them into three categories of positive, negative, and neutral. This is a simple form<a id="_idIndexMarker1372"/> of <strong class="keyWord">sentiment analysis</strong>, which analyzes the emotion of language. Computer software, including some R packages, may be able to help automate this process using models or rules designed to understand simple semantics. In addition to sentiment analysis, it may be possible to categorize text data by topic; in the churn example, perhaps customers tweeting about customer service are more likely to switch to another company than customers tweeting about price.</p>
    <div class="note">
      <p class="normal">There are many R packages that can perform sentiment analysis, some of which require subscriptions to paid services. To get started quickly and easily, check out the aptly named <code class="inlineCode">SentimentAnalysis</code> and <code class="inlineCode">RSentiment</code> packages, as well as the <code class="inlineCode">Syuzhet</code> package. All of these can classify sentences as positive or negative with just a couple of lines of R code. For a deeper dive into text mining and sentiment analysis, see the book <em class="italic">Text Mining with R: A Tidy Approach, 2017, Silge J and Robinson D</em>, which is available on the web at <a href="https://www.tidytextmining.com"><span class="url">https://www.tidytextmining.com</span></a>. Additionally, see <em class="italic">Text Mining in Practice with R, 2017, Kwartler T</em>. </p>
    </div>
    <p class="normal">Beyond <a id="_idIndexMarker1373"/>coding the overt meaning of text, one of the subtle arts of feature engineering involves finding the covert insights hidden in the text data. In particular, there may be useful information encoded in the text that is not related to the direct interpretation of the text, but it appears in the text coincidentally or accidentally, like a “tell” in the game of poker—a micro-expression that reveals the player’s secret intentions.</p>
    <p class="normal">Hidden text data may help reveal aspects of a person’s identity, such as age, gender, career level, location, wealth, or socioeconomic status. Some examples include:</p>
    <ul>
      <li class="bulletList">Names and salutations such as Mr. and Mrs., or Jr. and Sr., traditional and modern names, male and female names, or names associated with wealth</li>
      <li class="bulletList">Job titles and categories such as CEO, president, assistant, senior, or director</li>
      <li class="bulletList">Geographic and spatial codes such as postal codes, building floor numbers, foreign and domestic regions, first-class tickets, PO boxes, and similar</li>
      <li class="bulletList">Linguistic markers such as slang or other expressions that may reveal pertinent aspects of identities</li>
    </ul>
    <p class="normal">To begin searching for these types of hidden insights, keep the outcome of interest in mind while systematically reviewing the text data. Read as many of the texts as possible while thinking <a id="_idIndexMarker1374"/>about any way in which the text might reveal a subtle clue that could impact the outcome. When a pattern emerges, construct a feature based on the insight. For instance, if the text data commonly includes job titles, create rules to classify the jobs into career levels such as entry-level, mid-career, and executive. These career levels could then be used to predict outcomes such as loan default or churn likelihood.</p>
    <h2 class="heading-2" id="_idParaDest-276">Hint 3: Transform numeric ranges</h2>
    <p class="normal">Certain learning algorithms are <a id="_idIndexMarker1375"/>more capable than others of learning from numeric data. Among algorithms that can utilize numeric data at all, some are better at learning the important cut points in the range of numeric values or are better at handling severely skewed data. Even a method like decision trees, which is certainly apt at using numeric features, has a tendency to overfit on numeric data and, thus, may benefit from a transformation that reduces the numeric range into a smaller number of potential cut points. Other methods like regression and neural networks may benefit from nonlinear transformations of numeric data, such as log scaling, normalization, and step functions.</p>
    <p class="normal">Many of these methods have been covered and applied in prior chapters. For example, in <em class="chapterRef">Chapter 4</em>, <em class="italic">Probabilistic Learning – Classification Using Naive Bayes</em>, we considered the technique of discretization (also known as “binning” or “bucketing”) as a means of transforming numeric data into categorical data so that it could be used by the naive Bayes algorithm. This technique is also sometimes useful for learners that can handle numeric data natively, as it can help clarify a decision boundary.</p>
    <p class="normal">The following figure illustrates this process for a hypothetical model predicting heart disease, using a numeric age predictor. On the left, we see that as the numeric age increases, the darker the color becomes, indicating a greater prevalence of heart disease with increasing age. Despite this seemingly clear trend, a decision tree model may struggle to identify an appropriate cut point and it may do so arbitrarily, or it may choose numerous small cut points; both of these are likely to be overfitted to the training data. Instead of leaving this choice to the model, it may be better to use <em class="italic">a priori</em> knowledge to create predefined groups for “young” and “old” patients. Although this loses some of the nuances of the true underlying gradient, it may help the model generalize <a id="_idIndexMarker1376"/>better to future data by trading the decision tree’s “high variance” approach for a “high bias” approach of theory-driven discretization.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_12_07.png"/></figure>
    <p class="packt_figref">Figure 12.7: Discretization and other numeric transformations can help learners identify patterns more easily</p>
    <p class="normal">In general, for datasets containing numeric features, it may be worth exploring each feature systematically, while also considering the learning algorithm’s approach to numeric data, to determine whether a transformation is necessary. Apply any domain or subject-matter expertise you may have to inform the creation of bins, buckets, step points, or nonlinear transformations in the final version of the feature. Even though many algorithms are <a id="_idIndexMarker1377"/>capable of handling numeric data without recoding or transformation, additional human intelligence may help guide the model to a better overall fit.</p>
    <h2 class="heading-2" id="_idParaDest-277">Hint 4: Observe neighbors’ behavior</h2>
    <p class="normal">One of the lesser-known <a id="_idIndexMarker1378"/>methods of surfacing hidden insights during feature engineering is to apply the common knowledge that “birds of a feather flock together.” We applied this principle to prediction in <em class="chapterRef">Chapter 3</em>, <em class="italic">Lazy Learning – Classification Using Nearest Neighbors</em>, but it is also a useful mindset for identifying useful predictors. The idea hinges on the fact that there may be explicit or implicit groupings across the dataset’s rows, and there may be insights found by examining how one example relates to the others in its neighborhood or grouping.</p>
    <p class="normal">An example of an explicit grouping found often in real-world data is households. Many datasets include not only rows based on individuals but also a household identifier, which allows you to link rows into household groups and, thus, create new features based on the groups’ compositions. </p>
    <p class="normal">For instance, knowing that someone is in a household may provide an indication of marital status and the number of children or dependents, even if these features were not included in the original individual-level dataset. Simply counting or aggregating some of the group’s features can result in highly useful predictors.</p>
    <p class="normal">From here, it is also possible to share information among records in the groups. For instance, knowing one spouse’s income is helpful, but knowing both provides a better indication of the total available income. Measures of variance within a group can also be enlightening. There may be aspects of households that provide a bonus effect if the partners match or disagree on certain attributes; for example, if both partners report satisfaction with a particular telephone company, they may be especially loyal compared to households where only one member is satisfied.</p>
    <p class="normal">These principles also apply to less obvious but still explicit groupings, like postal codes or geographic regions. By collecting the rows falling into the group, one can count, sum, average, take the maximum or minimum value, or examine the diversity within the group to construct new and potentially useful predictors. Groups with more or less agreement or diversity may be more or less robust to certain outcomes.</p>
    <p class="normal">There may be value in identifying implicit groupings as well—that is, a grouping not directly coded in the dataset. Clustering methods, such as those described in <em class="chapterRef">Chapter 9</em>, <em class="italic">Finding Groups of Data – Clustering with k-means</em>, are one potential method of finding these types of groupings, and the resulting clusters can be used directly as a predictor in the model. For example, in a churn project, using clusters as features for the model may reveal that some clusters are more likely to churn than others. This may imply that churn is related to the cluster’s underlying demographics, or that churn is somewhat contagious among cluster members.</p>
    <p class="normal">In other words, if birds of a feather flock together, it makes sense to borrow leading indicators from the experiences of similar neighbors—they may have a similar reaction to some external factor or may directly influence one another. Implicit groups that exhibit rare or unique traits may be interesting in themselves; perhaps some are the bellwether or “canary in the coal mine”—trendsetters that respond to change earlier than other groups. Observing their behavior and coding these groups explicitly into the model may<a id="_idIndexMarker1379"/> improve the model’s predictive ability.</p>
    <div class="packt_tip">
      <p class="normal">If you do use information from neighbors (or from related rows, as described in the next section), beware of the problem of data leakage, which was described in <em class="chapterRef">Chapter 11</em>, <em class="italic">Being Successful with Machine Learning</em>. Be sure to only engineer features using information that will be available at the time of prediction when the model is deployed. For example, it would be unwise to use both spouses’ data for a credit scoring model if only one household member completes the loan application and the other spouse’s data is added after the loan is approved.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-278">Hint 5: Utilize related rows</h2>
    <p class="normal">The practice of utilizing “follow the leader” behavior as hinted in the previous section can be especially powerful given the related rows of time series datasets, where the same attribute is measured repeatedly at different points in time. Data that contains repeated measures offers many<a id="_idIndexMarker1380"/> such additional opportunities to construct useful predictors. Whereas the previous section considered grouping related data <em class="italic">across</em> the unit of analysis, the current section considers the value of grouping related observations <em class="italic">within</em> the unit of analysis. Essentially, by observing the same units of analysis repeatedly, we can examine their prior trends and make better predictions of the future.</p>
    <p class="normal">Revisiting the hypothetical churn example, suppose we have access to the past 24 months of data from subscribers to an online video streaming service. The unit of observation is the customer-month (one row per customer per month), while our unit of analysis is the customer. Our goal is to predict which customers are most likely to churn so that we might intervene. To construct a dataset for machine learning, we must collect the units of observation and aggregate them into one row per customer. Here is where feature engineering is especially needed. In the process of “rolling up” the historical data into a single row for analysis, we can construct features that examine trends and loyalty, asking questions such as:</p>
    <ul>
      <li class="bulletList">Is the customer’s average monthly activity greater than or less than their peers?</li>
      <li class="bulletList">What is the customer’s monthly activity over time? Is it up, down, or stable?</li>
      <li class="bulletList">How frequent is their activity? Are they loyal? Is their loyalty stable across months?</li>
      <li class="bulletList">How consistent is the customer’s behavior? Does the behavior vary a lot from month to month?</li>
    </ul>
    <p class="normal">If you are familiar with basic calculus, it may help to reflect on the concept of the first and second derivative, as both can be useful features in a time series model. The first derivative here refers to the velocity of the behavior—that is, the behavior count over a unit of time. For example, we may compute the number of dollars spent per month on the streaming service, or the number of television shows and movies streamed per month. These are useful predictors alone, but they can be made even more useful in the context of the second derivative, which is the acceleration (or deceleration) of the behavior. The acceleration is the change in velocity over time, such as the change in monthly spending or the change in the shows streamed per month. High-velocity customers with high spending and usage might be less likely to churn, but a rapid deceleration (that is, a large reduction in usage or spending) from these same customers might indicate an impending churn.</p>
    <p class="normal">In addition to velocity <a id="_idIndexMarker1381"/>and acceleration, measures of consistency, reliability, and variability can be constructed to further enhance predictive ability. A very consistent behavior that suddenly changes may be more concerning than a wildly varying behavior that changes similarly. Calculating the proportion of recent months with a purchase, or with spending or behavior meeting a given threshold, provides a simple loyalty metric, but more sophisticated measures using variance are also possible.</p>
    <h2 class="heading-2" id="_idParaDest-279">Hint 6: Decompose time series</h2>
    <p class="normal">The repeated measures <a id="_idIndexMarker1382"/>time series data described in the previous section, with multiple related rows per unit of analysis, is said to be in the <strong class="keyWord">long format</strong>. This contrasts with the type of data required for most R-based machine learning methods. Unless a learning algorithm is designed to understand the related rows of repeated measures data, it will require time series data to be specified in the <strong class="keyWord">wide format</strong>, which transposes the repeated rows of data into repeated columns. For example, if a weight measurement is recorded monthly for 3 months for 1,000 patients, the long-format dataset will have 3 * 1,000 = 3,000 rows and 3 columns (patient identifier, month, and weight). As depicted in <em class="italic">Figure 12.8</em>, the same dataset in wide format would contain only 1,000 rows but 4 columns: 1 column for the patient identifier, and 3 columns for the monthly weight readings:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_12_08.png"/></figure>
    <p class="packt_figref">Figure 12.8: Most machine learning models require long-format time series data to be transformed into a wide format</p>
    <p class="normal">To construct a wide format dataset, one must first determine how much history will be useful for prediction. The more history that is needed, the more columns that will need to be added to the wide dataset. For example, if we wanted to forecast a customer’s energy usage 1 month into the future, we may decide to use their prior 12 months of energy use as predictors so that a full year of seasonality would be covered. Therefore, to build a model forecasting energy usage in June 2023, we might create 12 predictor columns measuring energy use in May 2023, April 2023, March 2023, and so on, for each of the 12 months prior to June 2023. A 13th column would be the target or dependent variable, recording the actual energy usage in June 2023. Note that a model trained upon this dataset would learn to predict energy use in June 2023 based on data in the months from June 2022 to May 2023, but it would not be able to predict other future months because the target and predictors are linked to specific months.</p>
    <p class="normal">Instead, a <a id="_idIndexMarker1383"/>better approach is to construct <strong class="keyWord">lagged variables</strong>, which<a id="_idIndexMarker1384"/> are computed relative to the target month. The lagged variables are essentially measures that are delayed in time to be carried forward to a later, more recent row in the dataset. A model using lagged variables can be retrained on a rolling, monthly basis as additional months of data become available over time. Rather than having column names like <code class="inlineCode">energy_june2023</code> and <code class="inlineCode">energy_may2023</code>, the resulting dataset will have names that indicate the relative nature of the measurements, such as <code class="inlineCode">energy_lag0</code>, <code class="inlineCode">energy_lag1</code>, and <code class="inlineCode">energy_lag2</code>, which indicate the energy use in the current month, the prior month, and 2 months ago. This model will always be applied to the most recent data to predict the forthcoming time period.</p>
    <p class="normal"><em class="italic">Figure 12.9</em> visualizes this approach. Each month, a model is trained on the past 13 months of data; the most recent month is used for the target or dependent variable (denoted as DV) while the earlier 12 months are used as lagged predictors. The model can then be used to predict the future month, which has not yet been observed. Each successive month following the first shifts the rolling window 1 month forward, such that data older than 13 months is unused in the model. A model trained using data constructed in this way does not learn the relationship between specific calendar months, as was the case with the non-lagged variables; rather, it learns how prior behavior relates to future behavior, regardless of the calendar month.</p>
    <figure class="mediaobject"><img alt="Timeline  Description automatically generated" src="../Images/B17290_12_09.png"/></figure>
    <p class="packt_figref">Figure 12.9: Constructing lagged predictors is one method to model time series data</p>
    <p class="normal">A problem with this<a id="_idIndexMarker1385"/> approach, however, is that this method has disregarded calendar time, yet certain calendar months may have an important impact on the target variable. For example, energy use may be higher in winter and summer than in spring and fall, and thus, it would be beneficial for the model to know not only the relationship between past and future behavior but also to gain a sense of seasonal effects, or other patterns broader than the local patterns, within the rows related to the unit of analysis.</p>
    <p class="normal">One might imagine that the value of the target to be predicted is composed of three sources of variation, which we would like to decompose into features for the model:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">There is the local or internal variation, which is based on the attributes unique to the unit of analysis. In the example of forecasting energy demand, the local variation may be related to the size and construction of the household, the residents’ energy needs, where the house is located, and so on.</li>
      <li class="numberedList">There may be broader global trends, such as fuel prices or weather patterns, that affect the energy use of most households.</li>
      <li class="numberedList">There may be seasonal effects, independent of the local and global effects, that explain changes in the target. This is not limited to the annual weather patterns mentioned before, but any cyclical or predictable pattern can be considered a seasonal effect.</li>
    </ol>
    <p class="normal">Some specific examples relevant to the energy forecasting project may include higher or lower demand on:</p>
    <ul>
      <li class="bulletList">Different days of the week, particularly weekdays versus weekends</li>
      <li class="bulletList">Religious or government holidays</li>
      <li class="bulletList">Traditional school or business vacation periods</li>
      <li class="bulletList">Mass gatherings such as sporting events, concerts, and elections</li>
    </ul>
    <p class="normal">If local, global, and seasonal features can be incorporated into the training dataset as predictors, the model can learn their effect on the outcome. The challenge thereafter is twofold: subject-matter knowledge or data exploration is required to identify the important seasonal factors, and there must be ample training data for the target to be observed in each of the included seasons. The latter implies that the training data should be composed of more than a single month cross-section of time; lacking this, the learning algorithm will obviously be unable to discover the relationship between the seasons and the target!</p>
    <p class="normal">Though it <a id="_idIndexMarker1386"/>would seem to follow that we should revert to the original long-format data, this is actually not the case. In fact, the wide data with lagged variables from each month can be stacked in a single unified dataset with multiple rows per unit of analysis. Each row indicates an individual at a particular moment in time, with a target variable measuring the outcome at that moment, and a wide set of columns that have been constructed as lagged variables for periods of time prior to the target. Additional columns can also be added to further widen the matrix and decompose the various components of time variance, such as indicators for seasons, days of the week, and holidays; these columns will indicate whether the given row falls within one of these periods of interest.</p>
    <p class="normal">The figure that follows depicts a hypothetical dataset using this approach. Each household (denoted by the <code class="inlineCode">household_id</code> column) can appear repeatedly with different values of the target (<code class="inlineCode">energy_use</code>) and predictors (<code class="inlineCode">season</code>, <code class="inlineCode">holiday_month</code>, <code class="inlineCode">energy_lag1</code>, and so on). Note that the lag variables are missing (as indicated by the <code class="inlineCode">NA</code> values) for the first few rows of the dataset, which means that these rows cannot be used for training or prediction. The remaining rows, however, can be used with any machine learning method capable of numeric prediction, and the trained model will readily forecast next month’s energy use given the row of data for the current month.</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_12_10.png"/></figure>
    <p class="packt_figref">Figure 12.10: Datasets including historical data may include both seasonal effects and lagged predictors</p>
    <p class="normal">Before <a id="_idIndexMarker1387"/>rushing into modeling time series data, it is crucial to understand an important caveat about the data preparation methods described here: because the rows from repeated observations from the same unit of analysis are related to one another, including them in the training data violates the assumption of independent observations for methods like regression. While models built upon such data may still be useful, other methods for formal time series modeling may be more appropriate, and it is best to consider the methods described here as a workaround to perform forecasting with the machine learning methods previously covered. Linear mixed models and recurrent neural networks are two potential approaches that can handle this type of data natively, although both methods are outside the scope of this book.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">lme4</code> package is used to build mixed models in R, but it would be unwise to jump in without understanding the statistical underpinnings of these types of models; they are a significant step up in complexity over traditional regression modeling. The book<em class="italic"> Linear Mixed-Effects Models Using R</em> (Gałecki &amp; Burzykowski, 2013) provides the theoretical background needed to build this type of model. To build recurrent neural networks, R may not be the right tool for the job, as specialized tools exist for this purpose. However, the <code class="inlineCode">rnn</code> package can build simple RNN models for time series forecasting.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-280">Hint 7: Append external data</h2>
    <p class="normal">Unlike the teaching examples in<a id="_idIndexMarker1388"/> this book, when a machine learning project begins in the real world, a dataset cannot simply be downloaded from the internet with prebuilt features and examples describing the topic of interest. It is unfortunate how many deeply interesting projects are killed before they begin for this simple reason. Businesses hoping to predict customer churn realize they have no historical data from which a model can be built; students hoping to optimize food distribution in poverty-stricken areas are limited by the scarce amounts of data from these areas; and countless projects that might increase profits or change the world for the better are stunted before they start. What begins as excitement around a machine learning project soon fizzles out due to the lack of data.</p>
    <p class="normal">Rather than ending with discouragement, it is better to channel this energy into an effort to create the necessary data from scratch. This may mean dialing colleagues on the telephone or firing off a series of email messages to connect with those that can grant access to databases containing relevant pieces of data. It may also require rolling up your sleeves and getting your hands dirty. After all, we live in the so-called big data era where data is not only plentiful but easily recorded, with assistance from electronic sensors and automated data entry tools.</p>
    <figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" src="../Images/B17290_12_11.png"/></figure>
    <p class="packt_figref">Figure 12.11: Little effort is often sufficient to generate datasets useful for machine learning</p>
    <p class="normal">In the worst case, an investment of time, effort, and imagination can build useful datasets from nothing. Typically, this is easier than one might think. The previous figure illustrates several cases in which I created datasets to satisfy my own curiosity. </p>
    <p class="normal">Fascinated by autonomous vehicles, I drove around my neighborhood and took pictures of road signs to build a stop sign classification algorithm. To predict used car prices, I copied and pasted hundreds of listings from used car websites. And, to understand exactly when and why names rhyming with “Aiden” became so popular in the United States, I gathered dozens of years of data from the Social Security baby name database. None of these projects required more than a few hours of effort, but enrolling friends, colleagues, or internet forums as a form of crowdsourcing the effort or even paying for data entry assistance could have parallelized the task and helped my database grow larger or faster. Paid services like <a id="_idIndexMarker1389"/>Amazon Mechanical Turk (<a href="https://www.mturk.com"><span class="url">https://www.mturk.com</span></a>) provide an affordable means of distributing large and tedious data entry or collection tasks.</p>
    <p class="normal">To further <a id="_idIndexMarker1390"/>enrich existing datasets, there is often the potential to append additional features from external sources. This is especially true when the main dataset of interest includes geographic identifiers such as postal codes, as many publicly available databases measure attributes for these regions. Of course, a postal code-level dataset will not reveal a specific individual’s exact characteristics; however, it may provide insight into whether the average person in the area is wealthier, healthier, younger, or more likely to have kids, among numerous other factors that may help improve the quality of a predictive model. These types of data can be readily found on many governmental agency websites and downloaded at no charge; simply merge them onto the main dataset for additional possible predictors.</p>
    <p class="normal">Lastly, many social media companies and data aggregator services like Facebook, Zillow, and LinkedIn provide free access to limited portions of their data. Zillow, for example, provides home value estimates for postal code regions. In some cases, these companies or other vendors may sell access to these datasets, which can be a powerful means of augmenting a predictive model. In addition to the financial cost of such acquisitions, they often pose a significant <a id="_idIndexMarker1391"/>challenge in terms of <strong class="keyWord">record linkage</strong>, which involves matching entities across datasets that share no common unique identifier. Solving this problem involves building a <strong class="keyWord">crosswalk</strong> table<a id="_idIndexMarker1392"/>, which maps each row in one source to the corresponding row in the other source. For instance, the crosswalk may link a person identified by a customer identification number in the main dataset to a unique website URL in an external social media dataset. Although there are R packages such as <code class="inlineCode">RecordLinkage</code> that can help perform such matching across sources, these rely on heuristics that may not perform as well as human intelligence and require significant computational expense, particularly for large databases. In general, it is safe to assume that record linkage is often costly from human resource and computational expense perspectives.</p>
    <div class="note">
      <p class="normal">When considering whether to acquire external data, be sure to research the source’s terms of use, as well as your region’s laws and organizational rules around using such sources. Some jurisdictions are stricter than others, and many rules are becoming stricter over time, so it is important to keep up to date on the legality and liability associated with outside data.</p>
    </div>
    <p class="normal">Given the <a id="_idIndexMarker1393"/>work involved in advanced data preparation, R itself has evolved to keep up with the new demands. Historically, R was notorious for struggling with very large and complex datasets, but over time, new packages have been developed to address these shortcomings and make it easier to perform the types of operations described so far in this chapter. In the remainder of this chapter, you will learn about these packages, which modernize the R syntax for real-world data challenges.</p>
    <h1 class="heading-1" id="_idParaDest-281">Exploring R’s tidyverse</h1>
    <p class="normal">A new approach has rapidly taken shape as the dominant paradigm for working with data in R. Championed by Hadley Wickham—the mind behind many of the packages that drove much of R’s initial surge in popularity—this new wave is now backed by a much larger team at Posit (formerly known as RStudio). The company’s user-friendly RStudio Desktop application integrates nicely into this new ecosystem, known as the <strong class="keyWord">tidyverse</strong>, because<a id="_idIndexMarker1394"/> it provides a universe of packages devoted to tidy data. The entire suite of tidyverse packages can be installed with the <code class="inlineCode">install.packages("tidyverse")</code> command.</p>
    <p class="normal">A growing number of resources are available online to learn more about the tidyverse, starting with its <a id="_idIndexMarker1395"/>homepage at <a href="https://www.tidyverse.org"><span class="url">https://www.tidyverse.org</span></a>. Here, you can learn about the various packages included in the set, a few of which will be described in this chapter. Additionally, the book <em class="italic">R for Data Science</em> by Hadley Wickham and Garrett Grolemund is available freely online at <a href="https://r4ds.hadley.nz"><span class="url">https://r4ds.hadley.nz</span></a> and illustrates how the tidyverse’s self-proclaimed “opinionated” approach simplifies data science projects.</p>
    <div class="note">
      <p class="normal">I am often asked the question of how R compares to Python for data science and machine learning. RStudio and the tidyverse are perhaps R’s greatest asset and point of distinction. There is arguably no easier way to begin a data science journey. Once you’ve learned the “tidy” way of doing data analysis, you are likely to wish the tidyverse functionality existed everywhere!</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-282">Making tidy table structures with tibbles</h2>
    <p class="normal">Whereas the data frame is the center of the base R universe, the data structure at the heart of the tidyverse is found in the <code class="inlineCode">tibble</code> package (<a href="https://tibble.tidyverse.org"><span class="url">https://tibble.tidyverse.org</span></a>), the name of which is a pun on the word “table” as well as a nod to the infamous “tribble” in <em class="italic">Star Trek</em> lore. A <strong class="keyWord">tibble</strong> acts <a id="_idIndexMarker1396"/>almost exactly like a data frame but includes additional modern functionality for convenience and simplicity. Tibbles can be used almost everywhere a data frame can be used. Detailed information about tibbles can be found by typing the command <code class="inlineCode">vignette("tibble")</code> in R.</p>
    <p class="normal">Most of the time, using tibbles<a id="_idIndexMarker1397"/> will be transparent and seamless, as tibbles can pass as a data frame in most R packages. However, in the rare case where you need to convert a tibble to a data frame, use the <code class="inlineCode">as.data.frame()</code> function. To go in the other direction and convert a data frame in to a tibble, use the <code class="inlineCode">as_tibble()</code> function. Here, we’ll create a tibble from the Titanic dataset first introduced in the previous chapter:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(tibble) <span class="hljs-comment"># not necessary if tidyverse is already loaded</span>
&gt; titanic_csv &lt;- read.csv(<span class="hljs-string">"titanic_train.csv"</span>)
&gt; titanic_tbl &lt;- as_tibble(titanic_csv)
</code></pre>
    <p class="normal">Typing the name of this object demonstrates the tibble’s cleaner and more informative output than a standard data frame:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated with medium confidence" src="../Images/B17290_12_12.png"/></figure>
    <p class="packt_figref">Figure 12.12: Displaying a tibble object results in more informative output than a standard data frame</p>
    <p class="normal">It is important to note the distinctions between tibbles and data frames, as the tidyverse will automatically create a tibble object for many of its operations. Overall, you are likely to find that tibbles are faster and easier to work with than data frames. They generally make smarter assumptions about the data, which means you will spend less time redoing R’s work—like recoding strings as factors or vice versa. </p>
    <p class="normal">Indeed, one simple distinction between tibbles and data frames is that a tibble never assumes <code class="inlineCode">stringsAsFactors = TRUE</code>, which was the default behavior in base R until relatively recently with the release of R version 4.0. As described in previous chapters, R’s <code class="inlineCode">stringsAsFactors</code> setting <a id="_idIndexMarker1398"/>sometimes led to confusion or programming bugs when character columns were automatically converted in to factors by default. Another distinction between tibbles and data frames is that, as long as the name is surrounded by the backtick (<code class="inlineCode">`</code>) character, a tibble can use non-standard column names like <code class="inlineCode">`my var`</code> that violate base R’s object naming rules. Other benefits of tibbles are unlocked by complementary tidyverse packages, as described in the sections that follow.</p>
    <h2 class="heading-2" id="_idParaDest-283">Reading rectangular files faster with readr and readxl</h2>
    <p class="normal">Nearly every chapter so far has used the <code class="inlineCode">read.csv()</code> function to load data into R data frames. Although we could convert these data frames in to tibbles, there is a faster and more direct path to get data<a id="_idIndexMarker1399"/> into the tibble format. The tidyverse<a id="_idIndexMarker1400"/> includes the <code class="inlineCode">readr</code> package (<a href="https://readr.tidyverse.org"><span class="url">https://readr.tidyverse.org</span></a>) for loading tabular data. This is described in the data import chapter in <em class="italic">R for Data Science</em> at <a href="https://r4ds.hadley.nz/data-import.html"><span class="url">https://r4ds.hadley.nz/data-import.html</span></a>, but the basic functionality is simple.</p>
    <p class="normal">The <code class="inlineCode">readr</code> package <a id="_idIndexMarker1401"/>provides a <code class="inlineCode">read_csv()</code> function that loads data from CSV files much like base R’s <code class="inlineCode">read.csv()</code> function. A key difference, aside from the subtle difference in their function names, is that the tidyverse’s version is much speedier—and not merely because it automatically converts the data into a tibble. It is about 10x faster at reading data according to the package authors. It is also smarter about the format of the columns to be loaded. For example, it has the capability to handle numbers with currency characters, parse date columns, and is better at handling international data.</p>
    <p class="normal">To create a tibble from a CSV file, simply use the <code class="inlineCode">read_csv()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(readr) <span class="hljs-comment"># not necessary if tidyverse is already loaded</span>
&gt; titanic_train &lt;- read_csv(<span class="hljs-string">"titanic_train.csv"</span>)
</code></pre>
    <p class="normal">This will <a id="_idIndexMarker1402"/>use the default parsing settings, which attempt to infer the correct data type (that is, character or numeric) for each column. The column specification will be displayed in the R output upon completion of the file read. The inferred data types may be overridden by providing the correct column specifications via a <code class="inlineCode">col()</code> function call passed to the <code class="inlineCode">read_csv()</code> function. For more information on the syntax, view the documentation using the <code class="inlineCode">vignette("readr")</code> command.</p>
    <p class="normal">The <code class="inlineCode">readxl</code> package (<a href="https://readxl.tidyverse.org"><span class="url">https://readxl.tidyverse.org</span></a>) provides<a id="_idIndexMarker1403"/> a method to read data directly<a id="_idIndexMarker1404"/> from the Microsoft Excel spreadsheet format. To create a tibble from an XLSX file, simply use the <code class="inlineCode">read_excel()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(readxl)
&gt; titanic_train &lt;- read_excel(<span class="hljs-string">"titanic_train.xlsx"</span>)
</code></pre>
    <p class="normal">Alternatively, as first introduced in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>, the RStudio desktop application can write the data import code for you. In the upper-right of the interface, under the <strong class="screenText">Environment</strong> tab, there is an <strong class="screenText">Import Dataset</strong> button. This menu reveals a list of <a id="_idIndexMarker1405"/>data import options, including plaintext formats like CSV files (using base R or the <code class="inlineCode">readr</code> package), as well as Excel and the SPSS, SAS, and Stata formats created by other statistical computing software tools. Using the <strong class="screenText">From Text (readr)</strong> option reveals the following graphical interface, allowing the import process to be easily customized:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_12_13.png"/></figure>
    <p class="packt_figref">Figure 12.13: RStudio’s Import Dataset feature automatically writes R code to easily import a variety of data formats</p>
    <p class="normal">The interface<a id="_idIndexMarker1406"/> displays a preview of the data that updates as the import parameters are customized. The default column data types can be customized by clicking on the drop-down menu in the column header, and the code preview in the lower-right will update accordingly. </p>
    <p class="normal">Clicking the <strong class="screenText">Import</strong> button will immediately execute the code, but a better practice is to copy and paste the code into your R source code file so that the import process can be easily run again in the future.</p>
    <h2 class="heading-2" id="_idParaDest-284">Preparing and piping data with dplyr</h2>
    <p class="normal">The <code class="inlineCode">dplyr</code> <a id="_idIndexMarker1407"/>package (<a href="https://dplyr.tidyverse.org"><span class="url">https://dplyr.tidyverse.org</span></a>) provides the infrastructure <a id="_idIndexMarker1408"/>for the tidyverse, as it includes the basic functionality that allows data to be transformed and manipulated. It also provides a straightforward way to begin working with larger datasets in R. Though there are other packages that have greater raw speed or are capable of handling even more massive datasets, dplyr is still quite capable and a good first step to take if you run into speed or memory limitations with base R.</p>
    <p class="normal">When<a id="_idIndexMarker1409"/> used with tibble objects, dplyr unlocks some impressive functionality:</p>
    <ul>
      <li class="bulletList">Because dplyr focuses on data frames rather than vectors, new operators are introduced that allow common data transformations to be performed with much less code while remaining highly readable.</li>
      <li class="bulletList">The package makes reasonable assumptions about data frames, which optimizes your effort as well as memory use. If possible, it avoids making copies of data by pointing to the original value instead.</li>
      <li class="bulletList">Key portions of the code are written in C++, which, according to the authors, yields a 20x to 1,000x performance increase over base R for many operations.</li>
      <li class="bulletList">R data frames are limited by available memory. With dplyr, tibbles can be linked transparently to disk-based databases exceeding what can be stored in memory.</li>
    </ul>
    <p class="normal">The dplyr grammar of working with data becomes second nature after the initial learning curve has been passed. There are five key verbs in the grammar, which perform many of the most common transformations to data tables. Beginning with a tibble, one may choose to:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">filter()</code> rows of data by values of the columns</li>
      <li class="bulletList"><code class="inlineCode">select()</code> columns of data by name</li>
      <li class="bulletList"><code class="inlineCode">arrange()</code> rows of data by sorting the values</li>
      <li class="bulletList"><code class="inlineCode">mutate()</code> columns into new columns by transforming the values</li>
      <li class="bulletList"><code class="inlineCode">summarize()</code> rows of data by aggregating values into a summary</li>
    </ul>
    <p class="normal">These five <a id="_idIndexMarker1410"/>dplyr verbs are brought together in sequences using a <strong class="keyWord">pipe operator</strong>, which is natively supported in R as of version 4.1 or later. Represented by<a id="_idIndexMarker1411"/> the <code class="inlineCode">|&gt;</code> symbols, which vaguely resembles an arrowhead pointing to the right, the pipe operator “pipes” data by moving it from one function to another. The use of pipes allows you to create powerful chains of functions to sequentially process datasets.</p>
    <div class="packt_tip">
      <p class="normal">In versions of R prior to 4.1.0 update, the pipe operator was denoted by the <code class="inlineCode">%&gt;%</code> character sequence and required the <code class="inlineCode">magrittr</code> package. The differences between the old and new pipe functionality are relatively minor, but as a native operator the new pipe may have a small speed advantage. For a shortcut to typing the pipe operator, the RStudio Desktop IDE, the key combination <em class="keystroke">ctrl</em> + <em class="keystroke">shift</em> + <em class="keystroke">m</em> will insert the character sequence. Note that for this shortcut to produce the updated pipe, you may need to change the setting to “<strong class="keyWord">Use the native pipe operator</strong>, <strong class="keyWord">|&gt;</strong>” in the RStudio “<strong class="keyWord">Global Options</strong>” menu under the “<strong class="keyWord">Code</strong>” heading.</p>
    </div>
    <p class="normal">After loading the package with the <code class="inlineCode">library(dplyr)</code> command, data transformations begin with a tibble being piped into one of the package’s verbs. For example, one might <code class="inlineCode">filter()</code> rows of the Titanic dataset to limit rows to women:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt; filter(Sex == <span class="hljs-string">"female"</span>)
</code></pre>
    <p class="normal">Similarly, one might <code class="inlineCode">select()</code> only the name, sex, and age columns:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt; select(Name, Sex, Age)
</code></pre>
    <p class="normal">Where dplyr starts to shine is through its ability to chain together verbs in a sequence with pipes. For example, we can combine the prior two verbs, sort alphabetically using the verb <code class="inlineCode">arrange()</code>, and save the output to a tibble as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_women &lt;- titanic_train |&gt;
    filter(Sex == <span class="hljs-string">"female"</span>) |&gt;
    select(Name, Sex, Age) |&gt;
    arrange(Name)
</code></pre>
    <p class="normal">Although this may not seem like a revelation just yet, when combined with the <code class="inlineCode">mutate()</code> verb, we can perform complex data transformations with simpler, more readable code than in the base R <a id="_idIndexMarker1412"/>language. We will see several examples of <code class="inlineCode">mutate()</code> later on, but for now, the important thing to remember is that it is used to create new columns in the tibble. For example, we might create a binary <code class="inlineCode">elderly</code> feature that indicates whether a passenger is at least 65 years old. </p>
    <p class="normal">This uses the dplyr package’s <code class="inlineCode">if_else()</code> function to assign a value of <code class="inlineCode">1</code> if the passenger is elderly, and <code class="inlineCode">0</code> otherwise:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt;
    mutate(elderly = if_else(Age &gt;= <span class="hljs-number">65</span>, <span class="hljs-number">1</span>,<span class="hljs-number"> 0</span>))
</code></pre>
    <p class="normal">By separating the statements by commas, multiple columns can be created within a single <code class="inlineCode">mutate()</code> statement. This is demonstrated here to create an additional <code class="inlineCode">child</code> feature that indicates whether the passenger is less than 18 years old:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt;
    mutate(
      elderly = if_else(Age &gt;= <span class="hljs-number">65</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>),
      child = if_else(Age &lt; <span class="hljs-number">18</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
    )
</code></pre>
    <p class="normal">The remaining <code class="inlineCode">dplyr</code> verb, <code class="inlineCode">summarize()</code>, allows us to create aggregated or summarized metrics by grouping rows in the tibble. For example, suppose we would like to compute the survival rate by age or sex. We’ll begin with sex, as it is the easier of the two cases. We simply pipe the data into the <code class="inlineCode">group_by(Sex)</code> function to create the male and female groups, then follow this with a <code class="inlineCode">summarize()</code> statement to create a <code class="inlineCode">survival_rate</code> feature that computes the average survival by group:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt;
    group_by(Sex) |&gt;
    summarize(survival_rate = mean(Survived))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 2 x 2
  Sex    survival_rate
  &lt;chr&gt;          &lt;dbl&gt;
1 female         0.742
2 male           0.189
</code></pre>
    <p class="normal">As shown in the output, females were substantially more likely to survive than males.</p>
    <p class="normal">To compute <a id="_idIndexMarker1413"/>survival by age, things are slightly more complicated due to the missing age values. We’ll need to filter out these rows and use the <code class="inlineCode">group_by()</code> function to compare children (less than 18 years old) to adults as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt;
    filter(!<span class="hljs-built_in">is.na</span>(Age)) |&gt;
    mutate(child = if_else(Age &lt; <span class="hljs-number">18</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)) |&gt;
    group_by(child) |&gt;
    summarize(survival_rate = mean(Survived))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"># A tibble: 2 x 2
  child survival_rate
  &lt;dbl&gt;         &lt;dbl&gt;
1     0         0.381
2     1         0.540
</code></pre>
    <p class="normal">The results suggest that children were about 40% more likely to survive than adults. When combined with the comparison between males and females, this provides strong evidence of the hypothesized “women and children first” policy for evacuation of the sinking ship.</p>
    <div class="packt_tip">
      <p class="normal">Because summary statistics by group can be computed using other methods in base R (including the <code class="inlineCode">ave()</code> and <code class="inlineCode">aggregate()</code> functions described in previous chapters), it is important to note that the <code class="inlineCode">summarize()</code> command is also capable of much more than this. In particular, one might use it for the feature engineering hints described earlier in this chapter, such as observing neighbors’ behavior, utilizing related rows, and decomposing time series. All three of these cases involve <code class="inlineCode">group_by()</code> options like households, zip codes, or units of time. Using <code class="inlineCode">dplyr</code> to perform the aggregation for these data preparation operations is much easier than attempting to do so in base R.</p>
    </div>
    <p class="normal">To put together what we’ve<a id="_idIndexMarker1414"/> learned so far and provide one more example using pipes, let’s build a decision tree model of the Titanic dataset. We’ll <code class="inlineCode">filter()</code> missing age values, use <code class="inlineCode">mutate()</code> to create a new <code class="inlineCode">AgeGroup</code> feature, and <code class="inlineCode">select()</code> only the columns of interest for the decision tree model. The resulting dataset is piped to the <code class="inlineCode">rpart()</code> decision tree algorithm, which illustrates the ability to pipe data to functions outside of the tidyverse:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(rpart)
&gt; m_titanic &lt;- titanic_train |&gt;
    filter(!<span class="hljs-built_in">is.na</span>(Age)) |&gt;
    mutate(AgeGroup = if_else(Age &lt; <span class="hljs-number">18</span>, <span class="hljs-string">"Child"</span>, <span class="hljs-string">"Adult"</span>)) |&gt;
    select(Survived, Pclass, Sex, AgeGroup) |&gt;
    rpart(formula = Survived ~ ., data = _)
</code></pre>
    <p class="normal">Note that the series of steps reads almost like plain-language pseudocode. It is also worth noting the arguments within the <code class="inlineCode">rpart()</code> function call. The <code class="inlineCode">formula = Survived ~ .</code> argument uses R’s formula interface to model survival as a function of all predictors; the dot here represents the other features in the dataset not explicitly listed. The <code class="inlineCode">data = _</code> argument uses the underscore (_) as a placeholder to represent the data being fed to <code class="inlineCode">rpart()</code> by the pipe. The underscore can be used in this way to indicate the function parameter to which the data should be piped. </p>
    <p class="normal">This is usually unnecessary for dplyr’s built-in functions, because they look for the piped data as the first parameter by default, but functions outside the tidyverse may require the pipe to target a specific function parameter in this way.</p>
    <div class="packt_tip">
      <p class="normal">It is important to note that the underscore placeholder character is new as of R version 4.2 and will not work in prior versions! In older code that uses the <code class="inlineCode">magrittr</code> package, the dot character (.) was used as the placeholder.</p>
    </div>
    <p class="normal">For fun, we can visualize the resulting decision tree, which shows that women and children are more likely to survive than adults, men, and those in third passenger class:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(rpart.plot)
&gt; rpart.plot(m_titanic)
</code></pre>
    <p class="normal">This produces the following decision tree diagram:</p>
    <figure class="mediaobject"><img alt="Timeline  Description automatically generated" src="../Images/B17290_12_14.png"/></figure>
    <p class="packt_figref">Figure 12.14: A decision tree predicting Titanic survival, which was built using a series of pipes</p>
    <p class="normal">These are just a few <a id="_idIndexMarker1415"/>small examples of how sequences of dplyr commands can make complex data manipulation tasks simpler. This is on top of the fact that, due to dplyr’s more efficient code, the steps often execute more quickly than the equivalent commands in base R! Providing a complete dplyr tutorial is beyond the scope of this book, but there are many learning resources available online, including the <em class="italic">R for Data Science</em> chapter at <code class="inlineCode">https://r4ds.hadley.nz/transform.html</code>.</p>
    <h2 class="heading-2" id="_idParaDest-285">Transforming text with stringr</h2>
    <p class="normal">The <code class="inlineCode">stringr</code> package (<code class="inlineCode">https://stringr.tidyverse.org</code>) adds functions to analyze and transform character<a id="_idIndexMarker1416"/> strings. Base R, of course, can do this too, but the functions are <a id="_idIndexMarker1417"/>inconsistent in how they work on vectors and are relatively slow; <code class="inlineCode">stringr</code> implements these functions in a form more attuned to the tidyverse workflow. The free resource <em class="italic">R for Data Science</em> has a tutorial that introduces the package’s complete set of capabilities, at <a href="https://r4ds.hadley.nz/strings.html"><span class="url">https://r4ds.hadley.nz/strings.html</span></a>, but here, we’ll examine some of the aspects most relevant to feature engineering. If you’d like to follow along, be sure to load the Titanic dataset and install <a id="_idIndexMarker1418"/>and load the <code class="inlineCode">stringr</code> package before proceeding.</p>
    <p class="normal">Earlier in this chapter, the second tip for feature engineering was to “find insights hidden in text.” The <code class="inlineCode">stringr</code> package can assist with this effort by providing functions to slice strings and detect patterns within text. All <code class="inlineCode">stringr</code> functions begin with the prefix <code class="inlineCode">str_</code>, and a few relevant examples are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">str_detect()</code> determines whether a search term is found in a string</li>
      <li class="bulletList"><code class="inlineCode">str_sub()</code> slices a string by position and returns a substring</li>
      <li class="bulletList"><code class="inlineCode">str_extract()</code> searches for a string and returns the matching pattern</li>
      <li class="bulletList"><code class="inlineCode">str_replace()</code> replaces characters in a string with something else</li>
    </ul>
    <p class="normal">Although these functions seem quite similar, they are used for quite different purposes. To demonstrate these purposes, we’ll begin by examining the <code class="inlineCode">Cabin</code> feature to determine whether certain rooms on the <em class="italic">Titanic</em> are linked to greater survival. We cannot use this feature as-is, because each cabin code is unique. </p>
    <p class="normal">However, because the codes are in forms like <code class="inlineCode">A10</code>, <code class="inlineCode">B101</code>, or <code class="inlineCode">E67</code>, perhaps the alphabetical prefix indicates a position on the ship, and perhaps passengers in some of these locations may have been more able to escape the disaster. We’ll use the <code class="inlineCode">str_sub()</code> function to take a 1-character substring beginning and ending at position 1, and save this to a <code class="inlineCode">CabinCode</code> feature as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train &lt;- titanic_train |&gt;
    mutate(CabinCode = str_sub(Cabin, start = <span class="hljs-number">1</span>, end = <span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">To confirm that the cabin code is meaningful, we can use the <code class="inlineCode">table()</code> function to see a clear relationship between it and the passenger class. The <code class="inlineCode">useNA</code> parameter is set to <code class="inlineCode">"ifany"</code> to display the <code class="inlineCode">NA</code> values caused by missing cabin codes for some passengers:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; table(titanic_train$Pclass, titanic_train$CabinCode,
       useNA = <span class="hljs-string">"ifany"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   
      A   B   C   D   E   F   G   T &lt;NA&gt;
  1  15  47  59  29  25   0   0   1   40
  2   0   0   0   4   4   8   0   0  168
  3   0   0   0   0   3   5   4   0  479
</code></pre>
    <p class="normal">The <code class="inlineCode">NA</code> values appear to be more common in the lower ticket classes, so it seems plausible that cheaper fares may have not received a cabin code.</p>
    <p class="normal">We can also plot the <a id="_idIndexMarker1419"/>survival probability by cabin code by piping the file into a <code class="inlineCode">ggplot()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(ggplot2)
&gt; titanic_train |&gt; ggplot() +
    geom_bar(aes(x = CabinCode, y = Survived),
               stat = <span class="hljs-string">"summary"</span>, fun = <span class="hljs-string">"mean"</span>) +
    ggtitle(<span class="hljs-string">"Titanic Survival Rate by Cabin Code"</span>)
</code></pre>
    <p class="normal">The resulting figure shows that even within the first-class cabin types (codes A, B, and C) there are differences in survival rate; additionally, the passengers without a cabin code are the least likely to survive:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart, histogram  Description automatically generated" src="../Images/B17290_12_15.png"/></figure>
    <p class="packt_figref">Figure 12.15: The cabin code feature seems related to survival, even within first-class cabins (A, B, and C)</p>
    <p class="normal">Without processing the <code class="inlineCode">Cabin</code> text data first, a learning algorithm would be unable to use the feature as the codes are unique to each cabin. Yet by applying a simple text transformation, we’ve decoded the cabin codes into something that can be used to improve the model’s survival predictions.</p>
    <p class="normal">With this success<a id="_idIndexMarker1420"/> in mind, let’s examine another potential source of hidden data: the <code class="inlineCode">Name</code> column. One might assume that this is unusable in a model, because the name is a unique identifier per row and training a model on this data will inevitably lead to overfitting. Although this is true, there is useful information hiding within the names. Looking at the first few rows reveals some potentially useful text strings:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; head(titanic_train$Name)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "Braund, Mr. Owen Harris"                            
[2] "Cumings, Mrs. John Bradley (Florence Briggs Thayer)"
[3] "Heikkinen, Miss. Laina"                             
[4] "Futrelle, Mrs. Jacques Heath (Lily May Peel)"       
[5] "Allen, Mr. William Henry"                           
[6] "Moran, Mr. James"
</code></pre>
    <p class="normal">For one, the salutation (Mr., Mrs., and Miss.) might be helpful for prediction. The problem is that these titles are located at different positions within the name strings, so we cannot simply use the <code class="inlineCode">str_sub()</code> function to extract them. The correct tool for this job is <code class="inlineCode">str_extract()</code>, which is used to match and extract shorter patterns from longer strings. The trick with working with this function is knowing how to express a text pattern rather than typing each potential salutation separately.</p>
    <p class="normal">The shorthand used to express a text search <a id="_idIndexMarker1421"/>pattern is called a <strong class="keyWord">regular expression</strong>, or <strong class="keyWord">regex</strong> for short. Knowing how to create regular expressions is an incredibly useful skill, as they are used for the advanced find-and-replace features in many text editors, in addition to being useful for feature engineering in R. We’ll create a simple regex to extract the salutations from the name strings.</p>
    <p class="normal">The first step in using regular expressions is to identify the common elements across all the desired target strings. In the case of the Titanic names, it looks like each salutation is preceded by a comma followed by a blank space, then has a series of letters before ending with a period. This can be coded as the following regex string:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">", [A-z]+\\."</span>
</code></pre>
    <p class="normal">This seems to be nonsense but can be understood as a sequence that attempts to match a pattern character by character. The matching process begins with a comma and a blank space, as expected. Next, the square brackets tell the search function to look for any of the characters inside the brackets. For instance, <code class="inlineCode">[AB]</code> would search for <code class="inlineCode">A</code> or <code class="inlineCode">B</code>, and <code class="inlineCode">[ABC]</code> would search for <code class="inlineCode">A</code>, <code class="inlineCode">B</code>, or <code class="inlineCode">C</code>. In our usage, the dash is used to search for any characters within the range between <code class="inlineCode">A</code> and <code class="inlineCode">z</code>. Note that capitalization is important—that is, <code class="inlineCode">[A-Z]</code> is different from <code class="inlineCode">[A-z]</code>. The former will search 26 characters comprising the uppercase alphabet while the latter will search 52 characters, including uppercase and lowercase. Keep in mind that <code class="inlineCode">[A-z]</code> only matches a single character.</p>
    <p class="normal">To have the<a id="_idIndexMarker1422"/> expression match more characters, we follow the brackets with a <code class="inlineCode">+</code> symbol to tell the algorithm to continue matching characters until it reaches something not inside the brackets. Then, it checks to see whether the remaining part of the regex matches. </p>
    <p class="normal">The remaining piece is the <code class="inlineCode">\\.</code> sequence, which is three characters that represent the single period character at the end of our search pattern. Because the dot is a special term that represents an arbitrary character, we must escape the dot by prefixing it with a slash. Unfortunately, the slash is also a special character in R, so we must escape it as well by prefixing it with yet another slash.</p>
    <div class="note">
      <p class="normal">Regular expressions can be tricky to learn but are well worth the effort. You can find a deep dive into<a id="_idIndexMarker1423"/> understanding how they work at <a href="https://www.regular-expressions.info"><span class="url">https://www.regular-expressions.info</span></a>. Alternatively, there are many text editors and web applications that demonstrate matching in real time. These can be hugely helpful to understand how to develop the regex search patterns and diagnose errors. One of the best such tools is found at <a href="https://regexr.com"><span class="url">https://regexr.com</span></a>.</p>
    </div>
    <p class="normal">We can put this expression to work on the Titanic name data by combining it in a <code class="inlineCode">mutate()</code> function with <code class="inlineCode">str_extract()</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train &lt;- titanic_train |&gt;
    mutate(Title = str_extract(Name, <span class="hljs-string">", [A-z]+\\."</span>))
</code></pre>
    <p class="normal">Looking at the first few examples, it looks like these need to be cleaned up a bit:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; head(titanic_train$Title)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] ", Mr."   ", Mrs."  ", Miss." ", Mrs."  ", Mr."   ", Mr."
</code></pre>
    <p class="normal">Let’s use the <code class="inlineCode">str_replace()</code> function to eliminate the punctuation and blank spaces in these titles. We begin by constructing a regex to match the punctuation and empty space. One way to do this is to match the comma, blank space, and period using the <code class="inlineCode">"[, \\.]"</code> search string. Used with <code class="inlineCode">str_replace()</code> as shown here, any comma, blank space, and period characters in <code class="inlineCode">Title</code> will be replaced by the empty (null) string:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train &lt;- titanic_train |&gt;
    mutate(Title = str_replace_all(Title, <span class="hljs-string">"[, \\.]"</span>, <span class="hljs-string">""</span>))
</code></pre>
    <p class="normal">Note that the <code class="inlineCode">str_replace_all()</code> variant of the replace function was used due to the fact that multiple characters needed replacement; the basic <code class="inlineCode">str_replace()</code> would have only replaced the first instance of a matching character. Many of <code class="inlineCode">stringr</code>'s functions have “all” variants for this use case. Let’s see the result of our effort:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; table(titanic_train$Title)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    Capt      Col      Don       Dr Jonkheer     Lady 
       1        2        1        7        1        1 
   Major   Master     Miss     Mlle      Mme       Mr 
       2       40      182        2        1      517 
     Mrs       Ms      Rev      Sir 
     125        1        6        1
</code></pre>
    <p class="normal">Given the small counts for some of these titles and salutations, it may make sense to group them together. To this end, we can use dplyr’s <code class="inlineCode">recode()</code> function to change the categories. We’ll keep several of the high-count levels the same, while grouping the rest into variants of <code class="inlineCode">Miss</code> and a catch-all bucket, using the <code class="inlineCode">.missing</code> and <code class="inlineCode">.default</code> values to assign the <code class="inlineCode">Other</code> label to <code class="inlineCode">NA</code> values and anything else not already coded:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train &lt;- titanic_train |&gt;
    mutate(TitleGroup = recode(Title,
      <span class="hljs-string">"Mr"</span> = <span class="hljs-string">"Mr"</span>, <span class="hljs-string">"Mrs"</span> = <span class="hljs-string">"Mrs"</span>, <span class="hljs-string">"Master"</span> = <span class="hljs-string">"Master"</span>,
      <span class="hljs-string">"Miss"</span> = <span class="hljs-string">"Miss"</span>,
      <span class="hljs-string">"Ms"</span> = <span class="hljs-string">"Miss"</span>, <span class="hljs-string">"Mlle"</span> = <span class="hljs-string">"Miss"</span>, <span class="hljs-string">"Mme"</span> = <span class="hljs-string">"Miss"</span>,
      .missing = <span class="hljs-string">"Other"</span>,
      .default = <span class="hljs-string">"Other"</span>
      )
    )
</code></pre>
    <p class="normal">Checking our <a id="_idIndexMarker1424"/>work, we see that our cleanup worked as planned:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; table(titanic_train$TitleGroup)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Master   Miss     Mr    Mrs  Other 
    40    186    517    125     23 
</code></pre>
    <p class="normal">We can also see that the title is meaningful by examining a plot of survival rates by title:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; titanic_train |&gt; ggplot() +
    geom_bar(aes(x = TitleGroup, y = Survived),
             stat = <span class="hljs-string">"summary"</span>, fun = <span class="hljs-string">"mean"</span>) +
    ggtitle(<span class="hljs-string">"Titanic Survival Rate by Salutation"</span>)
</code></pre>
    <p class="normal">This <a id="_idIndexMarker1425"/>produces the following bar chart:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_12_16.png"/></figure>
    <p class="packt_figref">Figure 12.16: The constructed salutation captures the impact of both age and gender on survival likelihood</p>
    <p class="normal">The creation of <code class="inlineCode">CabinCode</code> and <code class="inlineCode">TitleGroup</code> features exemplifies the feature engineering technique of finding hidden information in text data. These new features are likely to provide additional information beyond the base features in the Titanic dataset, which learning algorithms can use to improve performance. A bit of creativity combined with <code class="inlineCode">stringr</code> and knowledge of regular expressions may provide the edge needed to surpass the competition.</p>
    <h2 class="heading-2" id="_idParaDest-286">Cleaning dates with lubridate</h2>
    <p class="normal">The <code class="inlineCode"><a id="_idIndexMarker1426"/></code><code class="inlineCode">lubridate</code> package (<a href="https://lubridate.tidyverse.org"><span class="url">https://lubridate.tidyverse.org</span></a>) is an important tool for working with date <a id="_idIndexMarker1427"/>and time data. It may not be needed for every analysis, but when it is needed, it can save a lot of grief. With dates and times, seemingly simple tasks can quickly turn into adventures, due to unforeseen subtleties like leap years and time zones—just ask anyone who has worked on birthday calculations, billing cycles, or similar date-sensitive tasks. </p>
    <p class="normal">As with the other tidyverse <a id="_idIndexMarker1428"/>packages, the <em class="italic">R for Data Science</em> resource has an in-depth lubridate tutorial at <code class="inlineCode">https://r4ds.hadley.nz/datetimes.html</code>, but we’ll briefly cover three of its most important feature engineering strengths here:</p>
    <ul>
      <li class="bulletList">Ensuring date and time data is loaded into R correctly while accounting for regional differences in how dates and times are expressed</li>
      <li class="bulletList">Accurately calculating differences between dates and times while accounting for time zones and leap years</li>
      <li class="bulletList">Accounting for differences in how increments in time are understood in the real world, such as the fact that people become “1 year older” on their birthday</li>
    </ul>
    <p class="normal">Reading dates into R is challenge number one, because dates are presented in many different formats. For example, the publication date of the first edition of <em class="italic">Machine Learning with R</em> can be expressed as:</p>
    <ul>
      <li class="bulletList">October 25, 2013 (a common longhand format in the United States)</li>
      <li class="bulletList">10/25/13 (a common shorthand format in the United States)</li>
      <li class="bulletList">25 October 2013 (a common longhand format in Europe)</li>
      <li class="bulletList">25.10.13 (a common shorthand format in Europe)</li>
      <li class="bulletList">2013-10-25 (the international standard)</li>
    </ul>
    <p class="normal">Given these diverse formats, lubridate is incapable of determining the correct format without help because months, days, and years can all fall in the range from 1 to 12. Instead, we provide it the correct date constructor—either <code class="inlineCode">mdy()</code>, <code class="inlineCode">dmy()</code>, or <code class="inlineCode">ymd()</code>, depending on the order of the month (<code class="inlineCode">m</code>), day (<code class="inlineCode">d</code>), and year (<code class="inlineCode">y</code>) components of the input data. Given the order of the date components, the functions will automatically parse longhand and shorthand variants, and will handle leading zeros and two- or four-digit years. To demonstrate this, the dates expressed previously can be handled with the appropriate lubridate function, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; mdy(c(<span class="hljs-string">"October 25, 2013"</span>, <span class="hljs-string">"10/25/2013"</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "2013-10-25" "2013-10-25"
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; dmy(c(<span class="hljs-string">"25 October 2013"</span>, <span class="hljs-string">"25.10.13"</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "2013-10-25" "2013-10-25"
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; ymd(<span class="hljs-string">"</span><span class="hljs-string">2013-10-25"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "2013-10-25"
</code></pre>
    <p class="normal">Notice that in each case, the resulting <code class="inlineCode">Date</code> object is exactly the same. Let’s create a similar object for each of the three previous editions of this book:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; MLwR_1stEd &lt;- mdy(<span class="hljs-string">"October 25, 2013"</span>)
&gt; MLwR_2ndEd &lt;- mdy(<span class="hljs-string">"July 31, 2015"</span>)
&gt; MLwR_3rdEd &lt;- mdy(<span class="hljs-string">"April 15, 2019"</span>)
</code></pre>
    <p class="normal">We can do simple math to compute the difference between two dates:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; MLwR_2ndEd - MLwR_1stEd
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Time difference of 644 days
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; MLwR_3rdEd - MLwR_2ndEd
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Time difference of 1354 days
</code></pre>
    <p class="normal">Notice that by<a id="_idIndexMarker1429"/> default, the difference between the two dates is returned as days. What if we hope to have an answer in years? Unfortunately, because these differences are a special lubridate <code class="inlineCode">difftime</code> object, we cannot simply divide these numbers by 365 days to perform the obvious calculation. One option is to convert them into a <strong class="keyWord">duration</strong>, which is one of the ways lubridate computes date differences, and in particular, tracks the passage of physical time—imagine it acting much like a stopwatch. The <code class="inlineCode">as.duration()</code> function performs the needed conversion:</p>
    <pre class="programlisting code"><code class="hljs-code"> &gt; as.duration(MLwR_2ndEd - MLwR_1stEd)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "55641600s (~1.76 years)"
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.duration(MLwR_3rdEd - MLwR_2ndEd)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "116985600s (~3.71 years)"
</code></pre>
    <p class="normal">We can see here that the gap between the 2nd and 3rd editions of <em class="italic">Machine Learning with R</em> was almost twice as long as the difference between the 1st and 2nd editions. We can also see that the duration seems to default to seconds while also providing the approximate number of years. To obtain only years, we can divide the duration by the duration of 1 year, which lubridate provides as a <code class="inlineCode">dyears()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; dyears()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "31557600s (~1 years)"
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.duration(MLwR_2ndEd - MLwR_1stEd) / dyears()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1.763176
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.duration(MLwR_3rdEd - MLwR_2ndEd) / dyears()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 3.70705
</code></pre>
    <p class="normal">You may find it more convenient or easier to remember the <code class="inlineCode">time_length()</code> function, which can perform the same calculation:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; time_length(MLwR_2ndEd - MLwR_1stEd, unit = <span class="hljs-string">"years"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 1.763176
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; time_length(MLwR_3rdEd - MLwR_2ndEd, unit = <span class="hljs-string">"years"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 3.70705
</code></pre>
    <p class="normal">The <code class="inlineCode">unit</code> argument can be set to units like days, months, and years, depending on the desired result. Notice, however, that these durations are exact to the second like a stopwatch, which is not always how people think about dates.</p>
    <p class="normal">In particular, for <a id="_idIndexMarker1430"/>birthdays and anniversaries, people tend to think in terms of calendar time—that is, the number of times the calendar has reached a particular milestone. In lubridate, this approach is called an <strong class="keyWord">interval</strong>, which implies a timeline-or calendar-based view of date differences, rather than the stopwatch-based approach of the duration methods discussed previously.</p>
    <p class="normal">Let’s imagine we’d like to compute the age of the United States, which was born, so to speak, on July 4, 1776. This means that on July 3, 2023, the country will be 246 birthdays old, and on July 5, 2023, it will be 247. Using durations, we don’t get quite the right answers:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; USA_DOB &lt;- mdy(<span class="hljs-string">"July 4, 1776"</span>) <span class="hljs-comment"># USA's Date of Birth</span>
&gt; time_length(mdy(<span class="hljs-string">"July 3 2023"</span>) - USA_DOB, unit = <span class="hljs-string">"years"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 246.9897
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; time_length(mdy(<span class="hljs-string">"July 5 2023"</span>) - USA_DOB, unit = <span class="hljs-string">"years"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 246.9952
</code></pre>
    <p class="normal">The problem has to do with the fact that durations deviate from calendar time due to calendar irregularities such as leap years and time changes. By explicitly converting the date difference into an interval with the <code class="inlineCode">interval()</code> function, and then dividing by the <code class="inlineCode">years()</code> function, we get closer to the right answer: </p>
    <pre class="programlisting code"><code class="hljs-code">&gt; interval(USA_DOB, mdy(<span class="hljs-string">"July 3 2023"</span>)) / years()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 246.9973
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; interval(USA_DOB, mdy(<span class="hljs-string">"July 5 2023"</span>)) / years()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 247
</code></pre>
    <p class="normal">Before going any further, be sure to notice the fact that the <code class="inlineCode">interval()</code> uses <code class="inlineCode">start, end</code> syntax, in contrast to the date difference, which used <code class="inlineCode">end - start</code>. Also note that the <code class="inlineCode">years()</code> function returns a lubridate <strong class="keyWord">period</strong>, which is yet another way to understand differences between dates and times. Periods are always relative to their position on a calendar, which means that a 1-hour period can be a 2-hour duration during a time change, and a 1-year period can include 365 or 366 1-day periods, depending on the calendar year—these are the types of challenging subtleties when working with dates that were mentioned in this section’s opening paragraph!</p>
    <p class="normal">To create our final age<a id="_idIndexMarker1431"/> calculation, we’ll use the <code class="inlineCode">%--%</code> interval construction operator as shorthand, and use the integer division operator <code class="inlineCode">%/%</code> to return only the integer component of the age. These return the expected age values:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; USA_DOB %--% mdy(<span class="hljs-string">"July 3 2023"</span>) %/% years()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 246
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; USA_DOB %--% mdy(<span class="hljs-string">"July 5 2023"</span>) %/% years()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 247
</code></pre>
    <p class="normal">Generalizing this work, we can create a function to compute the calendar-based age for a given date of birth as of today:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; age &lt;- function(birthdate) {
    birthdate %--% today() %/% years()
  }
</code></pre>
    <p class="normal">To prove that it works, we’ll check the ages of a few famous tech billionaires:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; age(mdy(<span class="hljs-string">"February 24, 1955"</span>)) <span class="hljs-comment"># Jeff Bezos</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 59
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; age(mdy(<span class="hljs-string">"June 28, 1971"</span>)) <span class="hljs-comment"># Elon Musk</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 51
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; age(mdy(<span class="hljs-string">"Oct 28, 1955"</span>)) <span class="hljs-comment"># Bill Gates</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 67
</code></pre>
    <p class="normal">If you are following along in<a id="_idIndexMarker1432"/> R, be aware that your results may vary depending on when you run the code—we’re all, unfortunately, still getting older by the day!</p>
    <h1 class="heading-1" id="_idParaDest-287">Summary</h1>
    <p class="normal">This chapter demonstrated the importance of data preparation. Because the tools and algorithms used to build machine learning models are the same across projects, data preparation is a key that unlocks the highest levels of model performance. This allows some aspects of human intelligence and creativity to have a large impact on the machine’s learning process, although clever practitioners use their strengths in concert with the machine’s by developing automated data engineering pipelines that take advantage of the computer’s ability to tirelessly search for useful insights in the data. These pipelines are especially important in the so-called “big data regime,” where data-hungry approaches like deep learning must be fed large amounts of data to avoid overfitting.</p>
    <p class="normal">In traditional small and medium data regimes, feature engineering by hand still reigns supreme. Using intuition and subject matter expertise, one can guide the model to the most useful signal in the training dataset. As this is more art than science, tips and tricks are learned on the job, or passed along second-hand from one data scientist to another. This chapter provided seven hints to help guide you on the journey, but the only way to truly become skilled at feature engineering is through practice.</p>
    <p class="normal">Tools like the tidyverse suite of R packages make it much less laborious than in years past to gain the necessary experience to perform feature engineering tasks. This chapter demonstrated how the tidyverse packages can be used to turn data into more useful predictors, and how information hidden in text data can be extracted to turn what seem like useless features into important predictors. The tidyverse packages are much more capable of handling large and ever-growing datasets than the base R functions, and they make R a pleasure to use even as datasets grow in size and complexity.</p>
    <p class="normal">The skills developed in this chapter will provide a foundation for the work to come. In the next chapter, you will add new tidyverse packages to your toolkit and see even more examples of how it integrates into the machine learning workflow. You will continue to see the importance of data preparation skills as you explore data issues that begin as relatively minor challenges but quickly grow into massive problems if taken to an extreme.</p>
    <h1 class="heading-1" id="_idParaDest-288">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>