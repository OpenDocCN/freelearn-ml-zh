- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Building an MLP Model for Fashion-MNIST with ChatGPT
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ChatGPT构建Fashion-MNIST的MLP模型
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Building upon our foundational understanding of predictive modeling, we now
    dive into the dynamic world of **Multilayer Perceptron** (**MLP**) models. In
    this chapter, we embark on a journey to construct an MLP model from scratch, leveraging
    the versatility and power of neural networks for predictive analytics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立预测建模的基础理解之上，我们现在深入到**多层感知器**（**MLP**）模型的动态世界。在本章中，我们将从头开始构建一个MLP模型，利用神经网络在预测分析中的灵活性和强大功能。
- en: Our exploration of MLPs represents a significant leap into the realm of complex
    modeling techniques. While linear regression provided valuable insights into modeling
    relationships within data, MLPs offer a rich framework for capturing intricate
    patterns and nonlinear dependencies, making them well suited for a wide range
    of predictive tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对MLP的探索标志着向复杂建模技术领域的重大飞跃。虽然线性回归为数据内部建模关系提供了有价值的见解，但MLP提供了一个丰富的框架来捕捉复杂的模式和非线性依赖关系，这使得它们非常适合广泛的预测任务。
- en: Through hands-on experimentation and iterative refinement, we will unravel the
    intricacies of MLP architecture and optimization. From designing the initial network
    structure to fine-tuning hyperparameters and incorporating advanced techniques
    such as batch normalization and dropout, we aim to equip you with the knowledge
    and skills to harness the full potential of neural networks in predictive modeling.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实际实验和迭代优化，我们将揭示MLP架构和优化的复杂性。从设计初始网络结构到微调超参数，以及采用批归一化和dropout等高级技术，我们的目标是让您掌握利用神经网络在预测建模中发挥其全部潜力的知识和技能。
- en: As we navigate through the construction and optimization of our MLP model, we
    will delve into the underlying principles of neural network dynamics, exploring
    how different architectural choices and optimization strategies influence model
    performance and generalization capabilities.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们导航MLP模型的构建和优化过程中，我们将深入研究神经网络动态的潜在原理，探讨不同的架构选择和优化策略如何影响模型性能和泛化能力。
- en: Business problem
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业问题
- en: A fashion e-commerce store seeks to optimize customer engagement and increase
    revenue by leveraging machine learning techniques to gain deeper insights into
    customer behavior and preferences. By analyzing image data representing various
    fashion items purchased by customers, the store aims to tailor its product recommendations,
    improve customer satisfaction, and enhance the overall shopping experience.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一家时尚电商店铺希望通过利用机器学习技术深入了解顾客行为和偏好，从而优化顾客参与度和增加收入。通过分析代表顾客购买的各种时尚商品的图像数据，该店铺旨在定制其产品推荐，提高顾客满意度，并提升整体购物体验。
- en: Problem and data domain
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与数据领域
- en: In this chapter, we will employ MLP models to understand the relationship between
    customers’ preferences and their purchasing patterns using the Fashion-MNIST dataset.
    MLP models offer a powerful framework for image classification tasks, allowing
    us to predict the type of clothing or accessory a customer is likely to purchase
    based on their interactions with the online store. By uncovering patterns in customer
    preferences, the e-commerce store can personalize recommendations and optimize
    inventory management to meet the diverse needs of its customer base.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Fashion-MNIST数据集，通过MLP模型来理解顾客偏好与购买模式之间的关系。MLP模型为图像分类任务提供了一个强大的框架，使我们能够根据顾客与在线店铺的互动来预测顾客可能购买的服装或配饰类型。通过揭示顾客偏好的模式，电商店铺可以个性化推荐并优化库存管理，以满足其客户群体的多样化需求。
- en: Dataset overview
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集概述
- en: The fashion e-commerce store collects image data representing various fashion
    items, categorized into different classes, from its customers. The Fashion-MNIST
    dataset comprises 70,000 grayscale images of clothing and accessories, each associated
    with a specific label indicating its category and of size 28x28.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 时尚电商店铺收集代表各种时尚商品的图像数据，这些商品被分类到不同的类别中，并来自其顾客。Fashion-MNIST数据集包含70,000张表示服装和配饰的灰度图像，每张图像都与一个特定的标签相关联，指示其类别，大小为28x28。
- en: 'Features in the dataset include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的特征包括：
- en: '**Image data**: Grayscale images of fashion items, each represented as a matrix
    of pixel intensities. These images serve as the input data for training the MLP
    model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像数据**：代表时尚商品的灰度图像，每个图像都表示为一个像素强度的矩阵。这些图像作为训练MLP模型的输入数据。'
- en: '**Label**: The category label assigned to each image, representing the type
    of clothing or accessory depicted. Labels range from 0 to 9, corresponding to
    classes such as T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,
    Bag, and Ankle boot.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：分配给每个图像的类别标签，代表所描绘的服装或配饰的类型。标签范围从0到9，对应于T恤/上衣、裤子、开衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴等类别。'
- en: By analyzing this image data and its corresponding labels, we aim to train an
    MLP model capable of accurately classifying fashion items based on their visual
    features. This predictive model will enable the e-commerce store to make personalized
    product recommendations, enhance customer engagement, and ultimately increase
    revenue by providing a seamless shopping experience tailored to individual preferences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些图像数据和相应的标签，我们旨在训练一个MLP模型，能够根据视觉特征准确地对时尚物品进行分类。这个预测模型将使电子商务商店能够提供个性化的产品推荐，增强客户参与度，并通过提供符合个人偏好的无缝购物体验来最终增加收入。
- en: '![](img/B21232_14_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21232_14_01.png)'
- en: 'Figure 14.1: Fashion-MNIST dataset'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：Fashion-MNIST数据集
- en: Breaking the problem down into features
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将问题分解为特征
- en: 'Given the nature of the Fashion-MNIST dataset, which comprises grayscale images
    of fashion items categorized into different classes, we will start by building
    a baseline MLP model. This will involve the following high-level steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Fashion-MNIST数据集的性质，它包含不同类别中时尚物品的灰度图像，我们将首先构建一个基线MLP模型。这包括以下高级步骤：
- en: '**Building the baseline model**: Users will understand the process of constructing
    a simple MLP model for image classification using ChatGPT. We will guide users
    through loading the Fashion-MNIST dataset, preprocessing the image data, splitting
    it into training and testing sets, defining the model architecture, training the
    model, making predictions, and evaluating its performance.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建基线模型**：用户将了解使用ChatGPT构建用于图像分类的简单MLP模型的过程。我们将指导用户加载Fashion-MNIST数据集，预处理图像数据，将其分为训练集和测试集，定义模型架构，训练模型，进行预测，并评估其性能。'
- en: '**Adding layers to the model**: Once the baseline model is established, users
    will learn how to experiment with adding additional layers to the MLP architecture.
    We will explore how increasing the depth or width of the model impacts its performance
    and capacity to capture complex patterns in the image data.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向模型添加层**：一旦建立了基线模型，用户将学习如何实验性地向MLP架构添加额外的层。我们将探讨增加模型的深度或宽度如何影响其性能和捕捉图像数据中复杂模式的能力。'
- en: '**Experimenting with batch sizes**: Users will experiment with different batch
    sizes during model training to observe their effects on training speed, convergence,
    and generalization performance. We will explore how varying batch sizes influence
    the trade-off between computation efficiency and model stability.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实验不同的批量大小**：用户将在模型训练过程中尝试不同的批量大小，以观察其对训练速度、收敛性和泛化性能的影响。我们将探讨批量大小如何影响计算效率和模型稳定性的权衡。'
- en: '**Adjusting the number of neurons**: Users will explore the impact of adjusting
    the number of neurons in each layer of the MLP model. By increasing or decreasing
    the number of neurons, users can observe changes in model capacity and its ability
    to learn intricate features from the image data.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整神经元数量**：用户将探索调整MLP模型每层的神经元数量对模型的影响。通过增加或减少神经元数量，用户可以观察到模型容量及其从图像数据中学习复杂特征的能力的变化。'
- en: '**Trying different optimizers**: Finally, users will experiment with different
    optimization algorithms, such as SGD, Adam, and RMSprop, to optimize the training
    process of the MLP model. We will observe how different optimizers influence training
    dynamics, convergence speed, and final model performance.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尝试不同的优化器**：最后，用户将尝试不同的优化算法，如SGD、Adam和RMSprop，以优化MLP模型的训练过程。我们将观察不同的优化器如何影响训练动态、收敛速度和最终模型性能。'
- en: By following these steps, users will gain a comprehensive understanding of building
    and optimizing MLP models for image classification tasks using the Fashion-MNIST
    dataset. They will learn how to iteratively refine the model architecture and
    training process to achieve optimal performance and accuracy in classifying fashion
    items.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，用户将全面了解使用Fashion-MNIST数据集构建和优化用于图像分类任务的MLP模型。他们将学习如何迭代地改进模型架构和训练过程，以实现分类时尚物品的最佳性能和准确性。
- en: Prompting strategy
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示策略
- en: To leverage ChatGPT for machine learning we need to have a clear understanding
    of how to implement the prompting strategies specifically for code generation
    for machine learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用ChatGPT进行机器学习，我们需要清楚地了解如何实现针对代码生成的特定提示策略。
- en: Let’s brainstorm what we would like to achieve in this task to get a better
    understanding of what needs to go into prompts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们头脑风暴一下，在这个任务中我们想要实现什么，以便更好地理解需要放入提示中的内容。
- en: 'Strategy 1: Task-Actions-Guidelines (TAG) prompt strategy'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略1：任务-行动-指南（TAG）提示策略
- en: '**1.1 - Task**: The specific task or goal is to create a classification model
    for the Fashion-MNIST dataset.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1 - 任务**：具体的任务或目标是创建一个用于Fashion-MNIST数据集的分类模型。'
- en: '**1.2 - Actions**: The key steps involved in creating a classification model
    using an MLP for the Fashion-MNIST dataset are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2 - 行动**：使用MLP为Fashion-MNIST数据集创建分类模型的关键步骤包括：'
- en: 'Data preprocessing: Normalize pixel values, flatten images into vectors, and
    encode categorical labels.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理：归一化像素值，将图像展平为向量，并编码分类标签。
- en: 'Data splitting: Partition the dataset into training, validation, and testing
    sets.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据拆分：将数据集划分为训练集、验证集和测试集。
- en: 'Model selection: Opt for an MLP as the classification model.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择：选择MLP作为分类模型。
- en: 'Model training: Train the MLP on the training data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练：在训练数据上训练MLP。
- en: 'Model evaluation: Use metrics like accuracy, precision, recall, and confusion
    matrix to evaluate the model’s performance.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估：使用准确率、精确率、召回率和混淆矩阵等指标来评估模型的表现。
- en: '**1.3 - Guidelines**: We will provide the following guidelines to ChatGPT in
    our prompt:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.3 - 指南**：在我们的提示中，我们将向ChatGPT提供以下指南：'
- en: The code should be compatible with Jupyter notebook.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码应与Jupyter笔记本兼容。
- en: Ensure that there are detailed comments for each line of code.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保对每一行代码都有详细的注释。
- en: You have to explain each line of code, which will be then copied into the text
    block of the notebook, in detail for each method used in the code before providing
    the code.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须详细解释代码的每一行，然后将这些代码复制到笔记本的文本块中，在提供代码之前对代码中使用的每种方法进行详细说明。
- en: 'Strategy 2: Persona-Instructions-Context (PIC) prompt strategy'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略2：角色扮演-指示-背景（PIC）提示策略
- en: '**2.1 - Persona**: We will adopt the persona of a beginner who needs to learn
    different steps of model creation, hence the code should be generated step by
    step.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.1 - 角色扮演**：我们将扮演一个需要学习模型创建不同步骤的初学者角色，因此代码应逐步生成。'
- en: '**2.2 - Instructions**: We have specified that we want the code generated for
    an MLP model with a single layer *and have instructed ChatGPT to provide one step
    at a time and wait for the user’s feedback.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.2 - 指示**：我们已经指定我们想要生成的代码用于具有单层的MLP模型，并指示ChatGPT一次提供一步，并等待用户的反馈。'
- en: '**2.3 - Context**: In this case, ChatGPT is already aware of the Fashion-MNIST
    dataset, as it is widely known, so we do not need to provide additional context.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.3 - 背景**：在这种情况下，ChatGPT已经了解Fashion-MNIST数据集，因为它广为人知，所以我们不需要提供额外的背景信息。'
- en: 'Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略3：学习-即兴发挥-反馈-评估（LIFE）提示策略
- en: '**3.1 - Learn**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1 - 学习**：'
- en: We want to learn about MLP models and how they work.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要了解MLP模型及其工作原理。
- en: '**3.2 - Improvise**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.2 - 即兴发挥**：'
- en: We will use this knowledge later when adding more layers, changing the batch
    size or activation functions, and so on.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在添加更多层、更改批量大小或激活函数等时使用这些知识。
- en: '**3.3 - Feedback**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3 - 反馈**：'
- en: Share code and model outputs for feedback to ensure effective learning and understanding.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分享代码和模型输出以供反馈，以确保有效的学习和理解。
- en: Incorporate suggestions and critiques to refine the model and the approach**.**
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合建议和批评来完善模型和方法**。**
- en: Provide errors to troubleshoot and resolve the issues.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供错误以排除故障和解决问题。
- en: '**3.4- Evaluate**:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.4-评估**:'
- en: Execute the code provided by ChatGPT to ensure accuracy and validity. This is
    used throughout the chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 执行ChatGPT提供的代码以确保准确性和有效性。这一点在本章中始终使用。
- en: Building an MLP model to accurately classify the Fashion-MNIST images using
    the free version of ChatGPT
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ChatGPT的免费版本构建MLP模型以准确分类Fashion-MNIST图像
- en: ChatGPT’s premium version has a code interpreter, but it doesn’t support the
    deep learning libraries such as Keras and TensorFlow required to execute the code.
    Hence, for this chapter, we will stick to the free version of ChatGPT.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的付费版本有一个代码解释器，但它不支持执行代码所需的深度学习库，如Keras和TensorFlow。因此，在本章中，我们将坚持使用ChatGPT的免费版本。
- en: 'Feature 1: Building the baseline model'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征1：构建基线模型
- en: Let’s craft our initial prompt for baseline model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为基线模型制定初始提示。
- en: '[P**rompt]**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt**]'
- en: I want to create a simple classification model for the Fashion-MNIST dataset
    (TAG 1.1) consisting of a single-layer MLP model (PIC 2.2). It should consist
    of the following steps (TAG1.2), provide one step at a time, and wait for the
    user’s feedback (PIC 2.2).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我想为Fashion-MNIST数据集（TAG 1.1）创建一个简单的分类模型，该模型由单层MLP模型（PIC 2.2）组成。它应包括以下步骤（TAG1.2），一次提供一个步骤，并等待用户的反馈（PIC
    2.2）。
- en: 'Data Preprocessing: Normalize pixel values, flatten images into vectors, and
    encode categorical labels.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理：归一化像素值，将图像展平为向量，并编码分类标签。
- en: 'Data Splitting: Partition the dataset into training, validation, and testing
    sets.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据拆分：将数据集分为训练集、验证集和测试集。
- en: 'Model Selection: Opt for a Multi-Layer Perceptron (MLP) as the classification
    model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择：选择多层感知器（MLP）作为分类模型。
- en: 'Model Training: Train the MLP on the training data.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练：在训练数据上训练MLP。
- en: 'Model Evaluation: Use metrics like accuracy, precision, recall, and confusion
    matrix to evaluate the model’s performance.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估：使用准确率、精确率、召回率和混淆矩阵等指标来评估模型性能。
- en: 'I am a beginner user (PIC 2.1), so please provide step-by-step code and strictly
    follow the below instructions (TAG 1.3):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一个初学者用户（PIC 2.1），所以请提供逐步的代码，并严格遵循以下说明（TAG 1.3）：
- en: The code should be compatible with the Jupyter Notebook.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代码应与Jupyter Notebook兼容。
- en: Ensure that there are detailed comments for each line of code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 确保每行代码都有详细的注释。
- en: You have to explain each line of code, which will be then copied into the text
    block of notebook, in detail for each method used in the code before providing
    the code.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须详细解释代码中的每一行，然后将这些代码复制到笔记本的文本块中，在提供代码之前对每个方法进行详细说明。
- en: '[E**nd of prompt]**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of prompt**]'
- en: You should receive a response similar to the following, where ChatGPT has provided
    us with the code for step 1, Data Preprocessing, along with detailed comments
    and explanations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该收到以下类似的响应，其中ChatGPT为我们提供了步骤1，数据预处理的代码，以及详细的注释和说明。
- en: '[P**rompt response]**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt response**]'
- en: 'Sure, let’s start with the first step: Data Preprocessing.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，让我们从第一步开始：数据预处理。
- en: 'Data Preprocessing:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理：
- en: Data preprocessing is a crucial step in machine learning, where we prepare our
    data for training by applying various transformations and normalization techniques.
    In this step, we’ll normalize the pixel values, flatten the images into vectors,
    and encode categorical labels.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理是机器学习中的一个关键步骤，我们通过应用各种转换和归一化技术来准备我们的数据以进行训练。在这个步骤中，我们将归一化像素值，将图像展平为向量，并编码分类标签。
- en: 'Normalize Pixel Values:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化像素值：
- en: Normalizing pixel values ensures that each pixel’s value is scaled to a range
    between 0 and 1\. This is important for the neural network to converge faster
    during training and to prevent any particular feature from dominating the learning
    process.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化像素值确保每个像素的值被缩放到0到1之间。这对于神经网络在训练过程中更快地收敛以及防止任何特定特征主导学习过程非常重要。
- en: 'Flatten Images into Vectors:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 展平图像为向量：
- en: MLP models cannot directly work with multi-dimensional data like images. Hence,
    we need to flatten the images into 1D vectors, where each pixel value becomes
    a feature.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP模型不能直接处理多维数据，如图像。因此，我们需要将图像展平为1D向量，其中每个像素值成为一个特征。
- en: 'Encode Categorical Labels:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类标签：
- en: Since our labels are categorical (e.g., different types of clothing items),
    we need to encode them into numerical values so that our model can understand
    and learn from them.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们的标签是分类的（例如，不同类型的服装项目），我们需要将它们编码为数值，以便我们的模型能够理解和学习。
- en: 'Now, let’s implement these steps in code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些步骤用代码实现：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Explanation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We import necessary libraries such as NumPy for numerical operations, TensorFlow
    for loading the Fashion-MNIST dataset, and scikit-learn for data preprocessing.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们导入必要的库，如NumPy进行数值运算，TensorFlow用于加载Fashion-MNIST数据集，以及scikit-learn进行数据预处理。
- en: We load the Fashion-MNIST dataset using the `fashion_mnist.load_data()` method
    and split it into training and testing sets.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`fashion_mnist.load_data()`方法加载Fashion-MNIST数据集，并将其分为训练集和测试集。
- en: Three functions, `normalize_images`, `flatten_images`, and `encode_labels`,
    are defined to perform each data preprocessing step.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了三个函数`normalize_images`、`flatten_images`和`encode_labels`来执行每个数据预处理步骤。
- en: 'Inside each function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个函数内部：
- en: '`normalize_images` divides pixel values by 255 to scale them between 0 and
    1.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_images` 函数通过将像素值除以 255 来将它们缩放到 0 到 1 之间。'
- en: '`flatten_images` reshapes the images into 1D vectors.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatten_images` 函数将图像重塑为 1D 向量。'
- en: '`encode_labels` converts categorical labels into numerical values using `LabelEncoder`.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encode_labels` 函数使用 `LabelEncoder` 将分类标签转换为数值。'
- en: Finally, we apply these preprocessing steps to the training and testing data
    and print their shapes to ensure everything is processed correctly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这些预处理步骤应用于训练数据和测试数据，并打印它们的形状以确保一切处理正确。
- en: '[E**nd of response]**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of response**]'
- en: We will now apply the prompt evaluation strategy (LIFE 3.4) and validate the
    code by executing the code for every step in the Google Colab environment. The
    code loads the model from Keras and normalizes the images, flattens them into
    vectors, encodes labels using LabelEncoder, then applies the preprocessing and
    prints the shape. The code runs perfectly fine without applying any modifications
    or any errors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将应用提示评估策略（LIFE 3.4）并通过在 Google Colab 环境中执行每个步骤来验证代码。该代码从 Keras 加载模型，规范化图像，将它们展平为向量，使用
    LabelEncoder 编码标签，然后应用预处理并打印形状。代码运行良好，无需任何修改或错误。
- en: Please refer to the `Learning MLP using Fashion-MNIST dataset with ChatGPT.ipynb`
    notebook for the executable code snippets or copy the code from the prompt response.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 `使用 ChatGPT 在 Fashion-MNIST 数据集上学习 MLP.ipynb` 笔记本中的可执行代码片段，或从提示响应中复制代码。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_02.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21232_14_02.png)'
- en: 'Figure 14.2: Code execution – data preprocessing'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：代码执行 – 数据预处理
- en: We will now instruct ChatGPT to proceed to the next step.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将指示 ChatGPT 继续下一步。
- en: '[P**rompt]**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt**]'
- en: Let’s proceed to the next step.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一步。
- en: '[E**nd of prompt]**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of prompt**]'
- en: We should receive a response like the following, where ChatGPT should provide
    the code for step 2 – Data Splitting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下响应，其中 ChatGPT 应该提供步骤 2 – 数据分割的代码。
- en: '[P**rompt response]**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt response**]'
- en: 'Great! Let’s move on to the next step:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！让我们继续下一步：
- en: 'Data Splitting:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分割：
- en: In this step, we’ll partition the dataset into training, validation, and testing
    sets. The training set will be used to train the model, the validation set will
    be used to tune hyperparameters and prevent overfitting, and the testing set will
    be used to evaluate the model’s performance on unseen data.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一步，我们将数据集划分为训练集、验证集和测试集。训练集将用于训练模型，验证集将用于调整超参数以防止过拟合，测试集将用于评估模型在未见数据上的性能。
- en: 'Training Set:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集：
- en: The training set is used to train the model’s parameters (weights and biases)
    using optimization algorithms like gradient descent.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集用于使用梯度下降等优化算法训练模型的参数（权重和偏差）。
- en: 'Validation Set:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集：
- en: The validation set is used to evaluate the model’s performance during training
    and adjust hyperparameters such as learning rate, batch size, and number of epochs
    to optimize the model’s performance.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集用于在训练期间评估模型性能并调整学习率、批大小和训练轮数等超参数以优化模型性能。
- en: 'Testing Set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集：
- en: The testing set is used to evaluate the final performance of the trained model
    on unseen data and assess its generalization capability.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集用于评估训练模型在未见数据上的最终性能，并评估其泛化能力。
- en: 'Now, let’s split the dataset:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分割数据集：
- en: '[PRE1]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Explanation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We define a `split_dataset` function to split the dataset into training, validation,
    and testing sets using the `train_test_split` function from scikit-learn.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个 `split_dataset` 函数，使用 scikit-learn 的 `train_test_split` 函数将数据集分割为训练集、验证集和测试集。
- en: The function takes training data, training labels, testing data, testing labels,
    and optional parameters such as validation size, test size, and random state.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该函数接受训练数据、训练标签、测试数据、测试标签以及可选参数，如验证集大小、测试集大小和随机状态。
- en: We split the training set into training and validation sets using the specified
    validation size.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用指定的验证集大小将训练集分为训练集和验证集。
- en: Then, we split the testing set into testing and validation sets using the specified
    test size.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用指定的测试集大小将测试集分割为测试集和验证集。
- en: Finally, we print the shapes of the split datasets to verify the split.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们打印分割数据集的形状以验证分割。
- en: '[E**nd of response]**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of response**]'
- en: We execute the code in the notebook and it runs successfully. The data is split
    into training, validation, and test sets with the specified sizes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在笔记本中执行代码，它成功运行。数据被分成训练集、验证集和测试集，大小如指定。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_03.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21232_14_03.png)'
- en: 'Figure 14.3: Code execution – data splitting'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：代码执行 – 数据拆分
- en: We will now instruct ChatGPT to proceed to the next step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将指示ChatGPT进行下一步。
- en: '[P**rompt]**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]'
- en: Let’s proceed to the next step and define a single-layer baseline MLP model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行下一步，并定义一个单层基线MLP模型。
- en: '[E**nd of prompt]**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]'
- en: We should receive a response like the following where ChatGPT should provide
    the code for a single-layer MLP model with an input layer, one hidden layer, and
    an output layer.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT应提供具有输入层、一个隐藏层和一个输出层的单层MLP模型的代码。
- en: '[P**rompt response]**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示响应]'
- en: 'Sure, let’s move on to the next step:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，让我们继续下一步：
- en: 'Model Selection:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择：
- en: In this step, we’ll choose a Multi-Layer Perceptron (MLP) as the classification
    model. An MLP consists of one or more layers of neurons, each fully connected
    to the next layer. For simplicity, we’ll start with a single-layer MLP as a baseline
    model.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一步，我们将选择多层感知器（MLP）作为分类模型。MLP由一个或多个层组成，每层中的神经元都与下一层完全连接。为了简单起见，我们将从一个单层MLP作为基线模型开始。
- en: 'Single-Layer MLP:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 单层MLP：
- en: A single-layer MLP consists of an input layer, a hidden layer, and an output
    layer. However, since it has only one hidden layer, it’s often referred to as
    a single-layer perceptron. Each neuron in the hidden layer receives input from
    all neurons in the input layer and produces an output, which is then passed through
    an activation function to introduce non-linearity. Finally, the output layer produces
    the final prediction.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单层MLP由一个输入层、一个隐藏层和一个输出层组成。然而，由于它只有一个隐藏层，它通常被称为单层感知器。隐藏层中的每个神经元都从输入层中的所有神经元接收输入，并产生一个输出，然后通过激活函数传递以引入非线性。最后，输出层产生最终的预测。
- en: 'Now, let’s define the baseline single-layer MLP model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义基线单层MLP模型：
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Explanation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We import the necessary modules from `TensorFlow.keras` to define the MLP model.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从`TensorFlow.keras`导入必要的模块来定义MLP模型。
- en: We define a function `create_single_layer_mlp` to create a single-layer MLP
    model.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义一个函数`create_single_layer_mlp`来创建一个单层MLP模型。
- en: 'Inside the function:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在函数内部：
- en: We initialize a sequential model.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们初始化一个序列模型。
- en: We add a single hidden layer with 128 neurons and ReLU activation function.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们添加一个具有128个神经元和ReLU激活函数的单个隐藏层。
- en: We add an output layer with softmax activation function for multi-class classification.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们添加一个具有softmax激活函数的输出层，用于多类分类。
- en: We specify the input shape based on the shape of the training data and the number
    of output classes.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们根据训练数据的形状和输出类的数量指定输入形状。
- en: We create the single-layer MLP model using the defined function.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用定义的函数创建单层MLP模型。
- en: Finally, we print the summary of the model architecture.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们打印出模型架构的摘要。
- en: '[E**nd of response]**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束]'
- en: We execute the code and it runs without any errors and prints the model summary
    with the layer type, output shape, and number of parameters. It also provides
    us with the total parameters, trainable and non-trainable parameters. In our case,
    all the parameters are trainable parameters.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行代码，它运行无误，并打印出模型摘要，包括层类型、输出形状和参数数量。它还提供了总参数数、可训练参数和非可训练参数。在我们的情况下，所有参数都是可训练参数。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_04.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21232_14_04.png)'
- en: 'Figure 14.4: Code execution – baseline model architecture'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：代码执行 – 基线模型架构
- en: We will now instruct ChatGPT to proceed to the next step for model training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指示ChatGPT进行下一步以进行模型训练。
- en: '[P**rompt]**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]'
- en: Let’s proceed to the next step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行下一步。
- en: '[E**nd of prompt]**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]'
- en: We should receive a response like the following where ChatGPT should provide
    the code for model training along with explanations of the key concepts involved,
    such as loss functions and optimization algorithms along with comments.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT应提供模型训练的代码，并解释涉及的关键概念，例如损失函数和优化算法，以及注释。
- en: '[P**rompt response]**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示响应]'
- en: 'Sure, let’s move on to the next step:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，让我们继续下一步：
- en: 'Model Training:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练：
- en: In this step, we’ll train the MLP model on the training data. Training involves
    feeding the training data to the model, adjusting its parameters (weights and
    biases) using optimization algorithms like stochastic gradient descent, and minimizing
    a loss function to make predictions as close to the actual labels as possible.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一步，我们将训练数据用于训练MLP模型。训练包括将训练数据输入模型，使用诸如随机梯度下降之类的优化算法调整其参数（权重和偏置），并最小化损失函数以使预测尽可能接近实际标签。
- en: 'Loss Function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数：
- en: The loss function measures the difference between the predicted labels and the
    actual labels. For multi-class classification problems like Fashion-MNIST, we
    typically use categorical cross-entropy as the loss function.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数衡量预测标签与实际标签之间的差异。对于像Fashion-MNIST这样的多分类问题，我们通常使用交叉熵作为损失函数。
- en: 'Optimization Algorithm:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法：
- en: The optimization algorithm is responsible for updating the model’s parameters
    during training to minimize the loss function. We’ll use Adam optimizer, which
    is an extension to stochastic gradient descent and is widely used in deep learning.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法负责在训练过程中更新模型的参数以最小化损失函数。我们将使用Adam优化器，它是随机梯度下降的扩展，在深度学习中广泛使用。
- en: 'Training Parameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 训练参数：
- en: We’ll specify training parameters such as batch size and number of epochs. Batch
    size determines the number of samples processed before updating the model’s parameters,
    while the number of epochs specifies how many times the entire training dataset
    is passed forward and backward through the model.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将指定训练参数，如批量大小和训练轮数。批量大小决定了在更新模型参数之前处理的样本数量，而训练轮数指定了整个训练数据集通过模型前向和反向传递的次数。
- en: 'Now, let’s train the model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型：
- en: '[PRE3]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Explanation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We import the necessary modules from `TensorFlow.keras` to compile and train
    the model.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从`TensorFlow.keras`导入必要的模块来编译和训练模型。
- en: We define a `train_model` function to train the model on the training data.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个`train_model`函数，用于在训练数据上训练模型。
- en: 'Inside the function:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数内部：
- en: We convert the class labels to binary class matrices using one-hot encoding.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一维编码将类别标签转换为二进制类别矩阵。
- en: We compile the model with the categorical cross-entropy loss function and the
    Adam optimizer.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用交叉熵损失函数和Adam优化器编译模型。
- en: We train the model on the training data with the specified batch size and number
    of epochs.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用指定的批量大小和训练轮数在训练数据上训练模型。
- en: Finally, we train the model using the defined function and store the training
    history for visualization and analysis.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用定义的函数训练模型，并将训练历史存储以供可视化和分析。
- en: '[E**nd of response]**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束]**'
- en: We execute the code and it runs without any errors. It applies a one-hot encoding
    to class labels, complies the model with the categorical cross-entropy loss function
    and Adam optimizer, and trains the model with batch size 32 for 10 epochs. This
    achieves a validation accuracy of approximately 88%.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行代码，它运行没有任何错误。它对类别标签应用了一维编码，使模型符合交叉熵损失函数和Adam优化器，并使用批量大小32和10个训练轮数训练模型。这实现了大约88%的验证准确率。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_05.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B21232_14_05.png)'
- en: 'Figure 14.5: Code execution – model training'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：代码执行 – 模型训练
- en: We will now instruct ChatGPT to proceed to the next step for model evaluation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将指示ChatGPT进行模型评估的下一步。
- en: '[P**rompt]**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]**'
- en: Let’s proceed to the next step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一步。
- en: '[E**nd of prompt]**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]**'
- en: We should receive a response like the following where ChatGPT should provide
    the code for evaluating the model against the test set, along with explanations
    of concepts like accuracy, precision, recall, and the confusion matrix.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT应提供评估测试集的代码，并解释准确率、精确率、召回率和混淆矩阵等概念。
- en: '[P**rompt response]**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示响应]**'
- en: 'Model Evaluation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估：
- en: In this step, we’ll use various metrics to evaluate the performance of the trained
    MLP model on the testing data. Commonly used metrics for classification tasks
    include accuracy, precision, recall, and the confusion matrix.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用各种指标来评估训练的MLP模型在测试数据上的性能。分类任务中常用的指标包括准确率、精确率、召回率和混淆矩阵。
- en: 'Accuracy:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：
- en: Accuracy measures the proportion of correctly classified instances out of the
    total instances.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率衡量的是在总实例中正确分类的实例比例。
- en: 'Precision:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率：
- en: Precision measures the proportion of true positive predictions out of all positive
    predictions. It indicates the model’s ability to avoid false positives.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确率衡量的是所有正例中正确预测正例的比例。它表明模型避免假正例的能力。
- en: 'Recall:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率：
- en: Recall measures the proportion of true positive predictions out of all actual
    positive instances. It indicates the model’s ability to identify all positive
    instances.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回忆度衡量的是所有实际正例中正确预测正例的比例。它表明模型识别所有正例的能力。
- en: 'Confusion Matrix:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵：
- en: A confusion matrix provides a summary of the model’s performance by showing
    the number of true positive, true negative, false positive, and false negative
    predictions.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵通过显示正确预测、正确否定、错误预测和错误否定的预测数量，总结了模型的表现。
- en: 'Now, let’s evaluate the model:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型：
- en: '[PRE4]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Explanation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We import necessary modules from `matplotlib.pyplot` and scikit-learn to visualize
    the confusion matrix and calculate evaluation metrics.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从`matplotlib.pyplot`和scikit-learn导入必要的模块，以可视化混淆矩阵并计算评估指标。
- en: We define an `evaluate_model` function to evaluate the model’s performance on
    the testing data.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了一个`evaluate_model`函数来评估模型在测试数据上的性能。
- en: 'Inside the function:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在函数内部：
- en: We make predictions on the testing data using the trained model.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用训练好的模型在测试数据上进行预测。
- en: We convert the predicted labels and true labels to class labels.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将预测标签和真实标签转换为类别标签。
- en: We calculate accuracy, precision, recall, and confusion matrix using scikit-learn
    metrics functions.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn的指标函数计算准确率、精确率、召回率和混淆矩阵。
- en: Finally, we evaluate the model using the defined function, print the evaluation
    metrics, and visualize the confusion matrix.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用定义的函数评估模型，打印评估指标，并可视化混淆矩阵。
- en: '[E**nd of response]**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[**响应结束**]**'
- en: We execute the code and note that it experiences an error due to the test labels
    not being encoded, but this is not evident from the error itself. We can get help
    from ChatGPT to resolve the error by applying the prompting strategy of providing
    the error to ChatGPT.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行代码并注意到它由于测试标签未编码而出现错误，但这一点从错误本身并不明显。我们可以通过向ChatGPT提供错误信息来应用提示策略以获得帮助解决错误。
- en: '![A screenshot of a computer code  Description automatically generated](img/B21232_14_06.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![计算机代码的截图，描述自动生成](img/B21232_14_06.png)'
- en: 'Figure 14.6: Code execution – error for model evaluation'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：代码执行 – 模型评估错误
- en: We provided the error to ChatGPT to get assistance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向ChatGPT提供了错误信息以获得帮助。
- en: '[P**rompt]**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[**快速**]'
- en: 'Looks like there is some issue with the test labels. We received the below
    error:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来测试标签存在问题。我们收到了以下错误：
- en: '[PRE5]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[E**nd of prompt]**'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[**提示结束**]**'
- en: We should receive a response like the following where ChatGPT should provide
    the suggestions to troubleshoot the problem.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT应提供故障排除建议。
- en: '[P**rompt response]**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[**快速响应**]'
- en: It seems like there’s an issue with the shape of the labels. The error indicates
    that the axis specified for the argmax function is out of bounds. This typically
    occurs when the labels are not in the expected format or shape.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来标签的形状存在问题。错误表明argmax函数指定的轴超出了范围。这通常发生在标签不是预期的格式或形状时。
- en: 'Let’s double-check the shape of the labels and make sure they are in the correct
    format for evaluation:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查标签的形状，确保它们以正确的格式进行评估：
- en: '[PRE6]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will help us understand the shape of the labels and identify any discrepancies.
    Once we identify the issue, we can correct it accordingly.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们了解标签的形状并识别任何差异。一旦我们识别出问题，我们就可以相应地纠正它。
- en: '[E**nd of response]**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[**响应结束**]**'
- en: 'We print the shape of `y_test`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印`y_test`的形状：
- en: '![A white rectangular object with red text  Description automatically generated](img/B21232_14_07.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有红色文字的白色矩形物体，描述自动生成](img/B21232_14_07.png)'
- en: 'Figure 14.7: Code execution - shape of test labels'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：代码执行 - 测试标签的形状
- en: We will now provide ChatGPT with the output of the code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将向ChatGPT提供代码的输出。
- en: '[P**rompt]**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[**快速**]'
- en: '[PRE7]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[E**nd of prompt]**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[**提示结束**]**'
- en: We should receive a response like the following where ChatGPT understands the
    problem and provides us with the corrected code.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT应理解问题并为我们提供修正后的代码。
- en: '[P**rompt response]**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[**快速响应**]'
- en: Thank you for providing the shape of `y_test`. It seems that the labels are
    in 1D format, which means they are not one-hot encoded. In the evaluation function,
    we assumed that the labels are one-hot encoded, which caused the error.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您提供`y_test`的形状。看起来标签是1D格式，这意味着它们不是one-hot编码的。在评估函数中，我们假设标签是one-hot编码的，这导致了错误。
- en: To resolve this issue, we need to modify the evaluation function to handle 1D
    labels instead of one-hot encoded labels. We can achieve this by removing the
    conversion of labels to one-hot encoding and directly compare the predicted labels
    with the true labels.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们需要修改评估函数以处理1D标签而不是one-hot编码的标签。我们可以通过移除标签到one-hot编码的转换，直接比较预测标签和真实标签来实现这一点。
- en: 'Let’s update the evaluation function accordingly:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们相应地更新评估函数：
- en: '[PRE8]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Explanation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We removed the conversion of labels to one-hot encoding and directly used the
    `y_test` true labels in the evaluation function.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了标签到one-hot编码的转换，并在评估函数中直接使用了`y_test`真实标签。
- en: We updated the accuracy, precision, and recall calculation functions to use
    the 1D labels instead of one-hot encoded labels.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新了准确率、精确率和召回率计算函数，以使用1D标签而不是one-hot编码的标签。
- en: Now, the evaluation function should work correctly with the provided 1D labels,
    and the evaluation metrics should be calculated accurately.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，评估函数应该能够正确地使用提供的1D标签，并且评估指标应该被准确计算。
- en: '[E**nd of response]**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of response**]'
- en: We will execute the code and it works fine now because the code provided has
    been modified to work with 1D labels. It also prints the confusion matrix.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行代码，现在它运行正常，因为提供的代码已被修改以使用1D标签。它还打印了混淆矩阵。
- en: '![A screenshot of a graph  Description automatically generated](img/B21232_14_08.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![一个图表的截图，描述自动生成](img/B21232_14_08.png)'
- en: 'Figure 14.8: Code execution – confusion matrix'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：代码执行 – 混淆矩阵
- en: This completes feature 1\. We achieved an accuracy of 0.8796 (87.96%), a precision
    of 0.8817 (88.17%), and a recall of 0.8796 (87.96%) with our single-layer MLP
    model. Next, we will move to the second feature, adding more layers to the model.
    We will also learn how ChatGPT can assist us in that.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了功能1。我们使用单层MLP模型实现了0.8796（87.96%）的准确率、0.8817（88.17%）的精确率和0.8796（87.96%）的召回率。接下来，我们将转向第二个功能，向模型添加更多层。我们还将学习ChatGPT如何帮助我们。
- en: 'Feature 2: Adding layers to the model'
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能2：向模型添加层
- en: Now that we have a baseline model architecture, let us try adding layers to
    it. We will apply a prompting strategy to improvise a model and ask ChatGPT to
    provide us with the code for it.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了基线模型架构，让我们尝试向其中添加层。我们将应用提示策略来改进模型，并要求ChatGPT提供其代码。
- en: '[P**rompt]**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt**]'
- en: Now that we have a baseline model with a single layer, can you please provide
    me code with two layers and three layers and calculate and compare the model performance
    due to the addition of layers?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个单层的基线模型，您能提供具有两个层和三个层的代码，并计算和比较由于添加层而导致的模型性能吗？
- en: '[E**nd of prompt]**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[E**nd of prompt**]'
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for two-layer and three-layer models along
    with comparison results.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下响应，其中ChatGPT理解请求并提供两个层和三个层模型的代码以及比较结果。
- en: '[P**rompt response]**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[P**rompt response**]'
- en: Certainly! Let’s define and train multi-layer perceptron (MLP) models with two
    and three layers, and then compare their performance with the baseline single-layer
    model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们定义并训练具有两个和三个层的多层感知器（MLP）模型，然后比较它们与基线单层模型的性能。
- en: 'Define MLP Models with Two and Three Layers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 定义具有两个和三个层的MLP模型：
- en: We’ll define MLP models with additional hidden layers to see how the performance
    changes with the increase in the number of layers.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将定义具有额外隐藏层的MLP模型，以查看性能如何随着层数的增加而变化。
- en: 'Two-Layer MLP:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 两层MLP：
- en: The two-layer MLP will consist of an input layer, two hidden layers, and an
    output layer.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两层MLP将包括输入层、两个隐藏层和输出层。
- en: 'Three-Layer MLP:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 三层MLP：
- en: The three-layer MLP will consist of an input layer, three hidden layers, and
    an output layer.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三层MLP将包括输入层、三个隐藏层和输出层。
- en: 'Now, let’s define and train these models:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义并训练这些模型：
- en: '[PRE9]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Model Comparison:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy, precision, recall, and confusion matrix.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，我们可以根据评估指标（如准确率、精确率、召回率和混淆矩阵）比较它们的性能。
- en: '[PRE10]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Explanation:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We define two additional MLP models with two and three hidden layers using the
    previously defined functions.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用之前定义的函数定义了两个额外的具有两个和三个隐藏层的MLP模型。
- en: We train both models using the training data and validation data.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用训练数据和验证数据训练两个模型。
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后，我们使用测试数据评估每个模型的表现，并比较它们的准确率。
- en: The comparison helps us understand how the addition of layers affects the model’s
    performance.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 比较有助于我们了解添加层如何影响模型的表现。
- en: '[E**nd of response]**'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束]**'
- en: We will execute the code that defines the two additional models with two and
    three hidden layers. It also trains the models, evaluates the performance of both
    models, and prints the results.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行定义具有两个和三个隐藏层的两个额外模型的代码。它还训练模型，评估这两个模型的表现，并打印结果。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_09.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成描述](img/B21232_14_09.png)'
- en: 'Figure 14.9: Feature 2 – adding layers to the model'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9：特性2 – 向模型添加层
- en: Based on these results, we can observe that the two-layer MLP model achieved
    slightly higher accuracy compared to both the single-layer and three-layer MLP
    models. However, the differences in accuracy are relatively small, indicating
    that the addition of layers did not significantly improve the model’s performance
    in this case. Let’s move on to the next feature and experiment with batch sizes
    using the baseline model.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些结果，我们可以观察到，与单层和三层MLP模型相比，两层MLP模型实现了略高的准确率。然而，准确率之间的差异相对较小，这表明在这种情况下，添加层并没有显著提高模型的表现。让我们继续到下一个特性，并使用基线模型实验批量大小。
- en: 'Feature 3: Experimenting with batch sizes'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特性3：实验批量大小
- en: We have now learned how to add more layers, and in our case, due to the dataset
    being simple, there was not much impact. We will now experiment with different
    batch sizes. We will apply a prompting strategy to improvise a model and ask ChatGPT
    to provide us the code for it.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学会了如何添加更多层，在我们的案例中，由于数据集简单，影响不大。我们现在将尝试不同的批量大小。我们将应用提示策略来改进模型，并要求ChatGPT为我们提供相应的代码。
- en: '[P**rompt]**'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]**'
- en: Can you please provide me code with three combinations of batch sizes and compare
    the baseline single-layer model performance for these combinations?
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请提供具有三个批量大小组合的代码，并比较基线单层模型在这些组合中的性能？
- en: '[E**nd of prompt]**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]**'
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different batch sizes along with comparison
    results.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中ChatGPT理解请求，并为我们提供不同批量大小的代码以及比较结果。
- en: '[P**rompt response]**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示响应]**'
- en: 'Define and Train Models with Different Batch Sizes:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 定义和训练具有不同批量大小的模型：
- en: We’ll define and train the baseline single-layer MLP model with three different
    combinations of batch sizes.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将定义并训练具有三个不同批量大小组合的基线单层MLP模型。
- en: 'Combination 1: `Batch Size = 32`'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合1：`批量大小 = 32`
- en: 'Combination 2: `Batch Size = 64`'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合2：`批量大小 = 64`
- en: 'Combination 3: `Batch Size = 128`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合3：`批量大小 = 128`
- en: 'Now, let’s define and train these models:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义和训练这些模型：
- en: '[PRE11]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Model Comparison:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，我们可以根据评估指标如准确率来比较它们的性能。
- en: '[PRE12]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Explanation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We define and train three baseline single-layer MLP models with different combinations
    of batch sizes (32, 64, and 128).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义并训练了三个具有不同批量大小组合（32、64和128）的基线单层MLP模型。
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后，我们使用测试数据评估每个模型的表现，并比较它们的准确率。
- en: The comparison helps us understand how different batch sizes affect the model’s
    performance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 比较有助于我们了解不同的批量大小如何影响模型的表现。
- en: '[E**nd of response]**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束]**'
- en: We will execute the code that defines models with batch sizes 32, 64, and 128\.
    It also trains the models, evaluates their performance, and prints the results
    of the comparison of this performance.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行定义具有批量大小32、64和128的模型的代码。它还训练模型，评估其性能，并打印性能比较的结果。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_10.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成描述](img/B21232_14_10.png)'
- en: 'Figure 14.10: Code execution – batch size comparison'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10：代码执行 – 批量大小比较
- en: We can see that there is not much difference between the models’ accuracies,
    so we will apply the ‘Learn prompting strategy’ (LIFE 3.1) and ask ChatGPT for
    possible reasons as to why this might be.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型之间的准确率没有太大差异，因此我们将应用“学习提示策略”（LIFE 3.1）并询问ChatGPT可能的原因，为什么这可能会发生。
- en: '[P**rompt]**'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]**'
- en: '[PRE13]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Why there is not much difference in the accuracies?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么准确率之间没有太大差异？
- en: '[E**nd of prompt]**'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]**'
- en: We should receive a response like the following where ChatGPT explains the possible
    reasons.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下这样的响应，其中ChatGPT解释可能的原因。
- en: '[P**rompt response]**'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应提示]**'
- en: 'The differences in accuracy between the models trained with different batch
    sizes are relatively small. There could be several reasons for this:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同批量大小训练的模型之间的准确率差异相对较小。这可能有几个原因：
- en: '**Dataset Size**: The Fashion-MNIST dataset is relatively small compared to
    other datasets commonly used in deep learning. With a smaller dataset, the impact
    of batch size on model performance may be less pronounced.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小**：与深度学习中常用的其他数据集相比，Fashion-MNIST数据集相对较小。在较小的数据集上，批量大小对模型性能的影响可能不那么明显。'
- en: '**Model Complexity**: The baseline single-layer MLP model may not be complex
    enough to fully leverage the benefits of different batch sizes. In more complex
    models, such as deep neural networks with multiple layers, the impact of batch
    size on training dynamics may be more significant.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂度**：基准单层MLP模型可能不足以充分利用不同批量大小的优势。在更复杂的模型中，如多层深度神经网络，批量大小对训练动态的影响可能更为显著。'
- en: '**Optimization Algorithm**: The Adam optimizer used in training the models
    adapts the learning rate for each parameter based on the first and second moments
    of the gradients. This adaptive learning rate can mitigate the impact of batch
    size to some extent.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：在训练模型时使用的Adam优化器根据梯度的第一和第二矩来调整每个参数的学习率。这种自适应学习率可以在一定程度上减轻批量大小的影响。'
- en: '**Convergence**: The models may have already converged to a similar level of
    performance regardless of the batch size used. Once a model converges, further
    training with different batch sizes may not lead to substantial improvements in
    accuracy.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收敛**：无论使用什么批量大小，模型可能已经收敛到相似的性能水平。一旦模型收敛，使用不同批量大小进行进一步训练可能不会在准确率上带来实质性的改进。'
- en: Overall, while batch size can influence training dynamics and convergence speed,
    its impact on model performance may be less pronounced in certain scenarios, especially
    with relatively simple models and datasets. Experimenting with larger and more
    complex models, as well as different optimization algorithms, may reveal more
    significant differences in performance based on batch size.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，虽然批量大小可以影响训练动态和收敛速度，但在某些情况下，其对模型性能的影响可能不那么明显，尤其是在相对简单的模型和数据集上。尝试更大和更复杂的模型以及不同的优化算法，可能会根据批量大小揭示更显著的性能差异。
- en: '[E**nd of response]**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束]**'
- en: Now that we have learned how to change the batch size and what the possible
    reasons could be for it not having much impact on our model, let’s move on to
    the next feature and experiment with the number of neurons in the hidden layer
    of the baseline model.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何更改批量大小以及为什么它对我们的模型影响不大，让我们继续到下一个特性，并尝试基准模型隐藏层中神经元数量的变化。
- en: 'Feature 4: Experimenting with the number of neurons'
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特性4：实验神经元数量
- en: We will apply a prompting strategy to improvise a model and ask ChatGPT to provide
    us the code for experimenting with different numbers of neurons in the hidden
    layer.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用提示策略来改进模型，并要求ChatGPT提供实验不同隐藏层神经元数量的代码。
- en: '[P**rompt]**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示]**'
- en: Can you please provide me code with three combinations of numbers of neurons
    and compare the baseline single-layer model performance for the various combinations?
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请提供包含三个神经元数量组合的代码，并比较各种组合的基准单层模型性能？
- en: '[E**nd of prompt]**'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束]**'
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different numbers of neurons in the
    hidden layer, along with the comparison results.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下这样的响应，其中ChatGPT理解请求并提供不同隐藏层神经元数量的代码，以及比较结果。
- en: '[P**rompt response]**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应提示]**'
- en: Of course! Let’s define and train the baseline single-layer MLP model with three
    different combinations of the number of neurons in the hidden layer, and then
    compare their performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们定义并训练基线单层 MLP 模型，使用隐藏层中神经元数量的三种不同组合，然后比较它们的性能。
- en: 'Define and Train Models with Different Number of Neurons:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 定义并训练具有不同神经元数量的模型：
- en: 'We’ll define and train the baseline single-layer MLP model with three different
    combinations of the number of neurons in the hidden layer:'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将定义并训练基线单层 MLP 模型，使用隐藏层中神经元数量的三种不同组合：
- en: 'Combination 1: `Number of Neurons = 64`'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '组合 1: `神经元数量 = 64`'
- en: 'Combination 2: `Number of Neurons = 128`'
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '组合 2: `神经元数量 = 128`'
- en: 'Combination 3: `Number of Neurons = 256`'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '组合 3: `神经元数量 = 256`'
- en: 'Now, let’s define and train these models:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义并训练这些模型：
- en: '[PRE14]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Model Comparison:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以根据评估指标（如准确率）比较它们的性能。
- en: '[PRE15]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Explanation:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We define and train three baseline single-layer MLP models with different combinations
    of the number of neurons in the hidden layer (64, 128, and 256).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义并训练了三个基线单层 MLP 模型，它们在隐藏层中神经元数量的组合不同（64、128 和 256）。
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完成后，我们使用测试数据评估每个模型的性能，并比较它们的准确率。
- en: The comparison helps us understand how different numbers of neurons in the hidden
    layer affect the model’s performance.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 比较有助于我们了解隐藏层中不同数量的神经元如何影响模型性能。
- en: '[E**nd of response]**'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[响应结束**]'
- en: We will execute the code that defines the models with 64, 128, and 256 neurons.
    It also trains the model, evaluates the performance for all the model variations,
    and prints the results of the comparison of their performance.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行定义具有 64、128 和 256 个神经元的模型的代码。它还训练模型，评估所有模型变体的性能，并打印性能比较的结果。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_11.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图，描述自动生成](img/B21232_14_11.png)'
- en: 'Figure 14.11: Code execution – neurons comparison'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11：代码执行 – 神经元比较
- en: We can see that there is not much difference, so let’s move to our final feature
    of experimenting with different optimizers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到差异不大，因此让我们转向我们的最终功能，即尝试不同的优化器。
- en: 'Feature 5: Trying different optimizers'
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能 5：尝试不同的优化器
- en: We will apply a prompting strategy to improvise a model and ask ChatGPT to provide
    us code for experimenting with different optimizers such as SGD, Adam, and RMSProp.
    Also, as part of our prompting strategy, we will be specific and explicitly mention
    that the optimizer should be passed at compile time and that labels should be
    one-hot encoded, as these steps are commonly ignored by ChatGPT.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用提示策略来改进模型，并要求 ChatGPT 提供用于实验不同优化器（如 SGD、Adam 和 RMSProp）的代码。作为我们的提示策略的一部分，我们将具体明确地提到优化器应在编译时传递，并且标签应进行
    one-hot 编码，因为这些步骤通常被 ChatGPT 忽略。
- en: '[P**rompt]**'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示**]'
- en: Can you please provide me code with different optimizers of SGD, Adam, and RMSprop
    and compare the baseline single-layer model performance for various combinations?
    The optimizer should be passed in at compile time and you should ensure that labels
    are one-hot encoded.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请您提供具有不同优化器（SGD、Adam 和 RMSprop）的代码，并比较基线单层模型在不同组合下的性能。优化器应在编译时传递，并且您应确保标签进行
    one-hot 编码。
- en: '[E**nd of prompt]**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示结束**]'
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different optimizers along with comparison
    results.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到以下类似的响应，其中 ChatGPT 理解请求并提供不同优化器（如 SGD、Adam 和 RMSProp）的代码以及比较结果。
- en: '[P**rompt response]**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示响应**]'
- en: Certainly! Let’s define and train a baseline single-layer MLP model with three
    different optimizers (SGD, Adam, and RMSprop), and then compare their performance.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们定义并训练一个基线单层 MLP 模型，使用三种不同的优化器（SGD、Adam 和 RMSprop），然后比较它们的性能。
- en: 'Define and Train Models with Different Optimizers:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 定义并训练具有不同优化器的模型：
- en: 'We’ll define and train the baseline single-layer MLP model with three different
    optimizers: SGD, Adam, and RMSprop.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义并训练基线单层 MLP 模型，使用三种不同的优化器：SGD、Adam 和 RMSprop。
- en: 'Now, let’s define and train these models:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义并训练这些模型：
- en: '[PRE16]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Model Comparison:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 模型比较：
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以根据评估指标（如准确率）比较它们的性能。
- en: '[PRE17]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Explanation:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 说明：
- en: We define and train three baseline single-layer MLP models with different optimizers
    (SGD, Adam, and RMSprop).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义并训练了三个具有不同优化器（SGD、Adam 和 RMSprop）的基线单层 MLP 模型。
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后，我们使用测试数据评估每个模型的性能，并比较它们的准确率。
- en: The comparison helps us understand how different optimizers affect the model’s
    performance.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 比较有助于我们了解不同的优化器如何影响模型的表现。
- en: '**[End of response]**'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束回复]**'
- en: We execute the code that defines the models with different optimizers. It also
    trains the model, evaluates the performance of all the model variations, and prints
    the results for a comparison of their performance.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行定义具有不同优化器的模型的代码。它还训练模型，评估所有模型变体的性能，并打印出比较它们性能的结果。
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_12.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  描述自动生成](img/B21232_14_12.png)'
- en: 'Figure 14.12: Code Execution – Different Optimizers'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：代码执行 – 不同优化器
- en: Based on these results, we can observe that the Adam optimizer achieved the
    highest accuracy, followed by RMSprop and SGD. This indicates that Adam performed
    better in optimizing the model’s parameters and improving its performance on the
    testing data.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些结果，我们可以观察到 Adam 优化器实现了最高的准确率，其次是 RMSprop 和 SGD。这表明 Adam 在优化模型参数和改进测试数据上的性能方面表现更好。
- en: Assignment
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作业
- en: Compare the model performance of a single layer versus two hidden layers, but
    with 20 and 50 epochs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 比较单层与两层隐藏层的模型性能，但使用 20 和 50 个周期。
- en: Challenge
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: With the help of ChatGPT, improvise a model by adding a dropout layer and analyze
    the impact on model performance. Feel free to add more hidden layers to the model.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 的帮助下，通过添加 dropout 层改进模型，并分析其对模型性能的影响。请随意向模型添加更多隐藏层。
- en: Summary
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The prompt strategies used in this chapter provided a structured approach to
    learning and building a classification model using an MLP where ChatGPT assisted
    in generating code. The user validated the code using a Colab notebook and provided
    feedback to ChatGPT. By actively engaging with the material, you experimented
    with different techniques and iteratively refined your understanding, ultimately
    leading to a more comprehensive grasp of classification model creation using MLPs.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的提示策略提供了一种结构化的学习方法，用于使用 MLP 构建分类模型，其中 ChatGPT 协助生成代码。用户使用 Colab 笔记本验证代码，并向
    ChatGPT 提供反馈。通过积极参与材料，你尝试了不同的技术，并迭代地完善了对使用 MLP 创建分类模型的理解，最终达到了更全面的理解。
- en: In the next chapter, we will learn how to use ChatGPT to generate code for **Convolutional
    Neural Networks** (**CNNs**) with the help of the CIFAR-10 dataset.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何在 CIFAR-10 数据集的帮助下，使用 ChatGPT 生成用于 **卷积神经网络**（**CNNs**）的代码。
- en: Join our community on Discord
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/aicode](https://packt.link/aicode)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/aicode](https://packt.link/aicode)'
- en: '![](img/QR_Code510410532445718281.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![QR 码图片](img/QR_Code510410532445718281.png)'
