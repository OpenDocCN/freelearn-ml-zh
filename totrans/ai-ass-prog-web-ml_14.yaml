- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building an MLP Model for Fashion-MNIST with ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building upon our foundational understanding of predictive modeling, we now
    dive into the dynamic world of **Multilayer Perceptron** (**MLP**) models. In
    this chapter, we embark on a journey to construct an MLP model from scratch, leveraging
    the versatility and power of neural networks for predictive analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration of MLPs represents a significant leap into the realm of complex
    modeling techniques. While linear regression provided valuable insights into modeling
    relationships within data, MLPs offer a rich framework for capturing intricate
    patterns and nonlinear dependencies, making them well suited for a wide range
    of predictive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Through hands-on experimentation and iterative refinement, we will unravel the
    intricacies of MLP architecture and optimization. From designing the initial network
    structure to fine-tuning hyperparameters and incorporating advanced techniques
    such as batch normalization and dropout, we aim to equip you with the knowledge
    and skills to harness the full potential of neural networks in predictive modeling.
  prefs: []
  type: TYPE_NORMAL
- en: As we navigate through the construction and optimization of our MLP model, we
    will delve into the underlying principles of neural network dynamics, exploring
    how different architectural choices and optimization strategies influence model
    performance and generalization capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fashion e-commerce store seeks to optimize customer engagement and increase
    revenue by leveraging machine learning techniques to gain deeper insights into
    customer behavior and preferences. By analyzing image data representing various
    fashion items purchased by customers, the store aims to tailor its product recommendations,
    improve customer satisfaction, and enhance the overall shopping experience.
  prefs: []
  type: TYPE_NORMAL
- en: Problem and data domain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will employ MLP models to understand the relationship between
    customers’ preferences and their purchasing patterns using the Fashion-MNIST dataset.
    MLP models offer a powerful framework for image classification tasks, allowing
    us to predict the type of clothing or accessory a customer is likely to purchase
    based on their interactions with the online store. By uncovering patterns in customer
    preferences, the e-commerce store can personalize recommendations and optimize
    inventory management to meet the diverse needs of its customer base.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fashion e-commerce store collects image data representing various fashion
    items, categorized into different classes, from its customers. The Fashion-MNIST
    dataset comprises 70,000 grayscale images of clothing and accessories, each associated
    with a specific label indicating its category and of size 28x28.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features in the dataset include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image data**: Grayscale images of fashion items, each represented as a matrix
    of pixel intensities. These images serve as the input data for training the MLP
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label**: The category label assigned to each image, representing the type
    of clothing or accessory depicted. Labels range from 0 to 9, corresponding to
    classes such as T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,
    Bag, and Ankle boot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By analyzing this image data and its corresponding labels, we aim to train an
    MLP model capable of accurately classifying fashion items based on their visual
    features. This predictive model will enable the e-commerce store to make personalized
    product recommendations, enhance customer engagement, and ultimately increase
    revenue by providing a seamless shopping experience tailored to individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21232_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Fashion-MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the problem down into features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the nature of the Fashion-MNIST dataset, which comprises grayscale images
    of fashion items categorized into different classes, we will start by building
    a baseline MLP model. This will involve the following high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the baseline model**: Users will understand the process of constructing
    a simple MLP model for image classification using ChatGPT. We will guide users
    through loading the Fashion-MNIST dataset, preprocessing the image data, splitting
    it into training and testing sets, defining the model architecture, training the
    model, making predictions, and evaluating its performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adding layers to the model**: Once the baseline model is established, users
    will learn how to experiment with adding additional layers to the MLP architecture.
    We will explore how increasing the depth or width of the model impacts its performance
    and capacity to capture complex patterns in the image data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Experimenting with batch sizes**: Users will experiment with different batch
    sizes during model training to observe their effects on training speed, convergence,
    and generalization performance. We will explore how varying batch sizes influence
    the trade-off between computation efficiency and model stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adjusting the number of neurons**: Users will explore the impact of adjusting
    the number of neurons in each layer of the MLP model. By increasing or decreasing
    the number of neurons, users can observe changes in model capacity and its ability
    to learn intricate features from the image data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Trying different optimizers**: Finally, users will experiment with different
    optimization algorithms, such as SGD, Adam, and RMSprop, to optimize the training
    process of the MLP model. We will observe how different optimizers influence training
    dynamics, convergence speed, and final model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these steps, users will gain a comprehensive understanding of building
    and optimizing MLP models for image classification tasks using the Fashion-MNIST
    dataset. They will learn how to iteratively refine the model architecture and
    training process to achieve optimal performance and accuracy in classifying fashion
    items.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To leverage ChatGPT for machine learning we need to have a clear understanding
    of how to implement the prompting strategies specifically for code generation
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s brainstorm what we would like to achieve in this task to get a better
    understanding of what needs to go into prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 1: Task-Actions-Guidelines (TAG) prompt strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1.1 - Task**: The specific task or goal is to create a classification model
    for the Fashion-MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.2 - Actions**: The key steps involved in creating a classification model
    using an MLP for the Fashion-MNIST dataset are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data preprocessing: Normalize pixel values, flatten images into vectors, and
    encode categorical labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data splitting: Partition the dataset into training, validation, and testing
    sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model selection: Opt for an MLP as the classification model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model training: Train the MLP on the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model evaluation: Use metrics like accuracy, precision, recall, and confusion
    matrix to evaluate the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1.3 - Guidelines**: We will provide the following guidelines to ChatGPT in
    our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: The code should be compatible with Jupyter notebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that there are detailed comments for each line of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to explain each line of code, which will be then copied into the text
    block of the notebook, in detail for each method used in the code before providing
    the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strategy 2: Persona-Instructions-Context (PIC) prompt strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**2.1 - Persona**: We will adopt the persona of a beginner who needs to learn
    different steps of model creation, hence the code should be generated step by
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.2 - Instructions**: We have specified that we want the code generated for
    an MLP model with a single layer *and have instructed ChatGPT to provide one step
    at a time and wait for the user’s feedback.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.3 - Context**: In this case, ChatGPT is already aware of the Fashion-MNIST
    dataset, as it is widely known, so we do not need to provide additional context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategy 3: Learn-Improvise-Feedback-Evaluate (LIFE) prompt strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**3.1 - Learn**:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to learn about MLP models and how they work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3.2 - Improvise**:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use this knowledge later when adding more layers, changing the batch
    size or activation functions, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3.3 - Feedback**:'
  prefs: []
  type: TYPE_NORMAL
- en: Share code and model outputs for feedback to ensure effective learning and understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate suggestions and critiques to refine the model and the approach**.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide errors to troubleshoot and resolve the issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3.4- Evaluate**:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute the code provided by ChatGPT to ensure accuracy and validity. This is
    used throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building an MLP model to accurately classify the Fashion-MNIST images using
    the free version of ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT’s premium version has a code interpreter, but it doesn’t support the
    deep learning libraries such as Keras and TensorFlow required to execute the code.
    Hence, for this chapter, we will stick to the free version of ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 1: Building the baseline model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s craft our initial prompt for baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I want to create a simple classification model for the Fashion-MNIST dataset
    (TAG 1.1) consisting of a single-layer MLP model (PIC 2.2). It should consist
    of the following steps (TAG1.2), provide one step at a time, and wait for the
    user’s feedback (PIC 2.2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Preprocessing: Normalize pixel values, flatten images into vectors, and
    encode categorical labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Splitting: Partition the dataset into training, validation, and testing
    sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Selection: Opt for a Multi-Layer Perceptron (MLP) as the classification
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Training: Train the MLP on the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Evaluation: Use metrics like accuracy, precision, recall, and confusion
    matrix to evaluate the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I am a beginner user (PIC 2.1), so please provide step-by-step code and strictly
    follow the below instructions (TAG 1.3):'
  prefs: []
  type: TYPE_NORMAL
- en: The code should be compatible with the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that there are detailed comments for each line of code.
  prefs: []
  type: TYPE_NORMAL
- en: You have to explain each line of code, which will be then copied into the text
    block of notebook, in detail for each method used in the code before providing
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: You should receive a response similar to the following, where ChatGPT has provided
    us with the code for step 1, Data Preprocessing, along with detailed comments
    and explanations.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, let’s start with the first step: Data Preprocessing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing is a crucial step in machine learning, where we prepare our
    data for training by applying various transformations and normalization techniques.
    In this step, we’ll normalize the pixel values, flatten the images into vectors,
    and encode categorical labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalize Pixel Values:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing pixel values ensures that each pixel’s value is scaled to a range
    between 0 and 1\. This is important for the neural network to converge faster
    during training and to prevent any particular feature from dominating the learning
    process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flatten Images into Vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: MLP models cannot directly work with multi-dimensional data like images. Hence,
    we need to flatten the images into 1D vectors, where each pixel value becomes
    a feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encode Categorical Labels:'
  prefs: []
  type: TYPE_NORMAL
- en: Since our labels are categorical (e.g., different types of clothing items),
    we need to encode them into numerical values so that our model can understand
    and learn from them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s implement these steps in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We import necessary libraries such as NumPy for numerical operations, TensorFlow
    for loading the Fashion-MNIST dataset, and scikit-learn for data preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the Fashion-MNIST dataset using the `fashion_mnist.load_data()` method
    and split it into training and testing sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three functions, `normalize_images`, `flatten_images`, and `encode_labels`,
    are defined to perform each data preprocessing step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside each function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`normalize_images` divides pixel values by 255 to scale them between 0 and
    1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flatten_images` reshapes the images into 1D vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encode_labels` converts categorical labels into numerical values using `LabelEncoder`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we apply these preprocessing steps to the training and testing data
    and print their shapes to ensure everything is processed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will now apply the prompt evaluation strategy (LIFE 3.4) and validate the
    code by executing the code for every step in the Google Colab environment. The
    code loads the model from Keras and normalizes the images, flattens them into
    vectors, encodes labels using LabelEncoder, then applies the preprocessing and
    prints the shape. The code runs perfectly fine without applying any modifications
    or any errors.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the `Learning MLP using Fashion-MNIST dataset with ChatGPT.ipynb`
    notebook for the executable code snippets or copy the code from the prompt response.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Code execution – data preprocessing'
  prefs: []
  type: TYPE_NORMAL
- en: We will now instruct ChatGPT to proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following, where ChatGPT should provide
    the code for step 2 – Data Splitting.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! Let’s move on to the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Splitting:'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we’ll partition the dataset into training, validation, and testing
    sets. The training set will be used to train the model, the validation set will
    be used to tune hyperparameters and prevent overfitting, and the testing set will
    be used to evaluate the model’s performance on unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training Set:'
  prefs: []
  type: TYPE_NORMAL
- en: The training set is used to train the model’s parameters (weights and biases)
    using optimization algorithms like gradient descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation Set:'
  prefs: []
  type: TYPE_NORMAL
- en: The validation set is used to evaluate the model’s performance during training
    and adjust hyperparameters such as learning rate, batch size, and number of epochs
    to optimize the model’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Testing Set:'
  prefs: []
  type: TYPE_NORMAL
- en: The testing set is used to evaluate the final performance of the trained model
    on unseen data and assess its generalization capability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s split the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We define a `split_dataset` function to split the dataset into training, validation,
    and testing sets using the `train_test_split` function from scikit-learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function takes training data, training labels, testing data, testing labels,
    and optional parameters such as validation size, test size, and random state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We split the training set into training and validation sets using the specified
    validation size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we split the testing set into testing and validation sets using the specified
    test size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we print the shapes of the split datasets to verify the split.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We execute the code in the notebook and it runs successfully. The data is split
    into training, validation, and test sets with the specified sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Code execution – data splitting'
  prefs: []
  type: TYPE_NORMAL
- en: We will now instruct ChatGPT to proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to the next step and define a single-layer baseline MLP model.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT should provide
    the code for a single-layer MLP model with an input layer, one hidden layer, and
    an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, let’s move on to the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Selection:'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we’ll choose a Multi-Layer Perceptron (MLP) as the classification
    model. An MLP consists of one or more layers of neurons, each fully connected
    to the next layer. For simplicity, we’ll start with a single-layer MLP as a baseline
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Single-Layer MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: A single-layer MLP consists of an input layer, a hidden layer, and an output
    layer. However, since it has only one hidden layer, it’s often referred to as
    a single-layer perceptron. Each neuron in the hidden layer receives input from
    all neurons in the input layer and produces an output, which is then passed through
    an activation function to introduce non-linearity. Finally, the output layer produces
    the final prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s define the baseline single-layer MLP model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We import the necessary modules from `TensorFlow.keras` to define the MLP model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a function `create_single_layer_mlp` to create a single-layer MLP
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We initialize a sequential model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We add a single hidden layer with 128 neurons and ReLU activation function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We add an output layer with softmax activation function for multi-class classification.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We specify the input shape based on the shape of the training data and the number
    of output classes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We create the single-layer MLP model using the defined function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we print the summary of the model architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We execute the code and it runs without any errors and prints the model summary
    with the layer type, output shape, and number of parameters. It also provides
    us with the total parameters, trainable and non-trainable parameters. In our case,
    all the parameters are trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Code execution – baseline model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: We will now instruct ChatGPT to proceed to the next step for model training.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT should provide
    the code for model training along with explanations of the key concepts involved,
    such as loss functions and optimization algorithms along with comments.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, let’s move on to the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Training:'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we’ll train the MLP model on the training data. Training involves
    feeding the training data to the model, adjusting its parameters (weights and
    biases) using optimization algorithms like stochastic gradient descent, and minimizing
    a loss function to make predictions as close to the actual labels as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss Function:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss function measures the difference between the predicted labels and the
    actual labels. For multi-class classification problems like Fashion-MNIST, we
    typically use categorical cross-entropy as the loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimization Algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimization algorithm is responsible for updating the model’s parameters
    during training to minimize the loss function. We’ll use Adam optimizer, which
    is an extension to stochastic gradient descent and is widely used in deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll specify training parameters such as batch size and number of epochs. Batch
    size determines the number of samples processed before updating the model’s parameters,
    while the number of epochs specifies how many times the entire training dataset
    is passed forward and backward through the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We import the necessary modules from `TensorFlow.keras` to compile and train
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a `train_model` function to train the model on the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We convert the class labels to binary class matrices using one-hot encoding.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We compile the model with the categorical cross-entropy loss function and the
    Adam optimizer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the model on the training data with the specified batch size and number
    of epochs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we train the model using the defined function and store the training
    history for visualization and analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We execute the code and it runs without any errors. It applies a one-hot encoding
    to class labels, complies the model with the categorical cross-entropy loss function
    and Adam optimizer, and trains the model with batch size 32 for 10 epochs. This
    achieves a validation accuracy of approximately 88%.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Code execution – model training'
  prefs: []
  type: TYPE_NORMAL
- en: We will now instruct ChatGPT to proceed to the next step for model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT should provide
    the code for evaluating the model against the test set, along with explanations
    of concepts like accuracy, precision, recall, and the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we’ll use various metrics to evaluate the performance of the trained
    MLP model on the testing data. Commonly used metrics for classification tasks
    include accuracy, precision, recall, and the confusion matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy measures the proportion of correctly classified instances out of the
    total instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision measures the proportion of true positive predictions out of all positive
    predictions. It indicates the model’s ability to avoid false positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall measures the proportion of true positive predictions out of all actual
    positive instances. It indicates the model’s ability to identify all positive
    instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confusion Matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix provides a summary of the model’s performance by showing
    the number of true positive, true negative, false positive, and false negative
    predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We import necessary modules from `matplotlib.pyplot` and scikit-learn to visualize
    the confusion matrix and calculate evaluation metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define an `evaluate_model` function to evaluate the model’s performance on
    the testing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We make predictions on the testing data using the trained model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We convert the predicted labels and true labels to class labels.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We calculate accuracy, precision, recall, and confusion matrix using scikit-learn
    metrics functions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we evaluate the model using the defined function, print the evaluation
    metrics, and visualize the confusion matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We execute the code and note that it experiences an error due to the test labels
    not being encoded, but this is not evident from the error itself. We can get help
    from ChatGPT to resolve the error by applying the prompting strategy of providing
    the error to ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer code  Description automatically generated](img/B21232_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Code execution – error for model evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: We provided the error to ChatGPT to get assistance.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looks like there is some issue with the test labels. We received the below
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT should provide
    the suggestions to troubleshoot the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: It seems like there’s an issue with the shape of the labels. The error indicates
    that the axis specified for the argmax function is out of bounds. This typically
    occurs when the labels are not in the expected format or shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s double-check the shape of the labels and make sure they are in the correct
    format for evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will help us understand the shape of the labels and identify any discrepancies.
    Once we identify the issue, we can correct it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We print the shape of `y_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A white rectangular object with red text  Description automatically generated](img/B21232_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Code execution - shape of test labels'
  prefs: []
  type: TYPE_NORMAL
- en: We will now provide ChatGPT with the output of the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT understands the
    problem and provides us with the corrected code.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for providing the shape of `y_test`. It seems that the labels are
    in 1D format, which means they are not one-hot encoded. In the evaluation function,
    we assumed that the labels are one-hot encoded, which caused the error.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this issue, we need to modify the evaluation function to handle 1D
    labels instead of one-hot encoded labels. We can achieve this by removing the
    conversion of labels to one-hot encoding and directly compare the predicted labels
    with the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s update the evaluation function accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We removed the conversion of labels to one-hot encoding and directly used the
    `y_test` true labels in the evaluation function.
  prefs: []
  type: TYPE_NORMAL
- en: We updated the accuracy, precision, and recall calculation functions to use
    the 1D labels instead of one-hot encoded labels.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the evaluation function should work correctly with the provided 1D labels,
    and the evaluation metrics should be calculated accurately.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will execute the code and it works fine now because the code provided has
    been modified to work with 1D labels. It also prints the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  Description automatically generated](img/B21232_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Code execution – confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: This completes feature 1\. We achieved an accuracy of 0.8796 (87.96%), a precision
    of 0.8817 (88.17%), and a recall of 0.8796 (87.96%) with our single-layer MLP
    model. Next, we will move to the second feature, adding more layers to the model.
    We will also learn how ChatGPT can assist us in that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 2: Adding layers to the model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a baseline model architecture, let us try adding layers to
    it. We will apply a prompting strategy to improvise a model and ask ChatGPT to
    provide us with the code for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a baseline model with a single layer, can you please provide
    me code with two layers and three layers and calculate and compare the model performance
    due to the addition of layers?
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for two-layer and three-layer models along
    with comparison results.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Certainly! Let’s define and train multi-layer perceptron (MLP) models with two
    and three layers, and then compare their performance with the baseline single-layer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define MLP Models with Two and Three Layers:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll define MLP models with additional hidden layers to see how the performance
    changes with the increase in the number of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two-Layer MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: The two-layer MLP will consist of an input layer, two hidden layers, and an
    output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three-Layer MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: The three-layer MLP will consist of an input layer, three hidden layers, and
    an output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s define and train these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Model Comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy, precision, recall, and confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We define two additional MLP models with two and three hidden layers using the
    previously defined functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train both models using the training data and validation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparison helps us understand how the addition of layers affects the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will execute the code that defines the two additional models with two and
    three hidden layers. It also trains the models, evaluates the performance of both
    models, and prints the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Feature 2 – adding layers to the model'
  prefs: []
  type: TYPE_NORMAL
- en: Based on these results, we can observe that the two-layer MLP model achieved
    slightly higher accuracy compared to both the single-layer and three-layer MLP
    models. However, the differences in accuracy are relatively small, indicating
    that the addition of layers did not significantly improve the model’s performance
    in this case. Let’s move on to the next feature and experiment with batch sizes
    using the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 3: Experimenting with batch sizes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now learned how to add more layers, and in our case, due to the dataset
    being simple, there was not much impact. We will now experiment with different
    batch sizes. We will apply a prompting strategy to improvise a model and ask ChatGPT
    to provide us the code for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Can you please provide me code with three combinations of batch sizes and compare
    the baseline single-layer model performance for these combinations?
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different batch sizes along with comparison
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define and Train Models with Different Batch Sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll define and train the baseline single-layer MLP model with three different
    combinations of batch sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 1: `Batch Size = 32`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 2: `Batch Size = 64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 3: `Batch Size = 128`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s define and train these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Model Comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We define and train three baseline single-layer MLP models with different combinations
    of batch sizes (32, 64, and 128).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparison helps us understand how different batch sizes affect the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will execute the code that defines models with batch sizes 32, 64, and 128\.
    It also trains the models, evaluates their performance, and prints the results
    of the comparison of this performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Code execution – batch size comparison'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there is not much difference between the models’ accuracies,
    so we will apply the ‘Learn prompting strategy’ (LIFE 3.1) and ask ChatGPT for
    possible reasons as to why this might be.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Why there is not much difference in the accuracies?
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT explains the possible
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences in accuracy between the models trained with different batch
    sizes are relatively small. There could be several reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset Size**: The Fashion-MNIST dataset is relatively small compared to
    other datasets commonly used in deep learning. With a smaller dataset, the impact
    of batch size on model performance may be less pronounced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Complexity**: The baseline single-layer MLP model may not be complex
    enough to fully leverage the benefits of different batch sizes. In more complex
    models, such as deep neural networks with multiple layers, the impact of batch
    size on training dynamics may be more significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization Algorithm**: The Adam optimizer used in training the models
    adapts the learning rate for each parameter based on the first and second moments
    of the gradients. This adaptive learning rate can mitigate the impact of batch
    size to some extent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convergence**: The models may have already converged to a similar level of
    performance regardless of the batch size used. Once a model converges, further
    training with different batch sizes may not lead to substantial improvements in
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, while batch size can influence training dynamics and convergence speed,
    its impact on model performance may be less pronounced in certain scenarios, especially
    with relatively simple models and datasets. Experimenting with larger and more
    complex models, as well as different optimization algorithms, may reveal more
    significant differences in performance based on batch size.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to change the batch size and what the possible
    reasons could be for it not having much impact on our model, let’s move on to
    the next feature and experiment with the number of neurons in the hidden layer
    of the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 4: Experimenting with the number of neurons'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will apply a prompting strategy to improvise a model and ask ChatGPT to provide
    us the code for experimenting with different numbers of neurons in the hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Can you please provide me code with three combinations of numbers of neurons
    and compare the baseline single-layer model performance for the various combinations?
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different numbers of neurons in the
    hidden layer, along with the comparison results.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Of course! Let’s define and train the baseline single-layer MLP model with three
    different combinations of the number of neurons in the hidden layer, and then
    compare their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define and Train Models with Different Number of Neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll define and train the baseline single-layer MLP model with three different
    combinations of the number of neurons in the hidden layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 1: `Number of Neurons = 64`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 2: `Number of Neurons = 128`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination 3: `Number of Neurons = 256`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s define and train these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Model Comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We define and train three baseline single-layer MLP models with different combinations
    of the number of neurons in the hidden layer (64, 128, and 256).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparison helps us understand how different numbers of neurons in the hidden
    layer affect the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will execute the code that defines the models with 64, 128, and 256 neurons.
    It also trains the model, evaluates the performance for all the model variations,
    and prints the results of the comparison of their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Code execution – neurons comparison'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there is not much difference, so let’s move to our final feature
    of experimenting with different optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature 5: Trying different optimizers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will apply a prompting strategy to improvise a model and ask ChatGPT to provide
    us code for experimenting with different optimizers such as SGD, Adam, and RMSProp.
    Also, as part of our prompting strategy, we will be specific and explicitly mention
    that the optimizer should be passed at compile time and that labels should be
    one-hot encoded, as these steps are commonly ignored by ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Can you please provide me code with different optimizers of SGD, Adam, and RMSprop
    and compare the baseline single-layer model performance for various combinations?
    The optimizer should be passed in at compile time and you should ensure that labels
    are one-hot encoded.
  prefs: []
  type: TYPE_NORMAL
- en: '[E**nd of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We should receive a response like the following where ChatGPT understands the
    request and provides us with the code for different optimizers along with comparison
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '[P**rompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Certainly! Let’s define and train a baseline single-layer MLP model with three
    different optimizers (SGD, Adam, and RMSprop), and then compare their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define and Train Models with Different Optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll define and train the baseline single-layer MLP model with three different
    optimizers: SGD, Adam, and RMSprop.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s define and train these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Model Comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the models are trained, we can compare their performance based on evaluation
    metrics such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We define and train three baseline single-layer MLP models with different optimizers
    (SGD, Adam, and RMSprop).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After training, we evaluate the performance of each model using the testing
    data and compare their accuracies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comparison helps us understand how different optimizers affect the model’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We execute the code that defines the models with different optimizers. It also
    trains the model, evaluates the performance of all the model variations, and prints
    the results for a comparison of their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated](img/B21232_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Code Execution – Different Optimizers'
  prefs: []
  type: TYPE_NORMAL
- en: Based on these results, we can observe that the Adam optimizer achieved the
    highest accuracy, followed by RMSprop and SGD. This indicates that Adam performed
    better in optimizing the model’s parameters and improving its performance on the
    testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Assignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compare the model performance of a single layer versus two hidden layers, but
    with 20 and 50 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the help of ChatGPT, improvise a model by adding a dropout layer and analyze
    the impact on model performance. Feel free to add more hidden layers to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The prompt strategies used in this chapter provided a structured approach to
    learning and building a classification model using an MLP where ChatGPT assisted
    in generating code. The user validated the code using a Colab notebook and provided
    feedback to ChatGPT. By actively engaging with the material, you experimented
    with different techniques and iteratively refined your understanding, ultimately
    leading to a more comprehensive grasp of classification model creation using MLPs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to use ChatGPT to generate code for **Convolutional
    Neural Networks** (**CNNs**) with the help of the CIFAR-10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/aicode](https://packt.link/aicode)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code510410532445718281.png)'
  prefs: []
  type: TYPE_IMG
