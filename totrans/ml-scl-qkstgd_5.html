<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Scala for Dimensionality Reduction and Clustering</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we saw several examples of supervised learning, covering both classification and regression. We performed supervised learning techniques on structured and labelled data. However, as we mentioned previously, with the rise of cloud computing, IoT, and social media, unstructured data is increasing unprecedentedly. Collectively, more than 80% of this data is unstructured and which most of them are unlabeled.</p>
<p>Unsupervised learning techniques, such as clustering analysis and dimensionality reduction, are two of the key applications in data-driven research and industry settings for finding hidden structures in unstructured datasets. There are many clustering algorithms being proposed for this, such as k-means, bisecting k-means, and the G<span class="ILfuVd">aussian mixture model</span>. However, these algorithms cannot perform with high-dimensional input datasets and often suffer from the <em>curse of dimensionality</em>. So, reducing the dimensionality using algorithms like <strong>principal component analysis</strong> (<strong>PCA</strong>) and feeding the latent data is a useful technique for clustering billions of data points.</p>
<p>In this chapter, we will use a genetic variant (one kind of genomic data) to cluster the population according to their predominant ancestry, also called geographic ethnicity. We will evaluate the clustering analysis result, followed by the dimensionality reduction technique, to avoid the curse of dimensionality.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Overview of unsupervised learning</li>
<li>Learning clustering—clustering geographic ethnicity</li>
<li>Dimensionality reduction with PCA</li>
<li>Clustering with reduced dimensional data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.</p>
<p>The code files of this chapters can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05</a></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2ISwb3o" target="_blank">http://bit.ly/2ISwb3o</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of unsupervised learning</h1>
                </header>
            
            <article>
                
<p>In unsupervised learning, an input set is provided to the system during the training phase. In contrast to supervised learning, the input objects are not labeled with their class. Although in classification analysis the training dataset is labeled, we do not always have that advantage when we collect data in the real world, but still we want to find important values or hidden structures of the data. <span class="markup--quote markup--blockquote-quote is-other">In NeuralIPS' 2016, Facebook AI Chief Yann LeCun introduced the <em>cake analogy</em>:</span></p>
<div class="packt_quote"><span class="markup--quote markup--blockquote-quote is-other">"If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don't know how to make the cake."</span></div>
<p>In order to create such a cake, several unsupervised learning tasks, including clustering, dimensionality reduction, anomaly detection, and association rule mining, are used. If unsupervised learning algorithms help find previously unknown patterns in a dataset without needing a label, we can learn the following analogy for this chapter:</p>
<ul>
<li>K-means is a popular clustering analysis algorithm for grouping similar data points together</li>
<li>A dimensionality reduction algorithm, such as PCA, helps find the most relevant features in a dataset</li>
</ul>
<p>In this chapter, we'll discuss these two techniques for cluster analysis with a practical example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering analysis</h1>
                </header>
            
            <article>
                
<p>Clustering analysis and dimensionality reduction are the two most popular examples of unsupervised learning, which we will discuss throughout this chapter with examples. Suppose you have a large collection of legal MP3 files in your computer or smartphones. In such a case, how could you possibly group songs together if you do not have direct access to their metadata?</p>
<p>One possible approach could be to mix various ML techniques, but clustering is often the best solution. This is because we can develop a clustering model in order to automatically group similar songs and organize them into your favorite categories, such as country, rap, or rock. </p>
<p>Although the data points are not labeled, we can still do the necessary feature engineering and group similar objects together, which is commonly referred to as clustering.</p>
<div class="packt_infobox"><span>A cluster refers to a collection of data points grouped together based on certain similarity measures.</span></div>
<p>However, this is not easy for a human. Instead, a standard approach is to define a similarity measure between two objects and then look for any cluster of objects that is more similar to each other than it is to the objects in the other clusters. Once we have done the clustering of the data points (that is, the MP3 files) and the validation is completed, we know the pattern of the data (that is, what type of MP3 files fall in which group).</p>
<p>The left-hand side diagram shows all the <strong>MP3 tracks in a playlist</strong>, which are scattered. The right-hand side part shows how the MP3 are clustered based on genre: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-571 image-border" src="assets/5a19db36-cb05-4e82-92d5-44ed0b747360.png" style="width:34.58em;height:19.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering analysis algorithms</h1>
                </header>
            
            <article>
                
<p>The goal of a clustering algorithm is to <span>group a similar set of unlabeled data points together to discover underlying patterns</span>. Here are some of the algorithms that have been proposed and used for clustering analysis:</p>
<ul>
<li>K-means</li>
<li>Bisecting k-means</li>
<li><strong>Gaussian mixture model</strong> (<strong>GMM</strong>)</li>
<li><strong>Power iteration clustering</strong> (<strong>PIC</strong>)</li>
<li><strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>)</li>
<li>Streaming k-means</li>
</ul>
<p class="mce-root">K-means, bisecting k-means, and GMM are the most widely used. They will be covered in detail to show a quick-start clustering analysis. However, we will also look at an example based on only k-means.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means for clustering analysis</h1>
                </header>
            
            <article>
                
<p><span>K-means looks for a fixed number</span> <em>k</em> <span>of clusters (that is,</span> the <span>number of centroids), partitions the data points into <em>k</em> clusters, and</span> <span>allocates every data point to the nearest cluster by keeping the centroids as small as possible.</span></p>
<div class="packt_infobox"><span>A centroid is an imaginary or real location that represents the center of the cluster.</span></div>
<p><span>K-means computes the distance (usually the Euclidean distance) between data points and the center of the <em>k</em> clusters by minimizing the cost function, called <strong>within-cluster sum of squares</strong> (<strong>WCSS</strong>).</span> The k-means algorithm proceeds by alternating between two steps:</p>
<ul>
<li><strong>Cluster assignment step</strong>: Each data point is assigned to the cluster whose mean has the least-squared Euclidean distance, yielding the lowest WCSS</li>
<li><strong>Centroid update step</strong>: The new means of the observations in the new clusters are calculated and used as the new centroids</li>
</ul>
<p>The preceding steps can be represented in the following diagram: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-572 image-border" src="assets/df03a615-1cf4-4351-b4b6-7939864b274e.png" style="width:69.67em;height:24.17em;"/></p>
<p>The k-means algorithm is done when the centroids have stabilized or when the predefined number of iterations have been iterated. Although k-means uses Euclidean distance, there are other ways to calculate the distance too, for example:</p>
<ul>
<li>The <strong>Chebyshev distance</strong> can be used to measure the distance by considering only the most notable dimensions</li>
<li>The <strong>Hamming distance</strong> algorithm can identify the difference between two strings</li>
<li>To make the distance metric scale-undeviating, <strong>Mahalanobis distance</strong> can be used to normalize the covariance matrix</li>
<li>The <strong>Manhattan distance</strong> is used to measure the distance by considering only axis-aligned directions</li>
<li>The <strong>Minkowski distance</strong> algorithm is used to make the Euclidean distance, Manhattan distance, and Chebyshev distance</li>
<li>The <strong>haversine distance</strong> is used to measure the great-circle distances between two points on a sphere from the location, that is, longitudes and latitudes</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bisecting k-means</h1>
                </header>
            
            <article>
                
<p><span>Bisecting k-means can be thought of as a combination of k-means and hierarchical clustering, which</span> <span>starts with all the data points in a single cluster. Then, it randomly picks</span> a cluster to split, which returns two sub-clusters using basic k-means. This is called the <strong>bisecting step</strong>.</p>
<div class="packt_infobox">The bisecting k-means algorithm is based on a paper titled <em>A Comparison of Document Clustering Techniques</em> by <em>Michael Steinbach et al.</em>, <em><span>KDD Workshop on Text Mining</span><span>, 2000</span></em>, which has been extended to fit with Spark MLlib.</div>
<p>Then, the bisecting step is iterated for a predefined number of times (usually set by the user/developer), and all the splits are collected that produce the cluster with the highest similarity. These steps are continued until the desired number of clusters is reached. Although bisecting k-means is faster than regular k-means, it produces different clustering because bisecting k-means<span> initializes clusters randomly</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gaussian mixture model</h1>
                </header>
            
            <article>
                
<p><span>GMM is a probabilistic model with a strong assumption that all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. So, it is a distribution-based clustering algorithm too, which is based on an expectation-maximization approach.</span></p>
<p class="mce-root"/>
<p><span>GMM can also be considered as a generalized k-means where the</span> model parameters are optimized iteratively to fit the model better to the training dataset. The overall process can be written in a three-step pseudocode:</p>
<ul>
<li><strong>Objective function</strong>: Compute and maximize the log-likelihood using <strong>expectation-maximization</strong> (<strong>EM</strong>)</li>
<li><strong>EM step</strong>: This EM step consists of two sub-steps called expectation and maximization:<br/>
<ul>
<li><strong>Step E</strong>: Compute the posterior probability of the nearer data points</li>
<li><strong>Step M</strong>: Update and optimize the model parameters <span>for fitting mixture-of-Gaussian models</span></li>
</ul>
</li>
<li><strong>Assignment</strong>: Perform soft assignment during <em>step E</em></li>
</ul>
<p>The preceding steps can be visualized very naively as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-573 image-border" src="assets/6e09fc2f-8686-40aa-acb1-18c837d64212.png" style="width:162.50em;height:56.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other clustering analysis algorithms</h1>
                </header>
            
            <article>
                
<p>The other clustering algorithms includes PIC, which is used to cluster the nodes of a graph based on the given pairwise similarities, such as edge. The LDA is used often in text clustering use cases, such as topic modeling.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>On the other hand, streaming k-means is similar to k-means but applicable for streaming data. For example, when we want to estimate the clusters dynamically so that the clustering assignment will be updated when new data arrives, using streaming k-means is a good option. For a more detailed discussion with examples, interested readers can refer to the following links:</p>
<ul>
<li>Spark ML-based clustering algorithms (<a href="https://spark.apache.org/docs/latest/ml-clustering.html">https://spark.apache.org/docs/latest/ml-clustering.html</a>)<a href="https://spark.apache.org/docs/latest/ml-clustering.html"/></li>
<li>Spark MLlib-based clustering algorithms (<a href="https://spark.apache.org/docs/latest/mllib-clustering.html">https://spark.apache.org/docs/latest/mllib-clustering.html</a>)<a href="https://spark.apache.org/docs/latest/mllib-clustering.html"/></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering analysis through examples</h1>
                </header>
            
            <article>
                
<p>One of the most important tasks in clustering analysis is the analysis of genomic profiles to attribute individuals to specific ethnic populations, or the analysis of nucleotide haplotypes for diseases susceptibility. Human ancestry from Asia, Europe, Africa, and the Americas can be separated based on their genomic data. Research has shown that the Y chromosome lineage can be geographically localized, forming the evidence for clustering the human alleles of the human genotypes. <span>According to National Cancer Institute (<a href="https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant">https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant</a>):<br/></span></p>
<div class="packt_quote"><span><br/>
"Genetic variants are an alteration in the most common DNA nucleotide sequence. The term variant can be used to describe an alteration that may be benign, pathogenic, or of unknown significance. The term variant is increasingly being used in place of the term mutation."</span></div>
<p>A better understanding of genetic variations assists us in finding correlating population groups, identifying patients who are predisposed to common diseases, and solving rare diseases. In short, the idea is to cluster geographic ethnic groups based on their genetic variants. However, before going into this any further, let's get to know the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p>The data from the 1,000 Genomes Project is a large catalog of human genetic variants. This project is meant for determining genetic variants with frequencies above 1% in the populations that were studied. The third phase of the 1,000 Genomes Project finished in September 2014, covering 2,504 individuals from 26 populations and 84,4000,000 million genetic variants. The population samples are grouped into five super-population groups, according to their predominant ancestry:</p>
<ul>
<li>East Asian (CHB, JPT, CHS, CDX, and KHV)</li>
<li>European (CEU, TSI, FIN, GBR, and IBS)</li>
<li>African (YRI, LWK, GWD, MSL, ESN, ASW, and ACB)</li>
<li>American (MXL, PUR, CLM, and PEL)</li>
<li>South Asian (GIH, PJL, BEB, STU, and ITU)</li>
</ul>
<p>Each genotype comprises 23 chromosomes and a separate panel file that contains sample and population information. The data in <strong>Variant Call Format</strong> (<strong>VCF</strong>) as well the panel file can be downloaded from <a href="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/">ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the programming environment</h1>
                </header>
            
            <article>
                
<p>Since the third release of the 1,000 Genome Project contributes about 820 GB of data, using scalable software and hardware is required to process them. To do so, we will use a software stack that consists of the following components:</p>
<ul>
<li><strong>ADAM</strong>: This can be used to achieve the scalable genomics-data-analytics platform with support for the VCF file format so that we can transform genotype-based RDD into Spark DataFrames.</li>
<li><strong>Sparkling Water</strong>:<strong> </strong>H20 is an AI platform for machine learning and a web-based data-processing UI with support for programming languages such as Java, Python, and R. <span>In short, Sparkling Water equals H2O plus Spark.</span></li>
<li>Spark-ML-based k-means is trained for clustering analysis.</li>
</ul>
<p>For this example, we need to use multiple technology and software stacks, such as Spark, H2O, and Adam. Before using H20, make sure that your laptop has at least 16 GB RAM and sufficient storage space. I will develop this solution as a Maven project.</p>
<p>Let's define the properties tag on the <kbd>pom.xml</kbd> file for the Maven-friendly project:</p>
<pre class="mce-root"><strong>&lt;properties&gt;</strong><br/>        &lt;spark.version&gt;2.4.0&lt;/spark.version&gt;<br/>        &lt;scala.version&gt;2.11.7&lt;/scala.version&gt;<br/>        &lt;h2o.version&gt;3.22.1.1&lt;/h2o.version&gt;<br/>        &lt;sparklingwater.version&gt;2.4.1&lt;/sparklingwater.version&gt;<br/>        &lt;adam.version&gt;0.23.0&lt;/adam.version&gt;<br/><strong>&lt;/properties&gt;</strong></pre>
<p class="mce-root">Once you create a Maven project on Eclipse (from an IDE or using the <kbd>mvn install</kbd> command), all the required dependencies will be downloaded!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering geographic ethnicity</h1>
                </header>
            
            <article>
                
<p>24 VCF files contribute around 820 GB of data, which will impose a great computational challenge. To overcome this, use the genetic variants from the smallest chromosome, Y. The size of the VCF file for this is around 160 MB. Let's get started by creating <kbd>SparkSession</kbd>:</p>
<pre class="mce-root"><strong>val</strong> spark:SparkSession = SparkSession<br/>           .builder()<br/>            .appName("PopStrat")<br/>             .master("local[*]")<br/>              .config("spark.sql.warehouse.dir", "temp/") <br/>               .getOrCreate()</pre>
<p>Now, let's show Spark the path of both VCF and the PANEL file:</p>
<pre><strong>val</strong> genotypeFile = "Downloads/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf"<br/><strong>val</strong> panelFile = "Downloads/integrated_call_samples_v3.20130502.ALL.panel"</pre>
<p>We process the PANEL file using Spark to access the target population data and identify the population groups. First, we create a set of <kbd>populations</kbd> that we want to form clusters:</p>
<pre><strong>val</strong> populations = Set("FIN", "GBR", "ASW", "CHB", "CLM")</pre>
<p>Then, we need to create a map between the sample ID and the given population so that we can filter out the samples we are not interested in:</p>
<pre><strong>def</strong> extract(file: String, filter: (String, String) =&gt; Boolean): Map[String, String] = {<br/>      Source<br/>        .fromFile(file)<br/>        .getLines()<br/>        .map(line =&gt; {<br/>          val tokens = line.split(Array('\t', ' ')).toList<br/>          tokens(0) -&gt; tokens(1)<br/>        })<br/>        .toMap<br/>        .filter(tuple =&gt; filter(tuple._1, tuple._2))<br/>    }<br/><br/><strong>val</strong> panel: Map[String, String] = extract(<br/>      panelFile,(sampleID: String, pop: String) =&gt; populations.contains(pop))</pre>
<p>The panel file produces the sample ID of all individuals, population groups, ethnicity, super-population groups, and genders:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-574 image-border" src="assets/5d3126f4-ea27-4581-8734-c56bb5f15441.png" style="width:41.75em;height:6.58em;"/></p>
<div class="packt_infobox">
<p>Check out the details of the panel file at <a href="ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel">ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel</a>.</p>
</div>
<p>Then, load the ADAM genotypes and filter the genotypes so that we are left with only those in the populations we are interested in:</p>
<pre><strong>val</strong> allGenotypes: RDD[Genotype] = sc.loadGenotypes(genotypeFile).rdd<br/><strong>val</strong> genotypes: RDD[Genotype] = allGenotypes.filter(genotype =&gt; {<br/>      panel.contains(genotype.getSampleId)<br/>    })</pre>
<p>Next, convert the <kbd>Genotype</kbd> objects into our own <kbd>SampleVariant</kbd> objects to conserve memory. Then, the <kbd>genotype</kbd> object is converted into a <kbd>SampleVariant</kbd> object that contains the data that needs to be processed further:</p>
<ul>
<li><strong>Sample ID</strong>: To uniquely identify a particular sample</li>
<li><strong>Variant ID</strong>: To uniquely identify a particular genetic variant</li>
<li><strong>Alternate alleles count</strong>: Needed when the sample differs from the reference genome</li>
</ul>
<p class="mce-root"/>
<p>The signature that prepares a <kbd>SampleVariant</kbd> is given as follows, which takes <kbd>sampleID</kbd>, <kbd>variationId</kbd>, and the <kbd>alternateCount</kbd> objects:</p>
<pre>// Convert the Genotype objects to our own SampleVariant objects to try and conserve memory<br/><strong>case class</strong> SampleVariant(sampleId: String, variantId: Int, alternateCount: Int)</pre>
<p>Then, we have to find the <kbd>variantID</kbd> from the genotype file. A <kbd>varitantId</kbd> is a String type that consists of a name, and the start and end positions in the chromosome:</p>
<pre class="mce-root"><strong>def</strong> variantId(genotype: Genotype): String = {<br/>      val name = genotype.getVariant.getContigName<br/>      val start = genotype.getVariant.getStart<br/>      val end = genotype.getVariant.getEnd<br/>      s"$name:$start:$end"<br/>    }</pre>
<p>Once we have the <kbd>variantID</kbd>, we should hunt for the <kbd>alternateCount</kbd>. In the genotype file, objects for which an allele reference is present, would be the genetic alternates:</p>
<pre class="mce-root"><strong>def</strong> alternateCount(genotype: Genotype): Int = {<br/>        genotype.getAlleles.asScala.count(_ != GenotypeAllele.REF)<br/>    }</pre>
<p>Finally, we will construct a <kbd>SampleVariant</kbd> object. For this, we need to intern sample IDs as they will be repeated a lot in a VCF file:</p>
<pre class="mce-root"><strong>def</strong> toVariant(genotype: Genotype): SampleVariant = {<br/>      new SampleVariant(genotype.getSampleId.intern(),<br/>        variantId(genotype).hashCode(),<br/>        alternateCount(genotype))<br/>    }</pre>
<p>Now, we need is to prepare <kbd>variantsRDD</kbd>. First, we have to group the variants by sample ID so that we can process the variants sample by sample. Then, we can get the total number of samples to be used to find the variants that are missing for some samples. Finally, we have to group the variants by variant ID and filter out those variants that are missing from some samples:</p>
<pre><strong>val</strong> variantsRDD: RDD[SampleVariant] = genotypes.map(toVariant)<br/><strong>val</strong> variantsBySampleId: RDD[(String, Iterable[SampleVariant])] = variantsRDD.groupBy(_.sampleId)<br/><strong>val</strong> sampleCount: <strong>Long</strong> = variantsBySampleId.count()<br/><br/>println("Found " + sampleCount + " samples")<br/><strong>val</strong> variantsByVariantId: RDD[(Int, Iterable[SampleVariant])] =<br/>      variantsRDD.groupBy(_.variantId).filter {<br/>        case (_, sampleVariants) =&gt; sampleVariants.size == sampleCount<br/>      }<strong><br/></strong></pre>
<p>Now, let's map <kbd>variantId</kbd> with the count of samples with an alternate count of greater than zero. Then, we filter out the variants that are not in our desired frequency range. The objective here is to reduce the number of dimensions in the dataset to make it easier to train the model:</p>
<pre><strong>val</strong> variantFrequencies: collection.Map[Int, Int] = variantsByVariantId<br/>      .<strong>map</strong> {<br/>        <strong>case</strong> (variantId, sampleVariants) =&gt;<br/>          (variantId, sampleVariants.count(_.alternateCount &gt; 0))<br/>      }<br/>      .collectAsMap()</pre>
<p>The total number of samples (or individuals) has been determined. Now, before grouping them using their variant IDs, we can filter out less significant variants. Since we have more than 84 million genetic variants, filtering would help us deal with the curse of dimensionality.</p>
<p>The specified range is arbitrary and was chosen because it includes a reasonable number of variants, but not too many. To be more specific, for each variant, the frequency for alternate alleles has been calculated, and variants with fewer than 12 alternate alleles have been excluded, leaving about 3,000,000 variants in the analysis (for 23 chromosome files):</p>
<pre class="mce-root"><strong>val</strong> permittedRange = inclusive(11, 11) // variants with less than 12 alternate alleles <br/><strong>val</strong> filteredVariantsBySampleId: RDD[(String, Iterable[SampleVariant])] =<br/>      variantsBySampleId.map {<br/>        <strong>case</strong> (sampleId, sampleVariants) =&gt;<br/>          <strong>val</strong> filteredSampleVariants = sampleVariants.filter(<br/>            variant =&gt;<br/>              permittedRange.contains(<br/>                variantFrequencies.getOrElse(variant.variantId, -1)))<br/>          (sampleId, filteredSampleVariants)<br/>      }</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Once we have <kbd>filteredVariantsBySampleId</kbd>, we need to sort the variants for each sample ID. Each sample should now have the same number of sorted variants:</p>
<pre class="mce-root"><strong>val</strong> sortedVariantsBySampleId: RDD[(String, Array[SampleVariant])] =<br/>      filteredVariantsBySampleId.map {<br/>        <strong>case</strong> (sampleId, variants) =&gt;<br/>          (sampleId, variants.toArray.sortBy(_.variantId))<br/>      }<br/>    println(s"Sorted by Sample ID RDD: " + sortedVariantsBySampleId.first())</pre>
<p>All items in the RDD should now have the same variants in the same order. The final task is to use <kbd>sortedVariantsBySampleId</kbd> to construct an RDD of a row that contains the region and the alternate count:</p>
<pre><strong>val</strong> rowRDD: RDD[Row] = sortedVariantsBySampleId.map {<br/>      <strong>case</strong> (sampleId, sortedVariants) =&gt;<br/>        <strong>val</strong> region: Array[String] = Array(panel.getOrElse(sampleId, "Unknown"))<br/>        <strong>val</strong> alternateCounts: Array[Int] = sortedVariants.map(_.alternateCount)<br/>        Row.fromSeq(region ++ alternateCounts)<br/>    }<strong><br/></strong></pre>
<p>Therefore, we can just use the first one to construct our header for the training DataFrame: </p>
<pre><strong>val</strong> header = StructType(<br/>      Array(StructField("Region", StringType)) ++<br/>        sortedVariantsBySampleId<br/>        .first()<br/>        ._2<br/>        .map(variant =&gt; {<br/>          StructField(variant.variantId.toString, IntegerType)<br/>        }))<strong><br/></strong></pre>
<p>Well done! We have our RDD and the <kbd>StructType</kbd> header. Now, we can play with the Spark machine learning algorithm with minimal adjustment/conversion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the k-means algorithm</h1>
                </header>
            
            <article>
                
<p>Once we have the <kbd>rowRDD</kbd> and the header, we need to construct the rows of our schema DataFrame from the variants using the header and <kbd>rowRDD</kbd>:</p>
<pre class="mce-root">// Create the SchemaRDD from the header and rows and convert the SchemaRDD into a Spark DataFrame<br/><strong>val</strong> sqlContext = sparkSession.sqlContext<br/><strong>var</strong> schemaDF = sqlContext.createDataFrame(rowRDD, header)<br/>schemaDF.show(10)<br/>&gt;&gt;&gt; </pre>
<p>The preceding <kbd>show()</kbd> method should show a snapshot of the training dataset that contains the features and the <kbd>label</kbd> columns (that is, <kbd>Region</kbd>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-575 image-border" src="assets/b82f5318-3021-49a0-b8ed-ba8d1cdc66cc.png" style="width:39.83em;height:16.50em;"/></p>
<p>In the preceding DataFrame, only a few <kbd>feature</kbd> columns and the <kbd>label</kbd> column are shown so that it fits on the page. Since the training would be unsupervised, we need to drop the <kbd>label</kbd> column (that is, <kbd>Region</kbd>): </p>
<pre>schemaDF = sqlContext.createDataFrame(rowRDD, header).drop("Region")<br/>schemaDF.show(10)<br/>&gt;&gt;&gt; </pre>
<p>The preceding <kbd>show()</kbd> method shows the following snapshot of the training dataset for k-means. Note that there is no <kbd>label</kbd> column (that is, <kbd>Region</kbd>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-576 image-border" src="assets/084d8df1-14cb-450a-a607-23c68c8d0adc.png" style="width:47.67em;height:15.58em;"/></p>
<p>In <a href="33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Machine Learning with Scala</em>, and <a href="f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml" target="_blank">Chapter 2</a>, <em>Scala for Regression Analysis</em>, we saw that Spark expects two columns (<kbd>features</kbd> and <kbd>label</kbd>) for supervised training. However, for the unsupervised training, only a single column containing the features is required. Since we dropped the <kbd>label</kbd> column, we now need to amalgamate the entire <kbd>variable</kbd> column into a single <kbd>features</kbd> column. For this, we will use the <kbd>VectorAssembler()</kbd> transformer. Let's select the columns to be embedded into a vector space:</p>
<pre><strong>val</strong> featureCols = schemaDF.columns</pre>
<p>Then, we will instantiate the <kbd>VectorAssembler()</kbd> transformer by specifying the input columns and the output column:</p>
<pre class="mce-root">// Using vector assembler to create feature vector <br/><strong>val</strong> featureCols = schemaDF.columns<br/><strong>val</strong> assembler = new VectorAssembler()<br/>    .setInputCols(featureCols)<br/>    .setOutputCol("features")<br/><strong><br/>val</strong> assembleDF = assembler.transform(schemaDF).select("features")</pre>
<p class="mce-root">Now, let's see what the feature vectors for the k-means look like:</p>
<pre>assembleDF.show()</pre>
<p>The preceding line shows the assembled vectors, which can be used as the feature vectors for the k-means model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-577 image-border" src="assets/7e18458d-a0b4-4da2-8f8e-212bd5351897.png" style="width:9.75em;height:21.75em;"/></p>
<p>Finally, we are ready to train the k-means algorithm and evaluate the clustering by computing <strong>WCSS</strong>:</p>
<pre><strong>val</strong> kmeans = new KMeans().setK(5).setSeed(12345L)<br/><strong>val</strong> model = kmeans.fit(assembleDF)<br/><strong><br/>val</strong> WCSS = model.computeCost(assembleDF)<br/>println("Within Set Sum of Squared Errors for k = 5 is " + WCSS)<br/> }</pre>
<p>The following is the <strong>WCSS</strong> value for <kbd>k = 5</kbd>:</p>
<pre><strong>Within Set Sum of Squared Errors for k = 5 is 59.34564329865</strong></pre>
<p>We managed to apply k-means to cluster genetic variants. However, we saw that the WCSS was high because k-means was unable to separate the non-linearity among different correlated and high-dimensional features. This is because genomic sequencing datasets are very high dimensional due to a huge number of genetic variants.</p>
<p>In the next section, we will see how we can use dimensionality-reduction techniques, such as PCA, to reduce the dimensionality of the input data before feeding it to k-means in order to get better clustering quality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p><span>Since humans are visual creatures, understanding a high dimensional dataset (even with more than three dimensions) is impossible. Even for a machine (or say, our machine learning algorithm), it's difficult to model the non-linearity from correlated and high-dimensional features. Here, the dimensionality reduction technique is a savior.</span></p>
<p>Statistically, dimensionality reduction <span>is the process of </span>reducing<span> the number of random variables to find <span class="markup--quote markup--p-quote is-other">a low-dimensional representation of the data while preserving as much information as possible<strong>.</strong></span></span></p>
<p>The overall step in PCA can be visualized naively in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-578 image-border" src="assets/02ea50cd-5589-46e8-8bf3-388c3ed9c326.png" style="width:52.58em;height:17.17em;"/></p>
<p>PCA and <strong>singular-value decomposition</strong> (<strong>SVD</strong>) are the most popular algorithms for dimensionality reduction. Technically, PCA is a statistical technique that's used to emphasize variation and extract the most significant patterns (that is, features) from a dataset, which is not only useful for clustering but also for classification and visualization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal component analysis with Spark ML</h1>
                </header>
            
            <article>
                
<p>Spark-ML-based PCA can be used to project vectors to a low-dimensional space to reduce the dimensionality of genetic variant features before feeding them into the k-means model. The following example shows how to project 6D feature vectors into 4D principal components from the following feature vector:</p>
<pre class="mce-root"><strong>val</strong> data = Array(<br/>      Vectors.dense(1.2, 3.57, 6.8, 4.5, 2.25, 3.4),<br/>      Vectors.dense(4.60, 4.10, 9.0, 5.0, 1.67, 4.75),<br/>      Vectors.dense(5.60, 6.75, 1.11, 4.5, 2.25, 6.80))<br/><br/><strong>val</strong> df = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")<br/>df.show(false)</pre>
<p>Now that we have a feature DataFrame with a 6D-feature vector, it can be fed into the PCA model: </p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-579 image-border" src="assets/0069126d-2e94-400f-a8e4-9bf63f6f3ab0.png" style="width:16.50em;height:8.08em;"/></p>
<p>First, we have to instantiate the PCA model by setting the necessary parameters, as follows:</p>
<pre class="mce-root"><strong>val</strong> pca = new PCA()<br/>      .setInputCol("features")<br/>      .setOutputCol("pcaFeatures")<br/>      .setK(4)<br/>      .fit(df)</pre>
<p>To distinguish the original features from the principal component-based features, we set the output column name as <kbd>pcaFeatures</kbd> using the <kbd>setOutputCol()</kbd> method. Then, we set the dimension of the PCA (that is, the number of principal components). Finally, we fit the DataFrame to make the transformation. A model can be loaded from older data but will have an empty vector for <kbd>explainedVariance</kbd>. Now, let's show the resulting features:</p>
<pre><strong>val</strong> result = pca.transform(df).select("features", "pcaFeatures")<br/>result.show(<strong>false</strong>)</pre>
<p>The preceding code produces a feature DataFrame with 4D feature vectors as principal components using the PCA:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-580 image-border" src="assets/7b6c797c-957d-49ee-a048-f66fd3b659e2.png" style="width:37.75em;height:7.83em;"/></p>
<p>Similarly, we can transform the assembled DataFrame (that is, <kbd>assembleDF</kbd>) in the previous step and the top five principle components. You can adjust the number of principal components, though.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, to avoid any ambiguity, we renamed the <kbd>pcaFeatures</kbd> column to <kbd>features</kbd>:</p>
<pre class="mce-root"><strong>val</strong> pcaDF = pca.transform(assembleDF)<br/>           .select("pcaFeatures")<br/>           .withColumnRenamed("pcaFeatures", "features")<br/>pcaDF.show()</pre>
<p>The preceding lines of code show the embedded vectors, which can be used as the feature vectors for the k-means model:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-581 image-border" src="assets/79031b6c-40ce-4ec1-b5cc-a8369cfc143c.png" style="width:14.08em;height:31.42em;"/></p>
<p>The preceding screenshot shows the top five principal components as the most important features. Excellent—everything went smoothly. Finally, we are ready to train the k-means algorithm and evaluate clustering by computing WCSS:</p>
<pre><strong>val</strong> kmeans = new KMeans().setK(5).setSeed(12345L)<br/><strong>val</strong> model = kmeans.fit(pcaDF)<br/><strong><br/>val</strong> WCSS = model.computeCost(pcaDF)<br/>println("Within Set Sum of Squared Errors for k = 5 is " + WCSS)<br/>    }</pre>
<p>This time, the WCSS is slightly lower (compared to the previous value, which was <kbd>59.34564329865</kbd>): </p>
<pre><strong>Within Set Sum of Squared Errors for k = 5 is 52.712937492025276</strong></pre>
<p>Normally, we set the number of <kbd>k</kbd> (that is, <kbd>5</kbd>) randomly and computed the WCSS. However, this way, we cannot always set the optimal number of clusters. In order to find an optimal value, researchers have come up with two techniques, called the elbow method and silhouette analysis, which we'll look at in the following subsection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Determining the optimal number of clusters</h1>
                </header>
            
            <article>
                
<p>Sometimes, assuming the number of clu<span>sters naively and before starting the training</span> <span>may not be a good idea. I</span><span>f the assumption is too far from the optimal number of clusters, the model performs poorly because of the overfitting or underfitting issue that's introduced. So, determining the number of optimal clusters is a separate optimization problem. There are two popular techniques to tackle this:</span></p>
<ul>
<li>The heuristic approach, called the <strong>elbow method</strong></li>
<li><strong>Silhouette analysis</strong>, to observe the separation distance between predicted clusters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The elbow method</h1>
                </header>
            
            <article>
                
<p class="mce-root">We start by setting the <kbd>k</kbd> value to <kbd>2</kbd> and running the k-means algorithm on the same dataset by increasing <kbd>k</kbd> and observing the value of WCSS. As expected, a drastic drop should occur in the cost function (that is, WCSS values) at some point. However, after the drastic fall, the value of WCSS becomes marginal with the increasing value of <kbd>k</kbd>. As suggested by the elbow method, we can pick the optimal value of <kbd>k</kbd> after the last big drop of WCSS:</p>
<pre class="mce-root"><strong>val</strong> iterations = 20<br/>    <strong>for</strong> (k &lt;- 2 to iterations) {<br/>      // Trains a k-means model.<br/>      <strong>val</strong> kmeans = new KMeans().setK(k).setSeed(12345L)<br/>      <strong>val</strong> model = kmeans.fit(pcaDF)<br/><br/>// Evaluate clustering by computing Within Set Sum of Squared Errors.<br/><strong>val</strong> WCSS = model.computeCost(pcaDF)<br/>println("Within Set Sum of Squared Errors for k = " + k + " is " + WCSS)<br/>    }</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Now, let's see the WCSS values for a different number of clusters, such as between <kbd>2</kbd> and <kbd>20</kbd>:</p>
<pre><strong>Within Set Sum of Squared Errors for k = 2 is 135.0048361804504</strong><br/><strong>Within Set Sum of Squared Errors for k = 3 is 90.95271589232344</strong><br/><strong>...</strong><br/><strong>Within Set Sum of Squared Errors for k = 19 is 11.505990055606803</strong><br/><strong>Within Set Sum of Squared Errors for k = 20 is 12.26634441065655</strong></pre>
<p>As shown in the preceding code, we calculated the cost function, WCSS, as a function of a number of clusters for the k-means algorithm, and applied them to the Y chromosome genetic variants from the selected population groups. It can be observed that a big drop occurs when <kbd>k = 5</kbd> (which is not a drastic drop, though). Therefore, we chose a number of clusters to be 10.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The silhouette analysis</h1>
                </header>
            
            <article>
                
<p><span>Analyzing the silhouette is carried out by observing the separation distance between predicted clusters. Drawing a silhouette plot will show the distance between a data point from its neighboring clusters, and then we can visually inspect a number of clusters so that similar data points get well-separated.</span></p>
<p><span>The silhouette score, which is used to measure the clustering quality, has a range of [-1, 1]. E</span>valuate the clustering quality by computing the silhouette score:</p>
<pre class="mce-root"><strong>val</strong> evaluator = new ClusteringEvaluator()<br/><strong>for</strong> (k &lt;- 2 to 20 by 1) {<br/>      <strong>val</strong> kmeans = new KMeans().setK(k).setSeed(12345L)<br/>      <strong>val</strong> model = kmeans.fit(pcaDF)<br/>      <strong>val</strong> transformedDF = model.transform(pcaDF)<br/>      <strong>val</strong> score = evaluator.evaluate(transformedDF)<br/>      println("Silhouette with squared Euclidean distance for k = " + k + " is " + score)<br/>    }</pre>
<p>We get the following output:</p>
<pre class="mce-root"><strong>Silhouette with squared Euclidean distance for k = 2 is 0.9175803927739566</strong><br/><strong>Silhouette with squared Euclidean distance for k = 3 is 0.8288633816548874</strong><br/><strong>....</strong><br/><strong>Silhouette with squared Euclidean distance for k = 19 is 0.5327466913746908</strong><br/><strong>Silhouette with squared Euclidean distance for k = 20 is 0.45336547054142284</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">As shown in the preceding code, the height value of the silhouette is generated with <kbd>k = 2</kbd>, which is <kbd>0.9175803927739566</kbd>. However, this suggests that genetic variants should be clustered in two groups. The elbow method suggested <kbd>k = 5</kbd> as the optimal number of clusters.<br/>
Let's find out the<span> silhouette using the squared Euclidean distance, as shown in the following code block:</span></p>
<pre class="mce-root"><strong>val</strong> kmeansOptimal = new KMeans().setK(2).setSeed(12345L)<br/><strong>val</strong> modelOptimal = kmeansOptimal.fit(pcaDF)<br/><br/>// Making predictions<br/><strong>val</strong> predictionsOptimalDF = modelOptimal.transform(pcaDF)<br/>predictionsOptimalDF.show()    <br/><br/>// Evaluate clustering by computing Silhouette score<br/><strong>val</strong> evaluatorOptimal = new ClusteringEvaluator()<br/><strong>val</strong> silhouette = evaluatorOptimal.evaluate(predictionsOptimalDF)<br/>println(s"Silhouette with squared Euclidean distance = $silhouette")</pre>
<p>The silhouette with the squared Euclidean distance for <kbd>k = 2</kbd> is <kbd>0.9175803927739566</kbd>.</p>
<p>It has been found that the bisecting k-means algorithm can result in better cluster assignment for data points, converging to the global minima. On the other hand, k-means often gets stuck in the local minima. Please note that you might observe different values of the preceding parameters depending on your machine's hardware configuration and the random nature of the dataset.</p>
<div class="packt_tip">
<p>Interested readers should also refer to the Spark-MLlib-based clustering techniques at <a href="https://spark.apache.org/docs/latest/mllib-clustering.html">https://spark.apache.org/docs/latest/mllib-clustering.html</a> for more insights.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed some clustering analysis techniques, such as k-means, bisecting k-means, and GMM. We saw a step-by-step example of how to cluster ethnic groups based on their genetic variants. In particular, we used the PCA for dimensionality reduction, k-means for clustering, and H2O and ADAM for handling large-scale genomics datasets. Finally, we learned about the elbow and silhouette methods for finding the optimal number of clusters.</p>
<p class="mce-root"/>
<p>Clustering is the key to most data-driven applications. Readers can try to apply clustering algorithms on higher-dimensional datasets, such as gene expression or miRNA expression, in order to cluster similar and correlated genes. A great resource is the <span class="fontstyle2"><span class="fontstyle0">gene expression cancer RNA-Seq dataset, which is open source. This dataset can be downloaded from the UCI machine learning repository at</span> <a href="https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq" target="_blank">https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq</a><span class="fontstyle0">.</span></span></p>
<p>In the next chapter, we will discuss item-based collaborative filtering approaches for the recommender system. We'll learn how to develop a book recommendation system. Technically, it will be a model-based recommendation engine with Scala and Spark. We will see how we can interoperate between ALS and matrix factorization.</p>


            </article>

            
        </section>
    </body></html>