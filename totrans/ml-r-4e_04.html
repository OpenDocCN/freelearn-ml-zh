<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer164">
    <h1 class="chapterNumber">4</h1>
    <h1 class="chapterTitle" id="_idParaDest-82">Probabilistic Learning – Classification Using Naive Bayes</h1>
    <p class="normal">When a meteorologist provides a weather forecast, precipitation is typically described with phrases like “70 percent chance of rain.” Such forecasts are known as probability of precipitation reports. Have you ever considered how they are calculated? It is a puzzling question because, in reality, it will either rain or not with absolute certainty.</p>
    <p class="normal">Weather estimates are based on probabilistic methods, which are those concerned with describing uncertainty. They use data on past events to extrapolate future events. In the case of the weather, the chance of rain describes the proportion of prior days with similar atmospheric conditions on which precipitation occurred. A 70 percent chance of rain implies that in 7 out of 10 past cases with similar conditions, precipitation occurred somewhere in the area.</p>
    <p class="normal">This chapter covers the Naive Bayes algorithm, which uses probabilities in much the same way as a weather forecast. While studying this method, you will learn about:</p>
    <ul>
      <li class="bulletList">Basic principles of probability</li>
      <li class="bulletList">The specialized methods and data structures needed to analyze text data with R</li>
      <li class="bulletList">How to employ Naive Bayes to build a <strong class="keyWord">Short Message Service</strong> (<strong class="keyWord">SMS</strong>) junk message filter</li>
    </ul>
    <p class="normal">If you’ve taken a statistics class before, some of the material in this chapter may be a review. Even so, it may be helpful to refresh your knowledge of probability. You will find out that these principles are the basis of how Naive Bayes got such a strange name.</p>
    <h1 class="heading-1" id="_idParaDest-83">Understanding Naive Bayes</h1>
    <p class="normal">The basic statistical ideas<a id="_idIndexMarker369"/> necessary to understand the Naive Bayes algorithm have existed for centuries. The technique descended from the work of the 18th-century mathematician Thomas Bayes, who developed foundational principles for describing the probability of events and how these probabilities should be revised in light of additional information. These principles formed the foundation for what are now known<a id="_idIndexMarker370"/> as <strong class="keyWord">Bayesian methods</strong>.</p>
    <p class="normal">We will cover these methods in greater detail later. For now, it suffices to say that a probability is a number between zero and one (or from 0 to 100 percent) that captures the chance that an event will occur in light of the available evidence. The lower the probability, the less likely the event is to occur. A probability of zero indicates that the event will definitely not occur, while a probability of one indicates that the event will occur with absolute certainty. Life’s most interesting events tend to be those with uncertain probability; estimating the chance that they will occur helps us make better decisions by revealing the most likely outcomes.</p>
    <p class="normal">Classifiers based on Bayesian methods utilize training data to calculate the probability of each outcome based on the evidence provided by feature values. When the classifier is later applied to unlabeled data, it uses these calculated probabilities to predict the most likely class for the new example. It’s a simple idea, but it results in a method that can have results on par with more sophisticated algorithms. In fact, Bayesian classifiers have been used for:</p>
    <ul>
      <li class="bulletList">Text classification, such as junk email (spam) filtering</li>
      <li class="bulletList">Intrusion or anomaly detection in computer networks</li>
      <li class="bulletList">Diagnosing medical conditions given a set of observed symptoms</li>
    </ul>
    <p class="normal">Typically, Bayesian classifiers are best applied to problems for which the information from numerous attributes should be considered simultaneously to estimate the overall probability of an outcome. While many machine learning algorithms ignore features that have weak effects, Bayesian methods utilize all available evidence to subtly change the predictions. This implies that even if a large portion of features have relatively minor effects, their combined impact in a Bayesian model could be quite large.</p>
    <h2 class="heading-2" id="_idParaDest-84">Basic concepts of Bayesian methods</h2>
    <p class="normal">Before jumping into the Naive Bayes algorithm, it’s worth spending some time defining the concepts <a id="_idIndexMarker371"/>that are used across Bayesian methods. Summarized in a single sentence, Bayesian probability theory is rooted in the idea that the estimated likelihood of an <strong class="keyWord">event</strong>, or potential outcome, should be based on the evidence at hand across multiple <strong class="keyWord">trials</strong>, or opportunities for the event to occur.</p>
    <p class="normal">The following table illustrates <a id="_idIndexMarker372"/>events and<a id="_idIndexMarker373"/> trials for several real-world outcomes:</p>
    <table class="table-container" id="table001-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Event</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Trial</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Heads result</p>
          </td>
          <td class="table-cell">
            <p class="normal">A coin flip</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Rainy weather</p>
          </td>
          <td class="table-cell">
            <p class="normal">A single day (or another time period)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Message is spam</p>
          </td>
          <td class="table-cell">
            <p class="normal">An incoming email message</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Candidate becomes president</p>
          </td>
          <td class="table-cell">
            <p class="normal">A presidential election</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Mortality</p>
          </td>
          <td class="table-cell">
            <p class="normal">A hospital patient</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Winning the lottery</p>
          </td>
          <td class="table-cell">
            <p class="normal">A lottery ticket</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Bayesian methods provide insights into how the probability of these events can be estimated from observed data. To see how, we’ll need to formalize our understanding of probability.</p>
    <h3 class="heading-3" id="_idParaDest-85">Understanding probability</h3>
    <p class="normal">The<a id="_idIndexMarker374"/> probability <a id="_idIndexMarker375"/>of an event is estimated from observed data by dividing the number of trials in which the event occurred by the total number of trials. For instance, if it rained 3 out of 10 days with similar conditions as today, the probability of rain today can be estimated as <em class="italic">3 / 10 = 0.30</em> or 30 percent. Similarly, if 10 out of 50 prior email messages were spam, then the probability of any incoming message being spam can be estimated as <em class="italic">10 / 50 = 0.20</em> or 20 percent.</p>
    <p class="normal">To denote these probabilities, we use notation in the form <em class="italic">P(A)</em>, which signifies the probability of event <em class="italic">A</em>. For example, <em class="italic">P(rain) = 0.30</em> to indicate a 30 percent chance of rain or <em class="italic">P(spam) = 0.20</em> to describe a 20 percent probability of an incoming message being spam.</p>
    <p class="normal">Because a trial always results in some outcome happening, the probability of all possible outcomes of a trial must always sum to one. Thus, if the trial has exactly two outcomes and the outcomes cannot occur simultaneously, then knowing the probability of either outcome reveals the probability of the other. This is the case for many outcomes, such as heads or tails coin flips, or spam versus legitimate email messages, also known as “ham.” Using this principle, knowing that <em class="italic">P(spam) = 0.20</em> allows us to calculate <em class="italic">P(ham) = 1 – 0.20 = 0.80</em>. This only <a id="_idIndexMarker376"/>works because spam and ham are <strong class="keyWord">mutually exclusive and exhaustive events</strong>, which implies that they cannot occur at the same time and are the only possible outcomes.</p>
    <p class="normal">A single event <a id="_idIndexMarker377"/>cannot happen and not happen simultaneously. This means an event is always mutually exclusive and exhaustive with its <strong class="keyWord">complement</strong>, or the event comprising all other outcomes in which the event of interest does not happen. The complement of event <em class="italic">A</em> is typically denoted <em class="italic">A</em><sup class="superscript-italic" style="font-style: italic;">c</sup> or <em class="italic">A’</em>. </p>
    <p class="normal">Additionally, the shorthand notation <em class="italic">P(A</em><sup class="superscript-italic" style="font-style: italic;">c</sup><em class="italic">)</em> or <em class="italic">P(¬A)</em> can be used to denote the probability of event <em class="italic">A</em> not occurring. For example, the notation <em class="italic">P(¬spam) = 0.80</em> suggests that the probability of a message not being spam is 80%.</p>
    <p class="normal">To illustrate events and their complements, it is often helpful to imagine a two-dimensional space that is partitioned into probabilities for each event. In the following diagram, the rectangle represents the possible outcomes for an email message. The circle represents the 20 percent probability that the message is spam. The remaining 80 percent represents the complement <em class="italic">P(¬spam)</em>, or the messages that are not spam:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_04_01.png"/></figure>
    <p class="packt_figref">Figure 4.1: The probability space for all emails can be visualized as partitions of spam and ham</p>
    <h3 class="heading-3" id="_idParaDest-86">Understanding joint probability</h3>
    <p class="normal">Often, we <a id="_idIndexMarker378"/>are interested in monitoring several non-mutually exclusive events in the same trial. If certain events occur concurrently <a id="_idIndexMarker379"/>with the event of interest, we may be able to use them to make predictions. Consider, for instance, a second event based on the outcome that an email message contains the word <em class="italic">Viagra</em>. The preceding diagram, updated for this second event, might appear as shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_04_02.png"/></figure>
    <p class="packt_figref">Figure 4.2: Non-mutually exclusive events are depicted as overlapping partitions</p>
    <p class="normal">Notice in the diagram that the Viagra circle overlaps with both the spam and ham areas of the diagram and the spam circle includes an area not covered by the Viagra circle. This implies that not all spam messages contain the term Viagra and some messages with the term Viagra are ham. However, because this word appears very rarely outside spam, its presence in a new incoming message would be strong evidence that the message is spam. </p>
    <p class="normal">To zoom in for a closer look at the overlap between these circles, we’ll employ a visualization <a id="_idIndexMarker380"/>known as a <strong class="keyWord">Venn diagram</strong>. First used in the late 19th century by mathematician John Venn, the diagram uses circles to illustrate the overlap between sets of items. As in most Venn diagrams, the size and degree of overlap of the circles in the depiction is not meaningful. Instead, it is used as a reminder to allocate probability to all combinations of events. A Venn diagram for spam and Viagra might be depicted as follows:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_04_03.png"/></figure>
    <p class="packt_figref">Figure 4.3: A Venn diagram illustrates the overlap of the spam and Viagra events</p>
    <p class="normal">We <a id="_idIndexMarker381"/>know <a id="_idIndexMarker382"/>that 20 percent of all messages were spam (the left circle), and 5 percent of all messages contained the word <em class="italic">Viagra</em> (the right circle). We would like to quantify the degree of overlap between these two proportions. In other words, we hope to estimate the probability that both <em class="italic">P(spam)</em> and <em class="italic">P(Viagra)</em> occur, which can be written as <em class="italic">P(spam <img alt="" src="../Images/B17290_04_001.png"/> Viagra)</em>. The<em class="italic"> <img alt="" src="../Images/B17290_04_001.png"/> </em>symbol signifies the intersection of the two events; the notation <em class="italic">A <img alt="" src="../Images/B17290_04_001.png"/> B</em> refers to the event in which both <em class="italic">A</em> and <em class="italic">B</em> occur.</p>
    <p class="normal">Calculating <em class="italic">P(spam <img alt="" src="../Images/B17290_04_001.png"/> Viagra)</em> depends on the <strong class="keyWord">joint probability</strong> of the two events, or how the probability of one event is related to the probability of the other. If the two events are totally unrelated, they are<a id="_idIndexMarker383"/> called <strong class="keyWord">independent events</strong>. This is not to say that independent events cannot occur at the same time; event independence simply implies that knowing the outcome of one event does not provide any information about the outcome of the other. For instance, the outcome of a heads result on a coin flip is independent of whether the weather is rainy or sunny on any given day.</p>
    <p class="normal">If all events were independent, it would be impossible to predict one event by observing another. In other words, <strong class="keyWord">dependent events</strong> are <a id="_idIndexMarker384"/>the basis of predictive modeling. Just as the presence of clouds is predictive of a rainy day, the appearance of the word <em class="italic">Viagra</em> is predictive of a spam email.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_04_04.png"/></figure>
    <p class="packt_figref">Figure 4.4: Dependent events are required for machines to learn how to identify useful patterns</p>
    <p class="normal">Calculating <a id="_idIndexMarker385"/>the probability of dependent events is a bit more complex than for independent events. If <em class="italic">P(spam)</em> and <em class="italic">P(Viagra)</em> were independent, we <a id="_idIndexMarker386"/>could easily calculate <em class="italic">P(spam <img alt="" src="../Images/B17290_04_001.png"/> Viagra)</em>, the probability of both events happening at the same time. Because 20 percent of all messages are spam, and 5 percent of all emails contain the word <em class="italic">Viagra</em>, we could assume that 1 percent of all messages with the term <em class="italic">Viagra</em> are spam. This is because <em class="italic">0.05 * 0.20 = 0.01</em>. More generally, for independent events <em class="italic">A</em> and <em class="italic">B</em>, the probability of both happening can be computed as <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(A) * P(B)</em>.</p>
    <p class="normal">That said, we know that <em class="italic">P(spam)</em> and <em class="italic">P(Viagra)</em> are likely to be highly dependent, which means that this calculation is incorrect. To obtain a more reasonable estimate, we need to use a more careful formulation of the relationship between these two events, which is based on more advanced Bayesian methods.</p>
    <h3 class="heading-3" id="_idParaDest-87">Computing conditional probability with Bayes’ theorem</h3>
    <p class="normal">The relationships<a id="_idIndexMarker387"/> between dependent events can be described using <strong class="keyWord">Bayes’ theorem</strong>, which provides a way of thinking<a id="_idIndexMarker388"/> about how to revise an <a id="_idIndexMarker389"/>estimate of the probability of one event in light of the evidence provided by another. One formulation is as follows:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_008.png"/></p>
    <p class="normal">The <a id="_idIndexMarker390"/>notation <em class="italic">P(A|B)</em> is read as the probability<a id="_idIndexMarker391"/> of event <em class="italic">A</em> given that event <em class="italic">B</em> occurred. This is known as<strong class="keyWord"> conditional probability</strong> since the probability of <em class="italic">A</em> is dependent (that is, conditional) on what happened with event <em class="italic">B</em>. </p>
    <p class="normal">Bayes’ theorem <a id="_idIndexMarker392"/>states that the best <a id="_idIndexMarker393"/>estimate of <em class="italic">P(A|B)</em> is the proportion of trials in which <em class="italic">A</em> occurred with <em class="italic">B</em>, out of all the trials in which <em class="italic">B</em> occurred. This implies that the probability of event <em class="italic">A</em> is higher if <em class="italic">A</em> and <em class="italic">B</em> often occur together each time <em class="italic">B</em> is observed. Note that this formula adjusts <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B)</em> for the probability of <em class="italic">B</em> occurring. If <em class="italic">B</em> is extremely rare, <em class="italic">P(B)</em> and <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B)</em> will always be small; however, if <em class="italic">A</em> almost always happens together with <em class="italic">B</em>, <em class="italic">P(A|B)</em> will still be high in spite of <em class="italic">B</em>’s rarity.</p>
    <p class="normal">By definition, <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(A|B) * P(B)</em>, a fact that can be easily derived by applying a bit of algebra to the previous formula. Rearranging this formula once more with the knowledge that <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(B <img alt="" src="../Images/B17290_04_001.png"/> A)</em> results in the conclusion that <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(B|A) * P(A)</em>, which we can then use in the following formulation of Bayes’ theorem:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_016.png"/></p>
    <p class="normal">In fact, this is the traditional formulation of Bayes’ theorem for reasons that will become clear as we apply it to machine learning. First, to better understand how Bayes’ theorem works in practice, let’s revisit our hypothetical spam filter.</p>
    <p class="normal">Without knowledge of an incoming message’s content, the best estimate of its spam status would be <em class="italic">P(spam)</em>, the probability that any prior message was spam. This estimate is known as <a id="_idIndexMarker394"/>the <strong class="keyWord">prior probability</strong>. We found this previously to be 20 percent.</p>
    <p class="normal">Suppose that you obtained additional evidence by looking more carefully at the set of previously received messages and examining the frequency with which the term <em class="italic">Viagra</em> appeared. The probability that the word <em class="italic">Viagra</em> was used in previous spam messages, or <em class="italic">P(Viagra|spam)</em>, is called<a id="_idIndexMarker395"/> the <strong class="keyWord">likelihood</strong>. The probability that <em class="italic">Viagra</em> appeared in any message at all, or <em class="italic">P(Viagra)</em>, is <a id="_idIndexMarker396"/>known as the <strong class="keyWord">marginal likelihood</strong>.</p>
    <p class="normal">By applying Bayes’ theorem<a id="_idIndexMarker397"/> to this evidence, we can compute a <strong class="keyWord">posterior probability</strong> that measures how likely a message is to be spam. If the posterior probability is greater than 50 percent, the message is more likely to be spam than ham, and it should perhaps be filtered. The following formula shows how Bayes’ theorem is applied to the evidence provided by previous email messages:</p>
    <figure class="mediaobject"><img alt="Shape  Description automatically generated with medium confidence" src="../Images/B17290_04_05.png"/></figure>
    <p class="packt_figref">Figure 4.5: Bayes’ theorem acting on previously received emails</p>
    <p class="normal">To <a id="_idIndexMarker398"/>calculate <a id="_idIndexMarker399"/>the components of Bayes’ theorem, it helps to<a id="_idIndexMarker400"/> construct a <strong class="keyWord">frequency table</strong> (shown on the left in the tables that follow) recording the number of times <em class="italic">Viagra</em> appeared in spam and ham <a id="_idIndexMarker401"/>messages. Just like a two-way cross-tabulation, one dimension of the table indicates levels of the class variable (spam or ham), while the other dimension indicates levels for features (Viagra: yes or no). The cells then indicate the number of instances that have the specified combination of the class value and feature value.</p>
    <p class="normal">The frequency table can then be used to <a id="_idIndexMarker402"/>construct a <strong class="keyWord">likelihood table</strong>, as shown on the right in the following tables. The rows of the likelihood table indicate the conditional probabilities for <em class="italic">Viagra</em> (yes/no), given that an email was either spam or ham.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_04_06.png"/></figure>
    <p class="packt_figref">Figure 4.6: Frequency and likelihood tables are the basis for computing the posterior probability of spam</p>
    <p class="normal">The likelihood table reveals that <em class="italic">P(Viagra=Yes|spam) = 4 / 20 = 0.20</em>, indicating that there is a 20 percent probability that a message contains the term <em class="italic">Viagra</em> given that the message is spam. Additionally, since <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(B|A) * P(A)</em>, we can calculate <em class="italic">P(spam <img alt="" src="../Images/B17290_04_001.png"/> Viagra)</em> as <em class="italic">P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04</em>. The same result can be found in the frequency table, which notes that 4 out of 100 messages were spam and contained the term <em class="italic">Viagra</em>. Either way, this is four times greater than the previous estimate of 0.01 we calculated as <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(A) * P(B)</em> under the false assumption of independence. This, of course, illustrates the importance of Bayes’ theorem for estimating joint probability.</p>
    <p class="normal">To compute <a id="_idIndexMarker403"/>the posterior probability, <em class="italic">P(spam|Viagra)</em>, we simply take <em class="italic">P(Viagra|spam) * P(spam) / P(Viagra)</em>, or <em class="italic">(4 / 20) * (20 / 100) / (5 / 100) = 0.80</em>. Therefore, the probability that a message is spam is 80 percent <a id="_idIndexMarker404"/>given that it contains the word <em class="italic">Viagra</em>. In light of this finding, any message containing this term should probably be filtered.</p>
    <p class="normal">This <a id="_idIndexMarker405"/>is very much how commercial spam filters work, although they consider a much larger number of words simultaneously when computing the frequency and likelihood tables. In the next section, we’ll see how this method can be adapted to accommodate cases when additional features are involved.</p>
    <h2 class="heading-2" id="_idParaDest-88">The Naive Bayes algorithm</h2>
    <p class="normal">The <strong class="keyWord">Naive Bayes</strong> algorithm <a id="_idIndexMarker406"/>defines a simple method for applying Bayes’ theorem to classification problems. Although it is not the only machine learning method that utilizes Bayesian methods, it is the most common. It grew in popularity due to its successes in text classification, where it was once the de facto standard. The <a id="_idIndexMarker407"/>strengths <a id="_idIndexMarker408"/>and weaknesses of this algorithm are as follows:</p>
    <table class="table-container" id="table002-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Simple, fast, and very effective</li>
              <li class="bulletList">Does well with noisy and missing data and large numbers of features</li>
              <li class="bulletList">Requires relatively few examples for training</li>
              <li class="bulletList">Easy to obtain the estimated probability for a prediction</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Relies on an often-faulty assumption of equally important and independent features</li>
              <li class="bulletList">Not ideal for datasets with many numeric features</li>
              <li class="bulletList">Estimated probabilities are less reliable than the predicted classes</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The <a id="_idIndexMarker409"/>Naive Bayes algorithm is named as such because it makes some so-called “naive” assumptions about the data. In particular, Naive Bayes assumes that all of the features in the dataset are <strong class="keyWord">equally important and independent</strong>. These assumptions are rarely true in most real-world applications.</p>
    <p class="normal">For example, when attempting to identify spam by monitoring email messages, it is almost certainly true that some features will be more important than others. For example, the email sender may be a more important indicator of spam than the message text. Additionally, the words in the message body are not independent of one another, since the appearance of some words is a very good indication that other words are also likely to appear. A message with the word <em class="italic">Viagra</em> will probably also contain the word <em class="italic">prescription</em> or <em class="italic">drugs</em>.</p>
    <p class="normal">However, in most cases, even when these assumptions are violated, Naive Bayes still performs surprisingly well. This is true even in circumstances where strong dependencies are found among the features. Due to the algorithm’s versatility and accuracy across many types of conditions, particularly with smaller training datasets, Naive Bayes is often a reasonable baseline candidate for classification learning tasks.</p>
    <div class="note">
      <p class="normal">The exact reason why Naive Bayes works well in spite of its faulty assumptions has been the subject of much speculation. One explanation is that it is not important to obtain a precise estimate of probability so long as the predictions are accurate. For instance, if a spam filter correctly identifies spam, does it matter whether the predicted probability of spam was 51 percent or 99 percent? For one discussion of this topic, refer to <em class="italic">On the Optimality of the Simple Bayesian Classifier under Zero-One Loss, Domingos, P. and Pazzani, M., Machine Learning, 1997, Vol. 29, pp. 103-130</em>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-89">Classification with Naive Bayes</h3>
    <p class="normal">Let’s extend <a id="_idIndexMarker410"/>our spam filter by adding a <a id="_idIndexMarker411"/>few additional terms to be monitored in addition to the term <em class="italic">Viagra</em>: <em class="italic">money</em>, <em class="italic">groceries</em>, and <em class="italic">unsubscribe</em>. The Naive Bayes learner is trained by constructing a likelihood table for the appearance of these four words (labeled <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">1</sup>, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">2</sup>, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">3</sup>, and <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">4</sup>), as shown in the following diagram for 100 emails:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_04_07.png"/></figure>
    <p class="packt_figref">Figure 4.7: An expanded table adds likelihoods for additional terms in spam and ham messages</p>
    <p class="normal">As <a id="_idIndexMarker412"/>new messages are received, we need to calculate the posterior probability to determine whether they are more likely<a id="_idIndexMarker413"/> spam or ham, given the likelihood of the words being found in the message text. For example, suppose that a message contains the terms <em class="italic">Viagra</em> and <em class="italic">unsubscribe</em> but does not contain either <em class="italic">money</em> or <em class="italic">groceries</em>.</p>
    <p class="normal">Using Bayes’ theorem, we can define the problem as shown in the following formula. This computes the probability that a message is spam given that <em class="italic">Viagra = Yes</em>, <em class="italic">Money = No</em>, <em class="italic">Groceries = No</em>, and <em class="italic">Unsubscribe = Yes</em>:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_020.png"/></p>
    <p class="normal">For two reasons, this formula is computationally difficult to solve. First, as additional features are added, tremendous amounts of memory are needed to store the probabilities for all possible intersecting events. Imagine the complexity of a Venn diagram for the events for four words, let alone for hundreds or more. Second, many of these potential intersections will never have been observed in past data, which would lead to a joint probability of zero and problems that will become clear later.</p>
    <p class="normal">The computation becomes more reasonable if we exploit the fact that Naive Bayes makes the naive assumption of independence among events. Specifically, it assumes <strong class="keyWord">class-conditional independence</strong>, which <a id="_idIndexMarker414"/>means that events are independent so long as they are conditioned on the same class value. The conditional independence assumption allows us to use the probability rule for independent events, which states that <em class="italic">P(A <img alt="" src="../Images/B17290_04_001.png"/> B) = P(A) * P(B)</em>. This simplifies the numerator by allowing us to multiply the individual conditional probabilities rather than computing a complex conditional joint probability.</p>
    <p class="normal">Lastly, because the denominator does not depend on the target class (spam or ham), it is treated as a constant value and can be ignored for the time being. This means that the conditional probability of spam can be expressed as:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_022.png"/></p>
    <p class="normal">And the probability that the message is ham can be expressed as:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_023.png"/></p>
    <p class="normal">Note<a id="_idIndexMarker415"/> that the equals symbol has <a id="_idIndexMarker416"/>been replaced by the proportional-to symbol (similar to a sideways, open-ended “8”) to indicate the fact that the denominator has been omitted.</p>
    <p class="normal">Using the values in the likelihood table, we can start filling in numbers in these equations. The overall likelihood of spam is then:</p>
    <p class="center"><em class="italic">(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012</em></p>
    <p class="normal">While the likelihood of ham is:</p>
    <p class="center"><em class="italic">(1 / 80) * (66 / 80) * (71 / 80) * (23 / 80) * (80 / 100) = 0.002</em></p>
    <p class="normal">Because <em class="italic">0.012 / 0.002 = 6</em>, we can say that this message is 6 times more likely to be spam than ham. However, to convert these numbers into probabilities, we need one last step to reintroduce the denominator that has been excluded. Essentially, we must re-scale the likelihood of each outcome by dividing it by the total likelihood across all possible outcomes.</p>
    <p class="normal">In this way, the probability of spam is equal to the likelihood that the message is spam divided by the likelihood that the message is either spam or ham:</p>
    <p class="center"><em class="italic">0.012 / (0.012 + 0.002) = 0.857</em></p>
    <p class="normal">Similarly, the probability of ham is equal to the likelihood that the message is ham divided by the likelihood that the message is either spam or ham:</p>
    <p class="center"><em class="italic">0.002 / (0.012 + 0.002) = 0.143</em></p>
    <p class="normal">Given the pattern of words found in this message, we expect that the message is spam with an 85.7 percent probability, and ham with a 14.3 percent probability. Because these are mutually exclusive and exhaustive events, the probabilities sum up to 1.</p>
    <p class="normal">The Naive Bayes classification <a id="_idIndexMarker417"/>algorithm used in the preceding example can be summarized by the following formula. The probability of level <em class="italic">L</em> for class <em class="italic">C</em>, given the evidence provided by features <em class="italic">F</em><sub class="subscript-italic" style="font-style: italic;">1</sub> through <em class="italic">F</em><sub class="subscript-italic" style="font-style: italic;">n</sub>, is equal to the product of the probabilities of each piece of evidence conditioned on the class level, the prior probability of the class level, and a scaling factor <em class="italic">1 / Z</em>, which converts the likelihood values into probabilities. This is formulated as:</p>
    <p class="center"><img alt="" src="../Images/B17290_04_024.png"/></p>
    <p class="normal">Although this equation<a id="_idIndexMarker418"/> seems intimidating, as the spam filtering example illustrated, the series of steps is fairly straightforward. Begin by building a frequency table, use this to build a likelihood table, and multiply out the conditional probabilities with the naive assumption of independence. Finally, divide by the total likelihood to transform each class likelihood into a probability. After attempting this calculation a few times by hand, it will become second nature.</p>
    <h3 class="heading-3" id="_idParaDest-90">The Laplace estimator</h3>
    <p class="normal">Before we <a id="_idIndexMarker419"/>employ Naive Bayes on more complex problems, there<a id="_idIndexMarker420"/> are some nuances to consider. Suppose we received another message, this time containing all four terms: <em class="italic">Viagra</em>, <em class="italic">groceries</em>, <em class="italic">money</em>, and <em class="italic">unsubscribe</em>. Using the Naive Bayes algorithm as before, we can compute the likelihood of spam as:</p>
    <p class="center"><em class="italic">(4 / 20) * (10 / 20) * (0 / 20) * (12 / 20) * (20 / 100) = 0</em></p>
    <p class="normal">And the likelihood of ham is:</p>
    <p class="center"><em class="italic">(1 / 80) * (14 / 80) * (8 / 80) * (23 / 80) * (80 / 100) = 0.00005</em></p>
    <p class="normal">Therefore, the probability of spam is:</p>
    <p class="center"><em class="italic">0 / (0 + 0.00005) = 0</em></p>
    <p class="normal">And the probability of ham is:</p>
    <p class="center"><em class="italic">0.00005 / (0 + 0. 0.00005) = 1</em></p>
    <p class="normal">These results suggest that the message is spam with 0 percent probability and ham with 100 percent probability. Does this prediction make sense? Probably not. The message contains several words usually associated with spam, including <em class="italic">Viagra</em>, which is rarely used in legitimate messages. It is therefore very likely that the message has been incorrectly classified.</p>
    <p class="normal">This <a id="_idIndexMarker421"/>problem arises if an event never occurs for one or more levels of the class and therefore the resulting likelihoods are zero. For instance, the term <em class="italic">groceries</em> had never previously appeared in a spam message. Consequently, <em class="italic">P(groceries|spam) = 0%</em>.</p>
    <p class="normal">Now, because<a id="_idIndexMarker422"/> probabilities in the Naive Bayes formula are multiplied in a chain, this zero-percent value causes the posterior probability of spam to be zero, giving the word <em class="italic">groceries</em> the ability to effectively nullify and overrule all of the other evidence. Even if the email was otherwise overwhelmingly expected to be spam, the absence of the word <em class="italic">groceries</em> in spam will always veto the other evidence and result in the probability of spam being zero.</p>
    <p class="normal">A solution to this problem involves using something called the <strong class="keyWord">Laplace estimator</strong>, which is named after the French mathematician Pierre-Simon Laplace. The Laplace estimator adds a small number to each of the counts in the frequency table, which ensures that each feature has a non-zero probability of occurring with each class. Typically, the Laplace estimator is set to one, which ensures that each class-feature combination is found in the data at least once.</p>
    <div class="packt_tip">
      <p class="normal">The Laplace estimator can be set to any value and does not necessarily even have to be the same for each of the features. If you were a devoted Bayesian, you could use a Laplace estimator to reflect a presumed prior probability of how a feature relates to a class. In practice, given a large enough training dataset, this is excessive. Consequently, the value of one is almost always used.</p>
    </div>
    <p class="normal">Let’s see how this affects our prediction for this message. Using a Laplace value of 1, we add 1 to each numerator in the likelihood function. Then, we need to add 4 to each conditional probability denominator to compensate for the 4 additional values added to the numerator. The likelihood of spam is therefore:</p>
    <p class="center"><em class="italic">(5 / 24) * (11 / 24) * (1 / 24) * (13 / 24) * (20 / 100) = 0.0004</em></p>
    <p class="normal">And the likelihood of ham is:</p>
    <p class="center"><em class="italic">(2 / 84) * (15 / 84) * (9 / 84) * (24 / 84) * (80 / 100) = 0.0001</em></p>
    <p class="normal">By computing <em class="italic">0.0004 / (0.0004 + 0.0001)</em>, we find that the probability of spam is 80 percent <a id="_idIndexMarker423"/>and therefore the probability of ham is about 20 percent. This is a more plausible <a id="_idIndexMarker424"/>result than the <em class="italic">P(spam) = 0</em> computed when the term <em class="italic">groceries</em> alone determined the result.</p>
    <div class="packt_tip">
      <p class="normal">Although the Laplace estimator was added to the numerator and denominator of the likelihoods, it was not added to the prior probabilities—the values of 20/100 and 80/100. This is because our best estimate of the overall probability of spam and ham remains at 20% and 80% respectively given what was observed in the data.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-91">Using numeric features with Naive Bayes</h3>
    <p class="normal">Naive Bayes uses<a id="_idIndexMarker425"/> frequency tables for learning the data, which means that each feature must be categorical in order to create the <a id="_idIndexMarker426"/>combinations of class and feature values comprising the matrix. Since numeric features do not have categories of values, the preceding algorithm does not work directly with numeric data. There are, however, ways that this can be addressed.</p>
    <p class="normal">One easy and effective solution is<a id="_idIndexMarker427"/> to <strong class="keyWord">discretize</strong> numeric features, which<a id="_idIndexMarker428"/> simply means that the numbers are put into categories known as <strong class="keyWord">bins</strong>. For this reason, discretization is also <a id="_idIndexMarker429"/>sometimes called <strong class="keyWord">binning</strong>. This method works best when there are large amounts of training data.</p>
    <p class="normal">There are several different ways to discretize a numeric feature. Perhaps the most common is to explore the data for natural categories<a id="_idIndexMarker430"/> or <strong class="keyWord">cut points</strong> in the distribution. For example, suppose that you added a feature to the spam dataset that recorded the time of day or night the email was sent, from 0 to 24 hours past midnight. Depicted using a histogram, the time data might look something like the following diagram:</p>
    <figure class="mediaobject"><img alt="Chart, bar chart, histogram  Description automatically generated" src="../Images/B17290_04_08.png"/></figure>
    <p class="packt_figref">Figure 4.8: A histogram visualizing the distribution of the time emails were received</p>
    <p class="normal">In the<a id="_idIndexMarker431"/> early hours of the morning, message frequency is low. Activity picks up during business hours and <a id="_idIndexMarker432"/>tapers off in the evening. This creates four natural bins of activity, as partitioned by the dashed lines. These indicate places where the numeric data could be divided into levels to create a new categorical feature, which could then be used with Naive Bayes.</p>
    <p class="normal">The choice of four bins was based on the natural distribution of data and a hunch about how the proportion of spam might change throughout the day. We might expect that spammers operate in the late hours of the night, or they may operate during the day when people are likely to check their email. That said, to capture these trends, we could have just as easily used three bins or twelve.</p>
    <div class="packt_tip">
      <p class="normal">If there are no obvious cut points, one option is to discretize the feature using quantiles, which were introduced in <em class="chapterRef">Chapter 2</em>, <em class="italic">Managing and Understanding Data</em>. You could divide the data into three bins with tertiles, four bins with quartiles, or five bins with quintiles.</p>
    </div>
    <p class="normal">One thing to keep in mind is that discretizing a numeric feature always results in a reduction of information, as the feature’s original granularity is reduced to a smaller number of categories. It is important to strike a balance. Too few bins can result in important trends being obscured. Too many bins can result in small counts in the Naive Bayes frequency table, which can increase the algorithm’s sensitivity to noisy data.</p>
    <h1 class="heading-1" id="_idParaDest-92">Example – filtering mobile phone spam with the Naive Bayes algorithm</h1>
    <p class="normal">As the<a id="_idIndexMarker433"/> worldwide use of mobile phones has grown, a new avenue for electronic junk mail has opened for disreputable marketers. These advertisers utilize SMS text messages to target potential consumers with<a id="_idIndexMarker434"/> unwanted advertising known as SMS spam. This type of spam is troublesome because, unlike email spam, an SMS message is particularly disruptive due to the omnipresence of one’s mobile phone. Developing a classification algorithm that could filter SMS spam would provide a useful tool for cellular phone providers.</p>
    <p class="normal">Since Naive Bayes has been used successfully for email spam filtering, it seems likely that it could also be applied to SMS spam. However, relative to email spam, SMS spam poses additional challenges for automated filters. SMS messages are often limited to 160 characters, reducing the amount of text that can be used to identify whether a message is junk. The limit, combined with small mobile phone keyboards, has led many to adopt a form of SMS shorthand lingo, which further blurs the line between legitimate messages and spam. Let’s see how a simple Naive Bayes classifier handles these challenges.</p>
    <h2 class="heading-2" id="_idParaDest-93">Step 1 – collecting data</h2>
    <p class="normal">To develop<a id="_idIndexMarker435"/> the Naive Bayes classifier, we will use data adapted from the SMS Spam Collection at <a href="https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/"><span class="url">https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/</span></a>.</p>
    <div class="note">
      <p class="normal">To read more about how the SMS Spam Collection was developed, refer to <em class="italic">On the Validity of a New SMS Spam Collection, Gómez, J. M., Almeida, T. A., and Yamakami, A., Proceedings of the 11th IEEE International Conference on Machine Learning and Applications, 2012</em>.</p>
    </div>
    <p class="normal">This dataset includes the text of SMS messages, along with a label indicating whether the message is unwanted. Junk messages are labeled spam, while legitimate messages are labeled ham. Some <a id="_idIndexMarker436"/>examples of spam <a id="_idIndexMarker437"/>and ham are shown in the following table:</p>
    <table class="table-container" id="table003-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Sample SMS ham</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Sample SMS spam</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel bleh. But at least its not writhing pain kind of bleh.</li>
              <li class="bulletList">If he started searching he will get job in few days. he have great potential and talent.</li>
              <li class="bulletList">I got another job! The one at the hospital doing data analysis or something, starts on monday! Not sure when my thesis will got finished</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed &amp; Free entry 2 100 wkly draw txt MUSIC to 87066</li>
              <li class="bulletList">December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906</li>
              <li class="bulletList">Valentines Day Special! Win over £1000 in our quiz and take your partner on the trip of a lifetime! Send GO to 83600 now. 150p/msg rcvd.</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Looking at the <a id="_idIndexMarker438"/>preceding messages, do you notice any distinguishing characteristics of spam? One notable characteristic is that two of the three spam messages use the word <em class="italic">free</em>, yet this word does not appear in any of the ham messages. On the other hand, two of the ham messages cite specific days of the week, as compared to zero in spam messages.</p>
    <p class="normal">Our Naive Bayes classifier will take advantage of such patterns in the word frequency to determine whether the SMS messages seem to better fit the profile of spam or ham. While it’s not inconceivable that the word <em class="italic">free</em> would appear outside of a spam SMS, a legitimate message is likely to provide additional words giving context. </p>
    <p class="normal">For instance, a ham message might ask, “Are you free on Sunday?” whereas a spam message might use the phrase “free ringtones.” The classifier will compute the probability of spam and ham given the evidence provided by all the words in the message.</p>
    <h2 class="heading-2" id="_idParaDest-94">Step 2 – exploring and preparing the data</h2>
    <p class="normal">The first step <a id="_idIndexMarker439"/>toward <a id="_idIndexMarker440"/>constructing our classifier involves processing the raw data for analysis. Text data is challenging to prepare because it is necessary to transform the words and sentences into a form that a computer can understand. We will transform our SMS data into a representation known<a id="_idIndexMarker441"/> as <strong class="keyWord">bag-of-words</strong>, which provides a binary feature indicating whether each word appears in the given example while ignoring word order or the context in <a id="_idIndexMarker442"/>which the word appears. Although this is a relatively simple representation, as we<a id="_idIndexMarker443"/> will soon demonstrate, it performs well enough for many classification tasks.</p>
    <div class="packt_tip">
      <p class="normal">The dataset used here has been modified slightly from the original to make it easier to work with in R. If you plan on following along with the example, download the <code class="inlineCode">sms_spam.csv</code> file from the Packt website and save it to your R working directory.</p>
    </div>
    <p class="normal">We’ll begin by importing the CSV data and saving it to a data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_raw &lt;- read.csv(<span class="hljs-string">"sms_spam.csv"</span>)
</code></pre>
    <p class="normal">Using the <code class="inlineCode">str()</code> function, we see that the <code class="inlineCode">sms_raw</code> data frame includes 5,559 total SMS messages with two features: <code class="inlineCode">type</code> and <code class="inlineCode">text</code>. The SMS type has been coded as either ham or spam. The <code class="inlineCode">text</code> element stores the full raw SMS message text:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; str(sms_raw)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':    5559 obs. of  2 variables:
 $ type: chr  "ham" "ham" "ham" "spam" ...
 $ text: chr  "Hope you are having a good week. Just checking in"
"K..give back my thanks." "Am also doing in cbe only. But have to
pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your
URGENT collection. 09066364349 NOW from Landline not to lose out"|
__truncated__ ...
</code></pre>
    <p class="normal">The <code class="inlineCode">type</code> element is currently a character vector. Since this is a categorical variable, it would be better to convert it into a factor, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_raw$type &lt;- factor(sms_raw$type)
</code></pre>
    <p class="normal">Examining this with the <code class="inlineCode">str()</code> and <code class="inlineCode">table()</code> functions, we see that <code class="inlineCode">type</code> has now been appropriately recoded as a factor. Additionally, we see that 747 (about 13 percent) of SMS messages in our data were labeled as <code class="inlineCode">spam</code>, while the others were labeled as <code class="inlineCode">ham</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; str(sms_raw$type)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> Factor w/ 2 levels "ham","spam": 1 1 1 2 2 1 1 1 2 1 ...
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; table(sms_raw$type)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> ham spam
4812 747
</code></pre>
    <p class="normal">For now, we<a id="_idIndexMarker444"/> will <a id="_idIndexMarker445"/>leave the message text alone. As you will learn in the next section, processing raw SMS messages will require the use of a new set of powerful tools designed specifically for processing text data.</p>
    <h3 class="heading-3" id="_idParaDest-95">Data preparation – cleaning and standardizing text data</h3>
    <p class="normal">SMS messages<a id="_idIndexMarker446"/> are strings of text composed of words, spaces, numbers, and punctuation. Handling this type of complex data takes a large amount of thought and effort. One<a id="_idIndexMarker447"/> needs to consider how to remove numbers and punctuation; handle uninteresting words, such as <em class="italic">and</em>, <em class="italic">but</em>, and <em class="italic">or</em>; and break apart sentences into individual words. Thankfully, this functionality has been provided by members of the R community in a text-mining package titled <code class="inlineCode">tm</code>.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">tm</code> package was originally created by Ingo Feinerer as a dissertation project at the Vienna University of Economics and Business. To learn more, see <em class="italic">Text Mining Infrastructure in R, Feinerer, I., Hornik, K., and Meyer, D., Journal of Statistical Software, 2008, Vol. 25, pp. 1-54</em>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">tm</code> package can be installed via the <code class="inlineCode">install.packages("tm")</code> command and loaded with the <code class="inlineCode">library(tm)</code> command. Even if you already have it installed, it may be worth redoing the installation to ensure that your version is up to date, as the <code class="inlineCode">tm</code> package is still under active development. This occasionally results in changes to its functionality.</p>
    <div class="packt_tip">
      <p class="normal">This chapter was tested using <code class="inlineCode">tm</code> version 0.7-11, which was current as of May 2023. If you see differences in the output or if the code does not work, you may be using a different version. The Packt support page for this book, as well as its GitHub repository, will post solutions for future <code class="inlineCode">tm</code> package versions if significant changes are noted.</p>
    </div>
    <p class="normal">The first step in processing text data involves <a id="_idIndexMarker448"/>creating a <strong class="keyWord">corpus</strong>, which is a collection of text documents. The documents can be short or long, from individual news articles, pages in a book, pages from the web, or even entire books. In our case, the corpus will be a collection of SMS messages.</p>
    <p class="normal">To<a id="_idIndexMarker449"/> create<a id="_idIndexMarker450"/> a corpus, we’ll use the <code class="inlineCode">VCorpus()</code> function in the <code class="inlineCode">tm</code> package, which refers to a volatile corpus—the term “volatile” meaning that it is stored in memory as opposed to being stored on disk (the <code class="inlineCode">PCorpus()</code> function is used to access a permanent corpus stored in a database). This function requires us to specify the source of documents for the corpus, which could be a computer’s filesystem, a database, the web, or elsewhere.</p>
    <p class="normal">Since we already loaded the SMS message text into R, we’ll use the <code class="inlineCode">VectorSource()</code> reader function to create a source object from the existing <code class="inlineCode">sms_raw$text</code> vector, which can then be supplied to <code class="inlineCode">VCorpus()</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus &lt;- VCorpus(VectorSource(sms_raw$text))
</code></pre>
    <p class="normal">The resulting corpus object is saved with the name <code class="inlineCode">sms_corpus</code>.</p>
    <div class="packt_tip">
      <p class="normal">By specifying an optional <code class="inlineCode">readerControl</code> parameter, the <code class="inlineCode">VCorpus()</code> function can be used to import text from sources such as PDFs and Microsoft Word files. To learn more, examine the <em class="italic">Data Import</em> section in the <code class="inlineCode">tm</code> package vignette using the <code class="inlineCode">vignette("tm")</code> command.</p>
    </div>
    <p class="normal">By printing the corpus, we see that it contains documents for each of the 5,559 SMS messages in the training data:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; print(sms_corpus)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 5559
</code></pre>
    <p class="normal">Now, because the <code class="inlineCode">tm</code> corpus is essentially a complex list, we can use list operations to select documents in the corpus. The <code class="inlineCode">inspect()</code> function shows a summary of the result. For example, the following command will view a summary of the first and second SMS messages in the corpus:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; inspect(sms_corpus[1:2])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 2
[[1]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 49
[[2]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 23
</code></pre>
    <p class="normal">To view<a id="_idIndexMarker451"/> the actual message text, the <code class="inlineCode">as.character()</code> function must be applied to the desired <a id="_idIndexMarker452"/>messages. To view one message, use the <code class="inlineCode">as.character()</code> function on a single list element, noting that the double-bracket notation is required:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.character(sms_corpus[[<span class="hljs-number">1</span>]])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "Hope you are having a good week. Just checking in"
</code></pre>
    <p class="normal">To view multiple documents, we’ll need to apply <code class="inlineCode">as.character()</code> to several items in the <code class="inlineCode">sms_corpus</code> object. For this, we’ll use the <code class="inlineCode">lapply()</code> function, which is part of a family of R functions that applies a procedure to each element of an R data structure. These functions, which include <code class="inlineCode">apply()</code> and <code class="inlineCode">sapply()</code> among others, are one of the key idioms of the R language. Experienced R coders use these much like the way <code class="inlineCode">for</code> or <code class="inlineCode">while</code> loops are used in other programming languages, as they result in more readable (and sometimes more efficient) code. The <code class="inlineCode">lapply()</code> function for applying <code class="inlineCode">as.character()</code> to a subset of corpus elements is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; lapply(sms_corpus[<span class="hljs-number">1:2</span>], as.character)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">$'1'
[1] "Hope you are having a good week. Just checking in"
$'2'
[1] "K..give back my thanks."
</code></pre>
    <p class="normal">As noted earlier, the corpus contains the raw text of 5,559 text messages. To perform our analysis, we need to divide these messages into individual words. First, we need to clean the text to standardize the words and remove punctuation characters that clutter the result. For example, we would like the strings <em class="italic">Hello!</em>, <em class="italic">HELLO</em>, and <em class="italic">hello</em> to be counted as instances of the same word.</p>
    <p class="normal">The <code class="inlineCode">tm_map()</code> function <a id="_idIndexMarker453"/>provides a method to apply a transformation (also known as a mapping) to a <code class="inlineCode">tm</code> corpus. We will use this function to clean up our corpus using a series of <a id="_idIndexMarker454"/>transformations and save the result in a new object called <code class="inlineCode">corpus_clean</code>.</p>
    <p class="normal">Our first transformation will standardize the messages to use only lowercase characters. To this end, R provides a <code class="inlineCode">tolower()</code> function that returns a lowercase version of text strings. In order to apply this function to the corpus, we need to use the <code class="inlineCode">tm</code> wrapper function <code class="inlineCode">content_transformer()</code> to treat <code class="inlineCode">tolower()</code> as a transformation function that can be used to access the corpus. The full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus,
    content_transformer(tolower))
</code></pre>
    <p class="normal">To check whether the command worked as expected, let’s inspect the first message in the original corpus and compare it to the same in the transformed corpus:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.character(sms_corpus[[<span class="hljs-number">1</span>]])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "Hope you are having a good week. Just checking in"
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; as.character(sms_corpus_clean[[<span class="hljs-number">1</span>]])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "hope you are having a good week. just checking in"
</code></pre>
    <p class="normal">As expected, uppercase letters in the clean corpus have been replaced with lowercase versions.</p>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">content_transformer()</code> function can be used to apply more sophisticated text processing and cleanup processes like <code class="inlineCode">grep</code> pattern matching and replacement. Simply write a custom function and wrap it before applying the <code class="inlineCode">tm_map()</code> function.</p>
    </div>
    <p class="normal">Let’s continue our cleanup by removing numbers from the SMS messages. Although some numbers may provide useful information, the majority are likely to be unique to individual senders and thus will not provide useful patterns across all messages. With this in mind, we’ll strip all numbers from the corpus as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeNumbers)
</code></pre>
    <p class="normal">Note that the preceding code did not use the <code class="inlineCode">content_transformer()</code> function. This is because <code class="inlineCode">removeNumbers()</code> is included with <code class="inlineCode">tm</code> along with several other mapping functions that do not need to be wrapped. To see the other built-in transformations, simply type <code class="inlineCode">getTransformations()</code>.</p>
    <p class="normal">Our next <a id="_idIndexMarker455"/>task is <a id="_idIndexMarker456"/>to remove filler words such as <em class="italic">to</em>, <em class="italic">and</em>, <em class="italic">but</em>, and <em class="italic">or</em> from the SMS messages. These terms are known <a id="_idIndexMarker457"/>as <strong class="keyWord">stop words</strong> and are typically removed prior to text mining. This is due to the fact that although they appear very frequently, they do not provide much useful information for our model as they are unlikely to distinguish between spam and ham.</p>
    <p class="normal">Rather than define a list of stop words ourselves, we’ll use the <code class="inlineCode">stopwords()</code> function provided by the <code class="inlineCode">tm</code> package. This function allows us to access sets of stop words from various languages. By default, common English language stop words are used. To see the default list, type <code class="inlineCode">stopwords()</code> at the R command prompt. To see the other languages and options available, type <code class="inlineCode">?stopwords</code> for the documentation page.</p>
    <div class="packt_tip">
      <p class="normal">Even within a single language, there is no single definitive list of stop words. For example, the default English list in <code class="inlineCode">tm</code> includes about 174 words, while another option includes 571 words. You can even specify your own list of stop words. Regardless of the list you choose, keep in mind the goal of this transformation, which is to eliminate useless data while keeping as much useful information as possible.</p>
    </div>
    <p class="normal">Defining the stop words alone is not a transformation. What we need is a way to remove any words that appear in the stop words list. The solution lies in the <code class="inlineCode">removeWords()</code> function, which is a transformation included with the <code class="inlineCode">tm</code> package. As we have done before, we’ll use the <code class="inlineCode">tm_map()</code> function to apply this mapping to the data, providing the <code class="inlineCode">stopwords()</code> function as a parameter to indicate the words we would like to remove. The full command is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean,
    removeWords, stopwords())
</code></pre>
    <p class="normal">Since <code class="inlineCode">stopwords()</code> simply returns a vector of stop words, if we had so chosen, we could have replaced this function call with our own vector of words to remove. In this way, we could expand or reduce the list of stop words to our liking or remove a different set of words entirely.</p>
    <p class="normal">Continuing our<a id="_idIndexMarker458"/> cleanup process, we can also eliminate any punctuation from the text messages using the built-in <code class="inlineCode">removePunctuation()</code> transformation:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removePunctuation)
</code></pre>
    <p class="normal">The <code class="inlineCode">removePunctuation()</code> transformation completely strips punctuation characters from the text, which<a id="_idIndexMarker459"/> can lead to unintended consequences. For example, consider what happens when it is applied as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; removePunctuation(<span class="hljs-string">"hello...world"</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "helloworld"
</code></pre>
    <p class="normal">As shown, the lack of a blank space after the ellipses caused the words <em class="italic">hello</em> and <em class="italic">world</em> to be joined as a single word. While this is not a substantial problem right now, it is worth noting for the future.</p>
    <div class="packt_tip">
      <p class="normal">To work around the default behavior of <code class="inlineCode">removePunctuation()</code>, it is possible to create a custom function that replaces rather than removes punctuation characters:</p>
      <pre class="programlisting code"><code class="hljs-code">&gt; replacePunctuation &lt;- function(x) {
    gsub(<span class="hljs-string">"[[:punct:]]+"</span>, <span class="hljs-string">" "</span>, x)
  }
</code></pre>
      <p class="normal">This uses R’s <code class="inlineCode">gsub()</code> function to substitute any punctuation characters in <code class="inlineCode">x</code> with a blank space. This <code class="inlineCode">replacePunctuation()</code> function can then be used with <code class="inlineCode">tm_map()</code> as with other transformations. The odd syntax of the <code class="inlineCode">gsub()</code> command here is due to the use <a id="_idIndexMarker460"/>of a <strong class="keyWord">regular expression</strong>, which specifies a pattern that matches text characters. Regular expressions are covered in more detail in <em class="chapterRef">Chapter 12</em>, <em class="italic">Advanced Data Preparation</em>.</p>
    </div>
    <p class="normal">Another common standardization for text data involves reducing words to their root form in a process called <strong class="keyWord">stemming</strong>. The<a id="_idIndexMarker461"/> stemming process takes words like <em class="italic">learned</em>, <em class="italic">learning</em>, and <em class="italic">learns</em> and strips the suffix in order to transform them into the base form, <em class="italic">learn</em>. This allows machine <a id="_idIndexMarker462"/>learning algorithms to treat the related terms as a single concept rather than attempting to learn a pattern for each variant.</p>
    <p class="normal">The <code class="inlineCode">tm</code> package <a id="_idIndexMarker463"/>provides stemming functionality via integration with the <code class="inlineCode">SnowballC</code> package. At the time of writing, <code class="inlineCode">SnowballC</code> is not installed by default with <code class="inlineCode">tm</code>, so do so with <code class="inlineCode">install.packages("SnowballC")</code> if you have not done so already.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">SnowballC</code> package is<a id="_idIndexMarker464"/> maintained by Milan Bouchet-Valat and provides an R interface for the C-based <code class="inlineCode">libstemmer</code> library, itself based on M. F. Porter’s “Snowball” word-stemming algorithm, a widely used open-source stemming method. For more details, see <a href="http://snowballstem.org"><span class="url">http://snowballstem.org</span></a>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">SnowballC</code> package<a id="_idIndexMarker465"/> provides a <code class="inlineCode">wordStem()</code> function, which for a character vector returns the same vector of terms in its root form. For example, the function correctly stems the variants of the word <em class="italic">learn</em> as described previously:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(SnowballC)
&gt; wordStem(c(<span class="hljs-string">"learn"</span>, <span class="hljs-string">"learned"</span>, <span class="hljs-string">"learning"</span>, <span class="hljs-string">"learns"</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] "learn"   "learn"   "learn"   "learn"
</code></pre>
    <p class="normal">To apply the <code class="inlineCode">wordStem()</code> function to an entire corpus of text documents, the <code class="inlineCode">tm</code> package includes a <code class="inlineCode">stemDocument()</code> transformation. We apply this to our corpus with the <code class="inlineCode">tm_map()</code> function exactly as before:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stemDocument)
</code></pre>
    <div class="packt_tip">
      <p class="normal">If you receive an error message when applying the <code class="inlineCode">stemDocument()</code> transformation, please confirm that you have the <code class="inlineCode">SnowballC</code> package installed.</p>
    </div>
    <p class="normal">After removing numbers, stop words, and punctuation, then also performing stemming, the text messages are left with the blank spaces that once separated the now-missing pieces. Therefore, the final step in our text cleanup process is to remove additional whitespace using the built-in <code class="inlineCode">stripWhitespace()</code> transformation:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stripWhitespace)
</code></pre>
    <p class="normal">The following table shows the first three messages in the SMS corpus before and after the cleaning process. The messages have been limited to the most interesting words, and punctuation<a id="_idIndexMarker466"/> and <a id="_idIndexMarker467"/>capitalization have been removed:</p>
    <table class="table-container" id="table004-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">SMS messages before cleaning</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">SMS messages after cleaning</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">&gt; as.character(sms_corpus[1:3])</code></p>
            <p class="normal"><code class="inlineCode">[[1]] Hope you are having a good</code></p>
            <p class="normal"><code class="inlineCode">week. Just checking in</code></p>
            <p class="normal"><code class="inlineCode">[[2]] K..give back my thanks.</code></p>
            <p class="normal"><code class="inlineCode">[[3]] Am also doing in cbe only.</code></p>
            <p class="normal"><code class="inlineCode">But have to pay.</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">&gt; as.character(sms_corpus_clean[1:3])</code></p>
            <p class="normal"><code class="inlineCode">[[1]] hope good week just check</code></p>
            <p class="normal"><code class="inlineCode">[[2]] kgive back thank</code></p>
            <p class="normal"><code class="inlineCode">[[3]] also cbe pay</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <h3 class="heading-3" id="_idParaDest-96">Data preparation – splitting text documents into words</h3>
    <p class="normal">Now that the <a id="_idIndexMarker468"/>data is processed to our liking, the final step is to split the messages into individual terms through a process called <strong class="keyWord">tokenization</strong>. A<a id="_idIndexMarker469"/> token is a single element of a text string; in this case, the tokens are words.</p>
    <p class="normal">As you might assume, the <code class="inlineCode">tm</code> package provides functionality to tokenize the SMS message corpus. The <code class="inlineCode">DocumentTermMatrix()</code> function takes a corpus and creates a data structure called<a id="_idIndexMarker470"/> a <strong class="keyWord">document-term matrix</strong> (<strong class="keyWord">DTM</strong>) in which rows indicate documents (SMS messages) and columns indicate terms (words).</p>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">tm</code> package also provides a data structure for a <strong class="keyWord">term-document matrix</strong> (<strong class="keyWord">TDM</strong>), which is <a id="_idIndexMarker471"/>simply a transposed DTM in which the rows are terms and the columns are documents. Why the need for both? Sometimes, it is more convenient to work with one or the other. For example, if the number of documents is small, while the word list is large, it may make sense to use a TDM because it is usually easier to display many rows than to display many columns. That said, machine learning algorithms will generally require a DTM, as the columns are the features and the rows are the examples.</p>
    </div>
    <p class="normal">Each cell in <a id="_idIndexMarker472"/>the matrix stores a number indicating a count of the times the word represented by the column appears in the document represented by the row. The following figure depicts only a small portion of the DTM for the SMS corpus, as the complete matrix has 5,559 rows and over 7,000 columns:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_04_09.png"/></figure>
    <p class="packt_figref">Figure 4.9: The DTM for the SMS messages is filled with mostly zeros</p>
    <p class="normal">The fact that each cell in the table is zero implies that none of the words listed at the top of the columns appear in any of the first five messages in the corpus. This highlights the reason why this data structure is <a id="_idIndexMarker473"/>called a <strong class="keyWord">sparse matrix</strong>; the vast majority of cells in the matrix are filled with zeros. Stated in real-world terms, although each message must contain at least one word, the probability of any one word appearing in a given message is small.</p>
    <p class="normal">Creating a DTM sparse matrix from a <em class="italic">tm</em> corpus involves a single command:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm &lt;- DocumentTermMatrix(sms_corpus_clean)
</code></pre>
    <p class="normal">This will create an <code class="inlineCode">sms_dtm</code> object that contains the tokenized corpus using the default settings, which apply minimal additional processing. The default settings are appropriate because we have already prepared the corpus manually.</p>
    <p class="normal">On the other hand, if we hadn’t already performed the preprocessing, we could do so here by providing a list of <code class="inlineCode">control</code> parameter options to override the defaults. For example, to create a DTM directly from the raw, unprocessed SMS corpus, we can use the following command:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm2 &lt;- DocumentTermMatrix(sms_corpus, control = <span class="hljs-built_in">list</span>(
    tolower = <span class="hljs-literal">TRUE</span>,
    removeNumbers = <span class="hljs-literal">TRUE</span>,
    stopwords = <span class="hljs-literal">TRUE</span>,
    removePunctuation = <span class="hljs-literal">TRUE</span>,
    stemming = <span class="hljs-literal">TRUE</span>
))
</code></pre>
    <p class="normal">This <a id="_idIndexMarker474"/>applies the same preprocessing steps to the SMS corpus in the same order as done earlier. However, comparing <code class="inlineCode">sms_dtm</code> to <code class="inlineCode">sms_dtm2</code>, we see a slight difference in the number of terms in the matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6559)&gt;&gt;
Non-/sparse entries: 42147/36419334
Sparsity           : 100%
Maximal term length: 40
Weighting          : term frequency (tf)
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm2
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6961)&gt;&gt;
Non-/sparse entries: 43221/38652978
Sparsity           : 100%
Maximal term length: 40
Weighting          : term frequency (tf)
</code></pre>
    <p class="normal">The reason for this discrepancy has to do with a minor difference in the ordering of the preprocessing steps. The <code class="inlineCode">DocumentTermMatrix()</code> function applies its cleanup functions to the text strings only after they have been split apart into words. Thus, it uses a slightly different stop word removal function. Consequently, some words are split differently than when they are cleaned before tokenization.</p>
    <div class="packt_tip">
      <p class="normal">To force the two prior DTMs to be identical, we can override the default stop words function with our own that uses the original replacement function. Simply replace <code class="inlineCode">stopwords = TRUE</code> with the following:</p>
      <pre class="programlisting code"><code class="hljs-code">stopwords = <span class="hljs-keyword">function</span>(x) { removeWords(x, stopwords()) }
</code></pre>
      <p class="normal">The code file for this chapter includes the full set of steps to create an identical DTM using a single function call. </p>
    </div>
    <p class="normal">The differences between these bring up an important principle of cleaning text data: the order of operations matters. With this in mind, it is very important to think through how early steps in the process are going to affect later ones. The order presented here will work in many<a id="_idIndexMarker475"/> cases, but when the process is tailored more carefully to specific datasets and use cases, it may require rethinking. For example, if there are certain terms you hope to exclude from the matrix, consider whether to search for them before or after stemming. Also, consider how the removal of punctuation—and whether the punctuation is eliminated or replaced by blank space—affects these steps.</p>
    <h3 class="heading-3" id="_idParaDest-97">Data preparation – creating training and test datasets</h3>
    <p class="normal">With our<a id="_idIndexMarker476"/> data prepared for analysis, we now need to split the data into training and test datasets so that after our spam classifier is built, it can be evaluated on data it has not previously seen. However, even though we need to keep the classifier blinded as to <a id="_idIndexMarker477"/>the contents of the test dataset, it is important that the split occurs after the data has been cleaned and processed. We need exactly the same preparation steps to have occurred on both the training and test datasets.</p>
    <p class="normal">We’ll divide the data into two portions: 75 percent for training and 25 percent for testing. Since the SMS messages are sorted in a random order, we can simply take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully, the DTM object acts very much like a data frame and can be split using the standard <code class="inlineCode">[row, col]</code> operations. As our DTM stores SMS messages as rows and words as columns, we must request a specific range of rows and all columns for each:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm_train &lt;- sms_dtm[<span class="hljs-number">1:4169</span>, ]
&gt; sms_dtm_test  &lt;- sms_dtm[<span class="hljs-number">4170:5559</span>, ]
</code></pre>
    <p class="normal">For convenience later, it is also helpful to save a pair of vectors with the labels for each of the rows in the training and testing matrices. These labels are not stored in the DTM, so we need to pull them from the original <code class="inlineCode">sms_raw</code> data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_train_labels &lt;- sms_raw[<span class="hljs-number">1:4169</span>, ]$type
&gt; sms_test_labels  &lt;- sms_raw[<span class="hljs-number">4170:5559</span>, ]$type
</code></pre>
    <p class="normal">To confirm that the subsets are representative of the complete set of SMS data, let’s compare the proportion of spam in the training and test data frames:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; prop.table(table(sms_train_labels))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">      ham      spam
0.8647158 0.1352842
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">&gt; prop.table(table(sms_test_labels))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">      ham      spam
0.8683453 0.1316547
</code></pre>
    <p class="normal">Both <a id="_idIndexMarker478"/>the <a id="_idIndexMarker479"/>training data and test data contain about 13 percent spam. This suggests that the spam messages were divided evenly between the two datasets.</p>
    <h3 class="heading-3" id="_idParaDest-98">Visualizing text data – word clouds</h3>
    <p class="normal">A <strong class="keyWord">word cloud</strong> is a<a id="_idIndexMarker480"/> way to<a id="_idIndexMarker481"/> visually depict the frequency at which words appear in text data. The cloud is composed of words scattered somewhat randomly around the figure. Words appearing more often in the text are shown in a larger font, while less common terms are shown in smaller fonts. This type of figure grew in popularity as a way to observe trending topics on social media websites.</p>
    <p class="normal">The <code class="inlineCode">wordcloud</code> package provides a simple R function to create this type of diagram. We’ll use it to visualize the words in SMS messages. Comparing the clouds for spam and ham messages will help us gauge whether our Naive Bayes spam filter is likely to be successful. If you haven’t already done so, install and load the package by typing <code class="inlineCode">install.packages("wordcloud")</code> and <code class="inlineCode">library(wordcloud)</code> at the R command line.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">wordcloud</code> package was written by Ian Fellows. For more information about this package, visit his blog at <a href="http://blog.fellstat.com/?cat=11"><span class="url">http://blog.fellstat.com/?cat=11</span></a>.</p>
    </div>
    <p class="normal">A word cloud can be created directly from a <em class="italic">tm</em> corpus object using the syntax:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wordcloud(sms_corpus_clean, min.freq = <span class="hljs-number">50</span>, random.order = <span class="hljs-literal">FALSE</span>)
</code></pre>
    <p class="normal">This will create a word cloud from our prepared SMS corpus. Since we specified <code class="inlineCode">random.order = FALSE</code>, the cloud will be arranged in a non-random order, with higher-frequency words placed closer to the center. If we do not specify <code class="inlineCode">random.order</code>, the cloud will be arranged randomly by default.</p>
    <p class="normal">The <code class="inlineCode">min.freq</code> parameter specifies the number of times a word must appear in the corpus before it will <a id="_idIndexMarker482"/>be displayed in the cloud. Since a frequency of 50 is about 1 percent of the corpus, this means that a word must be found in at least 1 percent of the SMS messages to be included in the cloud.</p>
    <div class="packt_tip">
      <p class="normal">You might get a warning message noting that R was unable to fit all the words in the figure. If so, try increasing the <code class="inlineCode">min.freq</code> to reduce the number of words in the cloud. It might also help to use the <code class="inlineCode">scale</code> parameter to reduce the font size.</p>
    </div>
    <p class="normal">The resulting word cloud should appear similar to the following:</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_04_10.png"/></figure>
    <p class="packt_figref">Figure 4.10: A word cloud depicting words appearing in all SMS messages</p>
    <p class="normal">A perhaps more interesting visualization involves comparing the clouds for SMS spam and ham. Since we did not construct separate corpora for spam and ham, this is an appropriate time to note a very helpful feature of the <code class="inlineCode">wordcloud()</code> function. Given a vector of raw text strings, it will automatically apply common text preparation processes before displaying the cloud.</p>
    <p class="normal">Let’s use R’s <code class="inlineCode">subset()</code> function to take a subset of the <code class="inlineCode">sms_raw</code> data by the SMS type. First, we’ll <a id="_idIndexMarker483"/>create a subset where the <code class="inlineCode">type</code> is <code class="inlineCode">spam</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; spam &lt;- subset(sms_raw, type == <span class="hljs-string">"spam"</span>)
</code></pre>
    <p class="normal">Next, we’ll do the same thing for the <code class="inlineCode">ham</code> subset:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; ham &lt;- subset(sms_raw, type == <span class="hljs-string">"ham"</span>)
</code></pre>
    <div class="packt_tip">
      <p class="normal">Be careful to note the double equals sign. Like many programming languages, R uses <code class="inlineCode">==</code> to test equality. If you accidentally use a single equals sign, you’ll end up with a subset much larger than you expected!</p>
    </div>
    <p class="normal">We now have two data frames, <code class="inlineCode">spam</code> and <code class="inlineCode">ham</code>, each with a <code class="inlineCode">text</code> feature containing the raw text strings for SMS messages. Creating word clouds is as simple as before. This time, we’ll use the <code class="inlineCode">max.words</code> parameter to look at the 40 most common words in each of the 2 sets. The <code class="inlineCode">scale</code> parameter adjusts the maximum and minimum font sizes for words in the cloud. Feel free to change these parameters as you see fit. This is illustrated in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; wordcloud(spam$text, max.words = <span class="hljs-number">40</span>, scale = <span class="hljs-built_in">c</span>(<span class="hljs-number">3</span>, <span class="hljs-number">0.5</span>))
&gt; wordcloud(ham$text, max.words = <span class="hljs-number">40</span>, scale = <span class="hljs-built_in">c</span>(<span class="hljs-number">3</span>, <span class="hljs-number">0.5</span>))
</code></pre>
    <div class="packt_tip">
      <p class="normal">Note that R provides warning messages when running this code that the “transformation drops documents.” The warnings are related to the <code class="inlineCode">removePunctuation()</code> and <code class="inlineCode">removeWords()</code> procedures that <code class="inlineCode">wordcloud()</code> performs by default when given raw text data rather than a term matrix. Basically, there are some messages that are excluded from the result because there is no remaining message text after cleaning. For example, the ham message with the text <em class="italic">:)</em> representing the smiley emoji is removed from the set after cleaning. This is not a problem for the word clouds and the warnings can be ignored.</p>
    </div>
    <p class="normal">The resulting word clouds should appear similar to those that follow. Do you have a hunch on which one is the spam cloud, and which represents ham?</p>
    <figure class="mediaobject"><img alt="Text, letter  Description automatically generated" src="../Images/B17290_04_11.png"/></figure>
    <p class="packt_figref">Figure 4.11: Side-by-side word clouds depicting SMS spam and ham messages</p>
    <p class="normal">As you <a id="_idIndexMarker484"/>probably guessed, the spam cloud is on the left. Spam messages include words such as <em class="italic">call</em>, <em class="italic">free</em>, <em class="italic">mobile</em>, <em class="italic">claim</em>, and <em class="italic">stop</em>; these terms do not appear in the ham cloud at all. Instead, ham messages use words such as <em class="italic">can</em>, <em class="italic">sorry</em>, <em class="italic">love</em>, and <em class="italic">time</em>. These stark differences suggest that our Naive Bayes model will have some strong keywords to differentiate between the classes.</p>
    <h3 class="heading-3" id="_idParaDest-99">Data preparation – creating indicator features for frequent words</h3>
    <p class="normal">The<a id="_idIndexMarker485"/> final step in the data preparation process is to transform the sparse matrix into a data structure that can be used to train a Naive Bayes classifier. Currently, the sparse matrix includes over 6,500 features; this is a feature for every word that appears in at least one SMS message. It’s unlikely that all of these are useful for classification. To reduce the number of features, we’ll eliminate any word that appears in less than 5 messages, or in less than about 0.1 percent of records in the training data.</p>
    <p class="normal">Finding frequent words requires the use of the <code class="inlineCode">findFreqTerms()</code> function in the <code class="inlineCode">tm</code> package. This function takes a DTM and returns a character vector containing words that appear at least a minimum number of times. For instance, the following command displays the words appearing at least five times in the <code class="inlineCode">sms_dtm_train</code> matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; findFreqTerms(sms_dtm_train, <span class="hljs-number">5</span>)
</code></pre>
    <p class="normal">The result of the function is a character vector, so let’s save our frequent words for later:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_freq_words &lt;- findFreqTerms(sms_dtm_train, <span class="hljs-number">5</span>)
</code></pre>
    <p class="normal">A <a id="_idIndexMarker486"/>peek into the contents of the vector shows us that there are 1,139 terms appearing in at least 5 SMS messages:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; str(sms_freq_words)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"> chr [1:1137] "£wk" "abiola" "abl" "abt" "accept" "access" "account" "across" "act" "activ" ...
</code></pre>
    <p class="normal">We now need to filter our DTM to include only the terms appearing in the frequent word vector. As before, we’ll use data frame-style <code class="inlineCode">[row, col]</code> operations to request specific sections of the DTM, noting that the DTM column names are based on the words the DTM contains. We can take advantage of this fact to limit the DTM to specific words. Since we want all rows but only the columns representing the words in the <code class="inlineCode">sms_freq_words</code> vector, our commands are:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_dtm_freq_train &lt;- sms_dtm_train[ , sms_freq_words]
&gt; sms_dtm_freq_test &lt;- sms_dtm_test[ , sms_freq_words]
</code></pre>
    <p class="normal">The training and test datasets now include 1,137 features, which correspond to words appearing in at least 5 messages.</p>
    <p class="normal">The Naive Bayes classifier is usually trained on data with categorical features. This poses a problem since the cells in the sparse matrix are numeric and measure the number of times a word appears in a message. We need to change this to a categorical variable that simply indicates yes or no, depending on whether the word appears at all.</p>
    <p class="normal">The following defines a <code class="inlineCode">convert_counts()</code> function to convert counts into <code class="inlineCode">Yes</code> or <code class="inlineCode">No</code> strings:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; convert_counts &lt;- <span class="hljs-keyword">function</span>(x) {
    x &lt;- ifelse(x &gt; 0, <span class="hljs-string">"Yes"</span>, <span class="hljs-string">"No"</span>)
}
</code></pre>
    <p class="normal">By now, some of the pieces of the preceding function should look familiar. The first line defines the function. The statement <code class="inlineCode">ifelse(x &gt; 0, "Yes", "No")</code> transforms the values in <code class="inlineCode">x</code> such that if the value is greater than <code class="inlineCode">0</code>, then it will be replaced with <code class="inlineCode">"Yes"</code>; otherwise, it will be replaced with a <code class="inlineCode">"No"</code> string. Lastly, the newly transformed vector <code class="inlineCode">x</code> is returned.</p>
    <p class="normal">We<a id="_idIndexMarker487"/> now need to apply <code class="inlineCode">convert_counts()</code> to each of the columns in our sparse matrix. You may be able to guess the name of the R function that does exactly this. The function is simply called <code class="inlineCode">apply()</code> and is used much like <code class="inlineCode">lapply()</code> was used previously.</p>
    <p class="normal">The <code class="inlineCode">apply()</code> function allows a function to be used on each of the rows or columns in a matrix. It uses a <code class="inlineCode">MARGIN</code> parameter to specify either rows or columns. Here, we’ll use <code class="inlineCode">MARGIN = 2</code> since we’re interested in the columns (<code class="inlineCode">MARGIN = 1</code> is used for rows). The commands to convert the training and test matrices are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_train &lt;- apply(sms_dtm_freq_train, MARGIN = <span class="hljs-number">2</span>,
    convert_counts)
&gt; sms_test  &lt;- apply(sms_dtm_freq_test, MARGIN = <span class="hljs-number">2</span>,
    convert_counts)
</code></pre>
    <p class="normal">The result will be two character-type matrices, each with cells indicating <code class="inlineCode">"Yes"</code> or <code class="inlineCode">"No"</code> for whether the word represented by the column appears at any point in the message represented by the row.</p>
    <h2 class="heading-2" id="_idParaDest-100">Step 3 – training a model on the data</h2>
    <p class="normal">Now that we <a id="_idIndexMarker488"/>have transformed the raw SMS messages into a format that can be represented by a statistical model, it is time to apply the Naive Bayes algorithm. The algorithm will use the presence or absence of words to estimate the probability that a given SMS message is spam.</p>
    <p class="normal">The Naive Bayes implementation we will employ is in the <code class="inlineCode">naivebayes</code> package. This package is maintained by Michal Majka and is a modern and efficient R implementation. If you have not done so already, be sure to install and load the package using the <code class="inlineCode">install.packages("naivebayes")</code> and <code class="inlineCode">library(naivebayes)</code> commands before continuing.</p>
    <div class="note">
      <p class="normal">Many machine learning approaches are implemented in more than one R package, and Naive Bayes is no exception. Another option is <code class="inlineCode">naiveBayes()</code> in the <code class="inlineCode">e1071</code> package, which was used in older editions of this book but is otherwise nearly identical to <code class="inlineCode">naive_bayes()</code> in usage. The <code class="inlineCode">naivebayes</code> package used in this edition offers better performance and more advanced functionality, which is described at its website: <a href="https://majkamichal.github.io/naivebayes/"><span class="url">https://majkamichal.github.io/naivebayes/</span></a>.</p>
    </div>
    <p class="normal">Unlike the k-NN <a id="_idIndexMarker489"/>algorithm we used for classification in the previous chapter, training a Naive Bayes learner and using it for classification occur in separate stages. Still, as shown in the following table, these steps are fairly straightforward:</p>
    <figure class="mediaobject"><img alt="Text, letter  Description automatically generated" src="../Images/B17290_04_12.png"/></figure>
    <p class="packt_figref">Figure 4.12: Naive Bayes classification syntax</p>
    <p class="normal">Using the <code class="inlineCode">sms_train</code> matrix, the following command trains a <code class="inlineCode">naive_bayes</code> classifier object that can be used to make predictions:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_classifier &lt;- naiveBayes(sms_train, sms_train_labels)
</code></pre>
    <p class="normal">After <a id="_idIndexMarker490"/>running the previous command, you may notice the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">There were 50 or more warnings (use warnings() to see the first 50)
</code></pre>
    <p class="normal">This is nothing to be alarmed about for now; typing the <code class="inlineCode">warnings()</code> command reveals the cause of this issue:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; warnings()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Warning messages:
1: naive_bayes(): Feature £wk – zero probabilities are present. Consider Laplace smoothing.
2: naive_bayes(): Feature 60 biola – zero probabilities are present. Consider Laplace smoothing.
3: naive_bayes(): Feature abl – zero probabilities are present. Consider Laplace smoothing.
4: naive_bayes(): Feature abt – zero probabilities are present. Consider Laplace smoothing.
5: naive_bayes(): Feature accept – zero probabilities are present. Consider Laplace smoothing.
</code></pre>
    <p class="normal">These warnings are caused by words that appeared in zero spam or zero ham messages and have veto power over the classification process due to their associated zero probabilities. For instance, because the word <em class="italic">accept</em> only appeared in ham messages in the training data, it does not mean that every future message with this word should be automatically classified as ham.</p>
    <p class="normal">There is an easy solution to this problem using the Laplace estimator described earlier, but for now, we will evaluate this model using <code class="inlineCode">laplace = 0</code>, which is the model’s default setting.</p>
    <h2 class="heading-2" id="_idParaDest-101">Step 4 – evaluating model performance</h2>
    <p class="normal">To <a id="_idIndexMarker491"/>evaluate the SMS classifier, we need to test its predictions on the unseen messages in the test data. Recall that the unseen message features are stored in a matrix named <code class="inlineCode">sms_test</code>, while the class labels (spam or ham) are stored in a vector named <code class="inlineCode">sms_test_labels</code>. The classifier that we trained has been named <code class="inlineCode">sms_classifier</code>. We will use this classifier to generate predictions and then compare the predicted values to the true values.</p>
    <p class="normal">The <code class="inlineCode">predict()</code> function is used to make the predictions. We will store these in a vector named <code class="inlineCode">sms_test_pred</code>. We simply supply this function with the names of our classifier and test dataset as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_test_pred &lt;- predict(sms_classifier, sms_test)
</code></pre>
    <p class="normal">To <a id="_idIndexMarker492"/>compare the predictions to the true values, we’ll use the <code class="inlineCode">CrossTable()</code> function in the <code class="inlineCode">gmodels</code> package, which we used in previous chapters. This time, we’ll add some additional parameters to eliminate unnecessary cell proportions, and use the <code class="inlineCode">dnn</code> parameter (dimension names) to relabel the rows and columns as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; library(gmodels)
&gt; CrossTable(sms_test_pred, sms_test_labels,
    prop.chisq = <span class="hljs-literal">FALSE</span>, prop.c = <span class="hljs-literal">FALSE</span>, prop.r = <span class="hljs-literal">FALSE</span>,
    dnn = <span class="hljs-built_in">c</span>(<span class="hljs-string">'predicted'</span>, <span class="hljs-string">'actual'</span>))
</code></pre>
    <p class="normal">This produces the following table:</p>
    <pre class="programlisting con"><code class="hljs-con">Total Observations in Table:  1390 
 
             | actual 
   predicted |       ham |      spam | Row Total |
-------------|-----------|-----------|-----------|
         ham |      1201 |        30 |      1231 |
             |     0.864 |     0.022 |           |
-------------|-----------|-----------|-----------|
        spam |         6 |       153 |       159 |
             |     0.004 |     0.110 |           |
-------------|-----------|-----------|-----------|
Column Total |      1207 |       183 |      1390 |
-------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">Looking at the table, we can see that a total of only <em class="italic">6 + 30 = 36</em> of 1,390 SMS messages were incorrectly classified (2.6 percent). Among the errors were 6 out of 1,207 ham messages that were misidentified as spam and 30 of 183 spam messages that were incorrectly labeled as ham. Considering the little effort that we put into the project, this level of performance seems quite impressive. This case study exemplifies the reason why Naive Bayes is so often used for text classification: directly out of the box, it performs surprisingly well.</p>
    <p class="normal">On the <a id="_idIndexMarker493"/>other hand, the six legitimate messages that were incorrectly classified as spam could cause significant problems for the deployment of our filtering algorithm because the filter could cause a person to miss an important text message. We should try to see whether we can slightly tweak the model to achieve better performance.</p>
    <h2 class="heading-2" id="_idParaDest-102">Step 5 – improving model performance</h2>
    <p class="normal">You<a id="_idIndexMarker494"/> may recall that we didn’t set a value for the Laplace estimator when training our model; in fact, it was hard to miss the message from R warning us about more than 50 features with zero probabilities! To address this issue, we’ll build a Naive Bayes model as before, but this time set <code class="inlineCode">laplace = 1</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_classifier2 &lt;- naiveBayes(sms_train, sms_train_labels,
    laplace = <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Next, we’ll make predictions as before:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; sms_test_pred2 &lt;- predict(sms_classifier2, sms_test)
</code></pre>
    <p class="normal">Finally, we’ll compare the predicted classes to the actual classifications using cross-tabulation:</p>
    <pre class="programlisting code"><code class="hljs-code">&gt; CrossTable(sms_test_pred2, sms_test_labels,
    prop.chisq = <span class="hljs-literal">FALSE</span>, prop.c = <span class="hljs-literal">FALSE</span>, prop.r = <span class="hljs-literal">FALSE</span>,
    dnn = <span class="hljs-built_in">c</span>(<span class="hljs-string">'predicted'</span>, <span class="hljs-string">'actual'</span>))
</code></pre>
    <p class="normal">This produces the following table:</p>
    <pre class="programlisting con"><code class="hljs-con">Total Observations in Table:  1390 
             | actual 
   predicted |       ham |      spam | Row Total |
-------------|-----------|-----------|-----------|
         ham |      1202 |        28 |      1230 |
             |     0.865 |     0.020 |           |
-------------|-----------|-----------|-----------|
        spam |         5 |       155 |       160 |
             |     0.004 |     0.112 |           |
-------------|-----------|-----------|-----------|
Column Total |      1207 |       183 |      1390 |
-------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">Adding a Laplace estimator by setting <code class="inlineCode">laplace = 1</code> reduced the number of false positives (ham messages erroneously classified as spam) from 6 to 5, and the number of false <a id="_idIndexMarker495"/>negatives from 30 to 28. Although this seems like a small change, it’s substantial considering that the model’s accuracy was already quite impressive. We’d need to be careful before tweaking the model too much more, as it is important to maintain a balance between being overly aggressive and overly passive when filtering spam. Users prefer that a small number of spam messages slip through the filter rather than the alternative, in which ham messages are filtered too aggressively.</p>
    <h1 class="heading-1" id="_idParaDest-103">Summary</h1>
    <p class="normal">In this chapter, we learned about classification using Naive Bayes. This algorithm constructs tables of probabilities that are used to estimate the likelihood that new examples belong to various classes. The probabilities are calculated using a formula known as Bayes’ theorem, which specifies how dependent events are related. Although Bayes’ theorem can be computationally expensive, a simplified version that makes so-called “naive” assumptions about the independence of features is capable of handling much larger datasets.</p>
    <p class="normal">The Naive Bayes classifier is often used for text classification. To illustrate its effectiveness, we employed Naive Bayes on a classification task involving spam SMS messages. Preparing the text data for analysis required the use of specialized R packages for text processing and visualization. Ultimately, the model was able to classify over 97 percent of all the SMS messages correctly as spam or ham.</p>
    <p class="normal">In the next chapter, we will examine two more machine learning methods. Each performs classification by partitioning data into groups of similar values. As you will discover shortly, these methods are quite useful on their own. Yet, looking further ahead, these basic algorithms also serve as an important foundation for some of the most powerful machine learning methods known today, which will be introduced later in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>.</p>
    <h1 class="heading-1" id="_idParaDest-104">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>