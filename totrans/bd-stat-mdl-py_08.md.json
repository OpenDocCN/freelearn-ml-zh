["```py\nimport pandas as pd\n# create gpa train data\ntrain = pd.DataFrame({'Admitted': [1, 1, 1,1, 1, 0, 1, 1, 0, 1,1,1, 1,1,0, 1, 0, 0, 0, 0, 0, 0, 0, 0 ,0 ,0, 1,1,1,1, 0],\n                    'GPA': [2.8, 3.3, 3.7, 3.7, 3.7, 3.3, 3.7, 3, 1.7, 3.6, 3.3, 4, 3.2, 3.4, 2.8, 4, 1.5, 2.7, 2.3, 2.3, 2.7, 2.2, 3.3,3.3, 4, 2.3, 3.6, 3.4, 4, 3.7, 2.3],\n                    'Exp': [8, 6, 5, 5, 6, 3, 4, 2, 1, 5, 5, 3, 6,5, 4, 4, 4, 1, 1, 2, 2, 2, 1, 4, 4, 4, 5, 2, 4, 6, 3]})\ntrain.head()\n```", "```py\ntest = pd.DataFrame({'Admitted': [1, 0, 1, 0, 1],\n                    'GPA': [2.9, 2.4, 3.8, 3, 3.3],\n                    'Exp': [9, 1, 6, 1,4 ]})\ntest.head()\n```", "```py\nimport statsmodels.formula.api as smf\n#fit logistic regression\nmodel = smf.logit('Admitted ~ GPA + Exp', data =train).fit()\n#summary\nmodel.summary()\n```", "```py\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n# X_test and y_test\nX_test = test[['GPA', 'Exp']]\ny_test = test['Admitted']\n#\ny_hat = model.predict(X_test)\npred = list(map(round, y_hat))\n# confusion matrix\ncm = confusion_matrix(y_test, pred)\nConfusionMatrixDisplay(cm).plot()\n# Accuracy\nprint('Test accuracy = ', accuracy_score(y_test, pred))\n```", "```py\n# import packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score,  ConfusionMatrixDisplay\nimport statsmodels.discrete.discrete_model as sm\n# import Iris data\niris = datasets.load_iris()\nprint(iris.feature_names)\nprint(iris.target_names)\n#create dataframe\ndf = pd.DataFrame(iris.data, columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\ndf['target'] = iris.target\ndf.head()\n# check missing values\ndf.isna().sum()\n# create train and test data\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state =1)\n# fit the model using sklearn\nmodel_sk = LogisticRegression(solver = 'newton-cg', multi_class = 'multinomial')\nmodel_sk.fit(X_train, y_train)\ny_hat_sk = model_sk.predict(X_test)\npred_sk = list(map(round, y_hat_sk))\n# confusion matrix\ncm_sk = confusion_matrix(y_test, pred_sk)\nConfusionMatrixDisplay(cm_sk).plot()\n# Accuracy\nprint('Test accuracy = ', accuracy_score(y_test, pred_sk))\n#fit the model using statsmodels\nmodel_stat = sm.MNLogit(y_train, X_train).fit(method='bfgs')\nmodel_stat.summary()\ny_hat_stat = model_stat.predict(X_test)\npred_stat = np.asarray(y_hat_stat).argmax(1)\n# confusion matrix\ncm_stat = confusion_matrix(y_test, pred_stat)\nConfusionMatrixDisplay(cm_stat).plot()\n# Accuracy\nprint('Test accuracy = ', accuracy_score(y_test, pred_stat))\n```", "```py\n# select variables\nX = df.groupby('isoweek').mean()[['atemp', 'season', 'weathersit','hum','windspeed', 'holiday']]\n# transform holiday variable as an indicator that a holiday occurs within that week\nX['holiday'] = X['holiday'].apply(lambda x: 1 if x > 0.1 else 0)\n# add a constant for the model\nX = sm.add_constant(X)\n# get the response variable\ny = df.groupby('isoweek').mean()['casual']\nfit_model = sm.Poisson(y, X).fit()\nfit_model.summary()\n```", "```py\nimport statsmodels.api as sm\ndata = sm.datasets.fair.load().data\ndata = sm.add_constant(data, prepend=False)\n```", "```py\nprint('Mean count of children per marriage: ', data['children'].mean())\nprint('Variance of the count of children per marriage: ', data['children'].var())\n```", "```py\ny = round(data['children'])\nX = data[['const','age','religious','yrs_married','educ','occupation','occupation_husb','affairs','rate_marriage']]\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n```", "```py\nfrom statsmodels.formula.api import ols as OLS\nimport statsmodels.api as sm\npoisson_model = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\ndf_aux = pd.DataFrame()\ndf_aux['y_mu_hat'] = poisson_model.mu\ndf_aux['children'] = y_train\ndf_aux['y_auxiliary'] = df_aux.apply(lambda x: ((x['children'] - x['y_mu_hat'])**2 - x['y_mu_hat']) / x['y_mu_hat'], axis=1)\nols_model = OLS('y_auxiliary ~ y_mu_hat - 1', df_aux).fit()\nprint(ols_model.params)\n```", "```py\nprint(ols_model.summary())\n```", "```py\nfrom statsmodels.genmod.families.family import NegativeBinomial\nnegative_binomial_model = sm.GLM(y_train, X_train, family=NegativeBinomial(alpha=ols_model.params.values)).fit()\nprint(negative_binomial_model.summary())\n```", "```py\nfrom sklearn.metrics import mean_squared_error as RMSE\nprint('Training Root Mean Squared Error: ', RMSE(y_train, negative_binomial_model.predict(X_train)) )\nprint('Testing Root Mean Squared Error: ', RMSE(y_test, negative_binomial_model.predict(X_test)))\n```"]