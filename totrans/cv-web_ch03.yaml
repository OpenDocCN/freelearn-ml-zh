- en: Chapter 3. Easy Object Detection for Everyone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, We discussed fundamental topics such as matrix operations
    and matrix convolutions. Moreover, we saw how to apply various image filters and
    how to use them in our applications. But those topics are mostly about image processing,
    not about Computer Vision! What do Computer Vision methods do by themselves? They
    provide the ability to understand an image by analyzing it in such a manner that
    the computer can provide the information about objects of an image scene. Libraries,
    which we discussed previously, provide various functionalities to find different
    objects in an image. In this chapter, we will mainly discuss methods that are
    included in the tracking.js ([http://trackingjs.com](http://trackingjs.com)) and
    JSFeat ([http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/))
    libraries to get objects from an image. We will see how to find a colored object,
    and how to find an object using a template. Further, we will create our own object
    detector. These techniques can be implemented not only for an image, but for a
    video too! Finally, we will move on to the object tracking topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Color object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digging into the tracking.js API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptors and object matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting color objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we worked mainly with grayscale images. Of course,
    the shape and intensity parts of objects are important, but what about the color
    information? Why don't we use that too? For example, a red apple on a table can
    be easily detected with just the color information. Actually, that is why color
    object detection sometimes performs much better than other detector methods. In
    addition, it is much faster to implement these algorithms, and a computer usually
    consumes less resources for these types of operation.
  prefs: []
  type: TYPE_NORMAL
- en: The tracking.js library provides an outstanding functionality to create a color
    detection application. We will start from a basic color tracking example. It is
    relatively simple, but you need to keep in mind that it performs best only when
    a colored object can be easily separated from the background.
  prefs: []
  type: TYPE_NORMAL
- en: Using predefined colors with the tracking.js library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Color detection is one of the methods provided by tracking.js. To use it properly,
    we need to learn some background first.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with the intuitive steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we find all connected regions with the specified color. This is the most
    computation consuming part of the algorithm. The smaller the color regions present
    on an image, the faster the algorithm works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we define bounding boxes or rectangles around each of those regions. Finally,
    we merge overlapping boxes to find the main object. Merging is done by just one
    pass. So, if an image has overlapping boxes that are produced after the first
    merge, then they will still overlap each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will run the algorithm using three colors: yellow, magenta, and cyan. Here
    is an example of the color detection algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using predefined colors with the tracking.js library](img/image00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it is really hard to get color objects from the first image.
    It is a bit easier to do so for the second, but it can be easily done only for
    the center of the flower, since it can be separated from the leaves and flower
    petals.
  prefs: []
  type: TYPE_NORMAL
- en: How can we do that with the tracking.js library? In this example, we will use
    the canvas, as we did in the previous chapters. It can be done for other tags
    too, which we will see a bit in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a `ColorTracker` object and add prebuilt colors to it. For
    now, there are only three colors available: `magenta`, `cyan`, and `yellow`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The tracker variable is just a holder for various parameters, based on which
    we will track an object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to define a function which will be called after the `track` event.
    In the example, we just want to show all our bounding boxes over the canvas and
    we execute the `draw` function for each rectangle here. Since the algorithm returns
    a color for a box, it will be easier to see the difference between results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did in the first chapter, we create a `<div>` element for our canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many ways of printing a result, for now, we take an example of the
    draw function from the tracking.js examples section. It will create a `<div>`
    element for each bounding box and append it to the `<div>` tag around the canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we only need to start the tracker by calling the `track` function.
    The first parameter defines the element that contains graphical information about
    where we need to detect a color. The second parameter holds the `tracker` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It was simple, wasn't it? We saw how the tracker works with not-so-easy examples.
    It can be applied for those cases, but it will not be smart to do that. Let's
    see examples where the color detection works really well.
  prefs: []
  type: TYPE_NORMAL
- en: Using your own colors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Only three colors?! Is that it? Of course not. If you want, you can register
    your own colors for object detection. It is not much harder than setting predefined
    colors. We first need to register a new color for the `ColorTracker` function.
    You create a mapping between a string and a function, where the function should
    return a Boolean condition based on three channels: R, G, and B. The `true` is
    returned when the color matches our condition and `false` if not. Here, we want
    to get all colors where the red channel prevails. Since we start from really dark
    pixels, we will call it the `darkRed` color:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'By doing that, we register the darkRed color for all color trackers which we
    create. Now, we need to define a new color tracker with the newly registered color:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'All other parts of the code are the same as they were in the previous example.
    The good thing is that the tracking.js library itself finds the color for a bounding
    box, we do not need to specify it. For example, we have picked a new image—two
    beautiful apples and the result looks something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using your own colors](img/image00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Do you see how the apples stand out from the cloth? That is an example of an
    image where color tracker shows its best performance. Use the color detection
    when an object can be easily separated from the background and you will not be
    disappointed. Furthermore, the basic advantages are that color detection is fast
    in terms of computation, and it is very easy to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Digging into the tracking.js API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw a color tracker and added our own color matcher. The tracking.js library
    provides an excellent functionality to add a new object detector. It has a clear
    API and good documentation to follow ([http://trackingjs.com/docs.html](http://trackingjs.com/docs.html)).
    But first, we will see how to use a tracker with different HTML tags and dig a
    bit into the tracker API.
  prefs: []
  type: TYPE_NORMAL
- en: Using the <img> and <video> tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The library uses a `<canvas>` tag to operate with images. If you run a tracker
    on a different tag, then the library will convert the information from it to the
    canvas automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, tracking can be applied to an `<img>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In that case, we can specify the image path not in a JavaScript code, but in
    the tag itself. To run a tracker, we just need to set the tag `id` as a first
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes the `<video>` tag. In our `<div>` element, which wraps the canvas,
    we need to add a `<video>` tag with the path to a video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The library will take each frame and process it separately. If we want to print
    the result on a canvas, we need to clear the canvas of the previous tracking results.
    We can do that using the `context.clearRect` function and add it to the postprocessing
    part of the tracker functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can draw new elements not only with the `<div>` tags, but also using the
    context itself. It is easier to use and the speed will be a bit faster. Here,
    in addition to just displaying a bounding box, we place a rectangle parameter
    around it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the tracker with a video file, we place the `id` parameters of a `<video>`
    tag, as we did previously with other content tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to work with a camera instead of a video file, you need to remove
    the source from a `<video>` tag and add a new, third, parameter to the `track`
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'What if you have a long-running video? In that case, you probably need to have
    full control over your application, as you may want to stop and rerun the tracker.
    First, you need to get a reference to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can stop or run a tracking task any time with those functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are still not satisfied with your control over the tracking methods,
    there are various functions that can help you while you do the tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setMinDimension`: This sets a bounding box of an object with minimum width
    and height; by default, it is 20 pixels. It helps to avoid noisy objects and focuses
    on objects that hold a larger space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMaxDimension`: This sets the maximum dimensions of a bounding box, it is
    infinity by default. In some cases, it helps to remove an image background which
    is of the same color as the object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMinGroupSize`: This sets the minimum number of pixels for a colored object
    to be classified as a rectangle; the default is 30\. This also helps to reduce
    the noise significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom tracker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is time to build your own tracker! The tracking.js library provides an abstract
    interface for that, but you need to write the core of the tracker—its brains.
    We will create a tracker that will determine edge pixels using the knowledge that
    you gained from the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will create a constructor of our new `CornerTracker` variable.
    We will use the Sobel filter for our example, so we only need one threshold parameter
    for it, which we define here as a field of our object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tracker must inherit from the basic tracker of the tracking.js library,
    which can be done using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important part of our tracker is a `track` function. It contains the
    tracker logic. As parameters, it takes an array of pixels from an image and the
    dimensions of that image. On the inside, we run the Sobel filter, and if you remember,
    it returns a 4-channel array for a grayscale image, but we need only one. We check
    whether the value exceeds the threshold and if so, we add a new edge pixel there.
    After all, we need to emit the computed data using the `emit` function. It sends
    the output data through the `track` event. The output for our example is the coordinates
    of pixels that passed the condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a new tracker, we call the constructor with a threshold parameter
    and set the threshold to 400:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of tracking process, we plot the result. The plot function simply
    puts the pixels on a canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To start our tracker, we need to initiate the tracking function as we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s match the result from the previous chapter with our tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a custom tracker](img/image00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: an image after the Sobel filter, Sobel filter thresholding
    and the result after our tracker.'
  prefs: []
  type: TYPE_NORMAL
- en: As we see, the results match together, so we have done everything right. Good
    job!
  prefs: []
  type: TYPE_NORMAL
- en: The tracking.js API provides a very good abstraction level for creating your
    own object tracker. There are not many trackers there yet, but you can always
    extend the functionality. The main advantage of this abstraction is that you can
    focus on the implementation of algorithms without wasting your time thinking about
    how to apply your algorithm to an image.
  prefs: []
  type: TYPE_NORMAL
- en: Image features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Color object detection and detection of changes in intensity of an image, is
    a simple Computer Vision method. It is a fundamental thing which every Computer
    Vision enthusiast should know. To get a better picture of Computer Vision capabilities,
    we will see how to find an object on a scene using a template. This topic includes
    several parts: feature extraction and descriptor matching. In this part, we will
    discuss feature detection and its application in Computer Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting key points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What information do we get when we see an object on an image? An object usually
    consists of some regular parts or unique points, which represent the particular
    object. Of course, we can compare each pixel of an image, but it is not a good
    idea in terms of computational speed. We can probably take unique points randomly,
    thus reducing the computation cost significantly. However, we will still not get
    much information from random points. Using the whole information, we can get too
    much noise and lose the important parts of an object representation. Eventually,
    we need to consider that both ideas—getting all pixels and selecting random pixels—are
    really bad. So, what can we do instead?
  prefs: []
  type: TYPE_NORMAL
- en: Since we are working with a grayscale image and we need to get a unique point,
    we need to focus on intensity information, for example, getting object edges like
    we did with the Canny edge detector or Sobel filter. We are closer to the solution!
    But still, not close enough. What if we have a long edge? Don't you think that
    it is a bit bad that we have too many unique pixels which lay on this edge? An
    edge of an object has end points or corners. If we reduce our edge to those corners,
    we will get enough unique pixels and remove unnecessary information.
  prefs: []
  type: TYPE_NORMAL
- en: There are various methods of getting keypoints from an image, many of them extract
    corners as keypoints. To get them, we will use the **Features from Accelerated
    Segment Test** (**FAST**) algorithm. It is really simple and you can easily implement
    it by yourself if you want. But you do not need to. The algorithm implementation
    is provided both by the tracking.js and JSFeat libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of the FAST algorithm can be captured from the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting key points](img/image00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Suppose we want to check whether the pixel **P** is a corner, we will check
    16 pixels around it. And if at least 9 pixels in an arc around **P** are much
    darker or brighter than the **P** value, then we say that **P** is a corner. How
    much darker or brighter the **P** pixels be? The decision is made by applying
    a threshold for the difference between the value of **P** and the value of pixels
    around **P**.
  prefs: []
  type: TYPE_NORMAL
- en: A practical example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will start with an example of FAST corner detection for the tracking.js
    library. Before we do something, we can set the detector threshold. The threshold
    defines the minimum difference between a tested corner and points around it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It is usually good practice to apply a Gaussian blur to an image before we
    start the method. It significantly reduces the noise of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, that the blur function returns a 4 channel array—RGBA. In that case,
    we need to convert it to a 1-channel array. Since we can easily skip other channels,
    it should not be a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run a corner detection function on our image array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result returns an array where length is twice that of the corners number.
    The array is returned in a format: `[x0,y0,x1,y1,...]`, where `[xn, yn]` are coordinates
    of a detected corner. To print the result on a canvas, we will use the `fillRect`
    function. Since the number of points is usually around several hundreds, we cannot
    efficiently use `<div>` tag for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will see an example with the JSFeat library, the steps for which are
    very similar to what we saw with tracking.js. First, we set the global threshold
    with a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply a Gaussian blur to the image matrix and run corner detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to preallocate keypoints for a corner''s result. The `keypoint_t` function
    is just a new type, which is useful for key points of an image. The first two
    parameters represent coordinates of a point, and the other parameters represent
    point score (is that point good enough to be a key point?) point level (which
    you can use in an image pyramid, for example), and point angle (which is usually
    used for the gradient orientation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After all this, we execute the FAST corner detection method. As a last parameter
    of the detection function, we define a border size. The border is used to constrain
    circles around each possible corner. For example, you cannot precisely say if
    the point is a corner for the `[0,0]` pixel. There is no `[0, -3]` pixel in our
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we preallocated the corners, the function returns the number of calculated
    corners for us. The result returns an array of structures with the `x` and `y`
    fields, so we can print it using those fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is nearly the same for both algorithms. The difference is in some
    parts of realization. Let''s look at the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A practical example](img/image00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: tracking.js without blur, JSFeat without blur, tracking.js
    and JSFeat with blur.'
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely, you can see the difference between tracking.js and JSFeat
    results, but it is not easy to spot. Look at how much noise was reduced by applying
    just a small 3 x 3 Gaussian filter! A lot of noisy points were removed from the
    background. And now the algorithm can focus on points that represent flowers and
    the pattern of the vase.
  prefs: []
  type: TYPE_NORMAL
- en: We extracted key points from our image, and we successfully reached the goal
    of reducing the number of keypoints and focusing on the unique points of an image.
    Now we need to compare or match those points somehow. How we can do that? We will
    cover this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptors and object matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image features by themselves are a bit useless. Yes, we have found unique points
    on an image. But what did we get? Only pixels values and that's it. If we try
    to compare those values it will not give us much information. Moreover, if we
    change the overall image brightness, we will not find the same keypoints on the
    same image! Taking into account all of this, we need the information which surrounds
    our key points. Moreover, we need a method to efficiently compare this information.
    First, we need to describe the image features, which comes from image descriptors.
    In this part, we will see how those descriptors can be extracted and matched.
    The tracking.js and JSFeat libraries provide different methods for image descriptors.
    We will discuss both.
  prefs: []
  type: TYPE_NORMAL
- en: The BRIEF and ORB descriptors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The descriptors theory is focused on changes in an image pixels' intensities.
    The tracking.js library provides the **Binary Robust Independent Elementary Features**
    (**BRIEF**) descriptors and the its JSFeat extension **Oriented FAST and Rotated
    BRIEF** (**ORB**). As we can see from the ORB naming, it is rotation invariant.
    This means that even if you rotate an object, the algorithm can still detect it.
    Moreover, the authors of the JSFeat library provide an example using the image
    pyramid, which is scale invariant too.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by explaining BRIEF, since it is the source for ORB descriptors.
    As a first step, the algorithm takes computed image features, and it takes the
    unique pairs of elements around each feature. Based on these pairs' intensities,
    it forms a binary string. For example, if we have a positions pair `i` and `j`,
    and if `I(i) < I(j)` (where `I(pos)` indicates the value of the image at the position
    `pos`), then the result is 1, otherwise 0\. We add this result to the binary string.
    We do this for `N` pairs, where `N` is taken as a power of 2 `(128, 256, 512)`.
    Since descriptors are just binary strings, we can compare them in an efficient
    manner. To match these strings, the Hamming distance is usually used. It shows
    the minimum number of substitutions required to change one string to another.
    For example, if we have two binary strings—`10011` and `11001`, the Hamming distance
    between them is `2`, since we need to change two bits of information to change
    the first string to the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSFeat library provides the functionality to apply the ORB descriptors.
    The core idea is very similar to that of BRIEF. There are two major differences:'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is scale invariant, since the descriptors are computed for
    an image pyramid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The descriptors are rotation invariant; the direction is computed using intensity
    of the patch around a feature. Using this orientation, ORB manages to compute
    a BRIEF descriptor in a rotation invariant manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptors implementation and their matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to find an object from a template on a scene image. We can do that
    by finding features and descriptors on both images and by matching descriptors
    from a template to an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start from the tracking.js library and BRIEF descriptors. The first thing
    that you can do is set the number of location pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By default, it is `256`, but you can choose a higher value. The larger the value
    the more information you will get and the more memory and computational cost it
    requires.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting the computation, do not forget to apply the Gaussian blur.
    Next, we find the FAST corners and compute descriptors on both images. In the
    following code, we use the suffix `Object` for a template image and `Scene` for
    a scene image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we do the matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We need to pass both the corners and descriptors information to the function,
    since it returns coordinate information as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we print both images on one canvas. To draw the matches using this trick,
    we need to shift our scene keypoints for the width of a template image. As a `keypoint1`
    the matching function returns points on a template image and `keypoint2` matched
    points from a scene image. The keypoint1/2 are arrays with `x` and `y` coordinates
    at `0` and `1` indexes, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The JSFeat library provides most of the code for pyramids and scale invariant
    features not in the library, but in the examples, which are available on [https://github.com/inspirit/jsfeat/blob/gh-pages/](https://github.com/inspirit/jsfeat/blob/gh-pages/)
    [sample_orb.html](http://sample_orb.html). We will not provide the full code here,
    because it requires too much space. But do not worry; we will highlight the main
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start from functions that are included in the library. First, we need
    to preallocate the descriptors matrix, where 32 is the length of a descriptor
    and 500 is the maximum number of descriptors. Again 32 is a power of two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compute the ORB descriptors for each corner, we need to do this for
    both template and scene images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'JSFeat does not provide a matching function in the library but it does in the
    examples section. The function uses global variables, which mainly define input
    descriptors and output matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `match_t` function contains the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`screen_idx`: This is the index of a scene descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pattern_lev`: This is the index of a pyramid level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pattern_idx`: This is the index of a template descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since ORB works with the image pyramid, it returns corners and matches for
    each level the pyramid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print each matching as follows. Again, we use *Shift*, since we computed
    descriptors on separate images, but print the result on one canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Finding an object location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We found a match. That is great. But what we did not do is find the object location.
    There is no function for that in the tracking.js library but JSFeat provides such
    a functionality in the examples section.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to compute a perspective transform matrix. Remember the first
    chapter? We have points from two images, but we do not have a transformation for
    the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a transform matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: To compute the homography, we need only four points. But after the matching,
    we get too many. In addition, there can be noisy points, which we want to skip
    somehow. For this, we use a **Random sample consensus** (**RANSAC**) algorithm.
    It is an iterative method for estimating a mathematical model from a dataset that
    contains outliers (noise). It estimates outliers and generates a model that is
    computed without the noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, we need to define the algorithm parameters. The first parameter
    is a match mask, where all matches will be marked as good (`1`) or bad (`0`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next parameter is a mathematical model which we want to obtain using the RANSAC
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Third, we need minimum number of points to estimate a model (4 points to get
    a homography), this can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it is useful to have a maximum threshold to classify a data point as
    an inlier or a good match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the variable which holds the main parameters, that is, the last two
    arguments define the maximum ratio of outliers and probability of success. The
    algorithm stops when the number of inliers is 99 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Then we run the RANSAC algorithm. The last parameter represents the number of
    maximum iterations for the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape finding can be applied for both tracking.js and JSFeat libraries,
    you just need to set matches as `object_xy` and `screen_xy`, where those arguments
    must hold an array of objects with the `x` and `y` fields. After we find the transformation
    matrix, we compute the projected shape of an object to a new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'After the computation is done, we draw the computed shapes on our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding an object location](img/image00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we see, our program successfully found an object in both cases. Actually,
    both methods can show different performance, it is mainly based on the thresholds
    you set.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We completed one of the hardest chapters in this book. Congratulations! We saw
    how to find and track a basic colored object and plunged into the depths of library
    APIs. Oh, and don't forget, we have completed our own object detector! The applications
    of Computer Vision methods vary. What we cannot accomplish with the simple color
    detection, we achieve with powerful feature detection and descriptor matching
    algorithms. Both libraries provide different functionalities to match the objects
    and some functions are not included in the libraries. But it should not stop you
    from using those excellent methods. To know how and, probably the more important
    part, when to use those algorithms are the most crucial things you need to know.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most commonly seen objects in our world is a person's face. We interact
    with people everywhere. However, we did not see how to detect such objects in
    an application. The algorithms we covered in this chapter are not so useful for
    face detection, which is why we need to introduce new methods for that. This is
    our topic of the next chapter. See you there!
  prefs: []
  type: TYPE_NORMAL
