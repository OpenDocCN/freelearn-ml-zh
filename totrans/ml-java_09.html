<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Activity Recognition with Mobile Phone Sensors</h1>
                </header>
            
            <article>
                
<p>While the previous chapter focused on pattern recognition in images, this chapter is all about recognizing patterns in sensor data, which, in contrast to images, has temporal dependencies. We will discuss how to recognize granular daily activities such as walking, sitting, and running using mobile phone inertial sensors. The chapter also provides references to related research and emphasizes best practices in the activity recognition community.</p>
<p>The topics covered in this chapter will include the following:</p>
<ul>
<li>Introducing activity recognition, covering mobile phone sensors and the activity recognition pipeline</li>
<li>Collecting sensor data from mobile devices</li>
<li>Discussing activity classification and model evaluation</li>
<li>Deploying an activity recognition model</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing activity recognition</h1>
                </header>
            
            <article>
                
<p>Activity recognition is an underpinning step in behavior analysis, addressing healthy lifestyles, fitness tracking, remote assistance, security applications, elderly care, and so on. Activity recognition transforms low-level sensor data from sensors, such as an accelerometer, gyroscope, pressure sensor, and GPS location, to a higher-level description of behavior primitives.</p>
<p>In most cases, these are basic activities, for example, walking, sitting, lying, jumping, and so on, as shown in the following diagram, or they could be more complex behaviors, such as going to work, preparing breakfast, and shopping:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-697 image-border" src="Images/d4d199bf-5cb5-4eb0-a589-d09e8154e63f.png" style="width:23.25em;height:15.00em;" width="380" height="245"/></p>
<p>In this chapter, we will discuss how to add the activity recognition functionality into a mobile application. We will first look at what an activity recognition problem looks like, what kind of data we need to collect, what the main challenges are, and how to address them.</p>
<p>Later, we will follow an example to see how to actually implement activity recognition in an Android application, including data collection, data transformation, and building a classifier.</p>
<p>Let's start!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Mobile phone sensors</h1>
                </header>
            
            <article>
                
<p>Let's first review what kind of mobile phone sensors there are and what they report. Most smart devices are now equipped with several built-in sensors that measure the motion, position, orientation, and conditions of the ambient environment. As sensors provide measurements with high precision, frequency, and accuracy, it is possible to reconstruct complex user motions, gestures, and movements. Sensors are often incorporated in various applications; for example, gyroscope readings are used to steer an object in a game, GPS data is used to locate the user, and accelerometer data is used to infer the activity that the user is performing, for example, cycling, running, or walking.</p>
<p>The following diagram shows a couple of examples of what kinds of interactions the sensors are able to detect:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-494 image-border" src="Images/6527115b-62c1-444e-a989-6ee81830bc25.png" style="width:46.92em;height:18.42em;" width="670" height="263"/></p>
<p>Mobile phone sensors can be classified into the following three broad categories:</p>
<ul>
<li><strong>Motion sensors:</strong> This sensor measures acceleration and rotational forces along the three perpendicular axes. Examples of sensors in this category include accelerometers, gravity sensors, and gyroscopes.</li>
<li><strong>Environmental sensors:</strong> This sensor measures a variety of environmental parameters, such as illumination, air temperature, pressure, and humidity. This category includes barometers, photometers, and thermometers.</li>
<li><strong>Position sensors:</strong> This sensor measure the physical position of a device. This category includes orientation sensors and magnetometers.</li>
</ul>
<div class="packt_infobox">More detailed descriptions for different mobile platforms are available at the following links:
<ul>
<li><span><strong>Android sensors framework</strong>: </span><a href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">http://developer.android.com/guide/topics/sensors/sensors_overview.html</a></li>
<li><strong>iOS Core Motion framework</strong>:<span> </span><span class="URLPACKT"><a href="https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/">https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/</a></span></li>
<li><strong>Windows phone</strong>:<span> </span><span class="URLPACKT"><a href="https://msdn.microsoft.com/en-us/library/windows/apps/hh202968(v=vs.105).aspx">https://msdn.microsoft.com/en-us/library/windows/apps/hh202968(v=vs.105).aspx</a></span></li>
</ul>
</div>
<p>In this chapter, we will work only with Android's sensors framework.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Activity recognition pipeline</h1>
                </header>
            
            <article>
                
<p>Classifying multidimensional time series sensor data is inherently more complex than classifying traditional nominal data, as we saw in the previous chapters. First, each observation is temporally connected to the previous and following observations, making it very difficult to apply a straightforward classification of a single set of observations only. Second, the data obtained by sensors at different time points is stochastic, that is, unpredictable due to the influence of sensor noise, environmental disturbances, and many other factors. Moreover, an activity can consist of various sub-activities executed in a different manner and each person performs the activity a bit differently, which results in high intraclass differences. Finally, all these reasons make an activity recognition model imprecise, resulting in new data often <span>being</span><span> </span><span>misclassified. One of the highly desirable properties of an activity recognition classifier is to ensure continuity and consistency in the recognized activity sequence.</span></p>
<p>To deal with these challenges, activity recognition is applied to a pipeline, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-495 image-border" src="Images/5b904185-1b46-41ac-b1fa-73ebb843c034.png" style="width:35.25em;height:6.92em;" width="1185" height="233"/></p>
<p>In the first step, we attenuate as much noise as we can, for example, by reducing the sensor sampling rate, removing outliers, applying high-or low-pass filters, and so on. In the next phase, we construct a feature vector. For instance, we convert sensor data from time domain to frequency domain by applying a <strong>discrete Fourier transform</strong> (<strong>DFT</strong>). DFT is a method that takes a list of samples as an input and returns a list of sinusoid coefficients ordered by their frequencies. They represent a combination of frequencies that are present in the original list of samples.</p>
<div class="mce-root packt_infobox"><span>A gentle introduction to the Fourier transform was written by Pete Bevelacqua at </span><span class="URLPACKT"><a href="http://www.thefouriertransform.com/">http://www.thefouriertransform.com/</a></span><span>. If you want to get a more technical and theoretical background on the Fourier transform, take a look at the eighth and ninth lectures in the class by Robert Gallager and Lizhong Zheng at this MIT open course: </span><span class="URLPACKT"><a href="http://theopenacademy.com/content/principles-digital-communication">http://theopenacademy.com/content/principles-digital-communication</a>.</span></div>
<p>Next, based on the feature vector and set of training data, we can build an activity recognition model that assigns an atomic action to each observation. Therefore, for each new sensor reading, the model will output the most probable activity label. However, models make mistakes. Hence, the last phase smooths the transitions between activities by removing transitions that cannot occur in reality; for example, it is not physically feasible that the transition between the activities lying-standing-lying occur in less than half a second, hence such a transition between activities is smoothed as lying-lying-lying.</p>
<p>The activity recognition model is constructed with a supervised learning approach, which consists of training and classification steps. In the training step, a set of labeled data is provided to train the model. The second step is used to assign a label to the new unseen data by the trained model. The data in both phases must be preprocessed with the same set of tools, such as filtering and feature vector computation.</p>
<p>The post processing phase, that is, spurious activity removal, can also be a model itself and hence also requires a learning step. In this case, the preprocessing step also includes activity recognition, which makes such arrangement of classifiers into a meta-learning problem. To avoid overfitting, it is important that the dataset used for training the post processing phase is not the same as the one used for training the activity recognition model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The plan</h1>
                </header>
            
            <article>
                
<p>The plan consists of a training phase and a deployment phase. The training phase boils down to the following steps:</p>
<ol>
<li>Install Android Studio and import <kbd>MyRunsDataCollector.zip</kbd>.</li>
<li>Load the application on your Android phone.</li>
<li>Collect your data, for example, standing, walking, and running, and transform the data to a feature vector consisting of FFTs. Don't panic; low-level signal processing functions such as FFTs will not be written from scratch as we will use existing code to do that. The data will be saved on your phone in a file called <kbd>features.arff</kbd>.</li>
<li>Create and evaluate an activity recognition classifier using exported data and implement a filter for spurious activity transition removal.</li>
<li>Plug the classifier back into the mobile application.</li>
</ol>
<p>If you don't have an Android phone, or if you want to skip all the steps related to the mobile application, just grab the collected dataset located in <kbd>data/features.arff</kbd> and jump directly to the <em>Building a classifier</em> section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Collecting data from a mobile phone</h1>
                </header>
            
            <article>
                
<p>This section describes the first three steps from the plan. If you want to directly work with the data, you can just skip this section and continue to the <em>Building a classifier</em> section. The application implements the essentials to collect sensor data for different activity classes, for example, standing, walking, running, and others.</p>
<p>Let's start by preparing the Android development environment. If you have already installed it, jump to the <em>Loading the data collector</em> section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing Android Studio</h1>
                </header>
            
            <article>
                
<p>Android Studio is a development environment for the Android platform. We will quickly review the installation steps and basic configurations required to start the app on a mobile phone. For a more detailed introduction to Android development, I would recommend an introductory book, <em>Android 5 Programming by Example</em> by Kyle Mew, Packt Publishing.</p>
<p>Grab the latest Android Studio for developers at <a href="https://developer.android.com/studio/">https://developer.android.com/studio/</a> and follow the installation instructions at <a href="http://developer.android.com/sdk/installing/index.html?pkg=studio">http://developer.android.com/sdk/installing/index.html?pkg=studio</a>. The installation will take around 10 minutes, occupying approximately 0.5 GB of space.</p>
<p>Follow the instructions and select your preferred options for installation, and finally click on <span class="packt_screen">Finish</span> to start the installation, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-698 image-border" src="Images/5b8f52fb-fe94-46e3-9211-02479536c078.png" style="width:65.50em;height:49.42em;" width="786" height="593"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data collector</h1>
                </header>
            
            <article>
                
<p>First, grab the source code of <kbd>MyRunsDataCollector</kbd> from GitHub. Once Android Studio is installed, choose the <span class="packt_screen">Open an existing Android Studio project </span><span>option, </span><span>as shown in the following screenshot, and select the</span> <kbd>MyRunsDataCollector</kbd> <span>folder. This will import the project to Android Studio:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-699 image-border" src="Images/01b5d01c-a503-4b36-a0d3-fba9979da67d.png" style="width:57.08em;height:48.42em;" width="685" height="581"/></div>
<p>After the project import is completed, you should be able to see the project file structure, as shown in the following screenshot. The collector consists of <kbd>CollectorActivity.java</kbd>, <kbd>Globals.java</kbd>, and <kbd>SensorsService.java</kbd>. The project also shows <kbd>FFT.java</kbd> implementing low-level signal processing:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-700 image-border" src="Images/a4f1dc26-7697-465a-b268-a5dfcd0a3284.png" style="width:28.25em;height:33.58em;" width="405" height="482"/></p>
<p>The main <kbd>myrunscollector</kbd> package contains the following classes:</p>
<ul>
<li><kbd>Globals.java</kbd>: This defines global constants, such as activity labels and IDs, and data filenames.</li>
<li><kbd>CollectorActivity.java</kbd>: This implements user interface actions, that is, what happens when a specific button is pressed.</li>
<li><kbd>SensorsService.java</kbd>: This implements a service that collects data, calculates the feature vector, as we will discuss in the following sections, and stores the data into a file on the phone.</li>
</ul>
<p>The next question that we will address is how to design features.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature extraction</h1>
                </header>
            
            <article>
                
<p>Finding an appropriate representation of a person's activities is probably the most challenging part of activity recognition. The behavior needs to be represented with simple and general features so that the model using these features will also be general and work well on behaviors different from those in the learning set.</p>
<p>In fact, it is not difficult to design features specific to the captured observations in a training set; such features would work well on them. However, as the training set captures only a part of the whole range of human behavior, overly specific features would likely fail on general behavior:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-701 image-border" src="Images/33155cd1-f586-4674-a049-7ceb1c018cf9.png" style="width:66.50em;height:35.58em;" width="890" height="476"/></p>
<p>Let's see how this is implemented in <kbd>MyRunsDataCollector</kbd>. When the application is started, a method called <kbd>onSensorChanged()</kbd> gets a triple of accelerometer sensor readings (<strong>x</strong>, <strong>y</strong>, and <strong>z</strong>) with a specific timestamp and calculates the magnitude from the sensor readings. The methods buffers up to 64 consecutive magnitudes marked before computing the FFT coefficients.</p>
<p>Now, let's move on to the actual data collection.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Collecting training data</h1>
                </header>
            
            <article>
                
<p>We can now use the collector to collect training data for activity recognition. The collector supports three activities by default, standing, walking, and running, as shown in the following screenshot.</p>
<p>You can select an activity, that is, target class value, and start recording the data by clicking the <span class="packt_screen">START COLLECTING</span> button. Make sure that each activity is recorded for at least three minutes; for example, if the <span class="packt_screen">Walking</span> activity is selected, press <span class="packt_screen">START COLLECTING</span> and walk around for at least three minutes. At the end of the activity, press <span class="packt_screen">Stop collecting</span>. Repeat this for each of the activities.</p>
<p>You could also collect different scenarios involving these activities, for example, walking in the kitchen, walking outside, walking in a line, and so on. By doing so, you will have more data for each activity class and a better classifier. Makes sense, right? The more data, the less confused the classifier will be. If you only have a little data, overfitting will occur and the classifier will confuse classes—standing with walking, walking with running, and so on. However, the more data, the less they get confused. You might collect less than three minutes per class when you are debugging, but for your final polished product, the more data, the better it is. Multiple recording instances will simply be accumulated in the same file.</p>
<p>Note, the <span class="packt_screen">Delete Data</span> button removes the data that is stored in a file on the phone. If you want to start over again, hit <span class="packt_screen">Delete Data</span> before starting; otherwise, the new collected data will be appended at the end of the file:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-702 image-border" src="Images/c2686cb4-6318-41ff-a9fb-7477b78b3d24.png" style="width:21.92em;height:42.42em;" width="597" height="1154"/></p>
<p>The collector implements the diagram discussed in the previous sections: it collects accelerometer samples, computes the magnitudes, uses the <kbd>FFT.java</kbd> class to compute the coefficients, and produces the feature vectors. The data is then stored in a Weka-formatted <kbd>features.arff</kbd> file. The number of feature vectors will vary <span>based on the amount of data you collect</span>. The longer you collect the data, the more feature vectors are accumulated.</p>
<p>Once you stop collecting the training data using the collector tool, we need to grab the data to carry on the workflow. We can use the file explorer in <span class="packt_screen">Android Device Monitor</span> to upload the <kbd>features.arff</kbd> file from the phone and to store it on the computer. You can access your Android Device Monitor by clicking on the Android robot icon, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-501 image-border" src="Images/740619dc-a878-409f-80b4-853062f94596.png" style="width:83.08em;height:2.92em;" width="997" height="35"/></div>
<p>By selecting your device on the left, your phone storage content will be shown on the right-hand side. Navigate through <kbd>mnt/shell/emulated/Android/data/edu.dartmouth.cs.myrunscollector/files/features.arff</kbd>, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-703 image-border" src="Images/ff6dd849-fe2b-42f2-bb3b-a50b03814875.png" style="width:83.33em;height:61.58em;" width="1000" height="739"/></div>
<p>To upload this file to your computer, you need to select the file (it is highlighted) and click on <span class="packt_screen">Upload</span>.</p>
<p>Now, we are ready to build a classifier.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a classifier</h1>
                </header>
            
            <article>
                
<p>Once sensor samples are represented as feature vectors and have the class assigned, it is possible to apply standard techniques for supervised classification, including feature selection, feature discretization, model learning, k-fold cross-validation, and so on. The chapter will not delve into the details of the machine learning algorithms. Any algorithm that supports numerical features can be applied, including SVMs, random forest, AdaBoost, decision trees, neural networks, multilayer perceptrons, and others.</p>
<p>Therefore, let's start with a basic one: decision trees. Here, we will load the dataset, build the set class attribute, build a decision tree model, and output the model:</p>
<pre>String databasePath = "/Users/bostjan/Dropbox/ML Java Book/book/datasets/chap9/features.arff"; 
 
// Load the data in arff format 
Instances data = new Instances(new BufferedReader(new <br/>   FileReader(databasePath))); 
 
// Set class the last attribute as class 
data.setClassIndex(data.numAttributes() - 1); 
 
// Build a basic decision tree model 
String[] options = new String[]{}; 
J48 model = new J48(); 
model.setOptions(options); 
model.buildClassifier(data); 
 
// Output decision tree 
System.out.println("Decision tree model:\n"+model); </pre>
<p>The algorithm first outputs the model, as follows:</p>
<pre>    Decision tree model:
    J48 pruned tree
    ------------------
    
    max &lt;= 10.353474
    |   fft_coef_0000 &lt;= 38.193106: standing (46.0)
    |   fft_coef_0000 &gt; 38.193106
    |   |   fft_coef_0012 &lt;= 1.817792: walking (77.0/1.0)
    |   |   fft_coef_0012 &gt; 1.817792
    |   |   |   max &lt;= 4.573082: running (4.0/1.0)
    |   |   |   max &gt; 4.573082: walking (24.0/2.0)
    max &gt; 10.353474: running (93.0)
    
    Number of Leaves  : 5
    
    Size of the tree : 9</pre>
<p>The tree is quite simplistic and seemingly accurate, as majority class distributions in the terminal nodes are quite high. Let's run a basic classifier evaluation to validate the results, as follows:</p>
<pre>// Check accuracy of model using 10-fold cross-validation 
Evaluation eval = new Evaluation(data); 
eval.crossValidateModel(model, data, 10, new Random(1), new <br/>   String[] {}); 
System.out.println("Model performance:\n"+ <br/>   eval.toSummaryString()); </pre>
<p>This outputs the following model performance:</p>
<pre>    <strong>Correctly Classified Instances         226               92.623  %</strong>
    Incorrectly Classified Instances        18                7.377  %
    Kappa statistic                          0.8839
    Mean absolute error                      0.0421
    Root mean squared error                  0.1897
    Relative absolute error                 13.1828 %
    Root relative squared error             47.519  %
    Coverage of cases (0.95 level)          93.0328 %
    Mean rel. region size (0.95 level)      27.8689 %
    Total Number of Instances              244     </pre>
<p>The classification accuracy scores very high, <kbd>92.62%</kbd>, which is an amazing result. One important reason why the result is so good lies in our evaluation design. What I mean here is the following: sequential instances are very similar to each other, so if we split them randomly during a 10-fold cross-validation, there is a high chance that we use almost identical instances for both training and testing; hence, straightforward k-fold cross-validation produces an optimistic estimate of model performance.</p>
<p>A better approach is to use folds that correspond to different sets of measurements or even different people. For example, we can use the application to collect learning data from five people. Then, it makes sense to run k-person cross-validation, where the model is trained on four people and tested on the fifth person. The procedure is repeated for each person and the results are averaged. This will give us a much more realistic estimate of the model performance.</p>
<p>Leaving evaluation comments aside, let's look at how to deal with classifier errors.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reducing spurious transitions</h1>
                </header>
            
            <article>
                
<p>At the end of the activity recognition pipeline, we want to make sure that the classifications are not too volatile, that is, we don't want activities to change every millisecond. A basic approach is to design a filter that ignores quick changes in the activity sequence.</p>
<p>We build a filter that remembers the last window activities and returns the most frequent one. If there are multiple activities with the same score, it returns the most recent one.</p>
<p>First, we create a new <kbd>SpuriousActivityRemoval</kbd> class, which will hold a list of activities and the <kbd>window</kbd> parameter:</p>
<pre>class SpuriousActivityRemoval{ 
   
  List&lt;Object&gt; last; 
  int window; 
   
  public SpuriousActivityRemoval(int window){ 
    this.last = new ArrayList&lt;Object&gt;(); 
    this.window = window; 
  } </pre>
<p>Next, we create the <kbd>Object filter(Object)</kbd> method, which will take an activity and return a filtered activity. The method first checks whether we have enough observations. If not, it simply stores the observation and returns the same value, as shown in the following code:</p>
<pre>  public Object filter(Object obj){ 
    if(last.size() &lt; window){ 
      last.add(obj); 
      return obj; 
  } </pre>
<p>If we already collected <kbd>window</kbd> observations, we simply return the most frequent observation, remove the oldest observation, and insert the new observation:</p>
<pre>    Object o = getMostFrequentElement(last); 
    last.add(obj); 
    last.remove(0); 
    return o; 
  } </pre>
<p>What is missing here is a function that returns the most frequent element from a list of objects. We implement this with a hash map, as follows:</p>
<pre>  private Object getMostFrequentElement(List&lt;Object&gt; list){ 
     
    HashMap&lt;String, Integer&gt; objectCounts = new HashMap&lt;String, <br/>       Integer&gt;(); 
    Integer frequntCount = 0; 
    Object frequentObject = null; </pre>
<p>Now, we iterate over all the elements in the list, insert each unique element into a hash map, or update its counter if it is already in the hash map. At the end of the loop, we store the most frequent element that we found so far, as follows:</p>
<pre>    for(Object obj : list){ 
      String key = obj.toString(); 
      Integer count = objectCounts.get(key); 
      if(count == null){ 
        count = 0; 
      } 
      objectCounts.put(key, ++count); 
       
      if(count &gt;= frequntCount){ 
        frequntCount = count; 
        frequentObject = obj; 
      } 
    } 
     
    return frequentObject; 
  } 
   
} </pre>
<p>Let's run a simple example:</p>
<pre>String[] activities = new String[]{"Walk", "Walk", "Walk", "Run", <br/>   "Walk", "Run", "Run", "Sit", "Sit", "Sit"}; 
SpuriousActivityRemoval dlpFilter = new <br/>   SpuriousActivityRemoval(3); 
for(String str : activities){ 
  System.out.println(str +" -&gt; "+ dlpFilter.filter(str)); 
} </pre>
<p>The example outputs the following activities:</p>
<pre>    Walk -&gt; Walk
    Walk -&gt; Walk
    Walk -&gt; Walk
    Run -&gt; Walk
    Walk -&gt; Walk
    Run -&gt; Walk
    Run -&gt; Run
    Sit -&gt; Run
    Sit -&gt; Run
    Sit -&gt; Sit</pre>
<p>The result is a continuous sequence of activities, that is, we do not have quick changes. This adds some delay, but unless this is absolutely critical for the application, it is acceptable.</p>
<p>Activity recognition may be enhanced by appending <em>n</em> previous activities, as recognized by the classifier, to the feature vector. The danger of appending previous activities is that the machine learning algorithm may learn that the current activity is always the same as the previous one, as this will often be the case. The problem may be solved by having two classifiers, A and B: classifier B's attribute vector contains <em>n</em> previous activities as recognized by classifier A. Classifier A's attribute vector does not contain any previous activities. This way, even if B gives a lot of weight to the previous activities, the previous activities as recognized by A will change as A is not burdened with B's inertia.</p>
<p>All that remains to do is to embed the classifier and filter it into our mobile application.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Plugging the classifier into a mobile app</h1>
                </header>
            
            <article>
                
<p>There are two ways to incorporate a classifier into a mobile application. The first one involves exporting a model in the Weka format, using the Weka library as a dependency in our mobile application, loading the model, and so on. The procedure is identical to the example we saw in <a href="e0c71e12-6bd7-4f63-b71d-78bb5a87b801.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Basic Algorithms–Classification, Regression, and Clustering</em>. The second approach is more lightweight: we export the model as source code, for example, we create a class implementing the decision tree classifier. Then, we can simply copy and paste the source code into our mobile app, without even importing any Weka dependencies.</p>
<p>Fortunately, some Weka models can be easily exported to source code by the <kbd>toSource(String)</kbd> function:</p>
<pre>// Output source code implementing the decision tree 
System.out.println("Source code:\n" +  
  model.toSource("ActivityRecognitionEngine")); </pre>
<p>This outputs an <kbd>ActivityRecognitionEngine</kbd> class that corresponds to our model. Now, let's take a closer look at the output code:</p>
<pre>class ActivityRecognitionEngine { 
 
  public static double classify(Object[] i) 
    throws Exception { 
 
    double p = Double.NaN; 
    p = ActivityRecognitionEngine.N17a7cec20(i); 
    return p; 
  } 
  static double N17a7cec20(Object []i) { 
    double p = Double.NaN; 
    if (i[64] == null) { 
      p = 1; 
    } else if (((Double) i[64]).doubleValue() &lt;= 10.353474) { 
    p = ActivityRecognitionEngine.N65b3120a1(i); 
    } else if (((Double) i[64]).doubleValue() &gt; 10.353474) { 
      p = 2; 
    }  
    return p; 
  } 
... </pre>
<p>The outputted <kbd>ActivityRecognitionEngine</kbd> class implements the decision tree that we discussed earlier. The machine-generated function names, such as <kbd>N17a7cec20(Object [])</kbd>, correspond to decision tree nodes. The classifier can be called by the <kbd>classify(Object[])</kbd> method, where we should pass a feature vector obtained by the same procedure as we discussed in the previous sections. As usual, it returns a <kbd>double</kbd>, indicating a class label index.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how to implement an activity recognition model for mobile applications. We looked into the completed process, including data collection, feature extraction, model building, evaluation, and model deployment.</p>
<p>In the next chapter, we will move on to another Java library targeted at text analysis: Mallet.</p>


            </article>

            
        </section>
    </div>



  </body></html>