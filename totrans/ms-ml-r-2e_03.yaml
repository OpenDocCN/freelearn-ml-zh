- en: Logistic Regression and Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The true logic of this world is the calculus of probabilities."'
  prefs: []
  type: TYPE_NORMAL
- en: '- James Clerk Maxwell, Scottish physicist'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we took a look at using **Ordinary Least Squares**
    (**OLS**) to predict a quantitative outcome, or in other words, linear regression.
    It is now time to shift gears somewhat and examine how we can develop algorithms
    to predict qualitative outcomes. Such outcome variables could be binary (male
    versus female, purchase versus does not purchase, tumor is benign versus malignant)
    or multinomial categories (education level or eye color). Regardless of whether
    the outcome of interest is binary or multinomial, the task of the analyst is to
    predict the probability of an observation belonging to a particular category of
    the outcome variable. In other words, we develop an algorithm in order to classify
    the observations.
  prefs: []
  type: TYPE_NORMAL
- en: To begin exploring classification problems, we will discuss why applying the
    OLS linear regression is not the correct technique and how the algorithms introduced
    in this chapter can solve these issues. We will then look at a problem of predicting
    whether or not a biopsied tumor mass is classified as benign or malignant. The
    dataset is the well-known and widely available **Wisconsin Breast Cancer Data**.
    To tackle this problem, we will begin by building and interpreting logistic regression
    models. We will also begin examining methods so as to select features and the
    most appropriate model. Next, we will discuss both linear and quadratic discriminant
    analyses and compare and contrast these with logistic regression. Then, building
    predictive models on the breast cancer data will follow. Finally, we will wrap
    it up by looking at multivariate regression splines and ways to select the best
    overall algorithm in order to address the question at hand. These methods (creating `test`/`train`
    datasets and cross-validation) will set the stage for more advanced machine learning
    methods in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Classification methods and linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, why can''t we just use the least square regression method that we learned
    in the previous chapter for a qualitative outcome? Well, as it turns out, you
    can, but at your own risk. Let''s assume for a second that you have an outcome
    that you are trying to predict and it has three different classes: mild, moderate,
    and severe. You and your colleagues also assume that the difference between mild
    and moderate and moderate and severe is an equivalent measure and a linear relationship.
    You can create a dummy variable where 0 is equal to mild, 1 is equal to moderate,
    and 2 is equal to severe. If you have reason to believe this, then linear regression
    might be an acceptable solution. However, qualitative assessments such as the
    previous ones might lend themselves to a high level of measurement error that
    can bias the OLS. In most business problems, there is no scientifically acceptable
    way to convert a qualitative response to one that is quantitative. What if you
    have a response with two outcomes, say fail and pass? Again, using the dummy variable
    approach, we could code the fail outcome as `0` and the pass outcome as `1`. Using
    linear regression, we could build a model where the predicted value is the probability
    of an observation of pass or fail. However, the estimates of `Y` in the model
    will most likely exceed the probability constraints of `[0,1]` and thus be a bit
    difficult to interpret.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As previously discussed, our classification problem is best modeled with the
    probabilities that are bound by `0` and `1`. We can do this for all of our observations
    with a number of different functions, but here we will focus on the logistic function.
    The logistic function used in logistic regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_01a.png)'
  prefs: []
  type: TYPE_IMG
- en: If you have ever placed a friendly wager on horse races or the World Cup, you
    may understand the concept better as odds. The logistic function can be turned
    to odds with the formulation of *Probability (Y) / 1 - Probability (Y)*. For instance,
    if the probability of Brazil winning the World Cup is 20 percent, then the odds
    are *0.2 / 1 - 0.2*, which is equal to *0.25*, translating to odds of one in four.
  prefs: []
  type: TYPE_NORMAL
- en: To translate the odds back to probability, take the odds and divide by one plus
    the odds. The World Cup example is thus *0.25 / 1 + 0.25*, which is equal to 20
    percent. Additionally, let's consider the odds ratio. Assume that the odds of
    Germany winning the Cup are *0.18*. We can compare the odds of Brazil and Germany
    with the odds ratio. In this example, the odds ratio would be the odds of Brazil
    divided by the odds of Germany. We will end up with an odds ratio equal to *0.25/0.18*,
    which is equal to *1.39*. Here, we will say that Brazil is *1.39* times more likely
    than Germany to win the World Cup.
  prefs: []
  type: TYPE_NORMAL
- en: One way to look at the relationship of logistic regression with linear regression
    is to show logistic regression as the log odds or *log (P(Y)/1 - P(Y))* is equal
    to *Bo + B1x*. The coefficients are estimated using a maximum likelihood instead
    of the OLS. The intuition behind the maximum likelihood is that we are calculating
    the estimates for *Bo* and *B1,* which will create a predicted probability for
    an observation that is as close as possible to the actual observed outcome of
    *Y*, a so-called likelihood. The R language does what other software packages
    do for the maximum likelihood, which is to find the optimal combination of beta
    values that maximize the likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: With these facts in mind, logistic regression is a very powerful technique to
    predict the problems involving classification and is often the starting point
    for model creation in such problems. Therefore, in this chapter, we will attack
    the upcoming business problem with logistic regression first and foremost.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dr. William H. Wolberg from the University of Wisconsin commissioned the Wisconsin
    Breast Cancer Data in 1990\. His goal behind collecting the data was to identify
    whether a tumor biopsy was malignant or benign. His team collected the samples
    using **Fine Needle Aspiration** (**FNA**). If a physician identifies the tumor
    through examination or imaging an area of abnormal tissue, then the next step
    is to collect a biopsy. FNA is a relatively safe method of collecting the tissue,
    and complications are rare. Pathologists examine the biopsy and attempt to determine
    the diagnosis (malignant or benign). As you can imagine, this is not a trivial
    conclusion. Benign breast tumors are not dangerous as there is no risk of the
    abnormal growth spreading to other body parts. If a benign tumor is large enough,
    surgery might be needed to remove it. On the other hand, a malignant tumor requires
    medical intervention. The level of treatment depends on a number of factors, but
    it's most likely that surgery will be required, which can be followed by radiation
    and/or chemotherapy.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the implications of a misdiagnosis can be extensive. A false positive
    for malignancy can lead to costly and unnecessary treatment, subjecting the patient
    to a tremendous emotional and physical burden. On the other hand, a false negative
    can deny a patient the treatment that they need, causing the cancer cells to spread
    and leading to premature death. Early treatment intervention in breast cancer
    patients can greatly improve their survival.
  prefs: []
  type: TYPE_NORMAL
- en: Our task then is to develop the best possible diagnostic machine learning algorithm
    in order to assist the patient's medical team in determining whether the tumor
    is malignant or not.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This dataset consists of tissue samples from 699 patients. It is in a data
    frame with 11 variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ID`: Sample code number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V1`: Thickness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V2`: Uniformity of the cell size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V3`: Uniformity of the cell shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V4`: Marginal adhesion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V5`: Single epithelial cell size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V6`: Bare nucleus (16 observations are missing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V7`: Bland chromatin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V8`: Normal nucleolus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`V9`: Mitosis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class`: Whether the tumor diagnosis is benign or malignant; this will be the
    outcome that we are trying to predict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The medical team has scored and coded each of the nine features on a scale of
    `1` to `10`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data frame is available in the R `MASS` package under the `biopsy` name.
    To prepare this data, we will load the data frame, confirm the structure, rename
    the variables to something meaningful, and delete the missing observations. At
    this point, we can begin to explore the data visually. Here is the code that will
    get us started when we first load the library and then the dataset; using the
    `str()` function, we will examine the underlying structure of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: An examination of the data structure shows that our features are integers and
    the outcome is a factor. No transformation of the data to a different structure
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now get rid of the `ID` column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will rename the variables and confirm that the code has worked as
    intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will delete the missing observations. As there are only 16 observations
    with the missing data, it is safe to get rid of them as they account for only
    2 percent of all the observations. A thorough discussion of how to handle the
    missing data is outside the scope of this chapter and has been included in the
    [Appendix A](5d62fd21-3280-4845-a21e-4700d91225d1.xhtml), *R Fundamentals*, where
    I cover data manipulation. In deleting these observations, a new working data
    frame is created. One line of code does this trick with the `na.omit` function,
    which deletes all the missing observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the package in R that you are using to analyze the data, the outcome
    needs to be numeric, which is `0` or `1`. In order to accommodate that requirement,
    create the variable `y`, where benign is zero and malignant 1, using the `ifelse()`
    function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of ways in which we can understand the data visually in
    a classification problem, and I think a lot of it comes down to personal preference.
    One of the things that I like to do in these situations is examine the **boxplots**
    of the features that are split by the classification outcome. This is an excellent
    way to begin understanding which features may be important to the algorithm. Boxplots
    are a simple way to understand the distribution of the data at a glance. In my
    experience, it also provides you with an effective way to build the presentation
    story that you will deliver to your customers. There are a number of ways to do
    this quickly, and the `lattice` and `ggplot2` packages are quite good at this
    task. I will use `ggplot2` in this case with the additional package, `reshape2`.
    After loading the packages, you will need to create a data frame using the `melt()`
    function. The reason to do this is that melting the features will allow the creation
    of a matrix of boxplots, allowing us to easily conduct the following visual inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code melts the data by their values into one overall feature
    and groups them by class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Through the magic of `ggplot2`, we can create a 3x3 boxplot matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: How do we interpret a boxplot? First of all, in the preceding screenshot, the
    thick white boxes constitute the upper and lower quartiles of the data; in other
    words, half of all the observations fall in the thick white box area. The dark
    line cutting across the box is the median value. The lines extending from the
    boxes are also quartiles, terminating at the maximum and minimum values, outliers
    notwithstanding. The black dots constitute the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: By inspecting the plots and applying some judgment, it is difficult to determine
    which features will be important in our classification algorithm. However, I think
    it is safe to assume that the nuclei feature will be important, given the separation
    of the median values and corresponding distributions. Conversely, there appears
    to be little separation of the mitosis feature by class, and it will likely be
    an irrelevant feature. We shall see!
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of our features quantitative, we can also do a correlation analysis
    as we did with linear regression. Collinearity with logistic regression can bias
    our estimates just as we discussed with linear regression. Let''s load the `corrplot`
    package and examine the correlations as we did in the previous chapter, this time
    using a different type of correlation matrix, which has both shaded ovals and
    the correlation coefficients in the same plot, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: The correlation coefficients are indicating that we may have a problem with
    collinearity, in particular, the features of uniform shape and uniform size that
    are present. As part of the logistic regression modeling process, it will be necessary
    to incorporate the VIF analysis as we did with linear regression. The final task
    in the data preparation will be the creation of our `train` and `test` datasets.
    The purpose of creating two different datasets from the original one is to improve
    our ability so as to accurately predict the previously unused or unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, in machine learning, we should not be so concerned with how well
    we can predict the current observations and should be more focused on how well
    we can predict the observations that were not used in order to create the algorithm.
    So, we can create and select the best algorithm using the training data that maximizes
    our predictions on the `test` set. The models that we will build in this chapter
    will be evaluated by this criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of ways to proportionally split our data into `train` and
    `test` sets: 50/50, 60/40, 70/30, 80/20, and so forth. The data split that you
    select should be based on your experience and judgment. For this exercise, I will
    use a 70/30 split, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that we have a well-balanced outcome variable between the two datasets,
    we will perform the following check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is an acceptable ratio of our outcomes in the two datasets; with this,
    we can begin the modeling and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this part of the process, we will start with a logistic regression model
    of all the input variables and then narrow down the features with the best subsets.
    After this, we will try our hand at **discriminant analysis** and **Multivariate
    Adaptive Regression Splines** (**MARS**).
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve already discussed the theory behind logistic regression, so we can begin
    fitting our models. An R installation comes with the `glm()` function fitting the
    generalized linear models, which are a class of models that includes logistic
    regression. The code syntax is similar to the `lm()` function that we used in
    the previous chapter. One big difference is that we must use the `family = binomial`
    argument in the function, which tells R to run a logistic regression method instead
    of the other versions of the generalized linear models. We will start by creating
    a model that includes all of the features on the `train` set and see how it performs
    on the `test` set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary()` function allows us to inspect the coefficients and their p-values.
    We can see that only two features have p-values less than `0.05` (thickness and
    nuclei). An examination of the 95 percent confidence intervals can be called on
    with the `confint()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that the two significant features have confidence intervals that do not
    cross zero. You cannot translate the coefficients in logistic regression as the
    change in *Y* is based on a one-unit change in *X*. This is where the odds ratio
    can be quite helpful. The beta coefficients from the log function can be converted
    to odds ratios with an exponent (beta).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to produce the odds ratios in R, we will use the following `exp(coef())`
    syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The interpretation of an odds ratio is the change in the outcome odds resulting
    from a unit change in the feature. If the value is greater than 1, it indicates
    that, as the feature increases, the odds of the outcome increase. Conversely,
    a value less than 1 would mean that, as the feature increases, the odds of the
    outcome decrease. In this example, all the features except `u.size` will increase
    the log odds.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the issues pointed out during data exploration was the potential issue
    of multi-collinearity. It is possible to produce the VIF statistics that we did
    in linear regression with a logistic model in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: None of the values are greater than the VIF rule of thumb statistic of five,
    so collinearity does not seem to be a problem. Feature selection will be the next
    task; but, for now, let's produce some code to look at how well this model does
    on both the `train` and `test` sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will first have to create a vector of the predicted probabilities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to evaluate how well the model performed in training and then
    evaluate how it fits on the test set. A quick way to do this is to produce a confusion
    matrix. In later chapters, we will examine the version provided by the `caret`
    package. There is also a version provided in the `InformationValue` package. This
    is where we will need the outcome as 0''s and 1''s. The default value by which
    the function selects either benign or malignant is 0.50, which is to say that
    any probability at or above 0.50 is classified as malignant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The rows signify the predictions, and the columns signify the actual values.
    The diagonal elements are the correct classifications. The top right value, `7`,
    is the number of false negatives, and the bottom left value, `8`, is the number
    of false positives. We can also take a look at the error rate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It seems we have done a fairly good job with only a *3.16%* error rate on the
    training set. As we previously discussed, we must be able to accurately predict
    unseen data, in other words, our `test` set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method to create a confusion matrix for the `test` set is similar to how
    we did it for the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It looks like we have done pretty well in creating a model with all the features.
    The roughly 98 percent prediction rate is quite impressive. However, we must still
    check whether there is room for improvement. Imagine that you or your loved one
    is a patient who has been diagnosed incorrectly. As previously mentioned, the
    implications can be quite dramatic. With this in mind, is there perhaps a better
    way to create a classification algorithm? Let's have a look!
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression with cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of cross-validation is to improve our prediction of the `test` set
    and minimize the chance of overfitting. With the **K-fold cross-validation**,
    the dataset is split into K equal-sized parts. The algorithm learns by alternatively
    holding out one of the **K-sets**; it fits a model to the other **K-1 parts**,
    and obtains predictions for the left-out K-set. The results are then averaged
    so as to minimize the errors, and appropriate features are selected. You can also
    perform the **Leave-One-Out-Cross-Validation** (**LOOCV**) method, where K is
    equal to 1\. Simulations have shown that the LOOCV method can have averaged estimates
    that have a high variance. As a result, most machine learning experts will recommend
    that the number of K-folds should be 5 or 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'An R package that will automatically do `CV` for logistic regression is the
    `bestglm` package. This package is dependent on the `leaps` package that we used
    for linear regression. The syntax and formatting of the data require some care,
    so let''s walk through this in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the package, we will need our outcome coded to `0` or `1`. If
    left as a factor, it will not work. The other requirement to utilize the package
    is that your outcome, or `y`, is the last column and all the extraneous columns
    have been removed. A new data frame will do the trick for us, via the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the code to run in order to use the `CV` technique with our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax, `Xy = Xy`, points to our properly formatted data frame. `IC = "CV"`
    tells the package that the information criterion to use is cross-validation. `CVArgs`
    are the `CV` arguments that we want to use. The `HTF` method is K-fold, which
    is followed by the number of folds, `K = 10`, and we are asking it to do only
    one iteration of the random folds with `REP = 1`. Just as with `glm()`, we will
    need to use `family = binomial`. On a side note, you can use `bestglm` for linear
    regression as well by specifying `family = gaussian`. So, after running the analysis,
    we will end up with the following output, giving us three features for `Best Model`,
    such as `thick`, `u.size`, and `nucl`. The statement on `Morgan-Tatar search`
    simply means that a simple exhaustive search was done for all the possible subsets,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can put these features in `glm()` and then see how well the model did on
    the `test` set. A `predict()` function will not work with `bestglm`, so this is
    a required step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, the following code allows us to compare the predicted labels with
    the actual ones on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The reduced feature model is slightly less accurate than the full feature model,
    but all is not lost. We can utilize the `bestglm` package again, this time using
    the best subsets with the information criterion set to `BIC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'These four features provide the minimum `BIC` score for all possible subsets.
    Let''s try this and see how it predicts the `test` set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have five errors, just like the full model. The obvious question then
    is: which one is better? In any normal situation, the rule of thumb is to default
    to the simplest or the easiest-to-interpret model, given equality of generalization.
    We could run a completely new analysis with a new randomization and different
    ratios of the `train` and `test` sets among others. However, let''s assume for
    a moment that we''ve exhausted the limits of what logistic regression can do for
    us. We will come back to the full model and the model that we developed on a `BIC`
    minimum at the end and discuss the methods of model selection. Now, let''s move
    on to our discriminant analysis methods, which we will also include as possibilities
    in the final recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Discriminant Analysis** (**DA**), also known as **Fisher Discriminant Analysis**
    (**FDA**), is another popular classification technique. It can be an effective
    alternative to logistic regression when the classes are well-separated. If you
    have a classification problem where the outcome classes are well-separated, logistic
    regression can have unstable estimates, which is to say that the confidence intervals
    are wide and the estimates themselves likely vary from one sample to another (James,
    2013). DA does not suffer from this problem and, as a result, may outperform and
    be more generalized than logistic regression. Conversely, if there are complex
    relationships between the features and outcome variables, it may perform poorly
    on a classification task. For our breast cancer example, logistic regression performed
    well on the testing and training sets, and the classes were not well-separated.
    For the purpose of comparison with logistic regression, we will explore DA, both
    **Linear Discriminant Analysis** (**LDA**) and **Quadratic Discriminant Analysis**
    (**QDA**).'
  prefs: []
  type: TYPE_NORMAL
- en: DA utilizes **Baye's theorem** in order to determine the probability of the
    class membership for each observation. If you have two classes, for example, benign
    and malignant, then DA will calculate an observation's probability for both the
    classes and select the highest probability as the proper class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' theorem states that the probability of *Y* occurring--given that *X*
    has occurred--is equal to the probability of both *Y* and *X* occurring, divided
    by the probability of *X* occurring, which can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_aa.png)'
  prefs: []
  type: TYPE_IMG
- en: The numerator in this expression is the likelihood that an observation is from
    that class level and has these feature values. The denominator is the likelihood
    of an observation that has these feature values across all the levels. Again,
    the classification rule says that if you have the joint distribution of *X* and
    *Y* and if *X* is given, the optimal decision about which class to assign an observation
    to is by choosing the class with the larger probability (the posterior probability).
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of attaining posterior probabilities goes through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect data with a known class membership.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the prior probabilities; this represents the proportion of the sample
    that belongs to each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the mean for each feature by their class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the variance--covariance matrix for each feature; if it is an LDA,
    then this would be a pooled matrix of all the classes, giving us a linear classifier,
    and if it is a QDA, then a variance--covariance created for each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the normal distribution (Gaussian densities) for each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the `discriminant` function that is the rule for the classification
    of a new object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign an observation to a class based on the `discriminant` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will provide an expanded notation on the determination of the posterior
    probabilities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though LDA is elegantly simple, it is limited by the assumption that the
    observations of each class are said to have a multivariate normal distribution,
    and there is a common covariance across the classes. QDA still assumes that observations
    come from a normal distribution, but it also assumes that each class has its own
    covariance.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter? When you relax the common covariance assumption, you now
    allow quadratic terms into the discriminant score calculations, which was not
    possible with LDA. The mathematics behind this can be a bit intimidating and are outside
    the scope of this book. The important part to remember is that QDA is a more flexible
    technique than logistic regression, but we must keep in mind our **bias-variance**
    trade-off. With a more flexible technique, you are likely to have a lower bias
    but potentially a higher variance. Like a lot of flexible techniques, a robust
    set of training data is needed to mitigate a high classifier variance.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is performed in the `MASS` package, which we have already loaded so that
    we can access the biopsy data. The syntax is very similar to the `lm()` and `glm()`
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin fitting our LDA model, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This output shows us that `Prior probabilities of groups` are approximately
    64 percent for benign and 36 percent for malignancy. Next is `Group means`. This
    is the average of each feature by their class. `Coefficients of linear discriminants`
    are the standardized linear combination of the features that are used to determine
    an observation's discriminant score. The higher the score, the more likely that
    the classification is `malignant`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plot()` function in LDA will provide us with a histogram and/or the densities
    of the discriminant scores, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there is some overlap in the groups, indicating that there will
    be some incorrectly classified observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` function available with LDA provides a list of three elements:
    class, posterior, and x. The class element is the prediction of benign or malignant,
    the posterior is the probability score of x being in each class, and x is the
    linear discriminant score. Let''s just extract the probability of an observation
    being malignant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, unfortunately, it appears that our LDA model has performed much worse
    than the logistic regression models. The primary question is to see how this will
    perform on the `test` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: That's actually not as bad as I thought, given the lesser performance on the
    training data. From a correctly classified perspective, it still did not perform
    as well as logistic regression (96 percent versus almost 98 percent with logistic
    regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to fit a QDA model. In R, QDA is also part of the `MASS`
    package and the function is `qda()`. Building the model is rather straightforward
    again, and we will store it in an object called `qda.fit`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As with LDA, the output has `Group means` but does not have the coefficients
    because it is a quadratic function as discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predictions for the `train` and `test` data follow the same flow of code
    as with LDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can quickly tell that QDA has performed the worst on the training data with
    the confusion matrix, and it has classified the `test` set poorly with 11 incorrect
    predictions. In particular, it has a high rate of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Adaptive Regression Splines (MARS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you like a modeling technique that provides all of the following?
  prefs: []
  type: TYPE_NORMAL
- en: Offers the flexibility to build linear and nonlinear models for both regression
    and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can support variable interaction terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is simple to understand and explain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires little data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handles all types of data: numeric, factors, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well on unseen data, that is, it does well in bias-variance trade-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that all sounds appealing, then I cannot recommend the use of MARS models
    enough. The method was brought to my attention several months ago, and I have
    found it to perform extremely well. In fact, in a recent case of mine, it outperformed
    both a random forest and boosted trees on test/validation data. It has quickly
    become my baseline model and all others are competitors. The other benefit I've
    seen is that it has negated much of the feature engineering I was doing. Much
    of that was using **Weight-of-Evidence** (**WOE**) and **Information Values** (**IV**)
    to capture nonlinearity and recode the variables.  This WOE/IV technique was something
    I was planning to write about at length in this second edition. However, I've
    done a number of tests, and MARS does an excellent job of doing what that technique
    does (that is, capture non-linearity), so I will not discuss WOE/IV at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand MARS is quite simple. First, just start with a linear or generalized
    linear model like we discussed previously. Then, to capture any nonlinear relationship,
    a `hinge` function is added. These `hinges` are simply points in the input feature
    that equate to a coefficient change. For example, say we have *Y = 12.5 (our intercept)
    + 1.5(variable 1) + 3.3(variable 2); where variables 1 and 2 are on a scale of
    1 to 10*. Now, let''s see how a `hinge` function for variable *2* could come into
    play:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = 11 (new intercept) + 1.5(variable 1) + 4.26734(max(0, variable 2 - 5.5)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we read the `hinge` function as: we take the maximum of either *0* or
    variable *2 minus 5.50*. So, whenever variable *2* has a value greater than *5.5*,
    that value will be multiplied times the coefficient; otherwise, it will be zero.
    The method will accommodate multiple hinges for each variable and also interaction
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other interesting thing about MARS is automatic variable selection. This
    can be done with cross-validation, but the default is to build through a forward
    pass, much like forward step wise regression, then a backward pass to prune the
    model, which after the forward pass is likely to overfit the data. This backward
    pass prunes input features and removes hinges based on **Generalized Cross Validation**
    (**GCV**):'
  prefs: []
  type: TYPE_NORMAL
- en: '*GCV = RSS / (N * (1 - Effective Number of Parameters / N)²)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Effective Number of Parameters = Number of Input Features + Penalty * (Number
    of Input Features - 1) / 2*'
  prefs: []
  type: TYPE_NORMAL
- en: In the `earth` package, *Penalty = 2* for additive model and *3* for multiplicative
    model, that is one with interaction terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, there are quite a few parameters you can tune. I will demonstrate in
    the example an effective and simple way to implement the methodology. If you so
    desire, you can learn more about its flexibility in the excellent online *Notes
    on the earth package*, by *Stephen Milborrow*, available at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.milbo.org/doc/earth-notes.pdf](http://www.milbo.org/doc/earth-notes.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With that introduction out of the way, let''s get started. You can use the
    `MDA` package, but I learned on `earth`, so that is what I will present. The code
    is similar to the earlier examples, where we used `glm()`. However, it is important
    to specify how you want the model pruned and that it is a binomial response variable.
    Here, I specify a model selection of a five-fold cross validation (`pmethod =
    "cv"` and `nfold = 5`), repeated 3 times (`ncross = 3`), as an additive model
    only with no interactions (`degree = 1`) and only one hinge per input feature
    (`minspan = -1`). In the data I''ve been working with, both interaction terms
    and multiple hinges have led to overfitting.  Of course, your results may vary.
    The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We now examine the model summary. At first, it can seem a little confusing.
    Of course, we have the model formula and the logistic coefficients, including
    the `hinge` functions, followed by some commentary and numbers related to generalized
    R-squared, and so on.  What happens is that a MARS model is built on the data
    first as a standard linear regression where the response variable is internally
    coded to 0''s and 1''s.  After feature/variable pruning with final model creation,
    then it is translated to a GLM.  So, just ignore the R-squared values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The model gives us eight terms, including the Intercept and seven predictors.
    Two of the predictors have `hinge` functions--thickness and chromatin. If the
    thickness is greater than 3, the coefficient of 0.7019 is multiplied by that value;
    otherwise, it is 0\. For chromatin, if less than 3 then the coefficient is multiplied
    by the values; otherwise, it is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plots are available. The first one using the `plotmo()` function produces plots
    showing the model''s response when varying that predictor and holding the others
    constant. You can clearly see the hinge function at work for thickness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With `plotd()`, you can see density plots of the predicted probabilities by
    class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One can look at relative variable importance. Here we see the variable name,
    `nsubsets`, which is the number of model subsets that include the variable after
    the pruning pass, and the `gcv` and `rss` columns show the decrease in the respective
    value that the variable contributes (`gcv` and `rss` are scaled 0 to 100):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how well it did on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This is very comparable to our logistic regression models. We can now compare
    the models and see what our best choice would be.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What are we to conclude from all this work? We have the confusion matrices
    and error rates from our models to guide us, but we can get a little more sophisticated
    when it comes to selecting the classification models. An effective tool for a
    classification model comparison is the **Receiver Operating Characteristic** (**ROC**)
    chart. Very simply, ROC is a technique for visualizing, organizing, and selecting
    classifiers based on their performance (Fawcett, 2006). On the ROC chart, the
    y-axis is the **True Positive Rate** (**TPR**) and the x-axis is the **False Positive
    Rate** (**FPR**). The following are the calculations, which are quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TPR = Positives correctly classified / total positives*'
  prefs: []
  type: TYPE_NORMAL
- en: '*FPR = Negatives incorrectly classified / total negatives*'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the ROC results will generate a curve, and thus you are able to produce
    the **Area Under the Curve** (**AUC**). The AUC provides you with an effective
    indicator of performance, and it can be shown that the AUC is equal to the probability
    that the observer will correctly identify the positive case when presented with
    a randomly chosen pair of cases in which one case is positive and one case is
    negative (Hanley JA & McNeil BJ, 1982). In our case, we will just switch the observer
    with our algorithms and evaluate accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an ROC chart in R, you can use the `ROCR` package. I think this is
    a great package and allows you to build a chart in just three lines of code. The
    package also has an excellent companion website (with examples and a presentation)
    that can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://rocr.bioinf.mpi-sb.mpg.de/.](http://rocr.bioinf.mpi-sb.mpg.de/.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'What I want to show are three different plots on our ROC chart: the full model,
    the reduced model using BIC to select the features, the MARS model, and a bad
    model. This so-called bad model will include just one predictive feature and will
    provide an effective contrast to our other models. Therefore, let''s load the
    `ROCR` package, build this poorly performing model, and call it `bad.fit` for
    simplicity, using the `thick` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now possible to build the ROC chart with three lines of code per model
    using the `test` dataset. We will first create an object that saves the predicted
    probabilities with the actual classification. Next, we will use this object to
    create another object with the calculated TPR and FPR. Then, we will build the
    chart with the `plot()` function. Let''s get started with the model using all
    of the features or, as I call it, the full model. This was the initial one that
    we built back in the *Logistic regression model* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the performance object with the TPR and FPR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `plot` command with the title of `ROC` and `col=1` will color
    the line black:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As stated previously, the curve represents TPR on the y-axis and FPR on the
    x-axis. If you have the perfect classifier with no false positives, then the line
    will run vertically at *0.0* on the x-axis. If a model is no better than chance,
    then the line will run diagonally from the lower left corner to the upper right
    one. As a reminder, the full model missed out on five labels: three false positives
    and two false negatives. We can now add the other models for comparison using
    a similar code, starting with the model built using BIC (refer to the *Logistic
    regression with cross-validation* section of this chapter), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `add=TRUE` parameter in the `plot` command added the line to the existing
    chart. Finally, we will add the poorly performing model, the MARS model, and include
    a `legend` chart, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the `FULL` model, `BIC` model and the `MARS` model are nearly
    superimposed. It is also quite clear that the `BAD` model performed as poorly
    as was expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final thing that we can do here is compute the AUC. This is again done
    in the `ROCR` package with the creation of a `performance` object, except that
    you have to substitute `auc` for `tpr` and `fpr`. The code and output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The highest AUC is for the full model at `0.997`. We also see `99.4` percent
    for the BIC model, `89.6` percent for the bad model and `99.5` for MARS. So, to
    all intents and purposes, with the exception of the bad model we have no difference
    in predictive powers between them. What are we to do? A simple solution would
    be to re-randomize the `train` and `test` sets and try this analysis again, perhaps
    using a 60/40 split and a different randomization seed. But if we end up with
    a similar result, then what? I think a statistical purist would recommend selecting
    the most parsimonious model, while others may be more inclined to include all
    the variables. It comes down to trade-offs, that is, model accuracy versus interpretability,
    simplicity, and scalability. In this instance, it seems safe to default to the
    simpler model, which has the same accuracy. It goes without saying that we won't
    always get this level of predictability with just GLMs or discriminant analysis.
    We will tackle these problems in upcoming chapters with more complex techniques
    and hopefully improve our predictive ability. The beauty of machine learning is
    that there are several ways to skin the proverbial cat.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at using probabilistic linear models to predict
    a qualitative response with three methods: logistic regression, discriminant analysis,
    and MARS. Additionally, we began the process of using ROC charts in order to explore
    model selection visually and statistically. We also briefly discussed the model
    selection and trade-offs that you need to consider. In future chapters, we will
    revisit the breast cancer dataset to see how more complex techniques perform.'
  prefs: []
  type: TYPE_NORMAL
