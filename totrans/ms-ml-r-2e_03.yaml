- en: Logistic Regression and Discriminant Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归和判别分析
- en: '"The true logic of this world is the calculus of probabilities."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"这个世界的真正逻辑是概率论。"'
- en: '- James Clerk Maxwell, Scottish physicist'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 詹姆斯·克拉克·麦克斯韦，苏格兰物理学家'
- en: In the previous chapter, we took a look at using **Ordinary Least Squares**
    (**OLS**) to predict a quantitative outcome, or in other words, linear regression.
    It is now time to shift gears somewhat and examine how we can develop algorithms
    to predict qualitative outcomes. Such outcome variables could be binary (male
    versus female, purchase versus does not purchase, tumor is benign versus malignant)
    or multinomial categories (education level or eye color). Regardless of whether
    the outcome of interest is binary or multinomial, the task of the analyst is to
    predict the probability of an observation belonging to a particular category of
    the outcome variable. In other words, we develop an algorithm in order to classify
    the observations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了使用**普通最小二乘法**（**OLS**）来预测定量结果，换句话说，就是线性回归。现在是时候转换一下方向，看看我们如何开发算法来预测定性结果。这样的结果变量可以是二元的（男性与女性，购买与未购买，良性肿瘤与恶性肿瘤）或多项式分类（教育水平或眼睛颜色）。无论感兴趣的结果是二元还是多项式，分析师的任务是预测观察值属于结果变量特定类别的概率。换句话说，我们开发算法是为了对观察值进行分类。
- en: To begin exploring classification problems, we will discuss why applying the
    OLS linear regression is not the correct technique and how the algorithms introduced
    in this chapter can solve these issues. We will then look at a problem of predicting
    whether or not a biopsied tumor mass is classified as benign or malignant. The
    dataset is the well-known and widely available **Wisconsin Breast Cancer Data**.
    To tackle this problem, we will begin by building and interpreting logistic regression
    models. We will also begin examining methods so as to select features and the
    most appropriate model. Next, we will discuss both linear and quadratic discriminant
    analyses and compare and contrast these with logistic regression. Then, building
    predictive models on the breast cancer data will follow. Finally, we will wrap
    it up by looking at multivariate regression splines and ways to select the best
    overall algorithm in order to address the question at hand. These methods (creating `test`/`train`
    datasets and cross-validation) will set the stage for more advanced machine learning
    methods in subsequent chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始探索分类问题，我们将讨论为什么应用OLS线性回归不是正确的技术，以及本章介绍的计算算法如何解决这些问题。然后，我们将研究一个预测活检肿瘤是否为良性或恶性的问题。数据集是众所周知的、广泛可用的**威斯康星乳腺癌数据**。为了解决这个问题，我们将首先构建和解释逻辑回归模型。我们还将开始研究选择特征和最合适的模型的方法。接下来，我们将讨论线性判别分析和二次判别分析，并将它们与逻辑回归进行比较和对比。然后，我们将基于乳腺癌数据构建预测模型。最后，我们将通过查看多元回归样条和选择最佳整体算法的方法来总结，以解决手头的问题。这些方法（创建`测试`/`训练`数据集和交叉验证）将为后续章节中更高级的机器学习方法奠定基础。
- en: Classification methods and linear regression
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类方法和线性回归
- en: 'So, why can''t we just use the least square regression method that we learned
    in the previous chapter for a qualitative outcome? Well, as it turns out, you
    can, but at your own risk. Let''s assume for a second that you have an outcome
    that you are trying to predict and it has three different classes: mild, moderate,
    and severe. You and your colleagues also assume that the difference between mild
    and moderate and moderate and severe is an equivalent measure and a linear relationship.
    You can create a dummy variable where 0 is equal to mild, 1 is equal to moderate,
    and 2 is equal to severe. If you have reason to believe this, then linear regression
    might be an acceptable solution. However, qualitative assessments such as the
    previous ones might lend themselves to a high level of measurement error that
    can bias the OLS. In most business problems, there is no scientifically acceptable
    way to convert a qualitative response to one that is quantitative. What if you
    have a response with two outcomes, say fail and pass? Again, using the dummy variable
    approach, we could code the fail outcome as `0` and the pass outcome as `1`. Using
    linear regression, we could build a model where the predicted value is the probability
    of an observation of pass or fail. However, the estimates of `Y` in the model
    will most likely exceed the probability constraints of `[0,1]` and thus be a bit
    difficult to interpret.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不能直接使用我们在上一章中学到的最小二乘回归方法来处理一个定性结果呢？嗯，实际上你可以这样做，但风险自负。让我们假设一下，你有一个你试图预测的结果，它有三个不同的类别：轻微、中度和严重。你和你的同事也假设轻微和中度以及中度和严重之间的差异是一个等效的度量，并且是线性关系。你可以创建一个虚拟变量，其中0代表轻微，1代表中度，2代表严重。如果你有理由相信这一点，那么线性回归可能是一个可接受的解决方案。然而，像之前那样的定性评估可能会带来高水平的测量误差，这可能会偏误OLS。在大多数商业问题中，没有科学上可接受的方法将定性响应转换为定量响应。如果你有一个有两种结果（比如失败和通过）的响应呢？再次，使用虚拟变量方法，我们可以将失败结果编码为`0`，通过结果编码为`1`。使用线性回归，我们可以构建一个模型，其中预测值是观察通过或失败的概率。然而，模型中`Y`的估计很可能会超过`[0,1]`的概率约束，因此解释起来可能有些困难。
- en: Logistic regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'As previously discussed, our classification problem is best modeled with the
    probabilities that are bound by `0` and `1`. We can do this for all of our observations
    with a number of different functions, but here we will focus on the logistic function.
    The logistic function used in logistic regression is as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的分类问题最好用受限于`0`和`1`的概率来建模。我们可以用许多不同的函数来做所有观察，但在这里我们将专注于逻辑函数。逻辑回归中使用的逻辑函数如下：
- en: '![](img/image_03_01a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_01a.png)'
- en: If you have ever placed a friendly wager on horse races or the World Cup, you
    may understand the concept better as odds. The logistic function can be turned
    to odds with the formulation of *Probability (Y) / 1 - Probability (Y)*. For instance,
    if the probability of Brazil winning the World Cup is 20 percent, then the odds
    are *0.2 / 1 - 0.2*, which is equal to *0.25*, translating to odds of one in four.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经在对赛马或世界杯的友好赌注中下注，你可能更了解赔率的概念。逻辑函数可以通过以下公式转换为赔率：*概率（Y）/ 1 - 概率（Y）*。例如，如果巴西赢得世界杯的概率是20%，那么赔率是*0.2
    / 1 - 0.2*，等于*0.25*，这相当于四分之一赔率。
- en: To translate the odds back to probability, take the odds and divide by one plus
    the odds. The World Cup example is thus *0.25 / 1 + 0.25*, which is equal to 20
    percent. Additionally, let's consider the odds ratio. Assume that the odds of
    Germany winning the Cup are *0.18*. We can compare the odds of Brazil and Germany
    with the odds ratio. In this example, the odds ratio would be the odds of Brazil
    divided by the odds of Germany. We will end up with an odds ratio equal to *0.25/0.18*,
    which is equal to *1.39*. Here, we will say that Brazil is *1.39* times more likely
    than Germany to win the World Cup.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要将赔率转换回概率，将赔率除以一个加上的赔率。因此，世界杯的例子是*0.25 / 1 + 0.25*，等于20%。此外，让我们考虑一下赔率比。假设德国赢得奖杯的赔率是*0.18*。我们可以用赔率比来比较巴西和德国的赔率。在这个例子中，赔率比将是巴西赔率除以德国赔率。我们最终会得到一个赔率比等于*0.25/0.18*，等于*1.39*。在这里，我们将说巴西赢得世界杯的可能性比德国高*1.39*倍。
- en: One way to look at the relationship of logistic regression with linear regression
    is to show logistic regression as the log odds or *log (P(Y)/1 - P(Y))* is equal
    to *Bo + B1x*. The coefficients are estimated using a maximum likelihood instead
    of the OLS. The intuition behind the maximum likelihood is that we are calculating
    the estimates for *Bo* and *B1,* which will create a predicted probability for
    an observation that is as close as possible to the actual observed outcome of
    *Y*, a so-called likelihood. The R language does what other software packages
    do for the maximum likelihood, which is to find the optimal combination of beta
    values that maximize the likelihood.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待逻辑回归与线性回归之间关系的方法是将逻辑回归表示为对数几率或 *log (P(Y)/1 - P(Y))* 等于 *Bo + B1x*。系数是通过最大似然估计而不是
    OLS 来估计的。最大似然背后的直觉是我们正在计算 *Bo* 和 *B1* 的估计值，这将创建一个预测概率，该概率与实际观察到的 *Y* 的结果尽可能接近，这被称为似然。R
    语言所做的与其它软件包对最大似然所做的相同，即找到最大化似然的 beta 值的最佳组合。
- en: With these facts in mind, logistic regression is a very powerful technique to
    predict the problems involving classification and is often the starting point
    for model creation in such problems. Therefore, in this chapter, we will attack
    the upcoming business problem with logistic regression first and foremost.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些事实，逻辑回归是一种非常强大的技术，可以预测涉及分类的问题，并且通常是此类问题建模的起点。因此，在本章中，我们将首先使用逻辑回归来处理即将到来的业务问题。
- en: Business understanding
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务理解
- en: Dr. William H. Wolberg from the University of Wisconsin commissioned the Wisconsin
    Breast Cancer Data in 1990\. His goal behind collecting the data was to identify
    whether a tumor biopsy was malignant or benign. His team collected the samples
    using **Fine Needle Aspiration** (**FNA**). If a physician identifies the tumor
    through examination or imaging an area of abnormal tissue, then the next step
    is to collect a biopsy. FNA is a relatively safe method of collecting the tissue,
    and complications are rare. Pathologists examine the biopsy and attempt to determine
    the diagnosis (malignant or benign). As you can imagine, this is not a trivial
    conclusion. Benign breast tumors are not dangerous as there is no risk of the
    abnormal growth spreading to other body parts. If a benign tumor is large enough,
    surgery might be needed to remove it. On the other hand, a malignant tumor requires
    medical intervention. The level of treatment depends on a number of factors, but
    it's most likely that surgery will be required, which can be followed by radiation
    and/or chemotherapy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学的威廉·H·沃尔伯格博士于 1990 年委托收集了威斯康星乳腺癌数据。他收集数据背后的目标是确定肿瘤活检是否为恶性。他的团队使用 **细针穿刺活检**
    (**FNA**) 收集样本。如果医生通过检查或影像学检查发现异常组织区域，则下一步是收集活检。FNA 是一种相对安全的组织收集方法，并发症很少。病理学家检查活检并试图确定诊断（恶性或良性）。正如你可以想象的那样，这不是一个微不足道的结论。良性乳腺癌并不危险，因为没有风险使异常生长扩散到身体的其他部位。如果良性肿瘤足够大，可能需要进行手术来移除它。另一方面，恶性肿瘤需要医疗干预。治疗水平取决于许多因素，但最可能的是需要手术，随后可能是放疗和/或化疗。
- en: Therefore, the implications of a misdiagnosis can be extensive. A false positive
    for malignancy can lead to costly and unnecessary treatment, subjecting the patient
    to a tremendous emotional and physical burden. On the other hand, a false negative
    can deny a patient the treatment that they need, causing the cancer cells to spread
    and leading to premature death. Early treatment intervention in breast cancer
    patients can greatly improve their survival.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，误诊的后果可能是广泛的。对恶性的误诊可能导致昂贵且不必要的治疗，使患者承受巨大的情感和身体负担。另一方面，误诊可能导致患者得不到他们需要的治疗，使癌细胞扩散，导致过早死亡。对乳腺癌患者的早期治疗干预可以大大提高他们的生存率。
- en: Our task then is to develop the best possible diagnostic machine learning algorithm
    in order to assist the patient's medical team in determining whether the tumor
    is malignant or not.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的任务就是开发出最好的诊断机器学习算法，以便协助患者的医疗团队确定肿瘤是否为恶性。
- en: Data understanding and preparation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'This dataset consists of tissue samples from 699 patients. It is in a data
    frame with 11 variables, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含来自 699 名患者的组织样本。它在一个包含 11 个变量的数据框中，如下所示：
- en: '`ID`: Sample code number'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ID`: 样本代码号'
- en: '`V1`: Thickness'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V1`: 厚度'
- en: '`V2`: Uniformity of the cell size'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V2`: 细胞大小的均匀性'
- en: '`V3`: Uniformity of the cell shape'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V3`: 细胞形状的均匀性'
- en: '`V4`: Marginal adhesion'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V4`: 边缘粘附'
- en: '`V5`: Single epithelial cell size'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V5`: 单个上皮细胞大小'
- en: '`V6`: Bare nucleus (16 observations are missing)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V6`: 裸核（16个观测值缺失）'
- en: '`V7`: Bland chromatin'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V7`: 平滑染色质'
- en: '`V8`: Normal nucleolus'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V8`: 正常核仁'
- en: '`V9`: Mitosis'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V9`: 有丝分裂'
- en: '`class`: Whether the tumor diagnosis is benign or malignant; this will be the
    outcome that we are trying to predict'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class`: 肿瘤诊断是否为良性或恶性；这是我们试图预测的结果'
- en: The medical team has scored and coded each of the nine features on a scale of
    `1` to `10`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗团队已经对九个特征在每个`1`到`10`的量表上进行了评分和编码。
- en: 'The data frame is available in the R `MASS` package under the `biopsy` name.
    To prepare this data, we will load the data frame, confirm the structure, rename
    the variables to something meaningful, and delete the missing observations. At
    this point, we can begin to explore the data visually. Here is the code that will
    get us started when we first load the library and then the dataset; using the
    `str()` function, we will examine the underlying structure of the data:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框在R的`MASS`包下以`biopsy`名称可用。为了准备这些数据，我们将加载数据框，确认结构，将变量重命名为有意义的名称，并删除缺失的观测值。到这一点，我们可以开始从视觉上探索数据。以下是当我们首次加载库和数据集时将开始的代码；使用`str()`函数，我们将检查数据的底层结构：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: An examination of the data structure shows that our features are integers and
    the outcome is a factor. No transformation of the data to a different structure
    is needed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 检查数据结构显示，我们的特征是整数，结果是一个因子。不需要将数据转换为不同的结构。
- en: 'We can now get rid of the `ID` column, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以删除`ID`列，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will rename the variables and confirm that the code has worked as
    intended:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重命名变量并确认代码是否按预期工作：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we will delete the missing observations. As there are only 16 observations
    with the missing data, it is safe to get rid of them as they account for only
    2 percent of all the observations. A thorough discussion of how to handle the
    missing data is outside the scope of this chapter and has been included in the
    [Appendix A](5d62fd21-3280-4845-a21e-4700d91225d1.xhtml), *R Fundamentals*, where
    I cover data manipulation. In deleting these observations, a new working data
    frame is created. One line of code does this trick with the `na.omit` function,
    which deletes all the missing observations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将删除缺失的观测值。由于只有16个观测值带有缺失数据，它们只占所有观测值的2%，因此可以安全地删除它们。如何处理缺失数据的详细讨论超出了本章的范围，已在[附录A](5d62fd21-3280-4845-a21e-4700d91225d1.xhtml)，“R基础”，中包含，我在那里介绍了数据处理。在删除这些观测值时，将创建一个新的工作数据框。一行代码通过`na.omit`函数完成这个技巧，该函数删除所有缺失的观测值：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Depending on the package in R that you are using to analyze the data, the outcome
    needs to be numeric, which is `0` or `1`. In order to accommodate that requirement,
    create the variable `y`, where benign is zero and malignant 1, using the `ifelse()`
    function as shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的R包来分析数据，结果需要是数值型，即`0`或`1`。为了满足这一要求，创建变量`y`，其中良性为`0`，恶性为`1`，如以下所示使用`ifelse()`函数：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are a number of ways in which we can understand the data visually in
    a classification problem, and I think a lot of it comes down to personal preference.
    One of the things that I like to do in these situations is examine the **boxplots**
    of the features that are split by the classification outcome. This is an excellent
    way to begin understanding which features may be important to the algorithm. Boxplots
    are a simple way to understand the distribution of the data at a glance. In my
    experience, it also provides you with an effective way to build the presentation
    story that you will deliver to your customers. There are a number of ways to do
    this quickly, and the `lattice` and `ggplot2` packages are quite good at this
    task. I will use `ggplot2` in this case with the additional package, `reshape2`.
    After loading the packages, you will need to create a data frame using the `melt()`
    function. The reason to do this is that melting the features will allow the creation
    of a matrix of boxplots, allowing us to easily conduct the following visual inspection:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，我们可以通过多种方式直观地理解数据，我认为这很大程度上取决于个人喜好。在这些情况下，我喜欢做的事情之一是检查由分类结果分割的特征的**箱线图**。这是一种理解哪些特征可能对算法很重要的极好方式。箱线图是一种简单的方法，可以一眼看出数据的分布。根据我的经验，这还为你提供了一个有效的方式来构建你将向客户展示的演示故事。快速完成这项任务有多种方法，`lattice`和`ggplot2`包在这方面做得相当不错。在这种情况下，我将使用`ggplot2`，并附加`reshape2`包。在加载这些包之后，你需要使用`melt()`函数创建一个数据框。这样做的原因是，熔化特征将允许创建一个箱线图矩阵，使我们能够轻松地进行以下视觉检查：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code melts the data by their values into one overall feature
    and groups them by class:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码通过它们的值将数据熔化成一个整体特征，并按类别分组：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Through the magic of `ggplot2`, we can create a 3x3 boxplot matrix, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`ggplot2`的魔力，我们可以创建一个3x3的箱线图矩阵，如下所示：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the output of the preceding code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码的输出如下：
- en: '![](img/image_03_01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_01.png)'
- en: How do we interpret a boxplot? First of all, in the preceding screenshot, the
    thick white boxes constitute the upper and lower quartiles of the data; in other
    words, half of all the observations fall in the thick white box area. The dark
    line cutting across the box is the median value. The lines extending from the
    boxes are also quartiles, terminating at the maximum and minimum values, outliers
    notwithstanding. The black dots constitute the outliers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释箱线图？首先，在前面的截图上，粗白框构成了数据的上四分位数和下四分位数；换句话说，所有观察值的一半都落在粗白框区域内。穿过框的深色线是中位数。从框延伸出来的线也是四分位数，终止于最大值和最小值，不考虑异常值。黑色点构成了异常值。
- en: By inspecting the plots and applying some judgment, it is difficult to determine
    which features will be important in our classification algorithm. However, I think
    it is safe to assume that the nuclei feature will be important, given the separation
    of the median values and corresponding distributions. Conversely, there appears
    to be little separation of the mitosis feature by class, and it will likely be
    an irrelevant feature. We shall see!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查图表并应用一些判断，很难确定哪些特征将在我们的分类算法中很重要。然而，鉴于中位数值和相应分布的分离，我认为可以安全地假设核特征将很重要。相反，似乎有很少的类别的有丝分裂特征分离，它可能是一个无关紧要的特征。我们将拭目以待！
- en: 'With all of our features quantitative, we can also do a correlation analysis
    as we did with linear regression. Collinearity with logistic regression can bias
    our estimates just as we discussed with linear regression. Let''s load the `corrplot`
    package and examine the correlations as we did in the previous chapter, this time
    using a different type of correlation matrix, which has both shaded ovals and
    the correlation coefficients in the same plot, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有特征都是定量数据，我们也可以像在线性回归中做的那样进行相关性分析。与逻辑回归的共线性可能会像我们在线性回归中讨论的那样，对我们的估计产生偏差。让我们加载`corrplot`包，并像上一章那样检查相关性，这次使用不同类型的相关性矩阵，该矩阵在同一图表中既有阴影椭圆又有相关性系数，如下所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the output of the preceding code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码的输出如下：
- en: '![](img/image_03_02.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_02.png)'
- en: The correlation coefficients are indicating that we may have a problem with
    collinearity, in particular, the features of uniform shape and uniform size that
    are present. As part of the logistic regression modeling process, it will be necessary
    to incorporate the VIF analysis as we did with linear regression. The final task
    in the data preparation will be the creation of our `train` and `test` datasets.
    The purpose of creating two different datasets from the original one is to improve
    our ability so as to accurately predict the previously unused or unseen data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数表明我们可能存在共线性问题，特别是那些具有统一形状和统一大小的特征。作为逻辑回归建模过程的一部分，我们需要像线性回归那样进行VIF分析。数据准备的最后任务是创建我们的`train`和`test`数据集。从原始数据集中创建两个不同的数据集的目的是为了提高我们准确预测先前未使用或未见过的数据的能力。
- en: In essence, in machine learning, we should not be so concerned with how well
    we can predict the current observations and should be more focused on how well
    we can predict the observations that were not used in order to create the algorithm.
    So, we can create and select the best algorithm using the training data that maximizes
    our predictions on the `test` set. The models that we will build in this chapter
    will be evaluated by this criterion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，在机器学习中，我们不应该过于关注我们如何预测当前的观测值，而应该更多地关注我们如何预测那些未被用于创建算法的观测值。因此，我们可以使用训练数据创建和选择最佳算法，以最大化我们在`test`集上的预测。我们将在本章中构建的模型将根据这一标准进行评估。
- en: 'There are a number of ways to proportionally split our data into `train` and
    `test` sets: 50/50, 60/40, 70/30, 80/20, and so forth. The data split that you
    select should be based on your experience and judgment. For this exercise, I will
    use a 70/30 split, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以将我们的数据按比例分成`train`和`test`集：50/50、60/40、70/30、80/20等等。你选择的数据分割应该基于你的经验和判断。对于这个练习，我将使用70/30的分割，如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To ensure that we have a well-balanced outcome variable between the two datasets,
    we will perform the following check:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们在两个数据集之间有一个平衡的因变量，我们将执行以下检查：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is an acceptable ratio of our outcomes in the two datasets; with this,
    we can begin the modeling and evaluation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们两个数据集结果的合理比率；有了这个比率，我们可以开始建模和评估。
- en: Modeling and evaluation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: For this part of the process, we will start with a logistic regression model
    of all the input variables and then narrow down the features with the best subsets.
    After this, we will try our hand at **discriminant analysis** and **Multivariate
    Adaptive Regression Splines** (**MARS**).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程的部分，我们将从一个包含所有输入变量的逻辑回归模型开始，然后缩小到最佳子集的特征。之后，我们将尝试**判别分析**和**多元自适应回归样条**（**MARS**）。
- en: The logistic regression model
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: 'We''ve already discussed the theory behind logistic regression, so we can begin
    fitting our models. An R installation comes with the `glm()` function fitting the
    generalized linear models, which are a class of models that includes logistic
    regression. The code syntax is similar to the `lm()` function that we used in
    the previous chapter. One big difference is that we must use the `family = binomial`
    argument in the function, which tells R to run a logistic regression method instead
    of the other versions of the generalized linear models. We will start by creating
    a model that includes all of the features on the `train` set and see how it performs
    on the `test` set, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了逻辑回归背后的理论，因此我们可以开始拟合我们的模型。R安装包自带了`glm()`函数，用于拟合广义线性模型，其中包括逻辑回归。代码语法与我们之前章节中使用的`lm()`函数类似。一个很大的不同之处在于，我们必须在函数中使用`family
    = binomial`参数，这告诉R运行逻辑回归方法而不是其他广义线性模型的版本。我们将首先创建一个包含`train`集上所有特征的模型，并查看它在`test`集上的表现，如下所示：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `summary()` function allows us to inspect the coefficients and their p-values.
    We can see that only two features have p-values less than `0.05` (thickness and
    nuclei). An examination of the 95 percent confidence intervals can be called on
    with the `confint()` function, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary()`函数允许我们检查系数及其p值。我们可以看到只有两个特征的p值小于`0.05`（厚度和核）。我们可以使用`confint()`函数来检查95%置信区间，如下所示：'
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that the two significant features have confidence intervals that do not
    cross zero. You cannot translate the coefficients in logistic regression as the
    change in *Y* is based on a one-unit change in *X*. This is where the odds ratio
    can be quite helpful. The beta coefficients from the log function can be converted
    to odds ratios with an exponent (beta).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，两个显著特征的有信心区间没有穿过零。你不能像在逻辑回归中翻译系数那样翻译系数，因为*Y*的变化是基于*X*的一个单位变化。这就是为什么优势比可以非常有帮助。对数函数的beta系数可以通过指数（beta）转换为优势比。
- en: 'In order to produce the odds ratios in R, we will use the following `exp(coef())`
    syntax:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在R中生成优势比，我们将使用以下`exp(coef())`语法：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The interpretation of an odds ratio is the change in the outcome odds resulting
    from a unit change in the feature. If the value is greater than 1, it indicates
    that, as the feature increases, the odds of the outcome increase. Conversely,
    a value less than 1 would mean that, as the feature increases, the odds of the
    outcome decrease. In this example, all the features except `u.size` will increase
    the log odds.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 优势比的解释是特征单位变化导致的结果优势的变化。如果值大于1，这表明随着特征的增加，结果的优势增加。相反，一个小于1的值意味着随着特征的增加，结果的优势减少。在这个例子中，除了`u.size`之外的所有特征都会增加对数优势。
- en: 'One of the issues pointed out during data exploration was the potential issue
    of multi-collinearity. It is possible to produce the VIF statistics that we did
    in linear regression with a logistic model in the following way:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据探索期间指出的问题之一是多共线性的潜在问题。我们可以用以下方式在逻辑模型中生成我们在线性回归中生成的VIF统计量：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: None of the values are greater than the VIF rule of thumb statistic of five,
    so collinearity does not seem to be a problem. Feature selection will be the next
    task; but, for now, let's produce some code to look at how well this model does
    on both the `train` and `test` sets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何一个值大于VIF经验法则的五个标准，所以共线性似乎不是问题。特征选择将是下一个任务；但，现在，让我们编写一些代码来查看这个模型在`train`和`test`集上的表现如何。
- en: 'You will first have to create a vector of the predicted probabilities, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要创建一个预测概率的向量，如下所示：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we need to evaluate how well the model performed in training and then
    evaluate how it fits on the test set. A quick way to do this is to produce a confusion
    matrix. In later chapters, we will examine the version provided by the `caret`
    package. There is also a version provided in the `InformationValue` package. This
    is where we will need the outcome as 0''s and 1''s. The default value by which
    the function selects either benign or malignant is 0.50, which is to say that
    any probability at or above 0.50 is classified as malignant:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要评估模型在训练中的表现，然后评估它在测试集中的拟合度。快速做到这一点的一种方法就是生成一个混淆矩阵。在后面的章节中，我们将检查`caret`包提供的版本。`InformationValue`包也提供了一个版本。这里我们需要结果为0和1。函数选择良性或恶性的默认值是0.50，也就是说，任何概率在0.50或以上的都被分类为恶性：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The rows signify the predictions, and the columns signify the actual values.
    The diagonal elements are the correct classifications. The top right value, `7`,
    is the number of false negatives, and the bottom left value, `8`, is the number
    of false positives. We can also take a look at the error rate, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 行表示预测，列表示实际值。对角线元素是正确的分类。右上角的值`7`是假阴性的数量，左下角的值`8`是假阳性的数量。我们还可以查看错误率，如下所示：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It seems we have done a fairly good job with only a *3.16%* error rate on the
    training set. As we previously discussed, we must be able to accurately predict
    unseen data, in other words, our `test` set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们似乎在训练集上做得相当不错，只有*3.16%*的错误率。正如我们之前讨论的，我们必须能够准确预测未见数据，换句话说，我们的`test`集。
- en: 'The method to create a confusion matrix for the `test` set is similar to how
    we did it for the training data:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为`test`集创建混淆矩阵的方法与我们为训练数据所做的方法类似：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It looks like we have done pretty well in creating a model with all the features.
    The roughly 98 percent prediction rate is quite impressive. However, we must still
    check whether there is room for improvement. Imagine that you or your loved one
    is a patient who has been diagnosed incorrectly. As previously mentioned, the
    implications can be quite dramatic. With this in mind, is there perhaps a better
    way to create a classification algorithm? Let's have a look!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们在创建一个具有所有功能的模型方面做得相当不错。大约98%的预测准确率相当令人印象深刻。然而，我们仍然需要检查是否还有改进的空间。想象一下，如果你或你的亲人被错误地诊断了。正如之前提到的，其影响可能相当严重。考虑到这一点，也许有更好的方法来创建一个分类算法？让我们看看吧！
- en: Logistic regression with cross-validation
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有交叉验证的逻辑回归
- en: The purpose of cross-validation is to improve our prediction of the `test` set
    and minimize the chance of overfitting. With the **K-fold cross-validation**,
    the dataset is split into K equal-sized parts. The algorithm learns by alternatively
    holding out one of the **K-sets**; it fits a model to the other **K-1 parts**,
    and obtains predictions for the left-out K-set. The results are then averaged
    so as to minimize the errors, and appropriate features are selected. You can also
    perform the **Leave-One-Out-Cross-Validation** (**LOOCV**) method, where K is
    equal to 1\. Simulations have shown that the LOOCV method can have averaged estimates
    that have a high variance. As a result, most machine learning experts will recommend
    that the number of K-folds should be 5 or 10.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的目的是提高我们对`测试`集的预测，并最小化过拟合的机会。在**K折交叉验证**中，数据集被分成K个大小相等的部分。算法通过交替地保留一个**K-集**来学习；它将模型拟合到其他**K-1部分**，并为遗漏的K集获得预测。然后平均这些结果以最小化误差，并选择适当的特征。你也可以执行**留一法交叉验证（LOOCV**）方法，其中K等于1。模拟表明，LOOCV方法可以有平均估计值，其方差较高。因此，大多数机器学习专家会建议K折数应该是5或10。
- en: 'An R package that will automatically do `CV` for logistic regression is the
    `bestglm` package. This package is dependent on the `leaps` package that we used
    for linear regression. The syntax and formatting of the data require some care,
    so let''s walk through this in detail:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自动为逻辑回归执行`CV`的R包是`bestglm`包。此包依赖于我们用于线性回归的`leaps`包。数据的语法和格式需要一些注意，让我们详细说明这一点：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After loading the package, we will need our outcome coded to `0` or `1`. If
    left as a factor, it will not work. The other requirement to utilize the package
    is that your outcome, or `y`, is the last column and all the extraneous columns
    have been removed. A new data frame will do the trick for us, via the following
    code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载包之后，我们需要将我们的结果编码为`0`或`1`。如果保留为因子，则不会工作。要使用该包的另一个要求是，你的结果或`y`是最后一列，并且所有无关的列都已删除。以下代码将为我们创建一个新的数据框：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the code to run in order to use the `CV` technique with our data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是运行代码以使用我们的数据中的`CV`技术：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The syntax, `Xy = Xy`, points to our properly formatted data frame. `IC = "CV"`
    tells the package that the information criterion to use is cross-validation. `CVArgs`
    are the `CV` arguments that we want to use. The `HTF` method is K-fold, which
    is followed by the number of folds, `K = 10`, and we are asking it to do only
    one iteration of the random folds with `REP = 1`. Just as with `glm()`, we will
    need to use `family = binomial`. On a side note, you can use `bestglm` for linear
    regression as well by specifying `family = gaussian`. So, after running the analysis,
    we will end up with the following output, giving us three features for `Best Model`,
    such as `thick`, `u.size`, and `nucl`. The statement on `Morgan-Tatar search`
    simply means that a simple exhaustive search was done for all the possible subsets,
    as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 语法`Xy = Xy`指向我们正确格式化的数据框。`IC = "CV"`告诉包使用的信息标准是交叉验证。`CVArgs`是我们想要使用的`CV`参数。`HTF`方法是K折，后面跟着折数，`K
    = 10`，我们要求它只进行一次随机折叠的迭代，`REP = 1`。就像`glm()`一样，我们还需要使用`family = binomial`。顺便说一下，你也可以通过指定`family
    = gaussian`使用`bestglm`进行线性回归。因此，在运行分析后，我们将得到以下输出，给出`最佳模型`的三个特征，例如`thick`、`u.size`和`nucl`。关于`Morgan-Tatar搜索`的陈述仅仅意味着对所有可能的子集进行了简单的穷举搜索，如下所示：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can put these features in `glm()` and then see how well the model did on
    the `test` set. A `predict()` function will not work with `bestglm`, so this is
    a required step:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些特征放入`glm()`中，然后看看模型在`测试`集上的表现如何。`predict()`函数与`bestglm`不兼容，所以这是一个必要的步骤：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As before, the following code allows us to compare the predicted labels with
    the actual ones on the test set:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The reduced feature model is slightly less accurate than the full feature model,
    but all is not lost. We can utilize the `bestglm` package again, this time using
    the best subsets with the information criterion set to `BIC`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'These four features provide the minimum `BIC` score for all possible subsets.
    Let''s try this and see how it predicts the `test` set, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here we have five errors, just like the full model. The obvious question then
    is: which one is better? In any normal situation, the rule of thumb is to default
    to the simplest or the easiest-to-interpret model, given equality of generalization.
    We could run a completely new analysis with a new randomization and different
    ratios of the `train` and `test` sets among others. However, let''s assume for
    a moment that we''ve exhausted the limits of what logistic regression can do for
    us. We will come back to the full model and the model that we developed on a `BIC`
    minimum at the end and discuss the methods of model selection. Now, let''s move
    on to our discriminant analysis methods, which we will also include as possibilities
    in the final recommendation.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis overview
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Discriminant Analysis** (**DA**), also known as **Fisher Discriminant Analysis**
    (**FDA**), is another popular classification technique. It can be an effective
    alternative to logistic regression when the classes are well-separated. If you
    have a classification problem where the outcome classes are well-separated, logistic
    regression can have unstable estimates, which is to say that the confidence intervals
    are wide and the estimates themselves likely vary from one sample to another (James,
    2013). DA does not suffer from this problem and, as a result, may outperform and
    be more generalized than logistic regression. Conversely, if there are complex
    relationships between the features and outcome variables, it may perform poorly
    on a classification task. For our breast cancer example, logistic regression performed
    well on the testing and training sets, and the classes were not well-separated.
    For the purpose of comparison with logistic regression, we will explore DA, both
    **Linear Discriminant Analysis** (**LDA**) and **Quadratic Discriminant Analysis**
    (**QDA**).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: DA utilizes **Baye's theorem** in order to determine the probability of the
    class membership for each observation. If you have two classes, for example, benign
    and malignant, then DA will calculate an observation's probability for both the
    classes and select the highest probability as the proper class.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' theorem states that the probability of *Y* occurring--given that *X*
    has occurred--is equal to the probability of both *Y* and *X* occurring, divided
    by the probability of *X* occurring, which can be written as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_aa.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: The numerator in this expression is the likelihood that an observation is from
    that class level and has these feature values. The denominator is the likelihood
    of an observation that has these feature values across all the levels. Again,
    the classification rule says that if you have the joint distribution of *X* and
    *Y* and if *X* is given, the optimal decision about which class to assign an observation
    to is by choosing the class with the larger probability (the posterior probability).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of attaining posterior probabilities goes through the following
    steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Collect data with a known class membership.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the prior probabilities; this represents the proportion of the sample
    that belongs to each class.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the mean for each feature by their class.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the variance--covariance matrix for each feature; if it is an LDA,
    then this would be a pooled matrix of all the classes, giving us a linear classifier,
    and if it is a QDA, then a variance--covariance created for each class.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the normal distribution (Gaussian densities) for each class.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the `discriminant` function that is the rule for the classification
    of a new object.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign an observation to a class based on the `discriminant` function.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will provide an expanded notation on the determination of the posterior
    probabilities, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_bb.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Even though LDA is elegantly simple, it is limited by the assumption that the
    observations of each class are said to have a multivariate normal distribution,
    and there is a common covariance across the classes. QDA still assumes that observations
    come from a normal distribution, but it also assumes that each class has its own
    covariance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter? When you relax the common covariance assumption, you now
    allow quadratic terms into the discriminant score calculations, which was not
    possible with LDA. The mathematics behind this can be a bit intimidating and are outside
    the scope of this book. The important part to remember is that QDA is a more flexible
    technique than logistic regression, but we must keep in mind our **bias-variance**
    trade-off. With a more flexible technique, you are likely to have a lower bias
    but potentially a higher variance. Like a lot of flexible techniques, a robust
    set of training data is needed to mitigate a high classifier variance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis application
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is performed in the `MASS` package, which we have already loaded so that
    we can access the biopsy data. The syntax is very similar to the `lm()` and `glm()`
    functions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin fitting our LDA model, which is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This output shows us that `Prior probabilities of groups` are approximately
    64 percent for benign and 36 percent for malignancy. Next is `Group means`. This
    is the average of each feature by their class. `Coefficients of linear discriminants`
    are the standardized linear combination of the features that are used to determine
    an observation's discriminant score. The higher the score, the more likely that
    the classification is `malignant`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plot()` function in LDA will provide us with a histogram and/or the densities
    of the discriminant scores, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following is the output of the preceding command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_03.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: We can see that there is some overlap in the groups, indicating that there will
    be some incorrectly classified observations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` function available with LDA provides a list of three elements:
    class, posterior, and x. The class element is the prediction of benign or malignant,
    the posterior is the probability score of x being in each class, and x is the
    linear discriminant score. Let''s just extract the probability of an observation
    being malignant:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Well, unfortunately, it appears that our LDA model has performed much worse
    than the logistic regression models. The primary question is to see how this will
    perform on the `test` data:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: That's actually not as bad as I thought, given the lesser performance on the
    training data. From a correctly classified perspective, it still did not perform
    as well as logistic regression (96 percent versus almost 98 percent with logistic
    regression).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to fit a QDA model. In R, QDA is also part of the `MASS`
    package and the function is `qda()`. Building the model is rather straightforward
    again, and we will store it in an object called `qda.fit`, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As with LDA, the output has `Group means` but does not have the coefficients
    because it is a quadratic function as discussed previously.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'The predictions for the `train` and `test` data follow the same flow of code
    as with LDA:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can quickly tell that QDA has performed the worst on the training data with
    the confusion matrix, and it has classified the `test` set poorly with 11 incorrect
    predictions. In particular, it has a high rate of false positives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Adaptive Regression Splines (MARS)
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you like a modeling technique that provides all of the following?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Offers the flexibility to build linear and nonlinear models for both regression
    and classification
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can support variable interaction terms
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is simple to understand and explain
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires little data preprocessing
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handles all types of data: numeric, factors, and so on'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well on unseen data, that is, it does well in bias-variance trade-off
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that all sounds appealing, then I cannot recommend the use of MARS models
    enough. The method was brought to my attention several months ago, and I have
    found it to perform extremely well. In fact, in a recent case of mine, it outperformed
    both a random forest and boosted trees on test/validation data. It has quickly
    become my baseline model and all others are competitors. The other benefit I've
    seen is that it has negated much of the feature engineering I was doing. Much
    of that was using **Weight-of-Evidence** (**WOE**) and **Information Values** (**IV**)
    to capture nonlinearity and recode the variables.  This WOE/IV technique was something
    I was planning to write about at length in this second edition. However, I've
    done a number of tests, and MARS does an excellent job of doing what that technique
    does (that is, capture non-linearity), so I will not discuss WOE/IV at all.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand MARS is quite simple. First, just start with a linear or generalized
    linear model like we discussed previously. Then, to capture any nonlinear relationship,
    a `hinge` function is added. These `hinges` are simply points in the input feature
    that equate to a coefficient change. For example, say we have *Y = 12.5 (our intercept)
    + 1.5(variable 1) + 3.3(variable 2); where variables 1 and 2 are on a scale of
    1 to 10*. Now, let''s see how a `hinge` function for variable *2* could come into
    play:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = 11 (new intercept) + 1.5(variable 1) + 4.26734(max(0, variable 2 - 5.5)*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we read the `hinge` function as: we take the maximum of either *0* or
    variable *2 minus 5.50*. So, whenever variable *2* has a value greater than *5.5*,
    that value will be multiplied times the coefficient; otherwise, it will be zero.
    The method will accommodate multiple hinges for each variable and also interaction
    terms.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The other interesting thing about MARS is automatic variable selection. This
    can be done with cross-validation, but the default is to build through a forward
    pass, much like forward step wise regression, then a backward pass to prune the
    model, which after the forward pass is likely to overfit the data. This backward
    pass prunes input features and removes hinges based on **Generalized Cross Validation**
    (**GCV**):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '*GCV = RSS / (N * (1 - Effective Number of Parameters / N)²)*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*Effective Number of Parameters = Number of Input Features + Penalty * (Number
    of Input Features - 1) / 2*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In the `earth` package, *Penalty = 2* for additive model and *3* for multiplicative
    model, that is one with interaction terms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, there are quite a few parameters you can tune. I will demonstrate in
    the example an effective and simple way to implement the methodology. If you so
    desire, you can learn more about its flexibility in the excellent online *Notes
    on the earth package*, by *Stephen Milborrow*, available at this link:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.milbo.org/doc/earth-notes.pdf](http://www.milbo.org/doc/earth-notes.pdf)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'With that introduction out of the way, let''s get started. You can use the
    `MDA` package, but I learned on `earth`, so that is what I will present. The code
    is similar to the earlier examples, where we used `glm()`. However, it is important
    to specify how you want the model pruned and that it is a binomial response variable.
    Here, I specify a model selection of a five-fold cross validation (`pmethod =
    "cv"` and `nfold = 5`), repeated 3 times (`ncross = 3`), as an additive model
    only with no interactions (`degree = 1`) and only one hinge per input feature
    (`minspan = -1`). In the data I''ve been working with, both interaction terms
    and multiple hinges have led to overfitting.  Of course, your results may vary.
    The code is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We now examine the model summary. At first, it can seem a little confusing.
    Of course, we have the model formula and the logistic coefficients, including
    the `hinge` functions, followed by some commentary and numbers related to generalized
    R-squared, and so on.  What happens is that a MARS model is built on the data
    first as a standard linear regression where the response variable is internally
    coded to 0''s and 1''s.  After feature/variable pruning with final model creation,
    then it is translated to a GLM.  So, just ignore the R-squared values:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The model gives us eight terms, including the Intercept and seven predictors.
    Two of the predictors have `hinge` functions--thickness and chromatin. If the
    thickness is greater than 3, the coefficient of 0.7019 is multiplied by that value;
    otherwise, it is 0\. For chromatin, if less than 3 then the coefficient is multiplied
    by the values; otherwise, it is 0.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Plots are available. The first one using the `plotmo()` function produces plots
    showing the model''s response when varying that predictor and holding the others
    constant. You can clearly see the hinge function at work for thickness:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following is the output of the preceding command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_04.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'With `plotd()`, you can see density plots of the predicted probabilities by
    class label:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the output of the preceding command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_05.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'One can look at relative variable importance. Here we see the variable name,
    `nsubsets`, which is the number of model subsets that include the variable after
    the pruning pass, and the `gcv` and `rss` columns show the decrease in the respective
    value that the variable contributes (`gcv` and `rss` are scaled 0 to 100):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s see how well it did on the test dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is very comparable to our logistic regression models. We can now compare
    the models and see what our best choice would be.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What are we to conclude from all this work? We have the confusion matrices
    and error rates from our models to guide us, but we can get a little more sophisticated
    when it comes to selecting the classification models. An effective tool for a
    classification model comparison is the **Receiver Operating Characteristic** (**ROC**)
    chart. Very simply, ROC is a technique for visualizing, organizing, and selecting
    classifiers based on their performance (Fawcett, 2006). On the ROC chart, the
    y-axis is the **True Positive Rate** (**TPR**) and the x-axis is the **False Positive
    Rate** (**FPR**). The following are the calculations, which are quite simple:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '*TPR = Positives correctly classified / total positives*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '*FPR = Negatives incorrectly classified / total negatives*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the ROC results will generate a curve, and thus you are able to produce
    the **Area Under the Curve** (**AUC**). The AUC provides you with an effective
    indicator of performance, and it can be shown that the AUC is equal to the probability
    that the observer will correctly identify the positive case when presented with
    a randomly chosen pair of cases in which one case is positive and one case is
    negative (Hanley JA & McNeil BJ, 1982). In our case, we will just switch the observer
    with our algorithms and evaluate accordingly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'To create an ROC chart in R, you can use the `ROCR` package. I think this is
    a great package and allows you to build a chart in just three lines of code. The
    package also has an excellent companion website (with examples and a presentation)
    that can be found at the following link:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[http://rocr.bioinf.mpi-sb.mpg.de/.](http://rocr.bioinf.mpi-sb.mpg.de/.)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'What I want to show are three different plots on our ROC chart: the full model,
    the reduced model using BIC to select the features, the MARS model, and a bad
    model. This so-called bad model will include just one predictive feature and will
    provide an effective contrast to our other models. Therefore, let''s load the
    `ROCR` package, build this poorly performing model, and call it `bad.fit` for
    simplicity, using the `thick` feature:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It is now possible to build the ROC chart with three lines of code per model
    using the `test` dataset. We will first create an object that saves the predicted
    probabilities with the actual classification. Next, we will use this object to
    create another object with the calculated TPR and FPR. Then, we will build the
    chart with the `plot()` function. Let''s get started with the model using all
    of the features or, as I call it, the full model. This was the initial one that
    we built back in the *Logistic regression model* section of this chapter:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following is the performance object with the TPR and FPR:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following `plot` command with the title of `ROC` and `col=1` will color
    the line black:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_09.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'As stated previously, the curve represents TPR on the y-axis and FPR on the
    x-axis. If you have the perfect classifier with no false positives, then the line
    will run vertically at *0.0* on the x-axis. If a model is no better than chance,
    then the line will run diagonally from the lower left corner to the upper right
    one. As a reminder, the full model missed out on five labels: three false positives
    and two false negatives. We can now add the other models for comparison using
    a similar code, starting with the model built using BIC (refer to the *Logistic
    regression with cross-validation* section of this chapter), as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `add=TRUE` parameter in the `plot` command added the line to the existing
    chart. Finally, we will add the poorly performing model, the MARS model, and include
    a `legend` chart, as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following is the output of the preceding code snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_03_06.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: We can see that the `FULL` model, `BIC` model and the `MARS` model are nearly
    superimposed. It is also quite clear that the `BAD` model performed as poorly
    as was expected.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The final thing that we can do here is compute the AUC. This is again done
    in the `ROCR` package with the creation of a `performance` object, except that
    you have to substitute `auc` for `tpr` and `fpr`. The code and output are as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The highest AUC is for the full model at `0.997`. We also see `99.4` percent
    for the BIC model, `89.6` percent for the bad model and `99.5` for MARS. So, to
    all intents and purposes, with the exception of the bad model we have no difference
    in predictive powers between them. What are we to do? A simple solution would
    be to re-randomize the `train` and `test` sets and try this analysis again, perhaps
    using a 60/40 split and a different randomization seed. But if we end up with
    a similar result, then what? I think a statistical purist would recommend selecting
    the most parsimonious model, while others may be more inclined to include all
    the variables. It comes down to trade-offs, that is, model accuracy versus interpretability,
    simplicity, and scalability. In this instance, it seems safe to default to the
    simpler model, which has the same accuracy. It goes without saying that we won't
    always get this level of predictability with just GLMs or discriminant analysis.
    We will tackle these problems in upcoming chapters with more complex techniques
    and hopefully improve our predictive ability. The beauty of machine learning is
    that there are several ways to skin the proverbial cat.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at using probabilistic linear models to predict
    a qualitative response with three methods: logistic regression, discriminant analysis,
    and MARS. Additionally, we began the process of using ROC charts in order to explore
    model selection visually and statistically. We also briefly discussed the model
    selection and trade-offs that you need to consider. In future chapters, we will
    revisit the breast cancer dataset to see how more complex techniques perform.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
