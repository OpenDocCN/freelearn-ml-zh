["```py\nimport numpy as np\nimport time\nfrom matplotlib import pyplot\n```", "```py\nnp.random.seed(1)\nstates = [\"Sunny\",\"Rainy\"]\n```", "```py\nTransStates = [[\"SuSu\",\"SuRa\"],[\"RaRa\",\"RaSu\"]]\nTransnMatrix = [[0.75,0.25],[0.30,0.70]]\n```", "```py\nif sum(TransnMatrix[0])+sum(TransnMatrix[1]) != 2:\n     print(\"Warning! Probabilities MUST ADD TO 1\\. Wrong transition matrix!!\")\n     raise ValueError(\"Probabilities MUST ADD TO 1\")\n```", "```py\nWT = list()\nNumberDays = 200\nWeatherToday = states[0]\nprint(\"Weather initial condition =\",WeatherToday)\n```", "```py\ni = 0\nwhile i < NumberDays:\n    if WeatherToday == \"Sunny\":\n    TransWeather = np.random.choice(TransStates[0],replace=True,p=TransnMatrix[0])\n        if TransWeather == \"SuSu\":\n            pass\n        else:\n            WeatherToday = \"Rainy\"\n    elif WeatherToday == \"Rainy\":\n        TransWeather = np.random.choice(TransStates[1],replace=True,p=TransnMatrix[1])\n        if TransWeather == \"RaRa\":\n            pass\n        else:\n            WeatherToday = \"Sunny\"\n    print(WeatherToday)\n    WT.append(WeatherToday)\n    i += 1    \n    time.sleep(0.2)\n```", "```py\npyplot.plot(WT)\npyplot.show()\n```", "```py\ndef KnapSackTable(weight, value, P, n):\nT = [[0 for w in range(P + 1)]\nfor i in range(n + 1)]\n```", "```py\nfor i in range(n + 1):\n    for w in range(P + 1):\n        if i == 0 or w == 0:\n            T[i][w] = 0\n        elif weight[i - 1] <= w:\n            T[i][w] = max(value[i - 1]\n                + T[i - 1][w - weight[i - 1]],\n                        T[i - 1][w])\n        else:\n            T[i][w] = T[i - 1][w]\n```", "```py\nres = T[n][P]\nprint(\"Total value: \" ,res)\n```", "```py\nw = P\ntotweight=0\nfor i in range(n, 0, -1):\n    if res <= 0:\n        break\n```", "```py\nif res == T[i - 1][w]:\n    continue\n```", "```py\nelse:\n    print(\"Item selected: \",weight[i - 1],value[i - 1])\n    totweight += weight[i - 1]\n    res = res - value[i - 1]\n    w = w - weight[i â€“ 1]\n```", "```py\nprint(\"Total weight: \",totweight)\n```", "```py\nobjects = [(5, 18),(2, 9), (4, 12), (6,25)]\nprint(\"Items available: \",objects)\nprint(\"***********************************\")\n```", "```py\nvalue = []\nweight = []\nfor item in objects:\n    weight.append(item[0])\n    value.append(item[1])\n```", "```py\nP = 10\nn = len(value)\n```", "```py\nKnapSackTable(weight, value, P, n)\nThe following results are returned:\nItems available: [(5, 18), (2, 9), (4, 12), (6, 25)]\n*********************************\nTotal value: 37\nItem selected: 6 25\nItem selected: 4 12\nTotal weight: 10\n```", "```py\nimport networkx as nx\nimport matplotlib.pyplot as plt\n```", "```py\nG = nx.Graph()\nG.add_node(1)\nG.add_node(2)\nG.add_node(3)\nG.add_node(4)\n```", "```py\nG.add_edge(1, 2, weight=2)\nG.add_edge(2, 3, weight=2)\nG.add_edge(3, 4, weight=3)\nG.add_edge(1, 3, weight=5)\nG.add_edge(2, 4, weight=6)\n```", "```py\npos = nx.spring_layout(G, scale=3)\nnx.draw(G, pos,with_labels=True, font_weight='bold')\nedge_labels = nx.get_edge_attributes(G,'r')\nnx.draw_networkx_edge_labels(G, pos, labels = edge_labels)\nplt.show()\n```", "```py\nprint(nx.shortest_path(G,1,4,weight='weight'))\n```", "```py\n[1, 2, 3, 4]\n```", "```py\nprint(nx.nx.shortest_path_length(G,1,4,weight='weight'))\n```", "```py\n7\n```", "```py\nimport gym\nimport numpy as np\n```", "```py\nenv = gym.make('FrozenLake-v0')\n```", "```py\nQTable = np.zeros([env.observation_space.n,env.action_space.n])\n```", "```py\nalpha = .80\ngamma = .95\nNumEpisodes = 2000\n```", "```py\nRewardsList = []\n```", "```py\nfor i in range(NumEpisodes):\n    CState = env.reset()\n    SumReward = 0\n    d = False\n    j = 0\n    while j < 99:\n        j+=1\n        Action = np.argmax(QTable[CState,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n        NState,Rewards,d,_ = env.step(Action)\n        QTable[CState,Action] = QTable[CState,Action] + alpha*(Rewards + gamma*np.max(QTable[NState,:]) - QTable[CState,Action])\n        SumReward += Rewards\n        CState = NState\n        if d == True:\n            break\n\n    RewardsList.append(SumReward)\n```", "```py\nprint (\"Score: \" +  str(sum(RewardsList)/NumEpisodes))\nprint (\"Final Q-Table Values\")\nprint (QTable)\n```", "```py\nimport gym\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Reshape\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n```", "```py\nENV_NAME = 'FrozenLake-v0'\nenv = gym.make(ENV_NAME)\nnp.random.seed(1)\nenv.seed(1)\n```", "```py\nActions = env.action_space.n\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(16, 4, input_length=1))\nmodel.add(Reshape((4,)))\nprint(model.summary())\n```", "```py\nmemory = SequentialMemory(limit=10000, window_length=1)\npolicy = BoltzmannQPolicy()\n```", "```py\nDqn = DQNAgent(model=model, nb_actions=Actions,\n               memory=memory, nb_steps_warmup=500,\n               target_model_update=1e-2, policy=policy,\n               enable_double_dqn=False, batch_size=512\n               )\n```", "```py\nDqn.compile(Adam())\nDqn.fit(env, nb_steps=1e5, visualize=False, verbose=1, log_interval=10000)\n```", "```py\nDqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n```", "```py\nDqn.test(env, nb_episodes=20, visualize=False)\n```", "```py\nimport numpy as np\nimport gym\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n```", "```py\nENV_NAME = 'CartPole-v0'\nenv = gym.make(ENV_NAME)\n```", "```py\nnp.random.seed(123)\nenv.seed(123)\n```", "```py\nnb_actions = env.action_space.n\n```", "```py\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n```", "```py\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\n```", "```py\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n               target_model_update=1e-2, policy=policy)\n```", "```py\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\ndqn.fit(env, nb_steps=1000, visualize=True, verbose=2)\n```", "```py\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n```", "```py\ndqn.test(env, nb_episodes=5, visualize=True)\n```", "```py\nimport numpy as np\nimport gym\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n```", "```py\nENV_NAME = 'CartPole-v0'\nenv = gym.make(ENV_NAME)\n```", "```py\nnp.random.seed(1)\nenv.seed(1)\n```", "```py\nnb_actions = env.action_space.n\n```", "```py\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n```", "```py\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\n```", "```py\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,   \n               nb_steps_warmup=10, enable_double_dqn=True,  \n               target_model_update=1e-2,policy=policy)\n```", "```py\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\ndqn.fit(env, nb_steps=1000, visualize=True, verbose=2)\n```", "```py\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n```", "```py\ndqn.test(env, nb_episodes=5, visualize=True)\n```", "```py\nimport numpy as np\nimport gym\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\n```", "```py\nENV_NAME = 'CartPole-v0'\nenv = gym.make(ENV_NAME)\n```", "```py\nnp.random.seed(2)\nenv.seed(2)\n```", "```py\nnb_actions = env.action_space.n\n```", "```py\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1,) + env.observation_space.shape))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n```", "```py\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy()\n```", "```py\ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n               nb_steps_warmup=10, enable_dueling_network=True,  \n               dueling_type='avg',target_model_update=1e-2,\n               policy=policy)\n```", "```py\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])\ndqn.fit(env, nb_steps=1000, visualize=True, verbose=2)\n```", "```py\ndqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n```", "```py\ndqn.test(env, nb_episodes=5, visualize=True)\n```"]