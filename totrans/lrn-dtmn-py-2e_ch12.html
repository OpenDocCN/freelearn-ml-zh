<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Working with Big Data</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Working with Big Data</h1>
            </header>

            <article>
                
<p>The amount of data is increasing at an exponential rate. Today's systems are generating and recording information on customer behavior, distributed systems, network analysis, sensors, and many, many more sources. While the current big trend of mobile data is pushing the current growth, the next big thing—<strong>the Internet of Things (IoT)</strong>—is going to further increase the rate of growth.</p>
<p>What this means for data mining is a new way of thinking. Complex algorithms with high runtimes need to be improved or discarded, while simpler algorithms that can deal with more samples are becoming more popular to use. As an example, while support vector machines are great classifiers, some variants are difficult to use on very large datasets. In contrast, simpler algorithms such as logistic regression can manage more easily in these scenarios.</p>
<p>This complexity versus distribution issue is just one of the reasons why deep neural networks (DNNs) have become so popular. You can create very complex models using DNNs, but you can also&#160;<em>distribute</em> the workload for training them across many computers quite easily.</p>
<p>In this chapter, we will investigate the following:</p>
<ul>
<li>Big data challenges and applications</li>
<li>The MapReduce paradigm</li>
<li>Hadoop MapReduce</li>
<li>mrjob, a Python library to run MapReduce programs on Amazon's AWS infrastructure</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Big data</h1>
            </header>

            <article>
                
<p>What makes big data different? Most big data proponents talk about the four Vs of big data:</p>
<ul>
<li><strong>Volume</strong>: The amount of data that we generate and store is growing at an increasing rate, and predictions of the future generally only suggest further increases. Today's multi-gigabyte-sized hard drives will turn into exabyte-sized drives in a few years, and network throughput traffic will be increasing as well. The signal-to-noise ratio can be quite difficult, with important data being lost in the mountain of non-important data.</li>
<li><strong>Velocity</strong>: While related to volume, the velocity of data is increasing too. Modern cars have hundreds of sensors that stream data into their computers, and information from these sensors needs to be analyzed at a sub-second level to operate the car. It isn't just a case of finding answers in the volume of data; those answers often need to come quickly. In some cases, we also simply do not have enough disk space to store data, meaning we also need to make decisions on what data to keep for later analysis.</li>
<li><strong>Variety</strong>: Nice datasets with clearly defined columns are only a small fraction&#160;of the datasets that we have these days. Consider a social media post that may have text, photos, user mentions, likes, comments, videos, geographic information, and other fields. Simply ignoring parts of this data that don't fit your model will lead to a loss of information, but integrating that information itself can be very difficult.</li>
<li><strong>Veracity</strong>: With this increase in the amount of data, it can be hard to determine whether the data is being correctly collected—whether it is outdated, noisy, contains outliers<span>—</span>or generally whether it is useful at all. Being able to trust the data is hard when a human can't reliably verify it. External datasets are being increasingly merged into internal ones too, giving rise to more troubles relating to the veracity of data.</li>
</ul>
<p>These main four Vs (others have proposed additional Vs) outline why big data is different from&#160;just <em>lots of data</em>. At these scales, the engineering problem of working with data is often more difficult—let alone the analysis. While there are lots of snake oil salesmen that overstate a particular product's ability to analyze big data, it is hard to deny the engineering challenges and the potential of big data analytics.</p>
<p>The algorithms we have used so far in the book&#160;load the dataset into memory and then work on the in-memory version. This gives a large benefit in terms of speed of computation (because using computer memory is faster than using hard drives), as it is much faster to compute on in-memory data than having to load a sample before we use it. In addition, in-memory data allows us to iterate over the data&#160;many times, thereby improving our machine learning model.&#160;</p>
<p>In big data, we can't load our data into memory. In many ways, this is a good definition for whether a problem is big data or not—if the data can fit in the memory on your computer, you aren't dealing with a big data problem.</p>
<div class="packt_tip">When looking at the data you create, such as log data from your company's internal applications, it might be tempting to simply throw it all into a file, unstructured, and use big-data concepts later to analyze it. It is best not to do this; instead, you should use structured&#160;formats for your own datasets. The reason is that the four Vs we just outlined are actually&#160;<em>problems</em> that need to be solved to perform data analysis, not goals to strive for!</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Applications of big data</h1>
            </header>

            <article>
                
<p>There are many use cases for big data, in public and private sectors.</p>
<p>The most common experience people have using a big-data-based system is in Internet search, such as Google. To run these systems, a search needs to be carried out over billions of websites in a fraction of a second. Doing a basic text-based search would be inadequate to deal with such a problem. Simply storing the text of all those websites is a large problem. In order to deal with queries, new data structures and data mining methods need to be created and implemented specifically for this application.</p>
<p>Big data is also used in many other scientific experiments such as the Large Hadron Collider, part of which is pictured next.&#160;It stretches over 27 kilometers and contains 150 million sensors monitoring hundreds of millions of particle collisions per second. The data from this experiment is massive, with 25 petabytes created daily, after a filtering process (if filtering were not used, there would be 150 million petabytes per year). Analysis on data this big has led to amazing insights about our universe, but it has been a significant engineering and analytics challenge.</p>
<div class="CDPAlignCenter CDPAlign"><img height="275" width="368" class="image-border" src="images/B06162_12_01.jpg"/></div>
<p>Governments are increasingly using big data too, to track populations, businesses, and other aspects related to their country. Tracking millions of people and billions of interactions (such as business transactions or health spending)&#160;leads to a need for big data analytics in many government organizations.</p>
<p>Traffic management is a particular focus of many governments around the world, who are tracking traffic using millions of sensors to determine which roads are the most congested and predicting the impact of new roads on traffic levels. These management systems will link with data from autonomous cars in the near future, leading to even more data about traffic conditions in real time. Cities that make use of this data will find that their traffic flows more freely.</p>
<p>Large retail organizations are using big data to improve customer experience and reduce costs. This involves predicting customer demand in order to have the correct level of inventory, upselling customers with products they may like to purchase, and tracking transactions to look for trends, patterns, and potential frauds. Companies that automatically create great predictions can have higher sales at lower costs.</p>
<p>Other large businesses are also leveraging big data to automate aspects of their business and improve their offering. This includes leveraging analytics to predict future trends in their sector and tracking external competitors. Large businesses also use analytics to manage their own employees—tracking employees to look for signs that an employee may leave the company, in order to intervene before they do.</p>
<p>The information security sector is also leveraging big data in order to look for malware infections in large networks, by monitoring network traffic. This can include looking for odd traffic patterns, evidence of malware spreading, and other oddities. Advanced Persistent Threats (APTs) is another problem, where a motivated attacker will hide their code within a large network to steal information or cause damage over a long period of time. Finding APTs is often a case of forensically examining many computers, a task which simply takes too long for a human to effectively perform themselves. Analytics helps automate and analyze these forensic images to find infections.</p>
<p>Big data is being used in an increasing number of sectors and applications, and this trend is likely to only continue.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">MapReduce</h1>
            </header>

            <article>
                
<p>There are a number of concepts to perform data mining and general computation on big data. One of the most popular is the MapReduce model, which can be used for general computation on arbitrarily large datasets.</p>
<p>MapReduce originates from Google, where it was developed with distributed computing in mind. It also introduces fault tolerance and scalability improvements. The <em>original</em>&#160;research for MapReduce was published in 2004, and since then there have been thousands of projects, implementations, and applications using it.</p>
<p>While the concept is similar to many previous concepts, MapReduce has become a staple in big data analytics.</p>
<p>There are two major stages in a MapReduce job.</p>
<ol>
<li>The first is Map, by which we take a function and a list of items, and apply that function to each item. Put another way, we take each item as the input to the function and store the result of that function call:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="334" width="272" class="image-border" src="images/B06162_12_02-e1493116032553.jpg"/></div>
<ol start="2">
<li>The second step is Reduce, where we take the results from the map step and combine them using a function. For statistics, this could be as simple as adding all the numbers together. The reduce function in this scenario is an add function, which would take the previous sum, and add the new result:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="371" width="271" class="image-border" src="images/B06162_12_03-e1493116048271.jpg"/></div>
<p>After these two steps, we will have transformed our data and reduced it to a final result.</p>
<p>MapReduce jobs can have many iterations, some of which are only&#160;Map jobs, some only Reduce jobs and some iterations with&#160;both a Map and Reduce step. Let us now have a look at some more tangible examples, first using built-in python functions and then using a specific tool for MapReduce jobs.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The intuition behind MapReduce</h1>
            </header>

            <article>
                
<p>MapReduce has two main steps: the <kbd>Map</kbd> step and the <kbd>Reduce</kbd> step. These are built on the functional programming concepts of mapping a function to a list and reducing the result. To explain the concept, we will develop code that will iterate over a list of lists and produce the sum of all numbers in those lists.</p>
<p>There are also <kbd>shuffle</kbd> and <kbd>combine</kbd> steps in the MapReduce paradigm, which we will see later.</p>
<p>To start with, the Map step takes a function and applies it to each element in a list. The returned result is a list of the same size, with the results of the function applied to each element.</p>
<p>To open a new Jupyter Notebook, start by creating a list of lists with numbers in each sublist:</p>
<pre>a = [[1,2,1], [3,2], [4,9,1,0,2]]
</pre>
<p>Next, we can perform a <kbd>map</kbd>&#160;using the sum function. This step will apply the <span class="packt_screen">sum</span> function to each element of <em>a</em>:</p>
<pre>sums = map(sum, a)
</pre>
<p>While <kbd>sums</kbd> is a generator (the actual value isn't computed until we ask for it), the preceding step is approximately equal to the following code:</p>
<pre>sums = []<br/>for sublist in a:<br/>    results = sum(sublist)<br/>    sums.append(results)
</pre>
<p>The <kbd>reduce</kbd> step is a little more complicated. It involves applying a function to each element of the returned result, to some starting value. We start with an initial value&#160;and then apply a given function to that initial value and the first value. We then apply the given function to the result and the next value, and so on</p>
<p>We start by creating a function that takes two numbers and adds them together.</p>
<pre>def add(a, b): <br/>    return a + b
</pre>
<p>We then perform the reduce. The signature of <kbd>reduce</kbd> is: <kbd>reduce(function, sequence, initial)</kbd>, where the function is applied at each step to the sequence. In the first step, the initial value is used as the first value rather than the first element of the list:</p>
<pre>from functools import reduce <br/>print(reduce(add, sums, 0))
</pre>
<p>The result, 25, is the sum of each of the values in the sums list and is consequently the sum of each of the elements in the original array.</p>
<p>The preceding code is similar to the following:</p>
<pre>initial = 0 <br/>current_result = initial <br/>for element in sums: <br/>    current_result = add(current_result, element)
</pre>
<p>In this simple example, our code would be greatly simplified if it didn't use the MapReduce paradigm, but the real gains come from distributing the computation. For instance, if we have a million sublists and each of those sublists contains a million elements, we can distribute this computation over many computers.</p>
<p>In order to do this, we distribute the <kbd>map</kbd> step by segmenting out data. For each of the elements in our list, we send it, along with a description of our function, to a computer. This computer then returns the result to our main computer (the master).</p>
<p>The master then sends the result to a computer for the <kbd>reduce</kbd> step. In our example of a million sublists, we would send a million jobs to different computers (the same computer may be reused after it completes our first job). The returned result would be just a single list of a million numbers, which we then compute the sum of.</p>
<p>The result is that no computer ever needed to store more than a million numbers, despite our original data having a trillion numbers in it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">A word count example</h1>
            </header>

            <article>
                
<p>Any <em>actual</em>&#160;implementation of MapReduce is a little more complex than just using a <kbd>map</kbd> and <kbd>reduce</kbd> step. Both steps are invoked using keys, which allows for the separation of data and tracking of values.</p>
<div class="packt_infobox">The map function takes a key-value pair and returns a list of <em>key/value</em> pairs. The keys for the input and output don't necessarily relate to each other.</div>
<p>For example, for a MapReduce program that performs a word count, the input key might be a sample document's ID value, while the output key would be a given word. The input value would be the text of the document and the output value would be the frequency of each word. &#160;We split the document to get the words, and then yield each of the word, count pairs. The word here is the key, with the count being the value in MapReduce terms:</p>
<pre>from collections import defaultdict<br/>def map_word_count(document_id, document):<br/>    counts = defaultdict(int) <br/>    for word in document.split(): <br/>        counts[word] += 1<br/>    for word in counts: <br/>        yield (word, counts[word])
</pre>
<div class="packt_tip">Have a really, really big dataset? You can just do&#160;<kbd>yield (word, 1)</kbd> when you come across a new word and then combine the ones in the shuffle step rather than count within the map step. Where you place it depends on your dataset size, per-document size, network capacity, and a whole range of factors. Big data is a big engineering problem and to get the maximum performance out of a system, you'll need to model how data will flow throughout the algorithm.</div>
<p>By using the word as the key, we can then perform a shuffle step, which groups all the values for each key:</p>
<pre>def shuffle_words(results):<br/>    records = defaultdict(list)<br/>    for results in results_generators: <br/>        for word, count in results: <br/>            records[word].append(count)<br/>    for word in records: <br/>        yield (word, records[word])
</pre>
<p>The final step is the reduce step, which takes a key-value pair (the value, in this case, is always a list) and produces a key-value pair as a result. In our example, the key is the word, the input list is the list of counts produced in the shuffle step, and the output value is the sum of the counts:</p>
<pre>def reduce_counts(word, list_of_counts): <br/>    return (word, sum(list_of_counts))
</pre>
<p>To see this in action, we can use the 20 newsgroups dataset, which is provided in scikit-learn. This dataset is not big data, but we can see the concepts in action here:</p>
<pre>from sklearn.datasets import fetch_20newsgroups <br/>dataset = fetch_20newsgroups(subset='train') <br/>documents = dataset.data
</pre>
<p>We then apply our map step. We use enumerate here to automatically generate document IDs for us. While they aren't important in this application, these keys are important in other applications:</p>
<pre>map_results = map(map_word_count, enumerate(documents))
</pre>
<p>The actual result here is just a generator; no actual counts have been produced. That said, it is a generator that emits <span class="packt_screen">(word, count)</span> pairs.</p>
<p>Next, we perform the shuffle step to sort these word counts:</p>
<pre>shuffle_results = shuffle_words(map_results)
</pre>
<p>This, in essence, is a MapReduce job; however, it is only running on a single thread, meaning we aren't getting any benefit from the MapReduce data format. In the next section, we will start using Hadoop, an open source provider of MapReduce, to start getting the benefits of this type of paradigm.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Hadoop MapReduce</h1>
            </header>

            <article>
                
<p>Hadoop is a set of open source tools from Apache that includes an implementation of MapReduce. In many cases, it is the de-facto implementation used by many. The project is managed by the Apache group (who are responsible for the famous web server of the same name).</p>
<p>The Hadoop ecosystem is quite complex, with a large number of tools. The main component we will use is Hadoop MapReduce. Other tools for working with big data that are included in Hadoop are as follows:</p>
<ul>
<li><strong>Hadoop Distributed File System (HDFS)</strong>: This is a file system that can store files over many computers, with the goal of being robust against hardware failure while providing high bandwidth.</li>
<li><strong>YARN</strong>: This is a method for scheduling applications and managing clusters of computers.</li>
<li><strong>Pig:</strong> This is a higher level programming language for MapReduce. Hadoop MapReduce is implemented in Java, and Pig sits on top of the Java implementation, allowing you to write programs in other languages—including Python.</li>
<li><strong>Hive:</strong> This is for managing data warehouses and performing queries.</li>
<li><strong>HBase</strong>: This is an implementation of Google's BigTable, a distributed database.</li>
</ul>
<p>These tools all solve different issues that come up when doing big data experiments, including data analytics.</p>
<p>There are also non-Hadoop-based implementations of MapReduce, as well as other projects with similar goals. In addition, many cloud providers have MapReduce-based systems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Applying MapReduce</h1>
            </header>

            <article>
                
<p>In this application, we will look at predicting the gender of a writer based on their use of different words. We will use a Naive Bayes method for this, trained in MapReduce. The final model doesn't need MapReduce, although we can use the Map step to do so—that is, run the prediction model on each document in a list. This is a common Map operation for data mining in MapReduce, with the reduce step simply organizing the list of predictions so they can be tracked back to the original document.</p>
<p>We will be using Amazon's infrastructure to run our application, allowing us to leverage their computing resources.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Getting the data</h1>
            </header>

            <article>
                
<p>The data we are going to use is a set of blog posts that are labeled for age, gender, industry (that is, work) and, funnily enough, star sign. This data was collected from <a href="http://blogger.com">http://blogger.com</a> in August 2004 and has over 140 million words in more than 600,000 posts. Each blog is <em>probably</em> written by just one person, with some work put into verifying this (although, we can never be really sure). Posts are also matched with the date of posting, making this a very rich dataset.</p>
<p>To get the data, go to <a href="http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm">http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm</a> and click on <span class="packt_screen">Download Corpus</span>. From there, unzip the file to a directory on your computer.</p>
<p>The dataset is organized with a single blog to a file, with the filename giving the classes. For instance, one of the filenames is as follows:</p>
<pre><strong>1005545.male.25.Engineering.Sagittarius.xml</strong>
</pre>
<p>The filename is separated by periods, and the fields are as follows:</p>
<ul>
<li><strong>Blogger ID</strong>: This a simple ID value to organize the identities.</li>
<li><strong>Gender</strong>: This is either male or female, and all the blogs are identified as one of these two options (no other options are included in this dataset).</li>
<li><strong>Age</strong>: The exact ages are given, but some gaps are deliberately present. Ages present are in the (inclusive) ranges of 13-17, 23-27, and 33-48. The reason for the gaps is to allow for splitting the blogs into age ranges with gaps, as it would be quite difficult to separate an 18-year old's writing from a 19-year-old, and it is possible that the age itself is a little outdated itself and would need to be updated to 19 anyway.</li>
<li><strong>Industry</strong>: In one of 40 different industries including science, engineering, arts, and real estate. Also, included is indUnk, for an&#160;unknown industry.</li>
<li><strong>Star Sign</strong>: This is one of the 12 astrological star signs.</li>
</ul>
<p>All values are self-reported, meaning there may be errors or inconsistencies with labeling, but are assumed to be mostly reliable—people had the option of not setting values if they wanted to preserve their privacy in those ways.</p>
<p>A single file is in a pseudo-XML format, containing a <kbd>&lt;Blog&gt;</kbd> tag and then a sequence of <kbd>&lt;post&gt;</kbd> tags. Each of the <kbd>&lt;post&gt;</kbd> tag is proceeded by a <kbd>&lt;date&gt;</kbd> tag as well. While we can parse this as XML, it is much simpler to parse it on a line-by-line basis as the files are not exactly well-formed XML, with some errors (mostly encoding problems). To read the posts in the file, we can use a loop to iterate over the lines.</p>
<p>We set a test filename so we can see this in action:</p>
<pre>import os <br/>filename = os.path.join(os.path.expanduser("~"), "Data", "blogs", "1005545.male.25.Engineering.Sagittarius.xml")
</pre>
<p>First, we create a list that will let us store each of the posts:</p>
<pre>all_posts = []
</pre>
<p>Then, we open the file to read:</p>
<pre>with open(filename) as inf:<br/>    post_start = False<br/>    post = []<br/>    for line in inf: <br/>        line = line.strip()<br/>        if line == "&lt;post&gt;":<br/>            # Found a new post<br/>            post_start = True <br/>        elif line == "&lt;/post&gt;":<br/>            # End of the current post, append to our list of posts and start a new one<br/>            post_start = False<br/>            all_posts.append("n".join(post))<br/>            post = []<br/>        elif post_start:<br/>            # In a current post, add the line to the text of the post<br/>            post.append(line)
</pre>
<p>If we aren't in a current post, we simply ignore the line.</p>
<p>We can then grab the text of each post:</p>
<pre>print(all_posts[0])
</pre>
<p><span>We can also find out how many posts this author created:</span></p>
<pre>print(len(all_posts))
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Naive Bayes prediction</h1>
            </header>

            <article>
                
<p>We are now going to implement the Naive Bayes algorithm using mrjob, allowing it to process our dataset. Technically our version will be a reduced version of most Naive Bayes' implementations, without many of the features that you would expect like smoothing small values.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The mrjob package</h1>
            </header>

            <article>
                
<p>The <strong>mrjob</strong> package allows us to create MapReduce jobs that can easily be computed on Amazon's infrastructure. While mrjob sounds like a sedulous addition to the Mr. Men series of children's books, it stands for <strong>Map Reduce Job</strong>.</p>
<div class="packt_tip">You can install mrjob&#160;using the following: <kbd>pip install&#160;</kbd><span><kbd>mrjob</kbd><br/>
I had to install the <span class="packt_screen">filechunkio</span> package separately using&#160;<kbd>conda install -c conda-forge filechunkio</kbd>, but this will depend on your system setup. There are other Anaconda channels for installing mrjob, check them with:<br/>
<kbd>anaconda search -t conda mrjob</kbd></span></div>
<p>In essence, <span class="packt_screen">mrjob</span> provides the standard functionality that most MapReduce jobs need. Its most amazing feature is that you can write the same code, test on your local machine (without heavy infrastructure like Hadoop), and then push to Amazon's EMR service or another Hadoop server.</p>
<p>This makes testing the code significantly easier, although it can't magically make a big problem small— any local testing uses a subset of the dataset, rather than the whole, big dataset. Instead, mrjob gives you a framework that you can test with a small problem and have more confidence that the solution will scale to a larger problem, distributed on different systems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Extracting the blog posts</h1>
            </header>

            <article>
                
<p>We are first going to create a MapReduce program that will extract each of the posts from each blog file and store them as separate entries. As we are interested in the gender of the author of the posts, we will extract that too and store it with the post.</p>
<div class="packt_infobox">We can't do this in a Jupyter Notebook, so instead open a Python IDE for development. If you don't have a Python IDE you can use a text editor. I recommend PyCharm, although it has a larger learning curve and it is probably a bit heavy for just this chapter's code.</div>
<p>At the very least, I recommend using an IDE that has syntax highlighting and basic completion of variable names (that last one helps find typos in your code easily.</p>
<div class="packt_tip">If you still can't find an IDE you like, you can write the code in an IPython Notebook and then click on <span class="packt_screen">File| Download As| Python</span>. Save this file to a directory and run it as we outlined in <a href="lrn-dtmn-py-2e_ch02.html" target="_blank">Chapter 11</a>, <em>Classifying Objects in Images using Deep Learning</em>.</div>
<p>To do this, we will need the <kbd>os</kbd> and <kbd>re</kbd> libraries as we will be obtaining environment variables and we will also use a regular expression for word separation:</p>
<pre>import os <br/>import re
</pre>
<p>We then import the MRJob class, which we will inherit from our MapReduce job:</p>
<pre><span>from mrjob.job import MRJob</span>
</pre>
<p>We then create a new class that subclasses MRJob.&#160;We will use a similar loop, as before, to extract blog posts from the file. The mapping function we will define next will work off each line, meaning we have to track different posts outside of the mapping function. For this reason, we make <kbd>post_start</kbd> and post class variables, rather than variables inside the function.&#160;We then define our mapper function—this takes a line from a file as input and yields blog posts. The lines are guaranteed to be ordered from the same per-job file. This allows us to use the above class variables to record current post data:</p>
<pre>class ExtractPosts(MRJob):<br/>    post_start = False <br/>    post = []<br/><br/>    def mapper(self, key, line):<br/>        filename = os.environ["map_input_file"]<br/>        # split the filename to get the gender (which is the second token)<br/>        gender = filename.split(".")[1]<br/>        line = line.strip()<br/>        if line == "&lt;post&gt;":<br/>            self.post_start = True<br/>        elif line == "&lt;/post&gt;":<br/>            self.post_start = False<br/>            yield gender, repr("n".join(self.post))<br/>            self.post = []<br/>        elif self.post_start:<br/>            self.post.append(line)
</pre>
<p>Rather than storing the posts in a list, as we did earlier, we yield them. This allows mrjob to track the output. We yield both the gender and the post so that we can keep a record of which gender each record matches. The rest of this function is defined in the same way as our loop above.</p>
<p>Finally, outside the function and class, we set the script to run this MapReduce job when it is called from the command line:</p>
<pre><strong>if __name__ == '__main__': </strong><br/><strong>    ExtractPosts.run()</strong>
</pre>
<p>Now, we can run this MapReduce job using the following shell command.</p>
<pre><strong>$ python extract_posts.py &lt;your_data_folder&gt;/blogs/51* </strong><br/><strong>  --output-dir=&lt;your_data_folder&gt;/blogposts --no-output</strong>
</pre>
<div class="packt_tip">Just a reminder that you don't need to enter the <span class="packt_screen">$</span> on the above line - this just indicates this is a command run from the command line, and not in a Jupyter Notebook.</div>
<p>The first parameter, <kbd>&lt;your_data_folder&gt;/blogs/51*</kbd> (just remember to change <kbd>&lt;your_data_folder&gt;</kbd> to the full path to your data folder), obtains a sample of the data (all files starting with 51, which is only 11 documents). We then set the output directory to a new folder, which we put in the data folder, and specify not to output the streamed data. Without the last option, the output data is shown to the command line when we run it—which isn't very helpful to us and slows down the computer quite a lot.</p>
<p>Run the script, and quite quickly each of the blog posts will be extracted and stored in our output folder. This script only ran on a single thread on the local computer so we didn't get a speedup at all, but we know the code runs.</p>
<p>We can now look in the output folder for the results. A bunch of files are created and each file contains each blog post on a separate line, preceded by the gender of the author of the blog.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Training Naive Bayes</h1>
            </header>

            <article>
                
<p>Now that we have extracted the blog posts, we can train our Naive Bayes model on them. The intuition is that we record the probability of a word being written by a particular gender, and record these values in our model. To classify a new sample, we would multiply the probabilities and find the most likely gender.</p>
<p>The aim of this code is to output a file that lists each word in the corpus, along with the frequencies of that word for each gender. The output file will look something like this:</p>
<pre>"'ailleurs" {"female": 0.003205128205128205}<br/>"'air" {"female": 0.003205128205128205}<br/>"'an" {"male": 0.0030581039755351682, "female": 0.004273504273504274}<br/>"'angoisse" {"female": 0.003205128205128205}<br/>"'apprendra" {"male": 0.0013047113868622459, "female": 0.0014172668603481887}<br/>"'attendent" {"female": 0.00641025641025641}<br/>"'autistic" {"male": 0.002150537634408602}<br/>"'auto" {"female": 0.003205128205128205}<br/>"'avais" {"female": 0.00641025641025641}<br/>"'avait" {"female": 0.004273504273504274}<br/>"'behind" {"male": 0.0024390243902439024} <br/>"'bout" {"female": 0.002034152292059272}
</pre>
<p>The first value is the word and the second is a dictionary mapping the genders to the frequency of that word in that gender's writings.</p>
<p>Open a new file in your Python IDE or text editor. We will again need the <kbd>os</kbd> and <kbd>re</kbd> libraries, as well as <kbd>NumPy</kbd> and <kbd>MRJob</kbd> from <kbd>mrjob</kbd>. We also need <kbd>itemgetter</kbd>, as we will be sorting a dictionary:</p>
<pre>import os <br/>import re <br/>import numpy as np <br/>from mrjob.job import MRJob <br/>from operator import itemgetter
</pre>
<p>We will also need <kbd>MRStep</kbd>, which outlines a step in a MapReduce job. Our previous job only had a single step, which is defined as a mapping function and then as a reducing function. This job will have multiple steps where we Map, Reduce, and then Map and Reduce again. The intuition is the same as the pipelines we used in earlier chapters, where the output of one step is the input to the next step:</p>
<pre>from mrjob.step import MRStep
</pre>
<p>We then create our word search regular expression and compile it, allowing us to find word boundaries. This type of regular expression is much more powerful than the simple split we used in some previous chapters, but if you are looking for a more accurate word splitter, I recommend using NLTK or Spacey as we did in <a href="lrn-dtmn-py-2e_ch06.html">Chapter 6</a><q>, Social Media Insight using Naive Bayes</q>:</p>
<pre><span>word_search_re = re.compile(r"[w']+") </span>
</pre>
<p>We define a new class for our training. I'll first provide the whole class as one code block, and then we will come back to each section to review what it does:</p>
<pre>class NaiveBayesTrainer(MRJob):<br/><br/>    def steps(self):<br/>    return [<br/>            MRStep(mapper=self.extract_words_mapping,<br/>                   reducer=self.reducer_count_words),<br/>            MRStep(reducer=self.compare_words_reducer),<br/>    ]<br/><br/>    def extract_words_mapping(self, key, value):<br/>        tokens = value.split()<br/>        gender = eval(tokens[0])<br/>        blog_post = eval(" ".join(tokens[1:]))<br/>        all_words = word_search_re.findall(blog_post)<br/>        all_words = [word.lower() for word in all_words]<br/>        for word in all_words:<br/>            # Occurence probability<br/>            yield (gender, word), 1. / len(all_words)<br/><br/>    def reducer_count_words(self, key, counts):<br/>        s = sum(counts)<br/>        gender, word = key #.split(":")<br/>        yield word, (gender, s)<br/><br/>    def compare_words_reducer(self, word, values):<br/>        per_gender = {}<br/>        for value in values:<br/>            gender, s = value<br/>            per_gender[gender] = s<br/>            yield word, per_gender<br/><br/>    def ratio_mapper(self, word, value):<br/>        counts = dict(value)<br/>        sum_of_counts = float(np.mean(counts.values()))<br/>        maximum_score = max(counts.items(), key=itemgetter(1))<br/>        current_ratio = maximum_score[1] / sum_of_counts<br/>        yield None, (word, sum_of_counts, value)<br/><br/>    def sorter_reducer(self, key, values):<br/>        ranked_list = sorted(values, key=itemgetter(1), reverse=True)<br/>        n_printed = 0<br/>        for word, sum_of_counts, scores in ranked_list:<br/>            if n_printed &lt; 20:<br/>                print((n_printed + 1), word, scores)<br/>            n_printed += 1<br/>        yield word, dict(scores)
</pre>
<p>Let's have a look at the sections of this code, one step at a time:</p>
<pre>class NaiveBayesTrainer(MRJob):
</pre>
<p>We define the steps of our MapReduce job. There are two steps:</p>
<p>The first step will extract the word occurrence probabilities. The second step will compare the two genders and output the probabilities for each to our output file. In each MRStep, we define the mapper and reducer functions, which are class functions in this NaiveBayesTrainer class (we will write those functions next):</p>
<pre>    def steps(self):<br/>        return [<br/>            MRStep(mapper=self.extract_words_mapping,<br/>                   reducer=self.reducer_count_words),<br/>            MRStep(reducer=self.compare_words_reducer),<br/>        ]
</pre>
<p>The first function is the mapper function for the first step. The goal of this function is to take each blog post, get all the words in that post, and then note the occurrence. We want the frequencies of the words, so we will return <kbd>1 / len(all_words)</kbd>, which allows us to later sum the values for frequencies. The computation here isn't exactly correct—we need to also normalize for the number of documents. In this dataset, however, the class sizes are the same, so we can conveniently ignore this with little impact on our final version.</p>
<p>We also output the gender of the post's author, as we will need that later:</p>
<pre>    def extract_words_mapping(self, key, value):<br/>        tokens = value.split()<br/>        gender = eval(tokens[0])<br/>        blog_post = eval(" ".join(tokens[1:]))<br/>        all_words = word_search_re.findall(blog_post)<br/>        all_words = [word.lower() for word in all_words]<br/>        for word in all_words:<br/>            # Occurence probability<br/>            yield (gender, word), 1. / len(all_words)
</pre>
<div class="packt_infobox">We used <kbd>eval</kbd> in the preceding code to simplify the parsing of the blog posts from the file, for this example. This is not recommended. Instead, use a format such as JSON to properly store and parse the data from the files. A malicious user with access to the dataset can insert code into these tokens and have that code run on your server.</div>
<p>In the reducer for the first step, we sum the frequencies for each gender and word pair. We also change the key to be the word, rather than the combination, as this allows us to search by word when we use the final trained model (although, we still need to output the gender for later use);</p>
<pre>    def reducer_count_words(self, key, counts):<br/>        s = sum(counts)<br/>        gender, word = key #.split(":")<br/>        yield word, (gender, s)
</pre>
<p>The final step doesn't need a mapper function, which is why we didn't add one. The data will pass straight through as a type of identity mapper. The reducer, however, will combine frequencies for each gender under the given word and then output the word and frequency dictionary.</p>
<p>This gives us the information we needed for our Naive Bayes implementation:</p>
<pre>    def compare_words_reducer(self, word, values):<br/>        per_gender = {}<br/>        for value in values:<br/>            gender, s = value<br/>            per_gender[gender] = s<br/>            yield word, per_gender
</pre>
<p>Finally, we set the code to run this model when the file is run as a script. We will need to add this code to the file:</p>
<pre>if __name__ == '__main__': <br/> NaiveBayesTrainer.run()
</pre>
<p>We can then run this script. The input to this script is the output of the previous post-extractor script (we can actually have them as different steps in the same MapReduce job if you are so inclined);</p>
<pre>$ python nb_train.py &lt;your_data_folder&gt;/blogposts/ <br/> --output-dir=&lt;your_data_folder&gt;/models/ --no-output
</pre>
<p>The output directory is a folder that will store a file containing the output from this MapReduce job, which will be the probabilities we need to run our Naive Bayes classifier.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p>We can now actually run the Naive Bayes classifier using these probabilities. We will do this in a Jupyter Notebook, although this processing itself can be transferred to a mrjob package to be performed at scale.</p>
<p>First, take a look at the <kbd>models</kbd> folder that was specified in the last MapReduce job. If the output was more than one file, we can merge the files by just appending them to each other using a command line function from within the <kbd>models</kbd> directory:</p>
<pre>cat * &gt; model.txt
</pre>
<p>If you do this, you'll need to update the following code with <kbd>model.txt</kbd> as the model filename.</p>
<p>Back to our Notebook, we first import some standard imports we need for our script:</p>
<pre>import os <br/>import re<br/>import numpy as np <br/>from collections import defaultdict <br/>from operator import itemgetter
</pre>
<p>We again redefine our word search regular expression—if you were doing this in a real application, I recommend centralizing the functionality. It is important that words are extracted in the same way for both training and testing:</p>
<pre>word_search_re = re.compile(r"[w']+")
</pre>
<div>Next, we create the function that loads our model from a given filename.&#160;The model parameters will take the form of a dictionary of dictionaries, where the first key is a word, and the inner dictionary maps each gender to a probability. We use <kbd>defaultdicts</kbd>, which will return zero if a value isn't present;</div>
<pre>def load_model(model_filename):<br/>    model = defaultdict(lambda: defaultdict(float))<br/>    with open(model_filename) as inf: <br/>        for line in inf:<br/>            word, values = line.split(maxsplit=1) <br/>            word = eval(word) <br/>            values = eval(values)<br/>            model[word] = values<br/>    return model
</pre>
<p>The line is split into two sections, separated by whitespace. The first is the word itself and the second is a dictionary of probabilities. For each, we run <kbd>eval</kbd> on them to get the actual value, which was stored using <kbd>repr</kbd> in the previous code.</p>
<p>Next, we load our actual model. You may need to change the model filename—it will be in the output dir of the last MapReduce job;</p>
<pre>model_filename = os.path.join(os.path.expanduser("~"), "models", "part-00000") <br/>model = load_model(model_filename)
</pre>
<p>As an example, we can see the difference in usage of the word <em>i</em> (all words are turned into lowercase in the MapReduce jobs) between males and females:</p>
<pre>model["i"]["male"], model["i"]["female"]
</pre>
<p>Next, we create a function that can use this model for prediction. We won't use the scikit-learn interface for this example, and just create a function instead. Our function takes the model and a document as the parameters and returns the most likely gender:</p>
<pre>def nb_predict(model, document):<br/>    probabilities = defaultdict(lambda : 1)<br/>    words = word_search_re.findall(document)<br/>    for word in set(words): <br/>        probabilities["male"] += np.log(model[word].get("male", 1e-15)) <br/>        probabilities["female"] += np.log(model[word].get("female", 1e-15))<br/>        most_likely_genders = sorted(probabilities.items(), key=itemgetter(1), reverse=True) <br/>    return most_likely_genders[0][0]
</pre>
<p>It is important to note that we used <kbd>np.log</kbd> to compute the probabilities. Probabilities in Naive Bayes models are often quite small. Multiplying small values, which is necessary for many statistical values, can lead to an underflow error where the computer's precision isn't good enough and just makes the whole value 0. In this case, it would cause the likelihoods for both genders to be zero, leading to incorrect predictions.</p>
<p>To get around this, we use log probabilities. For two values a and b, <em>log(a×&#160;b)</em> is equal to <em>log(a) + log(b)</em>. The log of a small probability is a negative value, but a relatively large one. For instance, log(0.00001) is about -11.5. This means that rather than multiplying actual probabilities and risking an underflow error, we can sum the log probabilities and compare the values in the same way (higher numbers still indicate a higher likelihood).</p>
<div class="packt_tip">If you want to obtain probabilities back from the log probabilities, make sure to undo the log operation by using e to the power of the value you are interested in. To revert -11.5 into the probability, take e<sup>-11.5</sup>, which equals 0.00001 (approximately).</div>
<p>One problem with using log probabilities is that they don't handle zero values well (although, neither does multiplying by zero probabilities). This is due to the fact that log(0) is undefined. In some implementations of Naive Bayes, a 1 is added to all counts to get rid of this, but there are other ways to address this. This is a simple form of smoothing of the values. In our code, we just return a very small value if the word hasn't been seen for our given gender.</p>
<div class="packt_tip">Adding one to all counts above is a form of smoothing. Another option is to initialise to a very small value, such as 10<sup>-16</sup> - as long as its not exactly 0!</div>
<p>Back to our prediction function, we can test this by copying a post from our dataset:</p>
<pre>new_post = """ Every day should be a half day. Took the afternoon off to hit the dentist, and while I was out I managed to get my oil changed, too. Remember that business with my car dealership this winter? Well, consider this the epilogue. The friendly fellas at the Valvoline Instant Oil Change on Snelling were nice enough to notice that my dipstick was broken, and the metal piece was too far down in its little dipstick tube to pull out. Looks like I'm going to need a magnet. Damn you, Kline Nissan, daaaaaaammmnnn yooouuuu.... Today I let my boss know that I've submitted my Corps application. The news has been greeted by everyone in the company with a level of enthusiasm that really floors me. The back deck has finally been cleared off by the construction company working on the place. This company, for anyone who's interested, consists mainly of one guy who spends his days cursing at his crew of Spanish-speaking laborers. Construction of my deck began around the time Nixon was getting out of office.<br/>"""
</pre>
<p>We then predict with the following code:</p>
<pre>nb_predict(model, new_post)
</pre>
<p>The resulting prediction, <em>male</em>, is correct for this example. Of course, we never test a model on a single sample. We used the file starting with 51 for training this model. It wasn't many samples, so we can't expect too high of an accuracy.</p>
<p>The first thing we should do is train on more samples. We will test on any file that starts with a 6 or 7 and train on the rest of the files.</p>
<p>In the command line and in your data folder (<kbd>cd &lt;your_data_folder&gt;</kbd>), where the blogs folder exists, create a copy of the <span class="packt_screen">blogs</span> folder into a new folder.</p>
<p><span>Make a folder for our training set:</span></p>
<pre>mkdir blogs_train
</pre>
<p>Move any file starting with a 6 or 7 into the test set, from the train set:</p>
<pre>cp blogs/4* blogs_train/ <br/>cp blogs/8* blogs_train/
</pre>
<p><span>Then, make a folder for our test set:</span></p>
<pre>mkdir blogs_test
</pre>
<p>Move any file starting with a 6 or 7 into the test set, from the train set:</p>
<pre>cp blogs/6* blogs_test/ <br/>cp blogs/7* blogs_test/
</pre>
<p>We will rerun the blog extraction on all files in the training set. However, this is a large computation that is better suited to cloud infrastructure than our system. For this reason, we will now move the parsing job to Amazon's infrastructure.</p>
<p>Run the following on the command line, as you did before. The only difference is that we train on a different folder of input files. Before you run the following code, delete all files in the blog posts and models folders:</p>
<pre>$ python extract_posts.py ~/Data/blogs_train --output-dir=/home/bob/Data/blogposts_train --no-output
</pre>
<p>Next up comes the training of our Naive Bayes model. &#160;The code here will take quite a bit longer to run. Many, many hours. You may want to skip running this locally, unless you have a really powerful system! If you do want to skip, head to the next section.</p>
<pre>$ python nb_train.py ~/Data/blogposts_train/ --output-dir=/home/bob/models/ --no-output
</pre>
<p>We will test on any blog file in our test set. To get the files, we need to extract them. We will use the <kbd>extract_posts.py</kbd> MapReduce job, but store the files in a separate folder:</p>
<pre>python extract_posts.py ~/Data/blogs_test --output-dir=/home/bob/Data/blogposts_test --no-output
</pre>
<p>Back in the Jupyter Notebook, we list all the outputted testing files:</p>
<pre>testing_folder = os.path.join(os.path.expanduser("~"), "Data", "blogposts_testing") <br/>testing_filenames = [] <br/>for filename in os.listdir(testing_folder): <br/>    testing_filenames.append(os.path.join(testing_folder, filename))
</pre>
<p>For each of these files, we extract the gender and document and then call the predict function. We do this in a generator, as there are a lot of documents, and we don't want to use too much memory. The generator yields the actual gender and the predicted gender:</p>
<pre>def nb_predict_many(model, input_filename): <br/>    with open(input_filename) as inf: # remove leading and trailing whitespace <br/>    for line in inf: <br/>        tokens = line.split() <br/>        actual_gender = eval(tokens[0]) <br/>        blog_post = eval(" ".join(tokens[1:])) <br/>        yield actual_gender, nb_predict(model, blog_post)
</pre>
<p>We then record the predictions and actual genders across our entire dataset. Our predictions here are either male or female. In order to use the <kbd>f1_score</kbd> function from scikit-learn, we need to turn these into ones and zeroes. In order to do that, we record a 0 if the gender is male and 1 if it is female. To do this, we use a Boolean test, seeing if the gender is female. We then convert these Boolean values to <kbd>int</kbd> using NumPy:</p>
<pre>y_true = []<br/>y_pred = [] <br/>for actual_gender, predicted_gender in nb_predict_many(model, testing_filenames[0]):                    <br/>    y_true.append(actual_gender == "female")   <br/>    y_pred.append(predicted_gender == "female") <br/>    y_true = np.array(y_true, dtype='int') <br/>    y_pred = np.array(y_pred, dtype='int')
</pre>
<p>Now, we test the quality of this result using the F1 score in scikit-learn:</p>
<pre>from sklearn.metrics import f1_score <br/>print("f1={:.4f}".format(f1_score(y_true, y_pred, pos_label=None)))
</pre>
<p>The result of 0.78 is quite reasonable. We can probably improve this by using more data, but to do that, we need to move to a more powerful infrastructure that can handle it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Training on Amazon's EMR infrastructure</h1>
            </header>

            <article>
                
<p>We are going to use Amazon's <strong>Elastic Map Reduce</strong> (<strong>EMR</strong>) infrastructure to run our parsing and model building jobs.</p>
<p>In order to do that, we first need to create a bucket in Amazon's storage cloud. To do this, open the Amazon S3 console in your web browser by going to <a href="http://console.aws.amazon.com/s3">http://console.aws.amazon.com/s3</a> and click on <span class="packt_screen">Create Bucket</span>. Remember the name of the bucket, as we will need it later.</p>
<p>Right-click on the new bucket and select Properties. Then, change the permissions, granting everyone full access. This is not a good security practice in general, and I recommend that you change the access permissions after you complete this chapter. You can use advanced permissions in Amazon's services to give your script access and also protect against third parties viewing your data.</p>
<p>Left-click the bucket to open it and click on Create Folder. Name the folder <span class="packt_screen">blogs_train</span>. We are going to upload our training data to this folder for processing on the cloud.</p>
<p>On your computer, we are going to use Amazon's AWS CLI, a command-line interface for processing on Amazon's cloud.</p>
<p>To install it, use the following:</p>
<pre>sudo pip install awscli
</pre>
<p>Follow the instructions at <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html">http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html</a> to set the credentials for this program.</p>
<p>We now want to upload our data to our new bucket. First, we want to create our dataset, which is all the blogs not starting with a 6 or 7. There are more graceful ways to do this copy, but none are cross-platform enough to recommend. Instead, simply copy all the files and then delete the ones that start with a 6 or 7, from the training dataset:</p>
<pre>cp -R ~/Data/blogs ~/Data/blogs_train_large <br/>rm ~/Data/blogs_train_large/blogs/6* <br/>rm ~/Data/blogs_train_large/blogs/7*
</pre>
<p>Next, upload the data to your Amazon S3 bucket. Note that this will take some time and use quite a lot of upload data (several hundred megabytes). For those with slower internet connections, it may be worth doing this at a location with a faster connection;</p>
<pre>aws s3 cp ~/Data/blogs_train_large/ s3://ch12/blogs_train_large --recursive --exclude "*" <br/>--include "*.xml"
</pre>
<p>We are going to connect to Amazon's EMR (Elastic Map Reduce) using mrjob—it handles the whole thing for us; it only needs our credentials to do so. Follow the instructions at <a href="https://pythonhosted.org/mrjob/guides/emr-quickstart.html">https://pythonhosted.org/mrjob/guides/emr-quickstart.html</a> to setup mrjob with your Amazon credentials.</p>
<p>After this is done, we alter our mrjob run, only slightly, to run on Amazon EMR. We just tell mrjob to use <span class="packt_screen">emr</span> using the <span class="packt_screen">-r</span> switch and then set our s3 containers as the input and output directories. Even though this will be run on Amazon's infrastructure, it will still take quite a long time to run,&#160;as the default settings for mrjob use a single, low powered computer.</p>
<pre>$ python extract_posts.py -r emr s3://ch12gender/blogs_train_large/ <br/>--output-dir=s3://ch12/blogposts_train/ --no-output <br/>$ python nb_train.py -r emr s3://ch12/blogposts_train/ --output-dir=s3://ch12/model/ --o-output
</pre>
<div class="packt_infobox">You will be charged for the usage of both S3 and EMR. This will only be a few dollars, but keep this in mind if you are going to keep running the jobs or doing other jobs on bigger datasets. I ran a very large number of jobs and was charged about $20 all up. Running just these few should be less than $4. However, you can check your balance and set up pricing alerts, by going to <a href="https://console.aws.amazon.com/billing/home">https://console.aws.amazon.com/billing/home</a></div>
<p>It isn't necessary for the <span class="packt_screen">blogposts_train</span> and model folders to exist—they will be created by EMR. In fact, if they exist, you will get an error. If you are rerunning this, just change the names of these folders to something new, but remember to change both commands to the same names (that is, the output directory of the first command is the input directory of the second command).</p>
<div class="packt_tip">If you are getting impatient, you can always stop the first job after a while and just use the training data gathered so far. I recommend leaving the job for an absolute minimum of 15 minutes and probably at least an hour. You can't stop the second job and get good results though; the second job will probably take about two to three times as long as the first job did.</div>
<p>If you have the ability to purchase more advanced hardware, mrjob supports the creation of clusters on Amazon's infrastructure and also the ability to use more powerful computing hardware. You can run a job on a cluster of machines by specifying the type and number at the command line. For instance, to use 16 c1.medium computers to extract the text, run the following command:</p>
<pre>$ python extract_posts.py -r emr s3://chapter12/blogs_train_large/blogs/ --output-dir=s3://chapter12/blogposts_train/ --no-output  --instance-type c1.medium --num-core-instances 16
</pre>
<div class="packt_tip">In addition, you can create clusters separately and reattach jobs to those clusters. See mrjob's documentation at&#160;<a href="https://pythonhosted.org/mrjob/guides/emr-advanced.html">https://pythonhosted.org/mrjob/guides/emr-advanced.html</a> for more information on this process. Keep in mind that more advanced options become an interaction between advanced features of mrjob and advanced features of Amazon's AWS infrastrucutre, meaning you will need to investigate both technologies to get high-powered processing. Keep in mind that if you run more instances of more powerful hardware, you will be charged more in turn.</div>
<p>You can now go back to the s3 console and download the output model from your bucket. Saving it locally, we can go back to our Jupyter Notebook and use the new model. We reenter the code here—only the differences are highlighted, just to update to our new model:</p>
<pre>ws_model_filename = os.path.join(os.path.expanduser("~"), "models", "aws_model")<br/>aws_model = load_model(aws_model_filename) <br/>y_true = [] <br/>y_pred = [] <br/>for actual_gender, predicted_gender in nb_predict_many(aws_model, testing_filenames[0]):<br/>    y_true.append(actual_gender == "female") <br/>    y_pred.append(predicted_gender == "female") <br/>y_true = np.array(y_true, dtype='int') <br/>y_pred = np.array(y_pred, dtype='int') <br/>print("f1={:.4f}".format(f1_score(y_true, y_pred, pos_label=None)))
</pre>
<p>The result is better with the extra data, at 0.81.</p>
<div class="packt_tip">If everything went as planned, you may want to remove the bucket from Amazon S3—you will be charged for the storage.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at running jobs on big data. By most standards, our dataset is actually quite small—only a few hundred megabytes. Many industrial datasets are much bigger, so extra processing power is needed to perform the computation. In addition, the algorithms we used can be optimized for different tasks to further increase the scalability.</p>
<p>Our approach extracted word frequencies from blog posts, in order to predict the gender of the author of a document. We extracted the blogs and word frequencies using MapReduce-based projects in mrjob. With those extracted, we can then perform a Naive Bayes-esque computation to predict the gender of a new document.</p>
<p>We only scratched the surface of what you can do with MapReduce, and we did not even use it to its full potential for this application. To take the lessons further, convert the prediction function to a MapReduce job. That is, you train the model on MapReduce to obtain a model, and you run the model with MapReduce to get a list of predictions. Extend this by also doing your evaluation in MapReduce, with the final result coming back as simply the F1-score!</p>
<p>We can use the mrjob library to test locally and then automatically set up and use Amazon's EMR cloud infrastructure. You can use other cloud infrastructure or even a custom built Amazon EMR cluster to run these MapReduce jobs, but there is a bit more tinkering needed to get them running.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>