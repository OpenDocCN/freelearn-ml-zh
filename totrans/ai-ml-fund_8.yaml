- en: '>'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is included to assist the students to perform the activities in
    the book. It includes detailed steps that are to be performed by the students
    to achieve the objectives of the activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 1: Principles of AI'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the code, backslash (\) indicates a line break, where the code does not fit
    a line. A backslash at the end of the line escapes the newline character. This
    means that the content in the line following the backslash should be read as if
    it started where the backslash character is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Generating All Possible Sequences of Steps in the tic-tac-toe Game'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section will explore the combinatoric explosion possible when two players
    play randomly. We will be using a program, building on the previous results that
    generate all possible sequences of moves between a computer player and a human
    player. Determine the number of different wins, losses, and draws in terms of
    action sequences. Assume that the human player may make any possible move. In
    this example, given that the computer player is playing randomly, we will examine
    the wins, losses, and draws belonging to two randomly playing players:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function that maps the `all_moves_from_board` function on each element
    of a list of boards. This way, we will have all of the nodes of a decision tree
    in each depth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The decision tree starts with `[ EMPTY_SIGN * 9 ]` , and expands after each
    move:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a `filter_wins` function that takes the ended games out from
    the list of moves and appends them in an array containing the board states won
    by the AI player and the opponent player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this function, the three lists can be considered as reference types. This
    means that the function does not return a value, instead but it manipulating these
    three lists without returning them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s finish this section. Then with a `count_possibilities` function that
    prints the number of decision tree leaves that ended with a draw, won by the first
    player, and won by the second player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have up to 9 steps in each state. In the 0th, 2nd, 4th, 6th, and 8th iteration,
    the AI player moves. In all other iterations, the opponent moves. We create all
    possible moves in all steps and take out the ended games from the move list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then execute the number of possibilities to experience the combinatoric explosion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the tree of board states consists of 266,073 leaves. The `count_possibilities`
    function essentially implements a breadth first search algorithm to traverse all
    the possible states of the game. Notice that we do count these states multiple
    times, because placing an X on the top-right corner on step 1 and placing an X
    on the top-left corner on step 3 leads to similar possible states as starting
    with the top-left corner and then placing an X on the top-right corner. If we
    implemented a detection of duplicate states, we would have to check less nodes.
    However, at this stage, due to the limited depth of the game, we omit this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 2: AI with Search Techniques and Games'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 2: Teach the agent realize situations when it defends against losses'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a function `player_can_win` such that it takes all moves from the board
    using the `all_moves_from_board` function and iterates over it using a variable
    `next_move` . On each iteration, it checks if the game can be won by the sign,
    then it return true else false.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will extend the AI move such that it prefers making safe moves. A move is
    safe if the opponent cannot win the game in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can test our new application. You will find the AI has made the correct
    move.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now place this logic in the state space generator and check how well
    the computer player is doing by generating all the possible games.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now place this logic in the state space generator and check how well
    the computer player is doing by generating all the possible games.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Count the possibilities that as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are doing better than before. We not only got rid of almost 2/3 of possible
    games again, but most of the time, the AI player either wins or settles for a
    draw. Despite our effort to make the AI better, it can still lose in 962 ways.
    We will eliminate all these losses in the next activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3: Fix the first and second moves of the AI to make it invincible'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Follow these steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: We will count the number of empty fields in the board and make a hard-coded
    move in case there are 9 or 7 empty fields. You can experiment with different
    hard coded moves. We found that occupying any corner, then occupying the opposite
    corner leads to no losses. If the opponent occupied the opposite corner, making
    a move in the middle results in no losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's verify the state space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After fixing the first two steps, we only need to deal with 8 possibilities
    instead of 504\. We also guided the AI into a state, where the hard-coded rules
    were sufficient for never losing a game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fixing the steps is not important because we would give the AI hard coded steps
    to start with, but it is important, because it is a tool to evaluate and compare
    each step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After fixing the first two steps, we only need to deal with 8 possibilities
    instead of 504\. We also guided the AI into a state, where the hard-coded rules
    were sufficient for never losing a game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Activity 4: Connect Four'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section will practice using the `EasyAI` library and develop a heuristic.
    We will be using connect four game. The game board is seven cells wide and cells
    high. When you make a move, you can only select the column in which you drop your
    token. Then gravity pulls the token down to the lowest possible empty cell. Your
    objective is to connect four of your own tokens horizontally, vertically, or diagonally,
    before your opponent does this, or you run out of empty spaces. The rules of the
    game can be found at: [https://en.wikipedia.org/wiki/Connect_Four](https://en.wikipedia.org/wiki/Connect_Four)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up the TwoPlayersGame framework:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can leave a few functions from the definition intact. We have to implement
    the following methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will reuse the basic scoring function from tic-tac-toe. Once you test out
    the game, you will see that the game is not unbeatable, but plays surprisingly
    well, even though we are only using basic heuristics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s write the init method. We will define the board as a one-dimensional
    list, similar to the tic-tac-toe example. We could use a two-dimensional list
    too, but modeling will not get much easier or harder. Beyond making initializations
    like we did in the tic-tac-toe game, we will work a bit ahead. We will generate
    all of the possible winning combinations in the game and save them for future
    use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's handle the moves. The possible moves function is a simple enumeration.
    Notice we are using column indices from 1 to 7 in the move names, because it is
    more convenient to start column indexing with 1 in the human player interface
    than with zero. For each column, we check if there is an unoccupied field. If
    there is one, we will make the column a possible move.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Making a move is similar to the possible moves function. We check the column
    of the move, and find the first empty cell starting from the bottom. Once we find
    it, we occupy it. You can also read the implementation of the dual of the make_move
    function: unmake_move. In the unmake_move function, we check the column from top
    to down, and we remove the move at the first non-empty cell. Notice we rely on
    the internal representation of easyAi so that it does not undo moves that it had
    not made. Otherwise, this function would remove a token of the other player without
    checking whose token got removed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we already have the tuples that we have to check, we can mostly reuse the
    lose function from the tic-tac-toe example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our last task is the show method that prints the board. We will reuse the tic-tac-toe
    implementation, and just change the variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that all functions are complete, you can try out the example. Feel free
    to play a round or two against the opponent. You can see that the opponent is
    not perfect, but it plays reasonably well. If you have a strong computer, you
    can increase the parameter of the Negamax algorithm. I encourage you to come up
    with a better heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 3: Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Activity 5: Predicting Population'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You are working at the government office of Metropolis, trying to forecast
    the need for elementary school capacity. Your task is to figure out a 2025 and
    2030 prediction for the number of children starting elementary school. Past data
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00073.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 Data of Elementary School
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Plot tendencies on a two-dimensional chart. Use linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Our features are the years ranging from 2001 to 2018\. For simplicity, we can
    indicate 2001 as year 1, and 2018 as year 18.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Use np.polyfit to determine the coefficients of the regression line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Plot the results using matplotlib.pyplot to determine future tendencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 6: Stock Price Prediction with Quadratic and Cubic Linear Polynomial
    Regression with Multiple Variables'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss how to perform linear, polynomial, and support vector
    regression with scikit-learn. We will also learn to predict the best fit model
    for a given task. We will be assuming that you are a software engineer at a financial
    institution and your employer wants to know whether linear regression, or support
    vector regression is a better fit for predicting stock prices. You will have to
    load all data of the S&P 500 from a data source. Then build a regressor using
    linear regression, cubic polynomial linear regression, and a support vector regression
    with a polynomial kernel of degree 3\. Then separate training and test data. Plot
    the test labels and the prediction results and compare them with the `y` =`x`
    line. And finally, compare how well the three models score.
  prefs: []
  type: TYPE_NORMAL
- en: Let's load the S&P 500 index data using `Quandl` , then prepare the data for
    prediction. You can read the process in the Predicting the Future section of the
    topic Linear Regression with Multiple Variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's first use a polynomial of degree 1 for the evaluation of the model and
    for the prediction. We are still recreating the main example from the second topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output always depends on the test data, so the values may differ after each
    run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/Image00074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3.22: Graph showing the output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The closer the dots are to the y=x line, the less error the model works with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The model is performing surprisingly well on test data. Therefore, we can already
    suspect our polynomials are overfitting for scenarios used in training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: We will now perform a Support Vector regression with a polynomial kernel of
    degree 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/Image00075.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3.23: Graph showing the output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output will be `0.06388628722032952` .
  prefs: []
  type: TYPE_NORMAL
- en: We will now perform a Support Vector regression with a polynomial kernel of
    degree 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 4: Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 7: Preparing Credit Data for Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss how to prepare data for a classifier. We will be using
    german.data from [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    , as an example and prepare the data for training and testing a classifier. Make
    sure all your labels are numeric, and the values are prepared for classification.
    Use 80% of the data points as training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save german.data from [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    , and open it in a text editor like Sublime Text or Atom. Add the following first
    row to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the data file using pandas and replace NA values with an outlier value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform label encoding. We need to transform all labels in the data frame to
    integers. We could create all labels in a one dimensional array. However, this
    would be highly ineffective, because each label occurs in exactly one column.
    It makes a lot more sense to group our labels per column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a label encoder for each column and encode the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify that we did everything correctly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: All the 21 columns are available, and the label encoders have been saved in
    an object too. Our data are now pre-processed.
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to save these label encoders if you don't wish to decode the
    encoded values. We just saved them for the sake of completeness.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is time to separate features from labels. We can apply the same method as
    the one we saw in the theory section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our features are not yet scaled. This is a problem, because the credit amount
    distances can be significantly higher than the differences in age for instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We must perform scaling of the training and testing data together, therefore,
    the latest step when we can still perform scaling is before we split training
    data from testing data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use a Min-Max scaler from scikit''s Preprocessing library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The final step is cross-validation. We will shuffle our data, and use 80% of
    all data for training, 20% for testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Activity 8: Increase the accuracy of credit scoring'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section will learn how the parametrization of the k-nearest neighbor classifier
    affects the end result. The accuracy of credit scoring is currently quite low:
    66.5%. Find a way to increase it by a few percentage points. And to ensure that
    it happens correctly, you will need to do the previous exercises.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to accomplish this exercise. In this solution, I will show
    you one way to increase the credit score by changing the parametrization.
  prefs: []
  type: TYPE_NORMAL
- en: You must have completed Exercise 13, to be able to complete this activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase the K-value of the k-nearest neighbor classifier from the default
    5 to 10, 15, 25, and 50\. Evaluate the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running these lines for all four `n_neighbors` values, I got the following
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Higher K values do not necessarily mean better score. In this example though,
    `K=50` yielded a better result than `K=5` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Activity 9: Support Vector Machine Optimization in scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss how to use the different parameters of a Support Vector
    Machine classifier. We will be using comparing and contrasting the different support
    vector regression classifier parameters you learned and find a set of parameters
    resulting in the highest classification data on the training and testing data
    loaded and prepared in previous activity. And to ensure that it happens correctly,
    you will need to have completed the previous activities and exercises.
  prefs: []
  type: TYPE_NORMAL
- en: We will try out a few combinations. You may choose different parameters, that
  prefs: []
  type: TYPE_NORMAL
- en: Linear kernel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Polynomial kernel of degree 4, C=2, gamma=0.05
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows: 0.705.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Polynomial kernel of degree 4, C=2, gamma=0.25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows: 0.76.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Polynomial kernel of degree 4, C=2, gamma=0.5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows: 0.72.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sigmoid kernel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows: 0.71.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Default kernel with a gamma of 0.15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows: 0.76.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Chapter 5: Using Trees for Predictive Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 10: Car Data Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section will discuss how to build a reliable decision tree model capable
    of aiding your company in finding cars clients are likely to buy. We will be assuming
    that you are employed by a car rental agency focusing on building a lasting relationship
    with its clients. Your task is to build a decision tree model classifying cars
    into one of four categories: unacceptable, acceptable, good, very good.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set can be accessed here: [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)
    . Click the Data Folder link to download the data set. Click the Data Set Description
    link to access the description of the attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the utility of your decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the car data file from here: [https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data)
    . Add a header line to the front of the CSV file to reference it in Python more
    easily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Buying,Maintenance,Doors,Persons,LuggageBoot,Safety,Class`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We simply call the label Class. We named the six features after their descriptions
    in [https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names)
    .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Load the data set into Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check if the data got loaded correctly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As classification works with numeric data, we have to perform label encoding
    as seen in previous chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s separate features from labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is time to separate training and testing data with the cross-validation
    (in newer versions model-selection) featue of scikit-learn. We will use 10% test
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the train_test_split method will be available in model_selection module,
    not in the cross_validation module starting in scikit-learn 0.20\. In previous
    versions, model_selection already contains the train_test_split method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have everything to build the decision tree classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the fit method is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see the parametrization of the decision tree classifier. There are quite
    a few options we could set to tweak the performance of the classifier model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s score our model based on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the point where your knowledge up until chapter 4 would take you on
    model evaluation. We will now go a bit further and create a deeper evaluation
    of the model based on the classification_report feature we learned in this topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model has been proven to be quite accurate. In case of such a high accuracy
    score, suspect the possibility of overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 11: Random Forest Classification for your Car Rental Company'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will optimize your classifier to satisfy your clients better when
    selecting future cars for your car fleet. We will be performing random forest
    and extreme random forest classification on your car dealership data set you worked
    on in Activity 1 of this chapter. Suggest further improvements to the model to
    improve the performance of the classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can reuse Steps 1 â€“ 5 of Activity 1\. The end of Step 5 looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are using IPython, your variables may already be accessible in your console.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's create a Random Forest and an Extremely Randomized Trees classifier and
    train the models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s estimate how well the two models perform on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for model 1 is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for model 1 is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also calculate the accuracy scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `extraTreesClassifier` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the random forest classifier is performing slightly better than
    the extra trees classifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a first optimization technique, let's see which features are more important
    and which features are less important. Due to randomization, removing the least
    important features may reduce the random noise in the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `extra_trees_classifier` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Both classifiers treats the third and the fifth attributes quite unimportant.
    We may not be sure about the fifth attribute, as the importance score is more
    than 5% in both models. However, we are quite certain that the third attribute
    is the least significant attribute in the decision. Let's see the feature names
    once again.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The least important feature is Doors. It is quite evident in hindsight: the
    number of doors doesn''t have as big of an influence in the car''s rating than
    the safety rating for instance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remove the third feature from the model and retrain the classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s compare how well the new models fare compared to the original ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Second Model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although we did improve a few percentage points, note that a direct comparison
    is not possible, because of following reasons. First, the train-test split selects
    different data for training and testing. A few badly selected data points may
    easily cause a few percentage point increase or decrease in the scores. Second,
    the way how we train the classifiers also has random elements. This randomization
    may also shift the performance of the classifiers a bit. Always use best judgement
    when interpreting results and measure your results multiple times on different
    train-test splits if needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s tweak the parametrization of the classifiers a bit more. The following
    set of parameters increase the F1 Score of the Random Forest Classifier to 97%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the same parameters on the Extra Trees Classifier, we also get surprisingly
    good results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Chapter 6: Clustering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 12: k-means Clustering of Sales Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will detect product sales that perform similarly in nature to recognize
    trends in product sales.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the Sales Transactions Weekly Dataset from this URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly)
    Perform clustering on the dataset using the k-means Algorithm. Make sure you prepare
    your data for clustering based on what you have learned in the previous chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Use the default settings for the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset using pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you examine the data in the CSV file, you can realize that the first column
    contains product id strings. These values just add noise to the clustering process.
    Also notice that for weeks 0 to 51, there is a W-prefixed label and a Normalized
    label. Using the normalized label makes more sense, so we can drop the regular
    weekly labels from the data set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our data points are normalized except for the min and max
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a k-means clustering model and fit the data points into 8 clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The labels belonging to each data point can be retrieved using the labels_ property.
    These labels determine the clustering of the rows of the original data frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Retrieve the center points and the labels from the clustering algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How are these labels beneficial?
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that in the original data frame, the product names are given. You can
    easily recognize that similar types of products sell similarly. There are also
    products that fluctuate a lot, and products that are seasonal in nature. For instance,
    if some products promoted fat loss and getting into shape, they tend to sell during
    the first half of the year, before the beach season.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Shape Recognition with the Mean Shift algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will learn how images can be clustered. We will be assuming that
    you are working for a company detecting human emotions from photos. Your task
    is to extract pixels making up a face in an avatar photo.
  prefs: []
  type: TYPE_NORMAL
- en: Create a clustering algorithm with Mean Shift to cluster pixels of images. Examine
    the results of the Mean Shift algorithm and check if any of the clusters contains
    a face when used on avatar images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then apply the k-means algorithm with a fixed default number of clusters: 8\.
    Compare your results with the Mean Shift clustering algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Select an image you would like to cluster and load the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We chose this image from the Author's Youtube channel:![](img/Image00076.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fig 7.13: An image with the Author''s picture'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The image size has been significantly reduced so that our algorithm would terminate
    more quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Transform the pixels into a data frame to perform clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Perform Mean Shift clustering on the image using scikit-learn. Note that this
    time we will skip normalization of the features, because proximity of the pixels
    and proximity of color components are represented in close to equal weight. The
    largest difference in pixels distance is 750, while the largest difference in
    a color component is 256.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The algorithm found the following two clusters:![](img/Image00077.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fig 7.14: Images after performing k-means Clustering'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The Mean Shift algorithm treated my skin and the yellow JavaScript and Destructuring
    text close enough to each other to form the same cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's use the k-means algorithm to formulate eight clusters on the same data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The 8 clusters are the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output for the first is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Images after performing K-means Clustering](img/Image00078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.15: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the second is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.16: Images after performing K-means Clustering](img/Image00079.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.16: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the third is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.17: Images after performing K-means Clustering](img/Image00080.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.17: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the fourth is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.18: Images after performing K-means Clustering](img/Image00081.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.18: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the fifth is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.19: Images after performing K-means Clustering](img/Image00082.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.19: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the sixth is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.20: Images after performing K-means Clustering](img/Image00083.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.20: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the seventh is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.21: Images after performing K-means Clustering](img/Image00084.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.21: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output for the eighth is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 7.22: Images after performing K-means Clustering](img/Image00085.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7.22: Images after performing k-means Clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the fifth cluster recognized my face quite well. The clustering
    algorithm indeed located data points that are close and contain similar colors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 7: Deep Learning with Neural Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 14: Written digit detection'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss how to provide more security for the cryptocurrency
    traders via the detection of hand-written digits. We will be using assuming that
    you are a software developer at a new Cryptocurrency trader platform. The latest
    security measure you are implementing requires the recognition of hand-written
    digits. Use the MNIST library to train a neural network to recognize digits. You
    can read more about this dataset on [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improve the accuracy of the model as much as possible. And to ensure that it
    happens correctly, you will need to complete the previous topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the dataset and format the input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set up the Tensorflow graph. Instead of the `sigmoid` function, we will now
    use the `relu` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Test the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By re-running the code segment responsible for training the data set, we can
    improve the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Second run: 0.5107'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Third run: 0.5276'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fourth run: 0.5683'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fifth run: 0.6002'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Sixth run: 0.6803'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Seventh run: 0.6989'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eighth run: 0.7074'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Ninth run: 0.713'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Tenth run: 0.7163'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Twentieth run: 0.7308'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Thirtieth run: 0.8188'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fortieth run: 0.8256'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fiftieth run: 0.8273'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'At the end of the fiftieth run, the improved confusion matrix looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Not a bad result. More than 8 out of 10 digits are accurately recognized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15 : Written Digit Detection with Deep Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss how deep learning improves the performance of your
    model. We will be assuming that your boss is not satisfied with the results you
    presented in previous activity and asks you to consider adding two hidden layers
    to your original model and determine whether new layers improve the accuracy of
    the model. And to ensure that it happens correctly, you will need to have knowledge
    of Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Execute the code of previous Activity and measure the accuracy of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the neural network by adding new layers. We will combine the `relu`
    and `softmax` activator functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Retrain the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluate the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calculating the accuracy score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is `0.4516` .
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy did not improve.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if further runs improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second run: 0.5216'
  prefs: []
  type: TYPE_NORMAL
- en: 'Third run: 0.5418'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fourth run: 0.5567'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fifth run: 0.564'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sixth run: 0.572'
  prefs: []
  type: TYPE_NORMAL
- en: 'Seventh run: 0.5723'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eighth run: 0.6001'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ninth run: 0.6076'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tenth run: 0.6834'
  prefs: []
  type: TYPE_NORMAL
- en: 'Twentieth run: 0.7439'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thirtieth run: 0.7496'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortieth run: 0.7518'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fiftieth run: 0.7536'
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterwards, we got the following results: 0.755, 0.7605, 0.7598, 0.7653'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: This deep neural network behaves even more chaotically than the single layer
    one. It took 600 iterations of 200 samples to get from an accuracy of 0.572 to
    0.5723\. Not long after this iteration, we jumped from 0.6076 to 0.6834 in that
    number of iterations.
  prefs: []
  type: TYPE_NORMAL
