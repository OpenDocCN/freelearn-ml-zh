<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Analyzing Image Content</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Operating on images using OpenCV-Python</li>
<li>Detecting edges</li>
<li>Histogram equalization</li>
<li>Detecting corners</li>
<li>Detecting SIFT feature points</li>
<li>Building a Star feature detector</li>
<li>Creating features using Visual Codebook and vector quantization</li>
<li>Training an image classifier using Extremely Random Forests</li>
<li>Building an object recognizer</li>
<li>Using LightGBM for image classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To go through the recipes in this chapter, you need the following files (available on GitHub):</p>
<ul>
<li><kbd><span>operating_on_images</span><span>.py</span></kbd></li>
<li><kbd>capri.jpg</kbd></li>
<li><kbd><span>edge_detector</span><span>.py</span></kbd></li>
<li><kbd><span>chair.jpg</span></kbd></li>
<li><kbd><span>histogram_equalizer</span><span>.py</span></kbd></li>
<li><kbd><span>sunrise.jpg</span></kbd></li>
<li><kbd><span>corner_detector</span><span>.py</span></kbd></li>
<li><kbd>box.png</kbd></li>
</ul>
<ul>
<li><kbd><span>feature_detector</span><span>.py</span></kbd></li>
<li><kbd>table.jpg</kbd></li>
<li><kbd><span>star_detector</span><span>.py</span></kbd></li>
<li><kbd><span>trainer</span><span>.py</span></kbd></li>
<li><kbd><span>object_recognizer</span><span>.py</span></kbd></li>
<li><kbd><span>LightgbmClassifier.py</span></kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing computer vision</h1>
                </header>
            
            <article>
                
<p><strong>Computer vision</strong> is a field that studies how to process, analyze, and understand the contents of visual data. In image content analysis, we use a lot of computer vision algorithms to build our understanding of the objects in the image. Computer vision covers various aspects of image analysis, such as object recognition, shape analysis, pose estimation, 3D modeling, visual search, and so on. Humans are really good at identifying and recognizing things around them! The ultimate goal of computer vision is to accurately model the human vision system using computers.</p>
<p>Computer vision consists of various levels of analysis. In low-level vision, we deal with pixel-processing tasks, such as <strong>edge detection</strong>, <strong>morphological processing</strong>, and <strong>optical flow</strong>. In middle-level and high-level vision, we deal with things such as <strong>object recognition</strong>, <strong>3D modeling</strong>, <strong>motion analysis</strong>, and various other aspects of visual data. As we go higher, we tend to delve deeper into the conceptual aspects of our visual system and try to extract a description of visual data, based on activities and intentions. One thing to note is that higher levels tend to rely on the outputs of the lower levels for analysis.</p>
<p>One of the most common questions here is this: how is computer vision different than image processing? <strong>Image processing</strong> studies image transformations at the pixel level. Both the input and output of an image processing system are images. Some common examples are edge detection, <strong>histogram equalization</strong>, and <strong>image compression</strong>. Computer vision algorithms heavily rely on image processing algorithms to perform their duties. In computer vision, we deal with more complex things that include understanding the visual data at a conceptual level. The reason for this is that we want to construct meaningful descriptions of the objects in the images. The output of a computer vision system is an interpretation of the 3D scene in the given image. This interpretation can come in various forms, depending on the task at hand.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating on images using OpenCV-Python</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we will use a library called <strong>Open Source Computer Vision Library</strong></span> (<strong>OpenCV</strong>)<span>, to analyze images. OpenCV is the world's most popular library for computer vision. As it has been highly optimized for many different platforms, it has become the de facto standard in the industry. Before you proceed, make sure that you install the library with Python support. You can download and install OpenCV at </span><a href="http://opencv.org"><span class="URLPACKT">http://opencv.org</span></a><span>. For detailed installation instructions on various operating systems, you can refer to the documentation section on the website.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will <span>take a look at how to operate on images using OpenCV-Python. In this recipe, we will look at how to load and display an image. We will also look at how to crop, resize, and save an image to an output file.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can operate on images using OpenCV-Python:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>operating_on_images</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2 </pre>
<ol start="2">
<li>Specify the input image as the first argument to the file, and read it using the image read function. We will use the <kbd>forest.jpg</kbd><span> file that is provided to you,</span> as follows:</li>
</ol>
<pre style="padding-left: 60px"># Load and display an image -- 'forest.jpg' 
input_file = sys.argv[1] 
img = cv2.imread(input_file) </pre>
<ol start="3">
<li>Display the input image, as follows:</li>
</ol>
<pre style="padding-left: 60px">cv2.imshow('Original', img)</pre>
<ol start="4">
<li>We will now crop this image. Extract the height and width of the input image, and then specify the boundaries:</li>
</ol>
<pre style="padding-left: 60px"># Cropping an image 
h, w = img.shape[:2] 
start_row, end_row = int(0.21*h), int(0.73*h) 
start_col, end_col= int(0.37*w), int(0.92*w) </pre>
<ol start="5">
<li>Crop the image using NumPy-style slicing and display it:</li>
</ol>
<pre style="padding-left: 60px">img_cropped = img[start_row:end_row, start_col:end_col] 
cv2.imshow('Cropped', img_cropped) </pre>
<ol start="6">
<li>Resize the image to <kbd>1.3</kbd> times its original size and display it:</li>
</ol>
<pre style="padding-left: 60px"># Resizing an image 
scaling_factor = 1.3 
img_scaled = cv2.resize(img, None, fx=scaling_factor,               fy=scaling_factor,  
interpolation=cv2.INTER_LINEAR) 
cv2.imshow('Uniform resizing', img_scaled) </pre>
<ol start="7">
<li>The previous method will uniformly scale the image on both dimensions. Let's assume that we want to skew the image based on specific output dimensions. We will use the following code:</li>
</ol>
<pre style="padding-left: 60px">img_scaled = cv2.resize(img, (250, 400), interpolation=cv2.INTER_AREA) 
cv2.imshow('Skewed resizing', img_scaled) </pre>
<ol start="8">
<li>Save the image to an output file:</li>
</ol>
<pre style="padding-left: 60px"># Save an image 
output_file = input_file[:-4] + '_cropped.jpg' 
cv2.imwrite(output_file, img_cropped) 
 
cv2.waitKey() </pre>
<p style="padding-left: 60px">The <kbd>waitKey()</kbd> function displays the images until you hit a key on the keyboard.</p>
<ol start="9">
<li>We will run the code in a Terminal window:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python operating_on_images.py capri.jpg</strong></pre>
<p style="padding-left: 60px">You will see the following four images on the screen (<em>Capri's Faraglioni (Italy)</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1107 image-border" src="assets/7512e226-cd6e-4f44-8a58-f924d96d324e.png" style="width:56.08em;height:33.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we learned how to<span> operate on images using the OpenCV-Python library. </span><span>The following tasks were performed:<br/></span></p>
<ul>
<li><span>Loading and displaying an image<br/></span></li>
<li><span>Cropping an image<br/></span></li>
<li><span>Resizing an image<br/></span></li>
<li><span>Saving an image</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>OpenCV is a free software library that was originally developed by Intel and the Nizhny Novgorod research center in Russia. Later, it was maintained by Willow Garage and is now maintained by Itseez. The programming language that's mainly used to develop with this library is C ++, but it is also possible to interface through C, Python, and Java.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of<span> the OpenCV library at <a href="http://opencv.org"><span class="URLPACKT">http://opencv.org</span></a></span></li>
<li>Refer to the OpenCV tutorials at <a href="https://docs.opencv.org/2.4/opencv_tutorials.pdf">https://docs.opencv.org/2.4/opencv_tutorials.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting edges</h1>
                </header>
            
            <article>
                
<p><strong>Edge detection</strong> is one of the most popular techniques in computer vision. It is used as a preprocessing step in many applications. With edge detection, you can mark points in a digital image where light intensity suddenly changes. The sudden changes in the properties of an image want to highlight important events or changes in the physical world of which the images are representations. These changes identify, for example, surface orientation discontinuities, depth discontinuities, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to <span>use different edge detectors </span><span>to detect edges in the input image.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can detect edges:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>edge_detector</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2 </pre>
<ol start="2">
<li>Load the input image. We will use <kbd>chair.jpg</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Load the input image -- 'chair.jpg' 
# Convert it to grayscale  
input_file = sys.argv[1] 
img = cv2.imread(input_file, cv2.IMREAD_GRAYSCALE)</pre>
<ol start="3">
<li>Extract the height and width of the image:</li>
</ol>
<pre style="padding-left: 60px">h, w = img.shape </pre>
<ol start="4">
<li>The <strong>Sobel filter</strong> is a type of edge detector that uses a 3 x 3 kernel to detect horizontal and vertical edges separately:</li>
</ol>
<pre style="padding-left: 60px">sobel_horizontal = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5) </pre>
<ol start="5">
<li>Run the vertical Sobel detector:</li>
</ol>
<pre style="padding-left: 60px">sobel_vertical = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5) </pre>
<ol start="6">
<li>The <strong>Laplacian edge detector</strong> detects edges in both directions. We use it as follows:</li>
</ol>
<pre style="padding-left: 60px">laplacian = cv2.Laplacian(img, cv2.CV_64F) </pre>
<ol start="7">
<li>Even though Laplacian addresses the shortcomings of Sobel, the output is still very noisy. The <strong>Canny edge detector</strong> outperforms all of them because of the way it treats the problem. It is a multistage process, and it uses hysteresis to come up with clean edges:</li>
</ol>
<pre style="padding-left: 60px">canny = cv2.Canny(img, 50, 240) </pre>
<ol start="8">
<li>Display all the output images:</li>
</ol>
<pre style="padding-left: 60px">cv2.imshow('Original', img) 
cv2.imshow('Sobel horizontal', sobel_horizontal) 
cv2.imshow('Sobel vertical', sobel_vertical) 
cv2.imshow('Laplacian', laplacian) 
cv2.imshow('Canny', canny) 
 
cv2.waitKey() </pre>
<ol start="9">
<li>We will run the code in the terminal window using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python edge_detector.py siracusa.jpg</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px"><span>You will see the following five images on the screen (<em>The ancient theatre of Siracusa (Italy)</em>):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1108 image-border" src="assets/308073ff-013c-4bb3-a8a5-cb2b1cac353e.png" style="width:132.08em;height:73.42em;"/></p>
<p style="padding-left: 60px">At the top of the screenshot is the original image, the horizontal Sobel edge detector output, and <span>the vertical Sobel edge detector output</span>. Note how the detected lines tend to be vertical. This is due to the fact that it's a horizontal edge detector, and it tends to detect changes in this direction. At the bottom of the screenshot is the Laplacian edge detector output and the <span>Canny edge detector, which detects all the edges nicely.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The Sobel operator is a differential operator, which calculates an approximate value of the gradient of a function that represents the brightness of the image. At each point in the image, the Sobel operator can correspond to the gradient vector or to the norm of that vector. The algorithm that's used by the Sobel operator is based on the convolution of the image with a filter, separated and of integer value, applied both in the vertical and horizontal direction, and is therefore economical in terms of the calculation power required.</span></p>
<p>The Laplacian edge detector is part of the zero-crossing methods that look for points where the second-order derivative goes through zero, which is usually the Laplacian function or a differential expression of a non-linear function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span>The Canny algorithm uses a multi-stage calculation method to find outlines of many of the types that are normally present in real images. To do this, the algorithm must identify and mark as many contours as possible in the image good location. Furthermore, the marked contours must be as close as possible to the real contours of the image. Finally, a given image contour must be marked only once, and if possible, the noise that's present in the image must not cause the detection of false contours.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Sobel Operator</em>: <a href="http://www.tutorialspoint.com/dip/sobel_operator.htm">http://www.tutorialspoint.com/dip/sobel_operator.htm</a></span></li>
<li><em>Laplacian edge detector</em>: <a href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm">http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm</a></li>
<li><em>Canny edge detector</em>: <a href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/canny.htm">http://homepages.inf.ed.ac.uk/rbf/HIPR2/canny.htm</a></li>
<li><em>Most Common Edge Detectors</em> (from the University of Minnesota): <a href="http://me.umn.edu/courses/me5286/vision/Notes/2015/ME5286-Lecture7.pdf">http://me.umn.edu/courses/me5286/vision/Notes/2015/ME5286-Lecture7.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Histogram equalization</h1>
                </header>
            
            <article>
                
<p><strong>Histogram equalization</strong> is the process of modifying the intensities of the image pixels to enhance the image's contrast. The human eye likes contrast! This is the reason why almost all camera systems use histogram equalization to make images look nice. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>The interesting thing is that the histogram equalization process is different for grayscale and color images. There's a catch when dealing with color images, and we'll see it in this recipe. Let's see how to do it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform histogram equalization:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>histogram_equalizer</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2   </pre>
<ol start="2">
<li>Load the input image. We will use the <kbd>sunrise.jpg</kbd> image:</li>
</ol>
<pre style="padding-left: 60px"># Load input image -- 'sunrise.jpg' 
input_file = sys.argv[1] 
img = cv2.imread(input_file) </pre>
<ol start="3">
<li>Convert the image into <kbd>grayscale</kbd> and display it:</li>
</ol>
<pre style="padding-left: 60px"># Convert it to grayscale 
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
cv2.imshow('Input grayscale image', img_gray) </pre>
<ol start="4">
<li>Equalize the histogram of the <kbd>grayscale</kbd> image and display it:</li>
</ol>
<pre style="padding-left: 60px"># Equalize the histogram 
img_gray_histeq = cv2.equalizeHist(img_gray) 
cv2.imshow('Histogram equalized - grayscale', img_gray_histeq) </pre>
<ol start="5">
<li> OpenCV loads images in the <kbd>BGR</kbd> format by default, so let's convert it from <kbd>BGR</kbd> into <kbd>YUV</kbd> first:</li>
</ol>
<pre style="padding-left: 60px"># Histogram equalization of color images 
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV) </pre>
<ol start="6">
<li>Equalize the Y channel, as follows:</li>
</ol>
<pre style="padding-left: 60px">img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0]) </pre>
<ol start="7">
<li>Convert it back into <kbd>BGR</kbd>:</li>
</ol>
<pre style="padding-left: 60px">img_histeq = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR) </pre>
<ol start="8">
<li>Display the input and output images:</li>
</ol>
<pre style="padding-left: 60px">cv2.imshow('Input color image', img) 
cv2.imshow('Histogram equalized - color', img_histeq) 
 
cv2.waitKey()</pre>
<ol start="9">
<li>We will run the code in a terminal window:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python histogram_equalizer.py gubbio.jpg</strong></pre>
<p style="padding-left: 60px"><span>You will see the following four images on the screen (<em>the medieval city of Gubbio (Italy)</em>):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1109 image-border" src="assets/e409691b-5e34-4d71-92a1-08eaa439ba53.png" style="width:103.50em;height:65.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Histogram equalization is a digital image processing method with which you can calibrate the contrast using the image histogram. Histogram equalization increases the general contrast of many images, particularly when the usable image data is represented by very close intensity values. With this adaptation, intensities can be better distributed on the histogram. In this way, the areas with low local contrast obtain a greater contrast. The equalization of the histogram is achieved by spreading most of the values of frequent intensity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span>To equalize the histogram of the color images, we need to follow a different procedure. Histogram equalization only applies to the intensity channel. An RGB image consists of three color channels, and we cannot apply the histogram equalization process on these channels separately. We need to separate the intensity information from the color information before we do anything. So, we convert it into a YUV colorspace first, equalize the Y channel, and then convert it back into RGB to get the output. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Contrast Enhancement</em> (from the Polytechnic University, Brooklyn): <a href="http://eeweb.poly.edu/~yao/EL5123/lecture3_contrast_enhancement.pdf">http://eeweb.poly.edu/~yao/EL5123/lecture3_contrast_enhancement.pdf</a></li>
<li><em>YUV Colorspace</em>: <a href="http://softpixel.com/~cwright/programming/colorspace/yuv/">http://softpixel.com/~cwright/programming/colorspace/yuv/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting corners</h1>
                </header>
            
            <article>
                
<p><strong>Corner detection</strong> is an important process in computer vision. It helps us identify the salient points in the image. This was one of the earliest feature extraction techniques that was used to develop image analysis systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to detect the corner of a box by placing markers at the points that are identified.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can detect corners:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>corner_detector</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2 
import numpy as np</pre>
<p class="mce-root"/>
<ol start="2">
<li>Load the input image. We will use <kbd>box.png</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Load input image -- 'box.png' 
input_file = sys.argv[1] 
img = cv2.imread(input_file) 
cv2.imshow('Input image', img) </pre>
<ol start="3">
<li>Convert the image into <kbd>grayscale</kbd> and cast it to floating-point values. We need the floating-point values for the corner detector to work:</li>
</ol>
<pre style="padding-left: 60px">img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
img_gray = np.float32(img_gray) </pre>
<ol start="4">
<li>Run the <kbd>Harris corner detector</kbd> function on the <kbd>grayscale</kbd> image:</li>
</ol>
<pre style="padding-left: 60px"># Harris corner detector  
img_harris = cv2.cornerHarris(img_gray, 7, 5, 0.04) </pre>
<ol start="5">
<li>To mark the corners, we need to dilate the image, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Resultant image is dilated to mark the corners 
img_harris = cv2.dilate(img_harris, None) </pre>
<ol start="6">
<li>Let's threshold the image to display the important points:</li>
</ol>
<pre style="padding-left: 60px"># Threshold the image  
img[img_harris &gt; 0.01 * img_harris.max()] = [0, 0, 0] </pre>
<ol start="7">
<li>Display the output image:</li>
</ol>
<pre style="padding-left: 60px">cv2.imshow('Harris Corners', img) 
cv2.waitKey() </pre>
<ol start="8">
<li>We will run the code in a terminal window:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python corner_detector.py box.png</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px"><span>You will see the following two images on the screen:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1110 image-border" src="assets/fb0530af-e029-465d-b6a7-900e99e5a7b3.png" style="width:99.08em;height:39.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Corner detection is an approach that's used in computer vision to extract types of features and infer the contents of the image. It is often used in motion detection, image recording, video tracking, image mosaicization, image panoramas creation, 3D modeling, and object recognition. It is a topic similar to the detection of points of interest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Corner detection methods can be subdivided into two groups:</p>
<ul>
<li>Techniques based on the extraction of the contours and the subsequent identification of the points corresponding to the maximum curvature, or where the edge segments intersect</li>
<li>Algorithms that search for corners directly from the intensity of the gray levels of the image pixels</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Harris Corner Detector</em> (from Penn State University): <a href="http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf">http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf</a></span></li>
<li><span>The official documentation of the </span>Harris corner detector: <a href="https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html">https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting SIFT feature points</h1>
                </header>
            
            <article>
                
<p><strong>Scale invariant feature transform</strong> (<strong>SIFT</strong>) is one of the most popular features in the field of computer vision. David Lowe first proposed this in his seminal paper. It has since become one of the most effective features to use for image recognition and content analysis. It is robust against scale, orientation, intensity, and so on. This forms the basis of our object recognition system. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to detect SIFT feature points.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can detect SIFT feature points:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>feature_detector</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2 
import numpy as np  </pre>
<ol start="2">
<li>Load the input image. We will use <kbd>table.jpg</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Load input image -- 'table.jpg' 
input_file = sys.argv[1] 
img = cv2.imread(input_file)</pre>
<ol start="3">
<li>Convert this image into grayscale:</li>
</ol>
<pre style="padding-left: 60px">img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) </pre>
<ol start="4">
<li>Initialize the SIFT detector object and extract the keypoints:</li>
</ol>
<pre style="padding-left: 60px">sift = cv2.xfeatures2d.SIFT_create() 
keypoints = sift.detect(img_gray, None) </pre>
<ol start="5">
<li>The keypoints are the salient points, but they are not the features. This basically gives us the location of the salient points. SIFT also functions as a very effective feature extractor.</li>
<li>Draw the keypoints on top of the input image, as follows:</li>
</ol>
<pre style="padding-left: 60px">img_sift = np.copy(img) 
cv2.drawKeypoints(img, keypoints, img_sift, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) </pre>
<ol start="7">
<li>Display the input and output images:</li>
</ol>
<pre style="padding-left: 60px">cv2.imshow('Input image', img) 
cv2.imshow('SIFT features', img_sift) 
cv2.waitKey() </pre>
<ol start="8">
<li>We will run this in a terminal window:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python feature_detector.py flowers.jpg</strong></pre>
<p style="padding-left: 60px"><span>You will see the following two images on the screen:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1111 image-border" src="assets/d879a540-7db8-4d25-bdfb-e9fcd02cef02.png" style="width:104.75em;height:40.50em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>For each object in an image, some interesting points are extracted to provide a description of the characteristics of the object. This feature, obtained from an image selected for training, is used to identify the object when trying to locate it in a test image that contains many other objects. To obtain a reliable recognition, the features that are extracted from the training image must be detectable, even with scale variations, noise, and lighting. These points are usually placed in high-contrast regions of the image, such as object contours.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>In Lowe's method, the key points of the SIFT objects are extracted from a set of reference images in the first phase and then they are stored in a database. The recognition of the object in a new image takes place by individually comparing each characteristic of the new image with the database that was obtained previously and looking for features based on the Euclidean distance of their feature vectors. From the complete set of matches in the new image, subsets of key points <span>are identified </span>that agree with the object and its position, scale, and orientation to filter the best matches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Introduction to SIFT (Scale-Invariant Feature Transform)</em>: <a href="https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html">https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html</a></li>
<li>The official documentation of the<span> <kbd>OpenCV.xfeatures2d.SIFT</kbd> function: <a href="https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html">https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html</a></span></li>
<li><em>Distinctive Image Features from Scale-Invariant Keypoints</em> (by David G Lowe from the University of British Columbia): <a href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Star feature detector</h1>
                </header>
            
            <article>
                
<p>The SIFT feature detector is good in many cases. However, when we build object recognition systems, we may want to use a different feature detector before we extract features using SIFT. This will give us the flexibility to cascade different blocks to get the best possible performance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will <span>use the<strong> S</strong></span><strong>tar</strong> <strong>feature detector</strong> to detect features fr<span>om an image.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can build a Star feature detector:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>star_detector</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import sys 
import cv2  </pre>
<ol start="2">
<li>Define a class to handle all the functions that are related to Star feature detection:</li>
</ol>
<pre style="padding-left: 60px">class StarFeatureDetector(object): 
    def __init__(self): 
        self.detector = cv2.xfeatures2d.StarDetector_create() </pre>
<ol start="3">
<li>Define a function to run the detector on the input image:</li>
</ol>
<pre>    def detect(self, img): 
        return self.detector.detect(img) </pre>
<ol start="4">
<li>Load the input image in the <kbd>main</kbd> function. We will use <kbd>table.jpg</kbd>:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    # Load input image -- 'table.jpg' 
    input_file = sys.argv[1] 
    input_img = cv2.imread(input_file) </pre>
<ol start="5">
<li>Convert the image into grayscale:</li>
</ol>
<pre>    # Convert to grayscale 
    img_gray = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY) </pre>
<ol start="6">
<li>Detect features using the Star feature detector:</li>
</ol>
<pre>    # Detect features using Star feature detector 
    keypoints = StarFeatureDetector().detect(input_img)</pre>
<ol start="7">
<li>Draw keypoints on top of the input image:</li>
</ol>
<pre>    cv2.drawKeypoints(input_img, keypoints, input_img,  
               flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) </pre>
<ol start="8">
<li>Display the output image:</li>
</ol>
<pre>    cv2.imshow('Star features', input_img) 
    cv2.waitKey() </pre>
<ol start="9">
<li>We will run the code in a terminal window:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python star_detector.py table.jpg</strong></pre>
<p style="padding-left: 60px"><span>You will see the following image on the screen:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1112 image-border" src="assets/f32de3b7-9417-4bc7-808e-ed23ada7d6d2.png" style="width:51.92em;height:39.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we learned how to use the OpenCV-Python library to build a Star feature detector. The following tasks were performed:</p>
<ul>
<li class="mce-root">Loading an image</li>
<li class="mce-root">Converting to grayscale</li>
<li class="mce-root">Detecting features using the Star feature detector</li>
<li class="mce-root">Drawing keypoints and displaying the image</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The <strong>Star function detector</strong> is based on <strong>CenSurE</strong> (<strong>Center Surrounded Extrema</strong>). The differences between the two detectors lie in the choice of polygons:</p>
<ul>
<li>CenSurE uses square, hexagons, and octagons as an alternative to the circle</li>
<li>Star approximates the circle with two superimposed squares: one vertical and one rotated 45 degrees</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of<span> OpenCV: <a href="https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=fast%20feature#StarFeatureDetector%20:%20public%20FeatureDetector">https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=fast%20feature#StarFeatureDetector%20:%20public%20FeatureDetector</a></span></li>
<li><span><em>CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching</em>, in Computer Vision–ECCV 2008 (pp. 102-115). Springer Berlin Heidelberg</span>: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1117&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1117&amp;rep=rep1&amp;type=pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating features using Visual Codebook and vector quantization</h1>
                </header>
            
            <article>
                
<p>To build an object recognition system, we need to extract feature vectors from each image. Each image needs to have a signature that can be used for matching. We use a concept called <strong>V</strong><strong>isual Codebook</strong> to build image signatures. This codebook is basically the dictionary that we will use to come up with a representation for the images in our training data<span> image signatures </span><span>set. We use <strong>vector quantization</strong> to cluster many feature points and come up with <strong>centroids</strong>. These centroids will serve as the elements of our Visual Codebook. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will create features using Visual Codebook and vector quantization. To build a robust object recognition system, you need tens of thousands of images. There is a dataset called<span> </span><kbd>Caltech256</kbd><span> </span>that's very popular in this field! It contains 256 classes of images, where each class contains thousands of samples. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can create features using Visual Codebook and vector quantization:</p>
<ol>
<li>This is a lengthy recipe, so we will only look at the important functions. The full code is given in the <kbd>build_features.py</kbd> file that is already provided for you. Let's look at the class that's defined to extract features:</li>
</ol>
<pre style="padding-left: 60px">class FeatureBuilder(object): </pre>
<ol start="2">
<li>Define a method to extract features from the input image. We will use the Star detector to get the keypoints and then use SIFT to extract descriptors from these locations:</li>
</ol>
<pre>    def extract_ features(self, img): 
        keypoints = StarFeatureDetector().detect(img) 
        keypoints, feature_vectors = compute_sift_features(img, keypoints) 
        return feature_vectors </pre>
<ol start="3">
<li>We need to extract centroids from all the descriptors:</li>
</ol>
<pre>    def get_codewords(self, input_map, scaling_size, max_samples=12): 
        keypoints_all = [] 
 
        count = 0 
        cur_label = '' </pre>
<ol start="4">
<li>Each image will give rise to a large number of descriptors. We will just use a small number of images because the centroids won't change much after this:</li>
</ol>
<pre>        for item in input_map: 
            if count &gt;= max_samples: 
                if cur_class != item['object_class']: 
                    count = 0 
            else: 
                continue 
 
        count += 1 </pre>
<ol start="5">
<li>The print progress is as follows:</li>
</ol>
<pre>        if count == max_samples: 
            print("Built centroids for", item['object_class']) </pre>
<ol start="6">
<li>Extract the current label:</li>
</ol>
<pre>        cur_class = item['object_class'] </pre>
<ol start="7">
<li>Read the image and resize it:</li>
</ol>
<pre>        img = cv2.imread(item['image_path']) 
        img = resize_image(img, scaling_size) </pre>
<ol start="8">
<li>Extract the features:</li>
</ol>
<pre>        feature_vectors = self.extract_image_features(img) 
        keypoints_all.extend(feature_vectors)  </pre>
<ol start="9">
<li>Use vector quantization to quantize the feature points. Vector quantization<strong> </strong>is the <em>N</em>-dimensional version of rounding off:</li>
</ol>
<pre>        kmeans, centroids = BagOfWords().cluster(keypoints_all) 
        return kmeans, centroids </pre>
<ol start="10">
<li>Define the class to handle the bag-of-words model and vector quantization:</li>
</ol>
<pre style="padding-left: 60px">class BagOfWords(object): 
    def __init__(self, num_clusters=32): 
        self.num_dims = 128 
        self.num_clusters = num_clusters 
        self.num_retries = 10</pre>
<p class="mce-root"/>
<ol start="11">
<li>Define a method to quantize the datapoints. We will use <strong>k-means clustering</strong> to achieve this:</li>
</ol>
<pre style="padding-left: 60px">def cluster(self, datapoints): 
    kmeans = KMeans(self.num_clusters,  
        n_init=max(self.num_retries, 1), 
        max_iter=10, tol=1.0) </pre>
<ol start="12">
<li>Extract the centroids, as follows:</li>
</ol>
<pre>    res = kmeans.fit(datapoints) 
    centroids = res.cluster_centers_ 
    return kmeans, centroids </pre>
<ol start="13">
<li>Define a method to normalize the data:</li>
</ol>
<pre style="padding-left: 60px">def normalize(self, input_data): 
    sum_input = np.sum(input_data) 
 
    if sum_input &gt; 0: 
        return input_data / sum_input 
    else: 
        return input_data </pre>
<ol start="14">
<li>Define a method to get the feature vector:</li>
</ol>
<pre style="padding-left: 60px">def construct_feature(self, img, kmeans, centroids): 
    keypoints = StarFeatureDetector().detect(img) 
    keypoints, feature_vectors = compute_sift_features(img, keypoints) 
    labels = kmeans.predict(feature_vectors) 
    feature_vector = np.zeros(self.num_clusters) </pre>
<ol start="15">
<li>Build a histogram and normalize it:</li>
</ol>
<pre style="padding-left: 60px">    for i, item in enumerate(feature_vectors): 
        feature_vector[labels[i]] += 1 
 
        feature_vector_img = np.reshape(feature_vector,  
((1, feature_vector.shape[0]))) 
        return self.normalize(feature_vector_img) </pre>
<ol start="16">
<li>Define a method, and then extract the SIFT features:</li>
</ol>
<pre style="padding-left: 60px"># Extract SIFT features 
def compute_sift_features(img, keypoints): 
    if img is None: 
        raise TypeError('Invalid input image') 
 
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    keypoints, descriptors = cv2.xfeatures2d.SIFT_create().compute(img_gray, keypoints) 
    return keypoints, descriptors </pre>
<p style="padding-left: 60px">As we mentioned earlier, please refer to <kbd>build_features.py</kbd> for the complete code. You should run the code in the following way:</p>
<pre style="padding-left: 60px"><strong>$ python build_features.py --data-folder /path/to/training_images/ --codebook-file codebook.pkl --feature-map-file feature_map.pkl</strong>
  </pre>
<p style="padding-left: 60px">This will generate two files called <kbd>codebook.pkl</kbd> and <kbd>feature_map.pkl</kbd>. We will use these files in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used a visual textbook as a dictionary, which we then used to create a representation for images in our image signatures, which are contained in the training set. So, we used vector quantization to group many characteristic points and create centroids. These centroids are served as elements of our visual textbook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>We extract the features from various points in the image, counting the frequency of the values of the extracted features and classifying the image based on the frequency found, which is a technique that's similar to the representation of a document in a vector space. It is a vector quantization process with which I create a dictionary to discretize the possible values of the feature space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Visual Codebook</em> (by Tae-Kyun Kim, from Sidney Sussex College): <a href="http://mi.eng.cam.ac.uk/~cipolla/lectures/PartIB/old/IB-visualcodebook.pdf">http://mi.eng.cam.ac.uk/~cipolla/lectures/PartIB/old/IB-visualcodebook.pdf</a></li>
<li><em>Caltech-256 images repository</em> (from the California Institute of Technology): <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">http://www.vision.caltech.edu/Image_Datasets/Caltech256/</a></li>
</ul>
<ul>
<li><em>Vector Quantization Overview</em> (from Binghamton University): <a href="http://www.ws.binghamton.edu/fowler/fowler%20personal%20page/EE523_files/Ch_10_1%20VQ%20Description%20(PPT).pdf">http://www.ws.binghamton.edu/fowler/fowler%20personal%20page/EE523_files/Ch_10_1%20VQ%20Description%20(PPT).pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training an image classifier using Extremely Random Forests</h1>
                </header>
            
            <article>
                
<p>An object recognition system uses an image classifier to classify the images into known categories. <strong>Extremely Random Forests</strong><span> (</span><strong>ERFs</strong><span>) </span>are very popular in the field of machine learning because of their speed and accuracy. This algorithm is based on decision trees. Their differences compared to classical decision trees are in the choice of the points of division of the tree. The best division to separate the samples of a node into two groups is done by creating random subdivisions for each of the randomly selected features and choosing the best division between those.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will <span>use </span>ERFs<span> to train our image classifier. We basically construct decision trees based on our image signatures, and then train the forest to make the right decision. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can train an image classifier using ERFs:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>trainer</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import argparse <br/>import _pickle as pickle<br/><br/>import numpy as np<br/>from sklearn.ensemble import ExtraTreesClassifier<br/>from sklearn import preprocessing</pre>
<ol start="2">
<li>Define an argument parser:</li>
</ol>
<pre style="padding-left: 60px">def build_arg_parser():<br/>    parser = argparse.ArgumentParser(description='Trains the classifier')<br/>    parser.add_argument("--feature-map-file", dest="feature_map_file", required=True,<br/>            help="Input pickle file containing the feature map")<br/>    parser.add_argument("--model-file", dest="model_file", required=False,<br/>            help="Output file where the trained model will be stored")<br/>    return parser</pre>
<ol start="3">
<li>Define a class to handle ERF training. We will use a label encoder to encode our training labels:</li>
</ol>
<pre style="padding-left: 60px">class ERFTrainer(object):<br/>    def __init__(self, X, label_words):<br/>        self.le = preprocessing.LabelEncoder() <br/>        self.clf = ExtraTreesClassifier(n_estimators=100, <br/>                max_depth=16, random_state=0) </pre>
<ol start="4">
<li>Encode the labels and train the classifier:</li>
</ol>
<pre>        y = self.encode_labels(label_words) 
        self.clf.fit(np.asarray(X), y) </pre>
<ol start="5">
<li>Define a function to encode the labels:</li>
</ol>
<pre style="padding-left: 60px">def encode_labels(self, label_words): 
    self.le.fit(label_words)  
    return np.array(self.le.transform(label_words), dtype=np.float32) </pre>
<ol start="6">
<li>Define a function to classify an unknown datapoint:</li>
</ol>
<pre style="padding-left: 60px">def classify(self, X): 
    label_nums = self.clf.predict(np.asarray(X)) 
    label_words = self.le.inverse_transform([int(x) for x in label_nums])  
    return label_words </pre>
<ol start="7">
<li>Define the <kbd>main</kbd> function and parse the input arguments:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    args = build_arg_parser().parse_args() 
    feature_map_file = args.feature_map_file 
    model_file = args.model_file</pre>
<ol start="8">
<li>Load the feature map that we created in the previous recipe:</li>
</ol>
<pre>    # Load the feature map 
    with open(feature_map_file, 'rb') as f: 
        feature_map = pickle.load(f) </pre>
<ol start="9">
<li>Extract the feature vectors:</li>
</ol>
<pre>    # Extract feature vectors and the labels<br/>    label_words = [x['object_class'] for x in feature_map]<br/>    dim_size = feature_map[0]['feature_vector'].shape[1] <br/>    X = [np.reshape(x['feature_vector'], (dim_size,)) for x in feature_map]</pre>
<ol start="10">
<li>Train the ERF, which is based on the training data:</li>
</ol>
<pre>    # Train the Extremely Random Forests classifier 
    erf = ERFTrainer(X, label_words)  </pre>
<ol start="11">
<li>Save the trained ERF model, as follows:</li>
</ol>
<pre>    if args.model_file: 
        with open(args.model_file, 'wb') as f: 
            pickle.dump(erf, f) </pre>
<ol start="12">
<li>Now, you should run the code in the Terminal:</li>
</ol>
<pre>    <strong>$ python trainer.py --feature-map-file feature_map.pkl <br/>    --model-file erf.pkl</strong></pre>
<p style="padding-left: 60px">This will generate a file called <kbd>erf.pkl</kbd>. We will use this file in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we <span>used </span>ERFs<span> to train our image classifier. First, we defined an argument parser function and a class to handle ERF training. We used a label encoder to encode our training labels. Then, we loaded the feature map we obtained in the <em>Creating features using Visual Codebook and vector quantization</em> recipe. So, we extracted feature vectors and the labels, and finally we trained the ERF classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>To train the image classifier, the<span> </span><span><kbd>sklearn.ensemble.ExtraTreesClassifier</kbd> function was used. This function builds an extremely randomized tree classifier.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the<span> <kbd>sklearn.ensemble.ExtraTreesClassifier</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier">https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier</a></span></li>
<li><em>Random Forests</em> (by Leo Breiman and Adele Cutler, from the University of California, Berkeley): <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></li>
<li><em>Extremely randomized trees</em> (by Pierre Geurts, Damien Ernst, and Louis Wehenkel, form <em>Machine learning Journal - Springer)</em>: <a href="https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf">https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an object recognizer</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, <em>Training an image classifier using Extremely Random Forests</em>,<span> we </span><span>used </span>ERFs<span> to train our image classifier. </span><span>Now that we have trained an ERF model, let's go ahead and build an object recognizer that can recognize the content of unknown images.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to use a trained ERF model to recognize the content of unknown images. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can build an object recognizer:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd><span>object_recognizer</span>.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import argparse <br/>import _pickle as pickle<br/><br/>import cv2<br/><br/>import build_features as bf<br/>from trainer import ERFTrainer</pre>
<ol start="2">
<li>Define the argument parser:</li>
</ol>
<pre style="padding-left: 60px">def build_arg_parser():<br/>    parser = argparse.ArgumentParser(description='Extracts features \<br/>            from each line and classifies the data')<br/>    parser.add_argument("--input-image", dest="input_image", required=True,<br/>            help="Input image to be classified")<br/>    parser.add_argument("--model-file", dest="model_file", required=True,<br/>            help="Input file containing the trained model")<br/>    parser.add_argument("--codebook-file", dest="codebook_file", <br/>            required=True, help="Input file containing the codebook")<br/>    return parser</pre>
<ol start="3">
<li>Define a class to handle the image tag extraction functions:</li>
</ol>
<pre style="padding-left: 60px">class ImageTagExtractor(object):<br/>    def __init__(self, model_file, codebook_file):<br/>        with open(model_file, 'rb') as f:<br/>            self.erf = pickle.load(f)<br/><br/>        with open(codebook_file, 'rb') as f:<br/>            self.kmeans, self.centroids = pickle.load(f)</pre>
<ol start="4">
<li>Define a function to predict the output using the trained ERF model:</li>
</ol>
<pre>    def predict(self, img, scaling_size):<br/>        img = bf.resize_image(img, scaling_size)<br/>        feature_vector = bf.BagOfWords().construct_feature(<br/>                img, self.kmeans, self.centroids)<br/>        image_tag = self.erf.classify(feature_vector)[0]<br/>        return image_tag</pre>
<ol start="5">
<li>Define the <kbd>main</kbd> function and load the input image:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__':<br/>    args = build_arg_parser().parse_args()<br/>    model_file = args.model_file<br/>    codebook_file = args.codebook_file<br/>    input_image = cv2.imread(args.input_image)</pre>
<ol start="6">
<li>Scale the image appropriately, as follows:</li>
</ol>
<pre>    scaling_size = 200 </pre>
<ol start="7">
<li>Print the output on the terminal:</li>
</ol>
<pre>    print("Output:", ImageTagExtractor(model_file, <br/>            codebook_file).predict(input_image, scaling_size))</pre>
<ol start="8">
<li>Now, you should run the code in the following way:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python object_recognizer.py --input-image imagefile.jpg --model-file erf.pkl --codebook-file codebook.pkl</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used a trained ERF model to recognize the content of unknown images. To do this, the algorithms discussed in the two previous recipes were used, namely, <em>Creating features using Visual Codebook and vector quantization</em> and <em>Training an image classifier using Extremely Random Forests.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>A <strong>random forest</strong> is an aggregate classifier that is made up of many decision trees and outputs the class that corresponds to the output of the individual tree classes. The algorithm for inducing a random forest was developed by Leo Breiman and Adele Cutler. It is based on the creation of a broad set of classifying trees, each of which is proposed to classify a single plant whose characteristics of any nature have been evaluated. Comparing the classification proposals provided by each tree in the forest shows the class to attribute the plant to it is the one that received the most indications or votes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Intro to random forest</em> (by Anthony Anh Quoc Doan, from California State University, Long Beach): <a href="https://web.csulb.edu/~tebert/teaching/lectures/551/random_forest.pdf">https://web.csulb.edu/~tebert/teaching/lectures/551/random_forest.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Light GBM for image classification</h1>
                </header>
            
            <article>
                
<p>Gradient boosting is used in regression and classification problems to produce a predictive model in the form of a set of weak predictive models, typically decision trees. This methodology is similar to the boosting methods and generalizes them, allowing for the optimization of an arbitrary differentiable <kbd>loss</kbd> function.</p>
<p>The <strong>Light Gradient Boosting Machine</strong> (<strong>LightGBM</strong>) is a particular variation of gradient boosting, with some modifications that make it particularly advantageous. It is based on classification trees, but the choice of splitting the leaf at each step is done more effectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn how to use <span>LightGBM to classify handwritten digits. To do this, the <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) dataset will be used. This is a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of NIST's larger dataset. The digits are of a 28 x 28 pixel resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each pixel value from the 28 x 28 matrix, and one value is the actual digit. The digits have been size-normalized and centered in a fixed-size image.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how we can use LightGBM for image classification:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is given in the <kbd>LightgbmClassifier.py</kbd> file that is provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import lightgbm as lgb<br/>from sklearn.metrics import mean_squared_error<br/>from keras.datasets import mnist<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import accuracy_score</pre>
<ol start="2">
<li>To import the <kbd>mnist</kbd> dataset, the following code must be used:</li>
</ol>
<pre style="padding-left: 60px">(XTrain, YTrain), (XTest, YTest) = mnist.load_data()</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The following tuples are returned:</p>
<ul>
<li><kbd>XTrain</kbd>, <kbd>XTest</kbd>: A <kbd>uint8</kbd> array of grayscale image data with the (<kbd>num_samples,</kbd> <kbd>28, 28</kbd>) shape</li>
<li><kbd>YTrain</kbd>, <kbd>YTest</kbd>: A <kbd>uint8</kbd> array of digit labels (integers in the range 0-9) with the (<kbd>num_samples</kbd>) shape</li>
</ul>
<ol start="3">
<li>So, each sample image consists of a 28 x 28 matrix. To reduce the dimensionality, we will flatten the 28 x 28 images into vectors of size 784:</li>
</ol>
<pre style="padding-left: 60px">XTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))<br/>XTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:]))) </pre>
<ol start="4">
<li>Now, we will extract from the dataset that contains the digits from 0 to 9, but only the first two (0 and 1) because we want to build a binary classifier. To do this, we will use the <kbd>numpy.where</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">TrainFilter = np.where((YTrain == 0 ) | (YTrain == 1))<br/>TestFilter = np.where((YTest == 0) | (YTest == 1))<br/><br/>XTrain, YTrain = XTrain[TrainFilter], YTrain[TrainFilter]<br/>XTest, YTest = XTest[TestFilter], YTest[TestFilter]</pre>
<ol start="5">
<li>We create a dataset for <kbd>lightgbm</kbd>:</li>
</ol>
<pre style="padding-left: 60px">LgbTrain = lgb.Dataset(XTrain, YTrain)<br/>LgbEval = lgb.Dataset(XTest, YTest, reference=LgbTrain)</pre>
<ol start="6">
<li>Now, we have to specify the model parameters as a dictionary:</li>
</ol>
<pre style="padding-left: 60px">Parameters = {<br/>    'boosting_type': 'gbdt',<br/>    'objective': 'binary',<br/>    'metric': 'binary_logloss',<br/>    'num_leaves': 31,<br/>    'learning_rate': 0.05,<br/>    'feature_fraction': 0.9,<br/>    'bagging_fraction': 0.8,<br/>    'bagging_freq': 5,<br/>    'verbose': 0<br/>}</pre>
<p class="mce-root"/>
<ol start="7">
<li>Let's train the model:</li>
</ol>
<pre style="padding-left: 60px">gbm = lgb.train(Parameters,<br/>                LgbTrain,<br/>                num_boost_round=10,<br/>                valid_sets=LgbTrain)</pre>
<ol start="8">
<li>Our model is now ready, so we can use it to classify the handwritten digits automatically. To do this, we will use the <kbd>predict()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">YPred = gbm.predict(XTest, num_iteration=gbm.best_iteration)<br/>YPred = np.round(YPred)<br/>YPred = YPred.astype(int)</pre>
<ol start="9">
<li>Now, we can evaluate the model:</li>
</ol>
<pre style="padding-left: 60px">print('Rmse of the model is:', mean_squared_error(YTest, YPred) ** 0.5)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Rmse of the model is: 0.05752992848417943</strong></pre>
<ol start="10">
<li>To analyze the errors that were made in the binary classification in more detail, we need to compute the confusion matrix:</li>
</ol>
<pre style="padding-left: 60px">ConfMatrix = confusion_matrix(YTest, YPred)<br/>print(ConfMatrix)</pre>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong>[[ 978 2]</strong><br/><strong> [ 5 1130]]</strong></pre>
<ol start="11">
<li>Finally, we will calculate the model's accuracy:</li>
</ol>
<pre style="padding-left: 60px">print(accuracy_score(YTest, YPred))</pre>
<p style="padding-left: 60px"><span>The following result is returned:</span></p>
<pre style="padding-left: 60px"><strong>0.9966903073286052</strong></pre>
<p style="padding-left: 60px">The model is therefore able to classify images of handwritten digits with a high accuracy.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, we used </span><span>LightGBM to classify handwritten digits. LightGBM is a particular variation of gradient boosting, with some modifications that make it particularly advantageous. It is based on classification trees, but the choice of splitting the leaf at each step is done more effectively.</span></p>
<p>While boosting operates a tree growth in depth, LightGBM makes this choice by combining two criteria:</p>
<ul>
<li>Optimization based on gradient descent</li>
<li>To avoid overfitting problems, a limit for the maximum depth is set</li>
</ul>
<p>This type of growth is called <strong>leaf-wise</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Light GBM has many advantages:</p>
<ul>
<li>The procedure that's presented is, on average, an order of magnitude faster than similar algorithms. This is because it does not completely grow trees, and it also makes use of the binning of variables (a procedure that divides these into sub-groups, both to speed up the calculations and as a regularization method).</li>
<li>More economical use of memory: the binning procedure involves less intensive use of memory.</li>
<li>Better accuracy compared to the usual boosting algorithms: since it uses a leaf-wise procedure, the obtained trees are more complex. At the same time, to avoid overfitting, a limit is placed on the maximum depth.</li>
<li>The algorithm is easily parallelizable.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the<span> </span><kbd>LightGBM</kbd> library: <a href="https://lightgbm.readthedocs.io/en/latest/">https://lightgbm.readthedocs.io/en/latest/</a></li>
<li><em>A Gentle Introduction to Gradient Boosting</em> (from Northeastern University): <a href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf">http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf</a></li>
<li><em>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</em> (by Guolin Ke and others): <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf">https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>