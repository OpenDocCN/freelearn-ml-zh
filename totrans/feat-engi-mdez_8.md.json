["```py\n# the olivetti face dataset\nfrom sklearn.datasets import fetch_lfw_people\n\n# feature extraction modules\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# feature scaling module\nfrom sklearn.preprocessing import StandardScaler \n\n# standard python modules\nfrom time import time \nimport numpy as np \nimport matplotlib.pyplot as plt\n\n%matplotlib inline # this ensures that your plotting will show directly in your jupyter notebook\n\n# scikit-learn model selection modules from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\n# metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n\n# machine learning modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline \n\n```", "```py\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n```", "```py\nn_samples, h, w = lfw_people.images.shape\nn_samples, h, w\n\n(1288, 50, 37) \n```", "```py\n# for machine learning we use the data directly (as relative pixel positions info is ignored by this model)\n\nX = lfw_people.data\ny = lfw_people.target\nn_features = X.shape[1]\n\nn_features\n1850 \n```", "```py\nX.shape\n\n(1288, 1850)\n```", "```py\n# plot one of the faces\nplt.imshow(X[0].reshape((h, w)), cmap=plt.cm.gray)\nlfw_people.target_names[y[0]]\n```", "```py\n'Hugo Chavez'\n```", "```py\nplt.imshow(StandardScaler().fit_transform(X)[0].reshape((h, w)), cmap=plt.cm.gray)\nlfw_people.target_names[y[0]]\n```", "```py\n'Hugo Chavez'\n```", "```py\n# the label to predict is the id of the person\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint \"Total dataset size:\"\nprint \"n_samples: %d\" % n_samples\nprint \"n_features: %d\" % n_features\nprint \"n_classes: %d\" % n_classes\n```", "```py\nTotal dataset size:\nn_samples: 1288\nn_features: 1850\nn_classes: 7\n```", "```py\n# let's split our dataset into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n```", "```py\n# instantiate the PCA module\npca = PCA(n_components=200, whiten=True)\n\n# create a pipeline called preprocessing that will scale data and then apply PCA\npreprocessing = Pipeline([('scale', StandardScaler()), ('pca', pca)])\n```", "```py\nprint \"Extracting the top %d eigenfaces from %d faces\" % (200, X_train.shape[0])\n\n# fit the pipeline to the training set\npreprocessing.fit(X_train)\n\n# grab the PCA from the pipeline\nextracted_pca = preprocessing.steps[1][1]\n```", "```py\nExtracting the top 200 eigenfaces from 966 faces\n```", "```py\n# Scree Plot\n\nplt.plot(np.cumsum(extracted_pca.explained_variance_ratio_))\n```", "```py\ncomp = extracted_pca.components_\nimage_shape = (h, w)\ndef plot_gallery(title, images, n_col, n_row):\n    plt.figure(figsize=(2\\. * n_col, 2.26 * n_row))\n    plt.suptitle(title, size=16)\n    for i, comp in enumerate(images):\n        plt.subplot(n_row, n_col, i + 1)\n        vmax = max(comp.max(), -comp.min())\n        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,            \n                   vmin=-vmax, vmax=vmax)\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n    plt.show()    \n```", "```py\nplot_gallery('PCA components', comp[:16], 4,4)\n```", "```py\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n```", "```py\n# fit without using PCA to see what the difference will be\nt0 = time()\n\nparam_grid = {'C': [1e-2, 1e-1,1e0,1e1, 1e2]}\nclf = GridSearchCV(logreg, param_grid)\nclf = clf.fit(X_train, y_train)\nbest_clf = clf.best_estimator_\n\n# Predicting people's names on the test set\ny_pred = best_clf.predict(X_test)\n\nprint accuracy_score(y_pred, y_test), \"Accuracy score for best estimator\"\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint plot_confusion_matrix(confusion_matrix(y_test, y_pred, labels=range(n_classes)), target_names)\nprint round((time() - t0), 1), \"seconds to grid search and predict the test set\"\n```", "```py\n0.813664596273 Accuracy score for best estimator\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.72      0.68      0.70        19\n     Colin Powell       0.85      0.71      0.77        55\n  Donald Rumsfeld       0.62      0.72      0.67        25\n    George W Bush       0.88      0.91      0.89       142\nGerhard Schroeder       0.79      0.84      0.81        31\n      Hugo Chavez       0.87      0.81      0.84        16\n       Tony Blair       0.71      0.71      0.71        34\n\n      avg / total       0.82      0.81      0.81       322\n\nNone\n39.9 seconds to grid search and predict the test set\n```", "```py\nt0 = time()\n\nface_pipeline = Pipeline(steps=[('PCA', PCA(n_components=200)), ('logistic', logreg)])\n\npipe_param_grid = {'logistic__C': [1e-2, 1e-1,1e0,1e1, 1e2]}\nclf = GridSearchCV(face_pipeline, pipe_param_grid)\nclf = clf.fit(X_train, y_train)\nbest_clf = clf.best_estimator_\n\n# Predicting people's names on the test set\ny_pred = best_clf.predict(X_test)\n\nprint accuracy_score(y_pred, y_test), \"Accuracy score for best estimator\"\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint plot_confusion_matrix(confusion_matrix(y_test, y_pred, labels=range(n_classes)), target_names)\nprint round((time() - t0), 1), \"seconds to grid search and predict the test set\"\n```", "```py\n0.739130434783 Accuracy score for best estimator\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.67      0.63      0.65        19\n     Colin Powell       0.69      0.60      0.64        55\n  Donald Rumsfeld       0.74      0.68      0.71        25\n    George W Bush       0.76      0.88      0.82       142\nGerhard Schroeder       0.77      0.77      0.77        31\n      Hugo Chavez       0.62      0.62      0.62        16\n       Tony Blair       0.77      0.50      0.61        34\n\n      avg / total       0.74      0.74      0.73       322\n\nNone\n74.5 seconds to grid search and predict the test set\n```", "```py\ndef get_best_model_and_accuracy(model, params, X, y):\n    grid = GridSearchCV(model,           # the model to grid search\n                        params,          # the parameter set to try \n                        error_score=0.)  # if a parameter set raises an error, continue and set the performance as a big, fat 0\n    grid.fit(X, y)           # fit the model and parameters\n    # our classical metric for performance\n    print \"Best Accuracy: {}\".format(grid.best_score_)\n    # the best parameters that caused the best accuracy\n    print \"Best Parameters: {}\".format(grid.best_params_)\n    # the average time it took a model to fit to the data (in seconds)\n    print \"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3))\n    # the average time it took a model to predict out of sample data (in seconds)\n    # this metric gives us insight into how this model will perform in real-time analysis\n    print \"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3))\n```", "```py\n# Create a larger pipeline to gridsearch\nface_params = {'logistic__C':[1e-2, 1e-1, 1e0, 1e1, 1e2], \n               'preprocessing__pca__n_components':[100, 150, 200, 250, 300],\n               'preprocessing__pca__whiten':[True, False],\n               'preprocessing__lda__n_components':range(1, 7)  \n               # [1, 2, 3, 4, 5, 6] recall the max allowed is n_classes-1\n              }\n\npca = PCA()\nlda = LinearDiscriminantAnalysis()\n\npreprocessing = Pipeline([('scale', StandardScaler()), ('pca', pca), ('lda', lda)])\n\nlogreg = LogisticRegression()\nface_pipeline = Pipeline(steps=[('preprocessing', preprocessing), ('logistic', logreg)])\n\nget_best_model_and_accuracy(face_pipeline, face_params, X, y)\n```", "```py\nBest Accuracy: 0.840062111801\nBest Parameters: {'logistic__C': 0.1, 'preprocessing__pca__n_components': 150, 'preprocessing__lda__n_components': 5, 'preprocessing__pca__whiten': False}\nAverage Time to Fit (s): 0.214\nAverage Time to Score (s): 0.009\n```", "```py\n# used for row normalization\nfrom sklearn.preprocessing import Normalizer\n\n# scikit-learn's KMeans clustering module\nfrom sklearn.cluster import KMeans\n\n# data manipulation tool\nimport pandas as pd\n\n# import a sentence tokenizer from nltk\nfrom nltk.tokenize import sent_tokenize\n\n# feature extraction module (TruncatedSVD will be explained soon)\nfrom sklearn.decomposition import PCA from sklearn.decomposition import TruncatedSVD\n```", "```py\nhotel_reviews = pd.read_csv('../data/7282_1.csv')\n```", "```py\nhotel_reviews.shape\n\n(35912, 19)\n```", "```py\nhotel_reviews.head()\n```", "```py\n# plot the lats and longs of reviews\nhotel_reviews.plot.scatter(x='longitude', y='latitude')\n```", "```py\n# Filter to only include reviews within the US\nhotel_reviews = hotel_reviews[((hotel_reviews['latitude']<=50.0) & (hotel_reviews['latitude']>=24.0)) & ((hotel_reviews['longitude']<=-65.0) & (hotel_reviews['longitude']>=-122.0))]\n\n# Plot the lats and longs again\nhotel_reviews.plot.scatter(x='longitude', y='latitude')\n# Only looking at reviews that are coming from the US\n```", "```py\nhotel_reviews.shape\n```", "```py\ntexts = hotel_reviews['reviews.text']\n```", "```py\nsent_tokenize(\"hello! I am Sinan. How are you??? I am fine\")\n\n['hello!', 'I am Sinan.', 'How are you???', 'I am fine']\n```", "```py\nsentences = reduce(lambda x, y:x+y, texts.apply(lambda x: sent_tokenize(str(x).decode('utf-8'))))\n```", "```py\n# the number of sentences\nlen(sentences)\n\n118151\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n\ntfidf_transformed = tfidf.fit_transform(sentences)\n\ntfidf_transformed\n```", "```py\n<118151x280901 sparse matrix of type '<type 'numpy.float64'>'\n        with 1180273 stored elements in Compressed Sparse Row format>\n```", "```py\n# try to fit PCA\n\nPCA(n_components=1000).fit(tfidf_transformed)\n```", "```py\nTypeError: PCA does not support sparse input. See TruncatedSVD for a possible alternative.\n```", "```py\n# import the Iris dataset from scikit-learn\nfrom sklearn.datasets import load_iris\n\n# load the Iris dataset\niris = load_iris()\n\n# seperate the features and response variable\niris_X, iris_y = iris.data, iris.target\n\nX_centered = StandardScaler(with_std=False).fit_transform(iris_X)\nX_scaled = StandardScaler().fit_transform(iris_X)\n```", "```py\n# test if we get the same components by using PCA and SVD\nsvd = TruncatedSVD(n_components=2)\npca = PCA(n_components=2)\n```", "```py\n# check if components of PCA and TruncatedSVD are same for a dataset\n# by substracting the two matricies and seeing if, on average, the elements are very close to 0\nprint (pca.fit(iris_X).components_ - svd.fit(iris_X).components_).mean() \n\n0.130183123094  # not close to 0\n# matrices are NOT the same\n\n# check if components of PCA and TruncatedSVD are same for a centered dataset\nprint (pca.fit(X_centered).components_ - svd.fit(X_centered).components_).mean() \n\n1.73472347598e-18  # close to 0\n# matrices ARE the same\n\n# check if components of PCA and TruncatedSVD are same for a scaled dataset\nprint (pca.fit(X_scaled).components_ - svd.fit(X_scaled).components_).mean() \n\n-1.59160878921e-16  # close to 0\n# matrices ARE the same\n```", "```py\nsvd = TruncatedSVD(n_components=1000)\nsvd.fit(tfidf_transformed)\n```", "```py\nTruncatedSVD(algorithm='randomized', n_components=1000, n_iter=5,\n       random_state=None, tol=0.0)\n```", "```py\n# Scree Plot\n\nplt.plot(np.cumsum(svd.explained_variance_ratio_))\n```", "```py\ntfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\nsvd = TruncatedSVD(n_components=10)  # will extract 10 \"topics\"\nnormalizer = Normalizer() # will give each document a unit norm\n\nlsa = Pipeline(steps=[('tfidf', tfidf), ('svd', svd), ('normalizer', normalizer)])\n```", "```py\nlsa_sentences = lsa.fit_transform(sentences)\n\nlsa_sentences.shape\n\n(118151, 10)\n```", "```py\ncluster = KMeans(n_clusters=10)\n\ncluster.fit(lsa_sentences)\n```", "```py\nKMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n    random_state=None, tol=0.0001, verbose=0)\n```", "```py\n%%timeit\n# time it takes to cluster on the original document-term matrix of shape (118151, 280901)\ncluster.fit(tfidf_transformed)\n```", "```py\n1 loop, best of 3: 4min 15s per loop\n```", "```py\n%%timeit\n# also time the prediction phase of the Kmeans clustering\ncluster.predict(tfidf_transformed)\n```", "```py\n10 loops, best of 3: 120 ms per loop\n```", "```py\n%%timeit\n# time the time to cluster after latent semantic analysis of shape (118151, 10)\ncluster.fit(lsa_sentences)\n```", "```py\n1 loop, best of 3: 3.6 s per loop\n```", "```py\n%%timeit\n# also time the prediction phase of the Kmeans clustering after LSA was performed\ncluster.predict(lsa_sentences)\n```", "```py\n10 loops, best of 3: 34 ms per loop\n```", "```py\ncluster.transform(lsa_sentences).shape\n(118151, 10)\npredicted_cluster = cluster.predict(lsa_sentences)\npredicted_cluster\n```", "```py\narray([2, 2, 2, ..., 2, 2, 6], dtype=int32)\n```", "```py\n# Distribution of \"topics\"\npd.Series(predicted_cluster).value_counts(normalize=True)# create DataFrame of texts and predicted topics\ntexts_df = pd.DataFrame({'text':sentences, 'topic':predicted_cluster})\n\ntexts_df.head()\n\nprint \"Top terms per cluster:\"\noriginal_space_centroids = svd.inverse_transform(cluster.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = lsa.steps[0][1].get_feature_names()\nfor i in range(10):\n    print \"Cluster %d:\" % i\n    print ', '.join([terms[ind] for ind in order_centroids[i, :5]])\n    print \n\nlsa.steps[0][1]\n```", "```py\nTop terms per cluster:\nCluster 0:\ngood, breakfast, breakfast good, room, great\n\nCluster 1:\nhotel, recommend, good, recommend hotel, nice hotel\n\nCluster 2:\nclean, room clean, rooms, clean comfortable, comfortable\n\nCluster 3:\nroom, room clean, hotel, nice, good\n\nCluster 4:\ngreat, location, breakfast, hotel, stay\n\nCluster 5:\nstay, hotel, good, enjoyed stay, enjoyed\n\nCluster 6:\ncomfortable, bed, clean comfortable, bed comfortable, room\n\nCluster 7:\nnice, room, hotel, staff, nice hotel\n\nCluster 8:\nhotel, room, good, great, stay\n\nCluster 9:\nstaff, friendly, staff friendly, helpful, friendly helpful\n```", "```py\n# topic prediction \nprint cluster.predict(lsa.transform(['I definitely recommend this hotel']))\n\nprint cluster.predict(lsa.transform(['super friendly staff. Love it!']))\n```", "```py\n[1]\n[9]\n```", "```py\nCluster 1:\nhotel, recommend, good, recommend hotel, nice hotel\n```", "```py\nCluster 9:\nstaff, friendly, staff friendly, helpful, friendly helpful\n```"]