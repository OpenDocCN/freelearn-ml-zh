- en: Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Learning Objectives
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Describe the fundamental concepts of classification
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述分类的基本概念
- en: Load and preprocess data for classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和预处理用于分类的数据
- en: Implement k-nearest neighbor and support vector machine classifiers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现k-最近邻和支持向量机分类器
- en: In chapter will focus on the goals of classification, and learn about k-nearest
    neighbors and support vector machines.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点关注分类的目标，并学习 k-最近邻和支持向量机。
- en: '4'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '4'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: In this chapter, we will learn about classifiers, especially the k-nearest neighbor
    classifier and support vector machines. We will use this classification to categorize
    data. Just as we did for regression, we will build a classifier based on training
    data, and test the performance of our classifier using testing data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习关于分类器的内容，特别是 k-最近邻分类器和支持向量机。我们将使用这种分类来对数据进行分类。就像我们对回归所做的那样，我们将基于训练数据构建一个分类器，并使用测试数据来测试我们分类器的性能。
- en: The Fundamentals of Classification
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类基础
- en: While regression focuses on creating a model that best fits our data to predict
    the future, classification is all about creating a model that separates our data
    into separate classes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当回归关注于创建一个最佳拟合我们数据的模型以预测未来时，分类则是关于创建一个将我们的数据分离成不同类别的模型。
- en: Assuming that you have some data belonging to separate classes, classification
    helps you predict the class a new data point belongs to. A classifier is a model
    that determines the label value belonging to any data point in the domain. Suppose
    you have a set of points, `P = {p1, p2, p3, ..., pm}` , and another set of points,
    `Q = {q1, q2, q3, ..., qn}` . You treat these points as members of different classes.
    For simplicity, we could imagine that `P` contains credit-worthy individuals,
    and `Q` contains individuals that are risky in terms of their credit repayment
    tendencies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些属于不同类别的数据，分类可以帮助你预测新数据点所属的类别。分类器是一个模型，它确定域中任何数据点的标签值。假设你有一组点，`P = {p1,
    p2, p3, ..., pm}`，以及另一组点，`Q = {q1, q2, q3, ..., qn}`。你将这些点视为不同类别的成员。为了简单起见，我们可以想象
    `P` 包含值得信赖的个人，而 `Q` 包含在信用还款倾向方面有风险的个人。
- en: You can divide the state space so that all points in `P` are on one cluster
    of the state space, and then disjoint from the state space cluster containing
    all points in `Q` . Once you find these bounded spaces, called **clusters** ,
    inside the state space, you have successfully performed **clustering** .
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将状态空间划分为所有点都在 `P` 的一个状态空间簇中，然后与包含 `Q` 中所有点的状态空间簇不相交。一旦你找到了这些被称为**簇**的边界空间，你就在状态空间内成功执行了**聚类**。
- en: Suppose we have a point, `x,` that's not equal to any of the previous points.
    Does point `x` belong to cluster `P` or cluster `Q` ? The answer to this question
    is a **classification exercise** , because we classify point `x` .
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个点 `x`，它不等于任何前面的点。点 `x` 属于簇 `P` 还是簇 `Q`？这个问题的答案是一个**分类练习**，因为我们正在对点 `x`
    进行分类。
- en: Classification is usually determined by proximity. The closer point `x` is to
    points in cluster `P` , the more likely it is that it belongs to cluster `P` .
    This is the idea behind nearest neighbor classification. In the case of k-nearest
    neighbor classification, we find the k-nearest neighbor of point `x` and classify
    it according to the maximum number of the nearest neighbors from the same class.
    Besides k-nearest neighbor, we will also use support vector machines for classification.
    In this chapter, we will be covering this credit scoring method in detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分类通常由邻近性决定。点 `x` 越接近簇 `P` 中的点，它属于簇 `P` 的可能性就越大。这是最近邻分类背后的理念。在 k-最近邻分类的情况下，我们找到点
    `x` 的 k 个最近邻，并根据来自同一类的最近邻的最大数量对其进行分类。除了 k-最近邻，我们还将使用支持向量机进行分类。在本章中，我们将详细介绍这种信用评分方法。
- en: We could either assemble random dummy data ourselves, or we could choose to
    use an online dataset with hundreds of data points. To make this learning experience
    as realistic as possible, we will choose the latter. Let's continue with an exercise
    that lets us download some data that we can use for classification. A popular
    place for downloading machine learning datasets is [https://archive.ics.uci.edu/ml/datasets.html](https://archive.ics.uci.edu/ml/datasets.html)
    . You can find five different datasets on credit approval. We will now load the
    dataset on German credit approvals, because the size of 1,000 data points is perfect
    for an example, and its documentation is available.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**The** `german` **dataset is available in the CSV format**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: CSV stands for comma-separated values. A CSV file is a simple text file, where
    each line of the file contains a data point in your dataset. The attributes of
    the data point are given in a fixed order, separated by a separator character
    such as a comma. This character may not occur in the data, otherwise we would
    not know if the separator character is part of the data or serves as a separator.
    Although the name comma-separated values suggests that the separator character
    is a comma, it is not always the case. For instance, in our example, the separator
    character is a space. CSV files are used in many applications, including Excel.
    CSV is a great, lean interface between different applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Loading Datasets'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visit [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)
    . The data files are located at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    .
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data from the space-separated `german.data` file. Make sure that you
    add headers to your DataFrame so that you can reference your features and labels
    by name instead of column number.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Save the `german.data` file locally. Insert header data into your CSV file.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first few lines of the dataset are as follows:'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The explanation to interpret this data is in the `german.doc` file, where you
    can see the list of attributes. These attributes are: Status of existing checking
    account (A11 – A14), Duration (numeric, number of months), Credit history (A30
    - A34), Purpose of credit (A40 – A410), Credit amount (numeric), Savings account/bonds
    (A61 – A65), Present employment since (A71 – A75), Disposable income percent rate
    (numeric), Personal status and sex (A91 – A95), Other debtors and guarantors (A101
    – A103), Present residence since (numeric), Property (A121 – A124), Age (numeric,
    years), Other installment plans (A141 – A143), Housing (A151 – A153), Number of
    existing credits at this bank, Job (A171 – A174), Number of people being liable
    to provide maintenance for (numeric) Telephone (A191 – A192) and Foreign worker
    (A201 – A202).'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The result of classification would be as follows: `1 means good debtor, while
    2 means bad debtor` .'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类结果如下：`1表示良好债务人，而2表示不良债务人`。
- en: 'Our task is to determine how to separate the state space of twenty input variables
    into two clusters: good debtors and bad debtors.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的任务是确定如何将二十个输入变量的状态空间分为两个簇：良好债务人和不良债务人。
- en: 'We will use the pandas library to load the data. Before loading the data, though,
    I suggest adding a header to the `german.data` file. Insert the following header
    line before the first line:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用pandas库来加载数据。不过，在加载数据之前，我建议在`german.data`文件中添加一个标题。在第一行之前插入以下标题行：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that the preceding header is just one line, which means that there is
    no newline character until the end of the 21st label, `CreditScore` .
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，前面的标题只有一行，这意味着直到第21个标签`CreditScore`的末尾都没有换行符。
- en: Note
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The header is of help because pandas can interpret the first line as the column
    name. In fact, this is the default behavior of the `read_csv` method of pandas.
    The first line of the `.csv` file is going to be the header, and the rest of the
    lines are the actual data.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标题很有帮助，因为pandas可以将第一行解释为列名。实际上，这是pandas的`read_csv`方法的默认行为。`.csv`文件的第一行将是标题，其余行是实际数据。
- en: 'Let''s import the CSV data using the `pandas.read_csv` method:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们使用`pandas.read_csv`方法导入CSV数据：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first argument of `read_csv` is the file path. If you saved it to the E
    drive of your Windows PC, for instance, then you can also write an absolute path
    there: *e:\german.data* .'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`read_csv`的第一个参数是文件路径。例如，如果你将其保存在Windows PC的E驱动器上，你也可以在那里写一个绝对路径：*e:\german.data*。'
- en: '![](img/Image00042.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Image00042.jpg)'
- en: 'Figure 4.1: Table displaying list of attributes in respective cells'
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.1：显示各自单元格中属性列表的表格
- en: 'Let''s see the format of the data. The `data_frame.head()` call prints the
    first five rows of the CSV file, structured by the pandas DataFrame:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据的格式。`data_frame.head()`调用打印CSV文件的前五行，由pandas DataFrame结构化：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have successfully loaded the data into the DataFrame.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功将数据加载到DataFrame中。
- en: Data Preprocessing
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Before building a classifier, we are better off formatting our data so that
    we can keep relevant data in the most suitable format for classification, and
    removing all data that we are not interested in.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类器之前，我们最好将我们的数据格式化，以便我们可以以最适合分类的格式保留相关数据，并删除我们不感兴趣的所有数据。
- en: 1\. **Replacing or dropping values**
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **替换或删除值**
- en: 'For instance, if there are `N/A` (or `NA` ) values in the dataset, we may be
    better off substituting these values with a numeric value we can handle. NA stands
    for Not Available. We may choose to ignore rows with NA values or replace them
    with an outlier value. An outlier value is a value such as -1,000,000 that clearly
    stands out from regular values in the dataset. The replace method of a DataFrame
    does this type of replacement. The replacement of NA values with an outlier looks
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果数据集中有`N/A`（或`NA`）值，我们可能更愿意用我们可以处理的数值替换这些值。NA代表不可用。我们可能选择忽略包含NA值的行，或者用异常值替换它们。异常值是一个像-1,000,000这样的值，它明显不同于数据集中的常规值。DataFrame的replace方法就是这样进行替换的。用异常值替换NA值看起来如下：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The replace method changes all NA values to numeric values.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: replace方法将所有NA值更改为数值。
- en: This numeric value should be far from any reasonable values in the DataFrame.
    Minus one million is recognized by the classifier as an exception, assuming that
    only positive values are there.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数值应该远远超出DataFrame中任何合理的值。减去一百万被分类器识别为异常，假设那里只有正值。
- en: 'The alternative to replacing unavailable data with extreme values is dropping
    the rows that have unavailable data:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用极端值替换不可用数据的替代方法是删除包含不可用数据的行：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first argument specifies that we drop rows, not columns. The second argument
    specifies that we perform the drop operation, without cloning the DataFrame. Dropping
    the NA values is less desirable, as you often lose a reasonable chunk of your
    dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指定我们删除行，而不是列。第二个参数指定我们执行删除操作，而不克隆DataFrame。删除NA值不太理想，因为你通常会丢失数据集的一部分。
- en: 2\. **Dropping columns**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2. **删除列**
- en: If there is a column we do not want to include in the classification, we are
    better off dropping it. Otherwise, the classifier may detect false patterns in
    places where there is absolutely no correlation. For instance, your phone number
    itself is very unlikely to correlate with your credit score. It is a 9 to 12-digit
    number that may very easily feed the classifier with a lot of noise. So we drop
    the phone column.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一列我们不希望包含在分类中，我们最好将其删除。否则，分类器可能会在完全没有相关性的地方检测到虚假模式。例如，你的电话号码本身与你的信用评分几乎不可能相关。它是一个
    9 到 12 位的数字，可能会很容易地向分类器提供大量噪声。因此，我们删除电话列。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The second argument indicates that we drop columns, not rows. The first argument
    is an enumeration of the columns we would like to drop. The `inplace` argument
    is so that the call modifies the original DataFrame.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数表示我们删除的是列，而不是行。第一个参数是我们想要删除的列的枚举。`inplace` 参数是为了让调用修改原始 DataFrame。
- en: 3\. **Transforming data**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **转换数据**
- en: 'Oftentimes, the data format we are working with is not always optimal for the
    classification process. We may want to transform our data into a different format
    for multiple reasons, such as the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们处理的数据格式并不总是最适合分类过程。我们可能出于多个原因想要将我们的数据转换成不同的格式，例如以下原因：
- en: To highlight aspects of data we are interested in (for example, Minmax scaling
    or normalization)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了突出我们感兴趣的数据方面（例如，Minmax 缩放或归一化）
- en: To drop aspects of data we are not interested in (for example, Binarization)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了删除我们不感兴趣的数据方面（例如，二值化）
- en: Label encoding
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签编码
- en: '**Minmax scaling** can be performed by the `MinMaxScaler` method of the scikit
    preprocessing utility:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**Minmax 缩放**可以通过 scikit 预处理工具的 `MinMaxScaler` 方法执行：'
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`MinMaxScaler` scales each column in the data so that the lowest number in
    the column becomes 0, the highest number becomes 1, and all of the values in between
    are proportionally scaled between zero and one.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`MinMaxScaler` 将数据中的每一列缩放，使得列中的最小数变为 0，最大数变为 1，而介于两者之间的所有值按比例缩放到 0 和 1 之间。'
- en: '**Binarization** transforms data into ones and zeros based on a condition:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**二值化**根据条件将数据转换为 1 和 0：'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Label encoding** is important for preparing your features for scikit-learn
    to process. While some of your features are string labels, scikit-learn expects
    this data to be numbers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签编码**对于准备你的特征以便 scikit-learn 处理非常重要。虽然你的某些特征是字符串标签，但 scikit-learn 预期这些数据是数字。'
- en: This is where the preprocessing library of scikit-learn comes into play.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 scikit-learn 预处理库发挥作用的地方。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You might have noticed that in the credit scoring example, there were two data
    files. One contained labels in string form, and the other in integer form. I asked
    you to load the data with string labels on purpose so that you got some experience
    of how to preprocess data properly with the label encoder.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在信用评分示例中，有两个数据文件。一个包含字符串形式的标签，另一个是整数形式。我故意让你用字符串标签加载数据，这样你就可以获得一些使用标签编码器正确预处理数据的经验。
- en: 'Label encoding is not rocket science. It creates a mapping between string labels
    and numeric values so that we can supply numbers to scikit-learn:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码不是什么高深的技术。它创建了一个字符串标签和数值之间的映射，这样我们就可以向 scikit-learn 提供数字：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s enumerate the encoding:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列举编码方式：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will be as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can use the encoder to transform values:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用编码器来转换值：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The inverse transformation that transforms encoded values back to labels is
    performed by the `inverse_transform` function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将编码值转换回标签的反向转换是通过 `inverse_transform` 函数执行的：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output will be as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Exercise 11: Pre-Processing Data'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 11：数据预处理
- en: In this exercise, we will use a dataset with pandas.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 pandas 数据集。
- en: Load the CSV data of the 2017-2018 January kickstarter projects from [https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv](https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv)
    and apply the preprocessing steps on the loaded data.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv](https://github.com/TrainingByPackt/Artificial-Intelligence-and-Machine-Learning-Fundamentals/blob/master/Lesson04/Exercise%2011%20Pre-processing%20Data/ks-projects-201801.csv)加载2017-2018年1月Kickstarter项目的CSV数据，并在加载的数据上应用预处理步骤。
- en: Note
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you need a working internet connection to complete this exercise.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，你需要一个有效的互联网连接才能完成这个练习。
- en: 'If you open the file, you will see that you don''t have to bother adding a
    header, because it is included in the CSV file:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你打开文件，你会看到你不需要麻烦地添加标题，因为它包含在CSV文件中：
- en: '[PRE18]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Import the data and create a DataFrame using pandas:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据并使用pandas创建DataFrame：
- en: '[PRE19]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The previous command prints the first five entries belonging to the dataset.
    We can see the name and format of each column. Now that we have the data, it's
    time to perform some preprocessing steps.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前的命令打印了属于数据集的前五个条目。我们可以看到每个列的名称和格式。现在我们有了数据，是时候进行一些预处理步骤了。
- en: 'Suppose you have some NA or N/A values in the dataset. You can replace them
    with the following `replace` operations:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设数据集中有一些NA或N/A值。你可以用以下`replace`操作来替换它们：
- en: '[PRE20]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When performing classification or regression, keeping the ID column is just
    asking for trouble. In most cases, the ID does not correlate with the end result.
    Therefore, it makes sense to drop the ID column:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行分类或回归时，保留ID列只会带来麻烦。在大多数情况下，ID与最终结果不相关。因此，删除ID列是有意义的：
- en: '[PRE21]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Suppose we are only interested in whether the projects had backers or not.
    This is a perfect case for binarization:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们只对项目是否有支持者感兴趣。这是一个二值化的完美案例：
- en: '[PRE22]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be as follows:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE23]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We are discarding the resulting binary array. To make use of the binary data,
    we would have to replace the backers column with it. We will omit this step for
    simplicity.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在丢弃生成的二进制数组。为了使用二进制数据，我们必须用它来替换支持者列。为了简单起见，我们将省略这一步。
- en: 'Let''s encode the labels so that they become numeric values that can be interpreted
    by the classifier:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编码标签，使它们成为可以被分类器解释的数值：
- en: '[PRE24]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output will be as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE25]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You have to know about all possible labels that can occur in your file. The
    documentation is responsible for providing you with the available options. In
    the unlikely case that the documentation is not available for you, you have to
    reverse engineer the possible values from the file.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须了解文件中可能出现的所有可能的标签。文档负责提供可用的选项。在不太可能的情况下，如果你无法获得文档，你必须从文件中逆向工程可能的值。
- en: 'Once the encoded array is returned, the same problem holds as in the previous
    point: we have to make use of these values by replacing the `currency` column
    of the DataFrame with these new values.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦返回编码后的数组，就与前一点相同的问题：我们必须利用这些值，通过用这些新值替换DataFrame中的`currency`列来使用这些值。
- en: Minmax Scaling of the Goal Column
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标列的Minmax缩放
- en: When Minmax scaling was introduced, you saw that instead of scaling the values
    of each vector in a matrix, the values of each coordinate in each vector were
    scaled together. This is how the matrix structure describes a dataset. One vector
    contains all attributes of a data point. When scaling just one attribute, we have
    to transpose the column we wish to scale.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当介绍Minmax缩放时，你看到的是，不是对矩阵中每个向量的值进行缩放，而是对每个向量中每个坐标的值一起缩放。这就是矩阵结构描述数据集的方式。一个向量包含一个数据点的所有属性。当只缩放一个属性时，我们必须转置我们想要缩放的列。
- en: 'You learned about the transpose operation of NumPy in *Chapter 1* , *Principles
    of Artificial Intelligence* :'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你在*第一章*，*人工智能原理*中学习了NumPy的转置操作：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we have to apply the `MinMaxScaler` to scale the transposed values. To
    get the results in one array, we can transpose the results back to their original
    form:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须应用`MinMaxScaler`来缩放转置的值。为了将结果放在一个数组中，我们可以将结果转置回原始形式：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The values look weird because there were some high goals on Kickstarter, possibly
    using seven figure values. Instead of linear Minmax scaling, it is also possible
    to use the magnitude and scale logarithmically, counting how many digits the goal
    price has. This is another transformation that could make sense for reducing the
    complexity of the classification exercise.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: As always, you have to place the results in the corresponding column of the
    DataFrame to make use of the transformed values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: We will stop preprocessing here. Hopefully, the usage of these different methods
    is now clear, and you will have a strong command of using these preprocessing
    methods in the future.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Features and Labels
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to regression, in classification, we must also separate our features
    and labels. Continuing from the original example, our features are all columns,
    except the last one, which contains the result of the credit scoring. Our only
    label is the credit scoring column.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use NumPy arrays to store our features and labels:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we are ready with our features and labels, we can use this data for
    cross-validation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation with scikit-learn
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another pertinent point about regression is that we can use cross-validation
    to train and test our model. This process is exactly the same as in the case of
    regression problems:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `train_test_split` method shuffles and then splits our features and labels
    into a training dataset and a testing dataset. We can specify the size of the
    testing dataset as a number between `0` and `1` . A `test_size` of `0.1` means
    that `10%` of the data will go into the testing dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Preparing Credit Data for Classification'
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to prepare data for a classifier. We will
    be using `german.data` from [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    as an example and will prepare the data for training and testing a classifier.
    Make sure that all of your labels are numeric, and that the values are prepared
    for classification. Use 80% of the data points as training data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Save `german.data` from [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    and open it in a text editor such as Sublime Text or Atom. Add the header row
    to it.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the data file using pandas and replace the NA values with an outlier
    value.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform label encoding. Transform all of the labels in the data frame into integers.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate features from labels. We can apply the same method as the one we saw
    in the theory section.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform scaling of the training and testing data together. Use `MinMaxScaler`
    from Scikit's Preprocessing library.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step is cross-validation. Shuffle our data and use 80% of all data
    for training and 20% for testing.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 276.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The k-nearest neighbor Classifier
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will continue from where we left off in the first topic. We have training
    and testing data, and it is now time to prepare our classifier to perform k-nearest
    neighbor classification. After introducing the K-Nearest Neighbor algorithm, we
    will use scikit-learn to perform classification.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the K-Nearest Neighbor Algorithm
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of classification algorithms is to divide data so that we can determine
    which data points belong to which region. Suppose that a set of classified points
    is given. Our task is to determine which class a new data point belongs to.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbor classifier receives classes of data points with given
    feature and label values. The goal of the algorithm is to classify data points.
    These data points contain feature coordinates, and the objective of the classification
    is to determine the label values. Classification is based on proximity. Proximity
    is defined as a Euclidean distance. Point `A` is closer to point `B` than to point
    `C` if the Euclidean distance between `A` and `B` is shorter than the Euclidean
    distance between `A` and `C` .
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbor classifier gets the k-nearest neighbors of a data point.
    The label belonging to point A is the most frequently occurring label value among
    the k-nearest neighbors of point A. Determining the value of `K` is a non-obvious
    task. Obviously, if there are two groups, such as credit-worthy and not credit-worthy,
    we need K to be 3 or greater, because otherwise, with `K=2` , we could easily
    have a tie between the number of neighbors. In general, though, the value of K
    does not depend on the number of groups or the number of features.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: A special case of k-nearest neighbors is when `K=1` . In this case, the classification
    boils down to finding the nearest neighbor of a point. `K=1` most often gives
    us significantly worse results than `K=3` or greater.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Distance Functions
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many distance metrics could work with the k-nearest neighbor algorithm. We will
    now calculate the Euclidean and the Manhattan distance of two data points. The
    Euclidean distance is a generalization of the way we calculate the distance of
    two points in the plane or in a three-dimensional space.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance between points `A = (a1, a2, …, an)` and `B=(b1, b2, …, bn)` is
    the length of the line segment connecting these two points:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 Distance between points A and B](img/Image00043.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Distance between points A and B'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Technically, we don't need to calculate the square root when we are just looking
    for the nearest neighbors, because the square root is a monotone function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will use the Euclidean distance in this book, let''s see how to calculate
    the distance of multiple points using one scikit-learn function call. We have
    to import `euclidean_distances` from `sklearn.metrics.pairwise` . This function
    accepts two sets of points and returns a matrix that contains the pairwise distance
    of each point from the first and the second sets of points:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For instance, the distance of (4,4) and (3,7) is approximately 3.162.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate the Euclidean distances between points in the same set:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**The Manhattan/Hamming Distance**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The Hamming and Manhattan distances represent the same formula.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The Manhattan distance relies on calculating the absolute value of the difference
    of the coordinates of the data points:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 The Manhattan and Hamming Distance](img/Image00044.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: The Manhattan and Hamming Distance'
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Euclidean distance is a more accurate generalization of distance, while
    the Manhattan distance is slightly easier to calculate.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Illustrating the K-nearest Neighbor Classifier Algorithm'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a list of employee data. Our features are the numbers of hours
    worked per week and yearly salary. Our label indicates whether an employee has
    stayed with our company for more than two years. The length of stay is represented
    by a zero if it is less than two years, and a one in case it is greater than or
    equal to two years.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: We would like to create a 3-nearest neighbor classifier that determines whether
    an employee stays with our company for at least two years.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Then, we would like to use this classifier to predict whether an employee with
    a request to work 32 hours a week and earning 52,000 dollars per year is going
    to stay with the company for two years or not.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Scale the features:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The scaled result is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'It makes sense to scale our requested employee as well at this point: *[32,
    52000]* becomes *[ (32-24)/(40 - 24), (52000-45000)/(72000 - 45000)] = [0.5, 0.25925925925925924]*
    .'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot these points on a two-dimensional plane such that the first two coordinates
    represent a point on the plane, and the third coordinate determines the color
    of the point:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 Points plotted on a two-dimensional plane](img/Image00045.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.4: Points plotted on a two-dimensional plane'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To calculate the distance of the blue point and all the other points, we will
    apply the transpose function from *Chapter 1, Principles of AI* . If we transpose
    the `scaledEmployee` matrix, we get three arrays of ten. The feature values are
    in the first two arrays. We can simply use the `[:2]` index to keep them. Then,
    transposing this matrix back to its original form gives us the array of feature
    data points:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Calculate the Euclidean distance using:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The shortest distances are as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '`0.14948471` for the point `[0.6, 0.37037037, 1.]`'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.17873968` for the point `[0.6, 0.11111111, 0.]`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.31271632` for the point [`0.6, 0.55555556, 1.]`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As two out of the three points have a label of 1, we found two green points
    and one red point. This means that our 3-nearest neighbor classifier classified
    the new employee as being more likely to stay for at least two years than not
    at all.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although, the fourth point just missed the top three by a very small margin.
    In fact, our algorithm would have found a tie if there were two points of a different
    color that had the third smallest distance from the target. In case of a race
    condition in distances, there could be a tie. This is an edge case, though, which
    should almost never occur in real-life problems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: k-nearest Neighbor Classification in scikit-learn'
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Split our data into four categories: `training` and `testing` , `features`
    , and `labels` :'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create a K-Nearest Neighbor classifier to perform this classification:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Since we have not mentioned the value of K, the default is *5* .
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check how well our classifier performs on the test data:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output is `0.665` .
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You might find higher ratings with other datasets, but it is understandable
    that more than 20 features may easily contain some random noise that makes it
    difficult to classify data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Prediction with the k-nearest neighbors classifier'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This code is built on the code of previous exercise.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create a data point that we will classify by taking the i*th* element
    of the i*th* test data point:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We have a one-dimensional array. The classifier expects an array containing
    data point arrays. Therefore, we must reshape our data point into an array of
    data points:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'With this, we have created a completely random persona, and we are interested
    in whether they are classified as credit-worthy or not:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, we can safely use prediction to determine the credit rating of the data
    point:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We have successfully rated a new user based on input data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Parameterization of the k-nearest neighbor Classifier in scikit-learn
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can access the documentation of the k-nearest neighbor classifier here:
    [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
    .'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The parameterization of the classifier may fine-tune the accuracy of your classifier.
    Since we haven't learned all of the possible variations of k-nearest neighbor,
    we will concentrate on the parameters that you already understand based on this
    topic.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**n_neighbors** : This is the k value of the k-nearest neighbor algorithm.
    The default value is 5.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**metric** : When creating the classifier, you will see a weird name – "Minkowski".
    Don''t worry about this name – you have learned about the first and second order
    Minkowski metric already. This metric has a power parameter. For *p=1* , the Minkowski
    metric is the same as the Manhattan metric. For *p=2* , the Minkowski metric is
    the same as the Euclidean metric.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '*p* : This is the power of the Minkowski metric. The default value is 2.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'You have to specify these parameters once you create the classifier:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Activity 8: Increasing the Accuracy of Credit Scoring'
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will learn how the parameterization of the k-nearest neighbor
    classifier affects the end result. The accuracy of credit scoring is currently
    quite low: 66.5%. Find a way to increase it by a few percentage points. To ensure
    that this happens correctly, you will need to have done the previous exercises.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to complete this exercise. In this solution, I will show
    you one way to increase the credit score, which will be done by changing the parameterization:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Increase the k-value of the k-nearest neighbor classifier from the default 5
    to 10, 15, 25, and 50.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run this classifier for all four `n_neighbors` values and observe the results.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Higher K values do not necessarily mean a better score. In this example, though,
    `K=50` yielded a better result than `K=5` .
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 280.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Classification with Support Vector Machines
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first used support vector machines for regression in Chapter 3, *Regression*
    . In this topic, you will find out how to use support vector machines for classification.
    As always, we will use scikit-learn to run our examples in practice.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: What are Support Vector Machine Classifiers?
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of a support vector machines defined on an n-dimensional vector space
    is to find a surface in that n-dimensional space that separates the data points
    in that space into multiple classes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions, this surface is often a straight line. In three dimensions,
    the support vector machines often finds a plane. In general, the support vector
    machines finds a hyperplane. These surfaces are optimal in the sense that, based
    on the information available to the machine, it optimizes the separation of the
    n-dimensional spaces.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The optimal separator found by the support vector machines is called the **best
    separating hyperplane** .
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: A support vector machines is used to find one surface that separates two sets
    of data points. In other words, support vector machines are **binary classifiers**
    . This does not mean that support vector machines can only be used for binary
    classification. Although we were only talking about one plane, support vector
    machines can be used to partition a space into any number of classes by generalizing
    the task itself.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The separator surface is optimal in the sense that it maximizes the distance
    of each data point from the separator surface.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: A vector is a mathematical structure defined on an n-dimensional space having
    a magnitude (length) and a direction. In two dimensions, you draw the vector (x,
    y) from the origin to the point (x, y). Based on geometry, you can calculate the
    length of the vector using the Pythagorean theorem, and the direction of the vector
    by calculating the angle between the horizontal axis and the vector.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in two dimensions, the vector (3, -4) has the following magnitude:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '`sqrt( 3 * 3 + 4 * 4 ) = sqrt( 25 ) = 5`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'And it has the following direction:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '`np.arctan(-4/3) / 2 / np.pi * 360 = -53.13010235415597 degrees`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Support Vector Machines
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that two sets of points, **Red** and **Blue** , are given. For simplicity,
    we can imagine a two-dimensional plane with two features: one mapped on the horizontal
    axis, and one on the vertical axis.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of the support vector machine is to find the best separating
    line that separates points **A** , **D** , **C** , **B** , and **H** from points
    **E** , **F** , and **G** :'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 Line separating red and blue members](img/Image00046.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Line separating red and blue members'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Separation is not always that obvious. For instance, if there is a blue point
    in between E, F, and G, there is no line that could separate all points without
    errors. If points in the blue class form a full circle around the points in the
    red class, there is no straight line that could separate the two sets:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 Graph with two outlier points](img/Image00047.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Graph with two outlier points'
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For instance, in the preceding graph, we tolerate two outlier points, O and
    P.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following solution, we do not tolerate outliers, and instead of a line,
    we create a best separating path consisting of two half lines:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 Graph removing the separation of the two outliers](img/Image00048.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Graph removing the separation of the two outliers'
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A perfect separation of all data points is rarely worth the resources. Therefore,
    the support vector machine can be regularized to simplify and restrict the definition
    of the best separating shape and to allow outliers.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The **regularization parameter** of a support vector machine determines the
    rate of error to allow or forbid misclassifications.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: A support vector machine has a kernel parameter. A linear kernel strictly uses
    a linear equation for describing the best separating hyperplane. A polynomial
    kernel uses a polynomial, while an exponential kernel uses an exponential expression
    to describe the hyperplane.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: A margin is an area centered around the separator and is bounded by the points
    closest to the separator. A balanced margin has points from each class that are
    equidistant from the line.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to defining the allowed error rate of the best separating hyperplane,
    a gamma parameter decides whether only the points near the separator count in
    determining the position of the separator, or whether the points farthest from
    the line count, too. The higher the gamma, the lower the amount of points that
    influence the location of the separator.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines in scikit-learn
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our entry point is the end result of previous activity. Once we have split
    the training and test data, we are ready to set up the classifier:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Instead of using the K-Nearest Neighbor classifier, we will use the `svm.SVC()`
    classifier:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The output is `0.745` .
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: It seems that the default support vector machine classifier of scikit-learn
    does a slightly better job than the k-nearest neighbor classifier.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Parameters of the scikit-learn SVM
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the parameters of the scikit-learn SVM:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel** : This is a string or callable parameter specifying the kernel used
    in the algorithm. The predefined kernels are linear, poly, rbf, sigmoid, and precomputed.
    The default value is rbf.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**Degree** : When using a polynomial, you can specify the degree of the polynomial.
    The default value is 3.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '**Gamma** : This is the kernel coefficient for rbf, poly, and sigmoid. The
    default value is auto, computed as 1/`number_of_features` .'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '**C** : This is a floating-point number with a default of 1.0 describing the
    penalty parameter of the error term.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: You can read about rest parameters in the reference documentation at [http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    .
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of SVM:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Activity 9: Support Vector Machine Optimization in scikit-learn'
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will discuss how to use the different parameters of a support
    vector machine classifier. We will be using, comparing, and contrasting the different
    support vector regression classifier parameters you have learned about and will
    find a set of parameters resulting in the highest classification data on the training
    and testing data that we loaded and prepared in the previous activity. To ensure
    that you can complete this activity, you will need to have completed the first
    activity of this chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'We will try out a few combinations. You may have to choose different parameters
    and check the results:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Let's first choose the linear kernel and check the classifier's fit and score.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are done with that, choose the polynomial kernel of degree 4, C=2,
    and gamma=0.05 and check the classifier's fit and score.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, choose the polynomial kernel of degree 4, C=2, and gamma=0.25 and check
    the classifier's fit and score.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, select the polynomial kernel of degree 4, C=2, and gamma=0.5 and
    check the classifier's fit and score.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the next classifier as sigmoid kernel.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, choose the default kernel with a gamma of 0.15 and check the classifier's
    fit and score.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 280.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we learned the basics of classification. After discovering
    the goals of classification, and loading and formatting data, we discovered two
    classification algorithms: K-Nearest Neighbors and support vector machines. We
    used custom classifiers based on these two methods to predict values. In the next
    chapter, we will use trees for predictive analysis.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
