<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Depth Estimation and Segmentation" id="aid-12AK81"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Depth Estimation and Segmentation</h1></div></div></div><p>This chapter shows you how to use data from a depth camera to identify foreground and background regions, so that we can limit an effect to only the foreground or only the background. As prerequisites, we need a depth camera, such as Microsoft Kinect, and we need to build OpenCV with support for our depth camera. For build instructions, see <a class="link" title="Chapter 1. Setting Up OpenCV" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV</em></span>.</p><p>We'll deal with two main topics in this chapter: depth estimation and segmentation. We will explore depth estimation with two distinct approaches: firstly, by using a depth camera (a prerequisite of the first part of the chapter), such as Microsoft Kinect, and then, by using stereo images, for which a normal camera will suffice. For instructions on how to build OpenCV with support for depth cameras, see <a class="link" title="Chapter 1. Setting Up OpenCV" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV</em></span>. The second part of the chapter is about segmentation, the technique that allows us to extract foreground objects from an image.</p><div class="section" title="Creating modules"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Creating modules</h1></div></div></div><p>The code to <a id="id198" class="indexterm"/>capture and manipulate depth-camera data will be reusable outside <code class="literal">Cameo.py</code>. So, we should separate it into a new module. Let's create a file called <code class="literal">depth.py</code> in the same directory as <code class="literal">Cameo.py</code>. We need the following <code class="literal">import</code> statement in <code class="literal">depth.py</code>:</p><div class="informalexample"><pre class="programlisting">import numpy</pre></div><p>We will also need to modify our preexisting <code class="literal">rects.py</code> file so that our copy operations can be limited to a nonrectangular subregion of a rectangle. To support the changes we are going to make, let's add the following <code class="literal">import</code> statements to <code class="literal">rects.py</code>:</p><div class="informalexample"><pre class="programlisting">import numpy
import utils</pre></div><p>Finally, the new version of our application will use depth-related functionalities. So, let's add the following <a id="id199" class="indexterm"/>
<code class="literal">import</code> statement to <code class="literal">Cameo.py</code>:</p><div class="informalexample"><pre class="programlisting">import depth</pre></div><p>Now, let's go deeper into the subject of depth.</p></div></div>
<div class="section" title="Capturing frames from a depth camera" id="aid-1394Q1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Capturing frames from a depth camera</h1></div></div></div><p>Back in <a class="link" title="Chapter 2. Handling Files, Cameras, and GUIs" href="part0019.xhtml#aid-I3QM2">Chapter 2</a>, <span class="emphasis"><em>Handling Files, Cameras, and GUIs</em></span>, we discussed the concept that a computer can <a id="id200" class="indexterm"/>have multiple video capture devices and <a id="id201" class="indexterm"/>each device can have multiple channels. Suppose a given device is a stereo camera. Each channel might correspond to a different lens and sensor. Also, each channel might correspond to different kinds of data, such as a normal color image versus a depth map. The C++ version of OpenCV defines some constants for the identifiers of certain devices and channels. However, these constants are not defined in the Python version.</p><p>To remedy this situation, let's add the following definitions in <code class="literal">depth.py</code>:</p><div class="informalexample"><pre class="programlisting"># Devices.CAP_OPENNI = 900 # OpenNI (for Microsoft Kinect)CAP_OPENNI_ASUS = 910 # OpenNI (for Asus Xtion)
# Channels of an OpenNI-compatible depth generator.CAP_OPENNI_DEPTH_MAP = 0 # Depth values in mm (16UC1)CAP_OPENNI_POINT_CLOUD_MAP = 1 # XYZ in meters (32FC3)CAP_OPENNI_DISPARITY_MAP = 2 # Disparity in pixels (8UC1)CAP_OPENNI_DISPARITY_MAP_32F = 3 # Disparity in pixels (32FC1)CAP_OPENNI_VALID_DEPTH_MASK = 4 # 8UC1
# Channels of an OpenNI-compatible RGB image generator.CAP_OPENNI_BGR_IMAGE = 5CAP_OPENNI_GRAY_IMAGE = 6</pre></div><p>The depth-related channels require some explanation, as given in the following list:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">A <span class="strong"><strong>depth map</strong></span> is <a id="id202" class="indexterm"/>a grayscale image in which each pixel value <a id="id203" class="indexterm"/>is the estimated distance from the camera to a surface. Specifically, an image from the <code class="literal">CAP_OPENNI_DEPTH_MAP</code> channel gives the distance as a floating-point number of millimeters.</li><li class="listitem">A <span class="strong"><strong>point cloud </strong></span><a id="id204" class="indexterm"/><span class="strong"><strong>map</strong></span> is a color image in which <a id="id205" class="indexterm"/>each color corresponds to an (x, y, or z) spatial dimension. Specifically, the <code class="literal">CAP_OPENNI_POINT_CLOUD_MAP</code> channel yields a BGR image, where B is x (blue is right), G is y (green is up), and R is z (red is deep), from the camera's perspective. The values are in meters.</li><li class="listitem">A <span class="strong"><strong>disparity map</strong></span> is <a id="id206" class="indexterm"/>a grayscale image in which each pixel value is the stereo disparity of a surface. To conceptualize stereo disparity, let's suppose we overlay two images of a scene, shot from different <a id="id207" class="indexterm"/>viewpoints. The result would be similar to <a id="id208" class="indexterm"/>seeing double images. For points on any pair of twin objects in the scene, we can measure the distance in pixels. This measurement is the stereo disparity. Nearby objects exhibit greater stereo disparity than far-off objects. Thus, nearby objects appear brighter in a disparity map.</li><li class="listitem">A <span class="strong"><strong>valid depth mask</strong></span> shows whether the depth information at a given pixel is believed to <a id="id209" class="indexterm"/>be valid (shown by a nonzero value) or invalid (shown by a value of zero). For example, if the depth camera <a id="id210" class="indexterm"/>depends on an infrared illuminator (an infrared flash), depth information is invalid in regions that are occluded (shadowed) from this light.</li></ul></div><p>The following screenshot <a id="id211" class="indexterm"/>shows a point cloud map of a man <a id="id212" class="indexterm"/>sitting behind a sculpture of a cat:</p><div class="mediaobject"><img src="../Images/image00200.jpeg" alt="Capturing frames from a depth camera"/></div><p style="clear:both; height: 1em;"> </p><p>The following <a id="id213" class="indexterm"/>screenshot has a disparity map of a man sitting behind a sculpture of a cat:</p><div class="mediaobject"><img src="../Images/image00201.jpeg" alt="Capturing frames from a depth camera"/></div><p style="clear:both; height: 1em;"> </p><p>A valid depth mask of a <a id="id214" class="indexterm"/>man sitting behind a sculpture of a <a id="id215" class="indexterm"/>cat is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00202.jpeg" alt="Capturing frames from a depth camera"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Creating a mask from a disparity map" id="aid-147LC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Creating a mask from a disparity map</h1></div></div></div><p>For the purposes <a id="id216" class="indexterm"/>of Cameo, we are interested in disparity <a id="id217" class="indexterm"/>maps and valid depth masks. They can help us refine our estimates of facial regions.</p><p>Using the <code class="literal">FaceTracker</code> function and a normal color image, we can obtain rectangular estimates of facial regions. By analyzing such a rectangular region in the corresponding disparity map, we can tell that some pixels within the rectangle are outliers—too near or too far to really be a part of the face. We can refine the facial region to exclude these outliers. However, we should only apply this test where the data is valid, as indicated by the valid depth mask.</p><p>Let's write a function to generate a mask whose values are <code class="literal">0</code> for the rejected regions of the facial rectangle and <code class="literal">1</code> for the accepted regions. This function should take a disparity map, valid depth mask, and a rectangle as arguments. We can implement it in <code class="literal">depth.py</code> as follows:</p><div class="informalexample"><pre class="programlisting">def createMedianMask(disparityMap, validDepthMask, rect = None):
    """Return a mask selecting the median layer, plus shadows."""
    if rect is not None:
        x, y, w, h = rect
        disparityMap = disparityMap[y:y+h, x:x+w]
        validDepthMask = validDepthMask[y:y+h, x:x+w]
    median = numpy.median(disparityMap)
    return numpy.where((validDepthMask == 0) | \
                       (abs(disparityMap - median) &lt; 12),
                       1.0, 0.0)</pre></div><p>To identify outliers in the disparity map, we first find the median using <code class="literal">numpy.median()</code>, which takes an array as an argument. If the array is of an odd length, <code class="literal">median()</code> returns the value that would lie in the middle of the array if the array were sorted. If the array is of even length, <code class="literal">median()</code> returns the average of the two values that would be sorted nearest to the middle of the array.</p><p>To generate a mask <a id="id218" class="indexterm"/>based on per-pixel Boolean operations, we use <code class="literal">numpy.where()</code> with three arguments. In the first argument, <code class="literal">where()</code> takes an <a id="id219" class="indexterm"/>array whose elements are evaluated for truth or falsity. An output array of like dimensions is returned. Wherever an element in the input array is <code class="literal">true</code>, the <code class="literal">where()</code> function's second argument is assigned to the corresponding element in the output array. Conversely, wherever an element in the input array is <code class="literal">false</code>, the <code class="literal">where()</code> function's third argument is assigned to the corresponding element in the output array.</p><p>Our implementation treats a pixel as an outlier when it has a valid disparity value that deviates from the median disparity value by 12 or more. I've chosen the value of 12 just by experimentation. Feel free to tweak this value later based on the results you encounter when running Cameo with your particular camera setup.</p></div>
<div class="section" title="Masking a copy operation" id="aid-1565U1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Masking a copy operation</h1></div></div></div><p>As part of the <a id="id220" class="indexterm"/>previous chapter's work, we wrote <code class="literal">copyRect()</code> as a copy operation that limits itself to the given rectangles of a source and destination image. Now, we want to apply further limits to this copy operation. We want to use a given mask that has the same dimensions as the source rectangle.</p><p>We shall copy only those pixels in the source rectangle where the mask's value is not zero. Other pixels shall retain their old values from the destination image. This logic, with an array of conditions and two arrays of possible output values, can be expressed concisely with the <code class="literal">numpy.where()</code> function that we have recently learned.</p><p>Let's open <code class="literal">rects.py</code> and edit <code class="literal">copyRect()</code> to add a new mask argument. This argument may be <code class="literal">None</code>, in which case, we fall back to our old implementation of the copy operation. Otherwise, we next ensure that mask and the images have the same number of channels. We assume that mask has one channel but the images may have three channels (BGR). We can add duplicate channels to mask using the <code class="literal">repeat()</code> and <code class="literal">reshape()</code> methods of <code class="literal">numpy.array</code>.</p><p>Finally, we perform the copy operation using <code class="literal">where()</code>. The complete implementation is as follows:</p><div class="informalexample"><pre class="programlisting">def copyRect(src, dst, srcRect, dstRect, mask = None,
             interpolation = cv2.INTER_LINEAR):
    """Copy part of the source to part of the destination."""
    
    x0, y0, w0, h0 = srcRect
    x1, y1, w1, h1 = dstRect
    
    # Resize the contents of the source sub-rectangle.
    # Put the result in the destination sub-rectangle.
    if mask is None:
        dst[y1:y1+h1, x1:x1+w1] = \
            cv2.resize(src[y0:y0+h0, x0:x0+w0], (w1, h1),
                       interpolation = interpolation)
    else:
        if not utils.isGray(src):
            # Convert the mask to 3 channels, like the image.
            mask = mask.repeat(3).reshape(h0, w0, 3)
        # Perform the copy, with the mask applied.
        dst[y1:y1+h1, x1:x1+w1] = \
            numpy.where(cv2.resize(mask, (w1, h1),
                                   interpolation = \
                                   cv2.INTER_NEAREST),
                        cv2.resize(src[y0:y0+h0, x0:x0+w0], (w1, h1),
                                   interpolation = interpolation),
                        dst[y1:y1+h1, x1:x1+w1])</pre></div><p>We also need to modify our <code class="literal">swapRects()</code> function, which uses <code class="literal">copyRect()</code> to perform a circular swap of a list of rectangular regions. The modifications to <code class="literal">swapRects()</code> are quite simple. We just need to add a new <code class="literal">masks</code> argument, which is a list of masks whose elements are passed <a id="id221" class="indexterm"/>to the respective <code class="literal">copyRect()</code> calls. If the value of the given <code class="literal">masks</code> argument is <code class="literal">None</code>, we pass <code class="literal">None</code> to every <code class="literal">copyRect()</code> call.</p><p>The following code shows you the full implementation of this:</p><div class="informalexample"><pre class="programlisting">def swapRects(src, dst, rects, masks = None,
              interpolation = cv2.INTER_LINEAR):
    """Copy the source with two or more sub-rectangles swapped."""
    
    if dst is not src:
        dst[:] = src
    
    numRects = len(rects)
    if numRects &lt; 2:
        return
    
    if masks is None:
        masks = [None] * numRects
    
    # Copy the contents of the last rectangle into temporary storage.
    x, y, w, h = rects[numRects - 1]
    temp = src[y:y+h, x:x+w].copy()
    
    # Copy the contents of each rectangle into the next.
    i = numRects - 2
    while i &gt;= 0:
        copyRect(src, dst, rects[i], rects[i+1], masks[i],
                 interpolation)
        i -= 1
    
    # Copy the temporarily stored content into the first rectangle.
    copyRect(temp, dst, (0, 0, w, h), rects[0], masks[numRects - 1],
             interpolation)</pre></div><p>Note that the <code class="literal">masks</code> <a id="id222" class="indexterm"/>argument in <code class="literal">copyRect()</code> and <code class="literal">swapRects()</code> both default to <code class="literal">None</code>. Thus, our new versions of these functions are backward compatible with our previous versions of Cameo.</p></div>
<div class="section" title="Depth estimation with a normal camera"><div class="titlepage" id="aid-164MG2"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Depth estimation with a normal camera</h1></div></div></div><p>A depth <a id="id223" class="indexterm"/>camera is a fantastic little device to capture images and estimate the distance of objects from the camera itself, but, how does the depth camera retrieve depth information? Also, is it possible to reproduce the same kind of calculations with a normal camera?</p><p>A depth camera, such as Microsoft Kinect, uses a traditional camera combined with an infrared sensor that helps the camera differentiate similar objects and calculate their distance from the camera. However, not everybody has access to a depth camera or a Kinect, and especially when you're just learning OpenCV, you're probably not going to invest in an expensive piece of equipment until you feel your skills are well-sharpened, and your interest in the subject is confirmed.</p><p>Our setup includes a simple camera, which is most likely integrated in our machine, or a webcam attached to our computer. So, we need to resort to less fancy means of estimating the difference in distance of objects from the camera.</p><p>Geometry will come to the rescue in this case, and in particular, Epipolar Geometry, which is the geometry of stereo vision. Stereo vision is a branch of computer vision that extracts three-dimensional information out of two different images of the same subject.</p><p>How does epipolar <a id="id224" class="indexterm"/>geometry work? Conceptually, it traces imaginary lines from the camera to each object in the image, then does the same on the second image, and calculates the distance of objects based on the intersection of the lines corresponding to the same object. Here is a representation of this concept:</p><div class="mediaobject"><img src="../Images/image00203.jpeg" alt="Depth estimation with a normal camera"/></div><p style="clear:both; height: 1em;"> </p><p>Let's see how OpenCV applies epipolar geometry to calculate a so-called disparity map, which is basically a representation of the different depths detected in the images. This will enable us to extract the foreground of a picture and discard the rest.</p><p>Firstly, we need two images of the same subject taken from different points of view, but paying attention to the fact that the pictures are taken at an equal distance from the object, otherwise the calculations will fail and the disparity map will be meaningless.</p><p>So, moving on to an example:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2

def update(val = 0):
    # disparity range is tuned for 'aloe' image pair
    stereo.setBlockSize(cv2.getTrackbarPos('window_size', 'disparity'))
    stereo.setUniquenessRatio(cv2.getTrackbarPos('uniquenessRatio', 'disparity'))
    stereo.setSpeckleWindowSize(cv2.getTrackbarPos('speckleWindowSize', 'disparity'))
    stereo.setSpeckleRange(cv2.getTrackbarPos('speckleRange', 'disparity'))
    stereo.setDisp12MaxDiff(cv2.getTrackbarPos('disp12MaxDiff', 'disparity'))

    print 'computing disparity...'
    disp = stereo.compute(imgL, imgR).astype(np.float32) / 16.0

    cv2.imshow('left', imgL)
    cv2.imshow('disparity', (disp-min_disp)/num_disp)
    

if __name__ == "__main__":
    window_size = 5
    min_disp = 16
    num_disp = 192-min_disp
    blockSize = window_size
    uniquenessRatio = 1
    speckleRange = 3
    speckleWindowSize = 3
    disp12MaxDiff = 200
    P1 = 600
    P2 = 2400
    imgL = cv2.imread('images/color1_small.jpg')
    imgR = cv2.imread('images/color2_small.jpg')    
    cv2.namedWindow('disparity')
    cv2.createTrackbar('speckleRange', 'disparity', speckleRange, 50, update)    
    cv2.createTrackbar('window_size', 'disparity', window_size, 21, update)
    cv2.createTrackbar('speckleWindowSize', 'disparity', speckleWindowSize, 200, update)
    cv2.createTrackbar('uniquenessRatio', 'disparity', uniquenessRatio, 50, update)
    cv2.createTrackbar('disp12MaxDiff', 'disparity', disp12MaxDiff, 250, update)
    stereo = cv2.StereoSGBM_create(
        minDisparity = min_disp,
        numDisparities = num_disp,
        blockSize = window_size,
        uniquenessRatio = uniquenessRatio,
        speckleRange = speckleRange,
        speckleWindowSize = speckleWindowSize,
        disp12MaxDiff = disp12MaxDiff,
        P1 = P1,
        P2 = P2
    )
    update()
    cv2.waitKey()</pre></div><p>In this example, we take two images of the same subject and calculate a disparity map, showing in brighter colors the points in the map that are closer to the camera. The areas marked in black represent the disparities.</p><p>First of all, we import <code class="literal">numpy</code> and <code class="literal">cv2</code> as usual.</p><p>Let's skip the <a id="id225" class="indexterm"/>definition of the <code class="literal">update</code> function for a <a id="id226" class="indexterm"/>second and take a look at the main code; the process is quite simple: load two images, create a <code class="literal">StereoSGBM</code> instance (<code class="literal">StereoSGBM</code> stands for <span class="strong"><strong>semiglobal block matching</strong></span>, and it is an algorithm used for computing disparity maps), and also create a few trackbars to play around with the parameters of the algorithm and call the <code class="literal">update</code> function.</p><p>The <code class="literal">update</code> function applies the trackbar values to the <code class="literal">StereoSGBM</code> instance, and then calls the <code class="literal">compute</code> method, which produces a disparity map. All in all, pretty simple! Here is the first image I've used:</p><div class="mediaobject"><img src="../Images/image00204.jpeg" alt="Depth estimation with a normal camera"/></div><p style="clear:both; height: 1em;"> </p><p>This is <a id="id227" class="indexterm"/>the second one:</p><div class="mediaobject"><img src="../Images/image00205.jpeg" alt="Depth estimation with a normal camera"/></div><p style="clear:both; height: 1em;"> </p><p>There you <a id="id228" class="indexterm"/>go: a nice and quite easy to interpret disparity map.</p><div class="mediaobject"><img src="../Images/image00206.jpeg" alt="Depth estimation with a normal camera"/></div><p style="clear:both; height: 1em;"> </p><p>The <a id="id229" class="indexterm"/>parameters used by <code class="literal">StereoSGBM</code> are as follows (taken from the OpenCV documentation):</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Parameter</p>
</th><th valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>
<code class="literal">minDisparity</code>
</p>
</td><td valign="top">
<p>This <a id="id230" class="indexterm"/>parameter refers to the minimum possible disparity value. Normally, it is zero but sometimes, rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">numDisparities</code>
</p>
</td><td valign="top">
<p>This <a id="id231" class="indexterm"/>parameter refers to the maximum disparity minus minimum disparity. The resultant value is always greater than zero. In the current implementation, this parameter must be divisible by 16.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">windowSize</code>
</p>
</td><td valign="top">
<p>This <a id="id232" class="indexterm"/>parameter refers to a matched block size. It must be an odd number greater than or equal to 1. Normally, it should be somewhere in the 3-11 range.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">P1</code>
</p>
</td><td valign="top">
<p>This <a id="id233" class="indexterm"/>parameter refers to the first parameter controlling the disparity smoothness. See the next point.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">P2</code>
</p>
</td><td valign="top">
<p>This <a id="id234" class="indexterm"/>parameter refers to the second parameter that controls the disparity smoothness. The larger the values are, the smoother the disparity is. <code class="literal">P1</code> is the penalty on the disparity change by plus or minus 1 between neighbor pixels. <code class="literal">P2</code> is the penalty on the disparity change by more than 1 between neighbor pixels. The algorithm requires <code class="literal">P2 &gt; P1</code>.</p>
<p>See the <code class="literal">stereo_match.cpp</code> sample where some reasonably good <code class="literal">P1</code> and <code class="literal">P2</code> values are shown (such as <code class="literal">8*number_of_image_channels*windowSize*windowSize</code> and <code class="literal">32*number_of_image_channels*windowSize*windowSize</code>, respectively).</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">disp12MaxDiff</code>
</p>
</td><td valign="top">
<p>This <a id="id235" class="indexterm"/>parameter refers to the maximum allowed difference (in integer pixel units) in the left-right disparity check. Set it to a nonpositive value to disable the check.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">preFilterCap</code>
</p>
</td><td valign="top">
<p>This <a id="id236" class="indexterm"/>parameter refers to the truncation value for prefiltered image <a id="id237" class="indexterm"/>pixels. The algorithm first computes the x-derivative at each pixel and clips its value by the <code class="literal">[-preFilterCap, preFilterCap]</code> interval. The resultant values are passed to the Birchfield-Tomasi pixel cost function.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">uniquenessRatio</code>
</p>
</td><td valign="top">
<p>This <a id="id238" class="indexterm"/>parameter refers to the margin in percentage by which the best (minimum) computed cost function value should "win" the second best value to consider the found match to be correct. Normally, a value within the 5-15 range is good enough.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">speckleWindowSize</code>
</p>
</td><td valign="top">
<p>This <a id="id239" class="indexterm"/>parameter refers to the maximum size of smooth disparity regions to consider their noise speckles and invalidate. Set it to <code class="literal">0</code> to <a id="id240" class="indexterm"/>disable speckle filtering. Otherwise, set it somewhere in the 50-200 range.</p>
</td></tr><tr><td valign="top">
<p>
<code class="literal">speckleRange</code>
</p>
</td><td valign="top">
<p>This <a id="id241" class="indexterm"/>parameter refers to the maximum disparity variation within each connected component. If you do speckle filtering, set the parameter to a positive value; it will implicitly be multiplied by 16. Normally, 1 or 2 is good enough.</p>
</td></tr></tbody></table></div><p>With the <a id="id242" class="indexterm"/>preceding script, you'll be able to load the images and play around with parameters until you're happy with the disparity map generated by <code class="literal">StereoSGBM</code>.</p></div>
<div class="section" title="Object segmentation using the Watershed and GrabCut algorithms"><div class="titlepage" id="aid-173722"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Object segmentation using the Watershed and GrabCut algorithms</h1></div></div></div><p>Calculating <a id="id243" class="indexterm"/>a disparity map can be very <a id="id244" class="indexterm"/>useful to detect the foreground <a id="id245" class="indexterm"/>of an image, but <code class="literal">StereoSGBM</code> <a id="id246" class="indexterm"/>is not the only algorithm available to accomplish this, and in fact, <code class="literal">StereoSGBM</code> is more about gathering 3D information from 2D pictures, than anything else. <span class="strong"><strong>GrabCut</strong></span>, however, is a perfect tool for this purpose. The GrabCut algorithm follows a precise sequence of steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">A rectangle including the subject(s) of the picture is defined.</li><li class="listitem">The area lying outside the rectangle is automatically defined as a background.</li><li class="listitem">The data contained in the background is used as a reference to distinguish background areas from foreground areas within the user-defined rectangle.</li><li class="listitem">A <span class="strong"><strong>Gaussians Mixture Model</strong></span> (<span class="strong"><strong>GMM</strong></span>) models the foreground and background, and labels undefined pixels as probable background and foregrounds.</li><li class="listitem">Each pixel in the image is virtually connected to the surrounding pixels through virtual edges, and each edge gets a probability of being foreground or background, based on how similar it is in color to the pixels surrounding it.</li><li class="listitem">Each pixel (or node as it is conceptualized in the algorithm) is connected to either a foreground or a background node, which you can picture looking like this:<div class="mediaobject"><img src="../Images/image00207.jpeg" alt="Object segmentation using the Watershed and GrabCut algorithms"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">After the <a id="id247" class="indexterm"/>nodes have been <a id="id248" class="indexterm"/>connected to either <a id="id249" class="indexterm"/>terminal (background or foreground, also called a source <a id="id250" class="indexterm"/>and sink), the edges between nodes belonging to different terminals are cut (the famous cut part of the algorithm), which enables the separation of the parts of the image. This graph adequately represents the algorithm:<div class="mediaobject"><img src="../Images/image00208.jpeg" alt="Object segmentation using the Watershed and GrabCut algorithms"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><div class="section" title="Example of foreground detection with GrabCut"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>Example of foreground detection with GrabCut</h2></div></div></div><p>Let's look <a id="id251" class="indexterm"/>at an example. We start with the picture of a beautiful statue of an angel.</p><div class="mediaobject"><img src="../Images/image00209.jpeg" alt="Example of foreground detection with GrabCut"/></div><p style="clear:both; height: 1em;"> </p><p>We want to grab our angel and discard the background. To do this, we will create a relatively short script that will instantiate GrabCut, operate the separation, and then display the resulting image side by side to the original. We will do this using <code class="literal">matplotlib</code>, a <a id="id252" class="indexterm"/>very useful Python library, which makes displaying charts and images a trivial task:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

img = cv2.imread('images/statue_small.jpg')
mask = np.zeros(img.shape[:2],np.uint8)

bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

rect = (100,50,421,378)
cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)

mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img = img*mask2[:,:,np.newaxis]

plt.subplot(121), plt.imshow(img)
plt.title("grabcut"), plt.xticks([]), plt.yticks([])
plt.subplot(122), plt.imshow(cv2.cvtColor(cv2.imread('images/statue_small.jpg'), cv2.COLOR_BGR2RGB))
plt.title("original"), plt.xticks([]), plt.yticks([])
plt.show()</pre></div><p>This code is actually quite straightforward. Firstly, we load the image we want to process, and then we create a mask populated with zeros with the same shape as the image we've loaded:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt

img = cv2.imread('images/statue_small.jpg')
mask = np.zeros(img.shape[:2],np.uint8)</pre></div><p>We then create zero-filled foreground and background models:</p><div class="informalexample"><pre class="programlisting">bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)</pre></div><p>We could have populated these models with data, but we're going to initialize the GrabCut algorithm with a rectangle identifying the subject we want to isolate. So, background and foreground models are going to be determined based on the areas left out of the initial rectangle. This rectangle is defined in the next line:</p><div class="informalexample"><pre class="programlisting">rect = (100,50,421,378)</pre></div><p>Now to the interesting part! We run the GrabCut algorithm specifying the empty models and mask, and the fact that we're going to use a rectangle to initialize the operation:</p><div class="informalexample"><pre class="programlisting">cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)</pre></div><p>You'll also notice an integer after <code class="literal">fgdModel</code>, which is the number of iterations the algorithm is going to run on the image. You can increase these, but there is a point in which pixel classifications will converge, and effectively, you'll just be adding iterations without obtaining any more improvements.</p><p>After this, our <a id="id253" class="indexterm"/>mask will have changed to contain values between 0 and 3. The values, <code class="literal">0</code> and <code class="literal">2</code>, will be converted into zeros, and 1-3 into ones, and stored into <code class="literal">mask2</code>, which we can then use to filter out all zero-value pixels (theoretically leaving all foreground pixels intact):</p><div class="informalexample"><pre class="programlisting">mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img = img*mask2[:,:,np.newaxis]</pre></div><p>The last part of the code displays the images side by side, and here's the result:</p><div class="mediaobject"><img src="../Images/image00210.jpeg" alt="Example of foreground detection with GrabCut"/></div><p style="clear:both; height: 1em;"> </p><p>This is quite a <a id="id254" class="indexterm"/>satisfactory result. You'll notice that an area of background is left under the angel's arm. It is possible to apply touch strokes to apply more iterations; the technique is quite well illustrated in the <code class="literal">grabcut.py</code> file in <code class="literal">samples/python2</code> of your OpenCV installation.</p></div><div class="section" title="Image segmentation with the Watershed algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Image segmentation with the Watershed algorithm</h2></div></div></div><p>Finally, we <a id="id255" class="indexterm"/>take a quick look at the <a id="id256" class="indexterm"/>Watershed algorithm. The algorithm is called Watershed, because its conceptualization involves water. Imagine areas with low density (little to no change) in an image as valleys, and areas with high density (lots of change) as peaks. Start filling the valleys with water to the point where water from two different valleys is about to merge. To prevent the merging of water from different valleys, you build a barrier to keep them separated. The resulting barrier is the image segmentation.</p><p>As an Italian, I love food, and one of the things I love the most is a good plate of pasta with a pesto sauce. So here's a picture of the most vital ingredient for a pesto, basil:</p><div class="mediaobject"><img src="../Images/image00211.jpeg" alt="Image segmentation with the Watershed algorithm"/></div><p style="clear:both; height: 1em;"> </p><p>Now, we want to segment the image to separate the basil leaves from the white background.</p><p>Once more, we <a id="id257" class="indexterm"/>import <code class="literal">numpy</code>, <code class="literal">cv2</code>, and <code class="literal">matplotlib</code>, and then import our basil leaves' image:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import cv2
from matplotlib import pyplot as plt
img = cv2.imread('images/basil.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</pre></div><p>After changing <a id="id258" class="indexterm"/>the color to grayscale, we run a threshold on the image. This operation helps dividing the image in two, blacks and whites:</p><div class="informalexample"><pre class="programlisting">ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)</pre></div><p>Next up, we remove noise from the image by applying the <code class="literal">morphologyEx</code> transformation, an operation that consists of dilating and then eroding an image to extract features:</p><div class="informalexample"><pre class="programlisting">kernel = np.ones((3,3),np.uint8)
opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)</pre></div><p>By dilating the result of the <code class="literal">morphology</code> transformation, we can obtain areas of the image that are most certainly background:</p><div class="informalexample"><pre class="programlisting">sure_bg = cv2.dilate(opening,kernel,iterations=3)</pre></div><p>Conversely, we can obtain sure foreground areas by applying <code class="literal">distanceTransform</code>. In practical terms, of all the areas most likely to be foreground, the farther away from the "border" with the background a point is, the higher the chance it is foreground. Once we've obtained the <code class="literal">distanceTransform</code> representation of the image, we apply a threshold to determine with a highly mathematical probability whether the areas are foreground:</p><div class="informalexample"><pre class="programlisting">dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)
ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)</pre></div><p>At this stage, we have some <code class="literal">sure</code> foregrounds and backgrounds. Now, what about the areas in between? First of all, we need to determine these regions, which can be done by subtracting the <code class="literal">sure</code> foreground from the background:</p><div class="informalexample"><pre class="programlisting">sure_fg = np.uint8(sure_fg)
unknown = cv2.subtract(sure_bg,sure_fg)</pre></div><p>Now that we <a id="id259" class="indexterm"/>have these areas, we can build our <a id="id260" class="indexterm"/>famous "barriers" to stop the water from merging. This is done with the <code class="literal">connectedComponents</code> function. We took a glimpse at the graph theory when we analyzed the GrabCut algorithm, and conceptualized an image as a set of nodes that are connected by edges. Given the sure foreground areas, some of these nodes will be connected together, but some won't. This means that they belong to different water valleys, and there should be a barrier between them:</p><div class="informalexample"><pre class="programlisting">ret, markers = cv2.connectedComponents(sure_fg)</pre></div><p>Now we add 1 to the background areas because we only want unknowns to stay at <code class="literal">0</code>:</p><div class="informalexample"><pre class="programlisting">markers = markers+1
markers[unknown==255] = 0</pre></div><p>Finally, we open the gates! Let the water fall and our barriers be drawn in red:</p><div class="informalexample"><pre class="programlisting">markers = cv2.watershed(img,markers)
img[markers == -1] = [255,0,0]
plt.imshow(img)
plt.show()</pre></div><p>Now, let's show the result:</p><div class="mediaobject"><img src="../Images/image00212.jpeg" alt="Image segmentation with the Watershed algorithm"/></div><p style="clear:both; height: 1em;"> </p><p>Needless to <a id="id261" class="indexterm"/>say, I am now hungry!</p></div></div>
<div class="section" title="Summary" id="aid-181NK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Summary</h1></div></div></div><p>In this chapter, we learned about gathering three-dimensional information from bi-dimensional input (a video frame or an image). Firstly, we examined depth cameras, and then epipolar geometry and stereo images, so we are now able to calculate disparity maps. Finally, we looked at image segmentation with two of the most popular methods: GrabCut and Watershed.</p><p>This chapter has introduced us to the world of interpreting information provided by images and we are now ready to explore another important feature of OpenCV: feature descriptors and keypoint detection.</p></div></body></html>