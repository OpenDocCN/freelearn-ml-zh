<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Algorithms/Techniques Related to Machine Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are a few algorithms and techniques, related to the machine learning examples in this book, that we were not able to go into much detail about in the preceding chapters. We will address that here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article>
                
<p>In multiple examples (including those in <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml">Chapter 4</a>, <em>Regression</em> and <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a><em>, Classification</em>), we took advantage of an optimization technique called <strong>gradient descent</strong>. There are multiple variants of the gradient descent method, and, in general, you will see them pretty much everywhere in the machine learning world. Most prominently, they are utilized in the determination of optimal coefficients for algorithms such as linear or logistic regression, and thus, they often also play a role in more complicated techniques at least partially based on linear/logistic regression (such as neural networks).</p>
<p>The general idea of gradient descent methods is to determine a direction and magnitude of change in some parameters that will move you in the right direction to optimize some measure (such as error). Think about standing on some landscape. To move toward lower elevations, you need to take steps in the downward direction. This is basically what gradient descent is doing algorithmically when it is optimizing parameters.</p>
<p>Let's gain some more intuition about this process by looking at so-called <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>), which is an incremental kind of gradient descent. If you remember, we actually utilized SGD in our implementation of logistic regression in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml" target="_blank">Chapter 5</a>, <em>Classification</em>. In that example, we implemented the training or fitting of our logistic regression parameters as follows:</p>
<pre>// logisticRegression fits a logistic regression model<br/>// for the given data.<br/>func logisticRegression(features *mat64.Dense, labels []float64, numSteps int, learningRate float64) []float64 {<br/><br/>    // Initialize random weights.<br/>    _, numWeights := features.Dims()<br/>    weights := make([]float64, numWeights)<br/><br/>    s := rand.NewSource(time.Now().UnixNano())<br/>    r := rand.New(s)<br/><br/>    for idx, _ := range weights {<br/>        weights[idx] = r.Float64()<br/>    }<br/><br/>    // Iteratively optimize the weights.<br/>    for i := 0; i &lt; numSteps; i++ {<br/><br/>        // Initialize a variable to accumulate error for this iteration.<br/>        var sumError float64<br/><br/>        // Make predictions for each label and accumulate error.<br/>        for idx, label := range labels {<br/><br/>            // Get the features corresponding to this label.<br/>            featureRow := mat64.Row(nil, idx, features)<br/><br/>            // Calculate the error for this iteration's weights.<br/>            pred := logistic(featureRow[0]*weights[0] + featureRow[1]*weights[1])<br/>            predError := label - pred<br/>            sumError += math.Pow(predError, 2)<br/><br/>            // Update the feature weights.<br/>            for j := 0; j &lt; len(featureRow); j++ {<br/>                weights[j] += learningRate * predError * pred * (1 - pred) * featureRow[j]<br/>            }<br/>        }<br/>   }<br/><br/>   return weights<br/>}<br/><br/></pre>
<p>The loop under the <kbd>// Iteratively optimize the weights</kbd> comment implements SGD to optimize the logistic regression parameters. Let's pick apart this loop to determine what exactly is happening.</p>
<p>First, we calculate the output of our model with the current weights and the difference between our prediction and the ideal value (the actual observation, that is):</p>
<pre>// Calculate the error for this iteration's weights.<br/>pred := logistic(featureRow[0]*weights[0] + featureRow[1]*weights[1])<br/>predError := label - pred<br/>sumError += math.Pow(predError, 2)</pre>
<p>Then, according to SGD, we are going to calculate an update to our parameters (in this case <kbd>weights</kbd>) according to the following:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="24" width="486" src="assets/9585b444-6017-4174-95fd-103c4941f2f9.png"/></div>
<p>The <strong>gradient</strong> is the mathematical gradient of the cost function in your problem.</p>
<div class="packt_infobox">More detailed mathematical information about this quantity can be found here:<br/>
<a href="http://mathworld.wolfram.com/Gradient.html">http://mathworld.wolfram.com/Gradient.html</a></div>
<p>The update can then be applied to the parameters as follows:</p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="243" src="assets/2a207315-7ebb-4ca5-824f-5794482733b1.png"/></div>
<p>In the case of our logistic regression model, this works out to have the following form:</p>
<pre>// Update the feature weights.<br/>for j := 0; j &lt; len(featureRow); j++ {<br/>    weights[j] += learningRate * predError * pred * (1 - pred) * featureRow[j]<br/>}</pre>
<p>This type of SGD is pretty widely used in machine learning. However, in some cases, this kind of gradient descent can lead to overfitting or getting stuck in local minimums/maximums (rather than finding the global optimum).</p>
<p>To address some of these issues, you can utilize a variant of gradient descent called <strong>batch gradient descent</strong>. In batch gradient descent, you calculate each of the parameter updates based on gradients in all of the training dataset, as opposed to a gradient for a particular observation or row of the dataset. This helps you prevent overfitting, but it can also be rather slow and have memory issues because you need to calculate gradient with respect to a whole dataset for each parameter. <strong>Mini-batch</strong> <strong>gradient descent</strong>, which is another variant, attempts to maintain some of the benefits of batch gradient descent while being more computationally tractable. In mini-batch gradient descent, the gradients are calculated on subsets of the training dataset rather than the whole training dataset.</p>
<div class="packt_infobox">In the case of logistic regression, you may see the use of gradient ascent or descent, where gradient ascent is the same thing as gradient descent except that it is applied to the negative of the cost function. The logistic cost function gives you both of these options as long as you are consistent. This is further discussed at <a href="https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression">https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression</a>.</div>
<div class="packt_tip">Gradient descent methods are also already implemented by the gonum team in <kbd>gonum.org/v1/gonum/optimize</kbd>. See these docs for more information:<br/>
<a href="https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent">https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entropy, information gain, and related methods</h1>
                </header>
            
            <article>
                
<p>In <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a>, <em>Classification</em>, we explored decision tree methods in which models consisted of a tree of if/then statements. These if/then portions of the decision tree split the prediction logic based on one of the features of the training set. In an example where we were trying to classify medical patients into unhealthy or healthy categories, a decision tree might first split based on a gender feature, then based on an age feature, then based on a weight feature, and so on, eventually landing on healthy or unhealthy.</p>
<p>How does the algorithm choose which features to use first in the decision tree? In the preceding example, we could split on gender first, or weight first, and any other feature first. We need a way to arrange our splits in an optimal way, such that our model makes the best predictions that it can make.</p>
<p>Many decision tree model implementations, including the one we used in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml" target="_blank">Chapter 5</a>, <em>Classification</em>, use a quantity called <strong>entropy</strong> and an analysis of <strong>information gain</strong> to build up decision trees. To illustrate this process, let's consider an example. Assume that you have the following data about numbers of healthy people versus various characteristics of those people:</p>
<table style="width: 391px;height: 274px">
<tbody>
<tr>
<td/>
<td><strong>Healthy</strong></td>
<td><strong>Unhealthy</strong></td>
</tr>
<tr>
<td><strong>Vegan Diet</strong></td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td><strong>Vegetarian Diet</strong></td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td><strong>Meat Eating Diet</strong></td>
<td>3</td>
<td>4</td>
</tr>
</tbody>
</table>
<table style="width: 329px;height: 132px">
<tbody>
<tr>
<td/>
<td><strong>Healthy</strong></td>
<td><strong>Unhealthy</strong></td>
</tr>
<tr>
<td><strong>Age 40+</strong></td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td><strong>Age &lt; 40</strong></td>
<td>9</td>
<td>2</td>
</tr>
</tbody>
</table>
<p class="mce-root">Here, we have two features in our data, Diet and Age, and we would like to build a decision tree to predict if people are healthy or not based on Diet and Age. To do this, we need to decide whether we should split our decision tree first on Age or first on Diet. Notice that we also have a total of 12 healthy people and 7 unhealthy people represented in the data.</p>
<p>To begin, we will calculate the overall or total entropy of the classes in our data. This is defined as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="19" width="335" src="assets/0d24303f-5049-4827-8236-938d7f26a921.png"/></div>
<p>Here, <em>p<sub>1</sub></em>, <em>p<sub>2</sub>,</em> and so on, are the probabilities of a first category, a second category, and so on. In our particular case (because we have 12 healthy people and 7 unhealthy people), our total entropy is as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="36" width="388" src="assets/4ce3e27c-05ed-47d1-bcf9-44a96c149cce.png"/></div>
<p>This <em>0.95</em> measure represents the homogeneity of our health data. It goes between 0 and 1, with high values corresponding to less homogeneous data.</p>
<p>To determine whether we should split our tree first on Age or first on Diet, we will calculate which of these features gives us the most information gain. Simply put, we will find the feature that gives us the most homogeneity after splitting on that feature, as measured by the preceding entropy. This decrease in entropy is called <strong>information gain</strong>.</p>
<p>The information gain for a certain feature in our example is defined as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="230" src="assets/3158bf53-063b-4137-8163-579a8b5b2fea.png"/></div>
<p>Here, <em>E(Health, Feature)</em> is a second measure of entropy with respect to the given feature (<em>Age</em> or <em>Diet</em>). For Diet, this second measure would be calculated as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="383" src="assets/5c8cb814-8e7c-4157-8b5f-7dea151e83fb.png"/></div>
<p>The quantities <em>p<sub>40+</sub></em> and <em>p<sub>&lt;40</sub></em> are the probabilities of having an age of <em>40</em>+ or <em>&lt;40</em> (8/19 and 11/19, respectively). The quantities <em>E(Health,40+)</em> and <em>E(Health,&lt;40)</em> are the health entropies (as defined in the preceding formula) but only using the counts corresponding to those Age <em>40+</em> and Age <em>&lt;40</em>, respectively.</p>
<p>For our example data, the information gain for the Age feature comes out to <em>0.152</em> and the information gain for the Diet feature comes out to <em>0.079</em>. Thus, we would choose to split our decision tree on the Age feature first because it increases the overall homogeneity of our data the most.</p>
<div class="packt_tip">You can find out more about building decision trees based on entropy at <a href="http://www.saedsayad.com/decision_tree.htm">http://www.saedsayad.com/decision_tree.htm</a>, and you can see an example implementation in Go at <a href="https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go">https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p><a href="3e51d99f-2f6a-4d4b-876f-3b44f74b9a20.xhtml" target="_blank">Chapter 8</a>, <em>Neural Networks and Deep Learning</em>, included an example of a neural network built from scratch in Go. This neural network included an implementation of the backpropagation method to train neural networks, which can be found in almost any neural network code. We discussed some details in that chapter. However, this method is utilized so often that we wanted to go through it step by step here.</p>
<p>To train a neural network with backpropagation, we do the following for each of a series of epochs:</p>
<ol>
<li>Feed the training data through the neural network to produce output.</li>
<li>Calculate an error between the expected output and the predicted output.</li>
<li>Based on the error, calculate updates for the neural network weights and biases.</li>
<li>Propagate these updates back into the network.</li>
</ol>
<p>As a reminder, our implementation of this procedure for a network with a single hidden layer looked like the following (where <kbd>wHidden</kbd> and <kbd>wOut</kbd> are our hidden layer and output layer weights and <kbd>bHidden</kbd> and <kbd>bOut</kbd> are our hidden layer and output layer biases):</p>
<pre>105     // Define the output of the neural network.<br/>106     output := mat.NewDense(0, 0, nil)<br/>107 <br/>108     // Loop over the number of epochs utilizing<br/>109     // backpropagation to train our model.<br/>110     for i := 0; i &lt; nn.config.numEpochs; i++ {<br/>111<br/>112         // Complete the feed forward process.
113         hiddenLayerInput := mat.NewDense(0, 0, nil)
114         hiddenLayerInput.Mul(x, wHidden)
115         addBHidden := func(_, col int, v float64) float64 { return v + bHidden.At(0, col) }
116         hiddenLayerInput.Apply(addBHidden, hiddenLayerInput)
117 
118         hiddenLayerActivations := mat.NewDense(0, 0, nil)
119         applySigmoid := func(_, _ int, v float64) float64 { return sigmoid(v) }
120         hiddenLayerActivations.Apply(applySigmoid, hiddenLayerInput)
121 
122         outputLayerInput := mat.NewDense(0, 0, nil)
123         outputLayerInput.Mul(hiddenLayerActivations, wOut)
124         addBOut := func(_, col int, v float64) float64 { return v + bOut.At(0, col) }
125         outputLayerInput.Apply(addBOut, outputLayerInput)
126         output.Apply(applySigmoid, outputLayerInput)
127 
128         // Complete the backpropagation.
129         networkError := mat.NewDense(0, 0, nil)
130         networkError.Sub(y, output)
131 
132         slopeOutputLayer := mat.NewDense(0, 0, nil)
133         applySigmoidPrime := func(_, _ int, v float64) float64 { return sigmoidPrime(v) }
134         slopeOutputLayer.Apply(applySigmoidPrime, output)
135         slopeHiddenLayer := mat.NewDense(0, 0, nil)
136         slopeHiddenLayer.Apply(applySigmoidPrime, hiddenLayerActivations)
137 
138         dOutput := mat.NewDense(0, 0, nil)
139         dOutput.MulElem(networkError, slopeOutputLayer)
140         errorAtHiddenLayer := mat.NewDense(0, 0, nil)
141         errorAtHiddenLayer.Mul(dOutput, wOut.T())
142 
143         dHiddenLayer := mat.NewDense(0, 0, nil)
144         dHiddenLayer.MulElem(errorAtHiddenLayer, slopeHiddenLayer)
145 
146         // Adjust the parameters.
147         wOutAdj := mat.NewDense(0, 0, nil)
148         wOutAdj.Mul(hiddenLayerActivations.T(), dOutput)
149         wOutAdj.Scale(nn.config.learningRate, wOutAdj)
150         wOut.Add(wOut, wOutAdj)
151 
152         bOutAdj, err := sumAlongAxis(0, dOutput)
153         if err != nil {
154             return err
155         }
156         bOutAdj.Scale(nn.config.learningRate, bOutAdj)
157         bOut.Add(bOut, bOutAdj)
158 
159         wHiddenAdj := mat.NewDense(0, 0, nil)
160         wHiddenAdj.Mul(x.T(), dHiddenLayer)
161         wHiddenAdj.Scale(nn.config.learningRate, wHiddenAdj)
162         wHidden.Add(wHidden, wHiddenAdj)
163 
164         bHiddenAdj, err := sumAlongAxis(0, dHiddenLayer)
165         if err != nil {
166             return err
167         }
168         bHiddenAdj.Scale(nn.config.learningRate, bHiddenAdj)
169         bHidden.Add(bHidden, bHiddenAdj)
170     }</pre>
<p>Let's pick this implementation apart in detail to understand exactly what's happening.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The feed forward process that produces our output does the following:</p>
<ol>
<li>Multiplies the input data by the hidden layer weights, adds the hidden layer biases, and applies the sigmoid activation function to calculate the output of the hidden layer, <kbd>hiddenLayerActivations</kbd> (lines 112 to 120 in the preceding snippet).</li>
<li>Multiples the <kbd>hiddenLayerActivations</kbd> by the output layer weights, then adds the output layer biases, and applies the sigmoid activation function to calculate the <kbd>output</kbd> (lines 122 to 126).</li>
</ol>
<p>Notice that in the feed forward process, we are starting with the input data at the input layer and working our way forward through the hidden layer until we reach the output.</p>
<p>After the feed forward process, we need to calculate optimal updates to our weights and biases. As you might expect after going through the gradient descent portion of this Appendix, gradient descent is a perfect fit to find these weights and biases. Lines 129 through 144 in the preceding snippet implement SGD.</p>
<p>Finally, we need to apply these updates backward through the network in lines 147 through 169. This is the backward propagation of updates that gives backpropagation its name. There isn't anything too special about this process, we just perform the following:</p>
<ol>
<li>Apply the calculated updates to the output weights and biases (lines 147 to 157).</li>
<li>Apply the calculated updates to the hidden layer weights and biases (lines 159 to 169).</li>
</ol>
<p>Notice how we start at the output and work our way back to the input applying the changes.</p>
<div class="packt_tip">You can find a very detailed discussion of backpropagation, including mathematical proofs, here:<br/>
<a href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a></div>
<div class="packt_infobox">Backpropagation started to be widely utilized after a 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. Although the method is utilized across the industry in neural networks, Geoffrey Hinton recently came out to say that he is <em>deeply suspicious</em> of backpropagation and suggests that we need to work hard to find an alternative.</div>


            </article>

            
        </section>
    </body></html>