<html><head></head><body>
		<div id="_idContainer098">
			<h1 id="_idParaDest-85"><a id="_idTextAnchor088"/><em class="italic">Chapter 5</em>: Practical Exposure to Using LIME in ML</h1>
			<p>After reading the last chapter, you should now have a good conceptual understanding of <strong class="bold">Local Interpretable Model-agnostic Explanations (LIME)</strong>. We saw how the LIME Python framework can explain black-box models for classification problems. We also discussed some of the pros and cons of the LIME framework. In practice, LIME is still one of the most popular XAI frameworks as it can be easily applied to tabular datasets and text and image datasets. LIME can provide model-agnostic local explanations for solving both regression and classification problems. </p>
			<p>In this chapter, you will get much more in-depth practical exposure to using LIME in ML. These are the main topics of discussion for this chapter:</p>
			<ul>
				<li>Using LIME on tabular data</li>
				<li>Explaining image classifiers with LIME</li>
				<li>Using LIME on text data</li>
				<li>LIME for production-level systems</li>
			</ul>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor089"/>Technical requirements </h1>
			<p>Like the previous chapter, this chapter is very technical with code walk-throughs in Python and Jupyter notebooks. For this chapter, the code and dataset resources can be downloaded or cloned from the GitHub repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05</a>. Like the previous chapters, we will be using Python and Jupyter notebooks to run the code and generate the necessary outputs. All other relevant details are provided in the notebooks, and I recommend that you all run the notebooks while going through the chapter content to get a better understanding of the topics covered.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor090"/>Using LIME on tabular data</h1>
			<p>In the <em class="italic">Practical example of using LIME for classification problems</em> section of <a href="B18216_04_ePub.xhtml#_idTextAnchor076"><em class="italic">Chapter 4</em></a>, <em class="italic">LIME for Model Interpretability</em>, we discussed how to set up LIME in<a id="_idIndexMarker342"/> Python and how to use LIME to explain classification ML models. The dataset used for the tutorial in <a href="B18216_04_ePub.xhtml#_idTextAnchor076"><em class="italic">Chapter 4</em></a><em class="italic">, LIME for Model Interpretability</em> (<a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb</a>) was a tabular structured data. In this section, we will discuss using LIME to explain regression models that are built on tabular data.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor091"/>Setting up LIME</h2>
			<p>Before starting the code walk-through, I would ask you to check the following notebook, <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_tabular_data.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_tabular_data.ipynb</a>, which already contains the steps needed to understand the concept that we are going to discuss now in more depth. I assume that most of the Python libraries that we will use for this tutorial are already installed on your system. But if not, please run the following command to install the upgraded versions of the Python libraries that we are going to use:</p>
			<pre class="source-code">!pip install --upgrade pandas numpy matplotlib seaborn scikit-learn lime</pre>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor092"/>Discussion about the dataset</h2>
			<p>For this tutorial, we will <a id="_idIndexMarker343"/>use the <em class="italic">Diabetes dataset</em> from <em class="italic">scikit-learn datasets</em> (<a href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset</a>). This dataset is used to predict the <em class="italic">disease progression level</em> of diabetes. It contains around <em class="italic">442 samples</em> with <em class="italic">10 baseline features</em> – <em class="italic">age</em>, <em class="italic">sex</em>, <em class="italic">body mass index (bmi)</em>, <em class="italic">average blood pressure (bp)</em>, <em class="italic">total serum cholesterol (s1)</em>, <em class="italic">low-density lipoproteins (s2)</em>, <em class="italic">high-density lipoproteins (s3)</em>,<em class="italic"> total cholesterol / HDL (s4)</em>, <em class="italic">possibly log of serum triglycerides level (s5)</em>, and <em class="italic">blood sugar level (s6)</em>. The dataset is quite interesting and relevant, considering that the underlying problem of monitoring diabetes progression is an important practical problem. </p>
			<p>The feature variables provided in the dataset are already normalized by centering the feature values around the mean and scaling by the standard deviation times the number of samples (N):</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/Formula01.jpg" alt=""/>
				</div>
			</div>
			<p>More information about the original dataset can be found at <a href="https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html">https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html</a>.</p>
			<p>To load the dataset, just execute the following lines of code:</p>
			<pre class="source-code">from sklearn import datasets</pre>
			<pre class="source-code">dataset = datasets.load_diabetes()</pre>
			<pre class="source-code">features, labels = dataset.data, dataset.target</pre>
			<p>You can perform the<a id="_idIndexMarker344"/> necessary EDA steps if needed, but since our main focus is to use LIME for explaining black-box models, we will not spend too much effort on EDA for the purpose of this tutorial.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor093"/>Discussions about the model</h2>
			<p>As demonstrated in the notebook tutorial, we have used a <strong class="bold">Gradient Boosting Regressor</strong> (<strong class="bold">GBR</strong>) (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html</a>) model to train our<a id="_idIndexMarker345"/> predictive model. However, any regression ML algorithm can be used instead of GBR as the model itself is regarded as any black-box model by the LIME algorithm. Also, when we evaluated the trained model on the unseen data, we observed a <strong class="bold">Mean Absolute Percentage Error (MAPE)</strong> of 0.37, a <strong class="bold">Mean Square Error (MSE)</strong> of 2,538, and an <strong class="bold">R-squared coefficient</strong> score of 0.6. All these results indicate that our model is not very good and definitely has room for improvement. So, if such a model is deployed in production-level systems, there can be many questions asked by the end stakeholders as it is always difficult for them to trust models that are not accurate. Also, algorithms such as GBR are not inherently interpretable and the complexity of the algorithm depends on hyperparameters including the number of estimators and the depth of the tree. Thus, model explainability frameworks such as LIME are not<a id="_idIndexMarker346"/> just an add-on step, but a necessary part of the process of building ML models. Next, we will see how easily LIME can be applied to explain black-box regression models with just a few lines of code.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor094"/>Application of LIME</h2>
			<p>As we have seen in the<a id="_idIndexMarker347"/> previous chapter, we can easily support the LIME framework for tabular data with the following commands:</p>
			<pre class="source-code">import lime</pre>
			<pre class="source-code">import lime.lime_tabular</pre>
			<p>Once the LIME module is successfully imported, we will need to create an explainer object:</p>
			<pre class="source-code">explainer = lime.lime_tabular.LimeTabularExplainer(</pre>
			<pre class="source-code">    x_train, mode='regression',</pre>
			<pre class="source-code">    class_names=['disease_progression'],</pre>
			<pre class="source-code">    feature_names=dataset.feature_names)</pre>
			<p>Then, we just need to take the data instance and provide local explainability to it:</p>
			<pre class="source-code">exp = explainer.explain_instance(x_test[i], model.predict, </pre>
			<pre class="source-code">                                 num_features=5)</pre>
			<pre class="source-code">exp.show_in_notebook(show_table=True)</pre>
			<p>We will get the following output from the preceding lines of code:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B18216_05_001.jpg" alt="Figure 5.1 – Output visualization from the LIME framework when applied to a regression model trained on a tabular dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Output visualization from the LIME framework when applied to a regression model trained on a tabular dataset</p>
			<p><em class="italic">Figure 5.1</em> illustrates the visualization-based explanations provided by the LIME framework. </p>
			<p>Next, let's try to understand<a id="_idIndexMarker348"/> what the output visualization in <em class="italic">Figure 5.1</em> is telling us:</p>
			<ul>
				<li>The left-most visualization from <em class="italic">Figure 5.1</em> shows a range of possible values and the position of the model's predicted outcome. Intuitively speaking, all model predictions should lie within the minimum and the maximum possible value as this indicates to the user to compare the current forecast with the best-case and the worst-case values.</li>
				<li>The middle visualization shows which features contribute to the prediction being on the higher side or the lower side. Considering our prior knowledge of diabetes, a higher BMI, as well as raised blood pressure and serum triglyceride levels, do indicate increasing progression of the disease.</li>
				<li>The right-most visualization in <em class="italic">Figure 5.1</em> shows us the actual local data values for the most important features identified, arranged in descending order of their relevance.</li>
			</ul>
			<p>The explanations provided by the LIME framework are human-interpretable to a great extent and do give us an indication of the feature-value pairs used by the black-box model to make predictions.</p>
			<p>So, this is how we can use LIME to explain black-box regression models trained on tabular data with just a few lines of code. But, as we discussed in <a href="B18216_04_ePub.xhtml#_idTextAnchor076"><em class="italic">Chapter 4</em></a>, <em class="italic">LIME for Model Interpretability</em>, under the <em class="italic">Potential pitfalls</em> section, explanations provided by LIME are not always holistic and may have some inconsistencies. This is something we all need to be mindful of. However, LIME explanations, coupled with a thorough EDA, data-centric XAI, counterfactual explanations, and other model<a id="_idIndexMarker349"/> explainability methods, can provide a powerful, holistic explainability to black-box models trained on tabular datasets.</p>
			<p>Now, let's explore how to use LIME for classifiers trained on unstructured data such as images in the next section.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor095"/>Explaining image classifiers with LIME</h1>
			<p>In the previous section, we have <a id="_idIndexMarker350"/>seen how we can easily apply LIME to explain models trained on tabular data. However, the main challenge always comes while explaining complex deep learning models trained on unstructured data such as images. Generally, deep learning models are much more efficient than conventional ML models on image data as these models have the ability to perform <em class="italic">auto feature extraction</em>. They can extract complex <em class="italic">low-level features</em> such as <em class="italic">stripes</em>, <em class="italic">edges</em>, <em class="italic">contours</em>, <em class="italic">corners</em>, and <em class="italic">motifs</em>, and even <em class="italic">higher-level features</em> such as <em class="italic">larger shapes</em> and <em class="italic">certain parts of the object</em>. These higher-level features are usually referred to as <strong class="bold">Regions of Interest</strong> <strong class="bold">(RoI)</strong> in the image, or <strong class="bold">superpixels</strong>, as they<a id="_idIndexMarker351"/> are collections of pixels of the image<a id="_idIndexMarker352"/> that cover a particular area of the image. Now, the low-level features are not human-interpretable, but the high-level features are human-interpretable, as any non-technical end user will relate to the images with respect to the higher-level features. LIME also works in a similar fashion. The algorithm tries to highlight the superpixels in images that contribute positively or negatively to the model's decision-making process. So, let's see how LIME can be used to explain image classifiers.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor096"/>Setting up the required Python modules</h2>
			<p>Before we begin the code walk-through, please check the notebook provided in the code repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data.ipynb</a>. The notebook contains the necessary details required for the practical application of the concepts. In this section, I will give you a walk-through of the code and explain all the steps covered in the notebook tutorial. Use the following command to install the upgraded versions of the Python libraries if not already installed:</p>
			<pre class="source-code">!pip install --upgrade pandas numpy matplotlib seaborn tensorflow lime scikit-image</pre>
			<p>Next, let's discuss the model<a id="_idIndexMarker353"/> used in this example.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor097"/>Using a pre-trained TensorFlow model as our black-box model</h2>
			<p>For this tutorial, we have used a <em class="italic">pre-trained TensorFlow Keras Xception model</em> as our black-box model. The model is pre-trained on the ImageNet dataset (<a href="https://www.image-net.org/">https://www.image-net.org/</a>), which is one of the most popular benchmarking datasets for image classification. The pre-trained model can be loaded with the following lines of code:</p>
			<pre class="source-code">from tensorflow.keras.applications.xception import Xception</pre>
			<pre class="source-code">model = Xception(weights="imagenet")</pre>
			<p>In order to use any inference data for image classification, we will also need to perform the necessary preprocessing steps. Please refer to the notebook at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data</a>.ipynb for the necessary pre-processing methods.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor098"/>Application of LIME Image Explainers</h2>
			<p>In this subsection, we will<a id="_idIndexMarker354"/> see how the LIME framework can be used to identify <em class="italic">super-pixels</em> or regions from the image used by the model to predict the specific outcome. We will first need to define an image <strong class="source-inline">explainer</strong> object:</p>
			<pre class="source-code">explainer = lime_image.LimeImageExplainer()</pre>
			<p>Next, we will need to pass the inference data (<strong class="source-inline">normalized_img[0]</strong>) to the <strong class="source-inline">explainer</strong> object and use the LIME framework to highlight superpixels that have the maximum positive and negative influence on the model's prediction:</p>
			<pre class="source-code">exp = explainer.explain_instance(normalized_img[0], </pre>
			<pre class="source-code">                                 model.predict, </pre>
			<pre class="source-code">                                 top_labels=5, </pre>
			<pre class="source-code">                                 hide_color=0, </pre>
			<pre class="source-code">                                 num_samples=1000)</pre>
			<pre class="source-code">image, mask = exp.get_image_and_mask(exp_class, </pre>
			<pre class="source-code">                                     positive_only=False,</pre>
			<pre class="source-code">                                     num_features=6,</pre>
			<pre class="source-code">                                     hide_rest=False,</pre>
			<pre class="source-code">                                     min_weight=0.01)</pre>
			<pre class="source-code">    </pre>
			<pre class="source-code">plt.imshow(mark_boundaries(image, mask))</pre>
			<pre class="source-code">plt.axis('off')</pre>
			<pre class="source-code">plt.show()</pre>
			<p>As an output to the preceding<a id="_idIndexMarker355"/> lines of code, we will get certain highlighted portions of the image that contribute to the model's prediction, in both a positive and negative manner:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18216_05_002.jpg" alt="Figure 5.2 – (Left) Original inference image. (Middle) Most important image superpixel. (Right) Image with superposed mask superpixel on the original data highlighted in green&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – (Left) Original inference image. (Middle) Most important image superpixel. (Right) Image with superposed mask superpixel on the original data highlighted in green</p>
			<p>In <em class="italic">Figure 5.2</em>, the left-most image was used as the inference image. When the trained model was applied to the inference image, the top prediction of the model was a <em class="italic">tiger shark</em>. </p>
			<p>The prediction was actually <a id="_idIndexMarker356"/>correct. However, in order to explain the model, the LIME algorithm can highlight the superpixel, which has the maximum influence on the prediction. From the middle and the right-most images in <em class="italic">Figure 5.2</em>, we can see that the black-box model was actually good and trustworthy as the relevant superpixel captured by the LIME algorithm indicates the presence of a tiger shark.</p>
			<p>The superpixel estimated by the LIME algorithm can be displayed using the following lines of code:</p>
			<pre class="source-code">plt.imshow(exp.segments)</pre>
			<pre class="source-code">plt.axis('off')</pre>
			<pre class="source-code">plt.show()</pre>
			<p>We can also form a heatmap highlighting the importance of each of the superpixels, which gives us further insight into the functioning of the black-box model:</p>
			<pre class="source-code">dict_heatmap = dict(exp.local_exp[exp.top_labels[0]])</pre>
			<pre class="source-code">heatmap = np.vectorize(dict_heatmap.get)(exp.segments) </pre>
			<pre class="source-code">plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(),</pre>
			<pre class="source-code">           vmax = heatmap.max())</pre>
			<pre class="source-code">plt.colorbar()</pre>
			<pre class="source-code">plt.show()</pre>
			<p>The output obtained is<a id="_idIndexMarker357"/> shown in <em class="italic">Figure 5.3</em>:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B18216_05_003.jpg" alt="Figure 5.3 – (Left) Image showing all the superpixels picked up by the LIME algorithm. &#13;&#10;(Right) Heatmap of the superpixels based on their importance in terms of the model's prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – (Left) Image showing all the superpixels picked up by the LIME algorithm. (Right) Heatmap of the superpixels based on their importance in terms of the model's prediction</p>
			<p>The heatmap from <em class="italic">Figure 5.3</em> provides us with some insight into important superpixels, which is also <a id="_idIndexMarker358"/>easy when it comes to any non-technical user interpreting any black-box model.</p>
			<p>So, we have seen how LIME can explain even complicated deep learning models trained on image data in just a few lines of code. I found LIME to be one of the most effective algorithms to visually explain deep learning-based image classifiers without presenting any complicated statistical or numerical values or complicated graphical visualizations. Unlike tabular data, I felt explanations provided to image classifiers are more robust, stable, and human-interpretable. It is definitely one of my favorite methods for interpreting image classifiers, and before moving any image classification model to production, I strongly recommend applying LIME as an additional evaluation step to gain more confidence in the trained model.</p>
			<p>In the next section, let's explore LIME for models trained on text data.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor099"/>Using LIME on text data</h1>
			<p>In the previous section, we <a id="_idIndexMarker359"/>discussed how LIME is an effective approach to explaining complicated black-box models trained on image datasets. Like images, text is also a form of unstructured data, which is very much different from structured tabular data. Explaining such black-box models trained on unstructured data is always very challenging. But LIME can also be applied to models trained on text data. </p>
			<p>Using the LIME algorithm, we can analyze whether the presence of a particular word or group of words increases the probability of predicting a specific outcome. In other words, LIME helps to <a id="_idIndexMarker360"/>highlight the importance of text tokens or words that can influence the model's outcome toward a particular class. In this section, we will see how LIME can be used to interpret text classifiers.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor100"/>Installing the required Python modules</h2>
			<p>Like the previous tutorials, the complete notebook tutorial is available at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_text_data.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_text_data.ipynb</a>. Although the necessary instructions needed to run the notebook are clearly documented in the notebook itself, similar to the previous tutorials, I will provide the necessary details to walk you through the implementation. Using the following commands, you can install the modules required to run the code:</p>
			<pre class="source-code">!pip install --upgrade pandas numpy matplotlib seaborn scikit-learn nltk lime xgboost swifter</pre>
			<p>For text-related operations on the underlying dataset, I will be mainly using the NLTK Python framework. So, you will need to download certain <strong class="source-inline">nltk</strong> modules by executing the following commands:</p>
			<pre class="source-code">nltk.download('stopwords')</pre>
			<pre class="source-code">nltk.download('wordnet')</pre>
			<pre class="source-code">nltk.download('punkt')</pre>
			<pre class="source-code">nltk.download('averaged_perceptron_tagger')</pre>
			<p>In the tutorial, we will try to explain a text classifier designed to perform sentiment analysis by classifying the text data into positive and negative classes. </p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor101"/>Discussions about the dataset used for training the model</h2>
			<p>We have used the <strong class="bold">Sentiment Polarity dataset v2.0</strong> consisting of <em class="italic">Movie reviews</em> for this tutorial used for sentiment <a id="_idIndexMarker361"/>analysis from text data. The dataset consists of about 1,000 samples of positive and negative movie reviews. More information about the dataset can be found on the source website: <a href="https://www.cs.cornell.edu/people/pabo/movie-review-data/">https://www.cs.cornell.edu/people/pabo/movie-review-data/</a>. The dataset is also provided in the GitHub repository of this chapter: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05</a>. </p>
			<p class="callout-heading">Sentiment Polarity dataset v2.0 </p>
			<p class="callout">This data was first used in Bo Pang and Lillian Lee, "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts", Proceedings of the ACL, 2004.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor102"/>Discussions about the text classification model </h2>
			<p>Unlike the previous image classifier tutorial, we have not used a pre-trained model. We have trained an <strong class="bold">XGBoost Classifier</strong> (<a href="https://xgboost.readthedocs.io/en/stable/">https://xgboost.readthedocs.io/en/stable/</a>) from scratch with the necessary data preprocessing, preparation, and feature extraction steps as covered in the <a id="_idIndexMarker362"/>notebook. XGBoost is an ensemble learning boosting algorithm that is not inherently interpretable. So, we will consider this as our black-box text classification model. We are not focused on improving the model's accuracy with necessary hyperparameter tuning, as LIME is completely model-agnostic. For this tutorial, we have created a scikit-learn pipeline to apply feature extraction first using <strong class="bold">TFIDF Vectorizer</strong> (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), and then the trained model:</p>
			<pre class="source-code">model_pipeline = make_pipeline(tfidf, model)</pre>
			<p>In the next subsection, we will see how the LIME framework can be easily applied with text data as well.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor103"/>Applying LIME Text Explainers</h2>
			<p>Like the previous tutorials <a id="_idIndexMarker363"/>with image and tabular data, applying LIME is simple with text data in a few lines of code. We will now define the LIME <strong class="source-inline">explainer</strong> object:</p>
			<pre class="source-code">from lime.lime_text import LimeTextExplainer</pre>
			<pre class="source-code">explainer = LimeTextExplainer(class_names=['negative', 'positive'])</pre>
			<p>Then, we will use the inference data instance to provide local explainability for that particular data instance:</p>
			<pre class="source-code">exp = explainer.explain_instance(x_test_df[idx], model_pipeline.predict_proba, num_features=5)</pre>
			<pre class="source-code">exp.show_in_notebook()</pre>
			<p>And that's it! In just a few lines of code, we can explain the text classifier that actually relies on TFIDF numerical features, but the explainability is provided with a human interpretable perspective as words that can positively or negatively influence the model outcome are highlighted. It is<a id="_idIndexMarker364"/> easier for any non-technical user to understand the working of the text model in this way, rather than providing explanations using numerically encoded features. </p>
			<p>Now, let's take a look at the output visualization provided by LIME when applied to text data.</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B18216_05_004.jpg" alt="Figure 5.4 – Output visualization when LIME is applied to a text classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Output visualization when LIME is applied to a text classifier</p>
			<p>In <em class="italic">Figure 5.4</em>, we can see the output visualization of the LIME framework when applied to text data.</p>
			<p>The output visualization is very similar to what we have observed with tabular data. It shows us the <em class="italic">prediction probability</em>, which can be used as a <em class="italic">model confidence</em> score. The algorithm highlights the most influential words that determine the model outcome, with a feature importance score. For example, from <em class="italic">Figure 5.4</em>, we can see that the inference data instance is predicted as negative by the model (which is predicted correctly as demonstrated in the notebook). The presence of words such as <em class="italic">waste</em>, <em class="italic">bad</em>, and <em class="italic">ridiculous</em> does indicate a <em class="italic">negative review</em>. This is human-interpretable as well, since if you ask a non-technical user to justify why the review is classified as negative, the user might refer to the usage of frequently used words in negative reviews or words used in sentences with a negative tone. </p>
			<p>Thus, we can see that LIME can be easily applied with text classifiers as well. Even with text data, the algorithm is<a id="_idIndexMarker365"/> simple, yet effective in providing a human-interpretable explanation. I would definitely recommend using LIME to explain black-box text classifiers as an additional model evaluation or quality inspection step.</p>
			<p>But so far, we have seen the application of LIME Python frameworks in the Jupyter notebook environment. The immediate question that you might have is – <em class="italic">Can we scale LIME for use in production-level systems?</em> Let's find out in the next section.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor104"/>LIME for production-level systems</h1>
			<p>The short answer to the<a id="_idIndexMarker366"/> question posted toward the end of the last section is <em class="italic">Yes</em>. LIME can definitely be scaled for use in production-level systems due to the following main reasons:</p>
			<ul>
				<li><strong class="bold">Minimal implementation complexity</strong>: The API structure of the LIME Python framework is concise and well structured. This allows us to add model explainability in just a few lines of code. For providing local explainability to inference data instances, the runtime complexity of the LIME algorithm is very low and, hence, this approach can also work for real-time applications.</li>
				<li><strong class="bold">Easy integration with other software applications</strong>: The API structure of the framework is modular. For consuming the explainability results, we do not need to solely depend on the in-built visualizations provided by the framework. We can utilize the raw explainability results and create our own custom visualization dashboards or reports. Also, we can create custom web API methods and host the web APIs on remote cloud servers, creating our own model explainability cloud-based service that can be integrated easily with other software applications. We will cover this in more detail in <a href="B18216_10_ePub.xhtml#_idTextAnchor209"><em class="italic">Chapter 10</em></a>, <em class="italic">XAI Industry Best Practices</em>.</li>
				<li><strong class="bold">Does not require heavy computational resources</strong>: The LIME framework works well with low computational resources. For real-time applications, the algorithms used need to be very fast and should have the ability to run on low computational resources, as otherwise, the user experience is affected.</li>
				<li><strong class="bold">Easy to set up and package</strong>: As we have already seen before running the tutorial notebooks, LIME is very easy to set up and does not have a dependency on packages that are difficult to install. Similarly, any Python program using LIME is easy to package or <strong class="bold">containerize</strong>. Most production-level systems have automated CI/CD pipelines to create <strong class="bold">Docker containers</strong> (https://www.docker.com/resources/what-container), which are deployed on production-level systems. The engineering effort needed to containerize a Python program using a LIME framework is low and hence it is easy to productionalize such software applications.</li>
			</ul>
			<p>These are the key<a id="_idIndexMarker367"/> reasons why LIME is the preferred model explainability method used in industrial applications, despite some of its well-known pitfalls. </p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor105"/>Summary</h1>
			<p>In this chapter, we have discussed the practical applications of the LIME Python framework on different types of datasets. The tutorials covered in the chapter are just the starting point and I strongly recommend you try out LIME explainability on other datasets. We have also discussed why LIME is a good fit for production-level ML systems. </p>
			<p>In the next chapter, we will discuss another very popular explainable AI Python framework called <strong class="bold">SHAP</strong>, which even considers the collective contribution of multiple features in influencing the model's outcome. </p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor106"/>References</h1>
			<p>Please refer to the following resources to gain additional information:</p>
			<ul>
				<li><em class="italic">"Why Should I Trust You?" Explaining the Predictions of Any Classifier</em>, by <em class="italic">Ribeiro et al.</em>: <a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a></li>
				<li><em class="italic">Local Interpretable Model-Agnostic Explanations (LIME): An Introduction</em>: <a href="https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/">https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/</a></li>
				<li><em class="italic">LIME GitHub Project</em>: <a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a></li>
				<li><em class="italic">Docker Blog</em>: <a href="https://www.docker.com/blog/">https://www.docker.com/blog/</a></li>
			</ul>
		</div>
	</body></html>