<html><head></head><body>
		<div id="_idContainer088">
			<h1 class="chapter-number"><a id="_idTextAnchor098"/>8</h1>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor099"/>Feature Engineering for  Natural Language Data</h1>
			<p>In the previous chapter, we explored how to extract features from numerical data and images. We explored a few algorithms that are used for that purpose. In this chapter, we’ll continue with the algorithms that extract features from natural <span class="No-Break">language data.</span></p>
			<p>Natural language is <a id="_idIndexMarker300"/>a special kind of data source in software engineering. With the introduction of GitHub Copilot and ChatGPT, it became evident that machine learning and artificial intelligence tools for software engineering tasks are no longer science fiction. Therefore, in this chapter, we’ll explore the first steps that made these technologies so powerful – feature extraction from natural <span class="No-Break">language data.</span></p>
			<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Tokenizers and their role in <span class="No-Break">feature extraction</span></li>
				<li>Bag-of-words as a simple technique for processing natural <span class="No-Break">language data</span></li>
				<li>Word embeddings as more advanced methods that can <span class="No-Break">capture contexts</span></li>
			</ul>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor100"/>Natural language data in software engineering and the rise of GitHub Copilot</h1>
			<p>Programming has always been a mixture of science, engineering, and creativity. Creating new programs and being able to instruct computers to do something has always been something that was considered worth paying for – that’s how all programmers make their living. There have been attempts to automate programming and to support smaller tasks – for example, provide programmers with suggestions on how to use a specific function or <span class="No-Break">library method.</span></p>
			<p>Good programmers, however, can make programs that last and that are readable for others. They can also make reliable programs that work without maintenance for a long period. The best programmers are the ones who can solve very difficult tasks and follow the principles and best practices of <span class="No-Break">software engineering.</span></p>
			<p>In 2020, something happened – GitHub Copilot entered the stage and showed that automated tools, based <a id="_idIndexMarker301"/>on <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), can provide much more than just suggestions for simple function calls. It has demonstrated that these language models are capable of providing suggestions for entire solutions and algorithms and even solving programming competitions. This opened up completely new possibilities for programmers – the best ones became extremely productive, and have been provided with the tools that allow them to focus on the complex parts of their programming tasks. The simple ones are now solved by GitHub Copilot <span class="No-Break">and others.</span></p>
			<p>The reason why these tools are so good is because they are based on LLMs, which are capable of finding and quantifying contexts of programs. Just like a great chess player can foresee several moves in advance, these tools can foresee what the programmers may need in advance and provide <span class="No-Break">useful suggestions.</span></p>
			<p>There are a few simple tricks that make these tools so effective, and one of them is feature engineering. Feature engineering for natural language tasks, including programming, is a process where a piece of text is transformed into a vector (or matrix) of numbers. These vectors can be simple – for example, quantifying the tokens – and also very complex – for example, finding an atomic piece of text linked to other tasks. We’ll explore these techniques in this chapter. We’ll start with a bit of a repetition of the bag-of-words technique (seen in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> and <a href="B19548_05.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>). We do not need to repeat the entire code, but we do need to provide a small re-cap to understand the limitations of these approaches. However, here is my best practice for choosing whether I need a tokenizer <span class="No-Break">or embeddings.</span></p>
			<p class="callout-heading">Best practice #44</p>
			<p class="callout">Use tokenizers for LLMs such as BERT and word embeddings for <span class="No-Break">simple tasks.</span></p>
			<p>For simple tasks, such as basic tokenization of the text for sentiment analysis or quickly understanding the dependencies in the text, I often use word embeddings. However, I usually use different tokenizers for working with LLMs such as BERT, RoBERTa, or AlBERT since these models are very good at finding dependencies on their own. However, for designing classifiers, I use word embeddings since they provide a fast way of creating feature vectors that are compatible with the “classical” machine <span class="No-Break">learning algorithms.</span></p>
			<p>Choosing a tokenizer needs to be done based on the task. We’ll look at this closer in this chapter, but the topic itself could occupy an entire book. For example, for tasks that require information about the part of speech (or, in many cases, part of the abstract syntax tree of a program), we need to use a tokenizer that is designed to capture that information – for example, from a programming language parser. These tokenizers provide more information to the model, but they impose more requirements on the data – an abstract syntax tree-based tokenizer requires the program to be <span class="No-Break">well formed.</span></p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor101"/>What a tokenizer is and what it does</h1>
			<p>The first step in feature engineering text data is to decide on the tokenization of the text. The tokenization<a id="_idIndexMarker302"/> of text is a process of extracting parts of words that capture the meaning of the text without too many <span class="No-Break">extra details.</span></p>
			<p>There are different ways to extract tokens, which we’ll explore in this chapter, but to illustrate the problem of extracting tokens, let’s look at one word that can take different forms – <em class="italic">print</em>. The word by itself can be a token, but it can be in different forms, such as <em class="italic">printing</em>, <em class="italic">printed</em>, <em class="italic">printer</em>, <em class="italic">prints</em>, <em class="italic">imprinted</em>, and many others. If we use a simple tokenizer, each of these words will be one token – which means quite a few tokens. However, all these tokens capture some sort of meaning related to printing, so maybe we do not need so many <span class="No-Break">of them.</span></p>
			<p>This is where tokenizers<a id="_idIndexMarker303"/> come in. Here, we can decide how to treat these different forms of the word. We could take the main part only – <em class="italic">print</em> – and then all the other forms would be counted as that, so both <em class="italic">imprinted</em> and <em class="italic">printing</em> would be counted as <em class="italic">print</em>. It decreases the number of tokens, but we reduce the expressiveness of our feature vector – some information is lost as we do not have the same number of tokens to use. We could pre-design the set of tokens – that is, use both <em class="italic">print</em> and <em class="italic">imprint</em> to distinguish between different contexts. We could also use bigrams (two words together) as tokens (for example, <em class="italic">is_going</em> versus is, <em class="italic">going</em> – the first one requires both words to be in the specific sequence, where the other one allows them to be in two different sequences), or we could add the information about whether the word is an object of the subject in <span class="No-Break">a sentence.</span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor102"/>Bag-of-words and simple tokenizers</h1>
			<p>In <em class="italic">Chapters 3</em> and <em class="italic">5</em>, we saw the use of the bag-of-words feature extraction technique. This technique takes the text and counts the number of tokens, which were words in <em class="italic">Chapters 3</em> and <em class="italic">5</em>. It is simple and computationally efficient, but it has a <span class="No-Break">few problems.</span></p>
			<p>When instantiating the<a id="_idIndexMarker304"/> bag-of-words tokenizer, we can use several parameters that strongly impact the results, as we did in the following fragment of code in the <span class="No-Break">previous chapters:</span></p>
			<pre class="source-code">
# create the feature extractor, i.e., BOW vectorizer
# please note the argument - max_features
# this argument says that we only want three features
# this will illustrate that we can get problems - e.g. noise
# when using too few feat<a id="_idTextAnchor103"/>ures
vectorizer = CountVectorizer(max_features = 3)</pre>			<p>The <strong class="source-inline">max_features</strong> parameter is a cut-off value that reduces the number of features, but it also can introduce noise where two (or more) distinct sentences have the same feature vector (we saw an example of such a sentence in <a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>). Since we discussed noise and the problems related to it, we could be tempted to use other parameters – <strong class="source-inline">max_df</strong> and <strong class="source-inline">min_df</strong>. These two parameters determine how often a word should appear in the document to be considered a token. The tokens that are too rare can (<strong class="source-inline">min_df</strong>) result in a sparse matrix – a lot of 0s in the feature matrix – but they can be a very good discriminant between data points. Maybe these rare words are just what we are looking for. The other parameter (<strong class="source-inline">max_df</strong>) results in more dense feature matrices, but they may not discriminate the data points completely. This means that it is not so simple to select these parameters – we need experiments and we need to use machine learning model training (and validation) to find the <span class="No-Break">right vector.</span></p>
			<p>There is also another way – we can perform a recursive search for such a feature vector that would discriminate all data points without adding too much noise. My team has experimented with such algorithms, which yield very good performance for model training and validation but are computationally very expensive. Such an algorithm is presented in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer087">
					<img alt="Figure 8.1 – An algorithm for finding a set of features that discriminate all data points in a text file. The flow has been simplified to illustrate the main points" src="image/B19548_08_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – An algorithm for finding a set of features that discriminate all data points in a text file. The flow has been simplified to illustrate the main points</p>
			<p>The <a id="_idIndexMarker305"/>algorithm works by adding new tokens if a data point has the same feature vector as any of the previous ones. It starts by taking the first token from the first line, then the second line. If the token can discriminate between these two lines, then it proceeds to the third line. Once the algorithm discovers that two different lines have the same feature vector, it finds out whether there is a token that can discriminate between these lines and adds it to the set of features. It continues until there are no new tokens to add or all lines have <span class="No-Break">been analyzed.</span></p>
			<p>This algorithm guarantees that the set of tokens that best discriminates the analyzed dataset is found. However, it has one large disadvantage – it is slow (as it must start from the first line once a new token is found/needed). The resulting feature matrix is also not optimal – it contains a lot of 0s since most of the tokens can only be found in one line. The feature matrix, in turn, can be much larger than the actual <span class="No-Break">raw dataset.</span></p>
			<p>This is where my next best practice <span class="No-Break">comes in.</span></p>
			<p class="callout-heading">Best practice #45</p>
			<p class="callout">Use bag-of-words tokenizers together with dictionaries when your task requires a pre-defined set <span class="No-Break">of words.</span></p>
			<p>I use <a id="_idIndexMarker306"/>bag-of-words tokenizers quite often when analyzing programming language code. I use a pre-defined set of keywords from the programming language to boost the tokenizer and then the standard <strong class="source-inline">CountVectorizer</strong>. This allows me to control part of the vocabulary that I am interested in – keywords – and allows the tokenizer to adjust to <span class="No-Break">the text.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor104"/>WordPiece tokenizer</h1>
			<p>A better way to tokenize<a id="_idIndexMarker307"/> and extract features from text documents is to use a WordPiece tokenizer. This tokenizer works in such a way that it finds the most common pieces of text that it can discriminate, and also the ones that are the most common. This kind of tokenizer needs to be trained – that is, we need to provide a set of representative texts to get the right <span class="No-Break">vocabulary (tokens).</span></p>
			<p>Let’s look at an example where we use a simple program, a module from an open source project, to train such a tokenizer and then apply this tokenizer to the famous “Hello World” program in C. Let’s start by creating <span class="No-Break">the tokenizer:</span></p>
			<pre class="source-code">
from tokenizers import BertWordPieceTokenizer
# initialize the actual tokenizer
tokenizer = BertWordPieceTokenizer(
    clean_text=True,
    handle_chinese_chars=False,
    strip_accents=False,
    lowercase=True
)</pre>			<p>In this example, we’re using the <a id="_idIndexMarker308"/>WordPiece tokenizer from the Hugging Face library, specifically the one that is prepared to work with LLMs such as BERT. There are several parameters that we can use, but let’s settle with the parameters that show that we are only interested in lowercase characters; we do not want to handle Chinese characters and want to start <span class="No-Break">from scratch.</span></p>
			<p>Now, we need to find a piece of text that we can train the tokenizer on. In this example, I’ll use one of the files from an open source project – AzureOS NetX. It’s a component written in C that handles parts of the internet HTTP protocol. We create a new variable – <strong class="source-inline">path</strong> – and add the path to that file there. Once we’ve prepared the text, we can train <span class="No-Break">the tokenizer:</span></p>
			<pre class="source-code">
# and train the tokenizer based on the text
tokenizer.train(files=paths,
                vocab_size=30_000,
                min_frequency=1,
                limit_alphabet=1000,
                wordpieces_prefix='##',
                special_tokens=['[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])</pre>			<p>We’ve set the tokenizer to a similar set of parameters, similar to <strong class="source-inline">CountVectorizer</strong> in the previous examples. This preceding code fragment finds the set of the most common pieces of words and uses them <span class="No-Break">as tokens.</span></p>
			<p>We can get the list of tokens through the <strong class="source-inline">tokenizer.get_vocab()</strong> statement, which results in a long dictionary of tokens. Here are the first <span class="No-Break">few ones:</span></p>
			<pre class="source-code">
'##ll': 183,
'disable': 326,
'al': 263,
'##cket': 90,
'##s': 65,
'computed': 484</pre>			<p>The first token is a piece of a word, which is denoted by the fact that it has two hashtags at the beginning of it. This token is mapped to the number <strong class="source-inline">183</strong> in the vocabulary. This mapping is important as the numbers are used later on by the machine <span class="No-Break">learning models.</span></p>
			<p>Another interesting observation is that some of the tokens, such as <strong class="source-inline">'disable'</strong>, are not pieces of words but entire words. This means that this token does not appear as a piece of the word anywhere and it does not contain any other pieces of other words in <span class="No-Break">the vocabulary.</span></p>
			<p>Once we’ve trained the <a id="_idIndexMarker309"/>WordPiece tokenizer, we can check how the tokenizer extracts features from a simple <span class="No-Break">C program:</span></p>
			<pre class="source-code">
strCProgram = '''
int main(int argc, void **argc)
{
  printf("%s", "Hello World\n");
  return 0;
}
'''
# now, let's see how the tokenizer works
# we invoke it based on the program above
tokenizedText = tokenizer.encode(strCProgram)
tokenizedText.tokens</pre>			<p>The preceding piece of code tokenizes the program. The result is the following list of tokens (only the first 10 tokens of 50 <span class="No-Break">are shown):</span></p>
			<pre class="source-code">
'in', '##t', 'ma', '##in', '(', 'in', '##t', 'a', '##r', '##g'</pre>			<p>The first line, which starts with the <strong class="source-inline">int</strong> token, has been tokenized in the following way. The first word – <strong class="source-inline">int</strong> – is split into two tokens: <strong class="source-inline">"in"</strong> and <strong class="source-inline">"##t"</strong>. This is because these two parts were used in the training program. We can also see that the second token – <strong class="source-inline">main</strong> – is split into two tokens: <strong class="source-inline">"ma"</strong> and <strong class="source-inline">"##in"</strong>. The IDs for these tokens are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
110, 57, 272, 104, 10, 110, 57, 30, 61, 63</pre>			<p>This means that this list of numbers is the feature vector for our simple <span class="No-Break">C program.</span></p>
			<p>WordPiece tokenization <a id="_idIndexMarker310"/>is very effective, but it depends a lot on the training data. If we use training data that is very different from the tokenized text, the set of tokens will not be very helpful. Therefore, my next best practice is about training <span class="No-Break">this tokenizer.</span></p>
			<p class="callout-heading">Best practice #46</p>
			<p class="callout">Use the WordPiece tokenizer as your <span class="No-Break">first choice.</span></p>
			<p>I usually use this tokenizer as my first choice. It is relatively flexible but quite fast. It allows us to capture a vocabulary that does the job most of the time and does not require a lot of setup. For simple tasks with straightforward language and a well-defined vocabulary, traditional word-level tokenization or other subword tokenization methods such as <strong class="bold">byte-pair encoding</strong> (<strong class="bold">BPE</strong>) may suffice. WordPiece tokenization can increase the size of the<a id="_idIndexMarker311"/> input data due to the introduction of subword tokens. This can impact memory and <span class="No-Break">computational requirements.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor105"/>BPE</h1>
			<p>A more advanced method for tokenizing text is the BPE algorithm. This algorithm is based on the same premises as the compression algorithm that was created in the 1990s by Gage. The algorithm compresses a series of bytes by the bytes not used in the compressed data. The<a id="_idIndexMarker312"/> BPE tokenizer does a similar thing, except that it replaces a series of tokens with new bytes that are not used in the text. In this way, the algorithm can create a much larger vocabulary than <strong class="source-inline">CountVectorizer</strong> and the WordPiece tokenizer. BPE is very popular both for its ability to handle large vocabulary and for its efficient implementation through the <span class="No-Break">fastBPE library.</span></p>
			<p>Let’s explore how to apply this tokenizer to the same data and check the difference between the previous two. The following code fragment shows how to instantiate this tokenizer from the Hugging <span class="No-Break">Face library:</span></p>
			<pre class="source-code">
# in this example we use the tokenizers
# from the HuggingFace library
from tokenizers import Tokenizer
from tokenizers.models import BPE
# we instantiate the tokenizer
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))</pre>			<p>This tokenizer requires training as it needs to find the optimal set of pairs of tokens. Therefore, we need to<a id="_idIndexMarker313"/> instantiate a trainer class and train it. The following piece of code does <span class="No-Break">just that:</span></p>
			<pre class="source-code">
from tokenizers.trainers import BpeTrainer
# here we instantiate the trainer
# which is a specific class that will manage
# the training process of the tokenizer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]",
                     "[SEP]", "[PAD]", "[MASK]"])
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()
# now, we need to prepare a dataset
# in our case, let's just read a dataset that is a code of a program
# in this example, I use the file from an open-source component - Azure NetX
# the actual part is not that important, as long as we have a set of
# tokens that we want to analyze
paths = ['/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c']
# finally, we are ready to train the tokenizer
tokenizer.train(paths, trainer)</pre>			<p>The important<a id="_idIndexMarker314"/> part of this training process is the use of a special pre-tokenizer. The pre-tokenizer is how we initially split words into tokens. In our case, we use the standard whitespaces, but we could use something more advanced. For example, we could use semicolons and therefore use entire lines of code <span class="No-Break">as tokens.</span></p>
			<p>After executing the preceding fragment of code, our tokenizer is trained and ready to use. We can check the tokens by writing <strong class="source-inline">tokenizer.get_vocab()</strong>. The set of tokens is as follows (the first <span class="No-Break">10 tokens):</span></p>
			<pre class="source-code">
'only': 565, 'he': 87, 'RTOS': 416, 'DE': 266, 'CH': 154, 'a': 54, 'ps': 534, 'will': 372, 'NX_SHIFT_BY': 311, 'O': 42,</pre>			<p>This set of tokens is very different from the set of tokens in previous cases. It contains a mix of words such as “will” and subwords such as “ol.” This is because the BPE tokenizer found some replicated tokens and replaced them with <span class="No-Break">dedicated bytes.</span></p>
			<p class="callout-heading">Best practice #47</p>
			<p class="callout">Use BPE when working with LLMs and large corpora <span class="No-Break">of text.</span></p>
			<p>I use BPE as my go-to when I analyze large pieces of text, such as large code bases. It is blazingly fast for this task and can capture complex dependencies. It is also heavily used in models such as BERT <span class="No-Break">or GPT.</span></p>
			<p>Now, in our case, the<a id="_idIndexMarker315"/> program’s source code that we used to train the BPE tokenizer was small, so a lot of words did not repeat themselves and the optimization does not make much sense. Therefore, a WordPiece tokenizer would do an equally (if not better) job. However, for larger text corpora, this tokenizer is much more effective and efficient than WordPiece or bag-of-words. It is also the basis for the next tokenizer – <span class="No-Break">SentencePiece.</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor106"/>The SentencePiece tokenizer</h1>
			<p>SentencePiece <a id="_idIndexMarker316"/>is a more general option than BPE for one more reason: it allows us to treat whitespaces as regular tokens. This allows us to find more complex dependencies and therefore train models that understand more than just pieces of words. Hence the name – SentencePiece. This tokenizer was originally introduced to enable the tokenization of languages such as Japanese, which do not use whitespaces in the same way as, for example, English. The tokenizer can be installed by running the <strong class="source-inline">pip install -q </strong><span class="No-Break"><strong class="source-inline">sentencepiece</strong></span><span class="No-Break"> command.</span></p>
			<p>In the following code example, we’re instantiating and training the <span class="No-Break">SentencePiece tokenizer:</span></p>
			<pre class="source-code">
import sentencepiece as spm
# this statement trains the tokenizer
spm.SentencePieceTrainer.train('--input="/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c" --model_prefix=m --vocab_size=200')
# makes segmenter instance and
# loads the model file (m.model)
sp = spm.SentencePieceProcessor()
sp.load('m.model')</pre>			<p>We’ve trained it on the <a id="_idIndexMarker317"/>same file as the previous tokenizers. The text was a programming file, so we could expect the tokenizer to give us a better understanding of what’s in a programming language than what’s in a normal piece of text. Something worth noting is the size of the vocabulary, which is 200, unlike 30,000 in the previous examples. This is important because this tokenizer tries to find as many tokens as this parameter. Since our input program is very short – one file with a few functions in it – the tokenizer cannot create more than about <span class="No-Break">300 tokens.</span></p>
			<p>The following fragment encodes the “Hello World” program using this tokenizer and prints the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
strCProgram = '''
int main(int argc, void **argc)
{
  printf("%s", "Hello World\n");
  return 0;
}
'''
print(sp.encode_as_pieces(strCProgram))</pre>			<p>The first 10 tokens are represented in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
'▁in', 't', '▁', 'm', 'a', 'in', '(', 'in', 't', '▁a'</pre>			<p>The new element in this tokenizer is the underscore character (<strong class="source-inline">_</strong>). It denotes whitespace in the text. This is unique and it allows us to use this tokenizer more effectively in programming language comprehension because it allows us to capture such programming constructs as nesting – that is, using tabs instead of spaces or writing several statements in the same line. This is all because this tokenizer treats whitespaces as <span class="No-Break">something important.</span></p>
			<p class="callout-heading">Best practice #48</p>
			<p class="callout">Use the SentencePiece tokenizer when no clear word boundaries <span class="No-Break">are present.</span></p>
			<p>I use SentencePiece when analyzing programming language code with a focus on programming styles – for example, when we focus on things such as camel-case variable naming. For this task, it is important to understand how programmers use spaces, formatting, and other compiler-transparent elements. Therefore, this tokenizer is perfect for <span class="No-Break">such tasks.</span></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor107"/>Word embeddings</h1>
			<p>Tokenizers are one way of extracting features from text. They are powerful and can be trained to create complex tokens and capture statistical dependencies of words. However, they are limited by the fact that they are completely unsupervised and do not capture any meaning or relationship between words. This means that the tokenizers are great at providing input to neural network models, such as BERT, but sometimes, we would like to have features that are more aligned with a <span class="No-Break">certain task.</span></p>
			<p>This is where <a id="_idIndexMarker318"/>word embeddings come to the rescue. The following code shows how to instantiate the word embedding model, which is imported from the <strong class="source-inline">gensim</strong> library. First, we need to prepare <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
from gensim.models import word2vec
# now, we need to prepare a dataset
# in our case, let's just read a dataset that is a code of a program
# in this example, I use the file from an open source component - Azure NetX
# the actual part is not that important, as long as we have a set of
# tokens that we want to analyze
path = '/content/drive/MyDrive/ds/cs_dos/nx_icmp_checksum_compute.c'
# read all lines into an array
with open(path, 'r') as r:
  lines = r.readlines()
# and see how many lines we got
print(f'The file (and thus our corpus) contains {len(lines)} lines')</pre>			<p>The preceding code <a id="_idIndexMarker319"/>fragment prepares the file differently compared to the tokenizers. It creates a list of lines, and each line is a list of tokens, separated by whitespaces. Now, we are ready to create the <strong class="source-inline">word2vec</strong> model and train it on <span class="No-Break">this data:</span></p>
			<pre class="source-code">
# we need to pass splitted sentences to the model
tokenized_sentences = [sentence.split() for sentence in lines]
model = word2vec.Word2Vec(tokenized_sentences,
                          vector_size=10,
                          window=1,
                          min_count=0,
                          workers=4)</pre>			<p>The result is that the model is trained on the corpus that we provided – the C program implementing a part of the HTTP protocol. We can look at the first 10 tokens that have been extracted by <span class="No-Break">writing </span><span class="No-Break"><strong class="source-inline">model.wv.key_to_index</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
'*/': 0, '/*': 1, 'the': 2, '=': 3, 'checksum': 4, '-&gt;': 5, 'packet': 6, 'if': 7, 'of': 8, '/**********************************************************************/': 9,</pre>			<p>In total, <strong class="source-inline">word2vec</strong> extracted <span class="No-Break">259 tokens.</span></p>
			<p>This word embedding model is different from the tokenizers that we used before. It embeds the values of the words (tokens) into a latent space, which allows us to utilize the lexical properties of these words more smartly. For example, we can check the similarity of words <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">model.wv.most_similar(positive=['add'])</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
('NX_LOWER_16_MASK;', 0.8372778296470642),
('Mask', 0.8019374012947083),
('DESCRIPTION', 0.7171915173530579),</pre>			<p>We can also pretend that these words are vectors and their similarity is captured in this vector. Therefore, we can write something like <strong class="source-inline">model.wv.most_similar(positive= ['file', 'function'], negative=['found'])</strong> and obtain a result <span class="No-Break">like this:</span></p>
			<pre class="source-code">
('again', 0.24998697638511658),
('word', 0.21356187760829926),
('05-19-2020', 0.21174617111682892),
('*current_packet;', 0.2079058289527893),</pre>			<p>The expression will be similar if we use mathematics to express it: <em class="italic">result = file + function – found</em>. The resulting list of similar words is the list of words that are the closest to the vector that was captured as the result of <span class="No-Break">this calculation.</span></p>
			<p>Word embeddings<a id="_idIndexMarker320"/> are very powerful when we want to capture the similarity of the words and expressions. However, the original implementation of this model has certain limitations – for example, it does not allow us to use words that are not part of the original vocabulary. Asking for a word that is similar to an unknown token (for example, <strong class="source-inline">model.wv.most_similar(positive=['return'])</strong>) results in <span class="No-Break">an error.</span></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor108"/>FastText</h1>
			<p>Luckily for us, there is<a id="_idIndexMarker321"/> an extension of the <strong class="source-inline">word2vec</strong> model that can approximate the unknown tokens – FastText. We can use it in a very similar way as we <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">word2vec</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from gensim.models import FastText
# create the instance of the model
model = FastText(vector_size=4,
                 window=3,
                 min_count=1)
# build a vocabulary
model.build_vocab(corpus_iterable=tokenized_sentences)
# and train the model
model.train(corpus_iterable=tokenized_sentences,
            total_examples=len(tokenized_sentences),
            epochs=10)</pre>			<p>In the preceding code fragment, the model is trained on the same set of data as <strong class="source-inline">word2vec</strong>. <strong class="source-inline">model = FastText(vector_size=4, window=3, min_count=1)</strong> creates an instance of the FastText model with <span class="No-Break">three hyperparameters:</span></p>
			<ul>
				<li><strong class="source-inline">vector_size</strong>: The number of elements in the resulting <span class="No-Break">feature vector</span></li>
				<li><strong class="source-inline">window</strong>: The size of the window used to capture <span class="No-Break">context words</span></li>
				<li><strong class="source-inline">min_count</strong>: The minimum frequency of a word to be included in <span class="No-Break">the vocabulary</span></li>
			</ul>
			<p><strong class="source-inline">model.build_vocab(corpus_iterable=tokenized_sentences)</strong> builds the vocabulary of the model by iterating through the <strong class="source-inline">tokenized_sentences</strong> iterable (which should contain a list of lists, with each inner list representing a sentence tokenized into individual words) and adding each word to the vocabulary if it meets the <strong class="source-inline">min_count</strong> threshold. <strong class="source-inline">model.train(corpus_iterable=tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)</strong> trains the FastText <a id="_idIndexMarker322"/>model using the <strong class="source-inline">tokenized_sentences</strong> iterable for a total of 10 epochs. During each epoch, the model iterates through the corpus again and updates its internal weights based on the context words surrounding each target word. The <strong class="source-inline">total_examples</strong> parameter tells the model how many total examples (that is, sentences) are in the corpus, which is used to calculate the <span class="No-Break">learning rate.</span></p>
			<p>The input is the same. However, if we invoke the similarity for the unknown token, such as <strong class="source-inline">model.wv.most_similar(positive=['return'])</strong>, we get the <span class="No-Break">following result:</span></p>
			<pre class="source-code">
('void', 0.5913326740264893),
('int', 0.43626993894577026),
('{', 0.2602742612361908),</pre>			<p>The set of three similar words indicates that the model can approximate an <span class="No-Break">unknown token.</span></p>
			<p>My next best practice is about the use <span class="No-Break">of FastText.</span></p>
			<p class="callout-heading">Best practice #49</p>
			<p class="callout">Use word embeddings, such as FastText, as a valuable feature representation for text classification tasks, but consider incorporating them into more comprehensive models for <span class="No-Break">optimal performance.</span></p>
			<p>Unless we need to use an LLM, this kind of feature extraction is a great alternative to the simple bag-of-words technique and powerful LLMs. It captures some parts of the meaning and allows us to design classifiers based on text data. It can also handle unknown tokens, which makes it <span class="No-Break">very flexible.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor109"/>From feature extraction to models</h1>
			<p>The feature extraction methods presented in this chapter are not the only ones we can use. Quite a few more exist (to say the least). However, they all work similarly. Unfortunately, no silver bullet exists, and all models have advantages and disadvantages. For the same task, but a different dataset, simpler models may be better than <span class="No-Break">complex ones.</span></p>
			<p>Now that we have seen how to extract features from text, images, and numerical data, it’s time we start training the models. This is what we’ll do in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor110"/>References</h1>
			<ul>
				<li><em class="italic">Al-Sabbagh, K.W., et al. Selective regression testing based on big data: comparing feature extraction techniques. in 2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW). </em><span class="No-Break"><em class="italic">2020. IEEE.</em></span></li>
				<li><em class="italic">Staron, M., et al. Improving Quality of Code Review Datasets–Token-Based Feature Extraction Method. in Software Quality: Future Perspectives on Software Engineering Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21, 2021, Proceedings 13. </em><span class="No-Break"><em class="italic">2021. Springer.</em></span></li>
				<li><em class="italic">Sennrich, R., B. Haddow, and A. Birch, Neural machine translation of rare words with subword units. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1508.07909, 2015.</em></span></li>
				<li><em class="italic">Gage, P., A new algorithm for data compression. C Users Journal, 1994. 12(2): </em><span class="No-Break"><em class="italic">p. 23-38.</em></span></li>
				<li><em class="italic">Kudo, T. and J. Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1808.06226, 2018.</em></span></li>
			</ul>
		</div>
	</body></html>