- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Privacy Issues in Real Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际数据中的隐私问题
- en: ML is becoming an essential part of our daily lives due to its varied applications.
    Thus, there is a growing concern about privacy issues in ML. Datasets and trained
    ML models may disclose personal and sensitive information, such as political views,
    biometric data, mental health, sexual orientation, and other private information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其多样化的应用，机器学习正成为我们日常生活的重要组成部分。因此，人们对机器学习中的隐私问题越来越关注。数据集和训练好的机器学习模型可能会泄露个人和敏感信息，如政治观点、生物识别数据、心理健康、性取向和其他私人信息。
- en: In this chapter, we will learn about privacy issues and why this is a concern
    in ML. Furthermore, we will provide a brief introduction to privacy-preserving
    ML.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习关于隐私问题以及为什么这是机器学习（ML）中的一个关注点。此外，我们还将简要介绍隐私保护机器学习。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Why is privacy an issue in ML?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么隐私在机器学习中成为一个问题？
- en: What exactly is the privacy problem in ML?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的隐私问题究竟是什么？
- en: Privacy-preserving ML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护机器学习
- en: Real data challenges and issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际数据挑战和问题
- en: Why is privacy an issue in ML?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么隐私在机器学习中成为一个问题？
- en: As we discussed in the previous chapters, ML models need large-scale training
    data to converge and train well. The data can be collected from social media,
    online transactions, surveys, questionaries, or other sources. Thus, the collected
    data may contain sensitive information that individuals may not want to share
    with some organizations or individuals. If the data was shared or accessed by
    others, and thus the identities of the individuals were identified, that may cause
    them personal abuse, financial issues, or identity theft.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中讨论的，机器学习模型需要大规模的训练数据才能收敛和良好训练。数据可以从社交媒体、在线交易、调查、问卷或其他来源收集。因此，收集到的数据可能包含个人可能不希望与某些组织或个人分享的敏感信息。如果数据被他人共享或访问，并且因此识别出个人的身份，这可能会给他们造成个人滥用、财务问题或身份盗窃。
- en: 'The complexity of a privacy breach in ML is closely related to the following
    main three factors:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中隐私泄露的复杂性紧密相关于以下三个主要因素：
- en: ML task
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习任务
- en: Dataset size
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集大小
- en: Regulations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规章制度
- en: Let’s take a closer look.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解一下。
- en: ML task
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习任务
- en: The *task* mainly defines the type of training data that we need to collect
    and annotate. Thus, some ML tasks, such as weather prediction and music generation,
    may have fewer privacy issues compared to other ML tasks, such as biometric authentication
    and medical image processing and analysis.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*主要定义了我们需要收集和标注的训练数据的类型。因此，一些机器学习任务，如天气预报和音乐生成，与其他机器学习任务（如生物识别认证、医学图像处理和分析）相比，可能具有较少的隐私问题。'
- en: Dataset size
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集大小
- en: The larger the dataset, the more issues you will have with privacy. If the dataset
    is a large-scale one, then you cannot store it on one device. Thus, you may need
    some cloud services and tools to manage your datasets, such as MongoDB, Hadoop,
    and Cassandra. Consequently, you may have less control over your data. Thus, you
    need to give more attention to the tools and services that process or manage your
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集越大，你将面临的隐私问题就越多。如果数据集是大规模的，那么你无法将其存储在一个设备上。因此，你可能需要一些云服务和技术来管理你的数据集，例如MongoDB、Hadoop和Cassandra。因此，你可能对你的数据控制较少。因此，你需要更加关注处理或管理你的数据的技术和服务。
- en: Regulations
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规章制度
- en: Many countries have clear and restrictive data protection regulations. For example,
    the UK has *The Data Protection Act 2018* ([https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted](https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted)).
    At the same time, similar regulations can be found in the EU, such as the *General
    Data Protection Regulation* ([https://gdpr-info.eu](https://gdpr-info.eu)). Thus,
    if you are using personal data, you have to consider data protection principles,
    such as transparency, being used for their specified purposes, and being kept
    for a specific period.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多国家都有明确和限制性的数据保护法规。例如，英国有《2018年数据保护法》([https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted](https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted))。同时，在欧盟也可以找到类似的规定，如《通用数据保护条例》([https://gdpr-info.eu](https://gdpr-info.eu))。因此，如果你正在使用个人数据，你必须考虑数据保护原则，如透明度、用于指定的目的，以及保留特定期限。
- en: If your dataset includes sensitive and confidential information, then you are
    subject to more restrictions. Your dataset may contain information about biometrics,
    gender, health, and religious and political opinions. Thus, if you repurpose a
    dataset that you, your organization, or someone else collected, this action may
    be illegal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集包含敏感和机密信息，那么你将受到更多的限制。你的数据集可能包含有关生物识别、性别、健康、宗教和政治观点的信息。因此，如果你重新使用你、你的组织或其他人收集的数据集，这种行为可能是非法的。
- en: One of the main limitations of utilizing real datasets is privacy issues. Due
    to this, we can see why synthetic data is a promising solution for these issues.
    In the next section, we will take a closer look at the privacy problem and its
    related topics, such as copyright and intellectual property infringement, which
    are major issues when collecting a new dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 利用真实数据集的一个主要限制是隐私问题。正因为如此，我们可以看到为什么合成数据是解决这些问题的有希望解决方案。在下一节中，我们将更深入地探讨隐私问题及其相关主题，如版权和知识产权侵权，这些是在收集新数据集时的主要问题。
- en: What exactly is the privacy problem in ML?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中隐私问题究竟是什么？
- en: Within the scope of privacy in ML, there are two main concerns. The first is
    regarding the dataset itself – that is, how to collect it, how to keep it private,
    and how to prevent unauthorized access to sensitive information. The second is
    associated with the vulnerability of ML models to reveal the training data, which
    we will discuss in the next section. For now, let’s examine the issues related
    to dataset privacy in ML.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习隐私的范围内，有两个主要关注点。第一个是关于数据集本身——也就是说，如何收集它，如何保持其隐私，以及如何防止未经授权访问敏感信息。第二个与机器学习模型容易泄露训练数据有关，我们将在下一节中讨论。现在，让我们检查与机器学习中数据集隐私相关的问题。
- en: Copyright and intellectual property infringement
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版权和知识产权侵权
- en: 'Copyright is a legal term that’s used to protect the ownership of intellectual
    property. It prevents or limits others from using your work without your permission.
    For example, if you take a photograph, record a video, or write a blog, your work
    is protected by copyright. Thus, others may not share, reproduce, or distribute
    your work without permission. Consequently, images, videos, text, or other information
    we see on the internet may have restrictive copyrights. Therefore, if you want
    to collect a dataset, you must consider the copyright problem carefully. As we
    know, there are different approaches to curating a real dataset. As an ML practitioner,
    you can do the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 版权是一个法律术语，用于保护知识产权的所有权。它阻止或限制他人未经你许可使用你的作品。例如，如果你拍照、录像或写博客，你的作品受版权保护。因此，其他人未经许可不得分享、复制或分发你的作品。因此，我们在互联网上看到的一些图像、视频、文本或其他信息可能受到版权限制。因此，如果你想收集一个数据集，你必须仔细考虑版权问题。众所周知，有不同方法来整理真实数据集。作为一名机器学习从业者，你可以做以下事情：
- en: '**Collect data yourself**: You can use a camera, microphone, sensors, questionaries,
    and other methods to collect a tremendous amount of data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自己收集数据**：你可以使用相机、麦克风、传感器、问卷和其他方法收集大量数据。'
- en: '**Collect data from the internet**: You can use search engines such as Google,
    Yahoo, Baidu, and DuckDuckGo to collect your dataset, similar to how the *ImageNet*
    ([https://www.image-net.org/](https://www.image-net.org/)) dataset was collected.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从互联网收集数据**：你可以使用谷歌、雅虎、百度和DuckDuckGo等搜索引擎来收集你的数据集，类似于*ImageNet* ([https://www.image-net.org/](https://www.image-net.org/))数据集的收集方式。'
- en: '**Collect data from other datasets**: You can combine different datasets to
    create a new dataset for a specific application – for example, if you are interested
    in visual object tracking for only one class – say, humans – in a specific scenario
    – say, adverse conditions. In this case, you can combine different datasets by
    excluding irrelevant classes and weather conditions. Thus, you create a new dataset
    for your particular problem.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从其他数据集中收集数据**：你可以结合不同的数据集，为特定应用创建一个新的数据集——例如，如果你只对特定场景（如恶劣条件）中的人类视觉对象跟踪感兴趣——在这种情况下，你可以通过排除无关类别和天气条件来结合不同的数据集。因此，你为你的特定问题创建了一个新的数据集。'
- en: Taking a photo of someone with our camera or recording their voice using our
    hardware does not permit us to use their information. At the same time, asking
    for permission is not a practical solution for large-scale datasets. To avoid
    similar issues some, datasets, such as **ImageNet**, were initially collected
    from the internet. Then, it was annotated by human annotators. It should be noted
    that not every image we see on the internet can be used to build our dataset;
    some images have copyright licenses that restrict how they can be used. Thus,
    the availability of data that you need on the web does not necessarily mean that
    you can leverage that data for your problem. The same copyright issues can be
    seen for combining different datasets as well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的相机拍摄某人的照片或使用我们的硬件录制他们的声音并不允许我们使用他们的信息。同时，对于大规模数据集来说，请求许可并不是一个实际的解决方案。为了避免类似的问题，一些数据集，如**ImageNet**，最初是从互联网上收集的。然后，由人工标注员进行标注。需要注意的是，我们看到的互联网上的每一张图片都不一定能用来构建我们的数据集；有些图片有版权许可证，限制了它们的使用方式。因此，网络上可用的数据并不一定意味着你可以利用这些数据来解决你的问题。将不同数据集合并时也会出现相同的版权问题。
- en: 'If your ML model was trained on, say, Van Gogh paintings and learned how to
    generate artwork, a question arises regarding who owns the rights to the generated
    images: Van Gogh, ML engineers, or both? In ML, this is a controversial and complex
    issue. **ChatGPT** ([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt))
    and **Imagen** (*Photorealistic Text-to-Image Diffusion Models with Deep Language
    Understanding*) are examples of such models. Thus, training your model on a real
    dataset may not give you the right to fully leverage the potential of your ML
    model. You may still be subject to copyright and the intellectual properties of
    the samples used in the training process. As you may expect, leveraging synthetic
    data seems a promising solution to these problems.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器学习模型是在梵高的画作上训练的，学会了如何生成艺术品，那么就会产生一个问题：生成的图像的版权归谁所有：梵高、机器学习工程师，还是两者都有？在机器学习中，这是一个有争议且复杂的问题。**ChatGPT**
    ([https://openai.com/blog/ChatGPT](https://openai.com/blog/ChatGPT)) 和 **Imagen**
    (*具有深度语言理解的逼真文本到图像扩散模型*) 就是这样的模型示例。因此，在真实数据集上训练你的模型可能并不能让你充分利用机器学习模型的潜力。你仍然可能受到训练过程中使用的样本的版权和知识产权的约束。正如你所预期的，利用合成数据似乎是解决这些问题的有希望的解决方案。
- en: Privacy and reproducibility of experiments
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验的隐私和可重复性
- en: One of the main issues in ML-based research is the difficulty in reproducing
    the experiments, and thus validating the results claimed in research papers. Data
    privacy is one of the key factors behind this issue. Many companies develop ML
    solutions using their own datasets. They may share trained models, but not the
    dataset itself because of many reasons related to privacy issues and regulations.
    Consequently, other researchers cannot reproduce the experiments and results.
    This creates a good chance for errors, bias, misinterpretation, falsification,
    and manipulation of research results. This is also another reason why using synthetic
    data in certain fields, such as healthcare and finance, can make the research
    more transparent and the results more trustworthy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习的研究中，主要问题之一是实验的可重复性困难，因此无法验证研究论文中声明的结果。数据隐私是导致这一问题的关键因素之一。许多公司使用自己的数据集开发机器学习解决方案。由于与隐私问题和法规相关的许多原因，他们可能分享训练好的模型，但不会分享数据集本身。因此，其他研究人员无法重复实验和结果。这为错误、偏见、误解、伪造和操纵研究结果创造了良好的机会。这也是在某些领域，如医疗保健和金融，使用合成数据可以使研究更加透明、结果更加可信的另一个原因。
- en: Privacy issues and bias
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私问题和偏见
- en: Bias is another issue closely related to privacy problems in ML models and real
    data. ML models can be trained on biased data, which can result in biased outcomes.
    For example, some face recognition commercial applications were found to be less
    accurate in recognizing people with darker skin, females, or people aged between
    18 and 30\. The issue is due to the bias in the training data of the ML models.
    Thus, when the training data is not available because of privacy concerns, certain
    bias issues may arise. Consequently, this can lead to unequal treatment and discrimination
    based on race, gender, and other factors. Next, we will delve into traditional
    solutions to privacy issues.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见是另一个与机器学习模型和真实数据中的隐私问题密切相关的问题。机器学习模型可以在有偏见的数据上训练，这可能导致有偏见的结果。例如，一些面部识别商业应用被发现对肤色较深的人、女性或18至30岁之间的人的识别准确性较低。这个问题是由于机器学习模型的训练数据中的偏见造成的。因此，当由于隐私问题而无法获得训练数据时，可能会出现某些偏见问题。因此，这可能导致基于种族、性别和其他因素的待遇不公和歧视。接下来，我们将深入探讨隐私问题的传统解决方案。
- en: Privacy-preserving ML
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私保护机器学习
- en: '**Privacy-preserving ML** is a hot topic since it proposes solutions for privacy
    issues in the ML field. Privacy-preserving ML proposes methods that allow researchers
    to use sensitive data to train ML models while withholding sensitive information
    from being shared or accessed by a third party or revealed by the ML model. In
    the first subsection, we will examine common methods for mitigating privacy issues
    in datasets.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私保护机器学习**是一个热门话题，因为它提出了解决机器学习领域隐私问题的解决方案。隐私保护机器学习提出的方法允许研究人员使用敏感数据来训练机器学习模型，同时防止敏感信息被第三方共享或访问，或被机器学习模型揭示。在下一小节中，我们将检查减轻数据集中隐私问题的常见方法。'
- en: Approaches for privacy-preserving datasets
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私保护数据集的方法
- en: In this section, we’ll delve into standard approaches for handling and protecting
    sensitive information in datasets. We will look at anonymization, centralized
    data, and differential privacy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨处理和保护数据集中敏感信息的标准方法。我们将探讨匿名化、集中式数据和差分隐私。
- en: Anonymization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 匿名化
- en: '**Anonymization** can be considered one of the earliest approaches for privacy
    issues in datasets. Let’s assume you are given a dataset of patients’ medical
    records that contains their addresses, phone numbers, and postal codes. To anonymize
    this dataset, you can simply remove this sensitive data while keeping other medical
    records. Unfortunately, this approach does not work very well for two reasons.
    Sometimes, you cannot remove this sensitive information because they are part
    of the task. At the same time, anonymization may not be sufficient, and thus individuals
    may be identified by linking and combining other information in the dataset or
    other datasets. As an example, check the paper titled *The ’Re-Identification’
    of Governor William Weld’s Medical Information: A Critical Re-Examination of Health
    Data Identification Risks and Privacy Protections, Then and* *Now* ([https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397)):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**匿名化**可以被认为是数据集中隐私问题的最早方法之一。假设你被提供了一个包含患者地址、电话号码和邮政编码的医疗记录数据集。为了匿名化这个数据集，你可以简单地移除这些敏感数据，同时保留其他医疗记录。不幸的是，这种方法并不很有效，有两个原因。有时，你无法移除这些敏感信息，因为它们是任务的一部分。同时，匿名化可能不足以防止个人被识别，因此个人可能通过将数据集中或其他数据集中的其他信息链接和组合来被识别。例如，查看题为《威廉·韦尔德州长医疗信息的“再识别”：对健康数据识别风险和隐私保护现在与当时的批判性再审视》的论文（[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397)）：'
- en: '| **Name** | **Surname** | **Has diabetes** | **Smoking** | **Gender** | **Age**
    | **Occupation** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **姓名** | **姓氏** | **有糖尿病** | **吸烟** | **性别** | **年龄** | **职业** |'
- en: '| Mike | Chris | Yes | No | Male | 60 | Programmer |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 迈克 | 克里斯 | 是 | 否 | 男 | 60 | 程序员 |'
- en: '| Emma | Cunningham | Yes | Yes | Female | 40 | Lawyer |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 艾玛 | 昆宁汉姆 | 是 | 是 | 女 | 40 | 律师 |'
- en: '| Jan | Wan | No | No | Male | 32 | PhD Student |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 一月 | 万 | 否 | 否 | 男 | 32 | 博士研究生 |'
- en: '| Olivia | Cunningham | No | No | Female | 23 | Student |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 奥利维亚 | 昆宁汉姆 | 否 | 否 | 女 | 23 | 学生 |'
- en: Figure 3.1 – A sample dataset that includes sensitive information
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 包含敏感信息的样本数据集
- en: Centralized data
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集中式数据
- en: 'Another approach is based on the idea of keeping the data in a private server
    and not sharing the data itself but instead answering queries about the data.
    For example, here are some examples of queries regarding the dataset shown in
    *Figure 3**.1*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Return the minimum age of patients with diabetes
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the number of female patients with diabetes
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the average age of patients with diabetes
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, organizations can keep sensitive data while still allowing other
    researchers or organizations to leverage this data. However, this approach is
    not resilient against cyberattacks. These queries can be used by attackers to
    identify individuals and disclose sensitive information by combining complex queries
    and linking information from other datasets. For example, if you remove the **Name**
    and **Surname** columns but still use information regarding **Gender**, **Age**,
    and **Occupation**, it may still be possible to use these details to identify
    some patients.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a complementary solution to the previous approaches, differential privacy
    seems a promising solution. However, there are some limitations, as we will see
    later.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea of this approach is keeping individuals’ information secured while
    still learning about the phenomena under consideration. This approach is based
    on queries to a server that contains all the sensitive data. Many companies utilize
    differential privacy, such as Uber, Google, and Apple. The algorithm may add random
    noise to the information in the dataset or the queries, as shown in *Figure 3**.2*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Local and global differential privacy](img/B18494_03_002.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Local and global differential privacy
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy has two main approaches:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**Local differential privacy** (**LDP**): The data taken from the users are
    processed with noise, such as Laplacian noise. This process makes the data more
    secure against attacks. Even if attackers can access the data, they will have
    a noisy version and they will not be able to identify individuals.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global differential privacy** (**GDP**): The raw data taken from clients
    is not altered but directly stored in the server without any noise being added.
    When a query is received, the raw answer (accurate) is returned after noise has
    been added to it. This process generates a private answer to protect data privacy.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, with these data protection mechanisms, it’s supposed that ML models can
    now be trained on sensitive data without disclosing individual sensitive information.
    Unfortunately, this assumption is not always valid, as we will see in the next
    few sections.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for privacy-preserving ML
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Privacy-preserving machine learning** (**PPML**) aims to prevent training
    data leakage. ML models may memorize sensitive information in the training process,
    and thus confidential information may be revealed by the ML model. Standard PPML
    methods rely on differential privacy, which we discussed earlier. Next, you will
    be introduced to federated learning.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私保护机器学习**（**PPML**）旨在防止训练数据泄露。机器学习模型可能在训练过程中记住敏感信息，因此机密信息可能会被机器学习模型泄露。标准的PPML方法依赖于我们之前讨论过的差分隐私。接下来，你将了解到联邦学习。'
- en: Federated learning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联邦学习
- en: '**Federated learning** is a novel strategy that allows ML to be trained on
    sensitive data while not transferring the data outside of the local server or
    node. Thus, organizations can share their sensitive data to train ML models while
    keeping the data in the organization’s local servers. At the same time, this is
    a solution for regulation that prevents sharing data with external parties. It
    is based on the paradigm of decentralized learning, which we will see next. Please
    refer to [https://github.com/topics/federated-learning](https://github.com/topics/federated-learning)
    for a wide range of federated learning frameworks and libraries such as *FATE*,
    *FedML*, and *PySyft*.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习**是一种新颖的策略，它允许在本地服务器或节点之外不传输数据的情况下对敏感数据进行训练。因此，组织可以在保持数据在其组织本地服务器上的同时，共享其敏感数据以训练机器学习模型。同时，这也是一种解决防止与外部方共享数据的法规的解决方案。它基于我们接下来将要看到的分布式学习范式。请参阅[https://github.com/topics/federated-learning](https://github.com/topics/federated-learning)以获取广泛的联邦学习框架和库，如*FATE*、*FedML*和*PySyft*。'
- en: 'To begin, let’s differentiate between centralized and decentralized ML systems:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们区分集中式和分布式机器学习系统：
- en: '**Centralized ML system**: In centralized ML systems, all the training is done
    in one server. It is easier to implement, and thus it was traditionally applied
    to ML problems. However, it has many limitations due to the communication between
    the users and the central server. As shown in *Figure 3**.3*, the users need to
    send information to the central server where the training process will be executed.
    In addition to the latency issues, this approach is more vulnerable to attacks:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中式机器学习系统**：在集中式机器学习系统中，所有训练都在一个服务器上完成。这更容易实现，因此传统上应用于机器学习问题。然而，由于用户与中央服务器之间的通信，它有许多局限性。如图**3.3**所示，用户需要将信息发送到执行训练过程的中央服务器。除了延迟问题外，这种方法更容易受到攻击：'
- en: '![Figure 3.3 – Centralized and decentralized ML systems](img/B18494_03_003.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 集中式和分布式机器学习系统](img/B18494_03_003.jpg)'
- en: Figure 3.3 – Centralized and decentralized ML systems
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 集中式和分布式机器学习系统
- en: '**Decentralized ML system**: In contrast to the centralized option, this system
    allows the ML model to be trained on the client node rather than the centralized
    server. The clients send the weights and biases of their trained ML models to
    the admin server. Following this, the weights and biases are utilized to construct
    the final ML model. This is a clever solution to many privacy issues. For example,
    many hospitals in the EU cannot share their patients’ data with organizations
    or people outside the hospital. Thus, using the decentralized ML system, other
    organizations may access patient data since their data will not leave the hospital’s
    servers and the training process will also be done on their servers.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式机器学习系统**：与集中式选项相反，该系统允许机器学习模型在客户端节点上而不是在集中式服务器上训练。客户端将他们训练的机器学习模型的权重和偏差发送到管理员服务器。随后，权重和偏差被用来构建最终的机器学习模型。这是解决许多隐私问题的巧妙解决方案。例如，欧盟的许多医院不能与医院外的组织或个人共享他们的患者数据。因此，使用分布式机器学习系统，其他组织可以访问患者数据，因为他们的数据不会离开医院的服务器，训练过程也将在其服务器上完成。'
- en: 'In general, we can see that decentralized ML systems have the following advantages:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以看到分布式机器学习系统有以下优点：
- en: Less communication between the server and clients
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器和客户端之间的通信减少
- en: Better privacy as the clients do not share raw data; instead, they share weights
    and biases
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的隐私性，因为客户端不共享原始数据；相反，他们共享权重和偏差
- en: Better privacy since data is stored in the local nodes and does not leave the
    organization during the training process.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的隐私性，因为数据存储在本地节点，并且在训练过程中不会离开组织。
- en: In the next section, we will briefly discuss the essence of the privacy issues
    in real data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论真实数据中隐私问题的本质。
- en: Real data challenges and issues
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实数据挑战和问题
- en: 'So far in this chapter, we have presented different approaches for mitigating
    the privacy issues in real data. As you can see, it is clear that these approaches
    have limitations and they are not always practical. One fundamental issue is that
    ML models memorize the training data. Thus, given a trained ML model, it may be
    possible to retrieve some of the training data. Many researchers recently raised
    a red flag about the privacy of ML models, even after applying standard privacy
    solutions. For more information, please refer to *How To Break Anonymity of the
    Netflix Prize Dataset* ([https://arxiv.org/abs/cs/0610105](https://arxiv.org/abs/cs/0610105))
    and *The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural*
    *Networks* ([https://arxiv.org/abs/1802.08232](https://arxiv.org/abs/1802.08232)).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经介绍了多种缓解真实数据中隐私问题的方法。正如您所看到的，这些方法显然存在局限性，并不总是实用的。一个基本问题是机器学习模型会记住训练数据。因此，给定一个训练好的机器学习模型，可能有可能检索到一些训练数据。许多研究人员最近对机器学习模型的隐私问题提出了警告，即使在应用了标准的隐私解决方案之后。更多信息，请参阅*如何破解Netflix
    Prize数据集的匿名性* ([https://arxiv.org/abs/cs/0610105](https://arxiv.org/abs/cs/0610105))
    和 *秘密分享者：评估和测试神经网络中的无意记忆* ([https://arxiv.org/abs/1802.08232](https://arxiv.org/abs/1802.08232))。
- en: The nature of the real data is the essence of the problem. For instance, if
    you are given real human faces and you do some operations to anonymize this data
    or if you apply state-of-the-art approaches for PPML training, the data is still
    at risk of being divulged. Thus, it seems that instead of proposing more complicated
    and sometimes impractical solutions for privacy issues in real data, it’s time
    to look at other alternatives and focus on the essence of the problem, which is
    real data. In other words, it seems that synthetic data represents a rich and
    safe source for training large-scale ML models, therefore solving these complex
    privacy issues.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 真实数据的本质是问题的核心。例如，如果您被提供了真实的人类面部数据，并对这些数据进行了一些匿名化操作，或者如果您应用了最先进的PPML训练方法，数据仍然存在被泄露的风险。因此，似乎我们不应该再提出更多复杂且有时不切实际的解决方案来解决真实数据中的隐私问题，而是应该考虑其他替代方案，并关注问题的本质，即真实数据。换句话说，似乎合成数据代表了一个丰富且安全的训练大规模机器学习模型的来源，从而解决了这些复杂的隐私问题。
- en: Summary
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed why privacy is a critical problem in ML. At the
    same time, we learned what exactly the privacy problem in this field is. We also
    learned about the main standard solutions and their limitations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了为什么隐私是机器学习中的一个关键问题。同时，我们学习了这个领域中隐私问题的确切内容。我们还了解了主要的标准解决方案及其局限性。
- en: In the next chapter, we will discover what synthetic data is. This will help
    us build a solid foundation so that we can learn how to utilize synthetic data
    as a solution to the real data problems that we examined in the previous chapters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨什么是合成数据。这将帮助我们打下坚实的基础，以便我们能够学习如何利用合成数据作为解决我们在前几章中考察的真实数据问题的方案。
- en: Part 2:An Overview of Synthetic Data for Machine Learning
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：机器学习合成数据概述
- en: In this part, you will be introduced to synthetic data. You will learn about
    the history and the main types of synthetic data. Then, you will explore its main
    advantages. You will understand why synthetic data is a promising solution for
    many complex issues, such as privacy, that hinder the progress of ML in certain
    fields. You will learn how synthetic data generation approaches can be utilized
    to generate data for rare scenarios that are usually expensive and dangerous to
    capture with real data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，您将了解合成数据。您将了解其历史和主要类型。然后，您将探索其主要优势。您将理解为什么合成数据是解决许多复杂问题（如隐私）的潜在解决方案，这些问题阻碍了机器学习在某些领域的进步。您将学习如何利用合成数据生成方法来生成数据，这些数据通常难以用真实数据进行捕获，且成本高昂、危险。
- en: 'This part has the following chapters:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B18494_04.xhtml#_idTextAnchor065), *An Introduction to Synthetic
    Data*'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B18494_04.xhtml#_idTextAnchor065)，*合成数据简介*'
- en: '[*Chapter 5*](B18494_05.xhtml#_idTextAnchor083), *Synthetic Data as a Solution*'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B18494_05.xhtml#_idTextAnchor083)，*合成数据作为解决方案*'
