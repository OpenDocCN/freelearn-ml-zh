- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Privacy Issues in Real Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is becoming an essential part of our daily lives due to its varied applications.
    Thus, there is a growing concern about privacy issues in ML. Datasets and trained
    ML models may disclose personal and sensitive information, such as political views,
    biometric data, mental health, sexual orientation, and other private information.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about privacy issues and why this is a concern
    in ML. Furthermore, we will provide a brief introduction to privacy-preserving
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is privacy an issue in ML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What exactly is the privacy problem in ML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy-preserving ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real data challenges and issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is privacy an issue in ML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in the previous chapters, ML models need large-scale training
    data to converge and train well. The data can be collected from social media,
    online transactions, surveys, questionaries, or other sources. Thus, the collected
    data may contain sensitive information that individuals may not want to share
    with some organizations or individuals. If the data was shared or accessed by
    others, and thus the identities of the individuals were identified, that may cause
    them personal abuse, financial issues, or identity theft.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complexity of a privacy breach in ML is closely related to the following
    main three factors:'
  prefs: []
  type: TYPE_NORMAL
- en: ML task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: ML task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *task* mainly defines the type of training data that we need to collect
    and annotate. Thus, some ML tasks, such as weather prediction and music generation,
    may have fewer privacy issues compared to other ML tasks, such as biometric authentication
    and medical image processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The larger the dataset, the more issues you will have with privacy. If the dataset
    is a large-scale one, then you cannot store it on one device. Thus, you may need
    some cloud services and tools to manage your datasets, such as MongoDB, Hadoop,
    and Cassandra. Consequently, you may have less control over your data. Thus, you
    need to give more attention to the tools and services that process or manage your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Regulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many countries have clear and restrictive data protection regulations. For example,
    the UK has *The Data Protection Act 2018* ([https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted](https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted)).
    At the same time, similar regulations can be found in the EU, such as the *General
    Data Protection Regulation* ([https://gdpr-info.eu](https://gdpr-info.eu)). Thus,
    if you are using personal data, you have to consider data protection principles,
    such as transparency, being used for their specified purposes, and being kept
    for a specific period.
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset includes sensitive and confidential information, then you are
    subject to more restrictions. Your dataset may contain information about biometrics,
    gender, health, and religious and political opinions. Thus, if you repurpose a
    dataset that you, your organization, or someone else collected, this action may
    be illegal.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main limitations of utilizing real datasets is privacy issues. Due
    to this, we can see why synthetic data is a promising solution for these issues.
    In the next section, we will take a closer look at the privacy problem and its
    related topics, such as copyright and intellectual property infringement, which
    are major issues when collecting a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is the privacy problem in ML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within the scope of privacy in ML, there are two main concerns. The first is
    regarding the dataset itself – that is, how to collect it, how to keep it private,
    and how to prevent unauthorized access to sensitive information. The second is
    associated with the vulnerability of ML models to reveal the training data, which
    we will discuss in the next section. For now, let’s examine the issues related
    to dataset privacy in ML.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright and intellectual property infringement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Copyright is a legal term that’s used to protect the ownership of intellectual
    property. It prevents or limits others from using your work without your permission.
    For example, if you take a photograph, record a video, or write a blog, your work
    is protected by copyright. Thus, others may not share, reproduce, or distribute
    your work without permission. Consequently, images, videos, text, or other information
    we see on the internet may have restrictive copyrights. Therefore, if you want
    to collect a dataset, you must consider the copyright problem carefully. As we
    know, there are different approaches to curating a real dataset. As an ML practitioner,
    you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect data yourself**: You can use a camera, microphone, sensors, questionaries,
    and other methods to collect a tremendous amount of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collect data from the internet**: You can use search engines such as Google,
    Yahoo, Baidu, and DuckDuckGo to collect your dataset, similar to how the *ImageNet*
    ([https://www.image-net.org/](https://www.image-net.org/)) dataset was collected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collect data from other datasets**: You can combine different datasets to
    create a new dataset for a specific application – for example, if you are interested
    in visual object tracking for only one class – say, humans – in a specific scenario
    – say, adverse conditions. In this case, you can combine different datasets by
    excluding irrelevant classes and weather conditions. Thus, you create a new dataset
    for your particular problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a photo of someone with our camera or recording their voice using our
    hardware does not permit us to use their information. At the same time, asking
    for permission is not a practical solution for large-scale datasets. To avoid
    similar issues some, datasets, such as **ImageNet**, were initially collected
    from the internet. Then, it was annotated by human annotators. It should be noted
    that not every image we see on the internet can be used to build our dataset;
    some images have copyright licenses that restrict how they can be used. Thus,
    the availability of data that you need on the web does not necessarily mean that
    you can leverage that data for your problem. The same copyright issues can be
    seen for combining different datasets as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your ML model was trained on, say, Van Gogh paintings and learned how to
    generate artwork, a question arises regarding who owns the rights to the generated
    images: Van Gogh, ML engineers, or both? In ML, this is a controversial and complex
    issue. **ChatGPT** ([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt))
    and **Imagen** (*Photorealistic Text-to-Image Diffusion Models with Deep Language
    Understanding*) are examples of such models. Thus, training your model on a real
    dataset may not give you the right to fully leverage the potential of your ML
    model. You may still be subject to copyright and the intellectual properties of
    the samples used in the training process. As you may expect, leveraging synthetic
    data seems a promising solution to these problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and reproducibility of experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main issues in ML-based research is the difficulty in reproducing
    the experiments, and thus validating the results claimed in research papers. Data
    privacy is one of the key factors behind this issue. Many companies develop ML
    solutions using their own datasets. They may share trained models, but not the
    dataset itself because of many reasons related to privacy issues and regulations.
    Consequently, other researchers cannot reproduce the experiments and results.
    This creates a good chance for errors, bias, misinterpretation, falsification,
    and manipulation of research results. This is also another reason why using synthetic
    data in certain fields, such as healthcare and finance, can make the research
    more transparent and the results more trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy issues and bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias is another issue closely related to privacy problems in ML models and real
    data. ML models can be trained on biased data, which can result in biased outcomes.
    For example, some face recognition commercial applications were found to be less
    accurate in recognizing people with darker skin, females, or people aged between
    18 and 30\. The issue is due to the bias in the training data of the ML models.
    Thus, when the training data is not available because of privacy concerns, certain
    bias issues may arise. Consequently, this can lead to unequal treatment and discrimination
    based on race, gender, and other factors. Next, we will delve into traditional
    solutions to privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-preserving ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Privacy-preserving ML** is a hot topic since it proposes solutions for privacy
    issues in the ML field. Privacy-preserving ML proposes methods that allow researchers
    to use sensitive data to train ML models while withholding sensitive information
    from being shared or accessed by a third party or revealed by the ML model. In
    the first subsection, we will examine common methods for mitigating privacy issues
    in datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for privacy-preserving datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll delve into standard approaches for handling and protecting
    sensitive information in datasets. We will look at anonymization, centralized
    data, and differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Anonymization** can be considered one of the earliest approaches for privacy
    issues in datasets. Let’s assume you are given a dataset of patients’ medical
    records that contains their addresses, phone numbers, and postal codes. To anonymize
    this dataset, you can simply remove this sensitive data while keeping other medical
    records. Unfortunately, this approach does not work very well for two reasons.
    Sometimes, you cannot remove this sensitive information because they are part
    of the task. At the same time, anonymization may not be sufficient, and thus individuals
    may be identified by linking and combining other information in the dataset or
    other datasets. As an example, check the paper titled *The ’Re-Identification’
    of Governor William Weld’s Medical Information: A Critical Re-Examination of Health
    Data Identification Risks and Privacy Protections, Then and* *Now* ([https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2076397)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Surname** | **Has diabetes** | **Smoking** | **Gender** | **Age**
    | **Occupation** |'
  prefs: []
  type: TYPE_TB
- en: '| Mike | Chris | Yes | No | Male | 60 | Programmer |'
  prefs: []
  type: TYPE_TB
- en: '| Emma | Cunningham | Yes | Yes | Female | 40 | Lawyer |'
  prefs: []
  type: TYPE_TB
- en: '| Jan | Wan | No | No | Male | 32 | PhD Student |'
  prefs: []
  type: TYPE_TB
- en: '| Olivia | Cunningham | No | No | Female | 23 | Student |'
  prefs: []
  type: TYPE_TB
- en: Figure 3.1 – A sample dataset that includes sensitive information
  prefs: []
  type: TYPE_NORMAL
- en: Centralized data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another approach is based on the idea of keeping the data in a private server
    and not sharing the data itself but instead answering queries about the data.
    For example, here are some examples of queries regarding the dataset shown in
    *Figure 3**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: Return the minimum age of patients with diabetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the number of female patients with diabetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the average age of patients with diabetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, organizations can keep sensitive data while still allowing other
    researchers or organizations to leverage this data. However, this approach is
    not resilient against cyberattacks. These queries can be used by attackers to
    identify individuals and disclose sensitive information by combining complex queries
    and linking information from other datasets. For example, if you remove the **Name**
    and **Surname** columns but still use information regarding **Gender**, **Age**,
    and **Occupation**, it may still be possible to use these details to identify
    some patients.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a complementary solution to the previous approaches, differential privacy
    seems a promising solution. However, there are some limitations, as we will see
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea of this approach is keeping individuals’ information secured while
    still learning about the phenomena under consideration. This approach is based
    on queries to a server that contains all the sensitive data. Many companies utilize
    differential privacy, such as Uber, Google, and Apple. The algorithm may add random
    noise to the information in the dataset or the queries, as shown in *Figure 3**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Local and global differential privacy](img/B18494_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Local and global differential privacy
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy has two main approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local differential privacy** (**LDP**): The data taken from the users are
    processed with noise, such as Laplacian noise. This process makes the data more
    secure against attacks. Even if attackers can access the data, they will have
    a noisy version and they will not be able to identify individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global differential privacy** (**GDP**): The raw data taken from clients
    is not altered but directly stored in the server without any noise being added.
    When a query is received, the raw answer (accurate) is returned after noise has
    been added to it. This process generates a private answer to protect data privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, with these data protection mechanisms, it’s supposed that ML models can
    now be trained on sensitive data without disclosing individual sensitive information.
    Unfortunately, this assumption is not always valid, as we will see in the next
    few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for privacy-preserving ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Privacy-preserving machine learning** (**PPML**) aims to prevent training
    data leakage. ML models may memorize sensitive information in the training process,
    and thus confidential information may be revealed by the ML model. Standard PPML
    methods rely on differential privacy, which we discussed earlier. Next, you will
    be introduced to federated learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Federated learning** is a novel strategy that allows ML to be trained on
    sensitive data while not transferring the data outside of the local server or
    node. Thus, organizations can share their sensitive data to train ML models while
    keeping the data in the organization’s local servers. At the same time, this is
    a solution for regulation that prevents sharing data with external parties. It
    is based on the paradigm of decentralized learning, which we will see next. Please
    refer to [https://github.com/topics/federated-learning](https://github.com/topics/federated-learning)
    for a wide range of federated learning frameworks and libraries such as *FATE*,
    *FedML*, and *PySyft*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, let’s differentiate between centralized and decentralized ML systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centralized ML system**: In centralized ML systems, all the training is done
    in one server. It is easier to implement, and thus it was traditionally applied
    to ML problems. However, it has many limitations due to the communication between
    the users and the central server. As shown in *Figure 3**.3*, the users need to
    send information to the central server where the training process will be executed.
    In addition to the latency issues, this approach is more vulnerable to attacks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Centralized and decentralized ML systems](img/B18494_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Centralized and decentralized ML systems
  prefs: []
  type: TYPE_NORMAL
- en: '**Decentralized ML system**: In contrast to the centralized option, this system
    allows the ML model to be trained on the client node rather than the centralized
    server. The clients send the weights and biases of their trained ML models to
    the admin server. Following this, the weights and biases are utilized to construct
    the final ML model. This is a clever solution to many privacy issues. For example,
    many hospitals in the EU cannot share their patients’ data with organizations
    or people outside the hospital. Thus, using the decentralized ML system, other
    organizations may access patient data since their data will not leave the hospital’s
    servers and the training process will also be done on their servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, we can see that decentralized ML systems have the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Less communication between the server and clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better privacy as the clients do not share raw data; instead, they share weights
    and biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better privacy since data is stored in the local nodes and does not leave the
    organization during the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will briefly discuss the essence of the privacy issues
    in real data.
  prefs: []
  type: TYPE_NORMAL
- en: Real data challenges and issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this chapter, we have presented different approaches for mitigating
    the privacy issues in real data. As you can see, it is clear that these approaches
    have limitations and they are not always practical. One fundamental issue is that
    ML models memorize the training data. Thus, given a trained ML model, it may be
    possible to retrieve some of the training data. Many researchers recently raised
    a red flag about the privacy of ML models, even after applying standard privacy
    solutions. For more information, please refer to *How To Break Anonymity of the
    Netflix Prize Dataset* ([https://arxiv.org/abs/cs/0610105](https://arxiv.org/abs/cs/0610105))
    and *The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural*
    *Networks* ([https://arxiv.org/abs/1802.08232](https://arxiv.org/abs/1802.08232)).'
  prefs: []
  type: TYPE_NORMAL
- en: The nature of the real data is the essence of the problem. For instance, if
    you are given real human faces and you do some operations to anonymize this data
    or if you apply state-of-the-art approaches for PPML training, the data is still
    at risk of being divulged. Thus, it seems that instead of proposing more complicated
    and sometimes impractical solutions for privacy issues in real data, it’s time
    to look at other alternatives and focus on the essence of the problem, which is
    real data. In other words, it seems that synthetic data represents a rich and
    safe source for training large-scale ML models, therefore solving these complex
    privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed why privacy is a critical problem in ML. At the
    same time, we learned what exactly the privacy problem in this field is. We also
    learned about the main standard solutions and their limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover what synthetic data is. This will help
    us build a solid foundation so that we can learn how to utilize synthetic data
    as a solution to the real data problems that we examined in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2:An Overview of Synthetic Data for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will be introduced to synthetic data. You will learn about
    the history and the main types of synthetic data. Then, you will explore its main
    advantages. You will understand why synthetic data is a promising solution for
    many complex issues, such as privacy, that hinder the progress of ML in certain
    fields. You will learn how synthetic data generation approaches can be utilized
    to generate data for rare scenarios that are usually expensive and dangerous to
    capture with real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18494_04.xhtml#_idTextAnchor065), *An Introduction to Synthetic
    Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18494_05.xhtml#_idTextAnchor083), *Synthetic Data as a Solution*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
