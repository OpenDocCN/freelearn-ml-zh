<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer160">
<h1 id="_idParaDest-114"><em class="italic"><a id="_idTextAnchor113"/>Chapter 9</em>: K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression</h1>
<p>As is true for support vector machines, K-nearest neighbors and decision tree models are best known as classification models. However, they can also be used for regression and present some advantages over classical linear regression. K-nearest neighbors and decision trees can handle nonlinearity well and no assumptions regarding the Gaussian distribution of features need to be made. Moreover, by adjusting our value of <em class="italic">k</em> for <strong class="bold">K-nearest neighbors</strong> (<strong class="bold">KNN</strong>) or maximal depth for decision trees, we can avoid fitting the training data too precisely.</p>
<p>This brings us back to a theme from the previous two chapters – how to increase model complexity, including accounting for nonlinearity, without overfitting. We have seen how allowing some bias can reduce variance and give us more reliable estimates of model performance. We will continue to explore that balance in this chapter.</p>
<p>Specifically, we will cover the following main topics:</p>
<ul>
<li>Key concepts for K-nearest neighbors regression</li>
<li>K-nearest neighbors regression</li>
<li>Key concepts for decision tree and random forest regression</li>
<li>Decision tree and random forest regression</li>
<li>Using gradient boosted regression</li>
</ul>
<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>Technical requirements</h1>
<p>In this chapter, we will work with the scikit-learn and <strong class="source-inline">matplotlib</strong> libraries. We will also work with XGBoost. You can use <strong class="source-inline">pip</strong> to install these packages. </p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor115"/>Key concepts for K-nearest neighbors regression</h1>
<p>Part <a id="_idIndexMarker734"/>of the appeal of the KNN algorithm is that it is quite straightforward and easy to interpret. For each observation where we need to predict the target, KNN finds the <em class="italic">k</em> training observations whose features are most similar to those of that observation. When the target is categorical, KNN selects the most frequent value of the target for the <em class="italic">k</em> training observations. (We often select an odd value for <em class="italic">k</em> for classification problems to avoid ties.)</p>
<p>When the target is numeric, KNN gives us the average value of the target for the <em class="italic">k</em> training observations. By <em class="italic">training</em> observation, I mean those observations that have known target values. No real training is done with KNN, as it is what is called a <a id="_idIndexMarker735"/>lazy learner. I will discuss that in more detail later in this section.</p>
<p><em class="italic">Figure 9.1</em> illustrates using K-nearest neighbors for classification with values of 1 and 3 for <em class="italic">k</em>. When <em class="italic">k</em> is 1, our new observation will be assigned the red label. When <em class="italic">k</em> is 3, it will be assigned blue:</p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 9.1 – K-nearest neighbors with a k of 1 and 3 " height="576" src="image/B17978_09_001.jpg" width="853"/>
</div>
</div>
<p class="figure-caption">Figure 9.1 – K-nearest neighbors with a k of 1 and 3</p>
<p>But<a id="_idIndexMarker736"/> what do we mean by similar, or nearest, observations? There are several ways to measure similarity, but a common measure is the Euclidean distance. The Euclidean distance<a id="_idIndexMarker737"/> is the sum of the squared difference between two points. This may remind you of the Pythagorean theorem. The Euclidean distance from point a to point b is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="" height="223" src="image/B17978_09_0011.jpg" width="495"/>
</div>
</div>
<p>A reasonable alternative to Euclidean distance is Manhattan distance. The Manhattan distance<a id="_idIndexMarker738"/> from point a to point b is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="" height="177" src="image/B17978_09_002.jpg" width="490"/>
</div>
</div>
<p>Manhattan distance<a id="_idIndexMarker739"/> is <a id="_idIndexMarker740"/>sometimes called taxicab distance. This is because it reflects the distance between two points along a path on a grid. <em class="italic">Figure 9.2</em> illustrates the Manhattan distance and compares it to the<a id="_idIndexMarker741"/> Euclidean distance:</p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 9.2 – Euclidean and Manhattan measures of distance " height="533" src="image/B17978_09_0021.jpg" width="479"/>
</div>
</div>
<p class="figure-caption">Figure 9.2 – Euclidean and Manhattan measures of distance</p>
<p>Using Manhattan distance can yield better results when features are very different in terms of type or scale. However, we can treat the choice of distance measure as an empirical question; that is, we can try both (or other distance measures) and see which gives us the best-performing model. We will demonstrate this with a grid search in the next section.</p>
<p>As you likely suspect, KNN models are sensitive to the choice of <em class="italic">k</em>. Lower values of <em class="italic">k</em> will result in a model that attempts to identify subtle distinctions between observations. Of course, there is a substantial risk of overfitting at very low values of <em class="italic">k</em>. But at higher values of <em class="italic">k</em>, our model may not be flexible enough. We are once again confronted with the variance-bias trade-off. Lower <em class="italic">k</em> values result in less bias and more variance, while high values result in the opposite.</p>
<p>There is no definitive answer to the choice of <em class="italic">k</em>. A good rule of thumb is to start with the square root of the number of observations. However, just as we would do for the distance measure, we should test a model’s performance at different values of <em class="italic">k</em>.</p>
<p>K-nearest neighbors is a lazy learner algorithm, as I have already mentioned. No calculations are performed at training time. The learning happens mainly during testing. This has some disadvantages. KNN may not be a good choice when there are many instances or dimensions in the data, and the speed of predictions matters. It also tends not to perform well when we have sparse data – that is, datasets with many 0 values.</p>
<p>K-nearest neighbors is a <a id="_idIndexMarker742"/>non-parametric algorithm. No assumptions are made about the attributes of the underlying data, such as linearity or normally distributed features. It can often give us decent results when a linear model would not. We will build a KNN regression model in the next section.</p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor116"/>K-nearest neighbors regression</h1>
<p>As mentioned <a id="_idIndexMarker743"/>previously, K-nearest neighbors can be a good alternative to linear regression when the assumptions of ordinary least squares do not hold, and the number of observations and dimensions is small. It is also very easy to specify, so even if we do not use it for our final model, it can be valuable for diagnostic purposes.</p>
<p>In this section, we will use KNN to build a model of the ratio of female to male incomes at the level of country. We will base this on labor force participation rates, educational attainment, teenage birth frequency, and female participation in politics at the highest level. This is a good dataset to experiment with because the small sample size and feature space mean that it is not likely to tax your system’s resources. The small number of features also makes it easier to interpret. The drawback is that it might be hard to find significant results. That being said, let’s see what we find.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will be working with the income gap dataset throughout this chapter. The dataset has been made available for public use by the United Nations Development Program at <a href="https://www.kaggle.com/datasets/undp/human-development">https://www.kaggle.com/datasets/undp/human-development</a>. There is one record per country with aggregate employment, income, and education data by gender for 2015.</p>
<p>Let’s start<a id="_idIndexMarker744"/> building our model:</p>
<ol>
<li>First, we must import some of the same <strong class="source-inline">sklearn</strong> libraries we used in the previous two chapters. We must also import <strong class="source-inline">KNeighborsRegressor</strong> and an old friend from <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em> – that is, <strong class="source-inline">SelectFromModel</strong>. We will use <strong class="source-inline">SelectFromModel</strong> to add feature selection to the pipeline we will construct:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from sklearn.neighbors import KNeighborsRegressor</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.feature_selection import SelectFromModel</p><p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>We also need the <strong class="source-inline">OutlierTrans</strong> class that we created in <a href="B17978_07_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 7</em></a>, <em class="italic">Linear </em><em class="italic">Regression Models</em>. We will use it to identify outliers based on the interquartile range, as we first discussed in <a href="B17978_03_ePub.xhtml#_idTextAnchor034"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying and Fixing Missing Values</em>:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Next, we <a id="_idIndexMarker745"/>must load the income data. We also need to construct a series for the ratio of female to male incomes, the years of education ratio, the labor force participation ratio, and the human development index ratio. Lower values for any of these measures suggest a possible advantage for males, assuming a positive relationship between these features and the ratio of female to male incomes. For example, we would expect the income ratio to improve – that is, to get closer to 1.0 – as the labor force participation ratio gets closer to 1.0– that is, when the labor force participation of women equals that of men.</li>
<li>We must drop rows where our target, <strong class="source-inline">incomeratio</strong>, is missing:<p class="source-code">un_income_gap = pd.read_csv("data/un_income_gap.csv")</p><p class="source-code">un_income_gap.set_index('country', inplace=True)</p><p class="source-code">un_income_gap['incomeratio'] = \</p><p class="source-code">  un_income_gap.femaleincomepercapita / \</p><p class="source-code">    un_income_gap.maleincomepercapita</p><p class="source-code">un_income_gap['educratio'] = \</p><p class="source-code">  un_income_gap.femaleyearseducation / \</p><p class="source-code">     un_income_gap.maleyearseducation</p><p class="source-code">un_income_gap['laborforcepartratio'] = \</p><p class="source-code">  un_income_gap.femalelaborforceparticipation / \</p><p class="source-code">     un_income_gap.malelaborforceparticipation</p><p class="source-code">un_income_gap['humandevratio'] = \</p><p class="source-code">  un_income_gap.femalehumandevelopment / \</p><p class="source-code">     un_income_gap.malehumandevelopment</p><p class="source-code">un_income_gap.dropna(subset=['incomeratio'], inplace=True)</p></li>
<li>Let’s look<a id="_idIndexMarker746"/> at a few rows of data:<p class="source-code">num_cols = ['educratio','laborforcepartratio',</p><p class="source-code">'humandevratio','genderinequality','maternalmortality',</p><p class="source-code">  'adolescentbirthrate','femaleperparliament',</p><p class="source-code">'incomepercapita']</p><p class="source-code">gap_sub = un_income_gap[['incomeratio'] + num_cols]</p><p class="source-code">gap_sub.head()</p><p class="source-code"><strong class="bold">incomeratio  educratio  laborforcepartratio  humandevratio\</strong></p><p class="source-code"><strong class="bold">country</strong></p><p class="source-code"><strong class="bold">Norway         0.78    1.02    0.89    1.00</strong></p><p class="source-code"><strong class="bold">Australia      0.66    1.02    0.82    0.98</strong></p><p class="source-code"><strong class="bold">Switzerland    0.64    0.88    0.83    0.95</strong></p><p class="source-code"><strong class="bold">Denmark        0.70    1.01    0.88    0.98</strong></p><p class="source-code"><strong class="bold">Netherlands    0.48    0.95    0.83    0.95</strong></p><p class="source-code"><strong class="bold">genderinequality  maternalmortality  adolescentbirthrate\</strong></p><p class="source-code"><strong class="bold">country</strong></p><p class="source-code"><strong class="bold">Norway        0.07    4.00    7.80</strong></p><p class="source-code"><strong class="bold">Australia     0.11    6.00    12.10</strong></p><p class="source-code"><strong class="bold">Switzerland   0.03    6.00    1.90</strong></p><p class="source-code"><strong class="bold">Denmark       0.05    5.00    5.10</strong></p><p class="source-code"><strong class="bold">Netherlands   0.06    6.00    6.20</strong></p><p class="source-code"><strong class="bold">             femaleperparliament  incomepercapita  </strong></p><p class="source-code"><strong class="bold">country                                            </strong></p><p class="source-code"><strong class="bold">Norway       39.60    64992</strong></p><p class="source-code"><strong class="bold">Australia    30.50    42261</strong></p><p class="source-code"><strong class="bold">Switzerland  28.50    56431</strong></p><p class="source-code"><strong class="bold">Denmark      38.00    44025</strong></p><p class="source-code"><strong class="bold">Netherlands  36.90    45435</strong></p></li>
<li>Let’s also <a id="_idIndexMarker747"/>look at some descriptive statistics:<p class="source-code">gap_sub.\</p><p class="source-code">  agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                    count  min    median    max</strong></p><p class="source-code"><strong class="bold">incomeratio         177.00 0.16   0.60      0.93</strong></p><p class="source-code"><strong class="bold">educratio           169.00 0.24   0.93      1.35</strong></p><p class="source-code"><strong class="bold">laborforcepartratio 177.00 0.19   0.75      1.04</strong></p><p class="source-code"><strong class="bold">humandevratio       161.00 0.60   0.95      1.03</strong></p><p class="source-code"><strong class="bold">genderinequality    155.00 0.02   0.39      0.74</strong></p><p class="source-code"><strong class="bold">maternalmortality   </strong><strong class="bold">174.00 1.00   60.00     1,100.00</strong></p><p class="source-code"><strong class="bold">adolescentbirthrate 177.00 0.60   40.90     204.80</strong></p><p class="source-code"><strong class="bold">femaleperparliament 174.00 0.00   19.35     57.50</strong></p><p class="source-code"><strong class="bold">incomepercapita     177.00 581.00 10,512.00 123,124.00</strong></p></li>
</ol>
<p>We <a id="_idIndexMarker748"/>have 177 observations with our target variable, <strong class="source-inline">incomeratio</strong>. A couple of features, <strong class="source-inline">humandevratio</strong> and <strong class="source-inline">genderinequality</strong>, have more than 15 missing values. We will need to impute some reasonable values there. We will also need to do some scaling as some features have very different ranges than others, from <strong class="source-inline">incomeratio</strong> and <strong class="source-inline">incomepercapita</strong> on one end to <strong class="source-inline">educratio</strong> and <strong class="source-inline">humandevratio</strong> on the other.</p>
<p class="callout-heading">Note</p>
<p class="callout">The dataset has separate human development indices for women and men. The index is a measure of health, access to knowledge, and standard of living. The <strong class="source-inline">humandevratio</strong> feature, which we calculated earlier, divides the score for women by the score for men. The <strong class="source-inline">genderinequality</strong> feature is an index of health and labor market policies in countries that have a disproportionate impact on women. <strong class="source-inline">femaleperparliament</strong> is the percentage of the highest national legislative body that is female.</p>
<ol>
<li value="7">We should also look at a heatmap of the correlations of features and the features with the target. It is a good idea to keep the higher correlations (either negative or positive) in mind when we are doing our modeling. The higher positive correlations are represented with the warmer colors. <strong class="source-inline">laborforcepartratio</strong>, <strong class="source-inline">humandevratio</strong>, and <strong class="source-inline">maternalmortality</strong> are all positively correlated with our target, the latter somewhat surprisingly. <strong class="source-inline">humandevratio</strong> and <strong class="source-inline">laborforcepartratio</strong> are also correlated, so our model may have some trouble disentangling the influence of each. Some feature selection should help us figure out which feature is more important. (We will need to use a wrapper or embedded feature selection method to tease that out well. We discuss those methods in detail in <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em>.) Look at the following code:<p class="source-code">corrmatrix = gap_sub.corr(method="pearson")</p><p class="source-code">corrmatrix</p><p class="source-code">sns.heatmap(corrmatrix, xticklabels=corrmatrix.columns,</p><p class="source-code">yticklabels=corrmatrix.columns, cmap="coolwarm")</p><p class="source-code">plt.title('Heat Map of Correlation Matrix')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker749"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="Figure 9.3 – Correlation matrix " height="462" src="image/B17978_09_003.jpg" width="613"/>
</div>
</div>
<p class="figure-caption">Figure 9.3 – Correlation matrix</p>
<ol>
<li value="8">Next, we<a id="_idIndexMarker750"/> must set up the training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(gap_sub[num_cols],\</p><p class="source-code">  gap_sub[['incomeratio']], test_size=0.2, random_state=0)</p></li>
</ol>
<p>We are now ready to set up the KNN regression model. We will also build a pipeline to handle outliers, do an imputation based on the median value of each feature, scale features, and do some feature selection with scikit-learn’s <strong class="source-inline">SelectFromModel</strong>. </p>
<ol>
<li value="9">We will use linear regression for our feature selection, but we can choose any algorithm that will return feature importance values. We will set the feature importance threshold to 80% of the mean feature importance. The mean is the default. Our choice here is fairly arbitrary, but I like the idea of keeping features that are just below the average feature importance level, in addition to those with higher importance of course:<p class="source-code">knnreg = KNeighborsRegressor()</p><p class="source-code">feature_sel = SelectFromModel(LinearRegression(), threshold="0.8*mean")</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(3), \</p><p class="source-code">  SimpleImputer(strategy="median"), StandardScaler(), \</p><p class="source-code">  feature_sel, knnreg)</p></li>
<li>We are now ready to do a grid search to find the best parameters. First, we will create a dictionary, <strong class="source-inline">knnreg_params</strong>, to indicate that we want the KNN model to select values of <em class="italic">k</em> from 3 to 19, skipping even numbers. We also want the grid search to find the best distance measure – Euclidean, Manhattan, or Minkowski:<p class="source-code">knnreg_params = {</p><p class="source-code"> 'kneighborsregressor__n_neighbors': \</p><p class="source-code">     np.arange(3, 21, 2),</p><p class="source-code"> 'kneighborsregressor__metric': \</p><p class="source-code">     ['euclidean','manhattan','minkowski']</p><p class="source-code">}</p></li>
<li>We <a id="_idIndexMarker751"/>will pass those parameters to the <strong class="source-inline">RandomizedSearchCV</strong> object and then fit the model. We can use the <strong class="source-inline">best_params_</strong> attribute of <strong class="source-inline">RandomizedSearchCV</strong> to see the selected hyperparameters for our feature selection and KNN regression. These results suggest that the best hyperparameter values are 11 for <em class="italic">k</em> for KNN and Manhattan for the distance metric:</li>
</ol>
<p>The best model has a negative mean squared error of -0.05. This is fairly decent, given the small sample size. It is less than 10% of the median value of <strong class="source-inline">incomeratio</strong>, which is 0.6:</p>
<p class="source-code">rs = RandomizedSearchCV(pipe1, knnreg_params, cv=4, n_iter=20, \</p>
<p class="source-code">  scoring='neg_mean_absolute_error', random_state=1)</p>
<p class="source-code">rs.fit(X_train, y_train)</p>
<p class="source-code">rs.best_params_</p>
<p class="source-code"><strong class="bold">{'kneighborsregressor__n_neighbors': 11,</strong></p>
<p class="source-code"><strong class="bold"> 'kneighborsregressor__metric': 'manhattan'}</strong></p>
<p class="source-code">rs.best_score_</p>
<p class="source-code"><strong class="bold">-0.05419731104389228</strong></p>
<ol>
<li value="12">Let’s <a id="_idIndexMarker752"/>take a look at the features that were selected during the feature selection step of the pipeline. Only two features were selected – <strong class="source-inline">laborforcepartratio</strong> and <strong class="source-inline">humandevratio</strong>. Note that this step is not necessary to run our model. It just helps us interpret it:<p class="source-code">selected = rs.best_estimator_['selectfrommodel'].get_support()</p><p class="source-code">np.array(num_cols)[selected]</p><p class="source-code"><strong class="bold">array(['laborforcepartratio', 'humandevratio'], dtype='&lt;U19')</strong></p></li>
<li>This is a tad easier if you are using <em class="italic">scikit-learn</em> 1.0 or later. You can use the <strong class="source-inline">get_feature_names_out</strong> method in that case:<p class="source-code">rs.best_estimator_['selectfrommodel'].\</p><p class="source-code">  get_feature_names_out(np.array(num_cols))</p><p class="source-code"><strong class="bold">array(['laborforcepartratio', 'humandevratio'], dtype=object)</strong></p></li>
<li>We should also take a peek at some of the other top results. There is a model that uses <strong class="source-inline">euclidean</strong> distance that performs nearly as well as the best model:<p class="source-code">results = \</p><p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p><p class="source-code">    columns=['meanscore']).\</p><p class="source-code">  join(pd.DataFrame(rs.cv_results_['params'])).\</p><p class="source-code">  sort_values(['meanscore'], ascending=False)</p><p class="source-code">results.head(3).T</p><p class="source-code"><strong class="bold">              </strong><strong class="bold">13       1      3</strong></p><p class="source-code"><strong class="bold">Meanscore   -0.05   -0.05   -0.05</strong></p><p class="source-code"><strong class="bold">regressor__kneighborsregressor__n_neighbors  11  13  9</strong></p><p class="source-code"><strong class="bold">regressor__kneighborsregressor__metric  manhattan  manhattan  euclidean</strong></p></li>
<li>Let’s<a id="_idIndexMarker753"/> look at the residuals for this model. We can use the predict method of the <strong class="source-inline">RandomizedSearchCV</strong> object to generate predictions on the testing data. The residuals are nicely balanced around 0. There is a little bit of negative skew but that’s not bad either. There is low kurtosis, but we are good with there not being much in the way of tails in this case. It likely reflects not very much in the way of outlier residuals:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.incomeratio-preddf.prediction</p><p class="source-code">preddf.resid.agg(['mean','median','skew','kurtosis'])</p><p class="source-code"><strong class="bold">mean            -0.01</strong></p><p class="source-code"><strong class="bold">median          -0.01</strong></p><p class="source-code"><strong class="bold">skew            -0.61</strong></p><p class="source-code"><strong class="bold">kurtosis         0.23</strong></p><p class="source-code"><strong class="bold">Name: resid, dtype: float64</strong></p></li>
<li>Let’s plot the residuals:<p class="source-code">plt.hist(preddf.resid, color="blue")</p><p class="source-code">plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Histogram of Residuals for Gax Tax Model")</p><p class="source-code">plt.xlabel("Residuals")</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.xlim()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This<a id="_idIndexMarker754"/> produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="Figure 9.4 – Residuals for the income ratio model with KNN regression " height="439" src="image/B17978_09_004.jpg" width="568"/>
</div>
</div>
<p class="figure-caption">Figure 9.4 – Residuals for the income ratio model with KNN regression</p>
<p>The residuals also look pretty decent when we plot them. There are a couple of countries, however, where we are more than 0.1 off in our prediction. We over-predict in both of those cases. (The dashed red line is the average residual amount.)</p>
<ol>
<li value="17">Let’s <a id="_idIndexMarker755"/>also look at a scatterplot. Here, we can see that the two large over-predictions are at different ends of the predicted range. In general, the residuals are fairly constant across the predicted income ratio range. We just may want to do something with the two outliers:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Income Gap")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 9.5 – Scatterplot of predictions and residuals for the income ratio model with KNN regression " height="446" src="image/B17978_09_005.jpg" width="589"/>
</div>
</div>
<p class="figure-caption">Figure 9.5 – Scatterplot of predictions and residuals for the income ratio model with KNN regression</p>
<p>We <a id="_idIndexMarker756"/>should take a closer look at the countries where there were high residuals. Our model does not do a good job of predicting income ratios for either Afghanistan or the Netherlands, over-predicting a fair bit in both cases. Recall that our feature selection step gave us a model with just two predictors: <strong class="source-inline">laborforcepartratio</strong> and <strong class="source-inline">humandevratio</strong>. </p>
<p>For Afghanistan, the labor force participation ratio (the participation of females relative to that of males) is very near the minimum of 0.19 and the human development ratio is at the minimum. This still does not get us close to predicting the very low income ratio (the income of women relative to that of men), which is also at the minimum.</p>
<p>For the Netherlands, the labor force participation ratio of 0.83 is a fair bit above the median of 0.75, but the human development ratio is right at the median. This is why our model predicts an income ratio a little above the median of 0.6. The actual income ratio for the Netherlands is, then, surprisingly low:</p>
<p class="source-code">preddf.loc[np.abs(preddf.resid)&gt;=0.1,</p>
<p class="source-code">  ['incomeratio', 'prediction', 'resid', </p>
<p class="source-code">   'laborforcepartratio', 'humandevratio']].T</p>
<p class="source-code"><strong class="bold">country                     Afghanistan    Netherlands</strong></p>
<p class="source-code"><strong class="bold">incomeratio                  0.16           0.48</strong></p>
<p class="source-code"><strong class="bold">prediction                   0.32           0.65</strong></p>
<p class="source-code"><strong class="bold">resid                       -0.16          -0.17</strong></p>
<p class="source-code"><strong class="bold">laborforcepartratio          0.20           0.83</strong></p>
<p class="source-code"><strong class="bold">humandevratio                0.60           0.95</strong></p>
<p>Here, we can <a id="_idIndexMarker757"/>see some of the advantages of KNN regression. We can get okay predictions on difficult-to-model data without spending a lot of time specifying a model. Other than some imputation and scaling, we did not do any transformations or create interaction effects. We did not need to worry about nonlinearity either. KNN regression can handle that fine.</p>
<p>But this approach would probably not scale very well. A lazy learner was fine in this example. For more industrial-level work, however, we often need to turn to an algorithm with many of the advantages of KNN, but without some of the disadvantages. We will explore decision trees and random forest regression in the remainder of this chapter.</p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor117"/>Key concepts for decision tree and random forest regression</h1>
<p>Decision trees<a id="_idIndexMarker758"/> are an exceptionally useful machine learning tool. They have some of the same advantages as KNN – they are non-parametric, easy to interpret, and can work with a wide range of data – but without some of the limitations. </p>
<p>Decision trees group<a id="_idIndexMarker759"/> the observations in a dataset based on the values of their features. This is done with a series of binary decisions, starting from an initial split at the root node, and ending with a leaf for each grouping. All observations with the same values, or the same range of values, along the branches from the root node to that leaf, get the same predicted value for the target. When the target is numeric, that is the average value for the <a id="_idIndexMarker760"/>target for the training observations at that leaf. <em class="italic">Figure 9.6</em> illustrates this:</p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 9.6 – Decision tree model of nightly hours of sleep " height="438" src="image/B17978_09_006.jpg" width="633"/>
</div>
</div>
<p class="figure-caption">Figure 9.6 – Decision tree model of nightly hours of sleep</p>
<p>This is a model of nightly hours of sleep for individuals based on weekly hours worked, number of children, number of other adults in the home, and whether the person is enrolled in school. (These results are based on hypothetical data.) The root node is based on weekly hours worked and splits the data into observations with hours worked greater than 30 and 30 or less. The numbers in parentheses are the percentage of the training data that reaches that node. 60% of the observations have hours worked greater than 30. On the left-hand side of the tree, our model further splits the data by the number of children and then by the number of other adults in the home. On the other side of the tree, which represents observations with hours worked less than or equal to 30, the only additional split is by enrollment in school. </p>
<p>I realize now that<a id="_idIndexMarker761"/> all readers will not see this in color. We can navigate up the tree from each leaf to describe how the tree has segmented the data. 15% of observations have 0 other adults in the home, more than 1 child, and weekly hours worked greater than 30. These observations have an average nightly hours slept value of 4.5 hours. This will be the predicted value for new observations with the same characteristics.</p>
<p>You might be wondering how the<a id="_idIndexMarker762"/> decision tree algorithm selected the threshold amounts for the numeric features. Why greater than 30 for weekly hours worked or greater than 1 for the number of children, for example? The algorithm selects the split at each level, starting with the root, which minimizes the sum of squared errors. More precisely, splits are chosen that minimize:</p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="" height="185" src="image/B17978_09_0031.jpg" width="527"/>
</div>
</div>
<p>You may have noticed the similarity with optimization for linear regression. But there are several advantages of decision tree regression over linear regression. Decision trees can be used to model both linear and nonlinear relationships without us having to modify features. We also can avoid feature scaling with decision trees, as the algorithm can deal with very different ranges in our features.</p>
<p>The main disadvantage<a id="_idIndexMarker763"/> of decision trees is their high variance. Depending on the characteristics of our data, we can get a very different model each time we fit a decision tree. We can use ensemble methods, such as bagging or random forest, to address this issue.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Using random forest regression</h2>
<p>Random forests, perhaps <a id="_idIndexMarker764"/>not surprisingly, are collections of decision trees. But this would not distinguish a random forest from bootstrap aggregating, commonly referred to as <a id="_idIndexMarker765"/>bagging. Bagging is often used to reduce the variance of machine <a id="_idIndexMarker766"/>learning algorithms that have high variances, such as decision trees. With bagging, we generate random samples from our dataset. Then, we run our model, such as a decision tree regression, on each of those samples, averaging the predictions.</p>
<p>However, the samples generated with bagging can be correlated, and the resulting decision trees may have many similarities. This is more likely to be the case when there are just a few features that explain much of the variation. Random forests address this issue by limiting the number of features that can be selected for each tree. A good rule of thumb is to divide the number <a id="_idIndexMarker767"/>of features available by 3 to determine the number of features to use for each split for each decision tree. For example, if there are 21 features, we would use seven for each split. We will build both decision tree and random forest regression models in the next section.</p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Decision tree and random forest regression</h1>
<p>We will use a decision<a id="_idIndexMarker768"/> tree and a random forest in this <a id="_idIndexMarker769"/>section to build a regression model with the same income gap data we worked with earlier in this chapter. We will also use tuning to identify the hyperparameters that give us the best-performing model, just as we did with KNN regression. Let’s get started:</p>
<ol>
<li value="1">We must load many of the same libraries as we did with KNN regression, plus <strong class="source-inline">DecisionTreeRegressor</strong> and <strong class="source-inline">RandomForestRegressor</strong> from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from sklearn.tree import DecisionTreeRegressor, plot_tree</p><p class="source-code">from sklearn.ensemble import RandomForestRegressor</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.feature_selection import SelectFromModel</p></li>
<li>We must also import our class for handling outliers:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We must<a id="_idIndexMarker770"/> load the same income gap data that <a id="_idIndexMarker771"/>we worked with previously and create testing and training DataFrames:<p class="source-code">un_income_gap = pd.read_csv("data/un_income_gap.csv")</p><p class="source-code">un_income_gap.set_index('country', inplace=True)</p><p class="source-code">un_income_gap['incomeratio'] = \</p><p class="source-code">  un_income_gap.femaleincomepercapita / \</p><p class="source-code">    un_income_gap.maleincomepercapita</p><p class="source-code">un_income_gap['educratio'] = \</p><p class="source-code">  un_income_gap.femaleyearseducation / \</p><p class="source-code">     un_income_gap.maleyearseducation</p><p class="source-code">un_income_gap['laborforcepartratio'] = \</p><p class="source-code">  un_income_gap.femalelaborforceparticipation / \</p><p class="source-code">     un_income_gap.malelaborforceparticipation</p><p class="source-code">un_income_gap['humandevratio'] = \</p><p class="source-code">  un_income_gap.femalehumandevelopment / \</p><p class="source-code">     un_income_gap.malehumandevelopment</p><p class="source-code">un_income_gap.dropna(subset=['incomeratio'], </p><p class="source-code">  inplace=True)</p><p class="source-code">num_cols = ['educratio','laborforcepartratio',</p><p class="source-code">  'humandevratio', 'genderinequality', </p><p class="source-code">  'maternalmortality', 'adolescentbirthrate', </p><p class="source-code">  'femaleperparliament', 'incomepercapita']</p><p class="source-code">gap_sub = un_income_gap[['incomeratio'] + num_cols]</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(gap_sub[num_cols],\</p><p class="source-code">  gap_sub[['incomeratio']], test_size=0.2, </p><p class="source-code">    random_state=0)</p></li>
</ol>
<p>Let’s start with a relatively simple decision tree– one without too many levels. A simple tree can easily be shown on one page.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>A decision tree example with interpretation</h2>
<p>Before we build<a id="_idIndexMarker772"/> our decision tree regressor, let’s just look at a quick example with maximum depth set to a low value. Decision trees are more difficult to explain and plot as the depth increases. Let’s get started:</p>
<ol>
<li value="1">We start by instantiating a decision tree regressor, limiting the depth to three, and requiring that each leaf has at least five observations. We create a pipeline that only preprocesses the data and passes the resulting NumPy array, <strong class="source-inline">X_train_imp</strong>, to the <strong class="source-inline">fit</strong> method of the decision tree regressor:<p class="source-code">dtreg_example = DecisionTreeRegressor(</p><p class="source-code">  min_samples_leaf=5,</p><p class="source-code">  max_depth=3)</p><p class="source-code">pipe0 = make_pipeline(OutlierTrans(3),</p><p class="source-code">  SimpleImputer(strategy="median"))</p><p class="source-code">X_train_imp = pipe0.fit_transform(X_train)</p><p class="source-code">dtreg_example.fit(X_train_imp, y_train)</p><p class="source-code">plot_tree(dtreg_example, </p><p class="source-code">  feature_names=X_train.columns,</p><p class="source-code">  label="root", fontsize=10)</p></li>
</ol>
<p>This generates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 9.7 – Decision tree example with a maximum depth of 3 " height="360" src="image/B17978_09_007.jpg" width="901"/>
</div>
</div>
<p class="figure-caption">Figure 9.7 – Decision tree example with a maximum depth of 3</p>
<p>We will not go over<a id="_idIndexMarker773"/> all nodes on this tree. We can get the general idea of how to interpret a decision tree regression plot by describing the path down to a couple of leaf nodes:</p>
<ul>
<li><strong class="bold">Interpreting the leaf node with labor force participation ratio &lt;= 0.307:</strong></li>
</ul>
<p>The root node split is based on labor force participation ratios less than or equal to 0.601. (Recall that the labor force participation ratio is the ratio of female participation rates to male participation rates.) 34 countries fall into that category. (True values for the split test are to the left. False values are to the right.) There is another split after that that is also based on the labor force participation ratio, this time with the split at 0.378. There are 13 countries with values less than or equal to that. Finally, we reach the leaf node furthest to the left for countries with a labor force participation ratio less than or equal to 0.307. Six countries have labor force participation ratios that low. Those six countries have an average income ratio of 0.197. Our decision tree regressor would then predict 0.197 for the income ratio for testing instances with labor force participation ratios less than or equal to 0.307.</p>
<ul>
<li><strong class="bold">Interpreting the leaf node with labor force participation ratio between 0.601 and 0.811, and humandevratio &lt;= 0.968:</strong></li>
</ul>
<p>There are<a id="_idIndexMarker774"/> 107 countries with labor force participation ratios greater than 0.601. This is shown on the right-hand side of the tree. There is another binary split when the labor force participation ratio is less than or equal to 0.811, which is split further based on the human development ratio being less than or equal to 0.968. This takes us to a leaf node that has 31 countries, those with human development ratio less than or equal to 0.968, and a labor force participation ratio less than or equal to 0.811, but greater than 0.601. The decision tree regressor would predict the average value for income ratio for those 31 countries, 0.556, for all testing instances with values for human development ratio and labor force participation ratio in those ranges.</p>
<p>Interestingly, we have not done any feature selection yet, but this first effort to build a decision tree model already suggests that income ratio can be predicted with just two features: <strong class="source-inline">laborforcepartratio</strong> and <strong class="source-inline">humandevratio</strong>.</p>
<p>Although the simplicity of this model makes it very easy to interpret, we have not done the work we need to do to find the best hyperparameters yet. Let’s do that next.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Building and interpreting our actual model</h2>
<p>Follow these steps:</p>
<ol>
<li value="1">First, we <a id="_idIndexMarker775"/>instantiate a new decision tree regressor and create a pipeline that uses it. We also create a dictionary for some of the<a id="_idIndexMarker776"/> hyperparameters– that is, for the maximum tree depth and the minimum number of samples (observations) for each leaf. Notice that we do not need to scale either our features or the target, as that is not necessary with a decision tree:<p class="source-code">dtreg = DecisionTreeRegressor()</p><p class="source-code">feature_sel = SelectFromModel(LinearRegression(),</p><p class="source-code">  threshold="0.8*mean")</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(3),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  feature_sel, dtreg)</p><p class="source-code">dtreg_params={</p><p class="source-code"> "decisiontreeregressor__max_depth": np.arange(2, 20),</p><p class="source-code"> "decisiontreeregressor__min_samples_leaf": np.arange(5, 11)</p><p class="source-code">}</p></li>
<li>Next, we must set up a randomized search based on the dictionary from the previous step. The best parameters for our decision tree are minimum samples of 5 and a maximum depth of 9:<p class="source-code">rs = RandomizedSearchCV(pipe1, dtreg_params, cv=4, n_iter=20,</p><p class="source-code">  scoring='neg_mean_absolute_error', random_state=1)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'decisiontreeregressor__min_samples_leaf': 5,</strong></p><p class="source-code"><strong class="bold"> 'decisiontreeregressor__max_depth': 9}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">-0.05268976358459662</strong></p></li>
</ol>
<p>As we discussed in the<a id="_idIndexMarker777"/> previous section, decision trees have many of the<a id="_idIndexMarker778"/> advantages of KNN for regression. They are easy to interpret and do not make many assumptions about the underlying data. However, decision trees can still work reasonably well with large datasets. A less important, but still helpful, advantage of decision trees is that they do not require feature scaling.</p>
<p>But decision trees do have high variance. It is often worth sacrificing the interpretability of decision trees for a related method, such as random forest, which can substantially reduce that variance. We discussed the random forest algorithm conceptually in the previous section. We’ll try it out with the income gap data in the next section.</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Random forest regression</h2>
<p>Recall that <a id="_idIndexMarker779"/>random forests can be thought of as decision trees with bagging; they improve bagging by reducing the correlation between samples. This sounds complicated but it is as easy to implement as decision trees are. Let’s take a look:</p>
<ol>
<li value="1">We will start by instantiating a random forest regressor and creating a dictionary for the hyperparameters. We will also create a pipeline for the pre-processing and the regressor:<p class="source-code">rfreg = RandomForestRegressor()</p><p class="source-code">rfreg_params = {</p><p class="source-code"> 'randomforestregressor__max_depth': np.arange(2, 20),</p><p class="source-code"> 'randomforestregressor__max_features': ['auto', 'sqrt'],</p><p class="source-code"> 'randomforestregressor__min_samples_leaf':  np.arange(5, 11)</p><p class="source-code">}</p><p class="source-code">pipe2 = make_pipeline(OutlierTrans(3), </p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  feature_sel, rfreg)</p></li>
<li>We will<a id="_idIndexMarker780"/> pass the pipeline and the hyperparameter dictionary to the <strong class="source-inline">RandomizedSearchCV</strong> object to run the grid search. There is a minor improvement in terms of the score:<p class="source-code">rs = RandomizedSearchCV(pipe2, rfreg_params, cv=4, n_iter=20,</p><p class="source-code">  scoring='neg_mean_absolute_error', random_state=1)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'randomforestregressor__min_samples_leaf': 5,</strong></p><p class="source-code"><strong class="bold"> 'randomforestregressor__max_features': 'auto',</strong></p><p class="source-code"><strong class="bold"> 'randomforestregressor__max_depth': 9}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">-0.04930503752638253</strong></p></li>
<li>Let’s take <a id="_idIndexMarker781"/>a look at the residuals:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">preddf = pd.DataFrame(pred, columns=['prediction'],</p><p class="source-code">  index=X_test.index).join(X_test).join(y_test)</p><p class="source-code">preddf['resid'] = preddf.incomegap-preddf.prediction</p><p class="source-code">plt.hist(preddf.resid, color="blue", bins=5)</p><p class="source-code">plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Histogram of Residuals for Income Gap")</p><p class="source-code">plt.xlabel("Residuals")</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.xlim()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 9.8 – Histogram of residuals for the random forest model on income ratio " height="443" src="image/B17978_09_008.jpg" width="559"/>
</div>
</div>
<p class="figure-caption">Figure 9.8 – Histogram of residuals for the random forest model on income ratio</p>
<ol>
<li value="4">Let’s also take<a id="_idIndexMarker782"/> a look at a scatterplot of residuals by predictions:<p class="source-code">plt.scatter(preddf.prediction, preddf.resid, color="blue")</p><p class="source-code">plt.axhline(0, color='red', linestyle='dashed', linewidth=1)</p><p class="source-code">plt.title("Scatterplot of Predictions and Residuals")</p><p class="source-code">plt.xlabel("Predicted Income Gap")</p><p class="source-code">plt.ylabel("Residuals")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 9.9 – Scatterplot of predictions by residuals for the random forest model on income ratio " height="444" src="image/B17978_09_009.jpg" width="590"/>
</div>
</div>
<p class="figure-caption">Figure 9.9 – Scatterplot of predictions by residuals for the random forest model on income ratio</p>
<ol>
<li value="5">Let’s take a<a id="_idIndexMarker783"/> look a closer look at the one significant outlier where we are seriously over-predicting:<p class="source-code">preddf.loc[np.abs(preddf.resid)&gt;=0.12,</p><p class="source-code">  ['incomeratio','prediction','resid',</p><p class="source-code">  'laborforcepartratio', 'humandevratio']].T</p><p class="source-code"><strong class="bold">country              Netherlands</strong></p><p class="source-code"><strong class="bold">incomeratio                 0.48</strong></p><p class="source-code"><strong class="bold">prediction                  0.66</strong></p><p class="source-code"><strong class="bold">resid                      -0.18</strong></p><p class="source-code"><strong class="bold">laborforcepartratio         0.83</strong></p><p class="source-code"><strong class="bold">humandevratio               0.95</strong></p></li>
</ol>
<p>We still have trouble with the Netherlands, but the fairly even distribution of residuals suggests that this is anomalous. It is actually good news, in terms of our ability to predict an income ratio for new instances, showing that our model is not working too hard to fit this unusual case.</p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Using gradient boosted regression</h1>
<p>We can <a id="_idIndexMarker784"/>sometimes improve upon random forest models by using gradient boosting instead. Similar to random forests, gradient boosting is an ensemble method that combines learners, typically trees. But unlike random forests, each tree is built to learn from the errors of previous trees. This can significantly improve our ability to model complexity.</p>
<p>Although gradient boosting is not particularly prone to overfitting, we have to be even more careful with our hyperparameter tuning than we have to be with random forest models. We can slow the learning rate, also known as shrinkage. We can also adjust the number of estimators (trees). The choice of learning rate influences the number of estimators needed. Typically, if we slow the learning rate, our model will require more estimators.</p>
<p>There are several tools for implementing gradient boosting. We will work with two of them: gradient boosted regression from scikit-learn and XGBoost.</p>
<p>We will work with data on housing prices in this section. We will try to predict housing prices in Kings County in Washington State in the United States, based on the characteristics of the home and of nearby homes.</p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on housing prices in Kings County can be downloaded by the public at <a href="https://www.kaggle.com/datasets/harlfoxem/housesalesprediction">https://www.kaggle.com/datasets/harlfoxem/housesalesprediction</a>. It has several bedrooms, bathrooms, and floors, the square feet of the home and the lot, the condition of the home, the square feet of the 15 nearest homes, and more as features.</p>
<p>Let’s start working on the model:</p>
<ol>
<li value="1">We will start by importing the modules we will need. The two new ones are <strong class="source-inline">GradientBoostingRegressor</strong> and <strong class="source-inline">XGBRegressor</strong> from XGBoost:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from sklearn.ensemble import GradientBoostingRegressor</p><p class="source-code">from xgboost import XGBRegressor</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.feature_selection import SelectFromModel</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from scipy.stats import randint</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Let’s<a id="_idIndexMarker785"/> load the housing data and look at a few instances:<p class="source-code">housing = pd.read_csv("data/kc_house_data.csv")</p><p class="source-code">housing.set_index('id', inplace=True)</p><p class="source-code">num_cols = ['bedrooms', 'bathrooms', 'sqft_living', </p><p class="source-code">  'sqft_lot', 'floors', 'view', 'condition', </p><p class="source-code">  'sqft_above', 'sqft_basement', 'yr_built', </p><p class="source-code">  'yr_renovated', 'sqft_living15', 'sqft_lot15']</p><p class="source-code">cat_cols = ['waterfront']</p><p class="source-code">housing[['price'] + num_cols + cat_cols].\</p><p class="source-code">  head(3).T</p><p class="source-code"><strong class="bold">id              7129300520  6414100192  5631500400</strong></p><p class="source-code"><strong class="bold">price           221,900     538,000     180,000</strong></p><p class="source-code"><strong class="bold">bedrooms        3           3           2</strong></p><p class="source-code"><strong class="bold">bathrooms       1           2           1</strong></p><p class="source-code"><strong class="bold">sqft_living     1,180       2,570       770</strong></p><p class="source-code"><strong class="bold">sqft_lot        5,650       7,242       10,000</strong></p><p class="source-code"><strong class="bold">floors          1           2           1</strong></p><p class="source-code"><strong class="bold">view            0           0           0</strong></p><p class="source-code"><strong class="bold">condition       3           3           3</strong></p><p class="source-code"><strong class="bold">sqft_above      1,180       2,170       770</strong></p><p class="source-code"><strong class="bold">sqft_basement   0           400         0</strong></p><p class="source-code"><strong class="bold">yr_built        1,955       1,951       1,933</strong></p><p class="source-code"><strong class="bold">yr_renovated    0           1,991       0</strong></p><p class="source-code"><strong class="bold">sqft_living15   1,340       1,690       2,720</strong></p><p class="source-code"><strong class="bold">sqft_lot15      5,650       7,639       8,062</strong></p><p class="source-code"><strong class="bold">waterfront      0           0           0</strong></p></li>
<li>We <a id="_idIndexMarker786"/>should also look at some descriptive statistics. We do not have any missing values. Our target variable, <strong class="source-inline">price</strong>, has some extreme values, not surprisingly. This will probably present a problem for modeling. We also need to handle some extreme values for our features:<p class="source-code">housing[['price'] + num_cols].\</p><p class="source-code">  agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                count   min      median    max</strong></p><p class="source-code"><strong class="bold">price          21,613   75,000   450,000   7,700,000</strong></p><p class="source-code"><strong class="bold">bedrooms       21,613   0        3         33</strong></p><p class="source-code"><strong class="bold">bathrooms      21,613   0        2         8</strong></p><p class="source-code"><strong class="bold">sqft_living    21,613   290      1,910     13,540</strong></p><p class="source-code"><strong class="bold">sqft_lot       21,613   520      7,618     1,651,359</strong></p><p class="source-code"><strong class="bold">floors         21,613   1        2         4</strong></p><p class="source-code"><strong class="bold">view           21,613   0        0         4</strong></p><p class="source-code"><strong class="bold">condition      21,613   1        3         5</strong></p><p class="source-code"><strong class="bold">sqft_above     21,613   290      1,560     9,410</strong></p><p class="source-code"><strong class="bold">sqft_basement  21,613   0        0         4,820</strong></p><p class="source-code"><strong class="bold">yr_built       21,613   1,900    1,975     2,015</strong></p><p class="source-code"><strong class="bold">yr_renovated   21,613   0        0         2,015</strong></p><p class="source-code"><strong class="bold">sqft_living15  21,613   399      1,840     6,210</strong></p><p class="source-code"><strong class="bold">sqft_lot15     21,613   651      7,620     871,200</strong></p></li>
<li>Let’s create a histogram of housing prices:<p class="source-code">plt.hist(housing.price/1000)</p><p class="source-code">plt.title("Housing Price (in thousands)")</p><p class="source-code">plt.xlabel('Price')</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker787"/>generates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 9.10 – Histogram of housing prices " height="446" src="image/B17978_09_010.jpg" width="592"/>
</div>
</div>
<p class="figure-caption">Figure 9.10 – Histogram of housing prices</p>
<ol>
<li value="5">We may have better luck if we use a log transformation of our target for our modeling, as we tried in <a href="B17978_04_ePub.xhtml#_idTextAnchor043"><em class="italic">Chapter 4</em></a><em class="italic">, Encoding, Transforming, and Scaling Features</em> with the COVID total cases data.<p class="source-code">housing['price_log'] = np.log(housing['price'])</p><p class="source-code">plt.hist(housing.price_log)</p><p class="source-code">plt.title("Housing Price Log")</p><p class="source-code">plt.xlabel('Price Log')</p><p class="source-code">plt.ylabel("Frequency")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker788"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="Figure 9.11 – Histogram of the housing price log " height="446" src="image/B17978_09_011.jpg" width="578"/>
</div>
</div>
<p class="figure-caption">Figure 9.11 – Histogram of the housing price log</p>
<ol>
<li value="6">This looks better. Let’s take a look at the skew and kurtosis for both the price and price log. The log looks like a big improvement:<p class="source-code">housing[['price','price_log']].agg(['kurtosis','skew'])</p><p class="source-code"><strong class="bold">                 price       price_log</strong></p><p class="source-code"><strong class="bold">kurtosis         34.59            0.69</strong></p><p class="source-code"><strong class="bold">skew              4.02            0.43</strong></p></li>
<li>We should also look at some correlations. The square feet of the living area, the square feet of the area above the ground level, the square feet of the living area of the nearest 15 homes, and the number of bathrooms are the features that are most correlated with price. The square feet of the living area and the square feet of the living area above ground level are very highly correlated. We will likely need to decide between one or the other in our model:<p class="source-code">corrmatrix = housing[['price_log'] + num_cols].\</p><p class="source-code">   corr(method="pearson")</p><p class="source-code">sns.heatmap(corrmatrix, </p><p class="source-code">  xticklabels=corrmatrix.columns,</p><p class="source-code">  yticklabels=corrmatrix.columns, cmap="coolwarm")</p><p class="source-code">plt.title('Heat Map of Correlation Matrix')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker789"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 9.12 – Correlation matrix of the housing features " height="463" src="image/B17978_09_012.jpg" width="602"/>
</div>
</div>
<p class="figure-caption">Figure 9.12 – Correlation matrix of the housing features</p>
<ol>
<li value="8">Next, we create training and testing DataFrames:<p class="source-code">target = housing[['price_log']]</p><p class="source-code">features = housing[num_cols + cat_cols]</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(features,\</p><p class="source-code">  target, test_size=0.2, random_state=0)</p></li>
<li>We also<a id="_idIndexMarker790"/> need to set up our column transformations. For all of the numeric features, which is every feature except for <strong class="source-inline">waterfront</strong>, we will check for extreme values and then scale the data:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(2),</p><p class="source-code">  SimpleImputer(strategy="median"),</p><p class="source-code">  MinMaxScaler())</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">    ("cat", cattrans, cat_cols)</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li> Now, we are ready to set up a pipeline for our pre-processing and our model. We will instantiate a <strong class="source-inline">GradientBoostingRegressor</strong> object and set up feature selection. We will also create a dictionary of hyperparameters to use in the randomized grid search we will do in the next step:<p class="source-code">gbr = GradientBoostingRegressor(random_state=0)</p><p class="source-code">feature_sel = SelectFromModel(LinearRegression(),</p><p class="source-code">  threshold="0.6*mean")</p><p class="source-code">gbr_params = {</p><p class="source-code"> 'gradientboostingregressor__learning_rate': uniform(loc=0.01, scale=0.5),</p><p class="source-code"> 'gradientboostingregressor__n_estimators': randint(500, 2000),</p><p class="source-code"> 'gradientboostingregressor__max_depth': randint(2, 20),</p><p class="source-code"> 'gradientboostingregressor__min_samples_leaf': randint(5, 11)</p><p class="source-code">}</p><p class="source-code">pipe1 = make_pipeline(coltrans, feature_sel, gbr)</p></li>
<li>Now, we <a id="_idIndexMarker791"/>are ready to run a randomized grid search. We get a pretty decent mean squared error score, given that the average for <strong class="source-inline">price_log</strong> is about 13:<p class="source-code">rs1 = RandomizedSearchCV(pipe1, gbr_params, cv=5, n_iter=20,</p><p class="source-code">  scoring='neg_mean_squared_error', random_state=0)</p><p class="source-code">rs1.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs1.best_params_</p><p class="source-code">{<strong class="bold">'gradientboostingregressor__learning_rate': 0.118275177212,</strong></p><p class="source-code"><strong class="bold"> 'gradientboostingregressor__max_depth': 2,</strong></p><p class="source-code"><strong class="bold"> 'gradientboostingregressor__min_samples_leaf': 5,</strong></p><p class="source-code"><strong class="bold"> 'gradientboostingregressor__n_estimators': 1577}</strong></p><p class="source-code">rs1.best_score_</p><p class="source-code"><strong class="bold">-0.10695077555421204</strong></p><p class="source-code">y_test.mean()</p><p class="source-code"><strong class="bold">price_log   13.03</strong></p><p class="source-code"><strong class="bold">dtype: float64</strong></p></li>
<li>Unfortunately, the<a id="_idIndexMarker792"/> mean fit time was quite long:<p class="source-code">print("fit time: %.3f, score time: %.3f"  %</p><p class="source-code">  (np.mean(rs1.cv_results_['mean_fit_time']),\</p><p class="source-code">  np.mean(rs1.cv_results_['mean_score_time'])))</p><p class="source-code"><strong class="bold">fit time: 35.695, score time: 0.152</strong></p></li>
<li>Let’s try XGBoost instead:<p class="source-code">xgb = XGBRegressor()</p><p class="source-code">xgb_params = {</p><p class="source-code"> 'xgbregressor__learning_rate': uniform(loc=0.01, scale=0.5),</p><p class="source-code"> 'xgbregressor__n_estimators': randint(500, 2000),</p><p class="source-code"> 'xgbregressor__max_depth': randint(2, 20)</p><p class="source-code">}</p><p class="source-code">pipe2 = make_pipeline(coltrans, feature_sel, xgb)</p></li>
<li>We do not get a<a id="_idIndexMarker793"/> better score, but the mean fit time and score time have improved dramatically:<p class="source-code">rs2 = RandomizedSearchCV(pipe2, xgb_params, cv=5, n_iter=20,</p><p class="source-code">  scoring='neg_mean_squared_error', random_state=0)</p><p class="source-code">rs2.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs2.best_params_</p><p class="source-code"><strong class="bold">{'xgbregressor__learning_rate': 0.019394900218177573,</strong></p><p class="source-code"><strong class="bold"> 'xgbregressor__max_depth': 7,</strong></p><p class="source-code"><strong class="bold"> 'xgbregressor__n_estimators': 1256}</strong></p><p class="source-code">rs2.best_score_</p><p class="source-code"><strong class="bold">-0.10574300757906044</strong></p><p class="source-code">print("fit time: %.3f, score time: %.3f"  %</p><p class="source-code">  (np.mean(rs2.cv_results_['mean_fit_time']),\</p><p class="source-code">  np.mean(rs2.cv_results_['mean_score_time'])))</p><p class="source-code"><strong class="bold">fit time: 3.931, score time: 0.046</strong></p></li>
</ol>
<p>XGBoost has <a id="_idIndexMarker794"/>become a very popular gradient boosting tool for many reasons, some of which you have seen in this example. It can produce very good results, very quickly, with little model specification. We do need to carefully tune our hyperparameters to get the preferred variance-bias trade-off, but this is also true with other gradient boosting tools, as we have seen.</p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>Summary</h1>
<p>In this chapter, we explored some of the most popular non-parametric regression algorithms: K-nearest neighbors, decision trees, and random forests. Models built with these algorithms can perform well, with a few limitations. We discussed some of the advantages and limitations of each of these techniques, including dimension and observation limits, as well as concerns about the time required for training, for KNN models. We discussed the key challenge with decision trees, which is high variance, but also how that can be addressed by a random forest model. We explored gradient boosted regression trees as well and discussed some of their advantages. We continued to improve our skills regarding hyperparameter tuning since each algorithm required a somewhat different strategy.</p>
<p>We discuss supervised learning algorithms where the target is categorical over the next few chapters, starting with perhaps the most familiar classification algorithm, logistic regression.</p>
</div>
</div>
</body></html>