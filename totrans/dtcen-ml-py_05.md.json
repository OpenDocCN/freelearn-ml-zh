["```py\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\n```", "```py\ndf = pd.read_csv('train_loan_prediction.csv')\ndf.head().T\n```", "```py\ncolumn_names = [cols for cols in df]\nprint(column_names)\n['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']\n```", "```py\nnum_underscore_present_columns = [cols for cols in column_names if '_' not in cols]\nnum_underscore_present_columns\n['Gender',\n 'Married',\n 'Dependents',\n 'Education',\n 'ApplicantIncome',\n 'CoapplicantIncome',\n 'LoanAmount']\n```", "```py\ncols_mappings = {}\nfor cols in num_underscore_present_columns:\n    uppercase_in_cols = [val.isupper() for val in cols]\n    num_uppercase_letters = sum(uppercase_in_cols)\n    cols_mappings[cols] = {\n        \"is_uppercase_letter\": uppercase_in_cols,\n        \"num_uppercase_letters\": num_uppercase_letters,\n        \"needs_underscore\": (num_uppercase_letters > 1)\n    }\n```", "```py\nfor key in cols_mappings.keys():\n    if cols_mappings[key]['needs_underscore']:\n        print()\n        print(f'{key} need the underscore at location ', cols_mappings[key]['is_uppercase_letter'].index(True, 1))\nApplicantIncome need the underscore at location  9\nCoapplicantIncome need the underscore at location  11\nLoanAmount need the underscore at location  4\n```", "```py\n'ApplicantIncome'[:9] + '_' + 'ApplicantIncome'[9:]\n'Applicant_Income'\n```", "```py\ncols_mappings = {}\nfor cols in num_underscore_present_columns:\n    uppercase_in_cols = [val.isupper() for val in cols]\n    num_uppercase_letters = sum(uppercase_in_cols)\n    if num_uppercase_letters > 1:\n        underscore_index = uppercase_in_cols.index(True, 1)\n        updated_column_name = cols[:underscore_index] + \"_\" + cols[underscore_index:]\n    else:\n        updated_column_name = cols\n    cols_mappings[cols] = {\n        \"is_uppercase_letter\": uppercase_in_cols,\n        \"num_uppercase_letters\": num_uppercase_letters,\n        \"needs_underscore\": (num_uppercase_letters > 1),\n        \"updated_column_name\": updated_column_name\n    }\n    if cols_mappings[cols]['needs_underscore']:\n        print(f\"{cols} will be renamed to {cols_mappings[cols]['updated_column_name']}\")\ncolumn_mappings = {key: cols_mappings[key][\"updated_column_name\"] for key in cols_mappings.keys()}\ncolumn_mappings\nApplicantIncome will be renamed to Applicant_Income\nCoapplicantIncome will be renamed to Coapplicant_Income\nLoanAmount will be renamed to Loan_Amount\n{'Gender': 'Gender',\n 'Married': 'Married',\n 'Dependents': 'Dependents',\n 'Education': 'Education',\n 'ApplicantIncome': 'Applicant_Income',\n 'CoapplicantIncome': 'Coapplicant_Income',\n 'LoanAmount': 'Loan_Amount'}\n```", "```py\ndf = df.rename(columns=column_mappings)\ncolumn_names = [cols for cols in df]\nprint(column_names)\n['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']\n```", "```py\nprint([cols.lower() for cols in df])\n['loan_id', 'gender', 'married', 'dependents', 'education', 'self_employed', 'applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term', 'credit_history', 'property_area', 'loan_status']\n```", "```py\ndf.columns = [cols.lower() for cols in df]\nprint(df.columns)\nIndex(['loan_id', 'gender', 'married', 'dependents', 'education',\n       'self_employed', 'applicant_income', 'coapplicant_income',\n       'loan_amount', 'loan_amount_term', 'credit_history', 'property_area',\n       'loan_status'],\n      dtype='object')\n```", "```py\nid_col = 'loan_id'\ntarget = 'loan_status'\ncat_cols = [cols for cols in df if df[cols].dtype == 'object' and cols not in [id_col, target]]\ncat_cols\n['gender',\n 'married',\n 'dependents',\n 'education',\n 'self_employed',\n 'property_area']\n```", "```py\nfor cols in cat_cols:\n    print(cols)\n    print(df[cols].unique())\n    print()\ngender\n['Male' 'Female' nan]\nmarried\n['No' 'Yes' nan]\ndependents\n['0' '1' '2' '3+' nan]\neducation\n['Graduate' 'Not Graduate']\nself_employed\n['No' 'Yes' nan]\nproperty_area\n['Urban' 'Rural' 'Semiurban']\n```", "```py\ndf_consistent = df.copy()\nfor col in cat_cols:\n    df_consistent[col] = df_consistent[col].apply(lambda val: val.lower() if isinstance(val, str) else val)\n    df_consistent[col] = df_consistent[col].apply(lambda val: val.replace(' ','_') if isinstance(val, str) else val)\nfor cols in cat_cols:\n    print(cols)\n    print(df_consistent[cols].unique())\n    print()\ngender\n['male' 'female' nan]\nmarried\n['no' 'yes' nan]\ndependents\n['0' '1' '2' '3+' nan]\neducation\n['graduate' 'not_graduate']\nself_employed\n['no' 'yes' nan]\nproperty_area\n['urban' 'rural' 'semiurban']\n```", "```py\ndf_consistent.dependents = df_consistent.dependents.apply(lambda val: float(val.replace('+','')) if isinstance(val, str) else float(val))\n```", "```py\nfor cols in ['married', 'self_employed']:\n    df_consistent[cols] = df_consistent[cols].map({\"yes\": 1, \"no\": 0})\ndf_consistent.education = df_consistent.education.map({\n    'graduate': 1,\n    'not_graduate': 0\n})\ndf_consistent.gender = df_consistent.gender.map({\n    'male': 1,\n    'female': 0\n})\nfor cols in cat_cols:\n    print(cols)\n    print(df_consistent[cols].unique())\n    print()\ngender\n[ 1.  0\\. nan]\nmarried\n[ 0.  1\\. nan]\ndependents\n[ 0.  1.  2.  3\\. nan]\neducation\n[1 0]\nself_employed\n[ 0.  1\\. nan]\nproperty_area\n['urban' 'rural' 'semiurban']\n```", "```py\ndef make_data_consistent(df, cat_cols) -> pd.DataFrame:\n    \"\"\"Function to make data consistent and meaningful\"\"\"\n    df = df.copy()\n    for col in cat_cols:\n        df[col] = df[col].apply(lambda val: val.lower() if isinstance(val, str) else val)\n        df[col] = df[col].apply(lambda val: val.replace(' ','_') if isinstance(val, str) else val)\n    df['dependents'] = df['dependents'].apply(lambda val: float(val.replace('+','')) if isinstance(val, str) else float(val))\n    for cols in ['married', 'self_employed']:\n        df[cols] = df[cols].map({\"yes\": 1, \"no\": 0})\n    df['education'] = df['education'].map({\n        'graduate': 1,\n        'not_graduate': 0\n    })\n    df['gender'] = df['gender'].map({\n        'male': 1,\n        'female': 0\n    })\n    return df\ndf_consistent = df.copy()\ndf_consistent = make_data_consistent(df=df_consistent, cat_cols=cat_cols)\nfor cols in cat_cols:\n    print(cols)\n    print(df_consistent[cols].unique())\n    print()\ngender\n[ 1.  0\\. nan]\nmarried\n[ 0.  1\\. nan]\ndependents\n[ 0.  1.  2.  3\\. nan]\neducation\n[1 0]\nself_employed\n[ 0.  1\\. nan]\nproperty_area\n['urban' 'rural' 'semiurban']\n```", "```py\ndf.loan_id.nunique(), df.shape[0]\n(614, 614)\n```", "```py\ndf[['applicant_income', 'coapplicant_income', 'loan_amount']].value_counts().reset_index(name='count')\n     applicant_income  coapplicant_income  loan_amount  count\n0                4333              2451.0        110.0      2\n1                 150              1800.0        135.0      1\n2                4887                 0.0        133.0      1\n3                4758                 0.0        158.0      1\n4                4817               923.0        120.0      1\n..                ...                 ...          ...    ...\n586              3166              2985.0        132.0      1\n587              3167                 0.0         74.0      1\n588              3167              2283.0        154.0      1\n589              3167              4000.0        180.0      1\n590             81000                 0.0        360.0      1\n[591 rows x 4 columns]\n```", "```py\ndf[(df.applicant_income == 4333) & (df.coapplicant_income == 2451) & (df.loan_amount == 110)]\n      loan_id  gender married dependents education self_employed  \\\n328  LP002086  Female     Yes          0  Graduate            No\n469  LP002505    Male     Yes          0  Graduate            No\n     applicant_income  coapplicant_income  loan_amount  loan_amount_term  \\\n328           4333              2451.0        110.0             360.0\n469          4333              2451.0        110.0             360.0\n     credit_history property_area loan_status\n328             1.0         Urban           N\n469             1.0         Urban           N\n```", "```py\ndf.gender.value_counts(normalize=True)\nMale      0.813644\nFemale    0.186356\nName: gender, dtype: float64\n```", "```py\nremaining_rows = df_consistent.dropna(axis=0).shape[0]\ntotal_records = df_consistent.shape[0]\nperc_dropped = ((total_records - remaining_rows)/total_records)*100\nprint(\"By dropping all missing data, only {:,} records will be left out of {:,}, a reduction by {:,.3f}%\".format(remaining_rows, total_records, perc_dropped))\nBy dropping all missing data, only 480 records will be left out of 614, a reduction by 21.824%\n```", "```py\nid_col = 'loan_id'\ntarget = 'loan_status'\nfeature_cols = [cols for cols in df_consistent if cols not in [id_col, target]]\nbinary_cols = [cols for cols in feature_cols if df_consistent[cols].nunique() == 2]\ncat_cols = [cols for cols in feature_cols if (df_consistent[cols].dtype == 'object' or df_consistent[cols].nunique() <= 15)]\nnum_cols = [cols for cols in feature_cols if cols not in cat_cols]\ncat_cols\n['gender',\n 'married',\n 'dependents',\n 'education',\n 'self_employed',\n 'loan_amount_term',\n 'credit_history',\n 'property_area']\nbinary_cols\n['gender', 'married', 'education', 'self_employed', 'credit_history']\nnum_cols\n['applicant_income', 'coapplicant_income', 'loan_amount']\n```", "```py\ndf_consistent.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 614 entries, 0 to 613\nData columns (total 13 columns):\n #   Column              Non-Null Count  Dtype\n---  ------              --------------  -----\n 0   loan_id             614 non-null    object\n 1   gender              601 non-null    float64\n 2   married             611 non-null    float64\n 3   dependents          599 non-null    float64\n 4   education           614 non-null    int64\n 5   self_employed       582 non-null    float64\n 6   applicant_income    614 non-null    int64\n 7   coapplicant_income  614 non-null    float64\n 8   loan_amount         592 non-null    float64\n 9   loan_amount_term    600 non-null    float64\n 10  credit_history      564 non-null    float64\n 11  property_area       614 non-null    object\n 12  loan_status         614 non-null    object\ndtypes: float64(8), int64(2), object(3)\nmemory usage: 62.5+ KB\n```", "```py\ndf_consistent.isnull().sum()\nloan_id                0\ngender                13\nmarried                3\ndependents            15\neducation              0\nself_employed         32\napplicant_income       0\ncoapplicant_income     0\nloan_amount           22\nloan_amount_term      14\ncredit_history        50\nproperty_area          0\nloan_status            0\ndtype: int64\n```", "```py\ndef missing_data_percentage(df: pd.DataFrame):\n    \"\"\"Function to print percentage of missing values\"\"\"\n    df = df.copy()\n    missing_data = df.isnull().sum()\n    total_records = df.shape[0]\n    perc_missing = round((missing_data/total_records)*100, 3)\n    missing_df = pd.DataFrame(data={'columm_name':perc_missing.index, 'perc_missing':perc_missing.values})\n    return missing_df\nmissing_data_percentage(df_consistent[feature_cols]).sort_values(by='perc_missing', ascending=False)\n           columm_name  perc_missing\n9       credit_history         8.143\n4        self_employed         5.212\n7          loan_amount         3.583\n2           dependents         2.443\n8     loan_amount_term         2.280\n0               gender         2.117\n1              married         0.489\n3            education         0.000\n5     applicant_income         0.000\n6   coapplicant_income         0.000\n10       property_area         0.000\n```", "```py\nmsno.matrix(df_consistent[feature_cols], figsize=(35, 15))\n<AxesSubplot: >\n```", "```py\nmsno.heatmap(df_consistent[feature_cols], labels=True)\n```", "```py\nmissing_cols = [cols for cols in feature_cols if df_consistent[cols].isnull().sum() > 0]\nmsno.dendrogram(df_consistent[missing_cols])\n```", "```py\ncat_missing = [cols for cols in cat_cols if df_consistent[cols].isnull().sum() > 0]\ndef cat_missing_association_with_outcome(data, missing_data_column, outcome):\n    \"\"\"Function to plot missing association of categorical varibles with outcome\"\"\"\n    df = data.copy()\n    df[f\"{missing_data_column}_is_missing\"] = df[missing_data_column].isnull().astype(int)\n    df.groupby([outcome]).agg({f\"{missing_data_column}_is_missing\": 'mean'}).plot.bar()\nfor cols in cat_missing:\n    cat_missing_association_with_outcome(df_consistent, cols, target)\n```", "```py\nnum_missing = [cols for cols in num_cols if df_consistent[cols].isnull().sum() > 0]\ndef num_missing_association_with_outcome(data, missing_data_column, outcome):\n    \"\"\"Function to plot missing association of categorical varibles with outcome\"\"\"\n    df = data.copy()\n    df[f\"{missing_data_column}_is_missing\"] = df[missing_data_column].isnull().astype(int)\n    df.groupby([outcome]).agg({f\"{missing_data_column}_is_missing\": 'mean'}).plot.bar()\nfor cols in num_missing:\n    num_missing_association_with_outcome(df, cols, target)\n```", "```py\ndf_consistent.loan_amount.plot.kde(color='orange', label='loan_amount', legend=True)\ndf_consistent.loan_amount.fillna(value=df.loan_amount.median()).plot.kde(color='b', label='loan_amount_imputed', alpha=0.5, figsize=(9,7), legend=True)\n```", "```py\nround(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).std(),2)\n(85.59, 84.11)\n```", "```py\ndf_consistent[num_cols].corr()\n                    applicant_income  coapplicant_income  loan_amount\napplicant_income            1.000000           -0.116605     0.570909\ncoapplicant_income         -0.116605            1.000000     0.188619\nloan_amount                 0.570909            0.188619     1.000000\n```", "```py\nobservation = df_consistent[df_consistent.loan_amount.isnull()]\nimputed_values = []\nfor idx in observation.index:\n    seed = int(observation.loc[idx,['applicant_income']])\n    imputed_value = df_consistent['loan_amount'].dropna().sample(1, random_state=seed)\n    imputed_values.append(imputed_value)\ndf_consistent.loc[df_consistent['loan_amount'].isnull(),'loan_amount_random_imputed']=imputed_values\ndf_consistent.loc[df['loan_amount'].isnull()==False,'loan_amount_random_imputed']=df_consistent[df_consistent['loan_amount'].isnull()==False]['loan_amount'].values\n```", "```py\ndf_consistent.loan_amount.plot.kde(color='orange', label='loan_amount', legend=True, linewidth=2)\ndf_consistent.loan_amount_random_imputed.plot.kde(color='g', label='loan_amount_random_imputed', legend=True, linewidth=2)\ndf_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).plot.kde(color='b', label='loan_amount_median_imputed', linewidth=1, alpha=0.5, figsize=(9,7), legend=True)\n<AxesSubplot: ylabel='Density'>\n```", "```py\nround(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).std(),2)\n(85.59, 85.57, 84.11)\n```", "```py\ndf_consistent['loan_amount_median_imputed'] = df_consistent['loan_amount'].fillna(value=df_consistent['loan_amount'].median())\ndf_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'applicant_income']].corr()\n```", "```py\ndf_consistent.credit_history.value_counts(normalize=True)\n1.0    0.842199\n0.0    0.157801\nName: credit_history, dtype: float64\ndf_consistent.credit_history.fillna(value=df_consistent.credit_history.mode()[0]).value_counts(normalize=True)\n1.0    0.855049\n0.0    0.144951\nName: credit_history, dtype: float64\n```", "```py\nfrom sklearn.impute import KNNImputer\nfrom feature_engine.wrappers import SklearnTransformerWrapper\nfrom sklearn.preprocessing import StandardScaler\n```", "```py\nnum_cols = [cols for cols in df_consistent if df_consistent[cols].nunique() > 15 and cols not in [id_col, target] and not cols.endswith('imputed')]\n```", "```py\ndf_num = df_consistent[num_cols].copy()\ndf_num.head()\n   applicant_income  coapplicant_income  loan_amount\n0              5849                 0.0          NaN\n1              4583              1508.0        128.0\n2              3000                 0.0         66.0\n3              2583              2358.0        120.0\n4              6000                 0.0        141.0\n```", "```py\ndef scale_data(df, scaler, columns):\n    \"\"\"Function to scale the data\"\"\"\n    df_scaled = df.copy()\n    if columns:\n        df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n    else:\n        columns = [cols for cols in df_scaled]\n        df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n    return df_scaled, scaler\n```", "```py\nscaler = StandardScaler()\ndf_scaled, scaler = scale_data(df_num, scaler=scaler, columns=num_cols)\n```", "```py\nknn_imputer = SklearnTransformerWrapper(\n    transformer = KNNImputer(n_neighbors=10, weights='distance'),\n    variables = num_cols\n)\n```", "```py\ndf_imputed = knn_imputer.fit_transform(df_scaled)\n```", "```py\ndf_imputed = pd.DataFrame(columns=num_cols, data=scaler.inverse_transform(df_imputed))\ndf_imputed.head()\n   applicant_income  coapplicant_income  loan_amount\n0            5849.0                 0.0   149.666345\n1            4583.0              1508.0   128.000000\n2            3000.0                 0.0    66.000000\n3            2583.0              2358.0   120.000000\n4            6000.0                 0.0   141.000000\n```", "```py\ndf_imputed['loan_amount'].plot.kde(color='orange', label='loan_amount_knn_imputed',linewidth=2, legend=True)\ndf_consistent['loan_amount'].plot.kde(color='b', label='loan_amount', legend=True, linewidth=2, figsize=(9,7), alpha=0.5)\n```", "```py\nround(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount_median_imputed.std(),2), round(df_imputed.loan_amount.std(),2)\n(85.59, 85.57, 84.11, 85.59)\n```", "```py\ndf_consistent['loan_amount_knn_imputed'] = df_imputed.loan_amount\ndf_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'loan_amount_knn_imputed', 'applicant_income']].corr()\n```", "```py\nfrom sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom feature_engine.encoding import OneHotEncoder\n```", "```py\nohe_cols = [cols for cols in cat_cols if df_consistent[cols].dtype == 'object']\nohe_cols\n['property_area']\n```", "```py\ndf_ohe_encoded = df_consistent.copy()\nohe = OneHotEncoder(variables=ohe_cols)\ndf_ohe_encoded = ohe.fit_transform(df_ohe_encoded)\n```", "```py\ndf_ohe_encoded[[cols for cols in df_ohe_encoded if 'property_area' in cols]].head()\n   property_area_urban  property_area_rural  property_area_semiurban\n0                    1                    0                        0\n1                    0                    1                        0\n2                    1                    0                        0\n3                    1                    0                        0\n4                    1                    0                        0\n```", "```py\ncat_cols = [cols for cols in df_ohe_encoded if df_ohe_encoded[cols].nunique() <= 15 and cols not in [id_col, target]]\ncat_cols\n['gender',\n 'married',\n 'dependents',\n 'education',\n 'self_employed',\n 'loan_amount_term',\n 'credit_history',\n 'property_area_urban',\n 'property_area_rural',\n 'property_area_semiurban']\n```", "```py\nmiss_forest_classifier = IterativeImputer(\n    estimator=ExtraTreesClassifier(n_estimators=100,\n                                   random_state=1,\n                                   bootstrap=True,\n                                   n_jobs=-1),\n    max_iter=10,\n    random_state=1,\n    add_indicator=True,\n    initial_strategy='median')\ndf_cat_imputed = miss_forest_classifier.fit_transform(df_ohe_encoded[cat_cols])\n```", "```py\ndf_cat_imputed = pd.DataFrame(\n    columns=miss_forest_classifier.get_feature_names_out(),\n    data=df_cat_imputed,\n    index=df_ohe_encoded.index)\n```", "```py\nfor cols in cat_cols:\n    print(cols)\n    print(df_cat_imputed[cols].unique())\n    print()\ngender\n[1\\. 0.]\nmarried\n[0\\. 1.]\ndependents\n[0\\. 1\\. 2\\. 3.]\neducation\n[1\\. 0.]\nself_employed\n[0\\. 1.]\nloan_amount_term\n[360\\. 120\\. 240\\. 180.  60\\. 300\\. 480.  36.  84.  12.]\ncredit_history\n[1\\. 0.]\nproperty_area_urban\n[1\\. 0.]\nproperty_area_rural\n[0\\. 1.]\nproperty_area_semiurban\n[0\\. 1.]\n```", "```py\nnum_cols = [cols for cols in df_consistent if cols not in df_cat_imputed and cols not in [id_col, target] + ohe_cols\n            and not cols.endswith(\"imputed\")]\ndf_combined = pd.concat([df_consistent[num_cols], df_cat_imputed], axis=1)\nfeature_cols = [cols for cols in df_combined]\nfeature_cols\n['applicant_income',\n 'coapplicant_income',\n 'loan_amount',\n 'gender',\n 'married',\n 'dependents',\n 'education',\n 'self_employed',\n 'loan_amount_term',\n 'credit_history',\n 'property_area_urban',\n 'property_area_rural',\n 'property_area_semiurban',\n 'missingindicator_gender',\n 'missingindicator_married',\n 'missingindicator_dependents',\n 'missingindicator_self_employed',\n 'missingindicator_loan_amount_term',\n 'missingindicator_credit_history']\n```", "```py\nmiss_forest_regressor = IterativeImputer(\n    estimator=ExtraTreesRegressor(n_estimators=100,\n                                  random_state=1,\n                                  bootstrap=True,\n                                  n_jobs=-1),\n    max_iter=10,\n    random_state=1,\n    add_indicator=True,\n    initial_strategy='median')\ndf_imputed = miss_forest_regressor.fit_transform(df_combined[feature_cols])\n```", "```py\ndf_imputed\ndf_imputed = pd.DataFrame(data=df_imputed,\n                           columns=miss_forest_regressor.get_feature_names_out(),\n                           index=df_combined.index)\n```", "```py\ndf_imputed.isnull().sum()\napplicant_income                     0\ncoapplicant_income                   0\nloan_amount                          0\ngender                               0\nmarried                              0\ndependents                           0\neducation                            0\nself_employed                        0\nloan_amount_term                     0\ncredit_history                       0\nproperty_area_urban                  0\nproperty_area_rural                  0\nproperty_area_semiurban              0\nmissingindicator_gender              0\nmissingindicator_married             0\nmissingindicator_dependents          0\nmissingindicator_self_employed       0\nmissingindicator_loan_amount_term    0\nmissingindicator_credit_history      0\nmissingindicator_loan_amount         0\ndtype: int64\n```", "```py\ndf_imputed['loan_amount'].plot.kde(color='orange', label='loan_amount_miss_forest_imputed',linewidth=2, legend=True)\ndf_consistent['loan_amount'].plot.kde(color='b', label='loan_amount', legend=True, linewidth=2, figsize=(9,7), alpha=0.5)\n<AxesSubplot: ylabel='Density'>\n```", "```py\nround(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount_median_imputed.std(),2), round(df_imputed.loan_amount.std(),2)\n(85.59, 85.57, 84.11, 85.41)\n```", "```py\ndf_consistent['loan_amount_miss_forest_imputed'] = df_imputed.loan_amount\ndf_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'loan_amount_miss_forest_imputed', 'applicant_income']].corr()\n```", "```py\ndf_consistent.drop([cols for cols in df_consistent if cols.endswith('imputed')], axis=1, inplace=True)\n```", "```py\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom typing import List\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n```", "```py\nfeature_cols = [cols for cols in df_consistent if cols not in [target, id_col]]\nX_train, X_test, y_train, y_test = train_test_split(df_consistent[feature_cols],\n                                                    df_consistent[target].map({'Y':1, 'N':0}),\n                                                    test_size=0.1,\n                                                    random_state=1,\n                                                    stratify=df_consistent[target].map({'Y':1, 'N':0}))\nfeature_cols\n['gender',\n 'married',\n 'dependents',\n 'education',\n 'self_employed',\n 'applicant_income',\n 'coapplicant_income',\n 'loan_amount',\n 'loan_amount_term',\n 'credit_history',\n 'property_area']\n```", "```py\ncat_cols = [cols for cols in X_train if X_train[cols].nunique() <= 15]\nnum_cols = [cols for cols in X_train if cols not in cat_cols]\n```", "```py\ndef miss_forest_categorical_transformer():\n    \"\"\"Function to define categorical pipeline\"\"\"\n    cat_transformer = Pipeline(\n        steps=[\n            (\"one_hot_encoding\",\n             OneHotEncoder(variables=ohe_cols)\n            ),\n            (\"miss_forest_classifier\",\n             IterativeImputer(\n                 estimator=ExtraTreesClassifier(\n                     n_estimators=100,\n                     random_state=1,\n                     bootstrap=True,\n                     n_jobs=-1),\n                max_iter=10,\n                random_state=1,\n                initial_strategy='median',\n                add_indicator=True)\n            )\n        ]\n    )\n    return cat_transformer\n```", "```py\ndef miss_forest_numerical_transformer():\n    \"\"\"Function to define numerical pipeline\"\"\"\n    num_transformer = Pipeline(\n        steps=[\n            (\"miss_forest\",\n             IterativeImputer(\n                estimator=ExtraTreesRegressor(n_estimators=100,\n                                              random_state=1,\n                                              bootstrap=True,\n                                              n_jobs=-1),\n                max_iter=10,\n                random_state=1,\n                initial_strategy='median',\n                add_indicator=True)\n            )\n        ]\n    )\n    return num_transformer\n```", "```py\ncat_transformer = miss_forest_categorical_transformer()\nnum_transformer = miss_forest_numerical_transformer()\nX_train_cat_imputed = cat_transformer.fit_transform(X_train[cat_cols])\nX_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])\nX_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,\n                                      columns=cat_transformer.get_feature_names_out(),\n                                      index=X_train.index)\nX_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,\n                                     columns=cat_transformer.get_feature_names_out(),\n                                     index=X_test.index)\nX_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train[num_cols]], axis=1)\nX_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)\nX_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)\nX_test_imputed = num_transformer.transform(X_test_cat_imputed_df)\nX_train_transformed = pd.DataFrame(data=X_train_imputed,\n                                   columns=num_transformer.get_feature_names_out(),\n                                   index=X_train.index)\nX_test_transformed = pd.DataFrame(data=X_test_imputed,\n                                  columns=num_transformer.get_feature_names_out(),\n                                  index=X_test.index)\n```", "```py\ny_train.mean(), y_test.mean()\n(0.6865942028985508, 0.6935483870967742)\n```", "```py\nd_param_grid = {\n    'max_features': [None, 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8,10,20],\n    'min_samples_leaf' : [1,3,5,8,10,12,15],\n    'min_samples_split': [2,6,10,16,20,24,30],\n    'criterion' : ['gini', 'entropy'],\n    'random_state' : [1],\n    'class_weight' : ['balanced']\n}\nd_clf = DecisionTreeClassifier()\n```", "```py\ndef train_custom_classifier(X_train, y_train, X_test, y_test, clf, params):\n    \"\"\"Function to train the decision tree classifier and return some metrics\"\"\"\n    d_clf_cv = GridSearchCV(estimator=d_clf, param_grid=d_param_grid, cv=10, scoring='roc_auc')\n    d_clf_cv.fit(X_train_transformed, y_train)\n    print(\"Decision tree optimised\")\n    d_best_params = d_clf_cv.best_params_\n    print(f\"Getting the best params which are {d_best_params}\")\n    model = DecisionTreeClassifier(**d_best_params)\n    model.fit(X_train_transformed, y_train)\n    training_predictions_prob = model.predict_proba(X_train_transformed)\n    testing_predictions_prob = model.predict_proba(X_test_transformed)\n    training_predictions = model.predict(X_train_transformed)\n    testing_predictions = model.predict(X_test_transformed)\n    training_roc_auc = roc_auc_score(y_train, training_predictions_prob[:,1])\n    testing_roc_auc = roc_auc_score(y_test, testing_predictions_prob[:,1])\n    training_acc = accuracy_score(y_train, training_predictions)\n    testing_acc = accuracy_score(y_test, testing_predictions)\n    print(f\"Training roc is {training_roc_auc}, and testing roc is {testing_roc_auc} \\n \\\n            training accuracy is {training_acc}, testing_acc as {testing_acc}\")\n    return model, testing_predictions, training_roc_auc, testing_roc_auc, training_acc, testing_acc\n```", "```py\nmodel, test_predictions, train_roc, test_roc, train_acc, test_acc  = train_custom_classifier(\n    X_train=X_train_transformed,\n    y_train=y_train,\n    X_test=X_test_transformed,\n    y_test=y_test,\n    clf=d_clf,\n    params=d_param_grid\n)\nDecision tree optimised\nGetting the best params which are {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 8, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 30, 'random_state': 1}\nTraining roc is 0.8763326063416048, and testing roc is 0.7858017135862914\n             training accuracy is 0.8152173913043478, testing_acc as 0.7903225806451613\n```", "```py\ncm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\n```", "```py\ncat_transformer = Pipeline(\n    steps=[\n        (\"one_hot_encoding\",\n         OneHotEncoder(variables=ohe_cols)\n        )\n    ]\n)\nimpute_transformer = Pipeline(\n    steps=[\n        (\"simple_imputer\",\n         SimpleImputer(strategy='median',\n                       add_indicator=True)\n        )\n    ]\n)\nX_train_ohe = cat_transformer.fit_transform(X_train)\nX_test_ohe = cat_transformer.transform(X_test)\nX_train_imputed = impute_transformer.fit_transform(X_train_ohe)\nX_test_imputed = impute_transformer.transform(X_test_ohe)\nX_train_transformed = pd.DataFrame(data=X_train_imputed,\n                                   columns=impute_transformer.get_feature_names_out(),\n                                   index=X_train.index)\nX_test_transformed = pd.DataFrame(data=X_test_imputed,\n                                  columns=impute_transformer.get_feature_names_out(),\n                                  index=X_test.index)\n```", "```py\nmodel, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(\n    X_train=X_train_transformed,\n    y_train=y_train,\n    X_test=X_test_transformed,\n    y_test=y_test,\n    clf=d_clf,\n    params=d_param_grid\n)\n```", "```py\ncm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\n```", "```py\ndata_cutoff_points = np.linspace(start=0.1, stop=1, num=10)\ndata_cutoff_points\narray([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\\. ])\n```", "```py\nscores = []\nfor cutoff in data_cutoff_points:\n    if cutoff < 1.0:\n        X_train_subset, X_train_rem, y_train_subset, y_train_rem = train_test_split(X_train,\n                     y_train,\n                          random_state=1,\n                             train_size=cutoff,\n                        stratify=y_train)\n    else:\n        X_train_subset = X_train.copy()\n        y_train_subset = y_train.copy()\n    print(f\"Model will be trained on {X_train_subset.shape[0]} rows out of {X_train.shape[0]}\")\n    cat_transformer = miss_forest_categorical_transformer()\n    num_transformer = miss_forest_numerical_transformer()\n    X_train_cat_imputed = cat_transformer.fit_transform(X_train_subset[cat_cols])\n    X_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])\n    X_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,\n                                          columns=cat_transformer.get_feature_names_out(),\n                                          index=X_train_subset.index)\n    X_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,\n                                         columns=cat_transformer.get_feature_names_out(),\n                                         index=X_test.index)\n    X_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train_subset[num_cols]], axis=1)\n    X_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)\n    X_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)\n    X_test_imputed = num_transformer.transform(X_test_cat_imputed_df)\n    X_train_transformed = pd.DataFrame(data=X_train_imputed,\n                                       columns=num_transformer.get_feature_names_out(),\n                                       index=X_train_subset.index)\n    X_test_transformed = pd.DataFrame(data=X_test_imputed,\n                                      columns=num_transformer.get_feature_names_out(),\n                                      index=X_test.index)\n    model, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(\n        X_train=X_train_transformed,\n        y_train=y_train_subset,\n        X_test=X_test_transformed,\n        y_test=y_test,\n        clf=d_clf,\n        params=d_param_grid)\n    scores.append((cutoff, train_roc, test_roc, train_acc, test_acc))\nModel will be trained on 55 rows out of 552\nTraining roc is 0.9094427244582044, and testing roc is 0.5917992656058751\n             training accuracy is 0.7454545454545455, testing_acc as 0.5806451612903226\nModel will be trained on 110 rows out of 552\nTraining roc is 0.901702786377709, and testing roc is 0.7552019583843328\n             training accuracy is 0.7272727272727273, testing_acc as 0.6290322580645161\nModel will be trained on 165 rows out of 552\nTraining roc is 0.8986555479918311, and testing roc is 0.7099143206854346\n             training accuracy is 0.7696969696969697, testing_acc as 0.5967741935483871\nModel will be trained on 220 rows out of 552\nTraining roc is 0.8207601497264613, and testing roc is 0.8084455324357405\n             training accuracy is 0.8318181818181818, testing_acc as 0.8064516129032258\nModel will be trained on 276 rows out of 552\nTraining roc is 0.8728942407103326, and testing roc is 0.7906976744186047\n             training accuracy is 0.822463768115942, testing_acc as 0.7419354838709677\nModel will be trained on 331 rows out of 552\nTraining roc is 0.9344501863774991, and testing roc is 0.7753977968176254\n             training accuracy is 0.8368580060422961, testing_acc as 0.7419354838709677\nModel will be trained on 386 rows out of 552\nTraining roc is 0.8977545610478715, and testing roc is 0.7184822521419829\n             training accuracy is 0.7849740932642487, testing_acc as 0.6612903225806451\nModel will be trained on 441 rows out of 552\nTraining roc is 0.8954656335198737, and testing roc is 0.7429620563035496\n             training accuracy is 0.81859410430839, testing_acc as 0.7258064516129032\nModel will be trained on 496 rows out of 552\nTraining roc is 0.9102355500898685, and testing roc is 0.7441860465116278\n             training accuracy is 0.8266129032258065, testing_acc as 0.7258064516129032\nModel will be trained on 552 rows out of 552\nTraining roc is 0.8763326063416048, and testing roc is 0.7858017135862914\n             training accuracy is 0.8152173913043478, testing_acc as 0.7903225806451613\n```", "```py\ndf = pd.DataFrame(data=scores, columns=['data_size', 'training_roc', 'testing_roc', \"training_acc\", \"testing_acc\"])\n```", "```py\nplt.plot(df.data_size, df.training_roc, label='training_roc')\nplt.plot(df.data_size, df.testing_roc, label='testing_roc')\nplt.xlabel(\"Data Size\")\nplt.ylabel(\"ROC\")\nplt.title(\"Error Analysis\")\nplt.legend()\n```", "```py\nplt.plot(df.data_size, df.training_acc, label='training_acc')\nplt.plot(df.data_size, df.testing_acc, label='testing_acc')\nplt.xlabel(\"Data Size\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Error Analysis\")\nplt.legend()\n```", "```py\nincome_variables = ['applicant_income', 'coapplicant_income']\nloan_variable = ['loan_amount']\nloan_term_variable = ['loan_amount_term']\n```", "```py\nfrom feature_engine.creation.math_features import MathFeatures\nfrom feature_engine.creation.relative_features import RelativeFeatures\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom feature_engine.selection import DropFeatures\n```", "```py\nclass MultiplyColumns(BaseEstimator, TransformerMixin):\n    \"\"\"Custom pipeline class to multiply columns passed in a DataFrame with a value\"\"\"\n    def __init__(self, multiply_by=1, variables=None):\n        self.multiply_by = multiply_by\n        self.variables = variables\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        if self.variables:\n            X[self.variables] = X[self.variables] * self.multiply_by\n        return X\n```", "```py\ncat_transformer = miss_forest_categorical_transformer()\nnum_transformer = miss_forest_numerical_transformer()\nfeature_transformer = Pipeline(\n    steps=[\n        (\"multiply_by_thousand\",\n         MultiplyColumns(\n             multiply_by=1000,\n             variables=loan_variable\n         )\n        ),\n        (\"add_columns\",\n         MathFeatures(\n             variables=income_variables,\n             func='sum'\n         )\n        ),\n        (\"income_to_loan_ratio\",\n         RelativeFeatures(variables=[f\"sum_{income_variables[0]}_{income_variables[1]}\"],\n                          reference=loan_variable,\n                          func=[\"div\"]\n                         )\n        ),\n        (\"emi\",\n         RelativeFeatures(variables=loan_variable,\n                          reference=loan_term_variable,\n                          func=[\"div\"])\n        ),\n        (\"drop_features\",\n         DropFeatures(features_to_drop=income_variables\n          ))\n    ]\n)\n```", "```py\nX_train_cat_imputed = cat_transformer.fit_transform(X_train[cat_cols])\nX_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])\nX_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,\n                                      columns=cat_transformer.get_feature_names_out(),\n                                      index=X_train.index)\nX_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,\n                                     columns=cat_transformer.get_feature_names_out(),\n                                     index=X_test.index)\nX_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train[num_cols]], axis=1)\nX_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)\n```", "```py\nX_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)\nX_test_imputed = num_transformer.transform(X_test_cat_imputed_df)\nX_train_imputed_df = pd.DataFrame(data=X_train_imputed,\n                                   columns=num_transformer.get_feature_names_out(),\n                                   index=X_train.index)\nX_test_imputed_df = pd.DataFrame(data=X_test_imputed,\n                                  columns=num_transformer.get_feature_names_out(),\n                                  index=X_test.index)\n```", "```py\nX_train_transformed = feature_transformer.fit_transform(X_train_imputed_df)\nX_test_transformed = feature_transformer.transform(X_test_imputed_df)\n```", "```py\nmodel, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(\n    X_train=X_train_transformed,\n    y_train=y_train,\n    X_test=X_test_transformed,\n    y_test=y_test,\n    clf=d_clf,\n    params=d_param_grid)\nTraining roc is 0.8465996614150411, and testing roc is 0.8188494492044063\n             training accuracy is 0.8206521739130435, testing_acc as 0.8225806451612904\n```", "```py\ncm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\n```", "```py\nnp.random.seed(1)\ndata = {\n    \"id\": np.linspace(start=1, stop=10, num=10, dtype=int),\n    \"population\" : np.random.randint(low=1000, high=100000, size=10),\n    \"property_area\": [\"urban\"]*4 + [\"semi_urban\"]*5 + [\"rural\"]*1\n}\ndf = pd.DataFrame(data=data)\n```", "```py\ndf.head()\n   id  population property_area\n0   1       99539         urban\n1   2       78708         urban\n2   3        6192         urban\n3   4       99047         urban\n4   5       51057    semi_urban\n```", "```py\ndf.property_area.value_counts(normalize=True)\nsemi_urban    0.5\nurban         0.4\nrural         0.1\nName: property_area, dtype: float64\n```", "```py\ndf.property_area.isin(['rural', 'urban']) == False\n0    False\n1    False\n2    False\n3    False\n4     True\n5     True\n6     True\n7     True\n8     True\n9    False\nName: property_area, dtype: bool\n```", "```py\nsum(df.property_area.isin(['rural', 'urban']) == False) / df.shape[0]\n0.5\n```", "```py\ndf['true_property_area'] = df.population.apply(lambda value: 'rural' if value <= 20000 else 'urban')\n```", "```py\ndf[['true_property_area', 'property_area', 'population']]\n  true_property_area property_area  population\n0              urban         urban       99539\n1              urban         urban       78708\n2              rural         urban        6192\n3              urban         urban       99047\n4              urban    semi_urban       51057\n5              urban    semi_urban       74349\n6              urban    semi_urban       22440\n7              urban    semi_urban       99448\n8              urban    semi_urban       21609\n9              urban         rural       50100\n```", "```py\nsum(df.property_area == df.true_property_area) / df.shape[0]\n0.3\n```", "```py\naccuracy_score(y_pred=df.property_area, y_true=df.true_property_area)\n0.3\n```", "```py\nfrom datetime import datetime, timedelta\nimport warnings\n```", "```py\nnumdays = 100\nbase = datetime.today()\ndate_list = [base - timedelta(days=day) for day in range(numdays)] # Subracting values from 1 to 100 from todays date\n```", "```py\n[date.date().strftime('%Y-%m-%d') for date in date_list[0:10]]\n['2023-02-04',\n '2023-02-03',\n '2023-02-02',\n '2023-02-01',\n '2023-01-31',\n '2023-01-30',\n '2023-01-29',\n '2023-01-28',\n '2023-01-27',\n '2023-01-26']\n```", "```py\nnp.random.seed(1)\ndata = {\n    \"id\": np.linspace(start=1, stop=100, num=100, dtype=int),\n    \"population\" : np.random.randint(low=1000, high=100000, size=100),\n    \"property_area\": [\"urban\"]*40 + [\"semi_urban\"]*50 + [\"rural\"]*10,\n    \"date_loaded\": date_list\n}\ndf = pd.DataFrame(data=data)\n```", "```py\ndf.head()\n   id  population property_area                date_loaded\n0   1       99539         urban 2023-02-04 11:18:46.771142\n1   2       78708         urban 2023-02-03 11:18:46.771142\n2   3        6192         urban 2023-02-02 11:18:46.771142\n3   4       99047         urban 2023-02-01 11:18:46.771142\n4   5       51057         urban 2023-01-31 11:18:46.771142\n```", "```py\n(datetime.now() - df.date_loaded.max()).days\n0\n```", "```py\ndef check_data_recency_days(df: pd.DataFrame, loaded_at_column: str, warning_at: int=5, error_at: int=10):\n    \"\"\"Function to detect data freshness\"\"\"\n    df = df.copy()\n    days_since_data_refreshed = (datetime.now() - df[loaded_at_column].max()).days\n    if days_since_data_refreshed < warning_at:\n        print(f\"Data is fresh and is {days_since_data_refreshed} days old\")\n    elif error_at > days_since_data_refreshed >= warning_at:\n        warnings.warn(f\"Warning: Data is not fresh, and is {days_since_data_refreshed} days old\")\n    else:\n        raise ValueError(f\"Date provided is too old and stale, please contact source provider: {days_since_data_refreshed} days old\")\n```", "```py\ncheck_data_recency_days(df, \"date_loaded\")\nData is fresh and is 0 days old\n```", "```py\ndf_filter_6_days = df[df.date_loaded <= (datetime.today() -  timedelta(days=6))]\ndf_filter_12_days = df[df.date_loaded <= (datetime.today() -  timedelta(days=12))]\n```", "```py\ncheck_data_recency_days(df_filter_6_days, \"date_loaded\")\n/var/folders/6f/p7312_7n4nq5hp35rfymms1h0000gn/T/ipykernel_5374/1750573000.py:11: UserWarning: Warning: Data is not fresh, and is 6 days old\n  warnings.warn(f\"Warning: Data is not fresh, and is {days_since_data_refreshed} days old\")\n```", "```py\nimport alibi\nfrom alibi_detect.cd import TabularDrift\n```", "```py\ncd = TabularDrift(x_ref=X_train_transformed.to_numpy(), p_val=.05 )\n```", "```py\npreds = cd.predict(X_test_transformed.to_numpy())\nlabels = ['No', 'Yes']\nprint('Drift: {}'.format(labels[preds['data']['is_drift']]))\nDrift: No\n```", "```py\n X_test_transformed['loan_amount'] = X_test_transformed['loan_amount']*1.5\nX_test_transformed['sum_applicant_income_coapplicant_income'] = X_test_transformed['sum_applicant_income_coapplicant_income']*1.2\nX_test_transformed.sum_applicant_income_coapplicant_income_div_loan_amount = X_test_transformed.sum_applicant_income_coapplicant_income/X_test_transformed.loan_amount\nX_test_transformed.loan_amount_div_loan_amount_term = X_test_transformed.loan_amount/X_test_transformed.loan_amount_term\n```", "```py\npreds = cd.predict(X_test_transformed.to_numpy())\nlabels = ['No', 'Yes']\nprint('Drift: {}'.format(labels[preds['data']['is_drift']]))\nDrift: Yes\n```", "```py\ntesting_predictions_prob = model.predict_proba(X_test_transformed)\ntesting_predictions = model.predict(X_test_transformed)\ntesting_roc_auc = roc_auc_score(y_test, testing_predictions_prob[:,1])\ntesting_acc = accuracy_score(y_test, testing_predictions)\nprint(f\"Testing roc is {testing_roc_auc} and testing_acc as {testing_acc}\")\nTesting roc is 0.747858017135863 and testing_acc as 0.6935483870967742\n```"]