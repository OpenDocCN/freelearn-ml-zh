<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch14" class="calibre6"/>Chapter 14. Learning from Examples</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Recognizing faces using nearest neighbors of local binary patterns</li><li class="listitem">Finding objects and faces with a cascade of Haar features</li><li class="listitem">Detecting objects and people with Support Vector Machines and histograms of oriented gradients</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch14lvl1sec80" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">Machine learning is nowadays, very often used to solve difficult machine vision problems. In fact, it is a rich field of research encompassing many important concepts that would deserve a complete cookbook by itself. This chapter surveys some of the main machine learning techniques and explains how these can be deployed in computer vision systems using OpenCV.</p><p class="calibre8">At the core of machine learning is the development of computer systems that can learn how to react to data inputs by themselves. Instead of being explicitly programmed, machine learning systems automatically adapt and evolve when examples of desired behaviors are presented to them. Once a successful training phase is completed, it is expected that the trained system will output the correct response to new unseen queries.</p><p class="calibre8">Machine learning can solve many types of problems; our focus here will be on classification problems. Formally, in order to build a classifier that can recognize instances of a specific class of concepts, this one must be trained with a large set of annotated samples. In a 2-class problem, this set will be made of <strong class="calibre15">positive samples</strong> representing instances of the class to be learned, and of <strong class="calibre15">negative samples</strong> made of counter-examples of instances not belonging to the class of interest. From these observations, a <strong class="calibre15">decision function</strong> predicting the correct class of any input instances has to be learned.</p><p class="calibre8">In computer vision, those samples are images (or video segments). The first thing to do is therefore find a representation that will ideally describe the content of each image in a compact and distinctive way. One simplistic representation could be to use a fixed-size thumbnail image. The row-by-row succession of the pixels of this thumbnail image forms a vector that can then be used as a training sample presented to a machine learning algorithm. Other alternative and probably more effective representations can also be used. The recipes of this chapter describe different image representations and introduce some well-known machine learning algorithms. We should emphasize that we will not be able to cover in detail, all the theoretical aspects of the different machine learning techniques discussed in the recipes; our objective is rather to present the main principles governing their functioning.</p></div></div>
<div><div><div><div><h1 class="title1"><a id="ch14lvl1sec81" class="calibre6"/>Recognizing faces using nearest neighbors of local binary patterns</h1></div></div></div><p class="calibre8">Our first exploration of machine learning techniques will start with what is probably the simplest approach, namely <strong class="calibre15">nearest neighbor classification</strong>. We will also present the local binary pattern feature, a popular representation encoding the textural patterns and contours of an image in a contrast independent way.</p><p class="calibre8">Our illustrative example will concern the face recognition problem. This is a very challenging problem that has been the object of numerous researches over the past 20 years. The basic solution we present here is one of the face recognition methods implemented in OpenCV. You will quickly realize that this solution is not very robust and works only under very favorable conditions. Nevertheless, this approach constitutes an excellent introduction to machine learning and to the face recognition problem.</p><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec241" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The OpenCV library proposes a number of face recognition methods implemented as a subclass of the generic <code class="literal">cv::face::FaceRecognizer</code>. In this recipe, we will have a look at the <code class="literal">cv::face::LBPHFaceRecognizer</code> class, which is interesting to us because it is based on a simple but often very effective classification approach, the nearest neighbor classifier. Moreover, the image representation it uses is built from the <strong class="calibre15">local binary pattern</strong> feature (<strong class="calibre15">LBP</strong>) which is a very popular way of describing image patterns.</p><p class="calibre8">In order to create an instance of the <code class="literal">cv::face::LBPHFaceRecognizer</code>, its static <code class="literal">create</code> method is called:</p><pre class="programlisting">    cv::Ptr&lt;cv::face::FaceRecognizer&gt; recognizer =
           cv::face::createLBPHFaceRecognizer(1, // radius of LBP pattern 
                   8,       // the number of neighboring pixels to consider 
                   8, 8,    // grid size 
                   200.8);  // minimum distance to nearest neighbor 
</pre><p class="calibre8">As will be explained in the next section, the first two arguments provided serve to describe the characteristic of the LBP feature to be used. The next step is to feed the recognizer with a number of reference face images. This is done by providing two vectors, one containing the face images and the other one containing the associated labels. Each label is an arbitrary integer value identifying a particular individual. The idea is to train the recognizer by showing it different images of each of the people to be recognized. As you may imagine, the more representative images you provide, the better the chances that the correct person will be identified. In our very simplistic example, we simply provide two images of two reference persons. The <code class="literal">train</code> method is the one to call:</p><pre class="programlisting">    // vectors of reference image and their labels 
    std::vector&lt;cv::Mat&gt; referenceImages; 
    std::vector&lt;int&gt; labels; 
    // open the reference images 
    referenceImages.push_back(cv::imread("face0_1.png",
                              cv::IMREAD_GRAYSCALE)); 
    labels.push_back(0); // person 0 
    referenceImages.push_back(cv::imread("face0_2.png",
                              cv::IMREAD_GRAYSCALE)); 
    labels.push_back(0); // person 0 
    referenceImages.push_back(cv::imread("face1_1.png",
                              cv::IMREAD_GRAYSCALE)); 
    labels.push_back(1); // person 1 
    referenceImages.push_back(cv::imread("face1_2.png",
                              cv::IMREAD_GRAYSCALE)); 
    labels.push_back(1); // person 1 
 
    // train the recognizer by computing the LBPHs 
    recognizer-&gt;train(referenceImages, labels); 
</pre><p class="calibre8">The images used are below, with the top row being images of person <code class="literal">0</code> and the second row images of person <code class="literal">1</code>:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The quality of these reference images is also very important. In addition, it would be a good idea to have them normalized such as to have the main facial features at standardized locations. For example, having the tip of the nose located in the middle of the image, and the two eyes horizontally aligned at a specific image row. Facial feature detection methods exist that can be used to automatically normalize face images this way. This was not done in our example, and the robustness of the recognizer will suffer from this. Nevertheless, this one is ready to be used, an input image can be provided, and it will try to predict the label to which this face image corresponds:</p><pre class="programlisting">    // predict the label of this image 
    recognizer-&gt;predict(inputImage,      // face image  
                        predictedLabel,  // predicted label of this image  
                        confidence);     // confidence of the prediction 
</pre><p class="calibre8">Our input image is the following:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_002.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Not only does the recognizer return the predicted label, but it also returns a confidence score. In the case of the <code class="literal">cv::face::LBPHFaceRecognizer</code>, the lower this confidence value is, the more confident is the recognizer of its prediction. Here, we obtain a correct label prediction (<code class="literal">1</code>) with a confidence value of <code class="literal">90.3</code>.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec242" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In order to understand the functioning of the face recognition approach presented in this recipe, we need to explain its two main components: the image representation used and the classification method that is applied.</p><p class="calibre8">As its name indicates, the <code class="literal">cv::face::LBPHFaceRecognizer</code> algorithm makes use of the LBP feature. This is a contrast independent way of describing image patterns present in an image. It is a local representation that transforms every pixel into a binary representation encoding the pattern of image intensities found in a neighborhood. To achieve this goal, a simple rule is applied; a local pixel is compared to each of its selected neighbors; if its value is greater than that of its neighbor, then a <code class="literal">0</code> is assigned to the corresponding bit position, if not, then a <code class="literal">1</code> is assigned. In its simplest and most common form, each pixel is compared to its <code class="literal">8</code> immediate neighbors, which generates an 8-bit pattern. For example, let's consider the following local pattern:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_14_19.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Applying the described rule generates the following binary values:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_14_20.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Taking as initial position, the top left pixel and moving clockwise, the central pixel will be replaced by the binary sequence <code class="literal">11011000</code>. Generating a complete 8-bit LBP image is then easily achieved by looping over all pixels of an image to produce all corresponding LBP bytes. This is accomplished by the following function:</p><pre class="programlisting">    //compute the Local Binary Patterns of a gray-level image 
    void lbp(const cv::Mat &amp;image, cv::Mat &amp;result) { 
 
      result.create(image.size(), CV_8U); // allocate if necessary 
 
      for (int j = 1; j&lt;image.rows - 1; j++) { 
        //for all rows (except first and last) 
 
        // pointers to the input rows 
        const uchar* previous = image.ptr&lt;const uchar&gt;(j - 1);    
        const uchar* current  = image.ptr&lt;const uchar&gt;(j);       
        const uchar* next     = image.ptr&lt;const uchar&gt;(j + 1);   
        uchar* output = result.ptr&lt;uchar&gt;(j);        //output row 
 
        for (int i = 1; i&lt;image.cols - 1; i++) { 
 
          // compose local binary pattern 
          *output =  previous[i - 1] &gt; current[i] ? 1 : 0; 
          *output |= previous[i] &gt; current[i] ?     2 : 0; 
          *output |= previous[i + 1] &gt; current[i] ? 4 : 0; 
          *output |= current[i - 1] &gt; current[i] ?  8 : 0; 
          *output |= current[i + 1] &gt; current[i] ? 16 : 0; 
          *output |= next[i - 1] &gt; current[i] ?    32 : 0; 
          *output |= next[i] &gt; current[i] ?        64 : 0; 
          *output |= next[i + 1] &gt; current[i] ?   128 : 0; 
          output++; // next pixel 
        } 
      } 
      // Set the unprocess pixels to 0 
      result.row(0).setTo(cv::Scalar(0)); 
      result.row(result.rows - 1).setTo(cv::Scalar(0)); 
      result.col(0).setTo(cv::Scalar(0)); 
      result.col(result.cols - 1).setTo(cv::Scalar(0)); 
    } 
</pre><p class="calibre8">The body of the loop compares each pixel with its <code class="literal">8</code> neighbors and the bit values are assigned through simple bit shifts. With the following image:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_14_003.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">An LBP image is obtained and can be displayed as a gray-level image:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_14_004.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This gray-level representation is not really interpretable, but it simply illustrates the encoding process that occurred.</p><p class="calibre8">Returning to our <code class="literal">cv::face::LBPHFaceRecognizer</code> class, it can be seen that the first two parameters of its <code class="literal">create</code> method specify the size (radius in pixels) and dimension (number of pixels along the circle, possibly applying interpolation) of the neighborhood to be considered. Once the LBP image is generated, the image is divided into a grid. The size of this grid is specified as the third parameter of the <code class="literal">create</code> method. For each block of this grid, a histogram of LBP values is constructed. A global image representation is finally obtained by concatenating the bin counts of all these histograms into one large vector. With an <code class="literal">8×8</code> grid, the set of computed 256-bin histograms then forms a 16384-dimensional vector.</p><p class="calibre8">The <code class="literal">train</code> method of the <code class="literal">cv::face::LBPHFaceRecognizer</code> class therefore generates this long vector for each of the provided reference images. Each face image can then be seen as a point in a very high dimensional space. When a new image is submitted to the recognizer through its <code class="literal">predict</code> method, the closest reference point to this image is found. The label associated with this point is therefore the predicted label and the confidence value will be the computed distance. This is the principle that defines a nearest neighbor classifier. One more ingredient is generally added. If the nearest neighbor of the input point is too far from it, then this could mean that this point in fact does not belong to any of the reference classes. How far away must this point be to be considered as an outlier? This is specified by the fourth parameter of the <code class="literal">create</code> method of the <code class="literal">cv::face::LBPHFaceRecognizer</code> class.</p><p class="calibre8">As you can see, this is a very simple idea and it turns out to be very effective when the different classes generate distinct clouds of points in the representational space. Another benefit of this approach is that the method implicitly handles multiple classes, as it simply reads the predicted class from its nearest neighbors. The main drawback is its computational cost. Finding the nearest neighbor in such a large space, possibly composed of many reference points, can take time. Storing all these reference points is also costly in memory.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec243" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article by <em class="calibre16">T. Ahonen</em>, <em class="calibre16">A. Hadid</em> and <em class="calibre16">M. Pietikainen</em>, <em class="calibre16">Face description with Local Binary Patterns: Application to Face Recognition</em> in IEEE transaction on<em class="calibre16"> Pattern Analysis and Machine Intelligence</em>, 2006 describes the use of LBP for face recognition</li><li class="listitem">The article by <em class="calibre16">B. Froba</em> and <em class="calibre16">A. Ernst</em>, <em class="calibre16">Face detection with the modified census transform</em> in IEEE conference on<em class="calibre16"> </em><em class="calibre16">Automatic Face and Gesture Recognition</em>, 2004 proposes a variant of the LBP features</li><li class="listitem">The article by <em class="calibre16">M. Uricar</em>, <em class="calibre16">V. Franc</em> and <em class="calibre16">V. Hlavac</em>, <em class="calibre16">Detector of Facial Landmarks Learned by the Structured Output SVM</em> in International Conference on <em class="calibre16">Computer Vision Theory and Applications</em>, 2012 describes a facial feature detector based on the SVMs discussed in the last recipe of this chapter</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch14lvl1sec82" class="calibre6"/>Finding objects and faces with a cascade of Haar features</h1></div></div></div><p class="calibre8">We learned in the previous recipe, some of the basic concepts of machine learning. We showed how a classifier can be built by collecting samples of the different classes of interest. However, for the approach that was considered in this previous recipe, training a classifier simply consists of storing all the samples' representations. From there, the label of any new instance can be predicted by looking at the closest (nearest neighbor) labeled point. For most machine learning methods, training is rather an iterative process during which machinery is built by looping over the samples. Performance of the classifier thus produced gradually improves as more samples are presented. Learning eventually stops when a certain performance criterion is reached or when no more improvements can be obtained by considering the current training dataset. This recipe will present a machine learning algorithm that follows this procedure, the <strong class="calibre15">cascade of boosted classifiers</strong>.</p><p class="calibre8">But before we look at this classifier, we will first turn our attention to the Haar feature image representation. We indeed learned that a good representation is an essential ingredient in the production of a robust classifier. LBPs, as described in the previous recipe, <em class="calibre16">Recognizing faces using nearest neighbors of local binary patterns</em>, constitute one possible choice; the next section describes another popular representation.</p><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec244" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">The first step in the generation of a classifier is to assemble a (preferably) large collection of image samples showing different instances of the classes of objects to be identified. The way these samples are represented has been shown to have an important impact on the performance of the classifier that is to be built from them. Pixel-level representations are generally considered to be too low-level to robustly describe the intrinsic characteristics of each class of objects. Representations that can describe, at various scales, the distinctive patterns present in an image are preferable. This is the objective of the <strong class="calibre15">Haar features</strong> also sometimes called Haar-like features because they derive from the Haar transform basis functions.</p><p class="calibre8">The Haar features define small rectangular areas of pixels, these later being compared through simple subtractions. Three different configurations are generally considered, namely the 2-rectangle, 3-rectangle, and 4-rectangle features</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/image_14_005.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These features can be of any size and applied on any area of the image to be represented. For example, here are two Haar features applied on a face image:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/image_14_006.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Building a Haar representation consists of selecting a number of Haar features of given types, sizes, and locations and applying them on images. The specific set of values obtained from the chosen set of Haar features constitutes the image representation. The challenge is then to determine which set of features to select. Indeed, to distinguish one class of objects from another, some Haar features must be more relevant than others. For example, in the case of the class of face images, applying a 3-rectangle Haar feature between the eyes (as shown in the figure above) could be a good idea as we expect all face images to consistently produce a high value in this case. Obviously, since there exist hundreds of thousands of possible Haar features, it would certainly be difficult to manually make a good selection. We are then looking for a machine learning method that would select the most relevant features for a given class of objects.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec245" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">In this recipe, we will learn how we can build, using OpenCV, a <strong class="calibre15">boosted cascade of features</strong> to produce a 2-class classifier. But before we do, let's explain the terminology that is used here. A 2-class classifier is one that can identify the instances of one class (for example, face images) from the rest (for example, images that do not contain faces). We therefore have in this case the <strong class="calibre15">positive samples</strong> (that is, face images) and the <strong class="calibre15">negative samples</strong> (that is, non-face images), these latter are also called the background images. The classifier of this recipe will be made of a cascade of simple classifiers that will be sequentially applied. Each stage of the cascade will make a quick decision about rejecting or not rejecting the object shown based on the values obtained for a small subset of features. This cascade is boosted in the sense that each stage improves (boosts) the performance of the previous ones by making more accurate decisions. The main advantage of this approach is that the early stages of the cascade are composed of simple tests that can then quickly reject instances that certainly do not belong to the class of interest. These early rejections make the cascade classifier quick, because when searching for a class of objects by scanning an image, most sub-windows to be tested will not belong to the class of interest. This way, only few windows will have to pass through all stages before being accepted or rejected.</p><p class="calibre8">In order to train a boosted classifier cascade for a specific class, OpenCV offers a software tool that will perform all the required operations. When you install the library, you should have two executable modules created and located in the appropriate <code class="literal">bin</code> directory, these are <code class="literal">opencv_createsamples.exe</code> and <code class="literal">opencv_traincascade.exe</code>. Make sure your system <code class="literal">PATH</code> points to this directory so that you can execute these tools from anywhere.</p><p class="calibre8">When training a classifier, the first thing to do is to collect the samples. The positive ones are made of images showing instances of the target class. In our simple example, we decided to train a classifier to recognize stop signs. Here are the few positive samples we have collected:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_007.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The list of the positive samples to be used must be specified in a text file that we have, here, named <code class="literal">stop.txt</code>. It contains image filenames and bounding box coordinates:</p><pre class="programlisting">    stop00.png 1 0 0 64 64 
    stop01.png 1 0 0 64 64 
    stop02.png 1 0 0 64 64 
    stop03.png 1 0 0 64 64 
    stop04.png 1 0 0 64 64 
    stop05.png 1 0 0 64 64 
    stop06.png 1 0 0 64 64 
    stop07.png 1 0 0 64 64 
</pre><p class="calibre8">The first number after the filename is the number of positive samples visible in the image. Next is the upper left coordinate of the bounding box containing this positive sample and finally its width and height. In our case, the positive samples have already been extracted from their original images, this is why we have always one sample per file and upper-left coordinates at <code class="literal">(0,0)</code>. Once this file is available, you can then create the positive sample file by running the extractor tool.</p><pre class="programlisting">
<strong class="calibre15">    opencv_createsamples -info stop.txt -vec stop.vec -w 24 -h 24 -num 8</strong>
</pre><p class="calibre8">This will create an output file <code class="literal">stop.vec</code> that will contain all the positive samples specified in the input text file. Note that we made the sample size smaller (<code class="literal">24×24</code>) than the original size (<code class="literal">64×64</code>). The extractor tool resizes all samples to the specified size. Usually, Haar features work better with smaller templates, but this is something that has to be validated on a case-by-case basis.</p><p class="calibre8">The negative samples are simply background images containing no instances of the class of interest (no stop signs in our case). But these images should show a good variety of what the classifier is expected to see. These negative images could be of any size, the training tool will extract random negative samples from them. Here is one example of a background image we wish to use.</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_008.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Once the positive and negative sample sets are in place, the classifier cascade is ready to be trained. Calling the tool is done as follows:</p><pre class="programlisting">     opencv_traincascade  -data classifier -vec stop.vec   
                     -bg neg.txt -numPos 9  -numNeg 20 
                     -numStages 20 -minHitRate 0.95  
                     -maxFalseAlarmRate 0.5 -w 24 -h 24 
</pre><p class="calibre8">The parameters used here will be explained in the next section. Note that this training process can take a very long time; in some complex cases with thousands of samples, it can even take days to execute. As it runs, the cascade trainer will print out performance reports each time the training of a stage is completed. In particular, the classifier will tell you what the current <strong class="calibre15">hit rate</strong> (<strong class="calibre15">HR</strong>) is; this is the percentage of positive samples that are currently accepted by the cascade (that is, correctly recognized as positive instances, they are also called the <strong class="calibre15">true positives</strong>). You want this number to be as close as possible to <code class="literal">1.0</code>. It will also give you the current <strong class="calibre15">false alarm rate</strong> (<strong class="calibre15">FA</strong>) which is the number of tested negative samples that are wrongly classified as positive instances (also called the <strong class="calibre15">false positives</strong>). You want this number to be as close as possible to <code class="literal">0.0</code>. These numbers are reported for each of the features introduced in each stage.</p><p class="calibre8">Our simple example took only few seconds. The structure of the classifier produced is described in an XML file that results from the training phase. The classifier is then ready to be used! You can submit any sample to it and it will tell you if it thinks that it is a positive or a negative one.</p><p class="calibre8">In our example, we trained our classifier with <code class="literal">24×24</code> images but in general, what you want is to find out if there are any instances of your class of objects somewhere in an image (of any size). To achieve this objective, you simply have to scan the input image and extract all possible windows of the sample size. If your classifier is accurate enough, only the windows that contain the seek objects will return a positive detection. But this works as long as the visible positive samples have the appropriate size. To detect instances at multiple scales, you then have to build a pyramid of images by reducing the size of the original image by a certain factor at each level of the pyramid. This way, bigger objects will eventually fit the trained sample size as we go down the pyramid. This is a long process, but the good news is that OpenCV provides a class that implements this process. Its use is pretty straightforward. First you construct the classifier by loading the appropriate XML file:</p><pre class="programlisting">    cv::CascadeClassifier cascade; 
    if (!cascade.load("stopSamples/classifier/cascade.xml")) { 
      std::cout &lt;&lt; "Error when loading the cascade classfier!"  
                &lt;&lt; std::endl;   
      return -1; 
    } 
</pre><p class="calibre8">Then, you call the <code class="literal">detection</code> method with an input image:</p><pre class="programlisting">    cascade.detectMultiScale(inputImage, // input image 
              detections,           // detection results 
              1.1,                  // scale reduction factor 
              2,                 // number of required neighbor detections 
              0,                    // flags (not used) 
              cv::Size(48, 48),     // minimum object size to be detected 
              cv::Size(128, 128));  // maximum object size to be detected 
</pre><p class="calibre8">The result is provided as a vector of <code class="literal">cv::Rect</code> instances. To visualize the detection results, you just have to draw these rectangles on your input image:</p><pre class="programlisting">    for (int i = 0; i &lt; detections.size(); i++) 
     cv::rectangle(inputImage, detections[i],  
                   cv::Scalar(255, 255, 255), 2); 
</pre><p class="calibre8">When our classifier is tested on an image, here is the result we obtained:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_009.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec246" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In the previous section we explained how it is possible to build an OpenCV cascade of classifiers using positive and negative samples of a class of objects. We will now overview the basic steps of the learning algorithm used to train this cascade. Our cascade has been trained using the Haar features that were described in the introductory section of this recipe but, as we will see, any other simple feature can be used to build a boosted cascade. As the theory and principles of boosted learning are pretty complex, we will not cover all aspects in this recipe; interested readers should refer to the articles listed in the last section.</p><p class="calibre8">Let's first restate that there are two core ideas behind the cascade of boosted classifiers. The first one is that a strong classifier can be built by combining together several weak classifiers (that is, those based on simple features). Secondly, because in machine vision, negative instances are found much more frequently than the positive ones, effective classification can be performed in stages. The early stages make quick rejection of obvious negative instances, and more refined decisions can be made at later stages for more difficult samples. Based on these two ideas, we now describe the boosted cascade learning algorithm. Our explanations are based on the variant of boosting called <strong class="calibre15">AdaBoost</strong>
<strong class="calibre15">,</strong> which is the one most often used. Our description will also allow us to explain some of the parameters used in the <code class="literal">opencv_traincascade</code> tool.</p><p class="calibre8">In this recipe, we use the Haar features in order to build our weak classifier. When one Haar feature is applied (of given type, size, and location), a value is obtained. A simple classifier is then obtained by finding the threshold value that would best classify the negative and positive class instances based on this feature value. To find this optimal threshold, we have at our disposal, a number of positive and negative samples (the number of positive and negative samples to be used at this step by <code class="literal">opencv_traincascade</code> is given by the <code class="literal">-numPos</code> and <code class="literal">-numNeg</code> parameters). Since we have a large number of possible Haar features, we examine all of them and select the one that best classifies our sample set. Obviously, this very basic classifier will make errors (that is, misclassify several samples); this is why we need to build several of these classifiers. These classifiers are added iteratively, each time searching for the new Haar feature giving the best classification. But since, at each iteration, we want to focus on the samples that are currently misclassified, the classification performance is measured by giving a higher weight to the misclassified samples. A set of simple classifiers is thus obtained and a strong classifier is then built from a weighted sum of these weak classifiers (classifiers with better performance being given a higher weight). Following this approach, a strong classifier with good performance can be obtained by combining a few hundred simple features.</p><p class="calibre8">But in order to build a cascade of classifiers in which early rejection is a central mechanism, we do not want a strong classifier made of a large number of weak classifiers. Instead, we need to find very small classifiers that will use only a handful of Haar features in order to quickly reject the obvious negative samples while keeping all positive ones. In its classical form, AdaBoost aims at minimizing the total classification error by counting the number of false negatives (a positive sample classified as a negative one) and false positives (a negative sample classified as a positive one). In the present case, we need to have most, if not all, the positive samples correctly classified while minimizing the false positive rate. Fortunately, it is possible to modify AdaBoost such that true positives are rewarded more strongly. Consequently, when training each stage of a cascade, two criteria must be set: the minimum hit rate and the maximum false alarm rate; in <code class="literal">opencv_traincascade</code> these are specified using the <code class="literal">-minHitRate </code>(<code class="literal">0.995</code> default value) and <code class="literal">-maxFalseAlarmRate </code>(<code class="literal">0.5</code> default value) parameters. Haar features are added to the stage until the two performance criteria are met. The minimum hit rate must be set pretty high to make sure the positive instances will go through the next stage; remember that if a positive instance is rejected by a stage, then this error cannot be recovered. Therefore, to facilitate the generation of a classifier of low complexity, you should set the maximum false alarm rate relatively high. Otherwise, your stage will need many Haar features in order to meet the performance criteria, which contradicts the idea of early rejection by simple and quick to compute classifier stages.</p><p class="calibre8">A good cascade will therefore be made of early stages with few features, the number of features per stage growing as you go up the cascade. In <code class="literal">opencv_traincascade</code>, the maximum number of features per stage is set using the <code class="literal">-maxWeakCount</code> (default is <code class="literal">100</code>) parameter and the number of stages is set using <code class="literal">-numStages</code> (default is <code class="literal">20</code>).</p><p class="calibre8">When the training of a new stage starts, then new negative samples must be collected. These are extracted from the provided background images. The difficulty here is to find negative samples that pass through all previous stages (that is, that are wrongly classified as positives). The more stages you have trained, the more difficult it will be to collect these negative samples. This is why it is important to provide the classifier with a large variety of background images. It will then be able to extract patches from these that are difficult to classify (because they resemble the positive samples). Note also that if at a given stage, the two performance criteria are met without adding any new features, then the cascade training is stopped at this point (you can use it as is, or re-train it by providing more difficult samples). Reciprocally, if the stage is unable to meet the performance criteria, the training will also be stopped; in this case you should retry a new training with easier performance criteria.</p><p class="calibre8">With a cascade made of <code class="literal">n</code> stages, it can easily be shown that the global performance of the classifier will be at least better than <code class="literal">minHitRate<sup class="calibre20">n</sup></code> and <code class="literal">maxFalseAlarmRate<sup class="calibre20">n</sup></code>. This is the result of each stage being built on top of the results of the previous cascade of stages. For example, if we consider the default values of <code class="literal">opencv_traincascade</code>, we expect our classifier to have an accuracy (hit rate) of <code class="literal">0.995<sup class="calibre20">20</sup></code> and a false alarm rate of <code class="literal">0.5<sup class="calibre20">20</sup></code>. This means that 90% of the positive instances will be correctly identified and 0.001% of negative samples will be wrongly classified as positive. Note that an important consequence of the fact that a fraction of the positive samples will be lost as we go up the cascade is that you always have to provide more positive samples than the specified number of samples to use in each stage. In the numerical example we just gave, we need <code class="literal">numPos</code> to be set at 90% of the number of available positive samples.</p><p class="calibre8">One important question is how many samples should be used for training? This is difficult to answer but, obviously, your positive sample set must be large enough to cover a wide range of possible appearances of your class instances. Your background images should also be relevant. In the case of our stop sign detector, we included urban images as stop sign are expected to be seen in that context. A usual rule of thumb is to have <code class="literal">numNeg= 2*numPos</code>, but this has be validated on your own dataset.</p><p class="calibre8">Finally, we explained in this recipe how to build a cascade of classifiers using Haar features. Such features can also be built using other features such as the Local Binary Patterns discussed in the previous recipes or the histograms of oriented gradient that will be presented in the next recipe. The <code class="literal">opencv_traincascade</code> has a <code class="literal">-featureType</code> parameter allowing selection of different feature types.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec247" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The OpenCV library proposes a number of pre-trained cascades that you can use to detect faces, facial features, people, and other things. You will find these cascades in the form of XML files in the data directory of the library source directory.</p><div><div><div><div><h3 class="title3"><a id="ch14lvl3sec53" class="calibre6"/>Face detection with a Haar cascade</h3></div></div></div><p class="calibre8">The pre-trained models are ready to be used. All you have to do is to create an instance of the <code class="literal">cv::CascadeClassifier</code> class using the appropriate XML file:</p><pre class="programlisting">    cv::CascadeClassifier faceCascade; 
    if (!faceCascade.load("haarcascade_frontalface_default.xml")) { 
      std::cout &lt;&lt; "Error when loading the face cascade classfier!"  
                &lt;&lt; std::endl; 
      return -1; 
    } 
</pre><p class="calibre8">Then to detect faces with Haar features, you proceed this way:</p><pre class="programlisting">    faceCascade.detectMultiScale(picture, // input image 
               detections,           // detection results 
               1.1,                  // scale reduction factor 
               3,                 // number of required neighbor detections 
               0,                    // flags (not used) 
               cv::Size(48, 48),     // minimum object size to be detected 
               cv::Size(128, 128));  // maximum object size to be detected 
  
    // draw detections on image 
    for (int i = 0; i &lt; detections.size(); i++) 
      cv::rectangle(picture, detections[i],  
                    cv::Scalar(255, 255, 255), 2); 
</pre><p class="calibre8">The same process can be repeated for an eye detector, and the following image is obtained:</p><p class="calibre8">
</p><div><img alt="Face detection with a Haar cascade" src="img/image_14_010.jpg" class="calibre17"/></div><p class="calibre8">
</p></div></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec248" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Describing and matching local intensity patterns</em> recipe in <a href="ch09.html" title="Chapter 9. Describing and Matching Interest Points">
Chapter 9
</a>, <em class="calibre16">Describing and Matching Interest Points</em>, described the SURF descriptor which also uses Haar-like features</li><li class="listitem">The article <em class="calibre16">Rapid object detection using a boosted cascade of simple features</em> by <em class="calibre16">P. Viola</em> and <em class="calibre16">M. Jones</em> in <em class="calibre16">Computer Vision and Pattern Recognition</em> conference, 2001, is the classical paper that describes the cascade of boosted classifiers and the Haar features</li><li class="listitem">The article <em class="calibre16">A short introduction to boosting</em> by <em class="calibre16">Y. Freund</em> and <em class="calibre16">R.E. Schapire</em> in <em class="calibre16">Journal of Japanese Society for Artificial Intelligence</em>, 1999 describes the theoretical foundations of boosting</li><li class="listitem">The article <em class="calibre16">Filtered Channel Features for Pedestrian Detection</em> by <em class="calibre16">S. Zhang</em>, <em class="calibre16">R. Benenson</em> and <em class="calibre16">B. Schiele</em> in <em class="calibre16">IEEE Conference on Computer Vision and Pattern Recognition</em>, 2015 presents features similar to Haar and that can produce highly accurate detections</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch14lvl1sec83" class="calibre6"/>Detecting objects and people with Support Vector Machines and histograms of oriented gradients</h1></div></div></div><p class="calibre8">This recipe presents another machine learning method, the <strong class="calibre15">Support Vector Machines</strong> (<strong class="calibre15">SVM</strong>), which can produce accurate 2-class classifiers from training data. They have been largely used to solve many computer vision problems. This time, classification is solved by using a mathematical formulation that looks at the geometry of the problem in high-dimension spaces.</p><p class="calibre8">In addition, we will also present a new image representation that is often used in conjunction with SVMs to produce robust object detectors.</p><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec249" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">Images of objects are mainly characterized by their shape and textural content. This is the aspect that is captured by the <strong class="calibre15">Histogram of Oriented Gradients </strong>(<strong class="calibre15">HOG</strong>) representation. As its name indicates, this representation is based on building histograms from image gradients. In particular, because we are more interested by shapes and textures, it is the distribution of the gradient orientations that is analyzed. In addition, in order to take into consideration the spatial distribution of these gradients, multiple histograms are computed over a grid that divides the image into regions.</p><p class="calibre8">The first step in building a HOG representation is therefore to compute the gradient of an image. The image is then subdivided into small cells (for example, <code class="literal">8×8</code> pixels) and histograms of gradient orientations are built for each of these cells. The range of possible orientations must therefore be divided into bins. Most often, only the gradient orientations are considered but not their directions (these are called unsigned gradients). In this case, the range of possible orientations is from <code class="literal">0</code> to <code class="literal">180</code> degrees. A 9-bin histogram in this case would divide the possible orientations into intervals of <code class="literal">20</code> degrees. Each gradient vector in a cell contributes to a bin with a weight corresponding to the magnitude of this gradient.</p><p class="calibre8">The cells are then grouped into blocks. A block is then made of a certain number of cells. These blocks that cover the image can overlap each other (that is, they can share cells). For example, in the case where blocks are made of <code class="literal">2×2</code> cells, a new block can be defined every one cell; this would represent a block stride of <code class="literal">1</code> cell and each cell (except the last one in a row) would then contribute to <code class="literal">2</code> blocks. Conversely, with a block stride of <code class="literal">2</code> cells, the blocks would not overlap at all. A block contains a certain number of cell histograms (for example, <code class="literal">4</code> in the case of a block made of <code class="literal">2×2</code> cells). These histograms are simply concatenated together to form a long vector (for example, <code class="literal">4</code> histograms of <code class="literal">9</code> bins each then produce a vector of length <code class="literal">36</code>). To make the representation invariant to changes in contrast, this vector is then normalized (for example, each element is divided by the magnitude of the vector). Finally you also concatenate together all the vectors associated with all blocks of the image (row order) into a very large one (for example, in a <code class="literal">64×64</code> image, you will have a total of seven <code class="literal">16×16</code> blocks when a stride of <code class="literal">1</code> is applied on cells of size <code class="literal">8×8</code>; this represents a final vector of <code class="literal">49x36 = 1764</code> dimensions). This long vector is the HOG representation of the image.</p><p class="calibre8">As you can see, the HOG of an image leads to a vector of very high dimension (see the <em class="calibre16">There's more...</em> section of this recipe that proposes a way to visualize a HOG representation). This vector characterizes the image and can then be used to classify images of different classes of objects. To achieve this goal, we therefore need a machine learning method that can handle vectors of very high dimension.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec250" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">In this recipe, we will build another stop sign classifier. This is obviously just a toy example that serves to illustrate the learning procedure. As we explained in the previous recipe, the first step is to collect samples for training. In our example, the set of positive samples that we will be using is the following:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_011.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">And our (very small) set of negative samples is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_14_012.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We will now learn how to differentiate these two classes using SVM as implemented in the <code class="literal">cv::svm </code>class. To build a robust classifier, we will represent our class instances using HOG as described in the introductory section of this recipe. More precisely, we will use <code class="literal">8×8 </code>blocks made of <code class="literal">2×2</code> cells with a block stride of <code class="literal">1</code> cell:</p><pre class="programlisting">    cv::HOGDescriptor hogDesc(positive.size(), // size of the window 
                              cv::Size(8, 8),  // block size 
                              cv::Size(4, 4),  // block stride 
                              cv::Size(4, 4),  // cell size 
                              9);              // number of bins 
</pre><p class="calibre8">With 9-bin histograms and <code class="literal">64×64</code> samples, this configuration produces HOG vectors (made of <code class="literal">225</code> blocks) of size <code class="literal">8100</code>. We compute this descriptor for each of our samples and transfer them into a single matrix (one HOG per row):</p><pre class="programlisting">    // compute first descriptor  
    std::vector&lt;float&gt; desc; 
    hogDesc.compute(positives[0], desc); 
 
    // the matrix of sample descriptors 
    int featureSize = desc.size(); 
    int numberOfSamples = positives.size() + negatives.size(); 
 
    // create the matrix that will contain the samples HOG 
    cv::Mat samples(numberOfSamples, featureSize, CV_32FC1); 
    // fill first row with first descriptor 
    for (int i = 0; i &lt; featureSize; i++) 
      samples.ptr&lt;float&gt;(0)[i] = desc[i]; 
 
    // compute descriptor of the positive samples 
    for (int j = 1; j &lt; positives.size(); j++) { 
      hogDesc.compute(positives[j], desc); 
      // fill the next row with current descriptor 
      for (int i = 0; i &lt; featureSize; i++) 
        samples.ptr&lt;float&gt;(j)[i] = desc[i]; 
    } 
    // compute descriptor of the negative samples 
    for (int j = 0; j &lt; negatives.size(); j++) { 
      hogDesc.compute(negatives[j], desc); 
      // fill the next row with current descriptor 
      for (int i = 0; i &lt; featureSize; i++) 
        samples.ptr&lt;float&gt;(j + positives.size())[i] = desc[i]; 
    } 
</pre><p class="calibre8">Note how we computed the first HOG in order to obtain the size of the descriptor and then created the matrix of descriptors. A second matrix is then created to contain the labels associated to each sample. In our case, the first rows are the positive samples (and must be assigned a label of <code class="literal">1</code>), the reminder rows are the negative  samples (labeled <code class="literal">-1</code>):</p><pre class="programlisting">    // Create the labels 
    cv::Mat labels(numberOfSamples, 1, CV_32SC1); 
    // labels of positive samples 
    labels.rowRange(0, positives.size()) = 1.0; 
    // labels of negative samples 
    labels.rowRange(positives.size(), numberOfSamples) = -1.0; 
</pre><p class="calibre8">The next step is to build the SVM classifier that will be used for training; we also select the type of SVM and the kernel to be used (these parameters will be discussed in the next section):</p><pre class="programlisting">    // create SVM classifier 
    cv::Ptr&lt;cv::ml::SVM&gt; svm = cv::ml::SVM::create(); 
    svm-&gt;setType(cv::ml::SVM::C_SVC); 
    svm-&gt;setKernel(cv::ml::SVM::LINEAR); 
</pre><p class="calibre8">We are now ready for training. The labeled samples are first provided to the classifier and the <code class="literal">train</code> method is called:</p><pre class="programlisting">    // prepare the training data 
    cv::Ptr&lt;cv::ml::TrainData&gt; trainingData = 
           cv::ml::TrainData::create(samples,
                              cv::ml::SampleTypes::ROW_SAMPLE, labels); 
    // SVM training 
    svm-&gt;train(trainingData); 
</pre><p class="calibre8">Once the training phase completes, any sample of unknown class can be submitted to the classifier, which will try to predict the class to which it belongs (here we test four samples):</p><pre class="programlisting">    cv::Mat queries(4, featureSize, CV_32FC1); 
 
    // fill the rows with query descriptors 
    hogDesc.compute(cv::imread("stop08.png",  
                           cv::IMREAD_GRAYSCALE), desc); 
    for (int i = 0; i &lt; featureSize; i++) 
      queries.ptr&lt;float&gt;(0)[i] = desc[i]; 
    hogDesc.compute(cv::imread("stop09.png",  
                           cv::IMREAD_GRAYSCALE), desc); 
    for (int i = 0; i &lt; featureSize; i++) 
      queries.ptr&lt;float&gt;(1)[i] = desc[i]; 
    hogDesc.compute(cv::imread("neg08.png",  
                           cv::IMREAD_GRAYSCALE), desc); 
    for (int i = 0; i &lt; featureSize; i++) 
      queries.ptr&lt;float&gt;(2)[i] = desc[i]; 
    hogDesc.compute(cv::imread("neg09.png",  
                           cv::IMREAD_GRAYSCALE), desc); 
    for (int i = 0; i &lt; featureSize; i++) 
      queries.ptr&lt;float&gt;(3)[i] = desc[i]; 
    cv::Mat predictions; 
 
    // Test the classifier  
    svm-&gt;predict(queries, predictions); 
    for (int i = 0; i &lt; 4; i++) 
      std::cout &lt;&lt; "query: " &lt;&lt; i &lt;&lt; ": " &lt;&lt;  
               ((predictions.at&lt;float&gt;(i,) &lt; 0.0)?  
                   "Negative" : "Positive") &lt;&lt; std::endl; 
</pre><p class="calibre8">If the classifier has been trained with representative samples, then it should be able to correctly predict the label of a new instance.</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec251" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In our stop sign recognition example, each instance of our class is represented by a point in an 8100-dimensional HOG space. It is obviously impossible to visualize such a large space but the idea behind support vector machines is to trace a boundary in that space that will segregate points that belongs to one class from points belonging to the other class. More specifically, this boundary will in fact be just a simple hyperplane. This idea is better explained considering a 2D space where each instance is represented as a 2D point. The hyperplane is, in this case, a simple line.</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_14_013.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This is obviously a trivial example but, conceptually, working in a two-dimensional space or in a 8100-dimensional space is the same thing. The preceding figure shows then, how a simple line can separate the points of the two classes well. In the case illustrated here, it can also be seen that many other lines could also achieve this perfect class separation. One question is therefore; which exact line one should choose. To answer this question, you must first realize that the samples we used to build our classifier constitute just a small snapshot of all possible instances that will need to be classified when the classifier is used in a target application. This means that we would like our classifier, not only to be able to correctly separate the provided sample sets but we also would like this one to make the best decision on the future instances shown to it. This concept is often referred to as the <strong class="calibre15">generalization</strong> power of a classifier. Intuitively, it would be reasonable to believe that our separating hyperplane should be located in between the two classes, not closer to one class than the other. More formally, SVMs propose setting the hyperplane at a position that maximizes the margin around the defined boundary. This <strong class="calibre15">margin</strong> is defined as the minimum distance between the separating hyperplane and the closest point in the positive sample set plus the distance between the hyperplane and the closest negative sample. The closest points (the ones that define the margin) are called the <strong class="calibre15">support vectors</strong>. The mathematics behind SVM defines an optimization function aiming at identifying these support vectors.</p><p class="calibre8">But the proposed solution to the classification problem cannot be that simple. What happens if the distribution of the sample points is as follows?</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_14_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In this case, a simple hyperplane (a line here) cannot achieve a proper separation. SVM solves this problem by introducing artificial variables that bring the problem into a higher dimensional space through some non-linear transformations. For example, in the example above, one might propose to add the distance to the origin as an additional variable, that is to compute <em class="calibre16">r= sqrt(x<sup class="calibre20">2</sup>+y<sup class="calibre20">2</sup>)</em> for each point. We now have a three-dimensional space; for simplicity, let's just draw the points on the <em class="calibre16">(r,x)</em> plane:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_14_015.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As you can see, our set of sample points can now be separated by a simple hyperplane. This implies that you now have to find the support vectors in this new space. In fact, in the SVM formulation, you do not have to bring all the points into that new space, you just have to define a way to measure point-to-hyperplane distance in that augmented space. SVM therefore defines <strong class="calibre15">kernel functions</strong> that allow you to measure this distance in higher space without having to explicitly compute the point coordinates in that space. This is just a mathematical trick that explains why support vectors producing the maximal margin can be efficiently computed in very high (artificial) dimensional space. This also explains why, when you want to use support vector machines, you need to specify which kernel you want to use. This is by applying these kernels that you will make non-linearly separable to become separable in the kernel space.</p><p class="calibre8">One important remark here however. Since with Support Vector Machines we often work with features of very high dimension (for example, <code class="literal">8100</code> dimension in our HOG example), then it may very well happen that our samples will be separable with a simple hyperplane. This is why it still make sense to not use non-linear kernels (or more precisely to use a linear kernel, that is, <code class="literal">cv::ml::SVM::LINEAR</code>) and work in the original feature space. The resulting classifier will then be computationally simpler. But for more challenging classification problems, kernels remain a very effective tool. OpenCV offers you a number of standard kernels (for example, radial basis functions, sigmoid functions, and so on); the objective of these is to send the samples into a larger non-linear space that will make the classes separable by a hyperplane. SVM has a number of variants; the most common is the C-SVM, which adds a penalty for each outlier sample that does not lie on the right side of the hyperplane.</p><p class="calibre8">Finally, we insist on the fact that, because of their strong mathematical foundations, SVMs work very well with features of very high dimension. In fact, they have been shown to operate best when the number of dimensions of the feature space is larger than the number of samples. They are also memory efficient, as they just have to store the support vectors (in contrast to a method like nearest-neighbor that requires keeping in memory all sample points).</p></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec252" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Histograms of oriented gradients and SVM form a good combination for the construction of good classifiers. One of the reasons for this success is the fact that HOG can be viewed as a robust high-dimensional descriptor that captures the essential aspects of an object class. HOG-SVM classifiers have been used successfully in many applications; pedestrian detection is one of them.</p><p class="calibre8">Finally, since this is the last recipe of this book, we will therefore end it with a perspective on a recent trend in machine learning that is revolutionizing computer vision and artificial intelligence.</p><div><div><div><div><h3 class="title3"><a id="ch14lvl3sec54" class="calibre6"/>HOG visualization</h3></div></div></div><p class="calibre8">HOGs are built from cells combined in overlapping blocks. It is therefore difficult to visualize this descriptor. Nevertheless, they are often represented by displaying the histograms associated to each cell. In this case, instead of aligning the orientation bins in a regular bar graph, a histogram of orientation can be more intuitively drawn in a star-shape where each line has the orientation associated to the bin it represents and the length of the line is proportional to that bin count. These HOGs representations can then be displayed over an image:</p><p class="calibre8">
</p><div><img alt="HOG visualization" src="img/image_14_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Each cell HOG representation can be produced by a simple function that accepts an iterator pointing to a histogram. Lines of proper orientation and length are then drawn for each <code class="literal">bin</code>:</p><pre class="programlisting">    //draw one HOG over one cell 
    void drawHOG(std::vector&lt;float&gt;::const_iterator hog,  
                       // iterator to the HOG 
                 int numberOfBins,       // number of bins inHOG 
                 cv::Mat &amp;image,         // image of the cell 
                 float scale=1.0) {      // length multiplier 
 
      const float PI = 3.1415927; 
      float binStep = PI / numberOfBins; 
      float maxLength = image.rows; 
      float cx = image.cols / 2.; 
      float cy = image.rows / 2.; 
 
      // for each bin 
      for (int bin = 0; bin &lt; numberOfBins; bin++) { 
 
        // bin orientation 
        float angle = bin*binStep; 
        float dirX = cos(angle); 
        float dirY = sin(angle); 
        // length of line proportion to bin size 
        float length = 0.5*maxLength* *(hog+bin); 
 
        // drawing the line 
        float x1 = cx - dirX * length * scale; 
        float y1 = cy - dirY * length * scale; 
        float x2 = cx + dirX * length * scale; 
        float y2 = cy + dirY * length * scale; 
        cv::line(image, cv::Point(x1, y1), cv::Point(x2, y2),  
                 CV_RGB(255, 255, 255), 1); 
      } 
    } 
</pre><p class="calibre8">A HOG visualization function will then call this preceding function for each cell:</p><pre class="programlisting">    // Draw HOG over an image 
    void drawHOGDescriptors(const cv::Mat &amp;image,  // the input image  
             cv::Mat &amp;hogImage, // the resulting HOG image 
             cv::Size cellSize, // size of each cell (blocks are ignored) 
             int nBins) {       // number of bins 
 
      // block size is image size 
      cv::HOGDescriptor hog( 
              cv::Size((image.cols / cellSize.width) * cellSize.width,     
                       (image.rows / cellSize.height) * cellSize.height), 
              cv::Size((image.cols / cellSize.width) * cellSize.width,   
                       (image.rows / cellSize.height) * cellSize.height),  
              cellSize,    // block stride (ony 1 block here) 
              cellSize,    // cell size 
              nBins);      // number of bins 
  
      //compute HOG 
      std::vector&lt;float&gt; descriptors; 
      hog.compute(image, descriptors); 
      ... 
      float scale= 2.0 / * 
                  std::max_element(descriptors.begin(),descriptors.end()); 
      hogImage.create(image.rows, image.cols, CV_8U); 
      std::vector&lt;float&gt;::const_iterator itDesc= descriptors.begin(); 
 
      for (int i = 0; i &lt; image.rows / cellSize.height; i++) { 
        for (int j = 0; j &lt; image.cols / cellSize.width; j++) { 
          //draw each cell 
             hogImage(cv::Rect(j*cellSize.width, i*cellSize.height,  
                      cellSize.width, cellSize.height)); 
           drawHOG(itDesc, nBins,  
                   hogImage(cv::Rect(j*cellSize.width,                  
                                     i*cellSize.height,  
                                     cellSize.width, cellSize.height)),  
                   scale); 
          itDesc += nBins; 
        } 
      } 
    } 
</pre><p class="calibre8">This function computes a HOG descriptor having the specified cell size but made of only one large block (that is, a block having the size of the image). This representation therefore ignores the effect of normalization that occurs at each block level.</p></div><div><div><div><div><h3 class="title3"><a id="ch14lvl3sec55" class="calibre6"/>People detection</h3></div></div></div><p class="calibre8">OpenCV offers a pre-trained people detector based on HOG and SVM. As for the classifier cascades of the previous recipe, this SVM classifier can be used to detect instances in a full image by scanning a window across the image, at multiple scales. You then just have to construct the classifier and perform the detection on an image:</p><pre class="programlisting">    // create the detector 
    std::vector&lt;cv::Rect&gt; peoples; 
    cv::HOGDescriptor peopleHog; 
    peopleHog.setSVMDetector( 
    cv::HOGDescriptor::getDefaultPeopleDetector()); 
    // detect peoples oin an image 
    peopleHog.detectMultiScale(myImage, // input image 
               peoples,           // ouput list of bounding boxes  
               0,       // threshold to consider a detection to be positive 
               cv::Size(4, 4),    // window stride  
               cv::Size(32, 32),  // image padding 
               1.1,               // scale factor 
               2);                // grouping threshold 
</pre><p class="calibre8">The window stride defines how the <code class="literal">128×64</code> template is moved over the image (every <code class="literal">4</code> pixels horizontally and vertically in our example). Longer strides make the detection faster (because less windows are evaluated) but you may then miss some people falling in between tested windows. The image padding parameter simply adds pixels on the border of the image such that people at the edge of the image can be detected. The standard threshold for an SVM classifier is <code class="literal">0</code> (since <code class="literal">1</code> is the value assigned to positive instances and <code class="literal">-1 </code>to the negative ones). But if you really want to be certain that what you detect is a person, then you can raise this threshold value (this means that you want <strong class="calibre15">high precision</strong> at the price of missing some people in the image). Reciprocally, if you want to be certain of detecting all people (that is you want a <strong class="calibre15">high recall</strong> rate), then you can lower the threshold; more false detections will occur in that case.</p><p class="calibre8">Here is an example of the detection results obtained:</p><p class="calibre8">
</p><div><img alt="People detection" src="img/image_14_017.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">It is important to note that when a classifier is applied to a full image, the multiple windows applied at successive locations will often lead to multiple detections around a positive sample. The best thing to do when two or more bounding boxes overlap at about the same location is to retain only one of them. There is a function called <code class="literal">cv::groupRectangles</code> that simply combines rectangles of similar size at similar locations (this function is automatically called by <code class="literal">detectMultiScale</code>). In fact, obtaining a group of detections at a particular location can even be seen as an indicator confirming that we indeed have a positive instance at this location. This is why the <code class="literal">cv::groupRectangles</code> function allows us to specify the minimum size for a detection cluster to be accepted as a positive detection (that is, isolated detection should be discarded). This is the last parameter of the <code class="literal">detectMultiScale</code> method. Setting this one at <code class="literal">0</code> will keep all detections (no grouping done) which, in our example, leads to the following result:</p><p class="calibre8">
</p><div><img alt="People detection" src="img/image_14_018.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h3 class="title3"><a id="ch14lvl3sec56" class="calibre6"/>Deep learning and Convolutional Neural Networks</h3></div></div></div><p class="calibre8">We cannot conclude this chapter on machine learning without mentioning deep convolutional neural networks. The application of these to computer vision classification problems has led to impressive results. In fact, their outstanding performance when applied to real-world problems is such that they now open the door to a new family of applications that could not be envisioned before.</p><p class="calibre8">Deep learning is based on the theory of neural networks that was introduced in the late 1950s. So why are they generating such great interest today? Basically for two reasons: first, the computational power that is available nowadays allows deploying neural networks of a size that makes them able to solve challenging problems. While the first neural network (the perceptron) has only one layer and few weight parameters to tune, today's networks can have hundreds of layers and millions of parameters to be optimized (hence the name deep networks). Second, the large amount of data available today makes their training possible. In order to perform well, deep networks, indeed, required thousands, if not millions, of annotated samples (this is required because of the very large number of parameters that need to be optimized).</p><p class="calibre8">The most popular deep networks are the <strong class="calibre15">Convolutional Neural Networks</strong> (<strong class="calibre15">CNN</strong>). As the name suggests, they are based on convolution operations (see <a href="ch06.html" title="Chapter 6. Filtering the Images">
Chapter 6
</a>, <em class="calibre16">Filtering the Images</em>). The parameters to learn, in this case, are therefore the values inside the kernel of all filters that compose the network. These filters are organized into layers, in which the early layers extract the fundamental shapes such as lines and corners while the higher layers progressively detect more complex patterns (such as, for example, the presence of eyes, mouth, hair, in a human detector).</p><p class="calibre8">OpenCV3 has a <strong class="calibre15">Deep Neural Network</strong> module, but this one is mainly for importing deep networks trained using other tools such as TensorFlow, Caffe, or Torch. When building your future computer vision applications, you will certainly have to have a look at the deep learning theory and its related tools.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch14lvl2sec253" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Describing and matching local intensity patterns</em> recipe in <a href="ch09.html" title="Chapter 9. Describing and Matching Interest Points">
Chapter 9
</a>, <em class="calibre16">Describing and Matching Interest Points</em>, described the SIFT descriptor which is similar to the HOG descriptor</li><li class="listitem">The article <em class="calibre16">Histograms of Oriented Gradients for Human Detection</em> by N. Dalal and B. Triggs in <em class="calibre16">Computer Vision and Pattern Recognition</em> conference, 2005 is the classical paper that introduces histograms of oriented gradients for people detection</li><li class="listitem">The article <em class="calibre16">Deep Learning</em> by Y. LeCun, Y. Bengio and G. Hinton in <em class="calibre16">Nature, no 521</em>, 2015, is a good starting point for exploring the world of deep learning</li></ul></div></div></div></body></html>