["```py\n    class TWEET:\n    ```", "```py\n        def __init__(self, id=False, src=False,\n    ```", "```py\n                     text=False, tf=False,\n    ```", "```py\n                     scores=False, tokens=False,\n    ```", "```py\n                     args=False):\n    ```", "```py\n            self.id = id\n    ```", "```py\n            self.src = src\n    ```", "```py\n            self.text = text\n    ```", "```py\n            self.GS = scores\n    ```", "```py\n            self.tokens = tokens\n    ```", "```py\n            self.tf = ormalize(tf)\n    ```", "```py\n            self.ARGS = args\n    ```", "```py\n        def __repr__(self):\n    ```", "```py\n            return self.text\n    ```", "```py\n        def makeIndex(self):\n    ```", "```py\n            index = {}\n    ```", "```py\n            for tweet in self.tweets:\n    ```", "```py\n                for token in tweet.tokens:\n    ```", "```py\n                    if not token in index\n    ```", "```py\n                        index[token] = len(index)\n    ```", "```py\n            return index\n    ```", "```py\nmakeIndex, we can construct a DATASET class as follows:\n\n```", "```py\n\nWe will need to convert the format of a given dataset into these representations, but once that has been done, we will use them throughout this and the following chapters. We will do this in stages.\nFirst, we will convert the dataset so that it looks like the SEM11 dataset – that is, a tab-separated file with a header that specifies that the first and second fields as the ID and the tweet itself, with the remaining columns as the various emotions, followed by a line per tweet with 0s and 1s in the appropriate columns (the following example has the tweet and the columns for emotions truncated, so it will fit on the page).\nThis format is a variant of the standard **one-hot** representation used in neural networks, where a choice from several discrete labels is represented by a vector where each position in the vector represents a possible option. Suppose, for instance, that the possible labels were **{angry, sad, happy, love}**. In this case, we could represent **angry** with the vector <1, 0, 0, 0>, **sad** with <0, 1, 0, 0>, **happy** with <0, 0, 1, 0>, and **love** with <0, 0, 0, 1>.\nThe advantage of the SEM11 version of this format is that it makes it easy to allow tweets to have an arbitrary number of labels, thus allowing us to treat multi-label datasets and single-label datasets uniformly:\n\n```", "```py\n\nExactly how we convert a given dataset into this format depends on how the dataset is supplied. The following code shows how we do it for the CARER dataset – the others are similar, but because their original format is slightly different, the code for converting it into the SEM11 format will be slightly different\nThe CARER dataset comes as two files: a file called `dataset_infos.json` containing information about the dataset and another called `data.jsonl` containing the actual data. We convert this into SEM11 format by finding the names of the labels in  `dataset_infos.json` and then converting the entries in `data.jsonl` so that they have a 1 in the appropriate column and a 0 in the others.\n`data.jsonl` looks as follows:\n\n```", "```py\n\nWe want to convert this into the following:\n\n  **ID**\n |\n  **text**\n |\n  **sadness**\n |\n  **joy**\n |\n  **love**\n |\n  **anger**\n |\n  **fear**\n |\n  **surprise**\n |\n\n  0\n |\n  i feel awful about it\n |\n  1\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n\n  1\n |\n  i really do feel proud of myself\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n\nWe do this by using the set of labels provided in `dataset_infos.json` as the header line, and then writing each entry in `data.jsonl` with a 1 in the column specified by its label (for example, in the first (sadness) column for tweet 1 and the second (joy) column for tweet 2):\n\n```", "```py\n\nOnce we have the data in SEM11 format, we can read it as a dataset. We read the data line by line, using the first line as a header where the first two items are `ID` and `tweet` and the remainder are the emotions, and then use `makeTweet` to convert subsequent lines into `tweets`. We then remove duplicates and shuffle the data, construct document frequency and inverse document frequency tables, and wrap the whole thing up as a `dataset`:\n\n```", "```py\n\n`makeTweet` does quite a lot of work. It splits the line that was read from the file (which is, at this point, still just a tab-separated string) into its component parts and converts the 0s and 1s into a NumPy array; does tokenization and stemming as required (for example, for Arabic, the default is to convert the text into a form using Latin characters, tokenize it by just splitting it at white space, and then use the stemmer described in *wwww, Preprocessing – Stemming, Tagging, and Parsing* to find roots and affixes, with similar steps for other languages); and then finally make a term frequency table for the tweet and wrap everything up in `tweet`. All of these functions have an argument called `args` that contains a set of parameters that are supplied at the top level and that control what happens – for example, what language we are using, which tokenizer and stemmer we want to use, and so on:\n\n```", "```py\n\nWe must also define an abstract class for classifiers:\n\n```", "```py\n\nAs we continue, we will define several concrete types of classifiers. These all need a method so that they can be applied to sets of tweets, though how they are applied to individual tweets will vary. Therefore, we will provide this abstract class, which says that to apply any classifier to a set of tweets, you just apply its `applyToTweet` method to each tweet in the dataset. `BASECLASSIFIER` lets us capture this in an abstract class: we will never actually make a `BASECLASSIFIER`, and indeed it does not have a constructor, but all our concrete classifiers will be subclasses of `BASECLASSIFIER` and hence will have this method.\nThe abstract class has no constructor and just one method, which simply says that to apply a classifier to a dataset, you must use its `applyToTweet` method on each tweet in the dataset, but it will prove useful as we continue. Different concrete subclasses of this class will each define a version of `applyToTweet`, but it is useful to have a generic method for applying a classifier to an entire dataset.\nWe will use the Jaccard score, macro-F1, and micro-F1 as performance measures. As noted in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061), *Building and Using a Dataset*, micro-F1 tends to be very forgiving in situations where there is one class that predominates and the learning algorithm performs well on this class but less so on the smaller classes. This is a useful measure if you want to know how well the algorithm performs overall, but if you wish to make sure that it performs well on all the classes, then macro-F1 is more reliable (and is typically lower). Again, from [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061), *Building and Using a Dataset* Jaccard and micro-F1 are monotonically linked – if the micro-F1 for one experiment is higher than the micro-F1 for another, then the Jaccard measure will also be higher. So, these two measures will always provide the same ranking for sets of classifiers, but since some papers report one and some the other, it makes sense to include both when comparing a new classifier with others in the literature.\nSentiment lexicons\nNow that we have all the machinery for reading and managing datasets, it is time to start trying to develop classifiers. The first one we will look at is based on the simple observation that individual words carry emotional weight. It may be, as we will see later, that exactly how they contribute to the overall content of the message depends on their relationships with other words in the text, but simply looking at the presence of emotionally laden words (and emojis and suchlike) will give you a pretty good idea:\n*I feel like she is a really sweet person as well* (from the CARER dataset)\n*I feel like she is a really horrible person as well* (one word changed)\n*I feel gracious as he hands me across a rough patch* (from the CARER dataset)\n*I feel irritated as he hands me across a rough patch* (one word changed)\nSo, the simplest imaginable emotion-mining algorithm would simply involve labeling words with sentiments and seeing which sentiment scored the most highly for each text. Nothing could be simpler to implement, so long as you have a lexicon that has been labeled with emotions.\nHow could you get such a lexicon? You could make one by hand (or find one that someone else has made by hand), or you could try to extract one from a labeled corpus.\nBoth these approaches involve a large amount of work. You either have to go through a long list of words and assign a set of emotion labels to each, possibly with a score (for example, *sweet* and *love* both express joy, but *love* probably expresses it more strongly than *sweet*, and quantifying just how much more strongly it does so would be very difficult); or you have to go through a long list of tweets and assign a set of emotion labels to them, again possibly with a score. Both of these require a considerable amount of work, which you can either do yourself or get someone else to do (for example, by crowdsourcing it via a platform such as Amazon’s Mechanical Turk). If someone else has already done it and made the results available, then so much the better. We will start by considering a well-known resource, namely the NRC Word-Emotion Association Lexicon (also known as **EMOLEX**) (Mohammad & Turney, 2013). This consists of a list of English forms, each labeled with zero or more labels from a set of eight emotions (**anger**, **anticipation**, **disgust**, **fear**, **joy**, **sadness**, **surprise**, and **trust**) plus two polarities (**negative** and **positive**):\n\n   |  **anger**\n |\n  **anticipation**\n |\n  **disgust**\n |\n  **fear**\n |\n  **joy**\n |\n  **negative**\n |\n  **positive**\n |\n  **sadness**\n |\n  **surprise**\n |\n  **trust**\n |\n\n  aback\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n\n  abacus\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  1\n |\n\n  abandon\n |\n  0\n |\n  0\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n\n  abandoned\n |\n  1\n |\n  0\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n\n  abandonment\n |\n  1\n |\n  0\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  1\n |\n  1\n |\n  0\n |\n\n  abate\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n\n  abatement\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n\n  abba\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n  0\n |\n\n  abbot\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  1\n |\n\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n  ...\n |\n\nFigure 5.1 – EMOLEX labels\nTo use this with a given dataset, we have to match the emotions in the lexicon with the labels in the dataset – we cannot use the lexicon for any emotions that it does not contain, and emotions that are in the lexicon but not in some dataset cannot be used for extracting emotions from that dataset.\nWe will start by reading the lexicon and converting it into a Python dictionary. This is very straightforward – read the lexicon line by line, where the first item on a line is a word and the remainder are the scores for the 11 emotions. The only complications are that the dataset we want to use it with may have a different set of emotions from the eleven in the lexicon; and that we might want to use a stemmer to get the root form of a word – for example, to treat *abandon* and *abandoned* as a single item. This may make little difference for English, but it can be important when using the non-English equivalents that are provided for several languages.\nEMOLEX comes in various forms. We are using the one where the first column is an English word, the next 11 are the values for each emotion, and the last is a translation of the given English word into some other language. The default is the one where the other language is Arabic, but we have done some experiments with a Spanish corpus, for which we need a Spanish stemmer. The way to extend this to other languages should be obvious.\n`ARGS` is a set of parameters for applying the algorithm in different settings – for example, for specifying which language we are using. The two major issues here are as follows:\n\n*   EMOLEX contains inflected forms of words, but our classifiers typically require the root forms\n*   The emotions in EMOLEX are not necessarily the same as the ones used in the datasets\n\nTo deal with the first of these, we have to use a stemmer – that is, one of the ones from [*Chapter 4*](B18714_04.xhtml#_idTextAnchor093), *Preprocessing – Stemming, Tagging, and Parsing*. For the second, we have to find the emotions that are shared between EMOLEX and the dataset and restrict our attention to those:\n\n```", "```py\n\nThe following table shows what happens when we use this lexicon with our English datasets (SEM4, SEM11, WASSA, CARER), simply tokenizing the text by splitting it at white space:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  **0.418**\n |\n  **0.683**\n |\n  **0.519**\n |\n  **0.489**\n |\n  **0.350**\n |\n\n  SEM11-EN\n |\n  **0.368**\n |\n  **0.401**\n |\n  **0.383**\n |\n  **0.333**\n |\n  **0.237**\n |\n\n  WASSA-EN\n |\n  **0.435**\n |\n  **0.738**\n |\n  **0.547**\n |\n  **0.524**\n |\n  **0.376**\n |\n\n  CARER-EN\n |\n  **0.229**\n |\n  **0.524**\n |\n  **0.318**\n |\n  **0.287**\n |\n  **0.189**\n |\n\nFigure 5.2 – EMOLEX-based classifiers, no stemming\nThese scores provide a baseline for comparing the more sophisticated models to be developed later. It is worth observing that the scores for SEM4 are better than those for SEM11 – this is unsurprising given that SEM4 only has four fairly basic emotions (**anger**, **fear**, **joy**, and **sadness**), whereas SEM11 adds several more challenging ones (**surprise**, **trust**, and **anticipation**).\nSome of the classifiers that we will look at later can take a long time to train, and it may be that losing a bit of accuracy is worth it if training the more accurate classifiers takes an infeasible amount of time. What matters is whether the classifier is any good at the task we want it to carry out. A classifier that takes a second to train but gets almost everything wrong is no use. Nonetheless, if two algorithms have very similar results but one is much faster to train than the other, it may make sense to choose the faster one. It is hard to imagine anything much faster than the EMOLEX-based one – less than a thousandth of a second to process a single tweet, so that’s a tenth of a second to train on our largest (411K) training set.\nThe basic EMOLEX-based classifier, then, is very fast but produces fairly poor results. Are there things we can do to improve its scores?\nThe first extension involves using the tokenizer and stemmer described in [*Chapter 4*](B18714_04.xhtml#_idTextAnchor093)*, Preprocessing – Stemming, Tagging, and Parsing*. This has a fairly substantial effect in that it improves the scores, as shown here (we will mark the highest score that we have seen to date in bold; since all the scores in the table that use stemming are better than the ones without, they are all marked in bold here):\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  **0.461**\n |\n  **0.622**\n |\n  **0.530**\n |\n  **0.538**\n |\n  **0.360**\n |\n\n  SEM11-EN\n |\n  **0.411**\n |\n  **0.430**\n |\n  **0.420**\n |\n  **0.363**\n |\n  **0.266**\n |\n\n  WASSA-EN\n |\n  **0.465**\n |\n  **0.666**\n |\n  **0.547**\n |\n  **0.545**\n |\n  **0.377**\n |\n\n  CARER-EN\n |\n  **0.378**\n |\n  **0.510**\n |\n  **0.434**\n |\n  **0.378**\n |\n  **0.278**\n |\n\nFigure 5.3 – EMOLEX-based classifiers with stemming\nEMOLEX also provides a route into other languages by including a target language equivalent for each English word:\n\n   |  **anger**\n |\n  **…**\n |\n  **negative**\n |\n  **positive**\n |\n  **sadness**\n |\n  **surprise**\n |\n  **trust**\n |\n  **Spanish**\n |\n\n  Aback\n |\n  0\n |\n  …\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  detrás\n |\n\n  Abacus\n |\n  0\n |\n  …\n |\n  0\n |\n  0\n |\n  0\n |\n  0\n |\n  1\n |\n  ábaco\n |\n\n  Abandon\n |\n  0\n |\n  …\n |\n  0\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n  abandonar\n |\n\n  Abandoned\n |\n  1\n |\n  …\n |\n  1\n |\n  0\n |\n  1\n |\n  0\n |\n  0\n |\n  abandonado\n |\n\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n  …\n |\n\nFigure 5.4 – EMOLEX entries with Spanish translations\nIn some cases, this can be leveraged to provide a classifier for the target language: the missing section from the previous definition of readNRC is given here – the key changes are that we use the last item in the line as the form and that we use the appropriate stemmer for the given language:\n\n```", "```py\n\nBy trying this on the SEM4 and SEM11 Spanish and Arabic datasets, we obtain the following results:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-ES\n |\n  **0.356**\n |\n  **0.100**\n |\n  **0.156**\n |\n  **0.144**\n |\n  **0.085**\n |\n\n  SEM11-ES\n |\n  **0.272**\n |\n  **0.070**\n |\n  **0.111**\n |\n  **0.096**\n |\n  **0.059**\n |\n\n  SEM4-AR\n |\n  **0.409**\n |\n  **0.362**\n |\n  **0.384**\n |\n  **0.372**\n |\n  **0.238**\n |\n\n  SEM11-AR\n |\n  **0.267**\n |\n  **0.259**\n |\n  **0.263**\n |\n  **0.232**\n |\n  **0.151**\n |\n\nFigure 5.5 – EMOLEX-based classifiers for Spanish and Arabic, no stemming\nThe recall for the Spanish sets is very poor, but apart from that, the scores are surprisingly good considering that we just have the English dataset with one translation of each English word, where the translation is in the canonical form (that is, Spanish verbs are in the infinitive, Arabic nouns are singular, and where a noun has both masculine and feminine forms, then the masculine is used). If we simply use the Spanish and Arabic stemmers from [*Chapter 4*](B18714_04.xhtml#_idTextAnchor093) *, Preprocessing – Stemming, Tagging, and Parsing* (which do not, remember, make use of any lexicon), we get the following:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-ES\n |\n  **0.406**\n |\n  **0.164**\n |\n  **0.234**\n |\n  **0.224**\n |\n  **0.132**\n |\n\n  SEM11-ES\n |\n  **0.255**\n |\n  **0.105**\n |\n  **0.149**\n |\n  **0.121**\n |\n  **0.080**\n |\n\n  SEM4-AR\n |\n  **0.452**\n |\n  **0.536**\n |\n  **0.490**\n |\n  **0.469**\n |\n  **0.325**\n |\n\n  SEM11-AR\n |\n  **0.284**\n |\n  **0.348**\n |\n  **0.313**\n |\n  **0.276**\n |\n  **0.185**\n |\n\nFigure 5.6 – EMOLEX-based classifiers for Spanish and Arabic, stemmed\nUsing the stemmed forms improves the recall in every case, and generally improves the precision. The key here is that by using stemmed forms, things that look different but have the same underlying form get matched – for example, if the lexicon contains قدرة (*qdrp*, using the Buckwalter transliteration scheme (Buckwalter, T, 2007)) and some tweet contains القدرات (*AlqdrAt*, the plural form of the same word with a definite article added) – then whatever emotions قدرة is associated with will be found for القدرات. This will improve the recall since more words in the lexicon will be retrieved. It is more surprising that it generally improves the precision: to see why this happens, consider a case where the unstemmed form retrieves one word that is linked with **anger** and **surprise** but the stemmed form retrieves that word plus one that is just linked with **anger**. In the first case, the tweet will be labeled overall as **anger+surprise**, while in the second, it will be linked with just **anger**.\nUsing a better stemmer will improve the performance of the non-English versions of this approach, but the performance of the English version provides an upper limit – after all, there will be cases where the English word expresses some emotion that the translation doesn’t, and in those cases, any inferences based on the translation will be wrong. Suppose, for instance, that the English word *sick* was marked as being positive (which it often is in informal texts, though EMOLEX doesn’t recognize this); it is very unlikely that the French word *malade*, which is given as a translation, has the same informal interpretation. However, using EMOLEX, as described previously, would lead to the same emotions being ascribed to a French text that contains *malade* as those ascribed to an English one containing *sick*.\nThe EMOLEX lexicon for English is fairly large (14K words) and has been constructed following fairly strict guidelines, so it gives a reasonable indication of what can be achieved using a manually constructed lexicon. Can we do any better by extracting a lexicon from a training corpus?\nExtracting a sentiment lexicon from a corpus\nExtracting a lexicon from a corpus marked up for emotions is easy (once you have a corpus that has been marked up for emotions, which can be an extremely time-consuming and laborious thing to get). Just look at each tweet in the corpus: if it is annotated as contributing to some emotion, increment the number of times it has voted for that emotion, and at the end find out which emotion it has voted for most often. The corpus is used to make an instance of a class called `SIMPLELEXCLASSIFIER`, which is a realization of the `BASECLASSIFIER` class introduced previously. The key methods of this class are `calculateScores`, which iterates the training data (embodied as `DATASET`) to create the lexicon, and `applyToTweet`:\n\n```", "```py\n\nThis gives a range of scores for each word for each emotion – *sorry*, for instance, scores **anger**:0.62, **fear**:0.10, **joy**:0.00, **sadness**:0.29 – that is, it expresses mainly anger (most tweets containing it have been labeled as **anger**) but also sadness and, to a slight extent, fear.\nGiven this range of scores for individual words, we can expect complete tweets to contain a mixture of scores. So, we need to choose a threshold at which we say a tweet expresses an emotion. Thus, the definition of `applyToTweet` is as follows:\n\n```", "```py\n\nThe choice of threshold is crucial. As we increase the threshold, the precision will go up (by definition, as the threshold goes up, fewer tweets will meet it; however, the ones that do meet or exceed it are more likely to be correct, so the proportion that is correctly assigned an emotion will increase) and the recall will go down (because fewer tweets will meet it and some of the ones that do not will be ones that should have been included). The following tables show what happens with different thresholds for our datasets (we have added the aclIMDB and KWT.M-AR sets at this point – neither of these worked at all with the EMOLEX-based classifier). The following table shows the scores we get for the various datasets using a threshold of 1 and no stemming. Note the extremely high score we obtain for aclIMDB: this is due largely to the fact that this dataset only contains two emotions, so if we simply made random guesses, we would expect to obtain a score of 0.5, whereas since the SEM11 datasets have 11 emotions, random guessing would have an expected score of 0.09:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  0.664\n |\n  0.664\n |\n  0.664\n |\n  0.664\n |\n  0.497\n |\n\n  SEM11-EN\n |\n  0.614\n |\n  0.258\n |\n  0.363\n |\n  0.365\n |\n  0.222\n |\n\n  WASSA-EN\n |\n  0.601\n |\n  0.601\n |\n  0.601\n |\n  0.601\n |\n  0.430\n |\n\n  CARER-EN\n |\n  0.503\n |\n  0.503\n |\n  0.503\n |\n  0.503\n |\n  0.336\n |\n\n  aclImdb-EN\n |\n  0.839\n |\n  0.839\n |\n  0.839\n |\n  0.839\n |\n  0.722\n |\n\n  SEM4-AR\n |\n  0.672\n |\n  0.672\n |\n  0.672\n |\n  0.672\n |\n  0.506\n |\n\n  SEM11-AR\n |\n  0.647\n |\n  0.283\n |\n  0.394\n |\n  0.413\n |\n  0.245\n |\n\n  KWT.M-AR\n |\n  0.768\n |\n  0.757\n |\n  0.762\n |\n  0.768\n |\n  0.616\n |\n\n  SEM4-ES\n |\n  0.541\n |\n  0.664\n |\n  0.596\n |\n  0.542\n |\n  0.425\n |\n\n  SEM11-ES\n |\n  0.486\n |\n  0.293\n |\n  0.365\n |\n  0.367\n |\n  0.224\n |\n\nFigure 5.7 – Simple lexicon-based classifier, threshold=1, no stemming\nThis contrasts with the results we get when we lower the threshold to 0.5, as shown in *Figure 5**.8*.\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  0.281\n |\n  0.997\n |\n  0.438\n |\n  0.465\n |\n  0.281\n |\n\n  **SEM11-EN**\n |\n  **0.365**\n |\n  **0.767**\n |\n  **0.494**\n |\n  **0.487**\n |\n  **0.328**\n |\n\n  WASSA-EN\n |\n  0.287\n |\n  0.989\n |\n  0.444\n |\n  0.471\n |\n  0.286\n |\n\n  CARER-EN\n |\n  0.365\n |\n  0.803\n |\n  0.502\n |\n  0.508\n |\n  0.335\n |\n\n  aclImdb-EN\n |\n  0.500\n |\n  1.000\n |\n  0.667\n |\n  0.667\n |\n  0.500\n |\n\n  SEM4-AR\n |\n  0.454\n |\n  0.858\n |\n  0.594\n |\n  0.654\n |\n  0.422\n |\n\n  **SEM11-AR**\n |\n  **0.430**\n |\n  **0.728**\n |\n  **0.541**\n |\n  **0.546**\n |\n  **0.371**\n |\n\n  **KWT.M-AR**\n |\n  **0.795**\n |\n  **0.785**\n |\n  **0.790**\n |\n  **0.795**\n |\n  **0.652**\n |\n\n  SEM4-ES\n |\n  0.311\n |\n  0.879\n |\n  0.460\n |\n  0.516\n |\n  0.299\n |\n\n  **SEM11-ES**\n |\n  **0.315**\n |\n  **0.625**\n |\n  **0.419**\n |\n  **0.421**\n |\n  **0.265**\n |\n\nFigure 5.8 – Simple lexicon-based classifier, threshold=0.5, no stemming\nAs expected, the precision decreases and the recall increases as we lower the threshold. The size of this effect varies from dataset to dataset, with different thresholds producing different Jaccard and macro-F1 scores – the Jaccard score for SEM4-EN at threshold 1 is better than the score for this dataset at threshold 0.5, whereas, for SEM-11-EN, the Jaccard score is better at 0.5 than at 1\\. Note that the scores for the SEM-11 and KWT.M cases are all better at the lower threshold: this happens because these cases all allow multiple emotions to be assigned to a single tweet. Lowering the threshold lets the classifier find more emotions, which is helpful if large numbers of tweets have multiple emotions. We will return to this issue in [*Chapter* *10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.\nWe can attempt to find the best threshold automatically: find the lowest and highest scores that any tweet has and then try a range of thresholds between these two values. We apply this algorithm to a small section of the training data – we cannot apply it to the test data, but experimentation shows that we do not need the full training set to arrive at good values for the threshold:\n\n```", "```py\n\nUsing this to find the optimal threshold, we find that in every case, automatically extracting the lexicon produces a better score than the original scores with EMOLEX:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  **0.617**\n |\n  **0.732**\n |\n  **0.670**\n |\n  **0.683**\n |\n  **0.503**\n |\n\n  SEM11-EN\n |\n  **0.475**\n |\n  **0.564**\n |\n  **0.515**\n |\n  **0.515**\n |\n  **0.347**\n |\n\n  WASSA-EN\n |\n  **0.571**\n |\n  **0.669**\n |\n  **0.616**\n |\n  **0.623**\n |\n  **0.445**\n |\n\n  CARER-EN\n |\n  **0.487**\n |\n  **0.554**\n |\n  **0.518**\n |\n  **0.522**\n |\n  **0.350**\n |\n\n  aclImdb-EN\n |\n  **0.839**\n |\n  **0.839**\n |\n  **0.839**\n |\n  **0.839**\n |\n  **0.722**\n |\n\n  SEM4-AR\n |\n  **0.672**\n |\n  **0.672**\n |\n  **0.672**\n |\n  **0.672**\n |\n  **0.506**\n |\n\n  SEM11-AR\n |\n  **0.485**\n |\n  **0.632**\n |\n  **0.549**\n |\n  **0.549**\n |\n  **0.378**\n |\n\n  KWT.M-AR\n |\n  **0.816**\n |\n  **0.812**\n |\n  **0.814**\n |\n  **0.817**\n |\n  **0.687**\n |\n\n  SEM4-ES\n |\n  **0.541**\n |\n  **0.664**\n |\n  **0.596**\n |\n  **0.542**\n |\n  **0.425**\n |\n\n  SEM11-ES\n |\n  **0.372**\n |\n  **0.493**\n |\n  **0.424**\n |\n  **0.429**\n |\n  **0.269**\n |\n\nFigure 5.9 – Standard datasets, optimal thresholds, no stemming\nUnsurprisingly, the scores here are as good as or better than the scores obtained with 1.0 or 0.5 as thresholds since we have tried a range of thresholds and chosen the best – if the best is indeed 1.0 or 0.5, then the score will be as in those tables, but if not, it must be better (or we would not have chosen it!).\nUsing the optimal thresholds with stemming produces worse results in several cases. In the English cases, the performance is, at best, fractionally better than when we do not do stemming, though it does help with some of the non-English cases:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  0.610\n |\n  0.729\n |\n  0.664\n |\n  0.677\n |\n  0.497\n |\n\n  **SEM11-EN**\n |\n  **0.478**\n |\n  **0.562**\n |\n  **0.516**\n |\n  **0.518**\n |\n  **0.348**\n |\n\n  WASSA-EN\n |\n  0.566\n |\n  0.658\n |\n  0.609\n |\n  0.615\n |\n  0.437\n |\n\n  **CARER-EN**\n |\n  **0.477**\n |\n  **0.569**\n |\n  **0.519**\n |\n  **0.522**\n |\n  **0.350**\n |\n\n  aclImdb-EN\n |\n  0.684\n |\n  0.964\n |\n  0.800\n |\n  0.827\n |\n  0.667\n |\n\n  **SEM4-AR**\n |\n  **0.651**\n |\n  **0.701**\n |\n  **0.675**\n |\n  **0.683**\n |\n  **0.509**\n |\n\n  **SEM11-AR**\n |\n  **0.497**\n |\n  **0.635**\n |\n  **0.557**\n |\n  **0.554**\n |\n  **0.386**\n |\n\n  KWT.M-AR\n |\n  0.802\n |\n  0.793\n |\n  0.797\n |\n  0.801\n |\n  0.663\n |\n\n  SEM4-ES\n |\n  0.516\n |\n  0.692\n |\n  0.591\n |\n  0.531\n |\n  0.420\n |\n\n  **SEM11-ES**\n |\n  **0.376**\n |\n  **0.493**\n |\n  **0.427**\n |\n  **0.431**\n |\n  **0.271**\n |\n\nFigure 5.10 – Standard datasets, optimal thresholds, stemmed\nIt is less surprising that we get the greatest improvement from the EMOLEX-based classifiers with the large dataset: EMOLEX contains 24.9K words,  the lexicons extracted from the SEM4-EN, SEM11-EN, and WASSA datasets contain 10.8K, 17.5K, and 10.9K words, respectively, and the lexicon extracted from CARER contains 53.4K words. In other words, the increase in the size of the extracted lexicon is much greater for the large dataset, which is why the improvement over the hand-coded one is also greater.\nThe various lexicons all link emotionally loaded words with the emotions they express. Using the CARER dataset, we can see that we get sensible associations for some common words that would be used to express emotions:\n\n   |  **anger**\n |\n  **fear**\n |\n  **joy**\n |\n  **love**\n |\n  **sadness**\n |\n  **surprise**\n |\n\n  adores\n |\n  0.11\n |\n  0.00\n |\n  0.44\n |\n  0.33\n |\n  0.11\n |\n  0.00\n |\n\n  happy\n |\n  0.08\n |\n  0.05\n |\n  0.62\n |\n  0.05\n |\n  0.17\n |\n  0.03\n |\n\n  hate\n |\n  0.22\n |\n  0.13\n |\n  0.16\n |\n  0.06\n |\n  0.42\n |\n  0.02\n |\n\n  joy\n |\n  0.07\n |\n  0.05\n |\n  0.53\n |\n  0.12\n |\n  0.21\n |\n  0.04\n |\n\n  love\n |\n  0.09\n |\n  0.07\n |\n  0.42\n |\n  0.19\n |\n  0.21\n |\n  0.03\n |\n\n  sad\n |\n  0.14\n |\n  0.08\n |\n  0.11\n |\n  0.03\n |\n  0.61\n |\n  0.03\n |\n\n  scared\n |\n  0.04\n |\n  0.71\n |\n  0.07\n |\n  0.01\n |\n  0.14\n |\n  0.02\n |\n\n  sorrow\n |\n  0.15\n |\n  0.04\n |\n  0.24\n |\n  0.13\n |\n  0.41\n |\n  0.04\n |\n\n  terrified\n |\n  0.01\n |\n  0.90\n |\n  0.03\n |\n  0.01\n |\n  0.04\n |\n  0.01\n |\n\nFigure 5.11 – Emotions associated with significant words, the CARER dataset\nIf we look at other words that would not be expected to have any emotional significance, however, we will find something surprising:\n\n   |  **anger**\n |\n  **fear**\n |\n  **joy**\n |\n  **love**\n |\n  **sadness**\n |\n  **surprise**\n |\n\n  a\n |\n  0.13\n |\n  0.12\n |\n  0.35\n |\n  0.10\n |\n  0.27\n |\n  0.04\n |\n\n  and\n |\n  0.13\n |\n  0.11\n |\n  0.35\n |\n  0.09\n |\n  0.28\n |\n  0.04\n |\n\n  the\n |\n  0.13\n |\n  0.11\n |\n  0.37\n |\n  0.10\n |\n  0.26\n |\n  0.04\n |\n\nFigure 5.12 – Emotions associated with common words, the CARER dataset\nThe word *a* occurs in almost every text in this dataset – every text that expresses anger, every text that expresses fear, and so on. So, it contains scores that reflect the distribution of emotions in the dataset: *a*, *and*, and *the* all get scores of around 0.13 for anger, which simply reflects the fact that about 13% of the tweets express this emotion; they each get scores of about 0.11 for fear because about 11% of the tweets express fear, and so on.\nThere are three obvious things we can do to try to solve this problem:\n\n*   We can manually produce a list of stop words. This tends to be a poor way to proceed since it relies very heavily on intuitions, which are often unreliable when people are thinking about words in isolation.\n*   We can try to weed out words that do not contribute to the distinctive meaning of the text we are looking at.\n*   We can adjust the degree to which a word votes more strongly for one emotion than for others.\n\nLet’s discuss the last two in detail.\n*Weeding out words that do not contribute much to the distinctive meaning of a text*: If a word occurs extremely frequently across a corpus, then it cannot be used as a good indicator of whether one text in the corpus is similar to another. This notion is widely used when computing similarity between texts, so it is worth looking at whether it can help us with the problem of common words voting for emotions.\nThe most commonly used measure for assessing the contribution that a word makes to the distinctiveness of a text is **term frequency/inverse document frequency** (**TF-IDF**) (Sparck Jones, 1972). Term frequency is the number of times the word in question occurs in a given document, whereas document frequency is the number of documents that it occurs in. So, if a word occurs frequently in a document, then it may be important for that document, but if it occurs in every single document, then it probably is not. It is customary to take the log of the document frequency to smooth out the effect of very common words, and it is essential to add 1 to the document frequency to make sure that we are not trying to take the log of 0:\n![<math  display=\"block\"><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mfenced open=\"(\" close=\")\"><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo>,</mo><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>r</mi><mi>p</mi><mi>u</mi><mi>s</mi></mrow></mfenced><mo>=</mo><mfrac><mfenced open=\"|\" close=\"|\"><mrow><mi>o</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfenced><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mfenced open=\"|\" close=\"|\"><mrow><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi></mrow></mfenced><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/14.png)\nUsing this measure to weight the contributions of individual words produces the following:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  0.546\n |\n  0.546\n |\n  0.546\n |\n  0.546\n |\n  0.375\n |\n\n  SEM11-EN\n |\n  0.554\n |\n  0.232\n |\n  0.327\n |\n  0.328\n |\n  0.195\n |\n\n  WASSA-EN\n |\n  0.492\n |\n  0.492\n |\n  0.492\n |\n  0.492\n |\n  0.326\n |\n\n  CARER-EN\n |\n  0.518\n |\n  0.518\n |\n  0.518\n |\n  0.518\n |\n  0.350\n |\n\n  aclImdb-EN\n |\n  0.815\n |\n  0.815\n |\n  0.815\n |\n  0.815\n |\n  0.687\n |\n\n  SEM4-AR\n |\n  0.638\n |\n  0.638\n |\n  0.638\n |\n  0.638\n |\n  0.468\n |\n\n  SEM11-AR\n |\n  0.592\n |\n  0.261\n |\n  0.362\n |\n  0.378\n |\n  0.221\n |\n\n  KWT.M-AR\n |\n  0.804\n |\n  0.789\n |\n  0.797\n |\n  0.802\n |\n  0.662\n |\n\n  SEM4-ES\n |\n  0.503\n |\n  0.661\n |\n  0.571\n |\n  0.510\n |\n  0.400\n |\n\n  SEM11-ES\n |\n  0.439\n |\n  0.279\n |\n  0.341\n |\n  0.348\n |\n  0.206\n |\n\nFigure 5.13 – Using TF-IDF to adjust the weights\nThese scores are not an improvement on the originals: using TF-IDF does not help with our task, at least not in isolation. We will find that it can be useful when used in combination with other measures, but by itself, it is not useful.\n*Adjusting the degree to which a word votes more strongly for one emotion than for others*: Revisiting the tables of weights for individual words, we can see that the weights for *a* are very evenly distributed, whereas the scores for *terrified* scores highly for **fear** and very low for anything else:\n\n   |  **anger**\n |\n  **fear**\n |\n  **joy**\n |\n  **love**\n |\n  **sadness**\n |\n  **surprise**\n |\n\n  a\n |\n  0.13\n |\n  0.12\n |\n  0.35\n |\n  0.10\n |\n  0.27\n |\n  0.04\n |\n\n  terrified\n |\n  0.01\n |\n  0.90\n |\n  0.03\n |\n  0.01\n |\n  0.04\n |\n  0.01\n |\n\nFigure 5.14 – Emotions associated with “a” and “terrified,” the CARER dataset\nIf we subtract the average for a score from the individual scores, we end up with a much more sensible set of scores: a conditional probability classifier, `CPCLASSIFIER`, is a subclass of `SIMPLELEXCLASSIFIER`, which simply has the definition of `calculateScores` changed to the following:\n\n```", "```py\n\nIn other words, the only change is that we subtract the average score for emotions for a given word from the original, so long as the result of doing that is greater than 0\\. This changes the values for a common word and an emotionally laden word, as shown here:\n\n   |  **anger**\n |\n  **fear**\n |\n  **joy**\n |\n  **love**\n |\n  **sadness**\n |\n  **surprise**\n |\n\n  a\n |\n  0.00\n |\n  0.00\n |\n  0.18\n |\n  0.00\n |\n  0.10\n |\n  0.00\n |\n\n  terrified\n |\n  0.00\n |\n  0.73\n |\n  0.00\n |\n  0.00\n |\n  0.00\n |\n  0.00\n |\n\nFigure 5.15 – Emotions associated with “a” and “terrified,” the CARER dataset, bias emphasized\nHere, the scores for *a* have been greatly flattened out, while *terrified* only votes for **fear**:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  **SEM4-EN**\n |\n  **0.714**\n |\n  **0.779**\n |\n  **0.745**\n |\n  **0.752**\n |\n  **0.593**\n |\n\n  **SEM11-EN**\n |\n  **0.471**\n |\n  **0.582**\n |\n  **0.521**\n |\n  **0.518**\n |\n  **0.352**\n |\n\n  **WASSA-EN**\n |\n  **0.604**\n |\n  **0.769**\n |\n  **0.677**\n |\n  **0.692**\n |\n  **0.512**\n |\n\n  CARER-EN\n |\n  0.539\n |\n  0.640\n |\n  0.585\n |\n  0.589\n |\n  0.414\n |\n\n  aclImdb-EN\n |\n  0.798\n |\n  0.883\n |\n  0.838\n |\n  0.847\n |\n  0.721\n |\n\n  SEM4-AR\n |\n  0.592\n |\n  0.747\n |\n  0.661\n |\n  0.684\n |\n  0.493\n |\n\n  SEM11-AR\n |\n  0.476\n |\n  0.624\n |\n  0.540\n |\n  0.540\n |\n  0.370\n |\n\n  KWT.M-AR\n |\n  0.814\n |\n  0.811\n |\n  0.813\n |\n  0.816\n |\n  0.684\n |\n\n  SEM4-ES\n |\n  0.194\n |\n  0.948\n |\n  0.321\n |\n  0.310\n |\n  0.191\n |\n\n  SEM11-ES\n |\n  0.400\n |\n  0.471\n |\n  0.433\n |\n  0.435\n |\n  0.276\n |\n\nFigure 5.16 – Increased bias lexicon-based classifier, optimal thresholds, no stemming\nChanging the weights in this way without stemming improves or has very little effect on the scores for nearly all the English cases:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  **SEM4-EN**\n |\n  **0.718**\n |\n  **0.772**\n |\n  **0.744**\n |\n  **0.750**\n |\n  **0.593**\n |\n\n  **SEM11-EN**\n |\n  **0.479**\n |\n  **0.573**\n |\n  **0.522**\n |\n  **0.520**\n |\n  **0.353**\n |\n\n  WASSA-EN\n |\n  0.641\n |\n  0.703\n |\n  0.671\n |\n  0.675\n |\n  0.505\n |\n\n  CARER-EN\n |\n  0.512\n |\n  0.633\n |\n  0.566\n |\n  0.570\n |\n  0.395\n |\n\n  **aclImdb-EN**\n |\n  **0.799**\n |\n  **0.882**\n |\n  **0.839**\n |\n  **0.848**\n |\n  **0.722**\n |\n\n  **SEM4-AR**\n |\n  **0.651**\n |\n  **0.709**\n |\n  **0.679**\n |\n  **0.686**\n |\n  **0.513**\n |\n\n  SEM11-AR\n |\n  0.501\n |\n  0.616\n |\n  0.553\n |\n  0.552\n |\n  0.382\n |\n\n  KWT.M-AR\n |\n  0.801\n |\n  0.797\n |\n  0.799\n |\n  0.803\n |\n  0.666\n |\n\n  SEM4-ES\n |\n  0.189\n |\n  0.733\n |\n  0.301\n |\n  0.284\n |\n  0.177\n |\n\n  **SEM11-ES**\n |\n  **0.397**\n |\n  **0.481**\n |\n  **0.435**\n |\n  **0.439**\n |\n  **0.278**\n |\n\nFigure 5.17 – Increased bias lexicon-based classifier, optimal thresholds, stemmed\nAs ever, stemming sometimes helps with non-English examples and sometimes it doesn’t.\nSo far in this chapter, we have looked at several ways of extracting a lexicon from a corpus that has been marked up with emotion labels and used this to assign emotions to unseen texts. The main lessons to be learned from these experiments are as follows:\n\n*   Lexicon-based classifiers can provide reasonable performance for very little computational cost, though the effort involved in making lexicons, either directly or by extracting them from annotated texts, is considerable.\n*   Refinements such as stemming and varying the weights associated with individual words can sometimes be useful, but what works for one corpus may not work for another. For this reason, it is sensible to divide your training data into training and development sets so that you can try out different combinations to see what works with your data, on the assumption that the data you are using for training is indeed similar to the data that you will be applying it on for real. For this reason, competition data is often split into training and development sets when it is distributed.\n*   Having a large amount of data can be useful but after a certain point, the improvements in performance tail off. It makes sense to plot data size against accuracy for subsets of your full dataset since this allows you to fit a curve of the relationship between the two. Given such a curve, it is possible to estimate what the accuracy would be if you were able to obtain more data, and hence to decide whether it is worth trying to do so. Such an estimate will only be an approximation, but if, for instance, it is clear that the curve has already flattened out, then it is unlikely that getting more data will make a difference.\n\nOne of the problems with this kind of approach is that the training data may not contain every emotion-bearing word. In the next section, we will try to extend lexicons of the kind we extracted previously by looking for “similar” words to fill in the gap.\nSimilarity measures and vector-space models\nOne of the problems that any lexicon-based classifier faces is that the lexicon may not contain all the words in the test set. For the English datasets we have been looking at, EMOLEX and the lexicon extracted from the training data contain the following percentages of the words in the development sets:\n\n   |  **% of words in the** **extracted dictionary**\n |\n  **% of words** **in EMOLEX**\n |\n\n  SEM4-EN\n |\n  0.46\n |\n  0.20\n |\n\n  SEM11-EN\n |\n  0.47\n |\n  0.19\n |\n\n  WASSA-EN\n |\n  0.55\n |\n  0.21\n |\n\n  CARER\n |\n  0.95\n |\n  0.44\n |\n\nFigure 5.18 – Words in the test sets that are in one of the lexicons\nMany of the words that are missing from EMOLEX will be function words (*a*, *the*, *in*, *and*, and so on) and words that carry no emotion, but it seems likely that adding more words to the lexicon will be helpful. If we knew that *adore* was very similar to *love*, but *adore* was not in the lexicon, then it would be very helpful if we could use the emotional weight of *love* when a text contained *adore*. The number of words that are missing from the extracted lexicons is more worrying. As the training data increases, the number of missing words goes down – 54% of the words in the test sets for SEM4-EN are missing in the training data, whereas only 5% are missing from CARER, but virtually none of the missing words in these cases are function words, so many are likely to be emotion-bearing.\nThere are numerous ways of estimating whether two words are similar. Nearly all are based on the notion that two words are similar if they occur in similar contexts, usually using sentences or local windows as contexts, and they nearly all make use of vector-space models. In this section, we will explore these two ideas before looking at how they may be used to supplement the lexicons being used for emotion detection.\nVector spaces\nIt is often useful to represent things as vectors in some high-dimensional space. An obvious example is the representation of a sentence as a point in a space where each word of the language is a dimension. Recall that `makeIndex` lets us make an index linking each word to a unique identifier; for example:\n\n```", "```py\n\n We can then use `sentence2vector` to convert a string of words into a vector. We make a vector full of zeros that is large enough to accommodate every word in the index. Then, we can scan the sentence and add 1 to the appropriate position in the vector for each word that we see:\n\n```", "```py\n\nGiven the preceding index, this would produce the following for the sentence *I don’t want to* *be sober*:\n\n```", "```py\n\nSuch vectors tend to be very sparse. The index we used for constructing this vector contained 18,263 words and the sentence contained 7 distinct words, so 18,256 entries in the vector are 0\\. This means that a lot of space is wasted, but also that calculations involving such vectors can be very slow. Python provides tools for handling such vectors: **sparse arrays**. The key to the way Python does this is that instead of keeping an array that contains a place for every value, you keep three arrays: the first contains the non-zero values, and the second and third contain the row and column where a value is to be found. For our example, we would have the following (we only need the column values because our array is just a vector):\n\n```", "```py\n\nIn other words, we have the value 1 at positions 2 (which was the index entry for *I*), 7 (*sober*), 8 (*do*), and so on.\nCalculating similarity\nThe commonest use of vector representations is for calculating the similarity between two objects. We will illustrate this, and explore it a bit further, by considering it as a way of comparing sentences, but given the number of things that can be represented as vectors, the technique has a very wide range of applications.\nConsider two vectors in a simple 2D space. There are two ways of assessing how similar they are: you can see how far apart their endpoints are, or you can calculate the angle between them. In the following diagram, it is clear that the angle between the two vectors <(0,0), (2.5, 2.5)> and <(0, 0), (4.6, 4.9)> is very small, but the distance between their endpoints is quite large. It is common practice when using vector-space representations to carry out normalization, by dividing the value in each dimension by the length of the vector:\n![Figure 5.19 – Vectors to (2.5, 2.5) and (4.6, 4.9)](img/B18714_05_19.jpg)\n\nFigure 5.19 – Vectors to (2.5, 2.5) and (4.6, 4.9)\nIf we normalize these two vectors, we get *Figure 5**.20*, where the angle between the vectors and the distance between their endpoints are both very small:\n![Figure 5.20 – Normalized versions of (2.5, 2.5) and (4.6, 4.9)](img/B18714_05_20.jpg)\n\nFigure 5.20 – Normalized versions of (2.5, 2.5) and (4.6, 4.9)\nMost applications use the (*N*-dimensional) cosine of the angle between the vectors, but it is worth noting that for `sklearn.metrics.pairwise` library provides `cosine_similarity` for this task.\nIf we apply `sentence2vector` to the sentences *John ate some pasta*, *John ate the pasta*, *John ate some potatoes*, and *Mary drank some beer*, we get the following:\n\n```", "```py\n\nThis means *John ate some pasta* is represented by a vector that has 1 as the value in the 63rd, 2,306th, 3,304th, and 7,616th dimensions and zero everywhere else, and similarly for the other sentences. If we compute the cosine similarity of each pair, we get the following:\n\n```", "```py\n\nIn other words, every sentence is identical to itself, `S0`, `S1`, and `S2` are quite similar to one another, and `S3` is fairly different from the others. This all seems fairly sensible, save that `S0`, `S1`, and `S2` all have **identical** similarity scores. That doesn’t seem quite as reasonable – surely *John ate some pasta* and *John ate the pasta* are more similar than *John ate some pasta* and *John ate* *some potatoes*.\nThe key here is that some words seem to be more important than others *when you are trying to calculate how similar two sentences are*. This is not to say that words such as *some* and *the* are not important when you are trying to work out what a sentence means, but if, for instance, you want to see whether two sentences are about the same general topic, then maybe these closed class items are less significant.\nYou could try to deal with this by providing a list of **stop words**, which should be ignored when you are turning a sentence into a vector. However, there are two problems with this approach:\n\n*   It is very hard to work out which words should be ignored and which ones shouldn’t\n*   It’s a very blunt instrument – some words seem to make very little difference when you are comparing sentences, some make a bit of difference but not much, and some are highly significant\n\nWhat we want is a number that we can use to weight different words for their significance.\nWe will use TF-IDF to assign weights to words. There are several minor variations on how to calculate TF-IDF, with some working better with long documents and some with shorter ones (for example, when a document is just a single sentence), but the following is a reasonably standard version. We start by calculating an **inverse document frequency** table. We walk through the set of documents, getting the set of words in each document, and then increment a counter for each word in the set. This gives us a count of the number of documents each word appears in. We then make the inverse table by getting the reciprocal of the log of each entry – we need the reciprocal because we are going to want to divide by these values. We may as well do that now so that we can replace division with multiplication later on. It is standard practice to use the log at this point, though there is no very strong theoretical reason for doing so and there are cases (particularly with very short documents) where the raw value works better:\n\n```", "```py\n\nThis produces a pair of tables, `df` and `idf`, as follows when applied to the tweets in SEM4-EN, where *a* and *the* appear in large numbers of documents and *man*, *cat*, and *loves* appear in a fairly small set, so `df` for *a* and *the* is high and their `idf` (which is the measure of how important they are in this document) is low:\n\n```", "```py\n\nWe can use this to change `sentence2vector` so that it increments the scores by the IDF value for each word, rather than always incrementing by 1 (this is the same as multiplying the sum of a series of increments by the IDF value):\n\n```", "```py\n\n*John ate the pasta* is now represented by a vector with values that represent how common the words in question are, and hence how much importance they should be given when comparing vectors:\n\n```", "```py\n\nUsing this weighted version of the various vectors, our similarity table for the four sentences becomes as follows:\n\n   |  **S0**\n |\n  **S1**\n |\n  **S2**\n |\n  **S3**\n |\n\n  S0\n |\n  1.0000\n |\n  0.9999\n |\n  0.5976\n |\n  0.0000\n |\n\n  S1\n |\n  0.9999\n |\n  1.0000\n |\n  0.5976\n |\n  0.0003\n |\n\n  S2\n |\n  0.5976\n |\n  0.5976\n |\n  1.0000\n |\n  0.0000\n |\n\n  S3\n |\n  0.0000\n |\n  0.0003\n |\n  0.0000\n |\n  1.0000\n |\n\nFigure 5.21 – Similarity table\n`S0` and `S1` are now very similar (so similar that we have had to print them to four decimal places for any difference to show up) because the weights for *some* and *the* are very low; `S1 and S2` are fairly similar to one another, and `S3` is different. By treating *the* and *some* as being less significant than *pasta* and *potatoes* for comparing similarity, we get a better measure of similarity.\nWe can use cosine similarity and TF-IDF weights to compare any items that can be represented as sequences of words. We will use this to calculate how similar two words are. We can represent a word using a **cooccurrence table** – that is, the set of words that occur in the same context, where a context could be an article, a sentence, a tweet, or a window around the word’s position in a text – it could also be defined by requiring the two words to be syntactically related (for example, *eat* and *cake* could be seen as occurring in the same context in *he ate some very rich cake* because *cake* is the object of *ate*, even though they are some way apart in the text). We can either simply count the cooccurrences or we can weigh them using an IDF table if we have one.\nLet’s assume that `getPairs` returns a cooccurrence table of words that have occurred in the same context:\n\n```", "```py\n\nThere are various ways of obtaining such a table. For the next few examples, we will use the fact that the BNC is already tagged to collect open class words (nouns, verbs, adjectives, and adverbs) that occur inside a window of three words on either side of the target word – for example, from the sentence, *It*-PN *is*-VB *often*-AV *said*-VV *that*-CJ *you*-PN *can*-VM *discover*-VV *a*-AT *great*-AJ *deal*-NN, we would get `{'often': {'said': 1}, 'said': {'often': 1}, 'discover': {'great': 1, 'deal': 1}, 'great': {'discover': 1, 'deal': 1}, 'deal': {'discover': 1, 'great': 1}}` because *often* and *said* are within a window of three positions of each other and so are *discover*, *great*, and *deal*. We save this table in `pairs0`.\nWe then make a document frequency table and reduce this so that it only contains the top *N* words (we do this by sorting it, getting the *N* highest-scoring cases, and then reconstructing it as a table), and we use the reduced table to get a cooccurrence table (`pairs1`) that only contains the top *N* words. If we only consider the top 10,000 words, we will get comparisons between most words that we are likely to be interested in and we will reduce the amount of computation to be carried out when constructing the similarity table. We weight the scores in this table by the document frequency for the words that it contains (we use a version of TF-IDF in which we do not take logs since this seems to work better in this case), storing this in `pairs2`. Finally, we convert `pairs2` into a sparse matrix and use `cosine_similarity` to calculate the similarity scores for every word in the matrix:\n\n```", "```py\n\nApplying this to the entire BNC (approximately 100 million words), we get an initial DF table and set of cooccurring pairs with just over 393K entries each, which means that if we do not reduce them to the commonest 10K words, the cooccurrence table would potentially have 393,000**2 entries – that is, about 15G entries. Reducing this so that only the top 10K words are included reduces the potential size of the cooccurrence table to 100M entries, but this table is fairly sparse, with the sparse representation containing just under 500K entries.\nTypical entries in the cooccurrence table look as follows (just showing the highest scoring co-occurring entries for each word). These all look reasonable enough – they are all words that you can imagine cooccurring with the given targets:\n\n```", "```py\n\nCalculating the pairwise similarities between rows in this table is remarkably quick, taking about 1.3 seconds on a standard MacBook with a 2.8 GHz processor. To make use of the similarity table, we have to map words to their indices to get into the matrix and then map indices back to words to interpret the results, but apart from that, finding the “most similar” words to a given target is very simple:\n\n```", "```py\n\nLooking at a set of common words, we can see that the most similar ones have quite a lot in common with the targets, so it seems plausible that calculating word similarity based on whether two words occur in the same contexts may be useful for a range of tasks:\n\n```", "```py\n\nSome of these are just the inflected forms of the originals, which shouldn’t be too surprising – *eat*,  *ate*, *eaten*, and *eating* are all very similar words! The ones that are not just inflected forms of the targets contain some plausible-looking pairs (*cat* and *dog* are returned as being very similar and the matches for *drink* are all things you can drink), along with some oddities. We will return to the question of whether this is useful for our task shortly.\nLatent semantic analysis\nUsing TF-IDF weights makes it possible to discount items that occur in large numbers of contexts, and which therefore are unlikely to be useful when distinguishing between contexts. An alternative strategy is to try to find combinations of weights that produce **fixed points** – that is, those that can be used to recreate the original data. If you remove the least significant parts of such combinations, you can approximate the essence of the original data and use that to calculate similarities.\nWe will learn how to use neural networks for this purpose later. For now, we will consider an approach known as **latent semantic analysis** (**LSA**) (Deerwester et al., 1990), which uses matrix algebra to produce lower-dimensional approximations of the original data. The key here is that given any MxN matrix, A, you can find an MxM matrix, U, a vector, S, of length M where the elements of U are given in decreasing order, and an NxM matrix, V, such that A = (U * S) dot V. U, S,  and V provide a fixed point of the original data. If S’ is obtained from S by setting some of the lower values of S to 0, then (U * S’) dot V becomes an approximation of A, where S’ is of a lower dimension than S.\nAs an example, we will start with a 6x8 array of random integers:\n\n```", "```py\n\nU, S, and V are as follows:\n\n```", "```py\n\nIf we set the last element of S to 0 and calculate (U * S) dot V, we get the following:\n\n```", "```py\n\nThis is a reasonable approximation to the original.\nLSA works by applying this notion to cooccurrence matrices of the kind we have been looking at. Given the size of such matrices, it can be difficult to calculate S in full. So, we must restrict the number of entries that we want on S, rather than calculating the full set and then zeroing some out.\nBy restricting the length of S to 1,000, we get the following nearest neighbors for *cat*, *dog*, *drink*, and *eat*:\n\n```", "```py\n\nThe changes from the original set are not dramatic – the inflected forms of *eat* have been demoted with various things that you can eat appearing high in the list, but apart from that, the changes are not all that significant.\nHowever, calculating the SVD of a cooccurrence matrix, particularly if we allow less common words to appear as columns, becomes infeasible as the matrix gets larger, and hence alternative solutions are required if we want to handle gigabytes of training data, rather than the 100 million words of the BNC. The `gensim` ([https://radimrehurek.com/gensim/intro.xhtml](https://radimrehurek.com/gensim/intro.xhtml)) version of `word2vec`).\nReturning to our task, the problem we were considering was that the training data may not contain all the words that appear in the test data. If a word in the test data should contribute to the emotional tag assigned to a sentence but is missing from the training data, then we cannot calculate its contribution to the emotion of that sentence. We can try to use these notions of similarity to fill in the gaps in our lexicons: if we have a word in the target text that does not appear in the emotion lexicon, we could substitute it with the nearest word according to our similarity metric that does. If the similarity lexicon returns words that have similar emotional associations, then that should improve the recall, and possibly the precision, of our emotion mining algorithms.\nWe can extend the method for calculating the scores for a given tweet like so. The key is that if some word is not in the sentiment lexicon, we use `chooseother` to select the nearest word according to the similarity metric:\n\n```", "```py\n\nThe following results show what happens when we combine a `word2vec` model derived from the entire BNC with the classification algorithm that we get by extracting a lexicon from the training data without stemming. The first table is just the one we had earlier for the English datasets (the `word2vec` model trained on the BNC will only work with the English datasets) with optimal thresholds, repeated here for ease of comparison:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  **0.718**\n |\n  **0.772**\n |\n  **0.744**\n |\n  **0.750**\n |\n  **0.593**\n |\n\n  SEM11-EN\n |\n  **0.474**\n |\n  **0.579**\n |\n  **0.521**\n |\n  **0.520**\n |\n  **0.353**\n |\n\n  WASSA-EN\n |\n  **0.641**\n |\n  **0.703**\n |\n  **0.671**\n |\n  **0.675**\n |\n  **0.505**\n |\n\n  CARER-EN\n |\n  **0.512**\n |\n  **0.633**\n |\n  **0.566**\n |\n  **0.570**\n |\n  **0.395**\n |\n\nFigure 5.22 – Lexicon-based classifier, basic English datasets, optimal thresholds, no stemming, no model\nWhen we try to use a `word2vec` model trained on the entire BNC, we get the following:\n\n   |  **Precision**\n |\n  **Recall**\n |\n  **Micro-F1**\n |\n  **Macro-F1**\n |\n  **Jaccard**\n |\n\n  SEM4-EN\n |\n  0.699\n |\n  0.753\n |\n  0.725\n |\n  0.731\n |\n  0.569\n |\n\n  SEM11-EN\n |\n  0.471\n |\n  0.574\n |\n  0.518\n |\n  0.515\n |\n  0.349\n |\n\n  WASSA-EN\n |\n  0.618\n |\n  0.682\n |\n  0.648\n |\n  0.654\n |\n  0.480\n |\n\n  CARER-EN\n |\n  0.510\n |\n  0.631\n |\n  0.564\n |\n  0.568\n |\n  0.393\n |\n\nFigure 5.23 – Lexicon-based classifier, basic English datasets, optimal thresholds, no stemming, word2vec as the model\nIn every case, using the `word2vec` model makes things worse. Why? We can look at the words that are substituted for missing words and the emotions that they carry:\n\n```", "```py\n\nMost of the substitutions seem reasonable – *cat* is like *kitten*, *fall* is like *plummet*, and *overweight* is like *obese*. The trouble is that while the emotion associated with the substitution is often appropriate for the substitution, it cannot be relied on to be appropriate for the target. It is conceivable that cats are linked to **fear**, but kittens are surely more likely to be linked to **joy**, and while sacred objects might invoke **fear**, ancient ones surely don’t.\nThe problem is that two words that are classified as being similar by one of these algorithms may not have similar emotional associations. The words that co-occur with a verb tend to be the kinds of things that can be involved when the action denoted by that verb is performed, the words that cooccur with a noun tend to be the kinds of actions it can be involved in, and the words that cooccur with an adjective tend to be the kinds of things that can have the property denoted by that adjective. If we look at the performance of our 100M-word `word2vec` model on a collection of emotionally laden words, the results are somewhat surprising:\n\n```", "```py\n\nFigure 5.24 – Most similar words to common emotionally-laden words, 100M-word word2vec model\nIn five out of the seven cases, the most similar word carries exactly the opposite emotions from the target. The problem is that the kinds of things that you can love or hate are very similar, and the kinds of things that are sad or funny are very similar. Because the training corpus contains no information about emotions, its notion of similarity pays no attention to emotions.\nThis is not just an artifact of the way that `word2vec` calculates similarity or of the training set we used. We get very similar results with other algorithms and other training corpora. The following table shows the most similar words for a set of common words and for a set of emotionally-laden words with four algorithms – a simple TF-IDF model trained on the 110 million words in the BNC using a window of three words before and after the target as the “document” in which it appears; the same model after latent semantic analysis using 100 elements of the diagonal; `word2vec` trained on the same corpus; and a version of GloVe trained on a corpus of 6 billion words:\n\n   |  **man**\n |\n  **woman**\n |\n  **king**\n |\n  **queen**\n |\n  **eat**\n |\n  **drink**\n |\n\n  GLOVEMODEL\n |\n  woman\n |\n  girl\n |\n  prince\n |\n  princess\n |\n  consume\n |\n  beer\n |\n\n  W2VMODEL\n |\n  woman\n |\n  girl\n |\n  duke\n |\n  bride\n |\n  cook\n |\n  coffee\n |\n\n  TF-IDFMODEL\n |\n  woman\n |\n  wise\n |\n  emperor\n |\n  grandmother\n |\n  hungry\n |\n  pour\n |\n\n  LSAMODEL\n |\n  priest\n |\n  wise\n |\n  bishop\n |\n  bride\n |\n  forget\n |\n  bath\n |\n\nFigure 5.25 – Nearest neighbors for common words, various models\nThe words that are returned as the nearest neighbors of the targets by the various algorithms are all reasonable enough (you do have to weed out cases where the nearest neighbor is an inflected form of the target: GLoVe is particularly prone to this, with *eats* and *ate*, for instance, being the words that are found to be most similar to *eat*). For nouns, the words that are returned are things that can do, or have done to them, the same kinds of things; for verbs, they are largely actions that can be performed on the same kinds of things (GLoVe and `word2vec` both return things that you can drink for *drink*).\nIf similar words tend to involve, or are involved in, the same kinds of actions, what happens when we look at emotionally laden words?\n\n   |  **love**\n |\n  **like**\n |\n  **hate**\n |\n  **detest**\n |\n  **joy**\n |\n  **sorrow**\n |\n\n  GLOVEMODEL\n |\n  me\n |\n  even\n |\n  hatred\n |\n  despise\n |\n  sadness\n |\n  sadness\n |\n\n  W2VMODEL\n |\n  envy\n |\n  crush\n |\n  despise\n |\n  daresay\n |\n  sorrow\n |\n  sadness\n |\n\n  TF-IDFMODEL\n |\n  hate\n |\n  think\n |\n  love\n |\n  --\n |\n  pleasure\n |\n  --\n |\n\n  LSAMODEL\n |\n  passion\n |\n  want\n |\n  imagine\n |\n  --\n |\n  pleasure\n |\n  --\n |\n\nFigure 5.26 – Nearest neighbors for emotionally laden words\nA number of the nearest neighbors simply carry no emotional weight – *me*, *even*, *think*, and *daresay*. In such cases, the strategy of looking for the nearest word that does carry such a weight would move on to the next case, but since this will produce different results with different lexicons, the effect is unpredictable until we choose a lexicon. In the remaining cases, we see the same phenomenon as before – some of the nearest neighbors carry the same emotions as the target (`word2vec`), `word2vec`), *hate* (TF-IDF), `word2vec`)). GLoVe trained on 6 billion words gives two words that carry the correct emotions, two that carry exactly the wrong ones, and two that carry none; `word2vec` trained on 100 million words gives two that carry the right emotions, two that carry the wrong ones and two that carry none; and TF-IDF and LSA do much the same. Using word similarity models that are trained on corpora that are not marked up for emotions can give very misleading information about emotions, and should only be used with extreme care.\nSummary\nWhat does all this add up to? You can make an emotion mining algorithm by making a lexicon with words marked up for emotions. Doing so by extracting the information from a corpus where texts, rather than words, have been marked will probably do better on target texts of the same kind than by using a lexicon where individual words have been marked. They are both time-consuming, labor-intensive activities, but you are going to have to do something like this because any machine learning algorithm is going to require training data. There are numerous minor variants that you can try – stemming, changing the bias, varying the threshold, or using a similarity metric for filling in gaps. They all produce improvements *under some circumstances*, depending on the nature of the corpus and the task, so it is worth trying combinations of techniques, but they do not produce large improvements. Lexicon-based algorithms form a good starting point, and they have the great advantage of being very easy to implement. To get substantially better performance, we will investigate more sophisticated machine learning algorithms in the following chapters.\nReferences\nTo learn more about the topics that were covered in this chapter, take a look at the following resources:\n\n*   Buckwalter, T. (2007). *Issues in Arabic morphological analysis*. Arabic Computational Morphology, 23–42.\n*   Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990). *Indexing by Latent Semantic Analysis*. Journal of the American Society of Information Science, 41(6), 391–407.\n*   Mikolov, T., Chen, K., Carrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space (1st* *ed.)*. [http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf).\n*   Mohammad, S. M., & Turney, P. D. (2013). *Crowdsourcing a Word-Emotion Association Lexicon*. Computational Intelligence, 29 (3), 436–465.\n*   Pennington, J., Socher, R., & Manning, C. (2014). *GLoVe: Global Vectors for Word Representation*. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1,532–1,543\\. [https://doi.org/10.3115/v1/D14-1162](https://doi.org/10.3115/v1/D14-1162).\n*   Sparck Jones, K. (1972). *A statistical interpretation of term specificity and its application in retrieval*. Journal of Documentation, 28(1), 11–21.\n\n```"]