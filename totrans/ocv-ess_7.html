<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. What Is He Doing? Motion</h1></div></div></div><p class="calibre7">In this chapter, we will show you different techniques related to motion, as estimated from video frames. After a short introduction and definitions, we will show you how to read video frames captured from a camera. Then, we will tackle the all-important Optical Flow technique. In the third section, we will show you different functions that can be used for tracking. The Motion history and Background subtraction techniques are explained in the fourth and fifth sections, respectively. Finally, image alignment with the ECC method is explained. Every example has been developed and tested for the latest version of OpenCV in GitHub. Most of the functions can work in the previous versions equally, leading to some changes that will be discussed. Most of the functions introduced in this chapter are in the <code class="email">video</code> module.</p><div><h3 class="title2"><a id="note31" class="calibre1"/>Note</h3><p class="calibre7">To test the latest source code available in GitHub, go to <a class="calibre1" href="https://github.com/itseez/opencv">https://github.com/itseez/opencv</a> and download the library code as a ZIP file. Then unzip it to a local folder and follow the same steps described in <a class="calibre1" title="Chapter 1. Getting Started" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre12">Getting Started</em>, to compile and install the library.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec49" class="calibre1"/>Motion history</h1></div></div></div><p class="calibre7">Motion<a id="id436" class="calibre1"/> is a very <a id="id437" class="calibre1"/>important topic in Computer Vision. Once we detect and isolate an object or person of interest, we can extract valuable data such as positions, velocity, acceleration, and so on. This information can be used for action recognition, behavior pattern studies, video stabilization, augmented reality, and so on.</p><p class="calibre7">The Optical Flow technique is a pattern of an object's apparent motion. Surfaces and edges in a visual scene are caused by relative motion between an observer and scene or between the camera and the scene. The concept of the Optical Flow technique is central in Computer Vision and is associated with techniques/tasks such as motion detection, object segmentation, time-to-control information, focus of expansion calculations, luminance, motion compensated encoding, and stereo disparity measurement.</p><p class="calibre7">
<strong class="calibre8">Video tracking</strong><a id="id438" class="calibre1"/> consists of locating a moving object (or multiple objects) over time using videos captured from a camera or file. The aim of video tracking is to associate target objects in consecutive video frames. It has a variety of uses, some of which are video editing, medical imaging, traffic control, augmented reality, video communication and compression, security and surveillance, and human-computer interaction.</p><p class="calibre7">
<strong class="calibre8">Motion</strong> templates<a id="id439" class="calibre1"/> were invented at the MIT Media Lab by Bobick and David in 1996. The use of the motion templates is a simple yet robust technique that tracks general movement. OpenCV motion template functions only work with single channel images. A silhouette (or part of a silhouette) of an object is needed. These silhouettes can be obtained in different ways. For example, segmentation techniques can be used to detect the interest object and then perform tracking with motion templates. Another option is to use the Background subtraction technique to detect foreground objects and then track them. There are other techniques too, although, in this chapter, we will see two examples that use the Background subtraction technique.</p><p class="calibre7">
<strong class="calibre8">Background subtraction</strong><a id="id440" class="calibre1"/> is a technique by which an image foreground or region of interest is extracted for further processing, for example, people, cars, text, and so on. The Background subtraction technique is a widely used approach for detecting moving objects in videos captured from static cameras. The essence of the Background subtraction technique is to detect the moving objects from differences between current frames and a reference image taken without target objects present, which is usually called a background image.</p><p class="calibre7">
<strong class="calibre8">Image alignment</strong><a id="id441" class="calibre1"/> can be seen as a mapping between the coordinate systems of two or more images taken from different points of view. The first step is, therefore, the choice of an appropriate geometric transformation that adequately models this mapping. This algorithm can be used in a wide range of applications, such as image registration, object tracking, super-resolution, and visual surveillance by moving cameras.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec50" class="calibre1"/>Reading video sequences</h1></div></div></div><p class="calibre7">To process a<a id="id442" class="calibre1"/> video sequence, we should be able to read each frame. OpenCV has developed an easy-to-use framework that can work with video files and camera input.</p><p class="calibre7">The following code is a <code class="email">videoCamera</code> example that works with a video captured from a video camera. This example is a modification of an example in <a class="calibre1" title="Chapter 1. Getting Started" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre12">Getting Started</em>, and we will use it as the basic structure for other examples in this chapter:</p><div><pre class="programlisting">#include "opencv2/opencv.hpp"

using namespace std;
using namespace cv;

int <strong class="calibre8">videoCamera()</strong>
{
    //1-Open the video camera
    VideoCapture capture(0);

    //Check if video camera is opened
    if(!capture.isOpened()) return 1;

    bool finish = false;
    Mat frame;
    Mat prev_frame;
    namedWindow("Video Camera");

    if(!capture.<strong class="calibre8">read</strong>(prev_frame)) return 1;

    //Convert to gray image
<strong class="calibre8">    cvtColor</strong>(prev_frame,prev_frame,<strong class="calibre8">COLOR_BGR2GRAY</strong>);

    while(!finish)
    {
        //2-Read each frame, if possible
        if(!capture.<strong class="calibre8">read</strong>(frame)) return 1;

        //Convert to gray image
<strong class="calibre8">        cvtColor</strong>(frame ,frame, <strong class="calibre8">COLOR_BGR2GRAY</strong>);

        //Here, we will put other functions 

        imshow("Video Camera", prev_frame);

        //Press Esc to finish
        if(waitKey(1)==27) finish = true;

        prev_frame = frame;
    }
    //Release the video camera
    capture.release();
    return 0;
}

int main( )
{
<strong class="calibre8">    videoCamera()</strong>;
}</pre></div><p class="calibre7">The <a id="id443" class="calibre1"/>preceding code example creates a window that shows you the grayscale video's camera capture. To initiate the capture, an instance of the <code class="email">VideoCapture</code> class<a id="id444" class="calibre1"/> has been created with the zero-based camera index. Then, we check whether the video capture can be successfully initiated. Each frame is then read from the video sequence using <a id="id445" class="calibre1"/>the <code class="email">read</code> method. This video sequence is converted to grayscale using the <code class="email">cvtColor</code> method<a id="id446" class="calibre1"/> with the <code class="email">COLOR_BGR2GRAY</code> parameter<a id="id447" class="calibre1"/> and is displayed on the screen until the user presses the <em class="calibre12">Esc</em> key. Then, the video sequence is finally released. The previous frame is stored because it will be used for some examples that follow.</p><div><h3 class="title2"><a id="note32" class="calibre1"/>Note</h3><p class="calibre7">The <code class="email">COLOR_BGR2GRAY</code> parameter can be used in OpenCV 3.0. In the previous versions, we also had <code class="email">CV_BGR2GRAY</code>.</p></div><p class="calibre7">In the summary, we have shown you a simple method that works with video sequences using a video camera. Most importantly, we have learned how to access each video frame and can now make any type of frame processing.</p><div><h3 class="title2"><a id="note33" class="calibre1"/>Note</h3><p class="calibre7">With regard to video and audio formats supported by OpenCV, more information can be found <a id="id448" class="calibre1"/>at the <a class="calibre1" href="http://ffmpeg.org">ffmpeg.org</a> website, which presents a complete open source and cross-platform solution for audio and video reading, recording, converting, and streaming. The OpenCV classes that work with video files are built on top of this library. The <a class="calibre1" href="http://Xvid.org">Xvid.org</a> website<a id="id449" class="calibre1"/> offers you an open source video codec library based on the MPEG-4 standard for video compression. This codec library has a competitor called DivX, which offers you proprietary but free codec and software tools.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec51" class="calibre1"/>The Lucas-Kanade optical flow</h1></div></div></div><p class="calibre7">The <strong class="calibre8">Lucas-Kanade</strong> (<strong class="calibre8">LK</strong>)<a id="id450" class="calibre1"/> algorithm<a id="id451" class="calibre1"/> was originally proposed in 1981, and it has become one of the most successful methods available in Computer Vision. Currently, this method is typically applied to a subset of key points in the input image. This method assumes that optical flow is a necessary constant in a local neighborhood of the pixel that is under consideration and solves the basic Optical Flow technique equations you can see equation (1), for each pixel (x, y) on that neighborhood. The method also assumes that displacements between two consecutive frames are small and are approximately a way to get an over-constrained system of the considered points:</p><div><pre class="programlisting">
<em class="calibre12">I(x, y, t) = I(x + ∆x, y + ∆y, t + ∆t)            (1)</em>
</pre></div><p class="calibre7">We will now focus on the <a id="id452" class="calibre1"/>
<strong class="calibre8">Pyramidal Lucas-Kanade</strong> method, which estimates the optical flow in a pyramid <a id="id453" class="calibre1"/>using the <code class="email">calcOpticalFlowPyrLK()</code> function. This method first estimates the optical flow at the top of the pyramid, thus avoiding the problems caused by violations of our assumptions of small and coherent motion. The motion estimate from this first level is then used as the starting point to estimate motion at the next level, as shown in the pyramid in the following diagram:</p><div><img src="img/00047.jpeg" alt="The Lucas-Kanade optical flow" class="calibre9"/><div><p class="calibre13">Pyramidal Lucas-Kanade</p></div></div><p class="calibre10"> </p><p class="calibre7">The following<a id="id454" class="calibre1"/> example uses the <code class="email">maxMovementLK</code> function to <a id="id455" class="calibre1"/>implement a motion detector:</p><div><pre class="programlisting">void <strong class="calibre8">maxMovementLK(Mat&amp; prev_frame, Mat&amp; frame)</strong>
{
    // 1-Detect right features to apply the Optical Flow technique
    vector&lt;Point2f&gt; initial_features;
<strong class="calibre8">    goodFeaturesToTrack(prev_frame, initial_features,MAX_FEATURES, 0.1, 0.2 );</strong>

    // 2-Set the parameters
    vector&lt;Point2f&gt;<strong class="calibre8">new_features</strong>;
    vector&lt;uchar&gt;<strong class="calibre8">status</strong>;
    vector&lt;float&gt; err;
    TermCriteria criteria(TermCriteria::COUNT | TermCriteria::EPS, 20, 0.03);
    Size window(10,10);
    int max_level = 3;
    int flags = 0;
    double min_eigT = 0.004;

    // 3-Lucas-Kanade method for the Optical Flow technique
<strong class="calibre8">    calcOpticalFlowPyrLK(prev_frame, frame, initial_features, new_features, status, err, window, max_level, criteria, flags, min_eigT );</strong>

    // 4-Show the results
    double max_move = 0;
    double movement = 0;
    for(int i=0; i&lt;initial_features.size(); i++)
    {
        Point pointA (initial_features[i].x, initial_features[i].y);
        Point pointB(new_features[i].x, new_features[i].y);
        line(prev_frame, pointA, pointB, Scalar(255,0,0), 2);

        movement = norm(pointA-pointB);
        if(movement &gt; max_move)
            max_move = movement;
    }
    if(max_move &gt;<strong class="calibre8">MAX_MOVEMENT</strong>)
    {
        putText(prev_frame,"INTRUDER",Point(100,100),FONT_ITALIC,3,Scalar(255,0,0),5);
        imshow("Video Camera", prev_frame);
        cout &lt;&lt; "Press a key to continue..." &lt;&lt; endl;
        waitKey();
    }
} </pre></div><p class="calibre7">The preceding example shows you a window with each movement. If there is a large movement, a message is displayed on the screen. Firstly, we need to obtain a set of appropriate key points in the image on which we can estimate the optical flow. The <code class="email">goodFeaturesToTrack()</code> function<a id="id456" class="calibre1"/> uses the method that was originally proposed by Shi and Tomasi to solve this problem in a reliable way, although you can also use other functions to detect important and easy-to-track features (see <a class="calibre1" title="Chapter 5. Focusing on the Interesting 2D Features" href="part0042_split_000.html#page">Chapter 5</a>, <em class="calibre12">Focusing on the Interesting 2D Features</em>). <code class="email">MAX_FEATURES</code> is set to <code class="email">500</code> to limit the number of key points. The Lucas-Kanade method <a id="id457" class="calibre1"/>parameters are then set and <code class="email">calcOpticalFlowPyrLK()</code> is called. When the function returns, the status (<code class="email">status</code>) array is checked to see which <a id="id458" class="calibre1"/>points were successfully tracked and that the new set of points (<code class="email">new_features</code>) with the estimated positions is used. Lines are drawn to represent the motion, and if there is a displacement greater than <code class="email">MAX_MOVEMENT</code>—for example—100, a message is shown on the screen. We can see two screen captures, as follows:</p><div><img src="img/00048.jpeg" alt="The Lucas-Kanade optical flow" class="calibre9"/><div><p class="calibre13">Output of the maxMovementLK example</p></div></div><p class="calibre10"> </p><p class="calibre7">Using the modified <code class="email">videoCamera</code> example, we have applied the <code class="email">maxMovementLK()</code> function<a id="id459" class="calibre1"/> to detect large movements:</p><div><pre class="programlisting">...
while(!finish)
{
   capture.read(frame);

   cvtColor(frame,frame,COLOR_BGR2GRAY);

<strong class="calibre8">// Detect Maximum Movement with Lucas-Kanade Method</strong>
<strong class="calibre8">   maxMovementLK(prev_frame, frame);</strong>
...</pre></div><p class="calibre7">This method is <a id="id460" class="calibre1"/>computationally efficient because tracking is only performed on<a id="id461" class="calibre1"/> important or interesting points.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec52" class="calibre1"/>The Gunnar-Farneback optical flow</h1></div></div></div><p class="calibre7">The <strong class="calibre8">Gunnar-Farneback</strong> algorithm<a id="id462" class="calibre1"/> was developed<a id="id463" class="calibre1"/> to produce dense Optical Flow technique results (that is, on a dense grid of points). The first step is to approximate each neighborhood of both frames by quadratic polynomials. Afterwards, considering these quadratic polynomials, a new signal is constructed by a global displacement. Finally, this global displacement is calculated by equating the coefficients in the quadratic polynomials' yields.</p><p class="calibre7">Let's now see the implementation of this method, which <a id="id464" class="calibre1"/>uses the <code class="email">calcOpticalFlowFarneback()</code>function. The following is an example (<code class="email">maxMovementFarneback</code>) that uses this function to detect the maximum movement as shown in the previous example:</p><div><pre class="programlisting">void <strong class="calibre8">maxMovementFarneback(Mat&amp; prev_frame, Mat&amp; frame)</strong>
{
    // 1-Set the Parameters
    Mat optical_flow = Mat(prev_frame.size(), COLOR_BGR2GRAY);
    double pyr_scale = 0.5;
    int levels = 3;
    int win_size = 5;
    int iterations = 5;
    int poly_n = 5;
    double poly_sigma = 1.1;
    int flags = 0;

    // 2-Farneback method for the Optical Flow technique
<strong class="calibre8">    calcOpticalFlowFarneback(prev_frame, frame, optical_flow, pyr_scale, levels, win_size, iterations, poly_n, poly_sigma, flags);</strong>

    // 3-Show the movements
    int max_move = 0;
    for (int i = 1; i &lt;<strong class="calibre8">optical_flow</strong>.rows ; i++)
    {
        for (int j = 1; j &lt;<strong class="calibre8">optical_flow</strong>.cols ; j++)
        {
            Point2f &amp;p = <strong class="calibre8">optical_flow</strong>.at&lt;<strong class="calibre8">Point2f</strong>&gt;(i, j);
            Point pA = Point(round(i + p.x),round(j + p.y));
            Point pB = Point(i, j);
            int move = sqrt(p.x*p.x + p.y*p.y);
            if( move &gt;<strong class="calibre8">MIN_MOVEMENT</strong> )
            {
                line(prev_frame, pA, pB, Scalar(255,0,0),2);
                if ( move &gt; max_move )
                    max_move = move;
            }
        }
    }
    if(max_move &gt;<strong class="calibre8">MAX_MOVEMENT</strong>)
    {
        putText(prev_frame,"INTRUDER",Point(100,100),FONT_ITALIC,3,Scalar(255,0,0),5);
        imshow("Video Camera", prev_frame);
        cout &lt;&lt; "Press a key to continue..." &lt;&lt; endl;
        waitKey();
    }
}</pre></div><p class="calibre7">This function<a id="id465" class="calibre1"/> receives two consecutive frames, estimates the optical flow<a id="id466" class="calibre1"/> with different parameters, and returns an array with the same size as the input frame, where each pixel is actually a point (<code class="email">Point2f</code>) that represents the displacement for that pixel. Firstly, different parameters are set for this function. Of course, you can also use your own criteria to configure the performance. Then, with these parameters, the Optical Flow technique is performed between each two consecutive frames. Consequently, we obtain an array with the estimations for <a id="id467" class="calibre1"/>each pixel, which is <code class="email">optical_flow</code>. Finally, the movements that are greater<a id="id468" class="calibre1"/> than <code class="email">MIN_MOVEMENT</code> are displayed on the screen. If the largest movement is greater than <code class="email">MAX_MOVEMENT</code>, then an <code class="email">INTRUDER</code> message is displayed.</p><p class="calibre7">Understandably, this method is quite slow because the Optical Flow technique is computed over each pixel on the frame. The output of this algorithm is similar to the previous method, although it's much slower.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec53" class="calibre1"/>The Mean-Shift tracker</h1></div></div></div><p class="calibre7">The <strong class="calibre8">Mean-Shift</strong> method<a id="id469" class="calibre1"/> allows you to locate the maximum of a density function given discrete data sampled from that function. It is, therefore, useful for detecting the modes of this density. Mean-Shift is an iterative method, and an initial estimation is needed.</p><p class="calibre7">The algorithm can be used for<a id="id470" class="calibre1"/> visual tracking. In this case, the color histogram of the tracked object is used to compute the confidence map. The simplest of such algorithm would create a confidence map in the new image based on the object histogram taken from the previous image, and Mean-Shift is used to find the peak of the confidence map near the object's previous position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. Next, we show you an example (<code class="email">trackingMeanShift</code>) using this function:</p><div><pre class="programlisting">void <strong class="calibre8">trackingMeanShift(Mat&amp; img, Rect search_window)</strong>
{
    // 1-Criteria to MeanShift function
    TermCriteria criteria(TermCriteria::COUNT | TermCriteria::EPS, 10, 1);

    // 2-Tracking using MeanShift
<strong class="calibre8">meanShift(img, search_window, criteria);</strong>

    // 3-Show the result
    rectangle(img, <strong class="calibre8">search_window</strong>, Scalar(0,255,0), 3);
} </pre></div><p class="calibre7">This example shows you a window with an initial centered rectangle where the tracking is performed. First, the criteria parameter is set. The function that implements the method needs three parameters: the main image, the interest area that we want to search, and the term criteria for different modes of tracking. Finally, a rectangle is obtained from <code class="email">meanShift()</code>, and <code class="email">search_window</code> is drawn on the main image.</p><p class="calibre7">Using a modified <code class="email">videoCamera</code> example, we apply this method for tracking. A static window of the screen is used to search. Of course, you can manually adjust another window or use other functions to detect interest objects and then perform the tracking on them:</p><div><pre class="programlisting">...
while(!finish)
{
   capture.read(frame);

   cvtColor(frame,frame,COLOR_BGR2GRAY);

<strong class="calibre8">// Tracking using MeanShift with an initial search window</strong>
<strong class="calibre8">  Rect search_window(200,150,100,100);</strong>
<strong class="calibre8">  trackingMeanShift(prev_frame, search_window);</strong>
...</pre></div><p class="calibre7">Here, we can see the<a id="id471" class="calibre1"/> following <a id="id472" class="calibre1"/>two screen captures:</p><div><img src="img/00049.jpeg" alt="The Mean-Shift tracker" class="calibre9"/><div><p class="calibre13">Output of the trackingMeanShift example</p></div></div><p class="calibre10"> </p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec54" class="calibre1"/>The CamShift tracker</h1></div></div></div><p class="calibre7">The <code class="email">CamShift</code> (<strong class="calibre8">Continuously Adaptive Mean Shift</strong>) algorithm is an image <a id="id473" class="calibre1"/>segmentation method that was <a id="id474" class="calibre1"/>introduced by Gary Bradski of OpenCV fame in 1998. It differs from <code class="email">MeanShift</code> in that a search window adjusts itself in size. If we have a well-segmented distribution (for example, face features that stay compact), this method will automatically adjust itself to the face sizes as the person moves closer or farther from the camera.</p><div><h3 class="title2"><a id="note34" class="calibre1"/>Note</h3><p class="calibre7">We can <a id="id475" class="calibre1"/>find a <code class="email">CamShift</code> reference at <a class="calibre1" href="http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html">http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html</a>.</p></div><p class="calibre7">We will now see the following example (<code class="email">trackingCamShift</code>) using this method:</p><div><pre class="programlisting">void <strong class="calibre8">trackingCamShift(Mat&amp; img, Rect search_window)</strong>
{
    //1-Criteria to CamShift function
    TermCriteria criteria(TermCriteria::COUNT | TermCriteria::EPS, 10, 1);

    //2-Tracking using CamShift
    RotatedRect found_object = <strong class="calibre8">CamShift(img, search_window, criteria)</strong>;

    //3-Bounding rectangle and show the result
    Rect found_rect = <strong class="calibre8">found_object.boundingRect()</strong>;
    rectangle(img, found_rect, Scalar(0,255,0),3);
}</pre></div><p class="calibre7">This function structure is very similar to the one in the preceding section; the only difference is that a bounding rectangle is returned from <code class="email">CamShift()</code>.</p></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec55" class="calibre1"/>The Motion templates</h1></div></div></div><p class="calibre7">Motion template <a id="id476" class="calibre1"/>is a technique in image processing for finding a small part of an image or silhouette that matches a template image. This template matcher is used to make comparisons with respect to similarity and to examine the likeness or difference. Templates might potentially require sampling of a large number of points. However, it is possible to reduce these numbers of points by reducing the resolution of the search; another technique to improve these templates is to use pyramid images.</p><p class="calibre7">In OpenCV's examples (<code class="email">[opencv_source_code]/samples/c/motempl.c</code>), a related program can be found.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec25" class="calibre1"/>The Motion history template</h2></div></div></div><p class="calibre7">We now <a id="id477" class="calibre1"/>assume that <a id="id478" class="calibre1"/>we have a good silhouette or template. New silhouettes are then captured and overlaid using the current time stamp as the weight. These sequentially fading silhouettes record the history of the previous movement and are thus referred to as the Motion history template. Silhouettes whose time stamp is more than a specified <code class="email">DURATION</code> older than the current time stamp are set to zero. We have created a simple example (<code class="email">motionHistory</code>) using the <code class="email">updateMotionHistory()</code>OpenCV function on two frames as follows:</p><div><pre class="programlisting">void <strong class="calibre8">updateMotionHistoryTemplate(Mat&amp; prev_frame, Mat&amp; frame, Mat&amp; history)</strong>
{
    //1-Calculate the silhouette of difference between the two 
    //frames
    absdiff(frame, prev_frame, prev_frame);

    //2-Applying a threshold on the difference image
    double threshold_val = 100; threshold(prev_frame,prev_frame,threshold_val,255,THRESH_BINARY);

    //3-Calculate the current time
    clock_t aux_time = clock();
    double current_time = (aux_time-INITIAL_TIME)/CLOCKS_PER_SEC;

    //4-Performing the Update Motion history template
<strong class="calibre8">    updateMotionHistory(prev_frame, history, current_time, DURATION)</strong>;
}</pre></div><div><h3 class="title2"><a id="note35" class="calibre1"/>Note</h3><p class="calibre7">The <code class="email">THRESH_BINARY</code> parameter can be used on OpenCV 3.0. In the previous versions, we also had <code class="email">CV_THRESH_BINARY</code>.</p></div><p class="calibre7">This example shows you a window where the motion history is drawn. The first step is to obtain a silhouette; the Background subtraction technique is used for this. The difference in the absolute value is obtained from the two input frames. In the second step, a binary thresholding is applied to remove noise from the silhouette. Then, the current time is obtained. The final step is to perform the updating of the Motion history template using OpenCV's function.</p><p class="calibre7">We have also<a id="id479" class="calibre1"/> set <code class="email">DURATION</code> to <code class="email">5</code>. Note that it is necessary to initialize <code class="email">INITIAL_TIME</code> and <code class="email">history</code>. Besides, we can use this function call from the<a id="id480" class="calibre1"/> modified <code class="email">videoCamera</code> example as follows:</p><div><pre class="programlisting">...
// Calculate the initial time
INITIAL_TIME = clock()/CLOCKS_PER_SEC;

//Create a Mat to save the Motion history template
Mat history(prev_frame.rows, prev_frame.cols, CV_32FC1);
while(!finish)
{
  capture.read(frame);

  cvtColor(frame,frame,COLOR_BGR2GRAY);

// Using Update Motion history template
  updateMotionHistoryTemplate(prev_frame, frame, history);

   imshow("Video Camera", history);
...</pre></div><p class="calibre7">To use the <code class="email">clock()</code> function, which gets the current time, we need to include <code class="email">&lt;ctime&gt;</code>. Some screen captures will be shown in which a person is walking in front of the camera.</p><div><img src="img/00050.jpeg" alt="The Motion history template" class="calibre9"/><div><p class="calibre13">Output of the motionHistory example</p></div></div><p class="calibre10"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec26" class="calibre1"/>The Motion gradient</h2></div></div></div><p class="calibre7">Once the<a id="id481" class="calibre1"/> Motion<a id="id482" class="calibre1"/> templates have a collection of object silhouettes overlaid in time, we can obtain the directions of movement by computing the gradients of the <code class="email">history</code> image. The following example (<code class="email">motionGradient</code>) computes the gradients:</p><div><pre class="programlisting">void <strong class="calibre8">motionGradientMethod(Mat&amp; history, Mat&amp; orientations)</strong>
{
    //1-Set the parameters
    double <strong class="calibre8">max_gradient</strong> = 3.0;
    double <strong class="calibre8">min_gradient</strong> = 1.0;
    //The default 3x3 Sobel filter
    int apertura_size = 3;
    //Distance to show the results
    int <strong class="calibre8">dist</strong> = 20;
    Mat mask = Mat::ones(<strong class="calibre8">history</strong>.rows, <strong class="calibre8">history</strong>.cols, CV_8UC1);

    //2-Calcule motion gradients
<strong class="calibre8">calcMotionGradient(history, mask, orientations, max_gradient, min_gradient, apertura_size);</strong>

    //3-Show the results
    Mat result = Mat::zeros(orientations.rows, orientations.cols, CV_32FC1);
    for (int i=0;i&lt;orientations.rows; i++)
    {
        for (int j=0;j&lt;orientations.cols; j++)
        {
            double angle = 360-orientations.at&lt;float&gt;(i,j);
            if (angle!=360)
            {
                Point point_a(j, i);
                Point point_b(round(j+ cos(angle)*<strong class="calibre8">dist</strong>), round(i+ sin(angle)*<strong class="calibre8">dist</strong>));
                line(result, point_a, point_b, Scalar(255,0,0), 1);
            }
        }
    }
    imshow("Result", result);
}</pre></div><p class="calibre7">A screen <a id="id483" class="calibre1"/>capture is shown with a person moving his head in front of the<a id="id484" class="calibre1"/> camera (see the following screenshot). Each line represents the gradient for each pixel. Different frames also overlap at a <code class="email">t</code> time:</p><div><img src="img/00051.jpeg" alt="The Motion gradient" class="calibre9"/><div><p class="calibre13">Output of the motionGradient example (a person is moving his head in front of the camera).</p></div></div><p class="calibre10"> </p><p class="calibre7">The preceding example shows you a window that displays the directions of movement. As the first step, the parameters are set (the maximum and minimum gradient value to be detected). The second <a id="id485" class="calibre1"/>step uses the <code class="email">calcMotionGradient()</code> function<a id="id486" class="calibre1"/> to obtain a matrix of the gradient direction angles. Finally, to show the results, these angles are drawn on the screen <a id="id487" class="calibre1"/>using a default distance, which is <code class="email">dist</code>. Again, we can use this function from the following modified <code class="email">videoCamera</code> example:</p><div><pre class="programlisting">...
<strong class="calibre8">//Create a Mat to save the Motion history template</strong>
<strong class="calibre8">Mat history(prev_frame.rows, prev_frame.cols, CV_32FC1);</strong>
while(!finish)
{
    capture.read(frame);

    cvtColor(frame,frame,COLOR_BGR2GRAY);

<strong class="calibre8">//Using Update Motion history template</strong>
<strong class="calibre8">    updateMotionHistoryTemplate(prev_frame, frame, history);</strong>

<strong class="calibre8">//Calculate motion gradients</strong>
<strong class="calibre8">Mat orientations = Mat::ones(history.rows, history.cols, CV_32FC1);</strong>
<strong class="calibre8">motionGradientMethod(history, orientations);</strong>
...</pre></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec56" class="calibre1"/>The Background subtraction technique</h1></div></div></div><p class="calibre7">The Background subtraction technique <a id="id488" class="calibre1"/>consists of obtaining the important objects over a background.</p><p class="calibre7">Now, let's see<a id="id489" class="calibre1"/> the methods <a id="id490" class="calibre1"/>available in OpenCV for the Background subtraction technique. Currently, the following four important techniques are required for this task:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre8">MOG</strong> (<strong class="calibre8">Mixture-of-Gaussian</strong>)</li><li class="listitem"><strong class="calibre8">MOG2</strong></li><li class="listitem"><strong class="calibre8">GMG</strong> (<strong class="calibre8">Geometric MultiGrip</strong>)</li><li class="listitem"><strong class="calibre8">KNN</strong> (<strong class="calibre8">K-Nearest Neighbors</strong>)</li></ul></div><p class="calibre7">Next, we are <a id="id491" class="calibre1"/>going to see an<a id="id492" class="calibre1"/> example (<code class="email">backgroundSubKNN</code>) using the<a id="id493" class="calibre1"/> KNN technique:</p><div><pre class="programlisting">#include&lt;opencv2/opencv.hpp&gt;

using namespace cv;
using namespace std;

int backGroundSubKNN()
{
    //1-Set the parameters and initializations
    Mat frame;
    Mat background;
    Mat foreground;
    bool finish = false;
    int history = 500;
    double dist2Threshold = 400.0;
    bool detectShadows = false;
    vector&lt; vector&lt;Point&gt;&gt; contours;
    namedWindow("Frame");
    namedWindow("Background");
    VideoCapture capture(0);

    //Check if the video camera is opened
    if(!capture.isOpened()) return 1;

    //2-Create the background subtractor KNN
    Ptr &lt;BackgroundSubtractorKNN&gt; bgKNN = createBackgroundSubtractorKNN (history, dist2Threshold, detectShadows);

    while(!finish)
    {
        //3-Read every frame if possible
        if(!capture.read(frame)) return 1;

        //4-Using apply and getBackgroundImage method to get
        //foreground and background from this frame
        bgKNN-&gt;apply(frame, foreground);
        bgKNN-&gt;getBackgroundImage(background); 
        //5-Reduce the foreground noise
        erode(foreground, foreground, Mat());
        dilate(foreground, foreground, Mat());

        //6-Find the foreground contours
        findContours(foreground,contours,RETR_EXTERNAL,CHAIN_APPROX_NONE);
        drawContours(frame,contours,-1,Scalar(0,0,255),2);

        //7-Show the results
        imshow("Frame", frame);
        imshow("Background", background);
        moveWindow("Frame", 0, 100);
        moveWindow("Background",800, 100);

        //Press Esc to finish
        if(waitKey(1) == 27) finish = true;
    }
    capture.release();
    return 0;
}

int main()
{
    backGroundSubKNN();
}</pre></div><div><h3 class="title2"><a id="note36" class="calibre1"/>Note</h3><p class="calibre7">The <code class="email">createBackgroundSubtractorKNN</code> method<a id="id494" class="calibre1"/> has only been included in Version 3.0 of OpenCV.</p></div><p class="calibre7">The <a id="id495" class="calibre1"/>Background subtracted frame and screen capture are shown in the following screenshot in which a person is walking in front of the camera:</p><div><img src="img/00052.jpeg" alt="The Background subtraction technique" class="calibre9"/><div><p class="calibre13">Output of the backgroundSubKNN example</p></div></div><p class="calibre10"> </p><p class="calibre7">The<a id="id496" class="calibre1"/> preceding example shows you two windows with the subtracted background images and draws contours of the person found. First, parameters are set as the distance threshold between background and each frame to detect objects (<code class="email">dist2Threshol</code>) and the disabling of the shadow detection (<code class="email">detectShadows</code>). In the second step, using the <code class="email">createBackgroundSubtractorKNN()</code> function, a background subtractor is created and a smart pointer construct is used (<code class="email">Ptr&lt;&gt;</code>) so that we will not have to release it. The third step is to read each frame, if possible. Using the <code class="email">apply()</code> and <code class="email">getBackgroundImage()</code> methods, the foreground and background images are obtained. The fifth step is to reduce the foreground noise by applying a morphological closing operation (in the erosion—<code class="email">erode()</code>—and dilation—<code class="email">dilate()</code>—order). Then, contours are detected on the foreground image and then they're drawn. Finally, the background and current frame image are shown.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec57" class="calibre1"/>Image alignment</h1></div></div></div><p class="calibre7">OpenCV now<a id="id497" class="calibre1"/> implements the ECC algorithm, which is only available as of Version 3.0. This method estimates the geometric transformation (warp) between the input and template frames and returns the warped input frame, which must be close to the first template. The estimated transformation is the one that maximizes the correlation coefficient between the template and the warped input frame. In the OpenCV examples (<code class="email">[opencv_source_code]/samples/cpp/image_alignment.cpp</code>), a related program can be found.</p><div><h3 class="title2"><a id="note37" class="calibre1"/>Note</h3><p class="calibre7">The ECC algorithm<a id="id498" class="calibre1"/> is based on the ECC criterion of the paper <em class="calibre12">Parametric Image Alignment Using Enhanced Correlation Coefficient Maximization</em>. You can find this at <a class="calibre1" href="http://xanthippi.ceid.upatras.gr/people/evangelidis/george_files/PAMI_2008.pdf">http://xanthippi.ceid.upatras.gr/people/evangelidis/george_files/PAMI_2008.pdf</a>.</p></div><p class="calibre7">We are now <a id="id499" class="calibre1"/>going to see an example (<code class="email">findCameraMovement</code>) that uses this ECC technique using the <code class="email">findTransformECC()</code> function:</p><div><pre class="programlisting">#include &lt;opencv2/opencv.hpp&gt;

using namespace cv;
using namespace std;

int findCameraMovement()
{
    //1-Set the parameters and initializations
    bool finish = false;
    Mat frame;
    Mat initial_frame;
    Mat warp_matrix;
    Mat warped_frame;
    int <strong class="calibre8">warp_mode = MOTION_HOMOGRAPHY</strong>;
    TermCriteria criteria(TermCriteria::COUNT | TermCriteria::EPS, 50, 0.001);
    VideoCapture capture(0);
<strong class="calibre8">    Rect rec(100,50,350,350);</strong>   //Initial rectangle
    Mat aux_initial_frame;
    bool follow = false;

    //Check if video camera is opened
    if(!capture.isOpened()) return 1;

    //2-Initial capture
    cout &lt;&lt; "\n Press 'c' key to continue..." &lt;&lt; endl;
    while(!follow)
    {
        if(!capture.read(initial_frame)) return 1;
        cvtColor(initial_frame ,initial_frame, COLOR_BGR2GRAY);
        aux_initial_frame = initial_frame.clone();
        rectangle(aux_initial_frame, rec, Scalar(255,255,255),3);
        imshow("Initial frame", aux_initial_frame);
        if (waitKey(1) == 99) follow = true;
    }
    Mat template_frame(rec.width,rec.height,CV_32F);
    template_frame = initial_frame.colRange(<strong class="calibre8">rec</strong>.x, <strong class="calibre8">rec</strong>.x + <strong class="calibre8">rec</strong>.width).rowRange(<strong class="calibre8">rec</strong>.y, <strong class="calibre8">rec</strong>.y + <strong class="calibre8">rec</strong>.height);
    imshow("Template image", template_frame);

    while(!finish)
    {
        cout &lt;&lt; "\n Press a key to continue..." &lt;&lt; endl;
        waitKey();

<strong class="calibre8">warp_matrix</strong> = Mat::eye(3, 3, CV_32F);

        //3-Read each frame, if possible
        if(!capture.read(frame)) return 1;

        //Convert to gray image
        cvtColor(frame ,frame, COLOR_BGR2GRAY);

        try
        {
            //4-Use findTransformECC function
<strong class="calibre8">            findTransformECC(template_frame, frame, warp_matrix, warp_mode, criteria);</strong>

            //5-Obtain the new perspective
<strong class="calibre8">            warped_frame</strong> = Mat(template_frame.rows, template_frame.cols, CV_32F);
<strong class="calibre8">            warpPerspective (frame, warped_frame, warp_matrix, warped_frame.size(), WARP_INVERSE_MAP + WARP_FILL_OUTLIERS);</strong>
        }
        catch(Exception e) { cout &lt;&lt; "Exception: " &lt;&lt; e.err &lt;&lt; endl;}

        imshow ("Frame", frame);
        imshow ("Warped frame", <strong class="calibre8">warped_frame</strong>);

        //Press Esc to finish
        if(waitKey(1) == 27) finish = true;
    }
    capture.release();
    return 0;
}

main()
{
    findCameraMovement();
}</pre></div><p class="calibre7">Some screen <a id="id500" class="calibre1"/>captures are shown in the following screenshot. The left-column frames represent the initial and template frames. The upper-right image is the current frame and the lower-right image is the warped frame.</p><div><img src="img/00053.jpeg" alt="Image alignment" class="calibre9"/><div><p class="calibre13">Output of the findCameraMovement example.</p></div></div><p class="calibre10"> </p><p class="calibre7">The<a id="id501" class="calibre1"/> code example shows you four windows: the initial template, the initial frame, the current frame, and the warped frame. The first step is to set the initial parameters as <code class="email">warp_mode</code> (<code class="email">MOTION_HOMOGRAPHY</code>). The second step is to check whether the video camera is opened and to obtain a template, which will be calculated using a centered rectangle. When the <em class="calibre12">C</em> key is pressed, this area will be captured as the template. The third step is to read the next frame and convert it to a gray frame. The <code class="email">findTransformECC()</code> function is applied to calculate <code class="email">warp_matrix</code> with this matrix, and using <code class="email">warpPerspective()</code>, the camera movement can be corrected using <code class="email">warped_frame</code>.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec58" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">This chapter covered an important subject in Computer Vision. Motion detection is an essential task, and in this chapter, we have provided the reader with the insight and samples that are required for the most useful methods available in OpenCV: working with video sequences (see the <code class="email">videoCamera</code> example), the Optical Flow technique (see the <code class="email">maxMovementLK</code> and <code class="email">maxMovementFarneback</code> examples), tracking (see the <code class="email">trackingMeanShift</code> and <code class="email">trackingCamShift</code> examples), the Motion templates (see the <code class="email">motionHistory</code> and <code class="email">motionGradient</code> examples), the Background subtraction technique (see the <code class="email">backgroundSubKNN</code> example), and image alignment (see the <code class="email">findCameraMovement</code> example).</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec59" class="calibre1"/>What else?</h1></div></div></div><p class="calibre7">Within the OpenCV libraries, there are other functions that deal with motion. Other Optical Flow technique methods are implemented, such as the Horn and Schunk (<code class="email">cvCalcOpticalFlowHS</code>), block machine (<code class="email">cvCalcOpticalFlowBM</code>), and simple flow (<code class="email">calcOpticalFlowSF</code>) methods. A method to estimate the global movement is also available (<code class="email">calcGlobalOrientation</code>). Finally, there are other methods to obtain backgrounds such as MOG (<code class="email">createBackgroundSubtractorMOG</code>), MOG2 (<code class="email">createBackgroundSubtractorMOG2</code>), and GMG (<code class="email">createBackgroundSubtractorGMG</code>) methods.</p></div></body></html>