<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Failures of Banks - Data Collection</h1>
                </header>
            
            <article>
                
<p>In each model development, we will need to obtain enough data to build the model. It is very common to read the expression g<em>arbage in, garbage out</em>, which relates to the fact that if you develop a model with bad data, the resulting model will be also bad.</p>
<p>Especially in machine learning applications, what we expect is to have a huge amount of data, although in many cases that's not the case. Regardless of the amount of information available, the quality of this data is the most important issue.</p>
<p>Moreover, as a developer, it is important to have structured data, because it can be immediately manipulated. However, data is commonly found in an unstructured form, meaning that it takes a lot of time to process and prepare for development. Many people consider machine learning applications to only be based on the use of new algorithms or techniques using lines of code. The process is more complex than this, and it requires more time spent understanding the data you have and to obtain the maximum of all your observations. Through the real-world cases that we will discuss in this book, we will observe that data collection, cleansing, and preparation are some of the most important and time-consuming steps.</p>
<p class="mce-root"><span>In this chapter, we will explore how to collect data for our problem statements:</span></p>
<ul>
<li>Collecting financial data</li>
<li>Collecting the target variable</li>
<li>Structuring data</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting financial data</h1>
                </header>
            
            <article>
                
<p>We will obtain our data from the <strong>Federal Deposit Insurance Corporation</strong> (<strong>FDIC</strong>) website (<a href="https://www.fdic.gov/">https://www.fdic.gov/</a>). The FDIC is an independent agency led by the US Congress with an aim to maintain the confidence of the people and the stability of the financial system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why FDIC?</h1>
                </header>
            
            <article>
                
<p>FDIC provides deposit insurance to depositors in US commercial banks and savings institutions. Thus, if a US bank fails and closes, the FDIC guarantees that the depositors do not lose their savings. Up to US$250,000 is guaranteed.</p>
<p>The FDIC also examines and supervises certain financial institutions. These institutions are obliged to periodically report detailed information about their financial statements related to the following:</p>
<ul>
<li>Capital level</li>
<li>Solvency</li>
<li>Quality, type, liquidity, and diversification of assets</li>
<li>Loan and investment concentrations</li>
<li>Earnings</li>
<li>Liquidity</li>
</ul>
<p>Information on banks is publicly available on the FDIC website, and we can download it for our purpose. We will find that information is already structured in the so-called <strong>Uniform Bank Performance Reports</strong> (<strong>UBPR</strong>), which includes several ratios combining different accounts from the financial statements.</p>
<p class="mce-root"/>
<p>For example, if you want a UBPR for a particular bank, or if you just wish to view any other UBPR report, you can select <span class="packt_screen">Uniform Bank Performance Report (UBPR)</span> at <a href="https://cdr.ffiec.gov/public/ManageFacsimiles.aspx">https://cdr.ffiec.gov/public/ManageFacsimiles.aspx</a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-678 image-border" src="assets/f168b9e0-cffe-46bc-aa23-9b5483195789.png" style=""/></div>
<p>The <span class="packt_screen">Report</span> drop-down menu allows the selection of the UBPR. We can search for an individual bank using the name or the FDIC certificate number among other alternatives. Additionally, information can be downloaded for all the available banks at the same time by visiting this link at <a href="https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx">https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx</a>.<a href="https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx"/></p>
<p>As an example, the following screenshot shows how it is possible to download the bulk data of financial ratios for 2016 in text format:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-679 image-border" src="assets/f4bcf192-e456-41a4-8c76-d5e182620f67.png" style=""/></div>
<p><span><span>You </span></span>should only select the <span class="packt_screen">UBPR Ratio -- Single Period</span> option, then select the desired date (<span class="packt_screen">12/31/2016</span>), and finally set the output format, for example, <span class="packt_screen">Tab Delimited</span>.</p>
<p>In this exercise, we will need to download many files, one for each year from 2002 to 2016. It is not necessary to download the data if you don't want to. After the application of a relevant step in the code, the R workspace is saved and this backup is available for readers without spending time running the code or downloading information.</p>
<p>In my experience of learning any programming language, spending time finding errors in code while the rest of your co-learners advance is very frustrating. For that reason, these workspaces allow the reader to never be frustrated by a problem with a specific line of code or even a concrete package that does not work properly on our computer.</p>
<p>In this case, information is downloaded in text delimited files, making it easier to upload on to R later. For each year, one ZIP file is downloaded containing several text files. Each of these text files contains quarterly-relevant information on specific areas of a bank. The size of all the ZIP files for the year 2002 to 2016 reaches 800MB.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Listing files</h1>
                </header>
            
            <article>
                
<p>We should create a folder for each year in our computer, where each ZIP file is decompressed.</p>
<p>Once folders are created, we can write the following code in R to list all the folders that we have created:</p>
<pre>myfiles &lt;- list.files(path = "../MachineLearning/Banks_model/Data", pattern = "20",  full.names = TRUE)<br/> <br/> print(myfiles)<br/>##  [1] "../MachineLearning/Banks_model/Data/2002"<br/>##  [2] "../MachineLearning/Banks_model/Data/2003"<br/>##  [3] "../MachineLearning/Banks_model/Data/2004"<br/>##  [4] "../MachineLearning/Banks_model/Data/2005"<br/>##  [5] "../MachineLearning/Banks_model/Data/2006"<br/>##  [6] "../MachineLearning/Banks_model/Data/2007"<br/>##  [7] "../MachineLearning/Banks_model/Data/2008"<br/>##  [8] "../MachineLearning/Banks_model/Data/2009"<br/>##  [9] "../MachineLearning/Banks_model/Data/2010"<br/>## [10] "../MachineLearning/Banks_model/Data/2011"<br/>## [11] "../MachineLearning/Banks_model/Data/2012"<br/>## [12] "../MachineLearning/Banks_model/Data/2013"<br/>## [13] "../MachineLearning/Banks_model/Data/2014"<br/>## [14] "../MachineLearning/Banks_model/Data/2015"<br/>## [15] "../MachineLearning/Banks_model/Data/2016"</pre>
<p>The <kbd>pattern</kbd> option allows us to search for all the folders that contain <kbd>20</kbd> in their names, going through all the folders that we have created before.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding files</h1>
                </header>
            
            <article>
                
<p>Let's read all the <kbd>.txt</kbd> files included in each folder in the <kbd>myfiles</kbd> list. Once <kbd>.txt</kbd> files are read for each year, they are merged together in only one table. This process takes several minutes to finish (in my case, almost 30 minutes):</p>
<pre>library(readr)<br/> <br/> t &lt;- proc.time()<br/> <br/> for (i in 1:length(myfiles)){<br/> <br/> tables&lt;-list()<br/> myfiles &lt;- list.files(path = "../MachineLearning/Banks_model/Data", pattern = "20",  full.names = TRUE)<br/> <br/> filelist &lt;- list.files(path = myfiles[i], pattern = "*",  full.names = TRUE)<br/> filelist&lt;-filelist[1:(length(filelist)-1)]<br/> <br/> for (h in 1:length(filelist)){<br/> <br/> aux = as.data.frame(read_delim(filelist[h],  "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE, skip = 2))<br/> <br/> variables&lt;-colnames(as.data.frame(read_delim(filelist[h],  "\t", escape_double = FALSE, col_names = TRUE, trim_ws = TRUE, skip = 0)))<br/> <br/> colnames(aux)&lt;-variables<br/> <br/> dataset_name&lt;-paste("aux",h,sep='')<br/> tables[[h]]&lt;-assign(dataset_name,aux)<br/> <br/> }<br/> <br/> final_data_name&lt;-paste("year",i+2001,sep='')<br/> union &lt;- Reduce(function(x, y) merge(x, y, all=T,<br/>     by=c("ID RSSD","Reporting Period")), tables, accumulate=F)<br/> <br/> assign(final_data_name,union)<br/> rm(list=ls()[! ls() %in% c(ls(pattern="year*"),"tables","t")])<br/> }<br/> <br/> proc.time() - t</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Hence, it first lists all the folders we have created. Then, it lists all the <kbd>.txt</kbd> files in each folder and reads them into R. Individual <kbd>.txt</kbd> files provide different data frames, which are then merged into a single table. As a result of the code, 16 different tables are created, each of them with information for one specific year.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining results</h1>
                </header>
            
            <article>
                
<p>Let's now merge the yearly tables using the <kbd>rbind</kbd> function. This is possible because all the tables contain exactly the same number of columns:</p>
<pre>rm(tables)<br/> database&lt;-rbind(year2002,year2003,year2004,year2005,year2006,year2007,year2008,year2009,year2010,year2011,year2012,year2013,year2014,year2015,year2016)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing tables</h1>
                </header>
            
            <article>
                
<p>With the <kbd>rm()</kbd> command, we can remove all the tables in the workspace apart from <kbd>database</kbd>:</p>
<pre>rm(list=ls()[! ls() %in% c(ls(pattern="database"))])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowing your observations</h1>
                </header>
            
            <article>
                
<p>The database contains a total of <kbd>420404</kbd> observations and <kbd>1571</kbd> columns:</p>
<pre>print("Number of observations:")<br/>## [1] "Number of observations:"<br/>print(nrow(database))<br/>## [1] 420404<br/>print("Number of columns/variables:")<br/>## [1] "Number of columns/variables:"<br/>ncol(database)<br/>## [1] 1571</pre>
<p>Let's see what the dataset now looks like, or at least, the first few observations and columns:</p>
<pre>head(database[,1:5])<br/> ##   ID RSSD       Reporting Period UBPR1795 UBPR3123.x UBPR4635<br/> ## 1 1000052 12/31/2002 11:59:59 PM      958       1264      996<br/> ## 2 1000100 12/31/2002 11:59:59 PM      -26       2250       33<br/> ## 3 1000276 12/31/2002 11:59:59 PM       46        719       86<br/> ## 4 1000409 12/31/2002 11:59:59 PM    13926      57059    19212<br/> ## 5 1000511 12/31/2002 11:59:59 PM       37        514       86<br/> ## 6 1000557 12/31/2002 11:59:59 PM        0        120       16</pre>
<p>As you can see, the first column is the identifier of each bank. In the second column, the reference date of the financial information is provided. The rest of the columns are codified with the <kbd>UBPR</kbd> prefix and a number. This situation is very common in real situations where a huge number of variables are available and their meanings are unknown. This situation can be very problematic because we do not exactly know if some variables are calculated considering the target variable, or if the variable will be available at the moment when the model will be implemented.</p>
<p>In our case, this issue is not really a problem because you can find a dictionary with the meaning of the variables at <a href="https://cdr.ffiec.gov/CDRDownload/CDR/UserGuide/v96/FFIEC%20UBPR%20Complete%20User%20Guide_2019-01-11.Zip">https://cdr.ffiec.gov/CDRDownload/CDR/UserGuide/v96/FFIEC%20UBPR%20Complete%20User%20Guide_2019-01-11.Zip</a>.</p>
<p>For example, the meaning of the first variable, <kbd>UBPR1795</kbd>, is net credit losses, measuring the total amount of loans that generated losses to a bank because they were not paid.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling duplications</h1>
                </header>
            
            <article>
                
<p>When we joined the different text files into only one table per year, some columns were duplicated because they were included in several text files at the same time. For example, all the ratios included in text files named <kbd>Summary ratios</kbd> will be duplicated in the other text files. In those cases, R assigns a <kbd>.x</kbd> or <kbd>.y</kbd> suffix to the variables.</p>
<p>In the following code, we remove the variables with the suffix <kbd>.x</kbd> because they are duplicated in the database:</p>
<pre>database[,grep(".x",colnames(database))]&lt;-NULL</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>grep</kbd> function searches for the <kbd>.x</kbd> pattern among the columns names. If this pattern is detected in a column, this column will be removed. Additionally, the <kbd>.y</kbd> suffix is removed from the column names:</p>
<pre>var_names&lt;-names(database)<br/> <br/> var_names&lt;-gsub(".y","",var_names)<br/> <br/> colnames(database)&lt;-var_names<br/> <br/> rm(var_names)</pre>
<p>Finally, the import process also creates some erroneous and uninformed variables. The name of these columns starts with <kbd>X</kbd>. These variables are also removed, as follows:</p>
<pre>database[,grep("X",colnames(database))]&lt;-NULL</pre>
<p>Let's save the workspace until this following step:</p>
<pre>save.image("Data1.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating our problem</h1>
                </header>
            
            <article>
                
<p>The database contains a column indicating the date of the financial statements for each bank (called the <kbd>Reporting Period</kbd> field). Each bank can appear several times in the dataset, once a quarter from December 2002 to December 2016.</p>
<p>However, this field is not recognized as a date format in R:</p>
<pre>class(database$'Reporting Period')<br/>## [1] "character"</pre>
<p>Let's transform this field into a date format:</p>
<ol>
<li>First, extract the left part of the <kbd>Reporting Period</kbd> column. The first 10 characters are extracted in a new variable called <kbd>Date</kbd>:</li>
</ol>
<pre style="padding-left: 60px">database$Date&lt;-substr(database$'Reporting Period',1,10)</pre>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">Let's convert this new column into a date using the <kbd>as.Date</kbd> command:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">database$Date&lt;-as.Date(database$Date,"%m/%d/%Y")</pre>
<ol start="3">
<li>Finally, remove the <kbd>Reporting Period</kbd> field as it is no longer relevant:</li>
</ol>
<pre style="padding-left: 60px">database$'Reporting Period'&lt;-NULL</pre>
<p>We have information about all the quarters from 2002 to 2016, but we are only interested in the financial information provided on year-end.</p>
<p>Let's filter the dataset to consider information from December of each year:</p>
<pre>database&lt;-database[as.numeric(format(database$Date, "%m"))==12,]</pre>
<p>After the preceding line of code, our database contains <kbd>110239</kbd> observations:</p>
<pre>print("Observations in the filtered dataset:")<br/>## [1] "Observations in the filtered dataset:"<br/>nrow(database)<br/>## [1] 110239</pre>
<p>In addition, it contains <kbd>1494</kbd> variables, as shown in the following code block:</p>
<pre>print("Columns in the filtered dataset:")<br/>## [1] "Columns in the filtered dataset:"<br/>ncol(database)<br/>## [1] 1494</pre>
<p>At this point, let's save a backup of the workspace:</p>
<pre>save.image("Data2.RData")</pre>
<p>You can now take a look at all the variables in the dataset:</p>
<pre>database_names&lt;-data.frame(colnames(database))</pre>
<p>As the number of variables is substantially high, it is recommended to save the name of variables in an Excel file:</p>
<pre>write.csv(database_names,file="database_names.csv")<br/>rm(database_names)</pre>
<p>As you can see, there are variables in the dataset whose names are a kind of code. And we also know that it is possible to obtain the meaning of each variable in the FDIC website. This situation is really normal, especially in credit risk applications, where information gives details about account movements or transactions.</p>
<p>It is important to understand in some way the meaning of variables, or at least how they are generated. If not, we can include some variables very close to the target variable as predictors, or even include variables that will not be available at the moment of model implementation. However, we know that there is no apparent target in the dataset. So let's collect the target variable for our problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting the target variable</h1>
                </header>
            
            <article>
                
<p>We need to determine whether or not a bank has failed in the past â€“ this will be our target. This information is also available on the FDIC website at <a href="https://www.fdic.gov/bank/individual/failed/banklist.html">https://www.fdic.gov/bank/individual/failed/banklist.html</a>.<a href="https://www.fdic.gov/bank/individual/failed/banklist.html"/></p>
<p>The website includes banks that have failed since October 2000, which covers all our dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-680 image-border" src="assets/0e7f89fb-1b52-410f-a94a-279333b63e4b.png" style=""/></div>
<p>Let's see the steps to achieve this:</p>
<ol>
<li>Download this information into a <kbd>.csv</kbd> file:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">download.file("https://www.fdic.gov/bank/individual/failed/banklist.csv", "failed_banks.csv",method="auto", quiet=FALSE, mode = "wb", cacheOK = TRUE)</pre>
<p style="padding-left: 60px">Even this list is updated periodically, as historical information does not change, but the results are still replicable. Anyway, the file used in the development is also available in the data repository of this book.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Now, upload the downloaded file into R, as follows:</li>
</ol>
<pre style="padding-left: 60px">failed_banks&lt;-read.csv("failed_banks.csv", header=TRUE)</pre>
<ol start="3">
<li>Use the following command to see all the variables and some details about the data contained in the list of failed banks:</li>
</ol>
<pre style="padding-left: 60px">str(failed_banks) </pre>
<ol start="4">
<li>Let's print the first ten rows, as follows:</li>
</ol>
<pre style="padding-left: 60px">head(failed_banks,n=10)# First 10 rows of dataset<br/> ##                                                Bank.Name<br/> ## 1                    Washington Federal Bank for Savings<br/> ## 2        The Farmers and Merchants State Bank of Argonia<br/> ## 3                                    Fayette County Bank<br/> ## 4  Guaranty Bank, (d/b/a BestBank in Georgia &amp; Michigan)<br/> ## 5                                         First NBC Bank<br/> ## 6                                          Proficio Bank<br/> ## 7                          Seaway Bank and Trust Company<br/> ## 8                                 Harvest Community Bank<br/> ## 9                                            Allied Bank<br/> ## 10                          The Woodbury Banking Company<br/> ##                  City ST  CERT               Acquiring.Institution<br/> ## 1             Chicago IL 30570                  Royal Savings Bank<br/> ## 2             Argonia KS 17719                         Conway Bank<br/> ## 3          Saint Elmo IL  1802           United Fidelity Bank, fsb<br/> ## 4           Milwaukee WI 30003 First-Citizens Bank &amp; Trust Company<br/> ## 5         New Orleans LA 58302                        Whitney Bank<br/> ## 6  Cottonwood Heights UT 35495                   Cache Valley Bank<br/> ## 7             Chicago IL 19328                 State Bank of Texas<br/> ## 8          Pennsville NJ 34951 First-Citizens Bank &amp; Trust Company<br/> ## 9            Mulberry AR    91                        Today's Bank<br/> ## 10           Woodbury GA 11297                         United Bank<br/> ##    Closing.Date Updated.Date<br/> ## 1     15-Dec-17    21-Feb-18<br/> ## 2     13-Oct-17    21-Feb-18<br/> ## 3     26-May-17    26-Jul-17<br/> ## 4      5-May-17    22-Mar-18<br/> ## 5     28-Apr-17     5-Dec-17<br/> ## 6      3-Mar-17     7-Mar-18<br/> ## 7     27-Jan-17    18-May-17<br/> ## 8     13-Jan-17    18-May-17<br/> ## 9     23-Sep-16    25-Sep-17<br/> ## 10    19-Aug-16    13-Dec-18</pre>
<p>The file contains these relevant pieces of information:</p>
<ul>
<li>The number of failed banks</li>
<li>The states where these banks were located</li>
<li>The date when they failed</li>
<li>The acquiring institution</li>
</ul>
<p>It will be quite interesting to plot the evolution of failures over time. For that purpose, let's check whether the <kbd>Closing.Date</kbd> column is recognized as a date:</p>
<pre>class(failed_banks$Closing.Date)<br/> ## [1] "factor"</pre>
<p>This column is not a date. Let's convert it to a date by using another command similar to <kbd>as.Date</kbd> using the <kbd>lubridate</kbd> library:</p>
<pre>library(lubridate)<br/>failed_banks$Closing.Date &lt;- dmy(failed_banks$Closing.Date)<br/>class(failed_banks$Closing.Date)<br/> ## [1] "Date"</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structuring data</h1>
                </header>
            
            <article>
                
<p>After having acquired our target variable and knowing our dataset, we can now move on to the actual data collection based on our target. Here, we will try acquiring the data of the bank according to different years as described in the <em>Collecting the target variable<span> </span></em>section.</p>
<p>To do this, we create a new variable extracting only the year when a bank went bankrupt, and then we count the number of banks by year:</p>
<pre>failed_banks$year&lt;-as.numeric(format(failed_banks$Closing.Date, "%Y"))<br/> <br/> Failed_by_Year&lt;-as.data.frame(table(failed_banks$year))<br/> colnames(Failed_by_Year)&lt;-c("year","Number_of_banks")<br/> <br/> print(Failed_by_Year)<br/> ##    year Number_of_banks<br/> ## 1  2000               2<br/> ## 2  2001               4<br/> ## 3  2002              11<br/> ## 4  2003               3<br/> ## 5  2004               4<br/> ## 6  2007               3<br/> ## 7  2008              25<br/> ## 8  2009             140<br/> ## 9  2010             157<br/> ## 10 2011              92<br/> ## 11 2012              51<br/> ## 12 2013              24<br/> ## 13 2014              18<br/> ## 14 2015               8<br/> ## 15 2016               5<br/> ## 16 2017               8</pre>
<p> Let's view our data graphically:</p>
<pre>library(ggplot2)<br/> <br/> theme_set(theme_classic())<br/> <br/> # Plot<br/> g &lt;- ggplot(Failed_by_Year, aes(year, Number_of_banks))<br/> g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +<br/>       labs(title="Number of failed banks over time",<br/>       caption="Source: FDIC list of failed banks")+<br/>       theme(axis.text.x = element_text(angle=65, vjust=0.6))</pre>
<p>The preceding code gives us the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-681 image-border" src="assets/ef3821cc-1c58-42ce-8217-4edcd35e9ae2.png" style=""/></div>
<p>As the preceding graph shows, the number of failed banks increased during the dot-com crisis in 2001 and 2002, and then during the financial crisis that started in 2008.</p>
<p>Now we need to merge the list of failed banks with our database. In the failed banks dataset, there is a column that contains the ID of each bank, specifically the <kbd>Certificate number</kbd> column. This is a number assigned by the FDIC that uniquely identifies institutions and insurance certificates.</p>
<p>Nevertheless, in the other database, which contains financial information, the ID number is called RSSD ID, which is different. This number is a unique identifier assigned to institutions by the Federal Reserve System.</p>
<p>So, how can we join both datasets? We need a mapping between both identifiers. This mapping can also be found on the FDIC website, again in the same part where we previously downloaded the bulk data of all financial statements. Remember, the website can be accessed at <a href="https://cdr.ffiec.gov/public/pws/downloadbulkdata.aspx">https://cdr.ffiec.gov/public/pws/downloadbulkdata.aspx</a>.<a href="https://cdr.ffiec.gov/public/pws/downloadbulkdata.aspx"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>On this website, we'll need to download the <span class="packt_screen">Call Reports -- Single Period</span> files during the relevant period (2002-2016):</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-682 image-border" src="assets/56f8df48-eb9b-4635-bd71-a7dadadde700.png" style=""/></div>
<p>In each of the recently downloaded files, we can find a file with the name <kbd>FFIEC CDR Call Bulk POR mmddyyyy.txt</kbd>.</p>
<p>This file contains all the information about each bank. First, we use them to assign an <kbd>ID RSSD</kbd> number for each bank in the failed bank list. Then, we can join the financial ratios <span><span>with</span></span> the list of failed banks using the <kbd>ID RSSD</kbd> field.</p>
<p>Once you have downloaded the files, list all the available files in your system using the <kbd>list.files</kbd> function.</p>
<p>We need to find all the files that contain <kbd>FFIEC CDR Call Bulk POR</kbd> in their names:</p>
<pre>myfiles &lt;- list.files(path = "../MachineLearning/Banks_model/Data/IDS", pattern = "FFIEC CDR Call Bulk POR",  full.names = TRUE)</pre>
<p>Now, we will read all the files into R and merge them into one data frame called <kbd>IDs</kbd>.</p>
<p>Additionally, a new column is created called <kbd>year</kbd>. This column reflects the year of the corresponding information. We need to store the <kbd>IDs</kbd> and the date because identifiers can change over the time. For example, changes could occur when two banks merge; one of the bank's will disappear in the dataset and the other can maintain the same number or get a new number.</p>
<p>You can create a new empty frame called <kbd>IDs</kbd>, as follows:</p>
<pre>IDs&lt;-matrix("NA",0,4)<br/> colnames(IDs)&lt;-c("ID RSSD","CERT","Name","id_year")<br/> IDs&lt;-as.data.frame(IDs)</pre>
<p>Then, we iteratively read all the text files and merge them together in this <kbd>IDs</kbd> data frame:</p>
<pre>for (i in 1:length(myfiles))<br/><br/> { <br/> aux &lt;- read.delim(myfiles[i])<br/> aux$year&lt;-as.numeric(2000+i)<br/> aux&lt;-aux[,c(1,2,6,ncol(aux))]<br/> colnames(aux)&lt;-c("ID RSSD","CERT","Name","id_year")<br/> IDs&lt;-rbind(IDs,aux)<br/> }</pre>
<p>Let's print the resulting table, as follows:</p>
<pre>head(IDs)<br/> ##   ID RSSD  CERT                             Name id_year<br/> ## 1      37 10057           BANK OF HANCOCK COUNTY    2001<br/> ## 2     242  3850 FIRST COMMUNITY BANK XENIA-FLORA    2001<br/> ## 3     279 28868      MINEOLA COMMUNITY BANK, SSB    2001<br/> ## 4     354 14083                 BISON STATE BANK    2001<br/> ## 5     439 16498                     PEOPLES BANK    2001<br/> ## 6     457 10202                 LOWRY STATE BANK    2001</pre>
<p>Now, a master table containing the <kbd>ID RSSD</kbd> frame and the <kbd>Certificate number</kbd> column of each bank over the time is available.</p>
<p>You can remove the irrelevant information as follows:</p>
<pre>rm(list=setdiff(ls(), c("database","failed_banks","IDs")))</pre>
<p>Next, we'll merge the <kbd>failed banks</kbd> list and the <kbd>IDs</kbd> dataset using the certificate date, but first, we need to convert the certificate numbers into a numeric format in both datasets:</p>
<pre>failed_banks$CERT&lt;-as.numeric(failed_banks$CERT)<br/> <br/> IDs$CERT&lt;-as.numeric(IDs$CERT)</pre>
<p>If we try to merge the list of failed banks with the <kbd>IDs</kbd> dataset, we will find a problem. In the <kbd>failed banks</kbd> list we have a column indicating the year when a bank went bankrupt, which will not be found in the table <kbd>IDs</kbd> if we join both tables using the column year.</p>
<p>As the <kbd>IDs</kbd> snapshot corresponds to December of each year, a failed bank cannot already be found at the end of this specific year.</p>
<p>In order to merge both datasets correctly, create a new variable (<kbd>id_year</kbd>) in the <kbd>failed banks</kbd> dataset, subtracting one year from the <kbd>year</kbd> column:</p>
<pre>failed_banks$id_year&lt;-failed_banks$year-1</pre>
<p>The failed banks are now joined with the <kbd>IDs</kbd> information using the <kbd>merge</kbd> function. Using this function is easy; you only need to specify both tables and the name of the columns used to make the join:</p>
<pre>failed_banks&lt;-merge(failed_banks,IDs,by.x=c("CERT","id_year"),all.x=TRUE)<br/>failed_banks&lt;-failed_banks[,c("CERT","ID RSSD","Closing.Date")]<br/>head(failed_banks)<br/> ##   CERT ID RSSD Closing.Date<br/> ## 1   91   28349   2016-09-23<br/> ## 2  151  270335   2011-02-18<br/> ## 3  182  454434   2010-09-17<br/> ## 4  416    3953   2012-06-08<br/> ## 5  513  124773   2011-05-20<br/> ## 6  916  215130   2014-10-24</pre>
<p>A new backup of the workspace is done as follows:</p>
<pre>save.image("Data3.RData")</pre>
<p>Now, it is possible to merge the database containing financial statements with the list of failed banks and to then create the target variable. We will join both tables using the <kbd>ID RSSD</kbd> identifier:</p>
<pre>database&lt;-merge(database,failed_banks,by=c("ID RSSD"),all.x = TRUE)<br/>## Warning in merge.data.frame(database, failed_banks, by = c("ID RSSD"),<br/> ## all.x = TRUE): column name 'UBPR4340' is duplicated in the result</pre>
<p>Two new columns are included in the database: <kbd>CERT</kbd> and <kbd>Closing.Date</kbd>. The preceding code alerted us to a previously non-detected duplicated column. So, we should remove one of the duplicated columns. Using the <kbd>grep</kbd> function, we will obtain the number of columns where the <kbd>UBPR4340</kbd> variable is present:</p>
<pre>grep("UBPR4340",colnames(database))<br/>## [1]  852 1454</pre>
<p>Remove the second column where the repeated variable appears:</p>
<pre>database[,1454]&lt;-NULL</pre>
<p>When a missing value is found in any of these two new variables (<kbd>CERT</kbd> and <kbd>Closing.Date</kbd>) it implies that this bank continues operating in the US financial system. On the other hand, if a bank contains information in these variables it implies that this bank failed. We can see how many failed observations are in our database:</p>
<pre>nrow(database[!is.na(database$Closing.Date),c('ID RSSD','Date','Closing.Date')])<br/>## [1] 3705</pre>
<p>There are <kbd>3.705</kbd> observations corresponding to failed banks in the dataset. As you can see, the number of failed observations makes up a small part of total observations.</p>
<p>Failed observations do not represent unique failed banks. It means that a failed bank has different financial statements some time before it finally goes to bankrupt. For example, f<span>or the bank mentioned in the following code block, different years of financial information are available. According to our database, this bank went bankrupt in 2010</span>:</p>
<pre>failed_data&lt;-database[!is.na(database$Closing.Date),c('ID RSSD','Date','Closing.Date')]<br/> head(failed_data)<br/> ##     ID RSSD       Date Closing.Date<br/> ## 259    2451 2003-12-31   2010-07-23<br/> ## 260    2451 2007-12-31   2010-07-23<br/> ## 261    2451 2008-12-31   2010-07-23<br/> ## 262    2451 2005-12-31   2010-07-23<br/> ## 263    2451 2004-12-31   2010-07-23<br/> ## 264    2451 2009-12-31   2010-07-23</pre>
<p>We should assess the time horizon of our predictive model. The higher the difference between the information date and the closing date, the lower the expected predictive power of our model. The explanation is quite simple; it is more difficult to forecast the failure of a bank from the current information in, for example, five years' time than in only one or two years' time.</p>
<p>Let's calculate the difference between the closing and the financial statements dates:</p>
<pre>database$Diff&lt;-as.numeric((database$Closing.Date-database$Date)/365)</pre>
<p>What will our target variable be? What do we want to predict? Well, we could develop a model to predict bankruptcies in the next six months, one year, or even five years after the current financial information.</p>
<p>The definition of the target variable should be done according to the purpose of the model, but also taking into account the number of failed banks or <em>bad</em> banks in the sample.</p>
<p>The standard period is different depending on the portfolio, the model's purpose, and the sample of <em>bad</em> banks or the minority class, which should be large enough to develop a robust model.</p>
<p>The definition of the time horizon is very important, determining the objective of our model and its future use.</p>
<p>For example, we can classify as <em>bad</em> those banks that failed less than a year after the financial statements in the dataset:</p>
<pre>database$Default0&lt;-ifelse(database$Diff&gt;=1 | is.na(database$Diff),0,1)</pre>
<p>According to this definition, the number of <em>bad</em> banks will be as follows:</p>
<pre>table(database$Default0)<br/>##<br/> ##      0      1<br/> ## 109763    476</pre>
<p>Only <kbd>476</kbd>  banks in the dataset failed less than one year after the observed financial information.</p>
<p>For example, the following bank failed only half of a year after the observed financial information:</p>
<pre>head(database[database$Default0==1,c('ID RSSD','Date','Closing.Date','Diff')],1)<br/> ##     ID RSSD       Date Closing.Date      Diff<br/> ## 264    2451 2009-12-31   2010-07-23 0.5589041<br/>database$Default0&lt;-NULL</pre>
<p>At this point, a new backup of the workspace is made:</p>
<pre>save.image("Data4.RData")</pre>
<p>In this problem, we have seen that most of the banks are <strong>solvent</strong> and these banks are repeated in the sample several times, although with different financial statements.</p>
<p>However, it is not relevant to keep all the good banks in the sample and increase the importance of <em>bad</em> banks. There are some techniques to deal with this problem.</p>
<p>One of them could be to assign different weights to each good and bad observation in such a way that two classes can be more balanced. This approach, although useful, makes the execution of the machine learning algorithms much more time consuming because we will be using the entire dataset, which in our case, is more than 100,00 observations.</p>
<p>Very unbalanced classes, as we find in this problem, can impact model fitting <span>in a negative way</span>. In order to keep all the observations, it is very common to subsample the data. Three main techniques are usually carried out:</p>
<ul>
<li><strong>Undersampling</strong>: This is perhaps the simplest strategy. It consists of randomly reducing the majority class to the same size of the minority class. By undersampling, the imbalance issue is solved, but in general, we reduce the dataset, especially in cases where the minority class is very scarce. If this is the case, it is very likely that model results will be poor.</li>
<li><strong>Oversampling</strong>: The minority class is randomly selected many times to reach the same size of the majority class. The most common way to do this is to replicate the minority observations several times. Oversampling could be problematic at this point of the problem solution, where we have not already selected what data will be used for training or testing our future algorithms. We will be duplicating examples of the minority that could be found in the future in both training and validation sets, resulting in both overfitting and misleading results.</li>
<li><strong>Other techniques</strong>: Techniques such as <strong>Synthetic Minority Over-sampling Technique</strong> (<strong>SMOTE</strong>) and <strong>Random Over-Sampling Examples</strong> (<strong>ROSE</strong>) reduce the majority class and create artificially new observations in the minority class.</li>
</ul>
<p>In this case, we'll follow a hybrid approach.</p>
<p>To make some of the following steps easier, we'll rename the first column, which contains the identifier of each bank:</p>
<pre class="mce-root">colnames(database)[1]&lt;-"ID_RSSD"</pre>
<p>Now we will treat failed and non-failed banks in a different way. Let's start with the part of the database that contains only the failed banks:</p>
<pre>database_Failed&lt;-database[!is.na(database$Diff),]</pre>
<p>There are <kbd>3705</kbd> observations containing information of failed banks:</p>
<pre>nrow(database_Failed)<br/>## [1] 3705</pre>
<p>Here is what this sample looks like:</p>
<pre>head(database_Failed[,c("ID_RSSD","Date","Diff")])<br/><br/> ##     ID_RSSD       Date      Diff<br/> ## 259    2451 2003-12-31 6.5643836<br/> ## 260    2451 2007-12-31 2.5616438<br/> ## 261    2451 2008-12-31 1.5589041<br/> ## 262    2451 2005-12-31 4.5616438<br/> ## 263    2451 2004-12-31 5.5616438<br/> ## 264    2451 2009-12-31 0.5589041</pre>
<p>As it is displayed, in the list of failed banks we have financial information for several years for the same banks. The closer financial information to the bankruptcy date for each bank will be finally selected.</p>
<p>For that, we create an auxiliary table. This table will contain the minimum distance of a bank observation to the failure date. For that purpose, we'll now use a useful package, <kbd>sqldf</kbd>. This package allows us to write queries as if we are using the SQL language:</p>
<pre>aux&lt;-database_Failed[,c('ID_RSSD','Diff')]<br/><br/>library(sqldf)<br/>aux&lt;-sqldf("SELECT ID_RSSD,<br/>       min(Diff) as min_diff,<br/>       max(Diff) as max_diff<br/>       from aux group by ID_RSSD")<br/> <br/> head(aux)<br/><br/> ##   ID_RSSD  min_diff max_diff<br/> ## 1    2451 0.5589041 7.564384<br/> ## 2    3953 0.4383562 9.443836<br/> ## 3   15536 0.8301370 6.835616<br/> ## 4   16337 0.7506849 7.756164<br/> ## 5   20370 0.4027397 8.408219<br/> ## 6   20866 0.5589041 7.564384</pre>
<p>Now, our sample with failed banks and this auxiliary table are merged together:</p>
<pre>database_Failed&lt;-merge(database_Failed,aux,by=c("ID_RSSD"))</pre>
<p>Then, we will only select observations where the differences between the financial statement date and the closing date are the same as the <kbd>min_diff</kbd> column:</p>
<pre>database_Failed&lt;-database_Failed[database_Failed$Diff==database_Failed$min_diff,]</pre>
<p>The recently created columns are removed as follows:</p>
<pre>database_Failed$min_diff&lt;-NULL<br/>database_Failed$max_diff&lt;-NULL</pre>
<p>Now we want to reduce the number of non-failed banks. For that purpose, we select one year of financial statements for each bank at random.</p>
<p>The total observations of non-failed banks is extracted with the following code:</p>
<pre>database_NonFailed&lt;-database[is.na(database$Diff),]</pre>
<p class="mce-root">To randomly select financial statements, we should follow these steps:</p>
<ol>
<li>First, establish a <kbd>seed</kbd>. A seed is needed to obtain reproducible results when random numbers are generated. Using the same <kbd>seed</kbd> will allow you to obtain the same results as those described in this book:</li>
</ol>
<pre style="padding-left: 60px">set.seed(10)</pre>
<ol start="2">
<li>Generate random numbers; we generate as many random numbers as the number of rows existing in the non-failed banks dataset:</li>
</ol>
<pre style="padding-left: 60px">Random&lt;-runif(nrow(database_NonFailed))</pre>
<ol start="3">
<li>Add the random numbers to the database as a new column:</li>
</ol>
<pre style="padding-left: 60px">database_NonFailed&lt;-cbind(database_NonFailed,Random)</pre>
<ol start="4">
<li>The maximum random number of each bank is calculated and a new data frame called <kbd>max</kbd> is created:</li>
</ol>
<pre style="padding-left: 60px">max&lt;-aggregate(database_NonFailed$Random, by = list(database_NonFailed$ID_RSSD), max)<br/><br/> colnames(max)&lt;-c("ID_RSSD","max")</pre>
<ol start="5">
<li>Join the data frame with the non-failed banks to the <kbd>max</kbd> data frame. Then, only the observations where the random number is the same as the maximum value for each bank is selected:</li>
</ol>
<pre style="padding-left: 60px">database_NonFailed&lt;-merge(database_NonFailed,max,all.x=TRUE)<br/> database_NonFailed&lt;-    database_NonFailed[database_NonFailed$max==database_NonFailed$Random,]</pre>
<ol start="6">
<li>Remove the irrelevant columns as follows:</li>
</ol>
<pre style="padding-left: 60px">database_NonFailed$max&lt;-NULL<br/>database_NonFailed$Random&lt;-NULL</pre>
<p>Using the <kbd>dim</kbd> function, we can obtain the number of observations of non-failed banks. You can see the number of <em>good</em> banks has been significantly reduced:</p>
<pre>dim(database_NonFailed)<br/><br/>## [1] 9654 1496</pre>
<p>There are only <kbd>9654</kbd> observations and <kbd>1496</kbd> variables.</p>
<p>Therefore, we can finally build our dataset to develop our model by combining both previous data frames:</p>
<pre>Model_database&lt;-rbind(database_NonFailed,database_Failed)</pre>
<p>The target variable can now be defined as well:</p>
<pre>Model_database$Default&lt;-ifelse(is.na(Model_database$Diff),0,1)</pre>
<p>The rest of the objects loaded in the workspace can be removed, as follows:</p>
<pre>rm(list=setdiff(ls(), c("Model_database")))</pre>
<p>It is usually possible to use current variables to define new features and then include them in the development. These new variables are usually named <strong>derived variables</strong>. Thus, we can define derived variables as new variables calculated from one or more base variables.</p>
<p>A very intuitive example is the calculation of a variable named <kbd>age</kbd> from a dataset with information about different customers. This variable could be calculated as the difference between the date when this customer is stored in the system and their birth date.</p>
<p>New variables should add useful and non-redundant information that will facilitate the subsequent learning and will help in generalizing steps.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Feature generation should not be confused with feature extraction. Feature extraction is related to dimensionality reduction as it transforms original features and selects a subset from the pool of potential original and derived features that can be used in our model. </p>
<p>However, in the problem we are dealing with, it is not very relevant to build additional variables. We have a very large dataset measuring all relevant aspects in the analysis of a financial institution.</p>
<p>Moreover, in this part of the development, variables that have been included at the data extraction or manipulation phase, but which serve no purpose for the model development, must be dropped.</p>
<p>Therefore, the following variables will be removed:</p>
<pre>Model_database$CERT&lt;-NULL<br/> <br/> Model_database$Closing.Date&lt;-NULL<br/> <br/> Model_database$Diff&lt;-NULL</pre>
<p>All these steps were needed to build our database. You can see how much time we have spent collecting the data, the target variable, and trying to organize all the data in this chapter. In the next chapter, we will start with the analysis of our dataset. You can do one final backup before continuing, as follows:</p>
<pre>save.image("Data5.RData")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have started collecting the data needed to develop our model to predict bank failures. In this case, we have downloaded a large amount of data and we have structured it. Moreover, we have created our target variable. At the end of this chapter, you should have learned that data collection is the first and one of the most important steps in the model development. When you deal with your own problems, take time to understand the problem and then think about what kind of data you need and how to obtain it. In the next chapter, we will do a descriptive analysis of the data that we have acquired.</p>


            </article>

            
        </section>
    </body></html>