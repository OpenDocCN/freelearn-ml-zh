<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Detecting and Recognizing Objects"><div class="titlepage" id="aid-1ENBI2"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Detecting and Recognizing Objects</h1></div></div></div><p>This chapter will introduce the concept of detecting and recognizing objects, which is one of the most common challenges in computer vision. You've come this far in the book, so at this stage, you're wondering how far are you from mounting a computer in your car that will give you information about cars and people surrounding you through the use of a camera. Well, You're not too far from your goal, actually.</p><p>In this chapter, we will expand on the concept of object detection, which we initially explored when talking about recognizing faces, and adapt it to all sorts of real-life objects, not just faces.</p><div class="section" title="Object detection and recognition techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec43"/>Object detection and recognition techniques</h1></div></div></div><p>We made a distinction in <a class="link" title="Chapter 5. Detecting and Recognizing Faces" href="part0043.xhtml#aid-190861">Chapter 5</a>, <span class="emphasis"><em>Detecting and Recognizing Faces</em></span>, which we'll reiterate for clarity: detecting an object is the ability of a program to determine if a certain region of an image <a id="id383" class="indexterm"/>contains an unidentified object, and recognizing is the ability of a program to identify this object. Recognizing normally only occurs in areas of interest where an object has been detected, for example, we have attempted to recognize faces on the areas of an image that contained a face in the first place.</p><p>When it comes to recognizing and detecting objects, there are a number of techniques used in computer vision, which we'll be examining:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Histogram of Oriented Gradients</li><li class="listitem">Image pyramids</li><li class="listitem">Sliding windows</li></ul></div><p>Unlike feature detection algorithms, these are not mutually exclusive techniques, rather, they are complimentary. You can perform a <span class="strong"><strong>Histogram of Oriented Gradients</strong></span> (<span class="strong"><strong>HOG</strong></span>) while applying the sliding windows technique.</p><p>So, let's take a look at <a id="id384" class="indexterm"/>HOG first and understand what it is.</p><div class="section" title="HOG descriptors"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec40"/>HOG descriptors</h2></div></div></div><p>HOG is a <a id="id385" class="indexterm"/>feature descriptor, so it belongs to the same family of algorithms, such as SIFT, SURF, and ORB.</p><p>It is <a id="id386" class="indexterm"/>used in image and video processing to detect objects. Its internal mechanism is really clever; an image is divided into portions and a gradient for each portion is calculated. We've observed a similar approach when we talked about face recognition through LBPH.</p><p>HOG, however, calculates histograms that are not based on color values, rather, they are based on gradients. As HOG is a feature descriptor, it is capable of delivering the type of information that is vital for feature matching and object detection/recognition.</p><p>Before diving into the technical details of how HOG works, let's first take a look at how HOG <span class="emphasis"><em>sees</em></span> the world; here is an image of a truck:</p><div class="mediaobject"><img src="../Images/image00231.jpeg" alt="HOG descriptors"/></div><p style="clear:both; height: 1em;"> </p><p>This is its HOG version:</p><div class="mediaobject"><img src="../Images/image00232.jpeg" alt="HOG descriptors"/></div><p style="clear:both; height: 1em;"> </p><p>You can easily recognize the wheels and the main structure of the vehicle. So, what is HOG <span class="emphasis"><em>seeing</em></span>? First of all, you can see how the image is divided into cells; these are 16x16 pixels cells. Each cell contains a visual representation of the calculated gradients of color in eight directions (N, NW, W, SW, S, SE, E, and NE).</p><p>These <a id="id387" class="indexterm"/>eight values contained in each cell are the famous histograms. Therefore, a single cell gets a <a id="id388" class="indexterm"/>unique <span class="emphasis"><em>signature</em></span>, which you can mentally visualize to be somewhat like this:</p><div class="mediaobject"><img src="../Images/image00233.jpeg" alt="HOG descriptors"/></div><p style="clear:both; height: 1em;"> </p><p>The extrapolation of histograms into descriptors is quite a complex process. First, local histograms for each cell are calculated. The cells are grouped into larger regions called blocks. These blocks can be made of any number of cells, but Dalal and Triggs found that 2x2 cell blocks yielded the best results when performing people detection. A block-wide vector is created so that it can be normalized, accounting for variations in illumination and shadowing (a single cell is too small a region to detect such variations). This improves the accuracy of detection as it reduces the illumination and shadowing difference between the sample and the block being examined.</p><p>Simply comparing cells in two images would not work unless the images are identical (both in terms of size and data).</p><p>There are <a id="id389" class="indexterm"/>two main problems to resolve:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Location</li><li class="listitem">Scale</li></ul></div><div class="section" title="The scale issue"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec24"/>The scale issue</h3></div></div></div><p>Imagine, for <a id="id390" class="indexterm"/>example, if your sample was a detail (say, a bike) extrapolated from a larger image, and you're trying to compare the two pictures. You would not obtain the same gradient signatures and the detection would fail (even though the bike is in both pictures).</p></div><div class="section" title="The location issue"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec25"/>The location issue</h3></div></div></div><p>Once we've <a id="id391" class="indexterm"/>resolved the scale problem, we have another obstacle in our path: a potentially detectable object can be anywhere in the image, so we need to scan the entire image in portions to make sure we can identify areas of interest, and within these areas, try to detect objects. Even if a sample image and object in the image are of identical size, there needs to be a way to instruct OpenCV to locate this object. So, the rest of the image is discarded and a comparison is made on potentially matching regions.</p><p>To obviate these problems, we need to familiarize ourselves with the concepts of image pyramid and sliding windows.</p><div class="section" title="Image pyramid"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec01"/>Image pyramid</h4></div></div></div><p>Many of the <a id="id392" class="indexterm"/>algorithms used in computer <a id="id393" class="indexterm"/>vision utilize a concept called <span class="strong"><strong>pyramid</strong></span>.</p><p>An image pyramid is a multiscale representation of an image. This diagram should help you understand this concept:</p><div class="mediaobject"><img src="../Images/image00234.jpeg" alt="Image pyramid"/></div><p style="clear:both; height: 1em;"> </p><p>A multiscale representation of an image, or an image pyramid, helps you resolve the problem of detecting objects at different scales. The importance of this concept is easily explained through real-life hard facts, such as it is extremely unlikely that an object will appear in an image at the exact scale it appeared in our sample image.</p><p>Moreover, you <a id="id394" class="indexterm"/>will learn that object classifiers (utilities that allow you to detect objects in OpenCV) need <span class="emphasis"><em>training</em></span>, and this training is provided through image databases made up of positive matches and negative matches. Among the positives, it is again unlikely that the object we want to identify will appear in the same scale throughout the training dataset.</p><p>We've got it, Joe. We need to take scale out of the equation, so now let's examine how an image pyramid is built.</p><p>An image pyramid is built through the following process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Take an image.</li><li class="listitem">Resize (smaller) the image using an arbitrary scale parameter.</li><li class="listitem">Smoothen the image (using Gaussian blurring).</li><li class="listitem">If the image is larger than an arbitrary minimum size, repeat the process from step 1.</li></ol><div style="height:10px; width: 1px"/></div><p>Despite exploring image pyramids, scale ratio, and minimum sizes only at this stage of the book, you've already dealt with them. If you recall <a class="link" title="Chapter 5. Detecting and Recognizing Faces" href="part0043.xhtml#aid-190861">Chapter 5</a>, <span class="emphasis"><em>Detecting and Recognizing Faces</em></span>, we used the <code class="literal">detectMultiScale</code> method of the <code class="literal">CascadeClassifier</code> object.</p><p>Straight away, <code class="literal">detectMultiScale</code> doesn't sound so obscure anymore; in fact, it has become self-explanatory. The cascade classifier object attempts at detecting an object at different scales of an input image. The second piece of information that should become much clearer is the <code class="literal">scaleFactor</code> parameter of the <code class="literal">detectMultiScale()</code> method. This parameter represents the ratio at which the image will be resampled to a smaller size at each step of the pyramid.</p><p>The smaller the <code class="literal">scaleFactor</code> parameter, the more layers in the pyramid, and the slower and more computationally intensive the operation will be, although—to an extent—more accurate in results.</p><p>So, by now, you <a id="id395" class="indexterm"/>should have an understanding of what an image pyramid is, and why it is used in computer vision. Let's now move on to sliding windows.</p></div><div class="section" title="Sliding windows"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec02"/>Sliding windows</h4></div></div></div><p>
<span class="strong"><strong>Sliding windows</strong></span> is <a id="id396" class="indexterm"/>a technique used in computer vision that consists of examining the shifting portions of an image (sliding windows) and operating detection on those using image pyramids. This is done so that an object can be detected at a multiscale level.</p><p>Sliding windows <a id="id397" class="indexterm"/>resolves location issues by scanning smaller regions of a larger image, and then repeating the scanning on different scales of the same image.</p><p>With this technique, each image is decomposed into portions, which allows discarding portions that are unlikely to contain objects, while the remaining portions are classified.</p><p>There is one problem that emerges with this approach, though: <span class="strong"><strong>overlapping regions</strong></span>.</p><p>Let's expand a little bit on this concept to clarify the nature of the problem. Say, you're operating face detection on an image and are using sliding windows.</p><p>Each window slides off a few pixels at a time, which means that a sliding window happens to be a positive match for the same face in four different positions. Naturally, we don't want to report four matches, rather only one; furthermore, we're not interested in the portion of the image with a good score, but simply in the portion with the highest score.</p><p>Here's where non-maximum suppression comes into play: given a set of overlapping regions, we can suppress all the regions that are not classified with the maximum score.</p></div></div><div class="section" title="Non-maximum (or non-maxima) suppression"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Non-maximum (or non-maxima) suppression</h3></div></div></div><p>Non-maximum (or non-maxima) suppression is a technique that suppresses all the results that <a id="id398" class="indexterm"/>relate to the same area of an image, which are not the maximum score for a particular area. This is because similarly colocated windows tend to have higher scores and <a id="id399" class="indexterm"/>overlapping areas are significant, but we are only interested in the window with the best result, and discarding overlapping windows with lower scores.</p><p>When examining an image with sliding windows, you want to make sure to retain the best window of a bunch of windows, all overlapping around the same subject.</p><p>To do this, you determine that all the windows with more than a threshold, <span class="emphasis"><em>x</em></span>, in common will be thrown into the non-maximum suppression operation.</p><p>This is quite complex, but it's also not the end of this process. Remember the image pyramid? We're scanning the image at smaller scales iteratively to make sure to detect objects in different scales.</p><p>This means that you will obtain a series of windows at different scales, then, compute the size of a window obtained in a smaller scale as if it were detected in the original scale, and, finally, throw this window into the original mix.</p><p>It does <a id="id400" class="indexterm"/>sound a bit complex. Thankfully, we're not the first to come across this problem, which has been resolved in several ways. The fastest algorithm in my experience was implemented by Dr. Tomasz Malisiewicz at <a class="ulink" href="http://www.computervisionblog.com/2011/08/blazing-fast-nmsm-from-exemplar-svm.html">http://www.computervisionblog.com/2011/08/blazing-fast-nmsm-from-exemplar-svm.html</a>. The example is in MATLAB, but in the application example, we will obviously use a Python version of it.</p><p>The general <a id="id401" class="indexterm"/>approach behind non-maximum suppression is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Once an image pyramid has been constructed, scan the image with the sliding window approach for object detection.</li><li class="listitem">Collect all the current windows that have returned a positive result (beyond a certain arbitrary threshold), and take a window, <code class="literal">W</code>, with the highest response.</li><li class="listitem">Eliminate all windows that overlap <code class="literal">W</code> significantly.</li><li class="listitem">Move on to the next window with the highest response and repeat the process for the current scale.</li></ol><div style="height:10px; width: 1px"/></div><p>When this process is complete, move up the next scale in the image pyramid and repeat the preceding process. To make sure windows are correctly represented at the end of the entire non-maximum suppression process, be sure to compute the window size in relation to the original size of the image (for example, if you detect a window at 50 percent scale of the original size in the pyramid, the detected window will actually be four times the size in the original image).</p><p>At the end of this process, you will have a set of maximum scored windows. Optionally, you can check for windows that are entirely contained in other windows (like we did for the people detection process at the beginning of the chapter) and eliminate those.</p><p>Now, how do we determine the score of a window? We need a classification system that determines whether a certain feature is present or not and a confidence score for this classification. This <a id="id402" class="indexterm"/>is where <span class="strong"><strong>support vector machines</strong></span> (<span class="strong"><strong>SVM</strong></span>) comes into play.</p></div><div class="section" title="Support vector machines"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Support vector machines</h3></div></div></div><p>Explaining <a id="id403" class="indexterm"/>in detail what an SVM is and does is beyond the scope of this book, but suffice it to say, SVM is an algorithm that—given labeled training data–enables the classification of this data by outputting an optimal <span class="emphasis"><em>hyperplane</em></span>, which, in plain English, is the optimal plane that divides differently classified data. A visual representation will help you understand this:</p><div class="mediaobject"><img src="../Images/image00235.jpeg" alt="Support vector machines"/></div><p style="clear:both; height: 1em;"> </p><p>Why is it so helpful in computer vision and object detection in particular? This is due to the fact that finding the optimal division line between pixels that belong to an object and those that don't is a vital component of object detection.</p><p>The SVM model has been around since the early 1960s; however, the current form of its implementation originates in a 1995 paper by Corinna Cortes and Vadimir Vapnik, which is available at <a class="ulink" href="http://link.springer.com/article/10.1007/BF00994018">http://link.springer.com/article/10.1007/BF00994018</a>.</p><p>Now that we <a id="id404" class="indexterm"/>have a good understanding of the concepts involved in object detection, we can start looking at a few examples. We will start from built-in functions and evolve into training our own custom object detectors.</p></div></div><div class="section" title="People detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec41"/>People detection</h2></div></div></div><p>OpenCV <a id="id405" class="indexterm"/>comes with <code class="literal">HOGDescriptor</code> that performs people detection.</p><p>Here's a pretty straightforward example:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

def is_inside(o, i):
    ox, oy, ow, oh = o
    ix, iy, iw, ih = i
    return ox &gt; ix and oy &gt; iy and ox + ow &lt; ix + iw and oy + oh &lt; iy + ih

def draw_person(image, person):
  x, y, w, h = person
  cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 2)

img = cv2.imread("../images/people.jpg")
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

found, w = hog.detectMultiScale(img)

found_filtered = []
for ri, r in enumerate(found):
    for qi, q in enumerate(found):
        if ri != qi and is_inside(r, q):
            break
    else:
        found_filtered.append(r)

for person in found_filtered:
  draw_person(img, person)

cv2.imshow("people detection", img)
cv2.waitKey(0)
cv2.destroyAllWindows()</pre></div><p>After the usual imports, we define two very simple functions: <code class="literal">is_inside</code> and <code class="literal">draw_person</code>, which perform two minimal tasks, namely, determining whether a rectangle is fully contained in another rectangle, and drawing rectangles around detected people.</p><p>We then load the <a id="id406" class="indexterm"/>image and create <code class="literal">HOGDescriptor</code> through a very simple and self-explanatory code:</p><div class="informalexample"><pre class="programlisting">cv2.HOGDescriptor()</pre></div><p>After this, we specify that <code class="literal">HOGDescriptor</code> will use a default people detector.</p><p>This is done through the <code class="literal">setSVMDetector()</code> method, which—after our introduction to SVM—sounds less obscure than it may have if we hadn't introduced SVMs.</p><p>Next, we apply <code class="literal">detectMultiScale</code> on the loaded image. Interestingly, unlike all the face detection algorithms, we don't need to convert the original image to grayscale before applying any form of object detection.</p><p>The detection method will return an array of rectangles, which would be a good enough source of information for us to start drawing shapes on the image. If we did this, however, you would notice something strange: some of the rectangles are entirely contained in other rectangles. This clearly indicates an error in detection, and we can safely assume that a rectangle entirely inside another one can be discarded.</p><p>This is precisely the <a id="id407" class="indexterm"/>reason why we defined an <code class="literal">is_inside</code> function, and why we iterate through the result of the detection to discard false positives.</p><p>If you run the script yourself, you will see rectangles around people in the image.</p></div><div class="section" title="Creating and training an object detector"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec42"/>Creating and training an object detector</h2></div></div></div><p>Using built-in <a id="id408" class="indexterm"/>features makes it easy to come up with a quick prototype for an application, and we're all very grateful to the OpenCV developers for making great features, such as face detection or people detection readily available (truly, we are).</p><p>However, whether <a id="id409" class="indexterm"/>you are a hobbyist or a computer vision professional, it's unlikely that you will only deal with people and faces.</p><p>Moreover, if you're like me, you wonder how the people detector feature was created in the first place and if you can improve it. Furthermore, you may also wonder whether you can apply the same concepts to detect the most diverse type of objects, ranging from cars to goblins.</p><p>In an enterprise environment, you may have to deal with very specific detection, such as registration plates, book covers, or whatever your company may deal with.</p><p>So, the question is, how do we come up with our own classifiers?</p><p>The answer lies in SVM and bag-of-words technique.</p><p>We've already talked about HOG and SVM, so let's take a closer look at bag-of-words.</p><div class="section" title="Bag-of-words"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec28"/>Bag-of-words</h3></div></div></div><p>
<span class="strong"><strong>Bag-of-words</strong></span> (<span class="strong"><strong>BOW</strong></span>) is a concept that was not initially intended for computer vision, rather, we use an <a id="id410" class="indexterm"/>evolved version of <a id="id411" class="indexterm"/>this concept in the context of computer vision. So, let's first talk about its basic version, which—as you may have guessed— originally belongs to the field of language analysis and information retrieval.</p><p>BOW is the technique by which we assign a count weight to each word in a series of documents; we then rerepresent these documents with vectors that represent these set of counts. Let's look at an example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Document 1</strong></span>: <code class="literal">I like OpenCV and I like Python</code></li><li class="listitem"><span class="strong"><strong>Document 2</strong></span>: <code class="literal">I like C++ and Python</code></li><li class="listitem"><span class="strong"><strong>Document 3</strong></span>: <code class="literal">I don't like artichokes</code></li></ul></div><p>These three <a id="id412" class="indexterm"/>documents allow us to build a dictionary (or codebook) with these values:</p><div class="informalexample"><pre class="programlisting">{
    I: 4,
    like: 4,
    OpenCV: 2,
    and: 2,
    Python: 2,
    C++: 1,
    dont: 1,
    artichokes: 1
}</pre></div><p>We have eight entries. Let's now rerepresent the original documents using eight-entry vectors, each vector containing all the words in the dictionary with values representing the count for each term in the document. The vector representation of the preceding three sentences is as follows:</p><div class="informalexample"><pre class="programlisting">[2, 2, 1, 1, 1, 0, 0, 0]
[1, 1, 0, 1, 1, 1, 0, 0]
[1, 1, 0, 0, 0, 0, 1, 1]</pre></div><p>This kind of representation of documents has many effective applications in the real world, such as spam filtering.</p><p>These vectors can be conceptualized as a histogram representation of documents or as a feature (the same way we extracted features from images in previous chapters), which can be used to train classifiers.</p><p>Now that we have a grasp of the basic concept of BOW or <span class="strong"><strong>bag of visual words</strong></span> (<span class="strong"><strong>BOVW</strong></span>) in computer vision, let's see how this applies to the world of computer vision.</p></div><div class="section" title="BOW in computer vision"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec29"/>BOW in computer vision</h3></div></div></div><p>We are by <a id="id413" class="indexterm"/>now familiar with the concept of image features. We've used feature extractors, such as SIFT, and SURF, to extract features from images so that we could match these features in another image.</p><p>We've also <a id="id414" class="indexterm"/>familiarized ourselves with the concept of codebook, and we know about SVM, a model that can be fed a set of features and utilizes complex algorithms to classify train data, and can predict the classification of new data.</p><p>So, the implementation of a BOW approach will involve the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Take a sample dataset.</li><li class="listitem">For each image in the dataset, extract descriptors (with SIFT, SURF, and so on).</li><li class="listitem">Add each descriptor to the BOW trainer.</li><li class="listitem">Cluster the descriptors to <span class="emphasis"><em>k</em></span> clusters (okay, this sounds obscure, but bear with me) whose centers (centroids) are our visual words.</li></ol><div style="height:10px; width: 1px"/></div><p>At this point, we have a dictionary of visual words ready to be used. As you can imagine, a large dataset will help make our dictionary richer in visual words. Up to an extent, the more words, the better!</p><p>After this, we are ready to test our classifier and attempt detection. The good news is that the process is very similar to the one outlined previously: given a test image, we can extract features and quantize them based on their distance to the nearest centroid to form a histogram.</p><p>Based on this, we can attempt to recognize visual words and locate them in the image. Here's a visual representation of the BOW process:</p><div class="mediaobject"><img src="../Images/image00236.jpeg" alt="BOW in computer vision"/></div><p style="clear:both; height: 1em;"> </p><p>This is the point in the chapter when you have built an appetite for a practical example, and are rearing to code. However, before proceeding, I feel that a quick digression into the theory of <a id="id415" class="indexterm"/>the k-means clustering is necessary so that you can fully understand how visual words are created, and gain a better understanding of the process of object detection using BOW and SVM.</p><div class="section" title="The k-means clustering"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec03"/>The k-means clustering</h4></div></div></div><p>The k-means <a id="id416" class="indexterm"/>clustering is a method of vector <a id="id417" class="indexterm"/>quantization to perform data analysis. Given a dataset, <span class="emphasis"><em>k</em></span> represents the number of clusters in which the dataset is going to be divided. The term "means" refers to the mathematical concept of mean, which is pretty basic, but for the sake of clarity, it's what people commonly refer to as average; when visually represented, the mean of a cluster is its <span class="strong"><strong>centroid</strong></span> or the geometrical center of points in the cluster.</p><div class="note" title="Note"><h3 class="title"><a id="note24"/>Note</h3><p>
<span class="strong"><strong>Clustering</strong></span> <a id="id418" class="indexterm"/>refers to the grouping of points in a dataset into clusters.</p></div><p>One of the classes we will be using to perform object detection is called <code class="literal">BagOfWordsKMeansTrainer</code>; by now, you should able to deduce what the responsibility of this class is to create:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"<code class="literal">kmeans()</code> -based class to train a visual vocabulary using the bag-of-words approach"</em></span></p></blockquote></div><p>This is as per the <a id="id419" class="indexterm"/>OpenCV documentation.</p><p>Here's a <a id="id420" class="indexterm"/>representation of a k-means clustering operation with five clusters:</p><div class="mediaobject"><img src="../Images/image00237.jpeg" alt="The k-means clustering"/></div><p style="clear:both; height: 1em;"> </p><p>After this long theoretical <a id="id421" class="indexterm"/>introduction, we can look at an example, and start training our object detector.</p></div></div></div></div></div>
<div class="section" title="Detecting cars"><div class="titlepage" id="aid-1FLS42"><div><div><h1 class="title"><a id="ch07lvl1sec44"/>Detecting cars</h1></div></div></div><p>There is no <a id="id422" class="indexterm"/>virtual limit to the type of objects you can detect in your images and videos. However, to obtain an acceptable level of accuracy, you need a sufficiently large dataset, containing train images that are identical in size.</p><p>This would be a time consuming operation if we were to do it all by ourselves (which is entirely possible).</p><p>We can avail of ready-made datasets; there are a number of them freely downloadable from various sources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>The University </strong></span><a id="id423" class="indexterm"/><span class="strong"><strong>of Illinois</strong></span>: <a class="ulink" href="http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz">http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz</a></li><li class="listitem"><span class="strong"><strong>Stanford </strong></span><a id="id424" class="indexterm"/><span class="strong"><strong>University</strong></span>: <a class="ulink" href="http://ai.stanford.edu/~jkrause/cars/car_dataset.html">http://ai.stanford.edu/~jkrause/cars/car_dataset.html</a></li></ul></div><div class="note" title="Note"><h3 class="title"><a id="note25"/>Note</h3><p>Note that training images and test images are available in separate files.</p></div><p>I'll be using the UIUC dataset in my example, but feel free to explore the Internet for other types of datasets.</p><p>Now, let's take a look at an example:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np
from os.path import join

datapath = "/home/d3athmast3r/dev/python/CarData/TrainImages/"
def path(cls,i):
  return "%s/%s%d.pgm"  % (datapath,cls,i+1)

pos, neg = "pos-", "neg-"

detect = cv2.xfeatures2d.SIFT_create()
extract = cv2.xfeatures2d.SIFT_create()

flann_params = dict(algorithm = 1, trees = 5)flann = cv2.FlannBasedMatcher(flann_params, {})

bow_kmeans_trainer = cv2.BOWKMeansTrainer(40)
extract_bow = cv2.BOWImgDescriptorExtractor(extract, flann)

def extract_sift(fn):
  im = cv2.imread(fn,0)
  return extract.compute(im, detect.detect(im))[1]
  
for i in range(8):
  bow_kmeans_trainer.add(extract_sift(path(pos,i)))
  bow_kmeans_trainer.add(extract_sift(path(neg,i)))
  
voc = bow_kmeans_trainer.cluster()
extract_bow.setVocabulary( voc )

def bow_features(fn):
  im = cv2.imread(fn,0)
  return extract_bow.compute(im, detect.detect(im))

traindata, trainlabels = [],[]
for i in range(20):
  traindata.extend(bow_features(path(pos, i))); trainlabels.append(1)
  traindata.extend(bow_features(path(neg, i))); trainlabels.append(-1)

svm = cv2.ml.SVM_create()
svm.train(np.array(traindata), cv2.ml.ROW_SAMPLE, np.array(trainlabels))

def predict(fn):
  f = bow_features(fn);  
  p = svm.predict(f)
  print fn, "\t", p[1][0][0]
  return p
  
car, notcar = "/home/d3athmast3r/dev/python/study/images/car.jpg", "/home/d3athmast3r/dev/python/study/images/bb.jpg"
car_img = cv2.imread(car)
notcar_img = cv2.imread(notcar)
car_predict = predict(car)
not_car_predict = predict(notcar)

font = cv2.FONT_HERSHEY_SIMPLEX

if (car_predict[1][0][0] == 1.0):
  cv2.putText(car_img,'Car Detected',(10,30), font, 1,(0,255,0),2,cv2.LINE_AA)

if (not_car_predict[1][0][0] == -1.0):
  cv2.putText(notcar_img,'Car Not Detected',(10,30), font, 1,(0,0, 255),2,cv2.LINE_AA)

cv2.imshow('BOW + SVM Success', car_img)
cv2.imshow('BOW + SVM Failure', notcar_img)
cv2.waitKey(0)
cv2.destroyAllWindows()</pre></div><div class="section" title="What did we just do?"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec43"/>What did we just do?</h2></div></div></div><p>This is quite a lot to assimilate, so let's go through what we've done:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First of all, our usual <a id="id425" class="indexterm"/>imports are followed by the declaration of the base path of our training images. This will come in handy to avoid rewriting the base path every time we process an image in a particular folder on our computer.</li><li class="listitem">After this, we declare a function, <code class="literal">path</code>:<div class="informalexample"><pre class="programlisting">def path(cls,i):
  return "%s/%s%d.pgm"  % (datapath,cls,i+1)

pos, neg = "pos-", "neg-"</pre></div><div class="note" title="Note"><h3 class="title"><a id="note26"/>Note</h3><p>
<span class="strong"><strong>More on the path function</strong></span>
</p><p>This function is a utility method: given the name of a class (in our case, we have two classes, <code class="literal">pos</code> and <code class="literal">neg</code>) and a numerical index, we return the full path to a particular testing image. Our car dataset contains images named in the following way: <code class="literal">pos-x.pgm</code> and <code class="literal">neg-x.pgm</code>, where <code class="literal">x</code> is a number.</p><p>Immediately, you will find the usefulness of this function when iterating through a range of numbers (say, 20), which will allow you to load all images from <code class="literal">pos-0.pgm</code> to <code class="literal">pos-20.pgm</code>, and the same goes for the negative class.</p></div></li><li class="listitem">Next up, we'll create two SIFT instances: one to extract keypoints, the other to extract features:<div class="informalexample"><pre class="programlisting">detect = cv2.xfeatures2d.SIFT_create()
extract = cv2.xfeatures2d.SIFT_create()</pre></div></li><li class="listitem">Whenever you see SIFT involved, you can be pretty sure some feature matching algorithm will be involved too. In our case, we'll create an instance for a FLANN matcher:<div class="informalexample"><pre class="programlisting">flann_params = dict(algorithm = 1, trees = 5)flann = cv2.FlannBasedMatcher(flann_params, {})</pre></div><div class="note" title="Note"><h3 class="title"><a id="note27"/>Note</h3><p>Note that currently, the <code class="literal">enum</code> values for FLANN are missing from the Python version of OpenCV 3, so, number <code class="literal">1</code>, which is passed as the algorithm parameter, represents the <code class="literal">FLANN_INDEX_KDTREE</code> algorithm. I suspect the final version will be <code class="literal">cv2.FLANN_INDEX_KDTREE</code>, which is a little more helpful. Make sure to check the <code class="literal">enum</code> values for the correct flags.</p></div></li><li class="listitem">Next, we mention the BOW trainer:<div class="informalexample"><pre class="programlisting">bow_kmeans_trainer = cv2.BOWKMeansTrainer(40)</pre></div></li><li class="listitem">This BOW <a id="id426" class="indexterm"/>trainer utilizes 40 clusters. After this, we'll initialize the BOW extractor. This is the BOW class that will be fed a vocabulary of visual words and will try to detect them in the test image:<div class="informalexample"><pre class="programlisting">extract_bow = cv2.BOWImgDescriptorExtractor(extract, flann)</pre></div></li><li class="listitem">To extract the SIFT features from an image, we build a utility method, which takes the path to the image, reads it in grayscale, and returns the descriptor:<div class="informalexample"><pre class="programlisting">def extract_sift(fn):
  im = cv2.imread(fn,0)
  return extract.compute(im, detect.detect(im))[1]</pre></div></li></ol><div style="height:10px; width: 1px"/></div><p>At this stage, we have everything we need to start training the BOW trainer.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's read eight images per class (eight positives and eight negatives) from our dataset:<div class="informalexample"><pre class="programlisting">for i in range(8):
  bow_kmeans_trainer.add(extract_sift(path(pos,i)))
  bow_kmeans_trainer.add(extract_sift(path(neg,i)))</pre></div></li><li class="listitem">To create the vocabulary of visual words, we'll call the <code class="literal">cluster()</code> method on the trainer, which performs the k-means classification and returns the said vocabulary. We'll assign this vocabulary to <code class="literal">BOWImgDescriptorExtractor</code> so that it can extract descriptors from test images:<div class="informalexample"><pre class="programlisting">vocabulary = bow_kmeans_trainer.cluster()
extract_bow.setVocabulary(vocabulary)</pre></div></li><li class="listitem">In line with other utility functions declared in this script, we'll declare a function that takes the path to an image and returns the descriptor as computed by the BOW descriptor extractor:<div class="informalexample"><pre class="programlisting">def bow_features(fn):
  im = cv2.imread(fn,0)
  return extract_bow.compute(im, detect.detect(im))</pre></div></li><li class="listitem">Let's create two arrays to accommodate the train data and labels, and populate them with the descriptors generated by <code class="literal">BOWImgDescriptorExtractor</code>, associating labels to the positive and negative images we're feeding (<code class="literal">1</code> stands for a positive match, <code class="literal">-1</code> for a negative):<div class="informalexample"><pre class="programlisting">traindata, trainlabels = [],[]
for i in range(20):
  traindata.extend(bow_features(path(pos, i))); trainlabels.append(1)
  traindata.extend(bow_features(path(neg, i))); trainlabels.append(-1)</pre></div></li><li class="listitem">Now, let's create an instance of an SVM:<div class="informalexample"><pre class="programlisting">svm = cv2.ml.SVM_create()</pre></div></li><li class="listitem">Then, train it by <a id="id427" class="indexterm"/>wrapping the train data and labels into the NumPy arrays:<div class="informalexample"><pre class="programlisting">svm.train(np.array(traindata), cv2.ml.ROW_SAMPLE, np.array(trainlabels))</pre></div></li></ol><div style="height:10px; width: 1px"/></div><p>We're all set with a trained SVM; all that is left to do is to feed the SVM a couple of sample images and see how it behaves.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's first define another utility method to print the result of our <code class="literal">predict</code> method and return it:<div class="informalexample"><pre class="programlisting">def predict(fn):
  f = bow_features(fn);  
  p = svm.predict(f)
  print fn, "\t", p[1][0][0]
  return p</pre></div></li><li class="listitem">Let's define two sample image paths and read them as the NumPy arrays:<div class="informalexample"><pre class="programlisting">car, notcar = "/home/d3athmast3r/dev/python/study/images/car.jpg", "/home/d3athmast3r/dev/python/study/images/bb.jpg"
car_img = cv2.imread(car)
notcar_img = cv2.imread(notcar)</pre></div></li><li class="listitem">We'll pass these images to the trained SVM, and get the result of the prediction:<div class="informalexample"><pre class="programlisting">car_predict = predict(car)
not_car_predict = predict(notcar)</pre></div><p>Naturally, we're hoping that the car image will be detected as a car (result of <code class="literal">predict()</code> should be <code class="literal">1.0</code>), and that the other image will not (result should be <code class="literal">-1.0</code>), so we will only add text to the images if the result is the expected one.</p></li><li class="listitem">At last, we'll present the images on the screen, hoping to see the correct caption on each:<div class="informalexample"><pre class="programlisting">font = cv2.FONT_HERSHEY_SIMPLEX

if (car_predict[1][0][0] == 1.0):
  cv2.putText(car_img,'Car Detected',(10,30), font, 1,(0,255,0),2,cv2.LINE_AA)

if (not_car_predict[1][0][0] == -1.0):
  cv2.putText(notcar_img,'Car Not Detected',(10,30), font, 1,(0,0, 255),2,cv2.LINE_AA)

cv2.imshow('BOW + SVM Success', car_img)
cv2.imshow('BOW + SVM Failure', notcar_img)
cv2.waitKey(0)
cv2.destroyAllWindows()</pre></div></li></ol><div style="height:10px; width: 1px"/></div><p>The preceding <a id="id428" class="indexterm"/>operation produces the following result:</p><div class="mediaobject"><img src="../Images/image00238.jpeg" alt="What did we just do?"/></div><p style="clear:both; height: 1em;"> </p><p>It also results in this:</p><div class="mediaobject"><img src="../Images/image00239.jpeg" alt="What did we just do?"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="SVM and sliding windows"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec44"/>SVM and sliding windows</h2></div></div></div><p>Having detected <a id="id429" class="indexterm"/>an object is an impressive achievement, but now we want to push this to the next level in these ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Detecting <a id="id430" class="indexterm"/>multiple objects of the same kind in an image</li><li class="listitem">Determining the position of a detected object in an image</li></ul></div><p>To accomplish this, we will use the sliding windows approach. If it's not already clear from the previous explanation of the concept of sliding windows, the rationale behind the adoption of this approach will become more apparent if we take a look at a diagram:</p><div class="mediaobject"><img src="../Images/image00240.jpeg" alt="SVM and sliding windows"/></div><p style="clear:both; height: 1em;"> </p><p>Observe the movement of the block:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We take a region of the image, classify it, and then move a predefined step size to the right-hand side. When we reach the rightmost end of the image, we'll reset the <span class="emphasis"><em>x</em></span> coordinate to <code class="literal">0</code> and move down a step, and repeat the entire process.</li><li class="listitem">At each step, we'll perform a classification with the SVM that was trained with BOW.</li><li class="listitem">Keep a track of all the blocks that have <span class="emphasis"><em>passed</em></span> the SVM predict test.</li><li class="listitem">When you've <a id="id431" class="indexterm"/>finished classifying the entire image, scale the image down and repeat the entire sliding windows process.</li></ol><div style="height:10px; width: 1px"/></div><p>Continue rescaling and <a id="id432" class="indexterm"/>classifying until you get to a minimum size.</p><p>This gives you the chance to detect objects in several regions of the image and at different scales.</p><p>At this stage, you will have collected important information about the content of the image; however, there's a problem: it's most likely that you will end up with a number of overlapping blocks that give you a positive score. This means that your image may contain one object that gets detected four or five times, and if you were to report the result of the detection, your report would be quite inaccurate, so here's where non-maximum suppression comes into play.</p><div class="section" title="Example – car detection in a scene"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec30"/>Example – car detection in a scene</h3></div></div></div><p>We are now <a id="id433" class="indexterm"/>ready to apply all the concepts we learned so far to a real-life example, and create a car detector application that scans an image and draws rectangles around cars.</p><p>Let's summarize the process before diving into the code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Obtain a train dataset.</li><li class="listitem">Create a BOW trainer and create a visual vocabulary.</li><li class="listitem">Train an SVM with the vocabulary.</li><li class="listitem">Attempt detection using sliding windows on an image pyramid of a test image.</li><li class="listitem">Apply non-maximum suppression to overlapping boxes.</li><li class="listitem">Output the result.</li></ol><div style="height:10px; width: 1px"/></div><p>Let's also take a look at the structure of the project, as it is a bit more complex than the classic standalone script approach we've adopted until now.</p><p>The project structure is as follows:</p><div class="informalexample"><pre class="programlisting">├── car_detector
│   ├── detector.py
│   ├── __init__.py
│   ├── non_maximum.py
│   ├── pyramid.py
│   └── sliding_w112661222.indow.py
└── car_sliding_windows.py</pre></div><p>The main program is in <code class="literal">car_sliding_windows.py</code>, and all the utilities are contained in the <code class="literal">car_detector</code> folder. As we're using Python 2.7, we'll need an <code class="literal">__init__.py</code> file in the folder for it to be detected as a module.</p><p>The four files in the <code class="literal">car_detector</code> module are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The SVM training model</li><li class="listitem">The non-maximum suppression function</li><li class="listitem">The image pyramid</li><li class="listitem">The sliding windows function</li></ul></div><p>Let's examine them one by one, starting from the image pyramid:</p><div class="informalexample"><pre class="programlisting">import cv2

def resize(img, scaleFactor):
  return cv2.resize(img, (int(img.shape[1] * (1 / scaleFactor)), int(img.shape[0] * (1 / scaleFactor))), interpolation=cv2.INTER_AREA)

def pyramid(image, scale=1.5, minSize=(200, 80)):
  yield image

  while True:
    image = resize(image, scale)
    if image.shape[0] &lt; minSize[1] or image.shape[1] &lt; minSize[0]:
      break

    yield image</pre></div><p>This module contains two function definitions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Resize <a id="id434" class="indexterm"/>takes an image and resizes it by a specified factor</li><li class="listitem">Pyramid takes an image and returns a resized version of it until the minimum constraints of width and height are reached</li></ul></div><div class="note" title="Note"><h3 class="title"><a id="note28"/>Note</h3><p>You will notice that the image is not returned with the <code class="literal">return</code> keyword but with the <code class="literal">yield</code> keyword. This is because this function is a so-called generator. If you are not familiar with generators, take a look at <a class="ulink" href="https://wiki.python.org/moin/Generators">https://wiki.python.org/moin/Generators</a>.</p><p>This will allow us to obtain a resized image to process in our main program.</p></div><p>Next up is the sliding windows function:</p><div class="informalexample"><pre class="programlisting">def sliding_window(image, stepSize, windowSize):
  for y in xrange(0, image.shape[0], stepSize):
    for x in xrange(0, image.shape[1], stepSize):
      yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])</pre></div><p>Again, this is a generator. Although a bit deep-nested, this mechanism is very simple: given an image, return a window that moves of an arbitrary sized step from the left margin towards the right, until the entire width of the image is covered, then goes back to the left margin but down a step, covering the width of the image repeatedly until the bottom right corner of the image is reached. You can visualize this as the same pattern used for writing on a piece of paper: start from the left margin and reach the right margin, then move onto the next line from the left margin.</p><p>The last utility is non-maximum suppression, which looks like this (Malisiewicz/Rosebrock's code):</p><div class="informalexample"><pre class="programlisting">def non_max_suppression_fast(boxes, overlapThresh):

  # if there are no boxes, return an empty list

  if len(boxes) == 0:

    return []



  # if the bounding boxes integers, convert them to floats --

  # this is important since we'll be doing a bunch of divisions

  if boxes.dtype.kind == "i":

    boxes = boxes.astype("float")



  # initialize the list of picked indexes 

  pick = []



  # grab the coordinates of the bounding boxes

  x1 = boxes[:,0]

  y1 = boxes[:,1]

  x2 = boxes[:,2]

  y2 = boxes[:,3]

  scores = boxes[:,4]

  # compute the area of the bounding boxes and sort the bounding

  # boxes by the score/probability of the bounding box

  area = (x2 - x1 + 1) * (y2 - y1 + 1)

  idxs = np.argsort(scores)[::-1]



  # keep looping while some indexes still remain in the indexes

  # list

  while len(idxs) &gt; 0:

    # grab the last index in the indexes list and add the

    # index value to the list of picked indexes

    last = len(idxs) - 1

    i = idxs[last]

    pick.append(i)



    # find the largest (x, y) coordinates for the start of

    # the bounding box and the smallest (x, y) coordinates

    # for the end of the bounding box

    xx1 = np.maximum(x1[i], x1[idxs[:last]])

    yy1 = np.maximum(y1[i], y1[idxs[:last]])

    xx2 = np.minimum(x2[i], x2[idxs[:last]])

    yy2 = np.minimum(y2[i], y2[idxs[:last]])



    # compute the width and height of the bounding box

    w = np.maximum(0, xx2 - xx1 + 1)

    h = np.maximum(0, yy2 - yy1 + 1)



    # compute the ratio of overlap

    overlap = (w * h) / area[idxs[:last]]



    # delete all indexes from the index list that have

    idxs = np.delete(idxs, np.concatenate(([last],

      np.where(overlap &gt; overlapThresh)[0])))



  # return only the bounding boxes that were picked using the

  # integer data type

  return boxes[pick].astype("int")</pre></div><p>This function simply takes a list of rectangles and sorts them by their score. Starting from the box with the <a id="id435" class="indexterm"/>highest score, it eliminates all boxes that overlap beyond a certain threshold by calculating the area of intersection and determining whether it is greater than a certain threshold.</p><div class="section" title="Examining detector.py"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec04"/>Examining detector.py</h4></div></div></div><p>Now, let's <a id="id436" class="indexterm"/>examine the heart of this program, which is <code class="literal">detector.py</code>. This a bit long and complex; however, everything should appear much clearer given our newfound familiarity with the concepts of BOW, SVM, and feature detection/extraction.</p><p>Here's the code:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

datapath = "/path/to/CarData/TrainImages/"
SAMPLES = 400

def path(cls,i):
    return "%s/%s%d.pgm"  % (datapath,cls,i+1)

def get_flann_matcher():
  flann_params = dict(algorithm = 1, trees = 5)
  return cv2.FlannBasedMatcher(flann_params, {})

def get_bow_extractor(extract, flann):
  return cv2.BOWImgDescriptorExtractor(extract, flann)

def get_extract_detect():
  return cv2.xfeatures2d.SIFT_create(), cv2.xfeatures2d.SIFT_create()

def extract_sift(fn, extractor, detector):
  im = cv2.imread(fn,0)
  return extractor.compute(im, detector.detect(im))[1]
    
def bow_features(img, extractor_bow, detector):
  return extractor_bow.compute(img, detector.detect(img))

def car_detector():
  pos, neg = "pos-", "neg-"
  detect, extract = get_extract_detect()
  matcher = get_flann_matcher()
  print "building BOWKMeansTrainer..."
  bow_kmeans_trainer = cv2.BOWKMeansTrainer(1000)
  extract_bow = cv2.BOWImgDescriptorExtractor(extract, flann)

  print "adding features to trainer"
  for i in range(SAMPLES):
    print i
    bow_kmeans_trainer.add(extract_sift(path(pos,i), extract, detect))
    bow_kmeans_trainer.add(extract_sift(path(neg,i), extract, detect))
    
  voc = bow_kmeans_trainer.cluster()
  extract_bow.setVocabulary( voc )

  traindata, trainlabels = [],[]
  print "adding to train data"
  for i in range(SAMPLES):
    print i
    traindata.extend(bow_features(cv2.imread(path(pos, i), 0), extract_bow, detect))
    trainlabels.append(1)
    traindata.extend(bow_features(cv2.imread(path(neg, i), 0), extract_bow, detect))
    trainlabels.append(-1)

  svm = cv2.ml.SVM_create()
  svm.setType(cv2.ml.SVM_C_SVC)
  svm.setGamma(0.5)
  svm.setC(30)
  svm.setKernel(cv2.ml.SVM_RBF)

  svm.train(np.array(traindata), cv2.ml.ROW_SAMPLE, np.array(trainlabels))
  return svm, extract_bow</pre></div><p>Let's go through it. First, we'll import our usual modules, and then set a path for the training images.</p><p>Then, we'll define a number of utility functions:</p><div class="informalexample"><pre class="programlisting">def path(cls,i):
    return "%s/%s%d.pgm"  % (datapath,cls,i+1)</pre></div><p>This function <a id="id437" class="indexterm"/>returns the path to an image given a base path and a class name. In our example, we're going to use the <code class="literal">neg-</code> and <code class="literal">pos-</code> class names, because this is what the training images are called (that is, <code class="literal">neg-1.pgm</code>). The last argument is an integer used to compose the final part of the image path.</p><p>Next, we'll define a utility function to obtain a FLANN matcher:</p><div class="informalexample"><pre class="programlisting">def get_flann_matcher():
  flann_params = dict(algorithm = 1, trees = 5)
  return cv2.FlannBasedMatcher(flann_params, {})</pre></div><p>Again, it's not that the integer, <code class="literal">1</code>, passed as an algorithm argument represents <code class="literal">FLANN_INDEX_KDTREE</code>.</p><p>The next two functions return the SIFT feature detectors/extractors and a BOW trainer:</p><div class="informalexample"><pre class="programlisting">def get_bow_extractor(extract, flann):
  return cv2.BOWImgDescriptorExtractor(extract, flann)

def get_extract_detect():
  return cv2.xfeatures2d.SIFT_create(), cv2.xfeatures2d.SIFT_create()</pre></div><p>The next utility is a function used to return features from an image:</p><div class="informalexample"><pre class="programlisting">def extract_sift(fn, extractor, detector):
  im = cv2.imread(fn,0)
  return extractor.compute(im, detector.detect(im))[1]</pre></div><div class="note" title="Note"><h3 class="title"><a id="note29"/>Note</h3><p>A SIFT detector detects features, while a SIFT extractor extracts and returns them.</p></div><p>We'll also define a similar utility function to extract the BOW features:</p><div class="informalexample"><pre class="programlisting">def bow_features(img, extractor_bow, detector):
  return extractor_bow.compute(img, detector.detect(img))</pre></div><p>In the <code class="literal">main car_detector</code> function, we'll first create the necessary object used to perform feature detection and extraction:</p><div class="informalexample"><pre class="programlisting">  pos, neg = "pos-", "neg-"
  detect, extract = get_extract_detect()
  matcher = get_flann_matcher()
  bow_kmeans_trainer = cv2.BOWKMeansTrainer(1000)
  extract_bow = cv2.BOWImgDescriptorExtractor(extract, flann)</pre></div><p>Then, we'll add features taken from training images to the trainer:</p><div class="informalexample"><pre class="programlisting">  print "adding features to trainer"
  for i in range(SAMPLES):
    print i
    bow_kmeans_trainer.add(extract_sift(path(pos,i), extract, detect))</pre></div><p>For each class, we'll <a id="id438" class="indexterm"/>add a positive image to the trainer and a negative image.</p><p>After this, we'll instruct the trainer to cluster the data into <span class="emphasis"><em>k</em></span> groups.</p><p>The clustered data is now our vocabulary of visual words, and we can set the <code class="literal">BOWImgDescriptorExtractor</code> class' vocabulary in this way:</p><div class="informalexample"><pre class="programlisting">vocabulary = bow_kmeans_trainer.cluster()
 extract_bow.setVocabulary(vocabulary)</pre></div></div><div class="section" title="Associating training data with classes"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec05"/>Associating training data with classes</h4></div></div></div><p>With a <a id="id439" class="indexterm"/>visual vocabulary ready, we can now associate train data with classes. In our case, we have two classes: <code class="literal">-1</code> for negative results and <code class="literal">1</code> for positive ones.</p><p>Let's populate two arrays, <code class="literal">traindata</code> and <code class="literal">trainlabels</code>, containing extracted features and their corresponding labels. Iterating through the dataset, we can quickly set this up with the following code:</p><div class="informalexample"><pre class="programlisting">traindata, trainlabels = [], []  
  print "adding to train data"
  for i in range(SAMPLES):
    print i
    traindata.extend(bow_features(cv2.imread(path(pos, i), 0), extract_bow, detect))
    trainlabels.append(1)
    traindata.extend(bow_features(cv2.imread(path(neg, i), 0), extract_bow, detect))
    trainlabels.append(-1)</pre></div><p>You will notice that at each cycle, we'll add one positive and one negative image, and then populate the labels with a <code class="literal">1</code> and a <code class="literal">-1</code> value to keep the data synchronized with the labels.</p><p>Should you wish to train more classes, you could do that by following this pattern:</p><div class="informalexample"><pre class="programlisting">  traindata, trainlabels = [], []
  print "adding to train data"
  for i in range(SAMPLES):
    print i
    traindata.extend(bow_features(cv2.imread(path(class1, i), 0), extract_bow, detect))
    trainlabels.append(1)
    traindata.extend(bow_features(cv2.imread(path(class2, i), 0), extract_bow, detect))
    trainlabels.append(2)
    traindata.extend(bow_features(cv2.imread(path(class3, i), 0), extract_bow, detect))
    trainlabels.append(3)</pre></div><p>For <a id="id440" class="indexterm"/>example, you could train a detector to detect cars and people and perform detection on these in an image containing both cars and people.</p><p>Lastly, we'll train the SVM with the following code:</p><div class="informalexample"><pre class="programlisting">  svm = cv2.ml.SVM_create()
  svm.setType(cv2.ml.SVM_C_SVC)
  svm.setGamma(0.5)
  svm.setC(30)
  svm.setKernel(cv2.ml.SVM_RBF)

  svm.train(np.array(traindata), cv2.ml.ROW_SAMPLE, np.array(trainlabels))
  return svm, extract_bow</pre></div><p>There are two parameters in particular that I'd like to focus your attention on:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>C</strong></span>: With this parameter, you could conceptualize the strictness or severity of the classifier. The higher the value, the less chances of misclassification, but the trade-off is that some positive results may not be detected. On the other hand, a low value may over-fit, so you risk getting false positives.</li><li class="listitem"><span class="strong"><strong>Kernel</strong></span>: This parameter determines the nature of the classifier: <code class="literal">SVM_LINEAR</code> indicates a linear <span class="emphasis"><em>hyperplane</em></span>, which, in practical terms, works very well for a binary classification (the test sample either belongs to a class or it doesn't), while <code class="literal">SVM_RBF</code> (<span class="strong"><strong>radial basis function</strong></span>) separates data using the Gaussian functions, which means that the data is split into several kernels defined by these functions. When training the SVM to classify for more than two classes, you will have to use RBF.</li></ul></div><p>Finally, we'll pass the <code class="literal">traindata</code> and <code class="literal">trainlabels</code> arrays into the SVM <code class="literal">train</code> method, and return the SVM and BOW extractor object. This is because in our applications, we <a id="id441" class="indexterm"/>don't want to have to recreate the vocabulary every time, so we expose it for reuse.</p></div></div><div class="section" title="Dude, where's my car?"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Dude, where's my car?</h3></div></div></div><p>We are <a id="id442" class="indexterm"/>ready to test our car detector!</p><p>Let's first create a simple program that loads an image, and then operates detection using the sliding windows and image pyramid techniques, respectively:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np
from car_detector.detector import car_detector, bow_features
from car_detector.pyramid import pyramid
from car_detector.non_maximum import non_max_suppression_fast as nms
from car_detector.sliding_window import sliding_window

def in_range(number, test, thresh=0.2):
  return abs(number - test) &lt; thresh

test_image = "/path/to/cars.jpg"

svm, extractor = car_detector()
detect = cv2.xfeatures2d.SIFT_create()

w, h = 100, 40
img = cv2.imread(test_img)

rectangles = []
counter = 1
scaleFactor = 1.25
scale = 1
font = cv2.FONT_HERSHEY_PLAIN

for resized in pyramid(img, scaleFactor):  
  scale = float(img.shape[1]) / float(resized.shape[1])
  for (x, y, roi) in sliding_window(resized, 20, (w, h)):
    
    if roi.shape[1] != w or roi.shape[0] != h:
      continue

    try:
      bf = bow_features(roi, extractor, detect)
      _, result = svm.predict(bf)
      a, res = svm.predict(bf, flags=cv2.ml.STAT_MODEL_RAW_OUTPUT)
      print "Class: %d, Score: %f" % (result[0][0], res[0][0])
      score = res[0][0]
      if result[0][0] == 1:
        if score &lt; -1.0:
          rx, ry, rx2, ry2 = int(x * scale), int(y * scale), int((x+w) * scale), int((y+h) * scale)
          rectangles.append([rx, ry, rx2, ry2, abs(score)])
    except:
      pass

    counter += 1

windows = np.array(rectangles)
boxes = nms(windows, 0.25)


for (x, y, x2, y2, score) in boxes:
  print x, y, x2, y2, score
  cv2.rectangle(img, (int(x),int(y)),(int(x2), int(y2)),(0, 255, 0), 1)
  cv2.putText(img, "%f" % score, (int(x),int(y)), font, 1, (0, 255, 0))

cv2.imshow("img", img)
cv2.waitKey(0)</pre></div><p>The notable <a id="id443" class="indexterm"/>part of the program is the function within the pyramid/sliding window loop:</p><div class="informalexample"><pre class="programlisting">      bf = bow_features(roi, extractor, detect)
      _, result = svm.predict(bf)
      a, res = svm.predict(bf, flags=cv2.ml.STAT_MODEL_RAW_OUTPUT)
      print "Class: %d, Score: %f" % (result[0][0], res[0][0])
      score = res[0][0]
      if result[0][0] == 1:
        if score &lt; -1.0:
          rx, ry, rx2, ry2 = int(x * scale), int(y * scale), int((x+w) * scale), int((y+h) * scale)
          rectangles.append([rx, ry, rx2, ry2, abs(score)])</pre></div><p>Here, we extract the features of the <span class="strong"><strong>region of interest</strong></span> (<span class="strong"><strong>ROI</strong></span>), which corresponds to the current sliding window, and then we call <code class="literal">predict</code> on the extracted features. The <code class="literal">predict</code> method has an optional parameter, <code class="literal">flags</code>, which returns the score of the prediction (contained at the <code class="literal">[0][0]</code> value).</p><div class="note" title="Note"><h3 class="title"><a id="note30"/>Note</h3><p>A word on the score of the prediction: the lower the value, the higher the confidence that the classified element really belongs to the class.</p></div><p>So, we'll set an arbitrary threshold of <code class="literal">-1.0</code> for classified windows, and all windows with less than <code class="literal">-1.0</code> are going to be taken as good results. As you experiment with your SVMs, you may tweak this to your liking until you find a golden mean that assures best results.</p><p>Finally, we add <a id="id444" class="indexterm"/>the computed coordinates of the sliding window (meaning, we multiply the current coordinates by the scale of the current layer in the image pyramid so that it gets correctly represented in the final drawing) to the array of rectangles.</p><p>There's one last operation we need to perform before drawing our final result: non-maximum suppression.</p><p>We turn the rectangles array into a NumPy array (to allow certain kind of operations that are only possible with NumPy), and then apply NMS:</p><div class="informalexample"><pre class="programlisting">windows = np.array(rectangles)
boxes = nms(windows, 0.25)</pre></div><p>Finally, we proceed with displaying all our results; for the sake of convenience, I've also printed the score obtained for all the remaining windows:</p><div class="mediaobject"><img src="../Images/image00241.jpeg" alt="Dude, where's my car?"/></div><p style="clear:both; height: 1em;"> </p><p>This is a <a id="id445" class="indexterm"/>remarkably accurate result!</p><p>A final note on SVM: you don't need to train a detector every time you want to use it, which would be extremely impractical. You can use the following code:</p><div class="informalexample"><pre class="programlisting">svm.save('/path/to/serialized/svmxml')</pre></div><p>You can subsequently reload it with a load method and feed it test images or frames.</p></div></div></div>
<div class="section" title="Summary" id="aid-1GKCM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec45"/>Summary</h1></div></div></div><p>In this chapter, we talked about numerous object detection concepts, such as HOG, BOW, SVM, and some useful techniques, such as image pyramid, sliding windows, and non-maximum suppression.</p><p>We introduced the concept of machine learning and explored the various approaches used to train a custom detector, including how to create or obtain a training dataset and classify data. Finally, we put this knowledge to good use by creating a car detector from scratch and verifying its correct functioning.</p><p>All these concepts form the foundation of the next chapter, in which we will utilize object detection and classification techniques in the context of making videos, and learn how to track objects to retain information that can potentially be used for business or application purposes.</p></div></body></html>