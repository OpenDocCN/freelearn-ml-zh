<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;11.&#xA0;Topic Modeling" id="2E6E41-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>Chapter 11. Topic Modeling</h1></div></div></div><p class="calibre8">Topic modeling is a relatively recent and exciting area that originated from the fields of natural language processing and information retrieval, but has seen applications in a number of other domains as well. Many problems in classification, such as sentiment analysis, involve assigning a single class to a particular observation. In topic modeling, the key idea is that we can assign a mixture of different classes to an observation. As this field takes its inspiration from information retrieval, we often think of our observations as documents and our output classes as topics. In many applications, this is actually the case and so we will focus <a id="id815" class="calibre1"/>on the domain of text documents and their topics, this being a very natural way to learn about this important model. In particular, we'll focus on a technique known as <span class="strong"><strong class="calibre2">Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong class="calibre2">LDA</strong></span>), which is the most prominently used method for topic modeling.</p></div>

<div class="book" title="Chapter&#xA0;11.&#xA0;Topic Modeling" id="2E6E41-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="An overview of topic modeling"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec75" class="calibre1"/>An overview of topic modeling</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 10. Probabilistic Graphical Models" href="part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7">Chapter 10</a>, <span class="strong"><em class="calibre9">Probabilistic Graphical Models</em></span>, we saw how we can use a bag of words as a feature <a id="id816" class="calibre1"/>of a Naïve Bayes model in order to perform sentiment analysis. There, the specific predictive task involved determining whether a particular movie review was expressing a positive sentiment or a negative sentiment. We explicitly assumed that the movie review was exclusively expressing only one possible sentiment. Each of the words used as features (such as <span class="strong"><em class="calibre9">bad</em></span>, <span class="strong"><em class="calibre9">good</em></span>, <span class="strong"><em class="calibre9">fun</em></span>, and so on) had a different likelihood of appearing in a review under each sentiment.</p><p class="calibre8">To compute the model's decision, we basically computed the likelihood of all the words in a particular review under one class, and compared this to the likelihood of all the words having been generated by the other class. We adjusted these likelihoods using the prior probability of each class, so that, when we know that one class is more popular in the training data, we expect to find it more frequently represented on unseen data in the future. There was no opportunity for a movie review to be partially positive, so that some of the words came from the positive class, and partially negative, so that the rest of the words occurred in the negative class.</p><p class="calibre8">The core premise behind <span class="strong"><strong class="calibre2">topic models</strong></span> is that in our problem we have a set of features and a set of hidden or latent variables that generate these features. Crucially, each observation in our data contains features that have been generated from a mixture or? a subset of these hidden <a id="id817" class="calibre1"/>variables. For example, an essay, website, or news article might have a central topic or theme such as politics, but might also include one or more <a id="id818" class="calibre1"/>elements from other themes as well, such as human rights, history, or economics.</p><p class="calibre8">In the image domain, we might be interested in identifying a particular object in a scene from a set of visual features such as shadows and surfaces. These, in turn, might be the product of a mixture of different objects. Our task in topic modeling is to observe the words inside a document, or the pixels and visual features of an image, and from these determine the underlying mix of topics and objects respectively.</p><p class="calibre8">Topic modeling on text data can be used in a number of different ways. One possible application is to group together similar documents, either based on their most predominant topic or based on their topical mix. Thus, it can be viewed as a form of clustering. By studying the topic composition, the most frequent words, as well as the relative sizes of the clusters we obtain, we are able to summarize information about a particular collection of documents.</p><p class="calibre8">We can use the most frequent words and topics of a cluster to describe a cluster directly, and in turn this might be useful for automatically generating tags, for example to improve the search capabilities of an information retrieval service for our documents. Yet another example might be to automatically recommending Twitter hashtags once we have built a topic model for a database of tweets.</p><p class="calibre8">When we describe documents such as websites using a bag of words approach, each document is essentially a vector indexed by the words in our dictionary. The elements of the vector are either counts of the various words or binary variables capturing whether a word was present in the document. Either way, this representation is a good method of encoding text into a numerical format, but the result is a sparse vector in a high-dimensional space as the word dictionary is typically large. Under a topic model, each document is represented by a mixture of topics. As this number tends to be much smaller than the dictionary size, topic modeling can also function as a form of dimensionality reduction.</p><p class="calibre8">Finally, topic modeling can also be viewed as a predictive task for classification. If we have a collection of documents labeled with a predominant theme label, we can perform topic modeling on this collection. If the predominant topic clustering we obtain from this method coincides with our labeled categories, we can use the model to predict a topical mixture for an unknown document and classify it according to the most dominant topic. We'll see an example of this later on in this chapter. We will now introduce the most well-known technique for performing topic modeling, Latent Dirichlet Allocation.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Latent Dirichlet Allocation"><div class="book" id="2F4UM2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec76" class="calibre1"/>Latent Dirichlet Allocation</h1></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong class="calibre2">LDA</strong></span>) is the prototypical method of performing topic modeling. Rather <a id="id819" class="calibre1"/>unfortunately, the acronym LDA is also used for another method in machine learning. This latter method is completely different from <a id="id820" class="calibre1"/>LDA and is commonly used as a way to perform dimensionality reduction and classification.</p><p class="calibre8">Although LDA involves a substantial amount of mathematics, it is worth exploring some of its <a id="id821" class="calibre1"/>technical details in order to understand how the model works and the assumptions that it uses. First and foremost, we should learn about the <span class="strong"><strong class="calibre2">Dirichlet distribution</strong></span>, which lends its name to LDA.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note41" class="calibre1"/>Note</h3><p class="calibre8">An excellent reference for a fuller treatment of Topic Models with LDA is the <span class="strong"><em class="calibre9">Topic Models</em></span> chapter in <span class="strong"><em class="calibre9">Text Mining: Classification, Clustering, and Applications</em></span>, edited by <span class="strong"><em class="calibre9">A. Srivastava</em></span> and <span class="strong"><em class="calibre9">M. Sahami</em></span> and published by <span class="strong"><em class="calibre9">Chapman &amp; Hall</em></span>, 2009.</p></div></div>

<div class="book" title="Latent Dirichlet Allocation">
<div class="book" title="The Dirichlet distribution"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec91" class="calibre1"/>The Dirichlet distribution</h2></div></div></div><p class="calibre8">Suppose <a id="id822" class="calibre1"/>we have a classification problem with <span class="strong"><em class="calibre9">K</em></span> classes and the probability of each class is fixed. Given a vector of length <span class="strong"><em class="calibre9">K</em></span> containing counts of the occurrence of each class, we can estimate the probabilities of each class by just dividing each entry in the vector by the sum of all the counts.</p><p class="calibre8">Now suppose we would like to predict the number of times each class will occur over a series of <span class="strong"><em class="calibre9">N</em></span> trials. If we have two classes, we can model this with a binomial distribution, as we would <a id="id823" class="calibre1"/>normally do in a coin flip experiment. For <span class="strong"><em class="calibre9">K</em></span> classes, the binomial distribution generalizes to the <span class="strong"><strong class="calibre2">multinomial distribution</strong></span>, where the probability of each class, <span class="strong"><em class="calibre9">pi</em></span>, is fixed and the sum of all instances of <span class="strong"><em class="calibre9">pi</em></span> equals one. Now, suppose that we wanted to model the random selection of a particular multinomial distribution with <span class="strong"><em class="calibre9">K</em></span> categories. The Dirichlet distribution achieves just that. Here is its form:</p><div class="mediaobject"><img src="../images/00183.jpeg" alt="The Dirichlet distribution" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This equation seems complex, but if we break it down to its constituent parts and label the symbols used, we will then be able to understand it better. To begin with, the <span class="strong"><img src="../images/00184.jpeg" alt="The Dirichlet distribution" class="calibre26"/></span> term is a vector with <span class="strong"><em class="calibre9">K</em></span> components, <span class="strong"><em class="calibre9">x<sub class="calibre14">k</sub></em></span>, representing a particular multinomial distribution. The vector <span class="strong"><img src="../images/00185.jpeg" alt="The Dirichlet distribution" class="calibre26"/></span> is also a <span class="strong"><em class="calibre9">K</em></span> component vector containing the <span class="strong"><em class="calibre9">K</em></span> parameters, <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span>, of the Dirichlet distribution. Thus, we are computing the probability of selecting a particular multinomial distribution given a particular parameter combination. Notice that we provide the Dirichlet distribution <a id="id824" class="calibre1"/>with a parameter vector whose length is the same as the number of classes of the multinomial distribution that it will return.</p><p class="calibre8">The fraction <a id="id825" class="calibre1"/>before the large product on the right-hand side of the equation is a normalizing constant, which depends only on the values of the Dirichlet parameters and is expressed in terms of the <span class="strong"><strong class="calibre2">gamma function</strong></span>. For completeness, the gamma function, a generalization of the factorial function, is given by the following:</p><div class="mediaobject"><img src="../images/00186.jpeg" alt="The Dirichlet distribution" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Lastly, in the final product, we see that every parameter, <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span>, is paired with the corresponding component of the multinomial distribution, <span class="strong"><em class="calibre9">x<sub class="calibre14">k</sub></em></span>, in forming the terms of the product. The important point to remember about this distribution is that, by modifying the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters, we are modifying the probabilities of the different multinomial distributions that we can draw.</p><p class="calibre8">We are <a id="id826" class="calibre1"/>especially interested in the total sum of the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters as well as the relative proportions among them. A large total for the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters tends to produce a smoother distribution involving a mix of many topics and this distribution is more likely to follow the pattern of alpha parameters in their relative proportions.</p><p class="calibre8">A special case of the Dirichlet distribution is the <span class="strong"><strong class="calibre2">Symmetrical Dirichlet distribution</strong></span>, in which all the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters have an identical value. When the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters are identical and large in value, we are likely to sample a multinomial distribution that is close to being uniform. Thus, the symmetrical Dirichlet distribution is used when we have no information about a preference over a particular topic distribution and we consider all topics to be equally likely.</p><p class="calibre8">Similarly, suppose we had a skewed vector of <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters with large absolute values. For example, we might have a vector in which one of the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters was much higher than the others, indicating a preference for selecting one of the topics. If we used this as an input to the Dirchlet distribution, we would likely sample a multinomial distribution in which the aforementioned topic was probable.</p><p class="calibre8">By contrast, if the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters add up to a small number, this usually results in a peaky distribution, in which only one or two of the topics are likely and the rest are unlikely. Consequently, if we wanted to model the process of drawing a multinomial with only a few topics selected, we would use low values for the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters, whereas, if we wanted a <a id="id827" class="calibre1"/>good mix, we would use larger values. The following two plots will help visualize this behavior. The first plot is for a symmetric Dirichlet distribution:</p><div class="mediaobject"><img src="../images/00187.jpeg" alt="The Dirichlet distribution" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In this plot, each column contains four random samples of a multinomial distribution generated using a symmetric Dirichlet distribution for five topics. In the first column, all the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters are set to 0.1. Note that the distributions are very peaky and, because all the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters are equally likely, there is no preference for which topic will tend to be chosen as the highest peak.</p><p class="calibre8">In the middle column, the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters are set to 1 and, as the sum of the parameters is now larger, we see a greater mix of topics, even if the distribution is still skewed. When we set the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters to the value of 10 in the third column, we see that the samples are now much <a id="id828" class="calibre1"/>closer to a uniform distribution.</p><p class="calibre8">In many <a id="id829" class="calibre1"/>scenarios, we use the Dirichlet distribution as a <span class="strong"><strong class="calibre2">prior distribution</strong></span>; that is, a distribution that describes our prior beliefs about the multinomial distribution that we are trying to sample. When the sum of the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters is high, we tend to think of our prior as having very strong beliefs. In the next plot, we will skew the distribution of our <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters to favor the first topic:</p><div class="mediaobject"><img src="../images/00188.jpeg" alt="The Dirichlet distribution" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the first column, the average value of the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters is 0.1, but we adjusted their distribution so that <span class="strong"><em class="calibre9">α<sub class="calibre14">1</sub></em></span>, which corresponds to the first topic, now has a value four times as high as the others. We see that this has increased the probability of the first topic featuring prominently in the sampled multinomial distribution, but it is not guaranteed to be the distribution's mode.</p><p class="calibre8">In the <a id="id830" class="calibre1"/>middle column, where the average of the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters is now 1 but with the same skew, Topic 1 is the mode of distribution in all the samples. Additionally, there is still a high variance in how the other topics will be selected. In the third column, we have a smoother distribution that simultaneously mixes all five topics but enforces the preference for the first topic.</p><p class="calibre8">Now that we have an idea of how this distribution works, we will see in the next section how it is used to build topic models with LDA.</p></div></div>

<div class="book" title="Latent Dirichlet Allocation">
<div class="book" title="The generative process"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec92" class="calibre1"/>The generative process</h2></div></div></div><p class="calibre8">We delved <a id="id831" class="calibre1"/>into the Dirichlet distribution with significant detail because it is at the heart of how topic modeling with LDA <a id="id832" class="calibre1"/>operates. Armed with this understanding, we'll now describe the <span class="strong"><strong class="calibre2">generative process</strong></span> behind LDA.</p><p class="calibre8">The generative process is aptly named as it describes how the LDA model assumes that documents, topics, and words are generated in our data. This process is essentially an illustration of the model's assumptions. The optimization procedures that are used in order to fit an LDA model to data essentially estimate the parameters of the generative process. We'll now see how this process works:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">For each of our K topics, draw a multinomial distribution, <span class="strong"><em class="calibre9">φ<sub class="calibre14">k</sub></em></span>, over the words in our vocabulary using a Dirichlet distribution parameterized by a vector, α. This vector has length <span class="strong"><em class="calibre9">V</em></span>, the size of our vocabulary. Even though we sample from the same Dirichlet distribution each time, we've seen that the sampled multinomial distributions will likely differ from each other.</li><li class="listitem" value="2">For every document, <span class="strong"><em class="calibre9">d</em></span>, that we want to generate:<div class="book"><ul class="itemizedlist1"><li class="listitem">Determine the mix of topics for this document by drawing a multinomial distribution, <span class="strong"><em class="calibre9">θ<sub class="calibre14">k</sub></em></span>, from a Dirichlet distribution, this time parameterized by a vector <span class="strong"><em class="calibre9">β</em></span> of length <span class="strong"><em class="calibre9">K</em></span>, the number of topics. Each document will thus have a different mix of topics.</li><li class="listitem">For every word, <span class="strong"><em class="calibre9">w</em></span>, in the document that we want to generate:</li><li class="listitem">Use the multinomial topic distribution for this document, <span class="strong"><em class="calibre9">θ<sub class="calibre14">k</sub></em></span>, to draw a topic with which this word will be associated</li><li class="listitem">Use that particular topic's distribution, <span class="strong"><em class="calibre9">φ<sub class="calibre14">k</sub></em></span>, to pick the actual word</li></ul></div></li></ol><div class="calibre13"/></div><p class="calibre8">Note that we use two differently parameterized Dirichlet distributions in our generative process, one for drawing a multinomial distribution of topics and another for drawing a multinomial <a id="id833" class="calibre1"/>distribution of words. Although the model is simple, it does capture certain intuitions about documents and topics. In particular, it captures the notion that documents about different topics will, in general, contain different words in them and in different proportions. Additionally, a particular word can be associated with more than one topic, but for some topics it will appear at a higher frequency than others. Documents may have a central topic, but they may also discuss other topics as well, and therefore we can think of a document as dealing with a mixture of topics. A topic that is more important in a document will be so because a greater percentage of the words in the document deal with that topic.</p><p class="calibre8">Dirichlet distributions can be smooth or skewed, and the mixture of components can be controlled via the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters. Consequently, by tuning the Dirichlet distributions appropriately, this process can produce documents with a single theme as well as documents covering many topics.</p><p class="calibre8">At the same time, it is important to bear in mind the limitations of the model through some of the simplifying assumptions that it makes. The model completely ignores the word order inside a document, and the generative process is memoryless in that, when it generates the <span class="strong"><em class="calibre9">n<sup class="calibre15">th</sup></em></span> word of a document, it does not take into account the existing <span class="strong"><em class="calibre9">n-1</em></span> words that were previously drawn for that document.</p><p class="calibre8">Furthermore, LDA does not attempt to model any relationships between the topics that are drawn for a document so that we do not try to organize topics that are more likely to co-occur, such as weather and travel or biology and chemistry. This is a significant limitation of the LDA model, for which there are proposed solutions. For example, one variant of LDA, <a id="id834" class="calibre1"/>known as the <span class="strong"><strong class="calibre2">Correlated Topic Model</strong></span> (<span class="strong"><strong class="calibre2">CTM</strong></span>), follows the same generative process as LDA but uses a different distribution that allows one to also model the correlations between the topics. In our experimental section, we will also see an implementation of the CTM model.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note42" class="calibre1"/>Note</h3><p class="calibre8">The correlated topic model was presented in <span class="strong"><em class="calibre9">A Correlated Topic Model of Science</em></span> by <span class="strong"><em class="calibre9">D. M. Blei</em></span> and <span class="strong"><em class="calibre9">J. D. Lafferty</em></span>, published by the <span class="strong"><em class="calibre9">Annals of Applied Statistics</em></span> in 2007.</p></div></div></div>

<div class="book" title="Latent Dirichlet Allocation">
<div class="book" title="Fitting an LDA model"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec93" class="calibre1"/>Fitting an LDA model</h2></div></div></div><p class="calibre8">Fitting an <a id="id835" class="calibre1"/>LDA model to a corpus of documents essentially involves computationally estimating the multinomial topic and word distributions, <span class="strong"><em class="calibre9">φ<sub class="calibre14">k</sub></em></span> and <span class="strong"><em class="calibre9">θ<sub class="calibre14">d</sub></em></span>, that would most likely be able to generate the data, assuming the LDA generative process. These variables are hidden or latent, which explains why the method is known as LDA.</p><p class="calibre8">A number <a id="id836" class="calibre1"/>of optimization procedures have been proposed to solve this problem, but the mathematical details are beyond the <a id="id837" class="calibre1"/>scope of this book. We will mention two of these, which we will encounter in the next section. The first method is known as <span class="strong"><strong class="calibre2">Variational Expectation Maximization</strong></span> (<span class="strong"><strong class="calibre2">VEM</strong></span>) and is a variant of the well-known <span class="strong"><strong class="calibre2">Expectation Maximization</strong></span> (<span class="strong"><strong class="calibre2">EM</strong></span>) algorithm. The second is known as Gibbs sampling, and <a id="id838" class="calibre1"/>is a method based on <span class="strong"><strong class="calibre2">Markov Chain Monte Carlo</strong></span> (<span class="strong"><strong class="calibre2">MCMC</strong></span>).</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note43" class="calibre1"/>Note</h3><p class="calibre8">For a tutorial on the EM algorithm and VEM, we recommend <span class="strong"><em class="calibre9">The Variational Approximation for Bayesian Inference</em></span> by <span class="strong"><em class="calibre9">Dimitris G. Tzikas</em></span> and others in the November 2008 issue of the <span class="strong"><em class="calibre9">IEEE Signal Processing Magazine</em></span>. For Gibbs sampling, there is a 1992 article in <span class="strong"><em class="calibre9">The American Statistician</em></span> by <span class="strong"><em class="calibre9">George Casella</em></span>, entitled <span class="strong"><em class="calibre9">Explaining the Gibbs Sampler</em></span>. Both are readable, but quite technical. A more thorough tutorial on Gibbs sampling is <span class="strong"><em class="calibre9">Gibbs Sampling for the Uninitiated</em></span> by <span class="strong"><em class="calibre9">Philip Resnik</em></span> and <span class="strong"><em class="calibre9">Eric Hardisty</em></span>. This last reference is less mathematically demanding and can be found online at <a class="calibre1" href="http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf">http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf</a>.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Modeling the topics of online news stories"><div class="book" id="2G3F82-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec77" class="calibre1"/>Modeling the topics of online news stories</h1></div></div></div><p class="calibre8">To see how <a id="id839" class="calibre1"/>topic models perform on real data, we will look at two datasets containing articles originating from BBC News during the period of 2004-2005. The first dataset, which we will refer to as the <span class="strong"><em class="calibre9">BBC dataset</em></span>, contains 2,225 articles that have been grouped into five topics. These are <span class="strong"><em class="calibre9">business</em></span>, <span class="strong"><em class="calibre9">entertainment</em></span>, <span class="strong"><em class="calibre9">politics</em></span>, <span class="strong"><em class="calibre9">sports</em></span>, and <span class="strong"><em class="calibre9">technology</em></span>.</p><p class="calibre8">The second dataset, which we will call the <span class="strong"><em class="calibre9">BBCSports dataset</em></span>, contains 737 articles only on sports. These are also grouped into five categories according to the type of sport being described. The five sports in question are <span class="strong"><em class="calibre9">athletics</em></span>, <span class="strong"><em class="calibre9">cricket</em></span>, <span class="strong"><em class="calibre9">football</em></span>, <span class="strong"><em class="calibre9">rugby</em></span>, and <span class="strong"><em class="calibre9">tennis</em></span>. Our objective will be to see if we can build topic models for each of these two datasets that will group together articles from the same major topic.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note44" class="calibre1"/>Note</h3><p class="calibre8">Both BBC datasets were presented in a paper by <span class="strong"><em class="calibre9">D. Greene</em></span> and <span class="strong"><em class="calibre9">P. Cunningham</em></span>, entitled <span class="strong"><em class="calibre9">Producing Accurate Interpretable Clusters from High-Dimensional Data</em></span> and published in the proceedings of the 9th European Conference on <span class="strong"><em class="calibre9">Principles and Practice of Knowledge Discovery in Databases (PKDD'05)</em></span> in October 2005.</p></div><p class="calibre8">The two <a id="id840" class="calibre1"/>datasets can be found at <a class="calibre1" href="http://mlg.ucd.ie/datasets/bbc.html">http://mlg.ucd.ie/datasets/bbc.html</a>. When downloaded, each dataset is a folder containing a few different files. We will use the variables <code class="email">bbc_folder</code> and <code class="email">bbcsports_folder</code> to store the paths of these folders on our computer.</p><p class="calibre8">Each <a id="id841" class="calibre1"/>folder contains three important files. The file with the extension <code class="email">.mtx</code> is essentially a file containing a term document matrix in sparse matrix form. Concretely, the rows of the matrix are terms that can be found in the articles and the columns are the articles themselves. An entry <span class="strong"><em class="calibre9">M[i,j]</em></span> in this matrix contains the number of times the term corresponding to row <span class="strong"><em class="calibre9">i</em></span> was found in the document corresponding to column <span class="strong"><em class="calibre9">j</em></span>. A term document matrix is thus a transposed document term matrix, which we encountered in <a class="calibre1" title="Chapter 8. Dimensionality Reduction" href="part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7">Chapter 8</a>, <span class="strong"><em class="calibre9">Probabilistic Graphical Models</em></span>. The specific format used to store the matrix in <a id="id842" class="calibre1"/>the file is a format known as the <span class="strong"><strong class="calibre2">Matrix Market format</strong></span>, where each line corresponds to a nonempty cell in the matrix.</p><p class="calibre8">Typically, when working with text such as news articles, we need to perform some preprocessing steps, such as stop-word removal, just as we did when using the <code class="email">tm</code> package in our example on sentiment analysis in <a class="calibre1" title="Chapter 8. Dimensionality Reduction" href="part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7">Chapter 8</a>, <span class="strong"><em class="calibre9">Probabilistic Graphical Models</em></span>. Fortunately for us, the articles in these datasets have already been processed so that they have been stemmed; stop words have been removed, as have any terms that appear fewer than three times.</p><p class="calibre8">In order to interpret the term document matrix, the files with the extension <code class="email">.terms</code> contain the actual terms, one per line, which are the row names of the term document matrix. Similarly, the document names that are the column names in the term document matrix are stored in the files with the extension <code class="email">.docs</code>.</p><p class="calibre8">We first create variables for the paths to the three files that we need for each dataset:</p><div class="informalexample"><pre class="programlisting">&gt;bbc_folder&lt;- "~/Downloads/bbc/"
&gt;bbcsports_folder&lt;- "~/Downloads/bbcsport/"
&gt;bbc_source&lt;- paste(bbc_folder, "bbc.mtx", sep = "")
&gt;bbc_source_terms&lt;- paste(bbc_folder, "bbc.terms", sep = "")
&gt;bbc_source_docs&lt;- paste(bbc_folder, "bbc.docs", sep = "")
&gt;bbcsports_source&lt;- paste(bbcsports_folder, "bbcsport.mtx", sep = "")
&gt;bbcsports_source_terms&lt;- paste(bbcsports_folder, 
"bbcsport.terms", sep = "")
&gt;bbcsports_source_docs&lt;- paste(bbcsports_folder, 
"bbcsport.docs", sep = "")</pre></div><p class="calibre8">In order to load data into R from a file in Market Matrix format, we can use the <code class="email">readMM()</code> function from the <code class="email">Matrix</code> R package. This function loads the data and stores it into a sparse matrix object. We can convert this into a term document matrix that the <code class="email">tm</code> package can interpret, using the <code class="email">as.TermDocumentMatrix()</code> function in the <code class="email">tm</code> package. Aside from the matrix <a id="id843" class="calibre1"/>object that is the first argument to that function, we also need to specify the <code class="email">weighting</code> parameter. This parameter describes what the numbers in the original matrix correspond to. In our case, we have raw term frequencies, so we specify the value <code class="email">weightTf</code>:</p><div class="informalexample"><pre class="programlisting">&gt; library("tm")
&gt; library("Matrix")
&gt;bbc_matrix&lt;- readMM(bbc_source)
&gt;bbc_tdm&lt;- as.TermDocumentMatrix(bbc_matrix, weightTf)
&gt;bbcsports_matrix&lt;- readMM(bbcsports_source)
&gt;bbcsports_tdm&lt;- as.TermDocumentMatrix(bbcsports_matrix, 
weightTf)</pre></div><p class="calibre8">Next, we load the terms and document identifiers from the two remaining files and use these to create appropriate row and column names respectively for the term document matrices. We can use the standard <code class="email">scan()</code> function to read files with a single entry per line and load the entries into vectors. Once we have a term vector and a document identifier vector, we will use these to update the row and column names for the term document matrix. Finally, we'll transpose this matrix into a document term matrix, as this is the format we will need for subsequent steps:</p><div class="informalexample"><pre class="programlisting">&gt;bbc_rows&lt;- scan(bbc_source_terms, what = "character")
Read 9635 items
&gt;bbc_cols&lt;- scan(bbc_source_docs, what = "character")
Read 2225 items
&gt;bbc_tdm$dimnames$Terms&lt;- bbc_rows
&gt;bbc_tdm$dimnames$Docs&lt;- bbc_cols
&gt; (bbc_dtm&lt;- t(bbc_tdm))
&lt;&lt;DocumentTermMatrix (documents: 2225, terms: 9635)&gt;&gt;
Non-/sparse entries: 286774/21151101
Sparsity           : 99%
Maximal term length: 24
Weighting          : term frequency (tf)
&gt;bbcsports_rows&lt;- scan(bbcsports_source_terms, what =  
"character")
Read 4613 items
&gt;bbcsports_cols&lt;- scan(bbcsports_source_docs, what =  
"character")
Read 737 items
&gt;bbcsports_tdm$dimnames$Terms&lt;- bbcsports_rows
&gt;bbcsports_tdm$dimnames$Docs&lt;- bbcsports_cols
&gt; (bbcsports_dtm&lt;- t(bbcsports_tdm))
&lt;&lt;DocumentTermMatrix (documents: 737, terms: 4613)&gt;&gt;
Non-/sparse entries: 85576/3314205
Sparsity           : 97%
Maximal term length: 17
Weighting          : term frequency (tf)</pre></div><p class="calibre8">We now have the document term matrices for our two datasets ready. We can see that there are roughly twice as many terms for the BBC dataset as there are for the BBCSports dataset, and the latter also has about a third of the number of documents, so it is a much smaller dataset. Before we build our topic models, we must also create the vectors containing <a id="id844" class="calibre1"/>the original topic classification of the articles. If we examine the document IDs, we can see that the format of each document identifier is <code class="email">&lt;topic&gt;.&lt;counter&gt;</code>:</p><div class="informalexample"><pre class="programlisting">&gt;bbc_cols[1:5]
[1] "business.001""business.002""business.003""business.004"
[5] "business.005"
&gt;bbcsports_cols[1:5]
[1] "athletics.001""athletics.002""athletics.003"
[4] "athletics.004""athletics.005"</pre></div><p class="calibre8">To create a vector with the correct topic assignments, we simply need to strip out the last four characters of each entry. If we then convert the result into a factor, we can see how many documents we have per topic:</p><div class="informalexample"><pre class="programlisting">&gt;bbc_gold_topics&lt;- sapply(bbc_cols, 
                           function(x) substr(x, 1, nchar(x) - 4))
&gt;bbc_gold_factor&lt;- factor(bbc_gold_topics)
&gt; summary(bbc_gold_factor)
     business entertainment      politics         sport 
          510           386           417           511 
         tech 
          401 
&gt;bbcsports_gold_topics&lt;- sapply(bbcsports_cols, 
                           function(x) substr(x, 1, nchar(x) - 4))
&gt;bbcsports_gold_factor&lt;- factor(bbcsports_gold_topics)
&gt; summary(bbcsports_gold_factor)
athletics   cricket  football     rugby    tennis 
      101       124       265       147       100</pre></div><p class="calibre8">This shows that the BBC dataset is fairly even in the distribution of its topics. In the BBCSports data, however, we see that there are roughly twice as many articles on football than the other four sports.</p><p class="calibre8">For each <a id="id845" class="calibre1"/>of our two datasets, we will now build some topic models using the package <code class="email">topicmodels</code>. This is a very useful package as it allows us to use data structures created with the <code class="email">tm</code> package to perform topic modeling. For each dataset, we will build the following four different topic models:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">LDA_VEM</code>: This is <a id="id846" class="calibre1"/>an LDA model trained with the <span class="strong"><strong class="calibre2">Variational Expectation Maximization</strong></span> (<span class="strong"><strong class="calibre2">VEM</strong></span>) method. This method automatically estimates the <code class="email">α</code>Dirichlet parameter vector.</li><li class="listitem"><code class="email">LDA_VEM_α</code>: This is an LDA model trained with VEM, but the difference here is that the <code class="email">α</code>Dirichlet parameter vector is not estimated.</li><li class="listitem"><code class="email">LDA_GIB</code>: This is an LDA model trained with Gibbs sampling.</li><li class="listitem"><code class="email">CTM_VEM</code>: This is an <a id="id847" class="calibre1"/>implementation of the <span class="strong"><strong class="calibre2">Correlated Topic Model</strong></span> (<span class="strong"><strong class="calibre2">CTM</strong></span>) model trained with VEM. Currently, the <code class="email">topicmodels</code> package does not support training this method with Gibbs sampling.</li></ul></div><p class="calibre8">To train an LDA model, the <code class="email">topicmodels</code> package provides us with the <code class="email">LDA()</code> function. We will use four key parameters for this function. The first of these specifies the document term matrix for which we want to build an LDA model. The second of these, <code class="email">k</code>, specifies the target number of topics we want to have in our model. The third parameter, <code class="email">method</code>, allows us to select which training algorithm to use. This is set to <code class="email">VEM</code> by default, so we only need to specify this for our <code class="email">LDA_GIB</code> model , which uses Gibbs sampling.</p><p class="calibre8">Finally, there is a <code class="email">control</code> parameter, which takes in a list of parameters that affect the fitting process. As there is an inherent random component involved in the training of topic models, we can specify a <code class="email">seed</code> parameter in this list in order to make the results reproducible. Additionally, this is where we can specify whether we want to estimate the <span class="strong"><em class="calibre9">α</em></span>Dirichlet parameter. This is also where we can include parameters for the Gibbs sampling procedure, such as the number of omitted Gibbs iterations at the start of the training procedure (<code class="email">burnin</code>), the number of omitted in-between iterations (<code class="email">thin</code>), and the total number of Gibbs iterations (<code class="email">iter</code>). To train a CTM model, the <code class="email">topicmodels</code> package provides us with the <code class="email">CTM()</code> function, which has a similar syntax to the <code class="email">LDA()</code> function.</p><p class="calibre8">Using this knowledge, we'll define a function that creates a list of four trained models given a particular document term matrix, the number of topics required, and the seed. For this function, we have used some standard values for the aforementioned training parameters with which the reader is encouraged to experiment, ideally after investigating the references provided for the two optimization methods:</p><div class="informalexample"><pre class="programlisting">compute_model_list&lt;- function (k, topic_seed, myDtm){
LDA_VEM&lt;- LDA(myDtm, k = k, control = list(seed = topic_seed))
LDA_VEM_a&lt;- LDA(myDtm, k = k, control = list(estimate.alpha = 
                   FALSE, seed = topic_seed))
LDA_GIB&lt;- LDA(myDtm, k = k, method = "Gibbs", control = 
                 list(seed = topic_seed, burnin = 1000, thin = 
                 100, iter = 1000))
CTM_VEM&lt;- CTM(myDtm, k = k, control = list(seed = topic_seed, 
var = list(tol = 10^-4), em = list(tol = 10^-3)))
  return(list(LDA_VEM = LDA_VEM, LDA_VEM_a = LDA_VEM_a, 
LDA_GIB = LDA_GIB, CTM_VEM = CTM_VEM))
}</pre></div><p class="calibre8">We'll now use this function to train a list of models for the two datasets:</p><div class="informalexample"><pre class="programlisting">&gt; library("topicmodels")
&gt; k &lt;- 5
&gt;topic_seed&lt;- 5798252
&gt;bbc_models&lt;- compute_model_list(k, topic_seed,bbc_dtm)
&gt;bbcsports_models&lt;- compute_model_list(k, topic_seed, 
bbcsports_dtm)</pre></div><p class="calibre8">To get a <a id="id848" class="calibre1"/>sense of how the topic models have performed, let's first see whether the five topics learned by each model correspond to the five topics to which the articles were originally assigned. Given one of these trained models, we can use the <code class="email">topics()</code> function to get a vector of the most likely topic chosen for each document.</p><p class="calibre8">This function actually takes a second parameter, <span class="strong"><em class="calibre9">k</em></span>, which is by default set to <code class="email">1</code> and returns the top <span class="strong"><em class="calibre9">k</em></span> topics predicted by the model. We only want one topic per model in this particular instance. Having found the most likely topic, we can then tabulate the predicted topics against the vector of labeled topics. These are the results for the <code class="email">LDA_VEM</code> model for the BBC dataset:</p><div class="informalexample"><pre class="programlisting">&gt;model_topics&lt;- topics(bbc_models$LDA_VEM)
&gt; table(model_topics, bbc_gold_factor)
bbc_gold_factor
model_topics business entertainment politics sport tech
           1       11           174        2     0  176
           2        4           192        1     0  202
           3      483             3       10     0    7
           4        9            17      403     4   15
           5        3             0        1   507    1</pre></div><p class="calibre8">Looking at this table, we can see that topic 5 corresponds almost exclusively to the <span class="strong"><em class="calibre9">sports</em></span> category. Similarly, topics 4 and 3 seem to match the <span class="strong"><em class="calibre9">politics</em></span> and <span class="strong"><em class="calibre9">business</em></span> categories respectively. Unfortunately, models 1 and 2 both contain a mixture of <span class="strong"><em class="calibre9">entertainment</em></span> and <span class="strong"><em class="calibre9">technology</em></span> articles, and as a result this model hasn't really succeeded in distinguishing between the categories that we want.</p><p class="calibre8">It should be clear that, in an ideal situation, each model topic should match to one gold topic (we often use the adjective <span class="strong"><em class="calibre9">gold</em></span> to refer to the correct or labeled value of a particular variable. This <a id="id849" class="calibre1"/>is derived from the expression <span class="strong"><em class="calibre9">gold standard</em></span>, which refers to a widely accepted standard). We can repeat this process on the <code class="email">LDA_GIB</code> model, where the story is different:</p><div class="informalexample"><pre class="programlisting">&gt;model_topics&lt;- topics(bbc_models$LDA_GIB)
&gt; table(model_topics, bbc_gold_factor)
bbc_gold_factor
model_topics business entertainment politics sport tech
           1      471             2       12     1    5
           2        0             0        3   506    3
           3        9             4        1     0  371
           4       27            16      399     3    9
           5        3           364        2     1   13</pre></div><p class="calibre8">Intuitively, we feel that this topic model is a better match to our original topics than the first, as evidenced by the fact that each model topic selects articles from primarily one gold topic.</p><p class="calibre8">A rough way to estimate the quality of the match between a topic model and our target vector of topics is to say that the largest value in every row corresponds to the gold topic assigned to the model topic represented by that row. Then, the total accuracy is the ratio of these maximum row values over the total number of documents. In the preceding example, for the <code class="email">LDA_GIB</code> model, this number would be <span class="strong"><em class="calibre9">(471+506+371+399+364)/2225 = 2111/2225= 94.9 %</em></span>. The following function computes this value given a model and a vector of gold topics:</p><div class="informalexample"><pre class="programlisting">compute_topic_model_accuracy&lt;- function(model, gold_factor) {
model_topics&lt;- topics(model)
model_table&lt;- table(model_topics, gold_factor)
model_matches&lt;- apply(model_table, 1, max)
model_accuracy&lt;- sum(model_matches) / sum(model_table)
  return(model_accuracy)
}</pre></div><p class="calibre8">Using this notion of accuracy, let's see which model performs better in our two datasets:</p><div class="informalexample"><pre class="programlisting">&gt;sapply(bbc_models, function(x) 
compute_topic_model_accuracy(x, bbc_gold_factor))
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.7959551 0.7923596 0.9487640 0.6148315 
&gt;sapply(bbcsports_models, function(x) 
compute_topic_model_accuracy(x, bbcsports_gold_factor))
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.7924016 0.7788331 0.7856174 0.7503392</pre></div><p class="calibre8">For the BBC dataset, we see that the <code class="email">LDA_GIB</code> model significantly outperforms the others and the <code class="email">CTM_VEM</code> model is significantly worse than the LDA models. For the BBCSports dataset, all the models perform roughly the same, but the <code class="email">LDA_VEM</code> model is slightly better.</p><p class="calibre8">Another <a id="id850" class="calibre1"/>way to assess the quality of a model fit is by computing the log likelihood of the data given the model, remembering that the larger this value, the better the fit. We can do this with the <code class="email">logLik()</code> function in the <code class="email">topicmodels</code> package, which suggests that the best model is the LDA model trained with Gibbs sampling in both cases:</p><div class="informalexample"><pre class="programlisting">&gt;sapply(bbc_models, logLik)
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
 -3201542  -3274005  -3017399  -3245828
&gt;sapply(bbcsports_models, logLik)
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
-864357.7 -886561.9 -813889.7 -868561.9 </pre></div></div>

<div class="book" title="Modeling the topics of online news stories">
<div class="book" title="Model stability"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec94" class="calibre1"/>Model stability</h2></div></div></div><p class="calibre8">It turns <a id="id851" class="calibre1"/>out that the random component of the optimization procedures involved in fitting these models often has a significant impact on the model that is trained. Put differently, we may find that if we use different random number seeds, the results may sometimes change appreciably.</p><p class="calibre8">Ideally, we <a id="id852" class="calibre1"/>would like our model to be <span class="strong"><strong class="calibre2">stable</strong></span>, which is to say that we would like the effect of the initial conditions of the optimization procedure, which are determined by a random number seed, to be minimal. It is a good idea to investigate the effect of different seeds on our four models by training them on multiple seeds:</p><div class="informalexample"><pre class="programlisting">&gt;seeded_bbc_models&lt;- lapply(5798252 : 5798256, 
              function(x) compute_model_list(k, x, bbc_dtm))
&gt;seeded_bbcsports_models&lt;- lapply(5798252 : 5798256, 
              function(x) compute_model_list(k, x, bbcsports_dtm))</pre></div><p class="calibre8">Here, we used a sequence of five consecutive seeds and trained our models on both datasets five times. Having done this, we can investigate the accuracy of our models for the various seeds. If the accuracy of a method does not vary by a large degree across the seeds, we can infer that the method is quite stable and produces similar topic models (although, in this case, we are only considering the most prominent topic per document).</p><div class="informalexample"><pre class="programlisting">&gt;seeded_bbc_models_acc&lt;- sapply(seeded_bbc_models, 
  function(x) sapply(x, function(y) 
compute_topic_model_accuracy(y, bbc_gold_factor)))
&gt;seeded_bbc_models_acc
               [,1]      [,2]      [,3]      [,4]      [,5]
LDA_VEM   0.7959551 0.7959551 0.7065169 0.7065169 0.7757303
LDA_VEM_a 0.7923596 0.7923596 0.6916854 0.6916854 0.7505618
LDA_GIB   0.9487640 0.9474157 0.9519101 0.9501124 0.9460674
CTM_VEM   0.6148315 0.5883146 0.9366292 0.8026966 0.7074157
&gt;seeded_bbcsports_models_acc&lt;- sapply(seeded_bbcsports_models, 
  function(x) sapply(x, function(y) 
compute_topic_model_accuracy(y, bbcsports_gold_factor)))
&gt;seeded_bbcsports_models_acc
               [,1]      [,2]      [,3]      [,4]      [,5]
LDA_VEM   0.7924016 0.7924016 0.8616011 0.8616011 0.9050204
LDA_VEM_a 0.7788331 0.7788331 0.8426052 0.8426052 0.8914518
LDA_GIB   0.7856174 0.7978290 0.8073270 0.7978290 0.7761194
CTM_VEM   0.7503392 0.6309362 0.7435550 0.8995929 0.6526459</pre></div><p class="calibre8">On both datasets, we can clearly see that Gibbs sampling results in a more stable model and, in the case of the BBC dataset, it is also the clear winner in terms of accuracy. Gibbs sampling generally tends to produce more accurate models but, even though it was not readily apparent on these datasets, it can become significantly slower than VEM methods once the dataset becomes large.</p><p class="calibre8">The two <a id="id853" class="calibre1"/>LDA models trained with variational methods exhibit scores that vary within a roughly 10 % range on both datasets. On both datasets, we see that <code class="email">LDA_VEM</code> is consistently better than <code class="email">LDA_VEM_a</code> by a small amount. This method also produces, on average, better accuracy among all models in the BBCSports dataset. The CTM model is the least stable of all the models, exhibiting a high degree of variance on both datasets. Interestingly, though, the highest performance of the CTM model across the five iterations performs marginally worse than the best accuracy possible using the other methods.</p><p class="calibre8">If we see that our model is not very stable across a few seeded iterations, we can specify the <code class="email">nstart</code> parameter during training, which specifies the number of random restarts that are used during the optimization procedure. To see how this works in practice, we have created a modified <code class="email">compute_model_list()</code> function that we named <code class="email">compute_model_list_r()</code>, which takes in an extra parameter, <code class="email">nstart</code>.</p><p class="calibre8">The other difference is that the <code class="email">seed</code> parameter now needs a vector of seeds with as many entries as the number of random restarts. To deal with this, we will simply create a suitably sized range of seeds starting from the one provided. Here is our new function:</p><div class="informalexample"><pre class="programlisting">compute_model_list_r&lt;- function (k, topic_seed, myDtm, nstart) {
seed_range&lt;- topic_seed : (topic_seed + nstart - 1)
LDA_VEM&lt;- LDA(myDtm, k = k, control = list(seed = seed_range, 
nstart = nstart))
LDA_VEM_a&lt;- LDA(myDtm, k = k, control = list(estimate.alpha = 
                 FALSE, seed = seed_range, nstart = nstart))
LDA_GIB&lt;- LDA(myDtm, k = k, method = "Gibbs", control = 
                 list(seed = seed_range, burnin = 1000, thin = 
                 100, iter = 1000, nstart = nstart))
CTM_VEM&lt;- CTM(myDtm, k = k, control = list(seed = seed_range, 
var = list(tol = 10^-4), em = list(tol = 10^-3), 
nstart = nstart))
  return(list(LDA_VEM = LDA_VEM, LDA_VEM_a = LDA_VEM_a, 
LDA_GIB = LDA_GIB, CTM_VEM = CTM_VEM))
}</pre></div><p class="calibre8">We will use this function to create a new model list. Note that using random restarts means we are <a id="id854" class="calibre1"/>increasing the amount of time needed to train, so these next few commands will take some time to complete:</p><div class="informalexample"><pre class="programlisting">&gt;nstart&lt;- 5
&gt;topic_seed&lt;- 5798252
&gt;nstarted_bbc_models_r&lt;- 
compute_model_list_r(k, topic_seed, bbc_dtm, nstart)
&gt;nstarted_bbcsports_models_r&lt;- 
compute_model_list_r(k, topic_seed, bbcsports_dtm, nstart)
&gt;sapply(nstarted_bbc_models_r, function(x) 
compute_topic_model_accuracy(x, bbc_gold_factor))
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.7959551 0.7923596 0.9487640 0.9366292 
&gt;sapply(nstarted_bbcsports_models_r, function(x) 
compute_topic_model_accuracy(x, bbcsports_gold_factor))
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.9050204 0.8426052 0.7991859 0.8995929</pre></div><p class="calibre8">Note that, even after using only five random restarts, the accuracy of the models has improved. More importantly, we now see that using random restarts has overcome the fluctuations that the CTM model experiences, and as a result it is now performing almost as well as the best model in each dataset.</p></div></div>

<div class="book" title="Modeling the topics of online news stories">
<div class="book" title="Finding the number of topics"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch11lvl2sec95" class="calibre1"/>Finding the number of topics</h2></div></div></div><p class="calibre8">In this <a id="id855" class="calibre1"/>predictive task, the number of different topics was known beforehand. This turned out to be very important because it is provided as an input to the functions that trained our models. The number of topics might not be known when we are using topic modeling as a form of exploratory analysis where our goal is simply to cluster documents together based on the similarity of their topics.</p><p class="calibre8">This is a challenging question and bears some similarity to the general problem of selecting the number of clusters when we perform clustering. One proposed solution to this problem is to perform cross-validation over a range of different numbers of topics. This approach will not scale well at all when the dataset is large, especially since training a single topic model is <a id="id856" class="calibre1"/>already quite computationally intensive when we factor, issues such as random restarts.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note45" class="calibre1"/>Note</h3><p class="calibre8">A paper that discusses a number of different approaches for estimating the number of topics in topic models is <span class="strong"><em class="calibre9">Reconceptualizing the classification of PNAS articles</em></span> by <span class="strong"><em class="calibre9">Edoardo M. Airoldi</em></span> and others. This appears in the <span class="strong"><em class="calibre9">Proceedings of the National Academy of Sciences</em></span>, volume 107, 2010.</p></div></div></div>

<div class="book" title="Modeling the topics of online news stories">
<div class="book" title="Topic distributions"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch11lvl2sec96" class="calibre1"/>Topic distributions</h2></div></div></div><p class="calibre8">We saw <a id="id857" class="calibre1"/>in the description of the generative process that we use a Dirichlet distribution to sample a multinomial distribution of topics. In the <code class="email">LDA_VEM</code> model, the <span class="strong"><em class="calibre9">αk</em></span> parameter vector is estimated. Note that, in all cases, a symmetric distribution is used in this implementation so that we are only estimating the value of <span class="strong"><em class="calibre9">α</em></span>, which is the value that all the <span class="strong"><em class="calibre9">α<sub class="calibre14">k</sub></em></span> parameters take on. For LDA models, we can investigate which value of this parameter is used with and without estimation:</p><div class="informalexample"><pre class="programlisting">&gt;bbc_models[[1]]@alpha
[1] 0.04893411
&gt;bbc_models[[2]]@alpha
[1] 10
&gt;bbcsports_models[[1]]@alpha
[1] 0.04037119
&gt;bbcsports_models[[2]]@alpha
[1] 10</pre></div><p class="calibre8">As we can see, when we estimate the value of <span class="strong"><em class="calibre9">α</em></span>, we obtain a much lower value of <span class="strong"><em class="calibre9">α</em></span> than we use by default, indicating that, for both datasets, the topic distribution is thought to be peaky. We can use the <code class="email">posterior()</code> function in order to view the distribution of topics for each model. For example, for the <code class="email">LDA_VEM</code> model on the BBC dataset, we find the following distributions of topics for the first few articles:</p><div class="informalexample"><pre class="programlisting">&gt; options(digits = 4)
&gt; head(posterior(bbc_models[[1]])$topics)
                     1         2      3         4         5
business.001 0.2700360 0.0477374 0.6818 0.0002222 0.0002222
business.002 0.0002545 0.0002545 0.9990 0.0002545 0.0002545
business.003 0.0003257 0.0003257 0.9987 0.0003257 0.0003257
business.004 0.0002153 0.0002153 0.9991 0.0002153 0.0002153
business.005 0.0337131 0.0004104 0.9651 0.0004104 0.0004104
business.006 0.0423153 0.0004740 0.9563 0.0004740 0.0004740</pre></div><p class="calibre8">The following plot is a histogram of the posterior probability of the most likely topic predicted by our four models. The <code class="email">LDA_VEM</code> model assumes a very peaky distribution, whereas the other models have a wider spread. The <code class="email">CTM_VEM</code> model also has a peak at very high probabilities but, unlike <code class="email">LDA_VEM</code>, the probability mass is spread over a wide range of values. We can see that the minimum probability for the most likely topic is 0.2 because we have five topics:</p><div class="mediaobject"><img src="../images/00189.jpeg" alt="Topic distributions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Another <a id="id858" class="calibre1"/>approach to estimating the smoothness of the topic distributions is to compute the <span class="strong"><em class="calibre9">model entropy</em></span>. We will define this as the average entropy of all the topic distributions across the different documents. Smooth distributions will exhibit higher entropy than peaky distributions. To compute the entropy of our model, we will define two functions. The function <code class="email">compute_entropy()</code> computes the entropy of a particular topic distribution of a document, and the <code class="email">compute_model_mean_entropy()</code> function computes the average entropy across all the different documents in the model:</p><div class="informalexample"><pre class="programlisting">compute_entropy&lt;- function(probs) {
  return(- sum(probs * log(probs)))
}
compute_model_mean_entropy&lt;- function(model) {
  topics &lt;- posterior(model)$topics
  return(mean(apply(topics, 1, compute_entropy)))
}</pre></div><p class="calibre8">Using these functions, we can compute the average model entropies for the models trained on our two datasets:</p><div class="informalexample"><pre class="programlisting">&gt;sapply(bbc_models, compute_model_mean_entropy)
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.3119491 1.2664310 1.2720891 0.8373708 
&gt;sapply(bbcsports_models, compute_model_mean_entropy)
LDA_VEMLDA_VEM_aLDA_GIBCTM_VEM
0.3058856 1.3084006 1.3421798 0.7545975</pre></div><p class="calibre8">These <a id="id859" class="calibre1"/>results are consistent with what the preceding plots show, which is that the <code class="email">LDA_VEM</code> model, which is the peakiest, has a much lower entropy than the other models.</p></div></div>

<div class="book" title="Modeling the topics of online news stories">
<div class="book" title="Word distributions"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch11lvl2sec97" class="calibre1"/>Word distributions</h2></div></div></div><p class="calibre8">Just as <a id="id860" class="calibre1"/>with the previous section, where we looked at the distribution of topics across different documents, we are often also interested in understanding the most important terms that are frequent in documents that are assigned to the same topic. We can see the <span class="strong"><em class="calibre9">k</em></span> most frequent terms of the topics of a model using the function <code class="email">terms()</code>. This takes in a model and a number specifying the number of most frequent terms that we want retrieved. Let's see the 10 most important words per topic in the <code class="email">LDA_GIB</code> model of the BBC dataset:</p><div class="informalexample"><pre class="programlisting">&gt;GIB_bbc_model&lt;- bbc_models[[3]]
&gt; terms(GIB_bbc_model, 10)
      Topic 1   Topic 2   Topic 3     Topic 4  Topic 5 
 [1,] "year""plai""peopl""govern""film"
 [2,] "compani""game""game""labour""year"
 [3,] "market""win""servic""parti""best"
 [4,] "sale""against""technolog""elect""show"
 [5,] "firm""england""mobil""minist""includ"
 [6,] "expect""first""on""plan""on"
 [7,] "share""back""phone""sai""award"
 [8,] "month""player""get""told""music"
 [9,] "bank""world""work""peopl""top"
[10,] "price""time""wai""public""star"</pre></div><p class="calibre8">As we can see, given this list of word stems, one could easily guess which of the five topic labels we should assign to each topic. A very handy way to visualize frequent terms in a collection of documents is through a <span class="strong"><strong class="calibre2">word cloud</strong></span>. The R package <code class="email">wordcloud</code> is useful for creating these. The function <code class="email">wordcloud()</code> allows us to specify a vector of terms followed by a vector of their frequencies, and this information is then used for plotting.</p><p class="calibre8">Unfortunately, we will have to do some manipulation on the document term matrices in order to <a id="id861" class="calibre1"/>get the word frequencies by topic so that we can <a id="id862" class="calibre1"/>feed them into this function. To that end, we've created our own function <code class="email">plot_wordcloud()</code>, as follows:</p><div class="informalexample"><pre class="programlisting">plot_wordcloud&lt;- function(model, myDtm, index, numTerms) {
model_terms&lt;- terms(model,numTerms)
model_topics&lt;- topics(model)
terms_i&lt;- model_terms[,index]
topic_i&lt;- model_topics == index
dtm_i&lt;- myDtm[topic_i, terms_i]
frequencies_i&lt;- colSums(as.matrix(dtm_i))
wordcloud(terms_i, frequencies_i, min.freq = 0)
}</pre></div><p class="calibre8">Our function takes in a model, a document term matrix, an index of a topic, and the number of most frequent terms that we want to display in the word cloud. We begin by first computing the most frequent terms for the model by topic, as we did earlier. We also compute the most probable topic assignments. Next, we subset the document term matrix so that we obtain only the cells involving the terms we are interested in and the documents corresponding to the topic with the index that we passed in as a parameter.</p><p class="calibre8">From this reduced document term matrix, we sum over the columns to compute the frequencies of the most frequent terms, and finally we plot the word cloud. We've used this function to plot word clouds for the topics in the BBC dataset using the <code class="email">LDA_GIB</code> model and 25 words per topic. This is shown here:</p><div class="mediaobject"><img src="../images/00190.jpeg" alt="Word distributions" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Modeling the topics of online news stories">
<div class="book" title="LDA extensions"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch11lvl2sec98" class="calibre1"/>LDA extensions</h2></div></div></div><p class="calibre8">Topic models <a id="id863" class="calibre1"/>are an active area of research, and as a result several extensions for the LDA model have been proposed. We will briefly mention two of these. The first is the <span class="strong"><strong class="calibre2">supervised LDA</strong></span> model, an implementation of which can be found <a id="id864" class="calibre1"/>in the <code class="email">lda</code> R package. This is a more direct way to model a response variable with the standard LDA method and would be a good next step for investigating the application discussed in this chapter.</p><p class="calibre8">A second <a id="id865" class="calibre1"/>interesting extension is the <span class="strong"><strong class="calibre2">author-topic model</strong></span>. This is designed to add an extra step in the generative process to account for authorship information and is a good model to use when building models that summarize or predict the writing habits and topics of authors.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note46" class="calibre1"/>Note</h3><p class="calibre8">The standard reference for supervised LDA is the paper <span class="strong"><em class="calibre9">Supervised Topic Models</em></span> by <span class="strong"><em class="calibre9">David M. Blei</em></span> and <span class="strong"><em class="calibre9">Jon D. McAuliffe</em></span>. This was published in 2007 in the journal <span class="strong"><em class="calibre9">Neural Information Processing Systems</em></span>. For the author-topic model, consult the paper entitled <span class="strong"><em class="calibre9">The Author-Topic Model for Authors and Documents</em></span> by <span class="strong"><em class="calibre9">Michal Rosen-Zvi</em></span> and others. This appears in the proceedings of the <span class="strong"><em class="calibre9">20th conference on Uncertainty In Artificial Intelligence</em></span>.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Modeling tweet topics" id="2H1VQ1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec78" class="calibre1"/>Modeling tweet topics</h1></div></div></div><p class="calibre8">In machine <a id="id866" class="calibre1"/>learning and natural language processing, a <span class="strong"><em class="calibre9">topic model</em></span> is a type of statistical model used to discover the abstract topics that occur in a collection of documents. A good example or use case to illustrate this concept is <span class="strong"><em class="calibre9">Twitter</em></span>. Suppose we could analyze an individual's (or an organization's) tweets to discover any overriding trend. Let's look at a simple example.</p><p class="calibre8">If you have a Twitter account, you can perform this exercise pretty easily (you can then apply the same process to an archive of tweets you want to focus on and/or model). First, we need to create a tweet archive file.</p><p class="calibre8">Under <span class="strong"><strong class="calibre2">Settings</strong></span>, you can submit a request to receive your tweets in an archive file. Once it's ready, you'll get an email with a link to download it:</p><div class="mediaobject"><img src="../images/00191.jpeg" alt="Modeling tweet topics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">And then save your file locally:</p><div class="mediaobject"><img src="../images/00192.jpeg" alt="Modeling tweet topics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now that <a id="id867" class="calibre1"/>we have a data source to work with, we can move the tweets into a list <span class="strong"><em class="calibre9">object</em></span> (we'll call it <span class="strong"><em class="calibre9">x</em></span>) and then convert that into an R data frame object (df1):</p><div class="mediaobject"><img src="../images/00193.jpeg" alt="Modeling tweet topics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The tweets were first converted to a <span class="strong"><em class="calibre9">data frame</em></span> before using the R <code class="email">tm</code> package to convert them to a <span class="strong"><em class="calibre9">corpus</em></span> or Corpus collection (of text documents) object:</p><div class="mediaobject"><img src="../images/00194.jpeg" alt="Modeling tweet topics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Next, we convert the Corpus to a <span class="strong"><em class="calibre9">Document-Term Matrix</em></span> object with the following code. This <a id="id868" class="calibre1"/>creates a <span class="strong"><em class="calibre9">mathematical matrix</em></span> that describes the <span class="strong"><em class="calibre9">frequency of terms</em></span> that occur in a collection of documents, in this case, our collection of tweets:</p><div class="mediaobject"><img src="../images/00195.jpeg" alt="Modeling tweet topics" class="calibre10"/></div><p class="calibre11"> </p></div>

<div class="book" title="Modeling tweet topics" id="2H1VQ1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Word clouding"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch11lvl2sec99" class="calibre1"/>Word clouding</h2></div></div></div><p class="calibre8">After <a id="id869" class="calibre1"/>building a document-term matrix (shown earlier), we can more easily show the importance of the words found within our tweets with a <span class="strong"><em class="calibre9">word cloud</em></span> (also known as a tag cloud). We can do this using the R package <code class="email">wordcloud</code>:</p><div class="mediaobject"><img src="../images/00196.jpeg" alt="Word clouding" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Finally, let's generate the word cloud visual:</p><div class="mediaobject"><img src="../images/00197.jpeg" alt="Word clouding" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Seems <a id="id870" class="calibre1"/>like there may be a theme involved here! The word cloud shows us that the words <span class="strong"><strong class="calibre2">south</strong></span> and <span class="strong"><strong class="calibre2">carolinas</strong></span> are the most important words.</p></div></div>
<div class="book" title="Summary" id="2I0GC1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec79" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter was devoted to learning about topic models; after sentiment analysis on movie reviews, this was our second foray into working with real-life text data. This time, our predictive task was classifying the topics of news articles on the web. The primary technique for topic modeling on which we focused was LDA. This derives its name from the fact that it assumes that the topic and word distributions that can be found inside a document arise from hidden multinomial distributions that are sampled from Dirichlet priors. We saw that the generative process of sampling words and topics from these multinomial distributions mirrors many of the natural intuitions that we have about this domain; however, it signally fails to account for correlations between the various topics that can co-occur inside a document.</p><p class="calibre8">In our experiments with LDA, we saw that there is more than one way to fit an LDA model, and in particular we saw that the method known as Gibbs sampling tends to be more accurate, even if it often is more computationally expensive. In terms of performance, we saw that, when the topics in question are quite distinct from each other, such as the topics in the BBC dataset, we got very high accuracy in our topic prediction.</p><p class="calibre8">At the same time, however, when we classified documents with topics that are more similar to each other, such as the different sports documents in the BBCSports dataset, we saw that this posed more of a challenge and our results were not quite as high. In our case, another factor that probably played a role is that both the documents and the available features were much fewer in number than in the BBCSports dataset. Currently, an increasing number of variations on LDA are being researched and developed in order to deal with limitations in both performance and training speed.</p><p class="calibre8">As an interesting exercise, we also downloaded an archive of tweets and used R commands to create a <span class="strong"><strong class="calibre2">document-term matrix</strong></span> object, which we then used as an input for creating a word cloud object that visualized the words found within the tweets.</p><p class="calibre8">Topic models can be viewed as a form of clustering, and this was our first glimpse in to this area. In the next chapter on recommendation systems, we will delve more deeply into the field of clustering in order to understand the way in which websites such as Amazon are able to make product recommendations by predicting which products a shopper is most likely to be interested in based on their previous shopping history and the shopping habits of similar shoppers.</p></div></body></html>