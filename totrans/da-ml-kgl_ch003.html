<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>Data Analysis and Machine Learning with Kaggletrue</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>

<section id="organizing-data-with-datasets" class="level1 pkt" data-number="3">
<h1 data-number="3">Organizing Data with Datasets</h1>
<p>In his story “The Adventure of the Copper Breeches”, Arthur Conan Doyle has Sherlock Holmes shout “<em>Data! Data! Data! I cannot make bricks without clay</em>”—and this mindset, which served the most famous detective in literature so well, should be adopted by every data scientist. For that reason, we begin the more technical part of this book with a chapter dedicated to data: specifically, in the Kaggle context, leveraging the power of Kaggle Datasets functionality for our purposes.</p>
<section id="setting-up-a-dataset" class="level2" data-number="3.1">
<h2 data-number="3.1">Setting up a dataset</h2>
<p>In principle, any data you can use (subject to limitations—see the legal caveats section below), you can upload to Kaggle. The specific limits at the time of writing this book are: 20 gigabytes per dataset and 100 gb total quota. Keep in mind that the size limit per single dataset is calculated uncompressed—uploading compressed versions speeds up the transfer but does not help against the limits. You can check the most recent documentation of the datasets at this link:</p>
<p><a href="https://www.kaggle.com/docs/datasets">https://www.kaggle.com/docs/datasets</a></p>
<p>Kaggle promotes itself as a “home of open data science” and the impressive collection of datasets available from the site certainly lends some credence to that claim: before uploading the data for your project into a dataset, make sure to check the existing content—for several popular applications, there is a chance it has already been stored there:</p>
<figure>
<img src="../media/file1.png" />
</figure>
<p>For the sake of this introduction, let us assume the kind of data you will be using in your project is not already there—so you need to create a new one. When you head to the menu with three lines on the left-hand side and click on <strong>Data</strong> you will be redirected to the dataset page:</p>
<figure>
<img src="../media/file2.png" />
</figure>
<p>When you click on <strong>New Dataset</strong> you will be prompted for the basics: uploading the actual data and giving it a title:</p>
<figure>
<img src="../media/file3.png" />
</figure>
<p>Keep in mind that Kaggle is a popular platform, so numerous people upload their data there—including private (not publicly visible) ones—so try to think of a non-generic title.</p>
<p>Voila! Your first dataset is ready. You can then head to the <strong>Data</strong> tab:</p>
<figure>
<img src="../media/file4.png" />
</figure>
<p>In principle you do not have to fill out all the fields—your newly created dataset is perfectly usable without them (and if it is a private one, you probably do not care—after all you know what is in it). However, the community etiquette would suggest filling the info for the ones you make public: the more you specify, the more usable the data will be to others (and measured by the usability score, displayed in the upper right corner).</p>
</section>
<section id="gathering-the-data" class="level2" data-number="3.2">
<h2 data-number="3.2">Gathering the data</h2>
<p>Apart from legal aspect (see the last section of this chapter), there is no real limit on the kind of content you can store in the datasets: tabular data, images, text—if you fit within the size requirements, you can store it. This includes data harvested from other sources: tweets by hashtag or topic are among the popular datasets at the time of writing:</p>
<figure>
<img src="../media/file5.png" />
</figure>
<p>Discussion of the different frameworks for harvesting data from social media (Twitter, Reddit etc) is outside the scope of this book.</p>
</section>
<section id="using-the-kaggle-datasets-outside-of-kaggle" class="level2" data-number="3.3">
<h2 data-number="3.3">Using the Kaggle datasets outside of Kaggle</h2>
<p>Kaggle kernels are free to use, but not without limits (more on that in <em>Chapter 4</em>)—and the first one you are likely to hit is the time limit of 8 hours. A popular alternative is to move to Google Colab a free Jupyter notebook environment that runs entirely in the cloud:</p>
<p><a href="https://colab.research.google.com">https://colab.research.google.com</a></p>
<p>But even once we move the computations there, we might still want to have access to the Kaggle datasets—so importing them into Colab is a rather handy feature.</p>
<p>The first thing we do—since you are reading this, we assume you already are registered on Kaggle—is head to the account page to generate the API token:</p>
<ul>
<li>Go to “your account” and click on <strong>Create New API Token</strong></li>
<li>A file named <code>kaggle.json</code> containing your username and token will be created</li>
</ul>
<figure>
<img src="../media/file6.png" />
</figure>
<p>Next step is to create a folder named “<code>Kaggle</code>” in your drive and upload the <code>.json</code> there</p>
<figure>
<img src="../media/file7.png" />
</figure>
<p>Once done, you need to create a new Colab notebook and mount your drive:</p>
<figure>
<img src="../media/file8.png" />
</figure>
<p>Get the authorization code from the URL prompt and provide in the empty box, then execute the following code to prove the path to the <code>.json</code> config:</p>
<figure>
<img src="../media/file9.png" />
</figure>
<figure>
<img src="../media/file10.png" />
</figure>
<p>We can download the dataset now: begin by going to Kaggle and copying the API command:</p>
<figure>
<img src="../media/file11.png" />
</figure>
<p>Run the code:</p>
<figure>
<img src="../media/file12.png" />
</figure>
<p>The dataset will be downloaded as a <code>.zip</code> archive—unpack it and you are good to go.</p>
</section>
<section id="building-around-datasets" class="level2" data-number="3.4">
<h2 data-number="3.4">Building around datasets</h2>
<p>Once you have created a dataset, you probably want to use in your analysis. You can start a kernel using your dataset as a primary source: go to the <strong>Activity</strong> tab in the upper menu of your dataset page and scroll to this block:</p>
<figure>
<img src="../media/file13.png" />
</figure>
<p>Alternatively, you can start a conversation around the data by clicking on <strong>Create a discussion</strong>.</p>
</section>
<section id="legal-caveats" class="level2" data-number="3.5">
<h2 data-number="3.5">Legal caveats</h2>
<p>Just because you can put some data on Kaggle does not necessarily mean that you should—excellent example would be the “People of Tinder dataset”: in 2017, a developer used the Tinder API to scrape the website for semi-private profiles and uploaded the data on Kaggle. After the issue became known, Kaggle ended up taking the dataset down. You can read the full story here:</p>
<p><a href="https://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-looted-again-this-time-for-teaching-ai-to-genderize-faces/?sh=1afb86b25454">https://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-looted-again-this-time-for-teaching-ai-to-genderize-faces/?sh=1afb86b25454</a></p>
<p>In general, before you upload anything to Kaggle ask yourself two questions: is it legal (from a copyright standpoint—always check the licenses) and are there any risks associated with this dataset (privacy or otherwise).</p>
</section>
</section>
</body>
</html>
