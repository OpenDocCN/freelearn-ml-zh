<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Risk versus Reward â€“ Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we <span>will </span>go a little deeper and learn about one of the hot topics in machine learning: reinforcement learning. We will cover several exciting examples to show how you can use this in your application. We'll go over a few algorithms, and then after our first, more formal example, we will take you to a final exciting example that you are sure to enjoy!</p>
<p>The following topics will covered in this chapter:</p>
<ul>
<li>Overviewing reinforcement learning</li>
<li>Types of learning</li>
<li>Q-learning</li>
<li>SARSA</li>
<li>Running our application</li>
<li>Tower of Hanoi</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overviewing reinforcement learning</h1>
                </header>
            
            <article>
                
<p>As mentioned in <a href="7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml" target="_blank">Chapter 1</a>, <em>Machine Learning Basics</em>, reinforcement learning is a case where the machine is trained for a specific outcome with the sole purpose of maximizing efficiency and/or performance. The algorithm is rewarded for making correct decisions and penalized for making incorrect ones, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-682 image-border" src="assets/33096271-1bd8-4af9-b961-c9d556df32c4.png" style=""/></div>
<p>Continual training is used to constantly improve performance. The focus here is on performance, meaning somehow finding a balance between unseen data and what the algorithms <span>have</span><span> </span><span>already learned. The algorithm applies an action to its environment, receives a reward or a penalty based on what it has done, repeats the process, and so on.</span></p>
<p>We're going to dive right into the application in this chapter, and we're going to use the incredible Accord.NET open source machine learning framework to highlight how we can use reinforcement learning to help an autonomous object get from its starting location, depicted by a black object, to a desired end point, depicted by a red object.</p>
<p>The concept is similar, although on a much lower scale of complexity, to what autonomous vehicles do to get you from point A to point B. Our example will allow you to use maps of various complexity, meaning various obstacles may appear in between your autonomous object and the desired location. Let's look at our application:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-683 image-border" src="assets/0e7b2b7c-9d43-4f03-9d8e-4e1d9576a2b5.png" style=""/></div>
<p>Here, you can see that we have a very basic map loaded, one with no obstacles but only exterior confining walls. The black block (start) is our autonomous object and the red block (stop) is our destination. Our goal in this application is to navigate the walls to get to our desired location. If our next move puts us onto a white block, our algorithm will be rewarded. If our next move puts us into a wall, it will be penalized. From this, our autonomous object should be able to get to its destination. The question is: how fast can it learn? In this example, there are absolutely no obstacles in its path, so there should be no issues solving the problem in the shortest number of moves possible.</p>
<p>The following is another example of a somewhat more complicated map for our environment:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-684 image-border" src="assets/4c05f744-440e-460c-a01b-07de77b354c9.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of learning</h1>
                </header>
            
            <article>
                
<p>On the right-hand side of our application are our settings, as seen in the following screenshot. The first thing that we see is the learning algorithm. In this application, we will be dealing with two distinct learning algorithms, <strong>Q-learning</strong> and <strong><span>S</span><span>tate-Action-Reward-State-A</span><span>ction</span></strong> (<strong><span>SARSA</span></strong>). Let's briefly discuss both of these algorithms:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-685 image-border" src="assets/4c0a67d9-da76-4634-b528-5f839ad71d04.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning</h1>
                </header>
            
            <article>
                
<p>Q-learning can identify an optimal action (that which has the highest value in each state) while in a given state without having a completely defined model of the environment. It is also great at handling problems with stochastic transitions and rewards without requiring tweaking or adaptations.</p>
<p>Here is the mathematical intuition for Q-learning:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/57e66621-c1c9-4b6b-9fd7-463cb2612e4b.png" style="width:49.67em;height:5.67em;"/></div>
<p>Perhaps it's easier to comprehend if we provide a very high-level abstract example. The agent starts at state 1. It then performs action 1 and gets reward 1. Next, it looks around and sees what the maximum possible reward for an action in state 2 is; it uses that to update the value of action 1. And so on!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SARSA</h1>
                </header>
            
            <article>
                
<p><strong>SARSA</strong> (you can already guess where this one is, going by the name) works like this:</p>
<ol>
<li>The agent starts at state 1</li>
<li>It then performs action 1 and gets reward 1</li>
<li>Next, it moves on to state 2, performs action 2, and gets reward 2</li>
<li>Then, the agent goes back and updates the value of action 1</li>
</ol>
<p>As you can see, the difference in the two algorithms is in the way the future reward is found. Q-learning uses the highest action possible from state 2, while SARSA uses the value of the action that is actually taken.</p>
<p>Here is the mathematical intuition for SARSA:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/a07b00f1-71ee-4b11-aee4-7d5eb98c7e82.png" style="width:33.50em;height:1.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running our application</h1>
                </header>
            
            <article>
                
<p>For now, let's start using our application with our default parameters. Simply click on the <span class="packt_screen">Start</span> button and the learning will commence. Once this is complete, you will be able to click on the <span class="packt_screen">Show Solution</span> button, and the learned path will be animated from start to finish.</p>
<p>Clicking on <span class="packt_screen">Start</span> will begin the learning stage and continue until the black object reaches its goal:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-686 image-border" src="assets/24bfed96-785a-4460-b149-73ccf8778c20.png" style=""/></div>
<p>Here you will see that as the learning progresses, we are sending the output to <kbd>ReflectInsight</kbd> to help us see and learn what the algorithm is doing internally. You see that for each iteration, different object positions are being evaluated, and so are their actions and rewards:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-687 image-border" src="assets/e2221e39-43ce-497d-9929-65eb7dcfc453.png" style=""/></div>
<p>Once the learning is complete, we can click on the <span class="packt_screen">Show Solution</span> button to replay the final solution. When complete, the black object will sit atop the red object:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-688 image-border" src="assets/6a2ea377-0fdb-44ab-8753-94ba7fd68298.png" style=""/></div>
<p>Now let's look at the code from our application. There are two methods of learning that we highlighted previously. Here's how Q-learning looks:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">int iteration = 0;<br/>             TabuSearchExploration tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;<br/>             EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy;<br/> <br/>             while ((!needToStop) &amp;&amp; (iteration &lt; learningIterations))<br/>             {<br/>                 explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate;<br/>                 qLearning.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate;<br/>                 tabuPolicy.ResetTabuList();<br/> <br/>                 var agentCurrentX = agentStartX;<br/>                 var agentCurrentY = agentStartY;<br/> <br/>                 int steps = 0;<br/>                 while ((!needToStop) &amp;&amp; ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY)))<br/>                 {<br/>                     steps++;<br/>                     int currentState = GetStateNumber(agentCurrentX, agentCurrentY);<br/>                     int action = qLearning.GetAction(currentState);<br/>                     double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);<br/>                     int nextState = GetStateNumber(agentCurrentX, agentCurrentY);<br/> <br/>                     // do learning of the agent - update his Q-function, set Tabu action<br/>                     qLearning.UpdateState(currentState, action, reward, nextState);<br/>                     tabuPolicy.SetTabuAction((action + 2) % 4, 1);<br/>                 }<br/> <br/>                 System.Diagnostics.Debug.WriteLine(steps);<br/>                 iteration++;<br/> <br/>                 SetText(iterationBox, iteration.ToString());<br/>             }</pre>
<p>How does SARSA learning differ? Let's take a look at the <kbd>while</kbd> loop of SARSA learning and understand:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">int iteration = 0;<br/>             TabuSearchExploration tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;<br/>             EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy;<br/> <br/>             while ((!needToStop) &amp;&amp; (iteration &lt; learningIterations))<br/>             {<br/>                 explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate;<br/>                 sarsa.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate;<br/>                 tabuPolicy.ResetTabuList();<br/> <br/>                 var agentCurrentX = agentStartX;<br/>                 var agentCurrentY = agentStartY;<br/>                 int steps = 1;<br/>                 int previousState = GetStateNumber(agentCurrentX, agentCurrentY);<br/>                 int previousAction = sarsa.GetAction(previousState);<br/>                 double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, previousAction);<br/> <br/>                 while ((!needToStop) &amp;&amp; ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY)))<br/>                 {<br/>                     steps++;<br/> <br/>                     tabuPolicy.SetTabuAction((previousAction + 2) % 4, 1);<br/>                     int nextState = GetStateNumber(agentCurrentX, agentCurrentY);<br/>                     int nextAction = sarsa.GetAction(nextState);<br/>                     sarsa.UpdateState(previousState, previousAction, reward, nextState, nextAction);<br/>                     reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, nextAction);<br/>                     previousState = nextState;<br/>                     previousAction = nextAction;<br/>                 }<br/> <br/>                 if (!needToStop)<br/>                 {<br/>                     sarsa.UpdateState(previousState, previousAction, reward);<br/>                 }<br/> <br/>                 System.Diagnostics.Debug.WriteLine(steps);<br/> <br/>                 iteration++;<br/> <br/>                 SetText(iterationBox, iteration.ToString());<br/>             }</pre>
<p>Our last step is to see how we can animate the solution. This will be needed for us to see that our algorithm achieved its goal. Here is the code:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">TabuSearchExploration tabuPolicy;<br/> <br/>             if (qLearning != null)<br/>                 tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;<br/>             else if (sarsa != null)<br/>                 tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;<br/>             else<br/>                 throw new Exception();<br/> <br/>             var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy;<br/>             explorationPolicy.Epsilon = 0;<br/>             tabuPolicy?.ResetTabuList();<br/>             int agentCurrentX = agentStartX, agentCurrentY = agentStartY;<br/>             Array.Copy(map, mapToDisplay, mapWidth * mapHeight);<br/>             mapToDisplay[agentStartY, agentStartX] = 2;<br/>             mapToDisplay[agentStopY, agentStopX] = 3;</pre>
<p>And here is our <kbd>while</kbd> loop where all the magic happens!</p>
<pre class="mce-root CDPAlignLeft CDPAlign">while (!needToStop)<br/>             {<br/>                 cellWorld.Map = mapToDisplay;<br/>                 Thread.Sleep(200);<br/> <br/>                 if ((agentCurrentX == agentStopX) &amp;&amp; (agentCurrentY == agentStopY))<br/>                 {<br/>                     mapToDisplay[agentStartY, agentStartX] = 2;<br/>                     mapToDisplay[agentStopY, agentStopX] = 3;<br/>                     agentCurrentX = agentStartX;<br/>                     agentCurrentY = agentStartY;<br/>                     cellWorld.Map = mapToDisplay;<br/>                     Thread.Sleep(200);<br/>                 }<br/> <br/>                 mapToDisplay[agentCurrentY, agentCurrentX] = 0;<br/>                 int currentState = GetStateNumber(agentCurrentX, agentCurrentY);<br/>                 int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState);<br/>                 UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);<br/>                 mapToDisplay[agentCurrentY, agentCurrentX] = 2;<br/>             }</pre>
<p class="mce-root">Let's break this down into more digestible sections. The first thing that we do is establish our tabu policy. If you are not familiar with tabu searching, note that it is designed to enhance the performance of a local search by relaxing its rule. At each step, sometimes worsening a move is acceptable if there are no alternatives (moves with reward).</p>
<p class="mce-root">Additionally, prohibitions (tabu) are put in place to ensure that the algorithm does not return to the previously visited solution.</p>
<pre class="mce-root CDPAlignLeft CDPAlign">            TabuSearchExploration tabuPolicy;<br/> <br/>             if (qLearning != null)<br/>                 tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;<br/>             else if (sarsa != null)<br/>                 tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;<br/>             else<br/>                 throw new Exception();<br/> <br/>             var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy;<br/>             explorationPolicy.Epsilon = 0;<br/>             tabuPolicy?.ResetTabuList();</pre>
<p>Next, we have to position our agent and prepare the map.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c364af85-1701-4fa3-a222-40c17923fc6d.png" style=""/></div>
<p>Here is our main execution loop, which will show the animated solution:</p>
<pre class="mce-root CDPAlignLeft CDPAlign">while (!needToStop)<br/>             {<br/>                 cellWorld.Map = mapToDisplay;<br/>                 Thread.Sleep(200);<br/> <br/>                 if ((agentCurrentX == agentStopX) &amp;&amp; (agentCurrentY == agentStopY))<br/>                 {<br/>                     mapToDisplay[agentStartY, agentStartX] = 2;<br/>                     mapToDisplay[agentStopY, agentStopX] = 3;<br/>                     agentCurrentX = agentStartX;<br/>                     agentCurrentY = agentStartY;<br/>                     cellWorld.Map = mapToDisplay;<br/>                     Thread.Sleep(200);<br/>                 }<br/> <br/>                 mapToDisplay[agentCurrentY, agentCurrentX] = 0;<br/>                 int currentState = GetStateNumber(agentCurrentX, agentCurrentY);<br/>                 int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState);<br/>                 UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);<br/>                 mapToDisplay[agentCurrentY, agentCurrentX] = 2;<br/>             }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tower of Hanoi</h1>
                </header>
            
            <article>
                
<p>Since we've discussed Q-learning, I want to spend the rest of this chapter highlighting some fantastic work done by Kenan Deen. His Tower of Hanoi solution is a great example of how you can use reinforcement learning to solve real-world problems.</p>
<p>This form of reinforcement learning is more formally known as a <strong>Markov Decision Process</strong> (<strong>MDP</strong>). An MDP is a discrete-time stochastic control process, which means that at each time step, the process is in state <em>x</em>. The decision maker may choose any available action for that state, and the process will respond at the next time step by randomly moving into a new state and giving the decision maker a reward. The probability that the process moves into its new state is determined by the chosen action. So, the next state depends on the current state and the decision maker's action. Given the state and the action, the next move is completely independent of all previous states and actions.</p>
<p>The Tower of Hanoi consists of three rods and several sequentially sized disks in the leftmost rod. The objective is to move all the disks from the leftmost rod to the rightmost one <strong>with the fewest possible number of moves</strong>.</p>
<p>Two important rules you have to follow are that you can move <span>only </span>one disk at a time, and you can't put a bigger disk on top of a smaller one; that is, in any rod, the order of disks must always be from the largest disk at the bottom to the smallest disk at the top, depicted as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4c35f9c3-1da9-4304-bec3-adb147155875.png" style="width:25.67em;height:9.42em;"/> </div>
<p>Let's say we are using three disks, as pictured just now. In this scenario, there are 3<sup>3</sup> possible states, shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4c79d9df-58e4-4e23-b5f8-3f99cce61828.png" style=""/></div>
<p>The total number of all possible states in a Tower of Hanoi puzzle is 3 raised to the number of disks.</p>
<div class="CDPAlignCenter CDPAlign"><em>||S|| = 3<sup>n</sup></em></div>
<p>Where ||<em>S</em>|| is the number of elements in the set states, and <em>n</em> is the number of disks.</p>
<p>So, in our example, we have <em>3 x 3 x 3 = 27</em> unique possible states of the distribution of disks over the three rods, including empty rods; but two empty rods can be in a state at max.</p>
<p>With the total number of states being defined, here are all the possible actions our algorithm has available to move from one state to another:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dd82cb57-d7fd-4e8f-8f1f-2214abf7135f.png" style=""/></div>
<p>The least possible number of moves for this puzzle is:<strong><br/></strong> <em>LeastPossibleMoves = 2<sup>n</sup> - 1</em></p>
<p>Where <em>n</em> is the number of disks.</p>
<p>The Q-learning algorithm can be formally defined as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/96bd1e3a-69a6-466b-b761-7b13295edd00.png" style="width:19.58em;height:1.50em;"/></div>
<p>In this Q-learning algorithm, we have the following variables being used:</p>
<ul>
<li><strong>Q matrix</strong>: A 2D array that, at first, is populated with a fixed value for all elements (usually 0). It is used to hold the calculated policy over all states; that is, for every state, it holds the rewards for the respective possible actions.</li>
<li><strong>R matrix:</strong> A 2D array that holds the initial rewards and allows the program to determine the list of possible actions for a specific state.</li>
<li><strong>Discount factor:</strong> Determines the policy of the agent in how it deals with rewards. A discount factor closer to 0 will make the agent greedy by only considering current rewards, while a discount factor approaching 1 will make it more strategic and farsighted for better rewards in the long run.</li>
</ul>
<p>We should briefly highlight some of the methods of our Q-learning class:</p>
<ul>
<li><kbd>Init</kbd>: Called for generation of all possible states as well as for the start of the learning process.</li>
<li><kbd>Learn</kbd>: Has sequential steps for the learning process.</li>
<li><kbd>InitRMatrix</kbd>: This initializes the reward matrix with one of these values:
<ul>
<li><kbd>0</kbd>: We do <strong>not</strong> have information about the reward when taking this action in this state</li>
<li><kbd>X</kbd>: There is no way to take this action in this state</li>
<li><kbd>100</kbd>: This is our big reward in the final state, where we want to go</li>
</ul>
</li>
</ul>
<ul>
<li><kbd>TrainQMatrix</kbd>: Contains the actual iterative value update rule of the Q matrix. When completed, we expect to have a trained agent.</li>
<li><kbd>NormalizeQMatrix</kbd>: This normalizes the values of the Q matrix making them percentages.</li>
<li><kbd>Test</kbd>: Provides textual input from the user and displays the optimal shortest path to solve the puzzle.</li>
</ul>
<p>Let's look deeper into our <kbd>TrainQMatrix</kbd> code:</p>
<pre>private void TrainQMatrix(int _StatesMaxCount)<br/>{<br/>  pickedActions = new Dictionary&lt;int, int&gt;();<br/>// list of available actions (will be based on R matrix which<br/>// contains the allowed next actions starting from some state as 0 values in the array<br/>  List&lt;int&gt; <strong>nextActions</strong> = new List&lt;int&gt;();<br/>  int <strong>counter</strong> = 0;<br/>  int <strong>rIndex</strong> = 0;<br/>// _StatesMaxCount is the number of all possible states of a puzzle<br/>// from my experience with this application, 4 times the number<br/>// of all possible moves has enough episodes to train Q matrix<br/>  while (<strong>counter</strong> &lt; 3 * _StatesMaxCount)<br/>  {<br/>    var <strong>init</strong> = Utility.GetRandomNumber(0, _StatesMaxCount);<br/>    do<br/>  {<br/>// get available actions<br/><strong>    nextActions</strong> = GetNextActions(_StatesMaxCount, <strong>init</strong>);<br/>// Choose any action out of the available actions randomly<br/>    if (<strong>nextActions</strong> != null)<br/>    {<br/>      var <strong>nextStep</strong> = Utility.GetRandomNumber(0, <strong>nextActions</strong>.Count);<br/><strong>      nextStep</strong> = <strong>nextActions</strong>[<strong>nextStep</strong>];<br/>// get available actions<br/><strong>      nextActions</strong> = GetNextActions(_StatesMaxCount, <strong>nextStep</strong>);<br/>// set the index of the action to take from this state<br/>      for (int <strong>i</strong> = 0; <strong>i</strong> &lt; 3; <strong>i</strong>++)<br/>      {<br/>        if (R != null &amp;&amp; R[<strong>init</strong>, <strong>i</strong>, 1] == <strong>nextStep</strong>)<br/><strong>        rIndex</strong> = <strong>i</strong>;<br/>      }<br/>// this is the value iteration update rule, discount factor is 0.8<br/>      Q[<strong>init</strong>, <strong>nextStep</strong>] = R[<strong>init</strong>, <strong>rIndex</strong>, 0] + 0.8 * Utility.GetMax(Q, <strong>nextStep</strong>, <strong>nextActions</strong>);<br/>// set the next step as the current step<br/><strong>      init</strong> = <strong>nextStep</strong>;<br/>      }<br/>    }<br/>  while (<strong>init</strong> != FinalStateIndex);<br/><strong>  counter</strong>++;<br/>  }<br/>}</pre>
<p>Running the application with three disks:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f64ce769-eca7-4410-a4e6-43b5ea7fdf3d.png"/></div>
<p>Running the application with four disks:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b105397f-e116-46e2-88ba-cd5b1b64c195.png"/></div>
<p>And here's running with seven disks. The optimal number of moves is 127, so you can see how fast the solution can multiply the possible combinations:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f284c705-27e8-4e11-b992-c8c3ca150248.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about reinforcement learning, various types of learning algorithms that go with it, and how you can apply it to real-world learning problems. In the next chapter, we're going to jump into fuzzy logic and see not only what it means, but also how we can apply it to everyday problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Wikipedia, Creative Commons ShareAlike License</li>
<li>Watkins, C.J.C.H. (1989),<span> </span><em>Learning from Delayed Rewards</em><span> </span>(Ph.D. thesis), Cambridge University</li>
<li><em>Online Q-Learning using Connectionist Systems</em>, Rummery &amp; Niranjan (1994)</li>
<li>Wiering, Marco; Schmidhuber, JÃ¼rgen (1998-10-01), <em>Fast Online Q(Î»)</em>. <em>Machine Learning</em>.<span> </span><strong>33</strong><span> </span>(1): 105-115</li>
<li>Copyright (c) 2009-2017, Accord.NET Authors at: <kbd>authors@accord-framework.net</kbd></li>
<li>Kenan Deen,<span> </span><a href="https://kenandeen.wordpress.com/">https://kenandeen.wordpress.com/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>