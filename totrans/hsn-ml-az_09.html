<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine Learning with Spark</h1>
                </header>
            
            <article>
                
<p>This chapter covers the use of Spark on the Microsoft platform and will also provide a walk-through on how to train ML models using Spark, along with the options available in Azure to perform Spark-based ML training.</p>
<p>We will be covering the following topics:</p>
<ul>
<li>ML with Azure Databricks</li>
<li>Azure HDInsight with Spark</li>
<li>Walkthroughs of some labs so that you can see exciting technologies in action</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine learning with Azure Databricks</h1>
                </header>
            
            <article>
                
<p>By adopting ML, enterprises are looking to improve their business, or even radically transform it by using data as the lifeblood for digital transformation. Databricks empowers companies to develop their data science competency quickly, and turn that into a competitive advantage by providing a fully integrated unified analytics platform in Azure.</p>
<p>Enterprises want to leverage the treasure trove of data they have collected historically. Organizations have begun to collect more data recently. This includes data in a variety of forms, including new customer data in the form of clickstreams, web logs, and sensor data from Internet of Things devices and machines, as well as audio, images, and videos.</p>
<p>Using insights from this data, enterprises across various verticals can improve business outcomes in many different ways that impact our daily lives. These include medical diagnosis, fraud detection, detecting cyber attacks, optimizing manufacturing pipelines, customer engagement, and so forth.</p>
<p>Databricks offers a unified analytics platform that brings data engineers, data scientists, and businesses together to collaborate across the data life cycle, starting from ETL programs, through to building analytic applications for production environments.</p>
<p class="mce-root"/>
<p>Data engineers can use Databricks' ETL capability to create new datasets from various sources, including structured, semi-structured, and unstructured data. Data scientists can choose from a variety of programming languages, such as SQL, R, Python, Scala, and Java, and <strong>Machine Learning</strong> (<strong>ML</strong>) frameworks and libraries including Scikit-learn, Apache Spark ML, TensorFlow, and Keras.</p>
<p>Databricks allows enterprises to explore data, and create and test their models in a collaborative way, using Databricks' notebook and visualization capacities. Time-to-delivery is quick and the process of sending ML pipelines to production is also quick.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What challenges is Databricks trying to solve?</h1>
                </header>
            
            <article>
                
<p>Integrating data is always difficult. However, integration challenges are even more difficult in ML because of the various frameworks and libraries that need to be integrated.</p>
<p>Databricks has a focus enterprise readiness of data science platforms in terms of security and manageability.</p>
<p>How do we get started with Apache Spark and Azure Databricks? <span>The first step is to get the Azure Databricks software set up in Azure.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Apache Spark and Azure Databricks</h1>
                </header>
            
            <article>
                
<p>In this walk-through, we will start to explore Azure Databricks. A key step in the process is to set up an instance of Azure Databricks, and this is covered here:</p>
<ol start="1">
<li class="mce-root"> Log in to the Azure portal (<a href="https://portal.azure.com/">https://portal.azure.com/</a>).</li>
<li> Select <span class="packt_screen">+ Create a resource | Analytics | Azure Databricks</span>.</li>
<li> In the Azure Databricks Service dialog, provide the workspace configuration.</li>
<li> <span class="packt_screen">Workspace name</span>:
<ul>
<li>Enter a name for your Azure Databricks workspace</li>
</ul>
</li>
<li><span class="packt_screen">Subscription</span>:
<ul>
<li> Select your <span class="packt_screen">Azure subscription</span></li>
</ul>
</li>
<li> <span class="packt_screen">Resource group</span>:
<ul>
<li> Create a new resource group (<a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview">https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview</a>) or use an existing one</li>
</ul>
</li>
</ol>
<p> </p>
<ol start="7">
<li> <span class="packt_screen">Location</span>:
<ul>
<li> Select a <span class="packt_screen">geographical region </span>(<a href="https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html">https://docs.azuredatabricks.net/administration-guide/cloud-configurations/regions.html</a>)</li>
</ul>
</li>
<li> <span class="packt_screen">Pricing Tier</span>:
<ul>
<li> Select a <span class="packt_screen">pricing tier </span>(<a href="https://azure.microsoft.com/en-us/pricing/details/databricks/">https://azure.microsoft.com/en-us/pricing/details/databricks/</a>). If you select <span class="packt_screen"><strong>Trial (Premium - 14-Days Free DBUs)</strong></span>, the workspace has access to free Premium Azure Databricks DBUs for 14 days.</li>
</ul>
</li>
<li> Select <strong><span class="packt_screen">Pin to dashboard</span></strong> and then click <span class="packt_screen"><strong>Create</strong></span>. The portal will display <span class="packt_screen">Deployment in progress</span>. After a few minutes, the Azure Databricks service page displays, as can be seen in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1005 image-border" src="Images/0d80d523-e551-4514-a52e-238742d48224.png" style="width:98.75em;height:43.42em;" width="1185" height="521"/></p>
<p>On the left-hand side, you can access fundamental Azure Databricks entities: workspace, clusters, tables, notebooks, jobs, and libraries.</p>
<p>The workspace is the special root folder that stores your Azure Databricks assets, such as notebooks and libraries, and the data that you import:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/bfb2c9d5-24f8-4f71-b84a-2462a473b7cf.png" style="width:27.50em;height:13.17em;" width="418" height="200"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a cluster</h1>
                </header>
            
            <article>
                
<p>A cluster is a collection of Azure Databricks computation resources:</p>
<ol>
<li><span>To create</span> <span>a cluster, click the </span><strong>Clusters</strong><span> button in the sidebar and click </span><strong>Create Clus</strong><strong>ter</strong>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1007 image-border" src="Images/edeef29f-8e46-49b1-91ac-e7b8d0c296eb.png" style="width:27.75em;height:19.33em;" width="414" height="288"/></p>
<ol start="2">
<li> On the <strong>New Cluster</strong> page, specify the cluster name.</li>
<li>Select <span class="packt_screen">4.2 (includes Apache Spark 2.3.1, Scala 11)</span> in the Databricks Runtime Version dropdown.</li>
<li>Click <span class="packt_screen">Create Cluster<span>.</span></span></li>
</ol>



            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Create a Databricks Notebook</h1>
                </header>
            
            <article>
                
<p>A Notebook is a collection of cells that run computations on a Spark cluster. To create a Notebook in the Workspace, follow these steps:</p>
<ol>
<li>In the sidebar, click the <span class="packt_screen">Workspace</span> button.</li>
<li>In the <kbd>Workspace</kbd> folder, select <span class="packt_screen">Create Notebook</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/bf8c4a5f-e3e5-48c3-9331-55a2e727e2b4.png" width="482" height="66"/></p>
<ol start="3">
<li> On the <span class="packt_screen">Create Notebook</span> dialog, enter a name and select <span class="packt_screen">SQL</span> in the <strong>Language</strong> dropdown.</li>
<li> Click <span class="packt_screen">Create</span>. The Notebook opens with an empty cell at the top.</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using SQL in Azure Databricks</h1>
                </header>
            
            <article>
                
<p>In this section, you can run a SQL statement to create a <span class="packt_screen">table</span> and work with data using SQL Statements:</p>
<ol>
<li>Copy and paste this code snippet into the notebook cell to see a list of the Azure Databricks datasets:</li>
</ol>
<pre style="padding-left: 60px">display(dbutils.fs.ls("/databricks-datasets"))</pre>
<ol start="2">
<li> The code appears as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>DROP</strong><strong>TABLE</strong><strong>IF</strong><strong>EXISTS</strong> diamonds;<strong>CREATE</strong><strong>TABLE</strong> diamonds<strong>USING</strong> csv<strong>OPTIONS</strong> (path "/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv", header "true")</pre>

<ol start="3">
<li>Press <em>Shift</em> + <em>Enter</em>. The notebook automatically attaches to the cluster you created in <em>Step 2</em>, creates the table, loads the data, and returns <kbd>OK</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e484bd3c-d107-42d8-aeaa-246d91fc84a9.png" style="width:45.83em;height:8.42em;" width="816" height="149"/></p>
<ol start="4">
<li>Next, you can run a SQL statement ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Displaying data</h1>
                </header>
            
            <article>
                
<p>Display a chart of the average diamond price by color:</p>
<ol>
<li>Click the <span class="packt_screen">Bar chart</span> icon</li>
<li>Click <span class="packt_screen">Plot</span> options</li>
<li>Drag <span class="packt_screen">color</span> into the <span class="packt_screen">Keys</span> box</li>
<li>Drag <span class="packt_screen">price</span> into the <span class="packt_screen">Values</span> box</li>
<li>In the <span class="packt_screen">Aggregation</span> dropdown, select <span class="packt_screen">AVG</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/497d3a75-697e-4db4-805a-e4bb75a1aad4.png" style="width:25.75em;height:31.50em;" width="371" height="454"/></p>
<ol start="6">
<li> Click <span class="packt_screen">Apply</span> to display the bar chart:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b2c3fccf-5845-49a3-98fb-cb1a87f91fdd.png" style="width:48.83em;height:19.08em;" width="698" height="273"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine Learning with HDInsight</h1>
                </header>
            
            <article>
                
<p>Apache Spark is the largest open source process in data processing. Since its release, Apache Spark has seen rapid adoption by enterprises across a wide range of industries. Apache Spark is a fast, in-memory data processing engine, with elegant and expressive development APIs to allow data workers to efficiently execute streaming. In addition, Apache Spark facilitates ML and SQL workloads that require fast iterative access to datasets.</p>
<p>The focus of the current chapter is Apache Spark, which is an open source system for fast, large-scale data processing and ML.</p>
<p>The Data Science virtual machine provides you with a standalone (single node in-process) instance of the Apache Spark platform.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is Spark?</h1>
                </header>
            
            <article>
                
<p>Spark is designed as a high-performance, general-purpose computation engine for fast, large-scale big data processes. Spark works by distributing its workload across different nodes in a cluster. Spark scales out to process large volumes of data. Spark is aimed at big data batch processing, and is excellent for producing analytics using low-latency, high-performance data as a basis for operations.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Apache Spark consists of Spark Core and a set of libraries. Core is the distributed execution engine and the Java, Scala, and Python APIs offer a platform for distributed ETL application development. This allows developers to quickly achieve success by writing applications in Java, Scala, or Python.</p>
<p>Spark is built on the concept of a <strong>Resilient Distributed Dataset</strong> (<strong>RDD</strong>), which has been a core Spark concept for working with data since the inception of Spark. RDDs are similar to dataframes in R. RDDs are high-level abstractions of the data that provide data scientists with a schema to retrieve and work with data. RDDs are immutable collections representing datasets and have the inbuilt capability of reliability and failure recovery. RDDs create new RDDs upon any operation, such as transformation or action. They also store the lineage, which is used to recover from failures. For example, it's possible to separate the data out into the appropriate field and columns in that dataset, which means that data scientists can work with them more intuitively.</p>
<p>The Spark process involves a number of steps, which can involve more than one RDD. So, it is possible to have more than one RDD during processing. Here is an example, which shows how RDDs can be the source and the output of different processing steps:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1010 image-border" src="Images/5adcc927-b958-45d5-acb2-b8356d7b5f0b.png" style="width:53.33em;height:10.17em;" width="930" height="177"/></p>
<p>Apache Spark allows for complex data engineering, and it comes with a built-in set of over 80 high-level operators. As well as longer processing, it is possible to interactively query data within the shell. In addition to the Map and Reduce operations, it supports SQL queries, Streaming Data, ML, and Graph Data Processing.</p>
<p>Developers can use these capabilities standalone, or combine them to run in a single data pipeline use case.</p>
<p>Spark powers a stack of libraries, including SQL and DataFrames (<a href="https://spark.apache.org/sql/">https://spark.apache.org/sql/</a>), MLlib (<a href="https://spark.apache.org/mllib/">https://spark.apache.org/mllib/</a>) for ML, GraphX (<a href="https://spark.apache.org/graphx/">https://spark.apache.org/graphx/</a>), and Spark Streaming (<a href="https://spark.apache.org/streaming/">https://spark.apache.org/streaming/</a>). You can combine these libraries seamlessly in the same application.</p>
<p class="mce-root"/>
<p>In this chapter, RDDs will be the focus of the ML exercises. We will be focusing on hands-on exercises using Jupyter Notebooks. Jupyter Notebooks are available on the Data Science Virtual Machine, and they are installed by default as a service with the Azure HDInsight deployment of Spark.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">HDInsight and Spark</h1>
                </header>
            
            <article>
                
<p>Apache Spark is an open source parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications. The Apache Spark cluster on HDInsight is compatible with Azure Storage (WASB), as well as Azure Data Lake Store.</p>
<p>When the developer creates a Spark cluster on HDInsight, the Azure compute resources are already created with Spark installed and configured. It only takes about 10 minutes to create a Spark cluster in HDInsight. The data to be processed is stored in Azure Storage or Azure Data Lake Storage.</p>
<p>Apache Spark provides primitives for in-memory cluster computing, which means that it is the perfect partner for HDInsight. An Apache Spark job can load and cache data ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The YARN operation system in Apache Spark</h1>
                </header>
            
            <article>
                
<p>YARN is one of the key features in the second-generation Hadoop 2 version of the Apache Software Foundation's open source distributed processing framework, and it is retained and progressed in Hadoop Version 3. YARN is implemented on Azure HDInsight to facilitate large-scale, distributed operating systems for big data applications and predictive analytics.</p>
<p class="mce-root"/>
<p>YARN is efficient because it decouples MapReduce's resource management and scheduling capabilities from the data processing component. Since Apache Spark uses this methodology, it empowers Hadoop to support more varied processing approaches and a broader array of applications.</p>
<p>How can we use Spark to conduct predictive analytics? ML focuses on taking data and applying a process to that data to produce a predicted output. There are many different types of ML algorithms that we can create and use with Spark. One of the most common methods is supervised ML, which works by taking in some data that consists of a vector of what we call features and a label. <span>What do we mean by this?</span></p>
<p>A vector is a set of information that we use in order to make a prediction. A label is a characteristic that is used to make the prediction.</p>
<p>We will take an example. Let's say that we have a set of information about people, and we would like to predict something about this group of people: whether they are likely to become homeless or not. The characteristics of these people might include their age, education level, earnings, military service, and so on. The characteristics of the people would be called the features, and the thing that we would like to predict is known as the <strong>label</strong>.</p>
<p>In this case, the data scientist would take some data where they know that they have already become homeless, and therefore the label value would be known to the researchers at that point.</p>
<p>Using Spark, we would then process the data and fit the data to the model to see how successful it is. The model would tell us what we need to see in the characteristics in order to see the likelihood of homelessness occurring for these individuals.</p>
<p>The model is essentially a function that specifies what we expect to see in the vector features to see the outcome, or prediction.</p>
<p>The next step is to take previously unseen, new data that doesn't contain a known label to see how it fits the model. This dataset just has the features because the actual label, or outcome, isn't known. In this supervised learning example, data with a known label is used to train the model to predict data with the known label, and then the model is faced with data that does not have the known label.</p>
<p class="mce-root"/>
<p>In unsupervised learning, the label is not known. Unsupervised learning takes a similar approach, where the data scientist will ingest data and feed it, which simply has the vector of features, and no label is present. With this type of data science methodology, I may simply be looking at the similarities that are found in the vector features in order to see whether there are any clusters or commonalities in the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with data in a Spark environment</h1>
                </header>
            
            <article>
                
<p>When data scientists work with data in an Apache Spark environment, they typically work with either RDDs or DataFrames. In our examples so far, the data may be stored in the RDD format, and it is fed into the model by building a predictive feed into the model.</p>
<p>In these exercises, the Spark library is called <kbd>spark.mllib</kbd>. The MLlib library is the original ML library that comes with Spark. The newer library is called <strong>Spark ML</strong>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Jupyter Notebooks</h1>
                </header>
            
            <article>
                
<p>The Jupyter Notebook is an incredibly powerful tool for collaboratively developing and producing data science projects. It integrates code, comments, and code output into a single document that combines code, data visualizations, narrative text, mathematical equations, and other data science artefacts. Notebooks are increasingly popular in today's data science workflows because they encourage iterative and rapid development for data science teams. Jupyter project is the successor to the earlier IPython Notebook. It is possible to use many different programming languages within Jupyter Notebooks, but this chapter will focus on Apache Spark.</p>
<p>Jupyter is free, open source, and browser-based. It can be used to create notebooks for working with your code in normal ways, such as writing and commenting code. One key feature of Jupyter Notebooks is that they are very useful for collaboration with other team members, thereby <span><span>enabling</span></span> productivity. The Jupyter Notebook supports a number of different engines, also known as kernels. The Jupyter Notebook can be used to run code on Python or Scala.</p>
<p>In this walk-through, the Spark ML tutorial will be used in order to introduce the concepts of Spark and ML.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Configuring the data science virtual machine</h1>
                </header>
            
            <article>
                
<p>If you are using the Ubuntu Linux DSVM edition, there is a requirement to do a one-time setup step to enable a local single node Hadoop HDFS and YARN instance. By default, Hadoop services are installed but disabled on the DSVM. In order to enable it, it is necessary to run the following commands as root the first time:</p>
<pre>echo -e 'y\n' | ssh-keygen -t rsa -P '' -f ~hadoop/.ssh/id_rsacat ~hadoop/.ssh/id_rsa.pub &gt;&gt; ~hadoop/.ssh/authorized_keyschmod 0600 ~hadoop/.ssh/authorized_keyschown hadoop:hadoop ~hadoop/.ssh/id_rsachown hadoop:hadoop ~hadoop/.ssh/id_rsa.pubchown hadoop:hadoop ~hadoop/.ssh/authorized_keyssystemctl start hadoop-namenode hadoop-datanode hadoop-yarn</pre>
<p>You can stop the Hadoop-related ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running Spark MLib commands in Jupyter</h1>
                </header>
            
            <article>
                
<p>The default Jupyter lab will demonstrate features and capabilities of Spark's MLlib toolkit for ML problems. The walk-through uses a sample dataset, which holds data from real trips of NYC taxis. The data holds the NYC taxi trip and fare dataset to show MLlib's modeling features for binary classification and regression problems.</p>
<p>In this lab, many different Spark MLib functions will be used, including data ingestion, data exploration, data preparation (featurizing and transformation), modeling, prediction, model persistence, and model evaluation on an independent validation dataset. Data visualization will also be used in order to demonstrate the results.</p>
<p>The lab will focus on two types of learning: classification offers the opportunity to try out supervised and unsupervised Learning. The first sample will use binary classification to predict whether a tip will be given. In the second sample, regression will be used to predict the level of tip given.</p>
<p class="mce-root"/>
<p>In Jupyter, the code is executed in the cell. The cell structure is a simple way of enabling the data scientist to query the RDD dataframe, and interactively display information in the Jupyter Notebook, including data visualization. It's a very flexible, intuitive, and powerful way to work with our data to use ML using Spark.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data ingestion</h1>
                </header>
            
            <article>
                
<p>The first activity is to set the appropriate directory paths.</p>
<ol>
<li>Set the location of training data:</li>
</ol>
<pre style="padding-left: 60px">taxi_train_file_loc = "../Data/JoinedTaxiTripFare.Point1Pct.Train.csv"taxi_valid_file_loc = "../Data/JoinedTaxiTripFare.Point1Pct.Valid.csv"</pre>
<ol start="2">
<li>Set the model storage directory path. This is where models will be saved:</li>
</ol>
<pre style="padding-left: 60px">modelDir = "../Outputs/"; # The last backslash is needed;</pre>
<ol start="3">
<li>In the Jupyter menu, put the cursor in <span class="packt_screen">cell</span> and select the <span class="packt_screen">Run</span> <span>option </span>from the menu. This will assign the training and test sets to the <kbd>taxi_train_file_loc</kbd> and <kbd>taxi_valid_file_loc </kbd><span>variables.</span></li>
<li>Next, the data will be set into a new dataframe, and it will be cleaned. Data ingestion was completed using the <kbd>spark.read.csv</kbd> <span>function,</span> which assigns the data to a new ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data exploration</h1>
                </header>
            
            <article>
                
<p>In the next step, it's important to explore the data. It's easy to visualize the data right in the Jupyter interface by plotting the target variables and features. The data is summarized using SQL. Then, the data is plotted using <kbd>matplotlib</kbd>. To plot the data, the dataframe will first have to be converted to a pandas dataframe. At that point, matplotlib can use it to generate plots.</p>
<p>Since Spark is designed to work with large big data datasets, if the Spark dataframe is large, a sample of the data can be used for data visualization purposes.</p>
<p>In the following example, 50% of the data was sampled prior to converting the data into the dataframe format, and then it was incorporated into a pandas dataframe.</p>
<p>The code is provided in the following snippet:</p>
<pre class="CDPAlignLeft CDPAlign"><span>%%sql -q -o sqlResults</span><br/>SELECT fare_amount, passenger_count, tip_amount, tipped FROM taxi_train<br/><br/>sqlResultsPD = spark.sql(sqlStatement).sample(False, 0.5, seed=1234).toPandas();<br/>%matplotlib inline<br/><br/># This query will show the tip by payment<br/><br/>ax1 = sqlResultsPD[['tip_amount']].plot(kind='hist', bins=25, facecolor='lightblue')<br/>ax1.set_title('Tip amount distribution')<br/>ax1.set_xlabel('Tip Amount ($)'); ax1.set_ylabel('Counts');<br/>plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()<br/><br/># TIP AMOUNT BY FARE AMOUNT, POINTS ARE SCALED BY PASSENGER COUNT<br/><br/>ax = sqlResultsPD.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(sqlResultsPD.passenger_count))<br/>ax.set_title('Tip amount by Fare amount')<br/>ax.set_xlabel('Fare Amount ($)'); ax.set_ylabel('Tip Amount ($)');<br/>plt.axis([-2, 80, -2, 20])<br/>plt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()</pre>
<p>This will produce a histogram of the results.</p>
<p>The relationship between the fare amount and the tip amount is shown in the following chart:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1012 image-border" src="Images/74038367-c36c-42f6-b2e0-2a0e7bc1f714.png" style="width:27.83em;height:20.58em;" width="511" height="378"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature engineering in Spark</h1>
                </header>
            
            <article>
                
<p>In feature engineering, we can take care of numerous data engineering tasks, such as creating new features and grouping, transforming, and cleaning up data. The data can undergo further indexing and be enriched by additional classification, grouping, and the encoding of categorical features.</p>
<p>In the following example code, we create a new feature by binning hours into traffic time buckets using Spark SQL:</p>
<pre>sqlStatement = """ SELECT *, CASEWHEN (pickup_hour &lt;= 6 OR pickup_hour &gt;= 20) THEN "Night"WHEN (pickup_hour &gt;= 7 AND pickup_hour &lt;= 10) THEN "AMRush"WHEN (pickup_hour &gt;= 11 AND pickup_hour &lt;= 15) THEN "Afternoon"WHEN (pickup_hour &gt;= 16 AND pickup_hour &lt;= 19) THEN "PMRush"END as TrafficTimeBinsFROM taxi_train</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Spark for prediction</h1>
                </header>
            
            <article>
                
<p>In this part of the chapter, the exercise is to use the Spark sample code to create a logistic regression model, save the model, and evaluate the performance of  the model on a test dataset.  For modeling, the features and class labels are specified using the <kbd>RFormula</kbd> function. <span>In this example, we will train the model using the pipeline formula and a logistic regression estimator. </span>This can be seen from the following code snippet:</p>
<pre>logReg = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The following code block sets up the training formula and assigns it to the <kbd>classFormula</kbd> <span>variable</span>, which can be seen from the following code:</p>
<pre>classFormula = RFormula(formula="tipped ~ pickup_hour + weekday + passenger_count + trip_time_in_secs + trip_distance + fare_amount + vendorVec + rateVec + paymentVec + TrafficTimeBinsVec")</pre>
<p>The following code block trains the pipeline model:</p>
<pre>model = Pipeline(stages=[classFormula, logReg]).fit(trainData)</pre>
<p>The following code block saves the model that we have created:</p>
<pre>datestamp = str(datetime.datetime.now()).replace(' ','').replace(':','_');<br/>fileName = "logisticRegModel_" + datestamp;<br/>logRegDirfilename = modelDir + fileName;<br/>model.save(logRegDirfilename)</pre>
<p>The following code block uses the model to predict results, using test data. The following code block helps to evaluate the model:</p>
<pre>predictions = model.transform(testData)<br/>predictionAndLabels = predictions.select("label","prediction").rdd<br/>metrics = BinaryClassificationMetrics(predictionAndLabels)<br/>print("Area under ROC = %s" % metrics.areaUnderROC)</pre>
<p>Next, we plot the ROC curve:</p>
<pre>%matplotlib inline<br/>predictions_pddf = predictions.toPandas()<br/>labels = predictions_pddf["label"]<br/>prob = []<br/>for dv in predictions_pddf["probability"]:<br/>prob.append(dv.values[1])<br/>fpr, tpr, thresholds = roc_curve(labels, prob, pos_label=1);<br/>roc_auc = auc(fpr, tpr)<br/>plt.figure(figsize=(5,5))<br/>plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)<br/>plt.plot([0, 1], [0, 1], 'k--')<br/>plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05]);<br/>plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate');<br/>plt.title('ROC Curve'); plt.legend(loc="lower right");<br/>plt.show()</pre>
<p class="mce-root"/>
<p>The regression output is shown next:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a4bf5967-5356-47d0-ba1f-5abf0099000f.png" style="width:24.58em;height:23.17em;" width="437" height="411"/></p>
<p>The ROC Curve is the blue curve, which sits very high at the top-left corner. The ROC result shows that the model is performing extremely well. As a heuristic, the closer the blue line is to the top of the upper-left side of the chart, the better the result.</p>
<p>In this section, the random forest model regression method will be used to predict how much of a tip will be given. In a standard classification tree, the data is split based on the homogeneity of the data. A decision tree is built top-down from a root node. The process involves partitioning data into subsets that contain instances that are homogeneous.</p>
<p>In a regression tree, the target variable is a real-value number. In this case, the data is fitted against a regression model to the target variable using each of the independent variables. For each independent variable, the data is split at several split points. We calculate the <strong>Sum of Squared Error </strong>(<strong>SSE</strong>) at each split boundary between the predicted value and the actual values. The variable resulting in the lowest SSE is selected for the node. Then, this process is recursively continued until all of the data is covered.</p>
<p>The code is as follows:</p>
<pre><span>## DEFINE REGRESSION FORMULA</span><br/>regFormula = RFormula(formula="tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + weekday + passenger_count + trip_time_in_secs + trip_distance + fare_amount")</pre>
<p class="mce-root"/>
<p>Then, we define the indexer for the categorical variables:</p>
<pre><span>## DEFINE INDEXER FOR CATEGORIAL VARIABLES</span><br/>featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=32)</pre>
<p>Then, we set up the random forest estimator. The value is set to the <kbd>randForest</kbd><span>variable:</span></p>
<pre><span>## DEFINE RANDOM FOREST ESTIMATOR</span><br/>randForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', numTrees=20,<br/>featureSubsetStrategy="auto",impurity='variance', maxDepth=6, maxBins=100)</pre>
<p>The next step is to fit the model using the defined formula and the relevant transformations:</p>
<pre><span>## Fit model, with formula and other transformations</span><br/>model = Pipeline(stages=[regFormula, featureIndexer, randForest]).fit(trainData)</pre>
<p>The next crucial step is to save the model:</p>
<pre><span>## SAVE MODEL</span><br/>datestamp = str(datetime.datetime.now()).replace(' ','').replace(':','_');<br/>fileName = "RandomForestRegressionModel_" + datestamp;<br/>andForestDirfilename = modelDir + fileName;<br/>model.save(randForestDirfilename)</pre>
<p>Then, we need to use the model to predict against the test data so that we can evaluate its success:</p>
<pre>predictions = model.transform(testData)<br/>predictionAndLabels = predictions.select("label","prediction").rdd<br/>testMetrics = RegressionMetrics(predictionAndLabels)<br/>print("RMSE = %s" % testMetrics.rootMeanSquaredError)<br/>print("R-sqr = %s" % testMetrics.r2)</pre>
<p>There's no substitute for visualizing the data. In the next code block, the data is visualized using a scattergram format. The end result is shown after the code block:</p>
<pre><span>## PLOC ACTUALS VS. PREDICTIONS</span><br/>predictionsPD = predictions.select("label","prediction").toPandas()ax = predictionsPD.plot(kind='scatter', figsize = (5,5), x='label', y='prediction', color='blue', alpha = 0.15, label='Actual vs. predicted');<br/>fit = np.polyfit(predictionsPD['label'], predictionsPD['prediction'], deg=1)<br/>ax.set_title('Actual vs. Predicted Tip Amounts ($)')<br/>ax.set_xlabel("Actual"); ax.set_ylabel("Predicted");<br/>ax.plot(predictionsPD['label'], fit[0] * predictionsPD['label'] + fit[1], color='magenta')<br/>plt.axis([-1, 15, -1, 15])<br/>plt.show(ax)</pre>
<p>The resulting chart can be found next:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/de0a0f54-8e02-4f8b-9bb1-0f00cf143aab.png" style="width:25.58em;height:27.83em;" width="435" height="474"/></p>
<p>The lower the <strong>Root Mean Square Error</strong> (<strong>RMSE</strong>) value, the better the absolute fit. The RMSE is calculated as the square root of the variance of the residuals. It specifies the absolute fit of the model to the data. In other words, it denotes how close the observed actual data points are to the model's predicted values. As the square root of a variance, RMSE can be conceived as the standard deviation of the unexplained variance. The RMSE has the useful property of being in the same units as the response variable, so it intuitively makes sense. Lower RMSE values indicate a better fit. RMSE is a good measure of how accurately the model predicts the response. RMSE is the most important criterion for fit in this case, since the main purpose of the model is prediction.</p>
<p class="mce-root"/>
<p><strong>R-sqr</strong> is intuitive. Its value ranges from zero to one, with zero indicating that the proposed model does not improve prediction over the mean model, and one indicating perfect prediction. Improvement in the regression model results in proportional increases in R-sqr.</p>
<p>Whereas R-sqr is a relative measure of fit, RMSE is an absolute measure of fit, and that's why it's shown here.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading a pipeline model and evaluating the test data</h1>
                </header>
            
            <article>
                
<p>In this example, we will load a pipeline model and then evaluate the test data:</p>
<pre><span>savedModel = PipelineModel.load(logRegDirfilename)</span>predictions = savedModel.transform(testData)predictionAndLabels = predictions.select("label","prediction").rddmetrics = BinaryClassificationMetrics(predictionAndLabels)print("Area under ROC = %s" % metrics.areaUnderROC)</pre>
<p>In the next step, we define random forest models:</p>
<pre>randForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label',featureSubsetStrategy="auto",impurity='variance', maxBins=100)</pre>
<p>Now, we will define a modeling pipeline that includes formulas, feature transformations, and an estimator:</p>
<pre>pipeline = Pipeline(stages=[regFormula, ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting up an HDInsight cluster with Spark</h1>
                </header>
            
            <article>
                
<p>It's crucial to provision an HDInsight Spark cluster in order to start the work.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Provisioning an HDInsight cluster</h1>
                </header>
            
            <article>
                
<div class="packt_tip">If you already have a Spark HDInsight cluster running, you can skip this procedure.</div>
<ol>
<li>In a web browser, navigate to <a href="http://portal.azure.com">http://portal.azure.com</a> and, if prompted, sign in using the Microsoft account that is associated with your Azure subscription.</li>
<li>In the Microsoft Azure portal, in the Hub Menu, click <span class="packt_screen">New</span>. Then, in the <span class="packt_screen">Data + Analytics</span> section, select <span class="packt_screen">HDInsight</span> and create a <span class="packt_screen">new HDInsight cluster</span> with the following settings:
<ul>
<li><strong>Cluster Name</strong>: Enter a unique name (and make a note of it!)</li>
<li><strong>Subscription</strong>: Select your Azure subscription</li>
<li><strong>Cluster Type</strong>: Spark</li>
<li><strong>Cluster Operating System</strong>: Linux</li>
<li><strong>HDInsight Version</strong>: Choose the latest version of Spark</li>
<li><strong>Cluster Tier</strong>: Standard</li>
<li><strong>Cluster Login Username</strong>: Enter a username of your choice (and ...</li></ul></li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you were introduced to some of the latest big data analytics technologies in Microsoft Azure. The chapter has focused on two main technologies: Azure HDInsight with Spark, and Azure Databricks.</p>
<p>In the chapter, we have looked at the different ways of modelling data, and we have covered useful tips to help you to understand what the models actually mean. Often, this is not the end of the data science process, because this may throw up new questions as you get new insights. So, it is a process rather than a race—but that's what makes it interesting!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further references</h1>
                </header>
            
            <article>
                
<ul>
<li>Review the<span> </span>Spark Machine Learning Programming Guide at<span> </span><a href="https://spark.apache.org/docs/latest/ml-guide.html">https://spark.apache.org/docs/latest/ml-guide.html</a></li>
<li> Documentation for<span> </span>Microsoft Azure HDInsight, including Spark Clusters is at<span> </span><a href="https://azure.microsoft.com/en-us/documentation/services/hdinsight">https://azure.microsoft.com/en-us/documentation/services/hdinsight</a></li>
<li> Documentation and getting started guidance for<span> </span>Programming with Scala is at<span> </span><a href="http://www.scala-lang.org/documentation/">http://www.scala-lang.org/documentation/</a></li>
<li> Documentation and getting started guidance for<span> </span>Programming with Python is at<span> </span><a href="https://www.python.org/doc/">https://www.python.org/doc/</a></li>
<li> You can view the<span> </span>Spark SQL and DataFrames Programming Guide at<span> </span><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">https://spark.apache.org/docs/latest/sql-programming-guide.html</a></li>
<li> Classification and Regression: <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html">https://spark.apache.org/docs/latest/ml-classification-regression.html</a></li>
<li> Pipelines: <a href="https://spark.apache.org/docs/latest/ml-pipeline.html">https://spark.apache.org/docs/latest/ml-pipeline.html ...</a></li></ul></article></section></div>



  </body></html>