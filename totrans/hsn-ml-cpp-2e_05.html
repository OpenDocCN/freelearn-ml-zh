<html><head></head><body>
		<div id="_idContainer231">
			<h1 class="chapter-number" id="_idParaDest-113"><a id="_idTextAnchor258"/>5</h1>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor259"/>Anomaly Detection</h1>
			<p><strong class="bold">Anomaly detection</strong> is <a id="_idIndexMarker550"/>where we search for unexpected values in a given dataset. An anomaly is a deviation in system behavior or data value from the standard or expected value. Anomalies are also known as outliers, errors, deviations, and exceptions. They can occur in data that’s of a diverse nature and structure as a result of technical failures, accidents, deliberate hacks, <span class="No-Break">and more.</span></p>
			<p>There are many methods and algorithms we can use to search for anomalies in various types of data. These methods use different approaches to solve the same problem. There are unsupervised, supervised, and semi-supervised algorithms. However, in practice, unsupervised methods are the most popular. The <strong class="bold">unsupervised anomaly detection</strong> technique <a id="_idIndexMarker551"/>detects anomalies in unlabeled test datasets, under the assumption that most of the dataset is normal. It does this by searching for data points that are unlikely to fit the rest of the dataset. Unsupervised algorithms are more popular because of the nature of anomaly events, which are significantly rare compared to normal or expected data, so it is usually very difficult to get a suitably labeled dataset for <span class="No-Break">anomaly detection.</span></p>
			<p>Broadly speaking, anomaly detection applies to a wide range of areas, such as intrusion detection, fraud detection, fault detection, health monitoring, event detection (in sensor networks), and the detection of environmental disruptions. Often, anomaly detection is used as the preprocessing step for data preparation, before the data is passed on to <span class="No-Break">other algorithms.</span></p>
			<p>So, in this chapter, we’ll discuss the most popular unsupervised algorithms for anomaly detection and <span class="No-Break">its applications.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Exploring the applications of <span class="No-Break">anomaly detection</span></li>
				<li>Learning approaches for <span class="No-Break">anomaly detection</span></li>
				<li>Examples of using different C++ libraries for <span class="No-Break">anomaly detectio<a id="_idTextAnchor260"/><a id="_idTextAnchor261"/>n</span></li>
			</ul>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor262"/>Technical requirements</h1>
			<p>The list of software that you’ll need to complete the examples in this chapter is <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">Shogun-toolbox</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">Shark-ML</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">PlotCpp</strong></span><span class="No-Break"> library</span></li>
				<li>Modern C++ compiler with <span class="No-Break">C++17 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.8</span></li>
			</ul>
			<p>The code files for this chapter can be found at the following GitHub <span class="No-Break">repo: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter05</span></a></p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor263"/>Exploring the applications of anomaly detection</h1>
			<p>Two areas in data analysis look for anomalies: <strong class="bold">outlier detection</strong> and <span class="No-Break"><strong class="bold">novelty detection</strong></span><span class="No-Break">.</span></p>
			<p>A <em class="italic">new object</em> or <em class="italic">novelty</em> is an<a id="_idIndexMarker552"/> object that differs in its properties from objects in the training <a id="_idIndexMarker553"/>dataset. Unlike an outlier, the new object is not in the dataset itself, but it can appear at any point after a system has started working. Its task is to detect when it appears. For example, if we were to analyze existing temperature measurements and identify abnormally high or low values, then we would be detecting outliers. On the other hand, if we were to create an algorithm that, for every new measurement, evaluates the temperature’s similarity to past values and identifies significantly unusual ones, then we would be <span class="No-Break">detecting novelties.</span></p>
			<p>The reasons for outliers <a id="_idIndexMarker554"/>appearing include data errors, the presence of noise, misclassified objects, and foreign objects from other datasets or distributions. Let’s explain two of the most obscure types of outliers: data errors and data from different distributions. Data errors can broadly refer to inaccuracies in measurements, rounding errors, and incorrect entries. An example of an object belonging to a different distribution is measurements that have come from a broken sensor. This is because these values will belong to a range that may be different from what <span class="No-Break">was expected.</span></p>
			<p>Novelties<a id="_idIndexMarker555"/> usually appear as a result of fundamentally new object behavior. For example, if our <a id="_idIndexMarker556"/>objects are computer system behavior descriptions, then after a virus has penetrated the computer and deleted some information from these descriptions, they will be rendered as novelties. Another example of a novelty could be a new group of customers that behave differently from others but have some similarities to other customers. The main feature of novelty objects is that they are new, in that it’s impossible to have information about all possible virus infections or breakdowns in the training set. Creating such a training dataset is a complicated process and often does not make sense. However, fortunately, we can obtain a large enough dataset by focusing on the ordinary (regular) operations of the system <span class="No-Break">or mechanism.</span></p>
			<p>Often, the task of anomaly detection is similar to the<a id="_idIndexMarker557"/> task of <strong class="bold">classification</strong>, but there is an essential difference: <strong class="bold">class imbalances</strong>. For example, equipment failures (anomalies) are significantly rarer than having the equipment <span class="No-Break">functioning normally.</span></p>
			<p>We can observe anomalies in different kinds of data. In the following graph, we can see an example of anomalies in a <span class="No-Break">numeric series:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer168">
					<img alt="Figure 5.1 – Example of anomalies in a numeric series" src="image/B19849_05_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Example of anomalies in a numeric series</p>
			<p>In the <a id="_idIndexMarker558"/>following diagram, we can see anomalies in graphs; these anomalies can be as edges as well as vertices (see elements marked with a <span class="No-Break">lighter color):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer169">
					<img alt="Figure 5.2 – Anomalies in graphs" src="image/B19849_05_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Anomalies in graphs</p>
			<p>The following text shows anomalies in a sequence <span class="No-Break">of characters:</span></p>
			<p><span class="No-Break"><strong class="source-inline">AABBCCCAABBCCCAACABBBCCCAABB</strong></span></p>
			<p>The quality or <a id="_idIndexMarker559"/>performance of anomaly detection tasks can be estimated, just like classification<a id="_idIndexMarker560"/> tasks can, by using, for example, <strong class="bold">Area Under the Receiver Operating Characteristic </strong><span class="No-Break"><strong class="bold">Curve</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AUC-ROC</strong></span><span class="No-Break">).</span></p>
			<p>We have discussed what anomalies are, so let’s see what approaches there are to <span class="No-Break">det<a id="_idTextAnchor264"/>e<a id="_idTextAnchor265"/>ct them.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor266"/>Learning approaches for anomaly detection</h1>
			<p>In this section, we’ll look at the most popular and straightforward methods we can use for <span class="No-Break">anomaly d<a id="_idTextAnchor267"/>e<a id="_idTextAnchor268"/>tection.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor269"/>Detecting anomalies with statistical tests</h2>
			<p><strong class="bold">Statistical tests</strong> are<a id="_idIndexMarker561"/> usually used to catch <a id="_idIndexMarker562"/>extreme values for individual features. The general name for <a id="_idIndexMarker563"/>this type of test is <strong class="bold">extreme-value analysis</strong>. An<a id="_idIndexMarker564"/> example of such a test is the use of the <span class="No-Break">Z-score measure:</span></p>
			<p><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;δ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/1.png" style="vertical-align:-0.513em;height:1.608em;width:4.081em"/></p>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_022.png"/> is a sample from the dataset, <em class="italic">µ</em> is the mean of all samples from the dataset, and <img alt="" role="presentation" src="image/B19849_Formula_032.png"/> is the standard deviation of samples in the dataset. A <em class="italic">Z</em>-score value tells us how many standard deviations a data point is distant from the mean. So, by choosing the appropriate threshold value, we can filter some values as anomalies. Any data points with a <em class="italic">Z</em>-score greater than the threshold will be considered anomalies or unusual values in the dataset. Typically, values above <strong class="source-inline">3</strong> or below <strong class="source-inline">-3</strong> are considered anomalies, but you can adjust this threshold based on your specific project requirements. The following graph shows which values from some type of normally distributed data can be treated as anomalies or outliers by using the <span class="No-Break">Z-score test:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer173">
					<img alt="Figure 5.3 – Z-score anomaly detection" src="image/B19849_05_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Z-score anomaly detection</p>
			<p>One <a id="_idIndexMarker565"/>important <a id="_idIndexMarker566"/>concept <a id="_idIndexMarker567"/>that we should mention is extreme values—the maximum and minimum values from the given dataset. It is important to understand that extreme values and anomalies are different concepts. The following is a small <span class="No-Break">data sample:</span></p>
			<p><strong class="source-inline">[1, 39, 2, 1, 101, 2, 1, 100, 1, 3, 101, 1, 3, 100, 101, </strong><span class="No-Break"><strong class="source-inline">100, 100]</strong></span></p>
			<p>We can<a id="_idIndexMarker568"/> consider the value <strong class="source-inline">39</strong> as an anomaly, but not because it is a maximal or minimal value. It is crucial to understand that an anomaly needn’t be an <span class="No-Break">extreme value.</span></p>
			<p>Although<a id="_idIndexMarker569"/> extreme values are not anomalies in general, in some cases, we can adapt methods of extreme-value analysis to the needs of anomaly detection. However, this depends on the task at hand and should be carefully analyzed by <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">)<a id="_idTextAnchor270"/> <a id="_idTextAnchor271"/>practitioners.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor272"/>Detecting anomalies with the Local Outlier Factor method</h2>
			<p>The<a id="_idIndexMarker570"/> distance measurement-based methods are widely used for solving different ML problems, as well as for <a id="_idIndexMarker571"/>anomaly detection. These methods assume that there is a specific metric in the object space that helps us find anomalies. The general assumption when we use distance-based methods for anomaly detection is that the anomaly only has a few neighbors, while a normal point has many. Therefore, for example, the distance to the <em class="italic">k</em><span class="superscript">th</span> neighbor can serve as a good measure of anomalies, as reflected in the <strong class="bold">Local Outlier Factor</strong> (<strong class="bold">LOF</strong>) method. This <a id="_idIndexMarker572"/>method is based on estimating the density of objects that have been checked for anomalies. Objects lying in the areas of lowest density are considered anomalies <span class="No-Break">or outliers.</span></p>
			<p>The advantage of the LOF method over other methods is that it works in conjunction with the local density of objects. Therefore, the LOF effectively identifies outliers even when there are objects of different classes in the dataset that may not be considered anomalies during training. For example, let’s assume that there is a distance, <em class="italic">k</em>-distance (<em class="italic">A</em>), from the object <em class="italic">(A)</em> to the <em class="italic">k</em><span class="superscript">th</span> nearest neighbor. Note that the set of <em class="italic">k</em> nearest neighbors includes all objects within this distance. We denote the set of <em class="italic">k</em> nearest neighbors as <em class="italic">N</em><span class="subscript">k</span><em class="italic">(A)</em>. This distance is used to determine the <span class="No-Break">reachability distance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer174">
					<img alt="" role="presentation" src="image/B19849_Formula_042.jpg"/>
				</div>
			</div>
			<p>If point <em class="italic">A</em> lies among <em class="italic">k</em> neighbors of point <em class="italic">B</em>, then <em class="italic">reachability-distance</em> will be equal to the <em class="italic">k-distance</em> of point <em class="italic">B</em>. Otherwise, it will be equal to the exact distance between points <em class="italic">A</em> and <em class="italic">B</em>, which is given by the <strong class="source-inline">dist</strong> function. The local reachability density of an object <em class="italic">A</em> is defined <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer175">
					<img alt="" role="presentation" src="image/B19849_Formula_052.jpg"/>
				</div>
			</div>
			<p>Local reachability density <a id="_idIndexMarker573"/>is the inverse of the average reachability distance of the object, <em class="italic">A</em>, from its neighbors. Note <a id="_idIndexMarker574"/>that this is not the average reachability distance of neighbors from <em class="italic">A</em> (which, by definition, should have been k-distance(<em class="italic">A</em>)), but is the distance at which <em class="italic">A</em> can be reached from its neighbors. The local reachability densities are then compared with the local reachability densities of <span class="No-Break">the neighbors:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer176">
					<img alt="" role="presentation" src="image/B19849_Formula_062.jpg"/>
				</div>
			</div>
			<p>The <a id="_idIndexMarker575"/>provided formula gives the average local reachability density of the neighbors, divided by the local reachability density of the <span class="No-Break">object itself:</span></p>
			<ul>
				<li>A value of approximately <strong class="source-inline">1</strong> means that the object can be compared with its neighbors (and therefore it is not <span class="No-Break">an outlier)</span></li>
				<li>A value less than <strong class="source-inline">1</strong> indicates a dense area (objects have <span class="No-Break">many neighbors)</span></li>
				<li>A value significantly larger than <strong class="source-inline">1</strong> <span class="No-Break">indicates anomalies</span></li>
			</ul>
			<p>The disadvantage of this method is the fact that the resulting values are difficult to interpret. A value of <strong class="source-inline">1</strong> or less indicates that a point is purely internal, but there is no clear rule by which a point will be an outlier. In one dataset, the value <strong class="source-inline">1.1</strong> may indicate an outlier. However, in another dataset with a different set of parameters (for example, if there is data with sharp local fluctuations), the value <strong class="source-inline">2</strong> may also indicate internal objects. These differences can also occur within a single dataset due to the local<a id="_idTextAnchor273"/>i<a id="_idTextAnchor274"/>ty of <span class="No-Break">the method.</span></p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor275"/>Detecting anomalies with isolation forest</h2>
			<p>The idea of <a id="_idIndexMarker576"/>an isolation forest is based<a id="_idIndexMarker577"/> on<a id="_idIndexMarker578"/> the <strong class="bold">Monte Carlo principle</strong>: a random partitioning of the feature space is carried out so that, on average, isolated points are cut off from normal ones. The final result is averaged over several runs of the stochastic<a id="_idIndexMarker579"/> algorithm, and the result will form an isolation forest of corresponding trees. The isolation tree algorithm then builds a random binary decision tree. The root of the tree is the whole feature space. In the next node, a <a id="_idIndexMarker580"/>random feature and a random partitioning threshold are selected, and they are sampled from a uniform distribution on the range of the minimum and maximum values of the selected feature. The isolation forest construction process ends when all objects in the node coincide identically. This is the stopping criterion. The mark of the leaves is the <strong class="source-inline">anomaly_score</strong> value of the algorithm, which is the depth of the leaves in the constructed tree. The following formula shows how the anomaly score can <span class="No-Break">be calculated:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer177">
					<img alt="" role="presentation" src="image/B19849_Formula_072.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_082.png"/> is the path length of the observation, <img alt="" role="presentation" src="image/B19849_Formula_091.png"/>, <img alt="" role="presentation" src="image/B19849_Formula_102.png"/> is an average of <img alt="" role="presentation" src="image/B19849_Formula_113.png"/> from a collection of isolation trees, <img alt="" role="presentation" src="image/B19849_Formula_124.png"/> is the average path length of the unsuccessful search in a binary search tree, and <img alt="" role="presentation" src="image/B19849_Formula_132.png"/> is the number of <span class="No-Break">external nodes.</span></p>
			<p>We’re <a id="_idIndexMarker581"/>assuming that it is common for anomalies to appear in leaves with a low depth, which is close to the root, but for regular objects, the tree will build several more levels. The number of such levels is proportional to the size of the cluster. Consequently, <strong class="source-inline">anomaly_score</strong> is proportional to the points lying <span class="No-Break">in it.</span></p>
			<p>This assumption means that objects from clusters of small sizes (which are potentially anomalies) will have a lower <strong class="source-inline">anomaly_score</strong> value than those from clusters of <span class="No-Break">regular data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer184">
					<img alt="Figure 5.4 – Isolation forest visualization" src="image/B19849_05_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Isolation forest visualization</p>
			<p>The <a id="_idIndexMarker582"/>isolation forest method is widely used<a id="_idIndexMarker583"/> and implemented in <span class="No-Break">v<a id="_idTextAnchor276"/>arious libraries.</span></p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor277"/>Detecting anomalies with One-Class Support Vector Machine</h2>
			<p>The support vector<a id="_idIndexMarker584"/> method is a binary <a id="_idIndexMarker585"/>classification method based on using a <strong class="bold">hyperplane</strong> to divide <a id="_idIndexMarker586"/>objects into classes. The dimensions of the hyperplane are always chosen so that they’re less than the dimensions of the original space. In <img alt="" role="presentation" src="image/B19849_Formula_143.png"/>, for example, a hyperplane is an ordinary two-dimensional plane. The distance from the hyperplane to each class should be as short as possible. The vectors that are closest to the separating hyperplane are called support vectors. In practice, cases where the data can be divided by a hyperplane—in other words, linear cases—are quite rare. In this case, all the elements of the training dataset are embedded in the higher dimension space, <img alt="" role="presentation" src="image/B19849_Formula_152.png"/>, using a special mapping. In this case, the mapping is chosen so that in the new space, <img alt="" role="presentation" src="image/B19849_Formula_152.png"/>, the dataset is linearly separable. Such mapping is based on kernel functions and is usually named the Kernel trick; it will be discussed more precisely in <a href="B19849_06.xhtml#_idTextAnchor301"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">.</span></p>
			<p><strong class="bold">One-Class Support Vector Machine</strong> (<strong class="bold">OCSVM</strong>) is an <a id="_idIndexMarker587"/>adaptation of the support vector method that focuses on anomaly detection. OCSVM differs from the standard version of <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) in a <a id="_idIndexMarker588"/>way that the resulting optimization problem <a id="_idIndexMarker589"/>includes an improvement for determining a small percentage of predetermined anomalous values, which allows this method to be used to detect anomalies. These<a id="_idIndexMarker590"/> anomalous values lie between the starting point and the optimal separating hyperplane. All other data belonging to the same class falls on the opposite side of the optimal <span class="No-Break">separating hyperplane.</span></p>
			<p>There’s also another type of OCSVM method that uses a spherical, instead of a planar (or linear), approach. The algorithm obtains a spherical boundary, in the feature space, around the data. The volume of this hypersphere is minimized to reduce the effect of incorporating outliers in <span class="No-Break">the solution.</span></p>
			<p>A <strong class="bold">spherical mapping</strong> is <a id="_idIndexMarker591"/>appropriate when the data has a spherical shape, such as when the data points are distributed evenly around the origin. A <strong class="bold">planar (or linear) mapping</strong> is <a id="_idIndexMarker592"/>more appropriate when the data has a planar shape, such as when the data points lie on a line or plane. Also, other kernel functions can be used to map the data into a higher-dimensional space where it is linearly separable. The choice of kernel function depends on the nature of <span class="No-Break">the data.</span></p>
			<p>OCSVM assigns a label, which is the distance from the test data point to the optimal hyperplane. Positive values in the OCSVM output represent normal behavior (with higher values representing greater normality), while negative values represent anomalous behavior (the lower the value, the more significant <span class="No-Break">the anomaly).</span></p>
			<p>In order to assign a label, the OCSVM first trains on a dataset that consists of only normal or expected behavior. This dataset is <a id="_idIndexMarker593"/>called the <strong class="bold">positive class</strong>. The OCSVM then tries to find a hyperplane that maximizes the distance between the positive class and the origin. This hyperplane is<a id="_idIndexMarker594"/> called the <span class="No-Break"><strong class="bold">decision boundary</strong></span><span class="No-Break">.</span></p>
			<p>Once the decision boundary has been found, any new data point that falls outside of this boundary is considered an anomaly or outlier. The OCSVM assigns a label of <strong class="source-inline">ano<a id="_idTextAnchor278"/>m<a id="_idTextAnchor279"/>aly</strong> to these <span class="No-Break">data points.</span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor280"/>Density estimation approach</h2>
			<p>One of the <a id="_idIndexMarker595"/>most popular <a id="_idIndexMarker596"/>approaches to anomaly detection is density estimation, which involves estimating the probability distribution of normal data and then flagging observations as anomalies if they fall outside the expected range. The basic idea behind density estimation is to fit a model to the data that represents the underlying distribution of normal behavior. This <a id="_idIndexMarker597"/>model can be a simple parametric distribution <a id="_idIndexMarker598"/>such as <strong class="bold">Gaussian</strong> or a more complex non-parametric model such as <strong class="bold">kernel density estimation</strong> (<strong class="bold">KDE</strong>). Once the model is trained on normal data, it can be used to estimate the density of new observations. Observations with low density are <span class="No-Break">considered anomalies.</span></p>
			<p>There are several advantages to using a density <span class="No-Break">estimation approach:</span></p>
			<ul>
				<li>It is flexible and can handle a wide range of <span class="No-Break">data types</span></li>
				<li>It does not require labeled <a id="_idIndexMarker599"/>data for training, making it suitable for <strong class="bold">unsupervised </strong><span class="No-Break"><strong class="bold">learning</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">UL</strong></span><span class="No-Break">)</span></li>
				<li>It can detect both point anomalies (observations that are significantly different from the rest) and contextual anomalies (observations that do not follow the normal pattern within a <span class="No-Break">specific context)</span></li>
			</ul>
			<p>However, there are also some challenges with <span class="No-Break">this approach:</span></p>
			<ul>
				<li>The choice of model and parameters can affect the performance of <span class="No-Break">the algorithm</span></li>
				<li>Outliers can influence the estimated density, leading to <span class="No-Break">false positives</span></li>
				<li>The algorithm may not be able to detect anomalies that are not well represented in the <span class="No-Break">training data</span></li>
			</ul>
			<p>Overall, density estimation<a id="_idIndexMarker600"/> is a powerful tool for anomaly detection that can be customized to suit the specific needs of an application. By carefully selecting the model and tuning the parameters, it is possible to achieve high accuracy and precision in <span class="No-Break">detecting anomalies.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor281"/>Using multivariate Gaussian distribution for anomaly detection</h2>
			<p>Let’s <a id="_idIndexMarker601"/>assume we have<a id="_idIndexMarker602"/> some samples <img alt="" role="presentation" src="image/B19849_Formula_172.png"/> in a dataset and that they are labeled and normally distributed (Gaussian distribution). In such a case, we can use distribution properties to detect anomalies. Let’s assume that the function <img alt="" role="presentation" src="image/B19849_Formula_182.png"/> gives us the probability of a sample being normal. A high probability corresponds to a regular sample, while a low probability corresponds to an anomaly. We can, therefore, choose thresholds to distinguish between regular values and anomalies with the <a id="_idIndexMarker603"/>following <strong class="bold">anomaly </strong><span class="No-Break"><strong class="bold">model formula</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer190">
					<img alt="" role="presentation" src="image/B19849_Formula_19.jpg"/>
				</div>
			</div>
			<p>If [<img alt="" role="presentation" src="image/B19849_Formula_201.png"/>] and <img alt="" role="presentation" src="image/B19849_Formula_212.png"/> follows the Gaussian distribution with the mean, <img alt="" role="presentation" src="image/B19849_Formula_222.png"/>, and the variance, <img alt="" role="presentation" src="image/B19849_Formula_231.png"/>, it is denoted <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer195">
					<img alt="" role="presentation" src="image/B19849_Formula_242.jpg"/>
				</div>
			</div>
			<p>The following formula gives the probability of <img alt="" role="presentation" src="image/B19849_Formula_251.png"/> in a <span class="No-Break">Gaussian distribution:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer197">
					<img alt="" role="presentation" src="image/B19849_Formula_261.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_272.png"/> is the mean and <img alt="" role="presentation" src="image/B19849_Formula_283.png"/> is the variance (<img alt="" role="presentation" src="image/B19849_Formula_291.png"/> is the standard deviation). This formula is known as parametrized <strong class="bold">probability density function</strong> (<strong class="bold">PDF</strong>), and it <a id="_idIndexMarker604"/>describes the relative likelihood of different outcomes in a continuous random variable. It is used to model the distribution of a random variable and provides information about the probability of observing a specific value or range of values for <span class="No-Break">that variable.</span></p>
			<p>Next, we’ll introduce an example of the general approach we follow for anomaly detection<a id="_idIndexMarker605"/> with Gaussian distribution <span class="No-Break">density estimation:</span></p>
			<ol>
				<li>Let’s say we’re given a new <span class="No-Break">example, <img alt="" role="presentation" src="image/B19849_Formula_302.png"/></span><span class="No-Break"><span class="subscript">.</span></span></li>
				<li>Select the features, <img alt="" role="presentation" src="image/B19849_Formula_312.png"/>, that are regular, meaning they determine <span class="No-Break">anomalous behavior.</span></li>
				<li>Fit the <img alt="" role="presentation" src="image/B19849_Formula_322.png"/>and <img alt="" role="presentation" src="image/B19849_Formula_33.png"/> <span class="No-Break">parameters.</span></li>
				<li>Compute <img alt="" role="presentation" src="image/B19849_Formula_34.png"/> using an equation to calculate the probability of <img alt="" role="presentation" src="image/B19849_Formula_35.png"/> in a <span class="No-Break">Gaussian distribution.</span></li>
				<li>Determine if <img alt="" role="presentation" src="image/B19849_Formula_04.png"/> is an anomaly by comparing it with the threshold, <img alt="" role="presentation" src="image/B19849_Formula_37.png"/>; see the anomaly <span class="No-Break">model formula.</span></li>
			</ol>
			<p>The<a id="_idIndexMarker606"/> following graph shows an example of Gaussian distribution density estimation for normally <span class="No-Break">distributed data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer209">
					<img alt="Figure 5.5 – Gaussian density estimation visualization" src="image/B19849_05_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Gaussian density estimation visualization</p>
			<p>In this<a id="_idIndexMarker607"/> approach, we assume that selected features are independent, but usually, in real data, there are some correlations between them. In such a case, we should use a multivariate Gaussian distribution model instead of a <span class="No-Break">univariate one.</span></p>
			<p>The<a id="_idIndexMarker608"/> following formula gives the probability of <img alt="" role="presentation" src="image/B19849_Formula_38.png"/> in a multivariate <span class="No-Break">Gaussian distribution:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer211">
					<img alt="" role="presentation" src="image/B19849_Formula_391.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_40.png"/> is the mean, <img alt="" role="presentation" src="image/B19849_Formula_41.png"/> is the correlation matrix, and <img alt="" role="presentation" src="image/B19849_Formula_421.png"/> is the determinant of the <span class="No-Break">matrix, <img alt="" role="presentation" src="image/B19849_Formula_41.png"/>:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer216">
					<img alt="" role="presentation" src="image/B19849_Formula_44.jpg"/>
				</div>
			</div>
			<p>The following graphs shows the difference between the univariate and the multivariate Gaussian distribution estimation models for a dataset with correlated data. Notice how distribution boundaries cover the regular data with a darker color, while anomalies are marked with a <span class="No-Break">lighter color:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer217">
					<img alt="Figure 5.6 – Univariate and multivariate Gaussian distributions" src="image/B19849_05_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Univariate and multivariate Gaussian distributions</p>
			<p>We can<a id="_idIndexMarker609"/> see that the multivariate Gaussian distribution can take into account correlations in the data and adapt its shape to them. This characteristic allows us to detect anomalies correctly for types of data whose distribution follows a Gaussian (normal) distribution shape. Also, we can see one of the advantages of this approach: results can be<a id="_idIndexMarker610"/> easily visualized in two or three dimensions, providing a clear understanding of <span class="No-Break">the data.</span></p>
			<p>In the next section, we will look at another approach for <span class="No-Break">anomaly detection.</span></p>
			<h3>KDE</h3>
			<p>In the <a id="_idIndexMarker611"/>KDE approach, our <a id="_idIndexMarker612"/>goal is to approximate a complex mixture of random<a id="_idIndexMarker613"/> distributions around point samples into a single function. So, the main idea is to center a probability distribution function at each data point and then take their average. It means that each discrete point in our dataset is replaced by an extended probability distribution, called a <strong class="bold">kernel</strong>. The probability density at any given point is then estimated as the<a id="_idIndexMarker614"/> sum of the kernel functions centered at each discrete point. If the point is close to many other points its estimated probability density will be larger than if it is far away from any sample point. This approach can be used to find anomalies as points where the estimated density <span class="No-Break">is minimal.</span></p>
			<p>Mathematically, we can formulate the KDE function for univariate kernels <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer218">
					<img alt="" role="presentation" src="image/B19849_Formula_45.jpg"/>
				</div>
			</div>
			<p>Here, K<span class="subscript">h</span> is a smoothed kernel defined with the <span class="No-Break">following formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer219">
					<img alt="" role="presentation" src="image/B19849_Formula_46.jpg"/>
				</div>
			</div>
			<p>Here, h is the bandwidth parameter that defines the width of a kernel. The bandwidth parameter controls the degree of smoothness or roughness of the kernel function. A larger bandwidth results in a smoother function, while a smaller bandwidth leads to a more rugged one. Choosing the right bandwidth is crucial for achieving good <span class="No-Break">generalization performance.</span></p>
			<p>K can be, for example, a Gaussian kernel function, which is the most <span class="No-Break">common one:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer220">
					<img alt="" role="presentation" src="image/B19849_Formula_471.jpg"/>
				</div>
			</div>
			<p>The following graph shows a plot of KDE of Gaussian distribution made from <span class="No-Break">individual kernels:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer221">
					<img alt="Figure 5.7 – KDE of Gaussian distribution" src="image/B19849_05_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – KDE of Gaussian distribution</p>
			<p>You<a id="_idIndexMarker615"/> can see in this graph that the density value will be the maximum for points around the value <strong class="source-inline">3</strong> because most sample points are located<a id="_idIndexMarker616"/> in this region. Another cluster of points is located around point <strong class="source-inline">8</strong>, but their density is quite a <span class="No-Break">bit smaller.</span></p>
			<p>Before training the KDE model, it is important to preprocess the data to ensure that it is suitable for the KDE algorithm. This may include scaling the data so that all features have similar ranges, removing outliers or anomalies, and transforming the data <span class="No-Break">if necessary.</span></p>
			<p>Handling high-dimensional data<a id="_idIndexMarker617"/> efficiently in KDE can be challenging due to the <strong class="bold">curse of dimensionality</strong> (<strong class="bold">CoD</strong>). One approach is to use dimensionality<a id="_idIndexMarker618"/> reduction techniques such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) or <strong class="bold">t-distributed stochastic neighbor embedding</strong> (<strong class="bold">t-SNE</strong>) to reduce the number <a id="_idIndexMarker619"/>of dimensions before applying KDE. Another approach is to use sparse KDE algorithms that only consider a subset of data points when calculating the density estimate for a <span class="No-Break">given location.</span></p>
			<p>In the following section, we will look at another approach for <span class="No-Break">anomaly detection.</span></p>
			<h3>Density estimation trees</h3>
			<p>The <strong class="bold">density estimation tree</strong> (<strong class="bold">DET</strong>) algorithm<a id="_idIndexMarker620"/> also<a id="_idIndexMarker621"/> can be used to detect anomalies by thresholding the density value for certain sample points. This is a non-parametric technique based on decision tree construction. The main advantage of this algorithm is the fast analytical complexity of density estimation at any given point, its <em class="italic">O(log n)</em> time, where n is<a id="_idIndexMarker622"/> the number of points in the tree. The tree is constructed iteratively in a top-to-bottom approach. Each leaf, <em class="italic">t</em>, is divided into two sub-leaves <em class="italic">t</em><span class="subscript">l</span> and <em class="italic">t</em><span class="subscript">r</span> by maximizing residual gain <em class="italic">s</em> that is defined <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer222">
					<img alt="" role="presentation" src="image/B19849_Formula_481.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">R(t)</em> is the tree <span class="No-Break">loss function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer223">
					<img alt="" role="presentation" src="image/B19849_Formula_491.jpg"/>
				</div>
			</div>
			<p><em class="italic">N</em> is the number of candidates the <em class="italic">t</em> leaf contains, and <em class="italic">V</em> is the leaf’s volume. Then, the actual density for a leaf <em class="italic">t</em> can be calculated <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer224">
					<img alt="" role="presentation" src="image/B19849_Formula_501.jpg"/>
				</div>
			</div>
			<p>So, to estimate a density value for a given point, we have to determine to which leaf it belongs and then get the <span class="No-Break">leaf’s density.</span></p>
			<p>In the current section, we discussed various anomaly detection approaches, and in the following sections, we will see how to use various C++ libraries to deal with the anomaly<a id="_idTextAnchor282"/> <span class="No-Break">detection task.</span></p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor283"/>Examples of using different C++ libraries for anomaly detection</h1>
			<p>In this section, we’ll look at some <a id="_idIndexMarker623"/>examples of how to implement the algorithms we described previously for <a id="_idTextAnchor284"/><span class="No-Break">a<a id="_idTextAnchor285"/>nomaly detection.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor286"/>C++ implementation of the isolation forest algorithm for anomaly detection</h2>
			<p><strong class="bold">Isolation forest algorithms</strong> can <a id="_idIndexMarker624"/>be easily implemented in pure C++ because their logic is <a id="_idIndexMarker625"/>pretty straightforward. Also, there are no implementations of this algorithm in popular C++ libraries. Let’s assume that our implementation will only be used with two-dimensional data. We are going to detect anomalies in a range of samples where each sample contains the same number <span class="No-Break">of features.</span></p>
			<p>Because our dataset is large enough, we can define a wrapper for the actual data container. This allows us to reduce the number of copy operations we perform on the <span class="No-Break">actual data:</span></p>
			<pre class="source-code">
using DataType = double;
template &lt;size_t Cols&gt;
using Sample = std::array&lt;DataType, Cols&gt;;
template &lt;size_t Cols&gt;
using Dataset = std::vector&lt;Sample&lt;Cols&gt;&gt;;
...
template &lt;size_t Cols&gt;
struct DatasetRange {
  DatasetRange(std::vector&lt;size_t&gt;&amp;&amp; indices,
               const Dataset&lt;Cols&gt;* dataset)
    : indices(std::move(indices)), dataset(dataset) {}
  size_t size() const { return indices.size(); }
  DataType at(size_t row, size_t col) const {
    return (*dataset)[indices[row]][col];
  }
  std::vector&lt;size_t&gt; indices;
  const Dataset&lt;Cols&gt;* dataset;
};</pre>			<p>The <strong class="source-inline">DatasetRange</strong> type holds a reference to the vector of <strong class="source-inline">Sample</strong> type objects and to the<a id="_idIndexMarker626"/> container of indices that point to the samples in the dataset. These indices define the exact dataset <a id="_idIndexMarker627"/>objects that this <strong class="source-inline">DatasetRange</strong> object <span class="No-Break">points to.</span></p>
			<p>Next, we define the elements of the isolation tree, with the first one being the <span class="No-Break"><strong class="source-inline">Node</strong></span><span class="No-Break"> type:</span></p>
			<pre class="source-code">
struct Node {
  Node() {}
  Node(const Node&amp;) = delete;
  Node&amp; operator=(const Node&amp;) = delete;
  Node(std::unique_ptr&lt;Node&gt; left,
     std::unique_ptr&lt;Node&gt; right, size_t split_col,
     DataType split_value)
    : left(std::move(left)),
      right(std::move(right)),
      split_col(split_col),
      split_value(split_value) {}
  Node(size_t size) : size(size), is_external(true) {}
  std::unique_ptr&lt;Node&gt; left;
  std::unique_ptr&lt;Node&gt; right;
  size_t split_col{0};
  DataType split_value{0};
  size_t size{0};
  bool is_external{false};
};</pre>			<p>This type is a <a id="_idIndexMarker628"/>regular tree-node structure. The following members are specific to the isolation<a id="_idIndexMarker629"/> <span class="No-Break">tree algorithm:</span></p>
			<ul>
				<li><strong class="source-inline">split_col</strong>: This is the index of the feature column where the algorithm caused <span class="No-Break">a split</span></li>
				<li><strong class="source-inline">split_value</strong>: This is the value of the feature where the algorithm caused <span class="No-Break">a split</span></li>
				<li><strong class="source-inline">size</strong>: This is the number of underlying items for <span class="No-Break">the node</span></li>
				<li><strong class="source-inline">is_external</strong>: This is the flag that indicates whether the node is <span class="No-Break">a leaf</span></li>
			</ul>
			<p>Taking the <strong class="source-inline">Node</strong> type as a basis, we can define the procedure of building an isolation tree. We aggregate this procedure with the auxiliary <strong class="source-inline">IsolationTree</strong> type. Because the current algorithm is based on random splits, the auxiliary data is the random <span class="No-Break">engine object.</span></p>
			<p>We only need to initialize this object once, and then it will be shared among all tree-type objects. This approach allows us to make the results of the algorithm reproducible in the case of constant seeding. Furthermore, it makes debugging the randomized algorithm <span class="No-Break">much</span><span class="No-Break"><a id="_idIndexMarker630"/></span><span class="No-Break"> simpler:</span></p>
			<pre class="source-code">
template &lt;size_t Cols&gt;
class IsolationTree {
 public:
  using Data = DatasetRange&lt;Cols&gt;;
  IsolationTree(const IsolationTree&amp;) = delete;
  IsolationTree&amp; operator=(const IsolationTree&amp;) = delete;
  IsolationTree(std::mt19937* rand_engine, Data data, size_t hlim)
      : rand_engine(rand_engine) {
    root = MakeIsolationTree(data, 0, hlim);
  }
  IsolationTree(IsolationTree&amp;&amp; tree) {
    rand_engine = std::move(tree.rand_engine);
    root = td::move(tree.root);
  }
  double PathLength(const Sample&lt;Cols&gt;&amp; sample) {
    return PathLength(sample, root.get(), 0);
  }
 private:
  std::unique_ptr&lt;Node&gt; MakeIsolationTree(const Data&amp; data,
                                          size_t height,
                                          size_t hlim);
  double PathLength(const Sample&lt;Cols&gt;&amp; sample,
                    const Node* node, double height);
 private:
  std::mt19937* rand_engine;
  std::unique_ptr&lt;Node&gt; root;
};</pre>			<p>Next, we’ll <a id="_idIndexMarker631"/>do the most critical work in the <strong class="source-inline">MakeIsolationTree()</strong> method, which is used in the <a id="_idIndexMarker632"/>constructor to initialize the root <span class="No-Break">data member:</span></p>
			<pre class="source-code">
std::unique_ptr&lt;Node&gt; MakeIsolationTree(const Data&amp; data,
                                        size_t height, size_t hlim) {
  auto len = data.size();
  if (height &gt;= hlim || len &lt;= 1) {
    return std::make_unique&lt;Node&gt;(len);
  } else {
    std::uniform_int_distribution&lt;size_t&gt; cols_dist(0, Cols - 1);
    auto rand_col = cols_dist(*rand_engine);
    std::unordered_set&lt;DataType&gt; values;
    for (size_t i = 0; i &lt; len; ++i) {
      auto value = data.at(i, rand_col);
      values.insert(value);
    }
    auto min_max = std::minmax_element(values.begin(), values.end());
    std::uniform_real_distribution&lt;DataType&gt; value_dist(
      *min_max.first, *min_max.second);
    auto split_value = value_dist(*rand_engine);
    std::vector&lt;size_t&gt; indices_left;
    std::vector&lt;size_t&gt; indices_right;
    for (size_t i = 0; i &lt; len; ++i) {
      auto value = data.at(i, rand_col);
      if (value &lt; split_value) {
        indices_left.push_back(data.indices[i]);
      } else {
        indices_right.push_back(data.indices[i]);
      }
    }
    return std::make_unique&lt;Node&gt;(
      MakeIsolationTree(
        Data{std::move(indices_left), data.dataset},
        height + 1, hlim),
      MakeIsolationTree(
        Data{std::move(indices_right), data.dataset},
        height + 1, hlim),
      rand_col, split_value);
  }
}</pre>			<p>Initially, we <a id="_idIndexMarker633"/>checked the termination conditions to stop the splitting process. If we meet them, we return a new node marked as an external leaf. Otherwise, we start splitting <a id="_idIndexMarker634"/>the passed data range. For splitting, we randomly select the <strong class="source-inline">feature</strong> column and determine the unique values of the selected feature. Then, we randomly select a value from an interval between the <strong class="source-inline">max</strong> and <strong class="source-inline">min</strong> values among the feature values from all the samples. After we make these random selections, we compare the values of the selected splitting feature to all the samples from the input data range and put their indices into two lists. One list is for values higher than the splitting values, while another list is for values that are lower than them. Then, we return a new tree node initialized with references to the left and right nodes, which are initialized with recursive calls to the <span class="No-Break"><strong class="source-inline">MakeIsolationTree()</strong></span><span class="No-Break"> method.</span></p>
			<p>Another vital method of the <strong class="source-inline">IsolationTree</strong> type is the <strong class="source-inline">PathLength()</strong> method. We use it for anomaly score calculations. It takes the sample as an input parameter and returns the amortized path length to the corresponding tree leaf from the <span class="No-Break">root node:</span></p>
			<pre class="source-code">
double PathLength(const Sample&lt;Cols&gt;&amp; sample, const Node* node,
                  double height) {
  assert(node != nullptr);
  if (node-&gt;is_external) {
    return height + CalcC(node-&gt;size);
  } else {
    auto col = node-&gt;split_col;
    if (sample[col] &lt; node-&gt;split_value) {
      return PathLength(sample, node-&gt;left.get(), height + 1);
    } else {
      return PathLength(sample, node-&gt;right.get(), height + 1);
    }
  }
}</pre>			<p>The <strong class="source-inline">PathLength()</strong> method<a id="_idIndexMarker635"/> finds the leaf node during tree traversal based on sample feature values. These values are used to select a tree traversal direction based on the current node-splitting values. During each step, this method also increases the resulting <a id="_idIndexMarker636"/>height. The result of this method is a sum of the actual tree traversal height and the value returned from the call to the <strong class="source-inline">CalcC()</strong> function, which then returns the average path’s length of unsuccessful searches in a binary search tree of equal height to the leaf node. The <strong class="source-inline">CalcC()</strong> function can be implemented in the following way, according to the formula from the original paper, which describes the isolation forest algorithm (you can find a reference to this in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section):</span></p>
			<pre class="source-code">
double CalcC(size_t n) {
  double c = 0;
  if (n &gt; 1)
  c = 2 * (log(n - 1) + 0.5772156649) - (
    2 * (n - 1) / n);
  return c;
}</pre>			<p>The final part of the algorithm’s implementation is the creation of the forest. The forest is an array of trees built from a limited number of samples, randomly chosen from the original dataset. The <a id="_idIndexMarker637"/>number of samples used to build the tree is a hyperparameter of this algorithm. Furthermore, this implementation uses heuristics as the stopping criteria, in that it is a maximum tree height <span class="No-Break"><strong class="source-inline">hlim</strong></span><span class="No-Break"> value.</span></p>
			<p>Let’s see how it is used in the tree-building procedure. The <strong class="source-inline">hlim</strong> value is calculated only once, and the following code shows this. Moreover, it is based on the number of samples that <a id="_idIndexMarker638"/>are used to build a <span class="No-Break">single tree:</span></p>
			<pre class="source-code">
template &lt;size_t Cols&gt;
class IsolationForest {
 public:
  using Data = DatasetRange&lt;Cols&gt;;
  IsolationForest(const IsolationForest&amp;) = delete;
  IsolationForest&amp; operator=(const IsolationForest&amp;) = delete;
  IsolationForest(const Dataset&lt;Cols&gt;&amp; dataset,
                  size_t num_trees, size_t sample_size)
      : rand_engine(2325) {
    std::vector&lt;size_t&gt; indices(dataset.size());
    std::iota(indices.begin(), indices.end(), 0);
    size_t hlim = static_cast&lt;size_t&gt;(ceil(log2(sample_size)));
    for (size_t i = 0; i &lt; num_trees; ++i) {
      std::vector&lt;size_t&gt; sample_indices;
      std::sample(indices.begin(), indices.end(),
                  std::back_insert_iterator(sample_indices),
                  sample_size, rand_engine);
      trees.emplace_back(
          &amp;rand_engine,
          Data(std::move(sample_indices), &amp;dataset), hlim);
    }
    double n = dataset.size();
    c = CalcC(n);
  }
  double AnomalyScore(const Sample&lt;Cols&gt;&amp; sample) {
    double avg_path_length = 0;
    for (auto&amp; tree : trees) {
      avg_path_length += tree.PathLength(sample);
    }
    avg_path_length /= trees.size();
    double anomaly_score = pow(2, -avg_path_length / c);
    return anomaly_score;
  }
 private:
  std::mt19937 rand_engine;
  std::vector&lt;IsolationTree&lt;Cols&gt;&gt; trees;
  double c{0};
};
}</pre>			<p>The<a id="_idIndexMarker639"/> tree forest is <a id="_idIndexMarker640"/>built in the constructor of the <strong class="source-inline">IsolationForest</strong> type. We also calculated the value of the average path length of the unsuccessful search in a binary search tree for all of the samples in the constructor. We use this forest in the <strong class="source-inline">AnomalyScore()</strong> method for the actual process of anomaly detection. It implements the formula for the anomaly score value for a given sample. It returns a value that can be interpreted in the following way: if the returned value is close to <strong class="source-inline">1</strong>, then the sample has anomalous features, while if the value is less than <strong class="source-inline">0.5</strong>, then we can assume that the sample is a <span class="No-Break">normal one.</span></p>
			<p>The following code shows how we can use this algorithm. Furthermore, it uses <strong class="source-inline">Dlib</strong> primitives for the <span class="No-Break">dataset’s representation:</span></p>
			<pre class="source-code">
void IsolationForest(const Matrix&amp; normal, const Matrix&amp; test) {
  iforest::Dataset&lt;2&gt; dataset;
  auto put_to_dataset = [&amp;](const Matrix&amp; samples) {
    for (long r = 0; r &lt; samples.nr(); ++r) {
      auto row = dlib::rowm(samples, r);
      double x = row(0, 0);
      double y = row(0, 1);
      dataset.push_back({x, y});
    }
  };
  put_to_dataset(normal);
  put_to_dataset(test);
  iforest::IsolationForest iforest(dataset, 300, 50);
  double threshold = 0.6;  // change this value to see isolation 
                          //boundary
  for (auto&amp; s : dataset) {
    auto anomaly_score = iforest.AnomalyScore(s);
    // std::cout &lt;&lt; anomaly_score &lt;&lt; " " &lt;&lt; s[0] &lt;&lt; " " &lt;&lt; s[1]
    // &lt;&lt; std::endl;
    if (anomaly_score &lt; threshold) {
      // Do something with normal
    } else {
      // Do something with anomalies
    }
  }
}</pre>			<p>In the preceding example, we<a id="_idIndexMarker641"/> converted and merged the given datasets for the container that’s suitable for our algorithm. Then, we initialized the object of the <strong class="source-inline">IsolationForest</strong> type, which immediately builds the isolation forest with the following hyperparameters: the number of trees is 100, and the number of samples used for one tree <span class="No-Break">is 50.</span></p>
			<p>Finally, we <a id="_idIndexMarker642"/>called the <strong class="source-inline">AnomalyScore()</strong> method for each sample from the dataset in order to detect anomalies with thresholds and return their values. In the following graph, we can see the result of anomaly detection after using the isolation forest algorithm. The points labeled as <strong class="source-inline">1 cls</strong> are <span class="No-Break">the anomalies:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer225">
					<img alt="Figure 5.8 – Anomaly detect﻿ion with isolation forest algorithm" src="image/B19849_05_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Anomaly detect<a id="_idTextAnchor287"/>ion with isolation forest algorithm</p>
			<p>In this <a id="_idIndexMarker643"/>section, we<a id="_idIndexMarker644"/> learned how to implement the isolation forest algorithm from scratch. The following section will show you how to use th<a id="_idTextAnchor288"/>e <strong class="source-inline">Dlib</strong> library for <span class="No-Break">anomaly detection.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor289"/>Using the Dlib library for anomaly detection</h2>
			<p>The <strong class="source-inline">Dlib</strong> library <a id="_idIndexMarker645"/>provides a couple of<a id="_idIndexMarker646"/> implemented algorithms that we can use for anomaly detection: the OCSVM mo<a id="_idTextAnchor290"/>d<a id="_idTextAnchor291"/>el and the multivariate <span class="No-Break">Gaussian model.</span></p>
			<h3>OCSVM with Dlib</h3>
			<p>There is only <a id="_idIndexMarker647"/>one algorithm that’s implemented in the <strong class="source-inline">Dlib</strong> library straight out of the box: OCSVM. There is a <strong class="source-inline">svm_one_class_trainer</strong> class in this library that can be used to train the corresponding algorithm, which should be configured with a kernel object, and the <strong class="source-inline">nu</strong> parameter, which controls the smoothness (in other words, the degree to which it controls the ratio between generalization and overfitting) of <span class="No-Break">the solution.</span></p>
			<p>The most widely used kernel is based on the Gaussian distribution and is known as the <strong class="bold">Radial Basis Function</strong> (<strong class="bold">RBF</strong>) kernel. It <a id="_idIndexMarker648"/>is implemented in the <strong class="source-inline">radial_basis_kernel</strong> class. Typically, we represent datasets in the <strong class="source-inline">Dlib</strong> library as a C++ vector of separate samples. Therefore before using this <strong class="source-inline">trainer</strong> object, we have to convert a matrix dataset into <span class="No-Break">a vector:</span></p>
			<pre class="source-code">
void OneClassSvm(const Matrix&amp; normal, const Matrix&amp; test) {
  typedef matrix&lt;double, 0, 1&gt; sample_type;
  typedef radial_basis_kernel&lt;sample_type&gt; kernel_type;
  svm_one_class_trainer&lt;kernel_type&gt; trainer;
  trainer.set_nu(0.5);  // control smoothness of the solution
  trainer.set_kernel(kernel_type(0.5));  // kernel bandwidth
  std::vector&lt;sample_type&gt; samples;
  for (long r = 0; r &lt; normal.nr(); ++r) {
    auto row = rowm(normal, r);
    samples.push_back(row);
  }
  decision_function&lt;kernel_type&gt; df = trainer.train(samples);
  Clusters clusters;
  double dist_threshold = -2.0;
  auto detect = [&amp;](auto samples) {
    for (long r = 0; r &lt; samples.nr(); ++r) {
      auto row = dlib::rowm(samples, r);
      auto dist = df(row);
      if (p &gt; dist_threshold) {
        // Do something with anomalies
      } else {
        // Do something with normal
      }
    }
  };
  detect(normal);
  detect(test);
}</pre>			<p>The result of the training process is a decision function object of the <strong class="source-inline">decision_function&lt;kernel_type&gt;</strong> class that we can use for single sample classification. Objects of this type can be used as a regular function. The result of a decision function is the distance from the normal class boundary, so the most distant samples can <a id="_idIndexMarker649"/>be classified as anomalies. The following graph shows an example of how the OCSVM algorithm from the <strong class="source-inline">Dlib</strong> library works. Note that the dots labeled as <strong class="source-inline">1 cls</strong> correspond <span class="No-Break">to anomalies:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer226">
					<img alt="Figure 5.9 – Anomaly﻿ detection with Dlib OCSVM implementation" src="image/B19849_05_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Anomaly<a id="_idTextAnchor292"/> detection with Dlib OCSVM implementation</p>
			<p>We can see that OCSVM solved the task well and detected anomalies that are very interpretable. In the next section, we will see how to use a multiva<a id="_idTextAnchor293"/>riate Gaussian model to <span class="No-Break">detect anomalies.</span></p>
			<h3>Multivariate Gaussian model with Dlib</h3>
			<p>Using the<a id="_idIndexMarker650"/> linear algebra facilities of the <strong class="source-inline">D</strong><strong class="source-inline">lib</strong> library (or any other library, for that matter), we can implement anomaly detection with the multivariate Gaussian distribution approach. The following example shows how to implement this approach with <strong class="source-inline">Dlib</strong> linear <span class="No-Break">algebra routines:</span></p>
			<pre class="source-code">
void multivariateGaussianDist(const Matrix&amp; normal,
                              const Matrix&amp; test) {
  // assume that rows are samples and columns are features
  // calculate per feature mean
  dlib::matrix&lt;double&gt; mu(1, normal.nc());
  dlib::set_all_elements(mu, 0);
  for (long c = 0; c &lt; normal.nc(); ++c) {
    auto col_mean = dlib::mean(dlib::colm(normal, c));
    dlib::set_colm(mu, c) = col_mean;
  }
  // calculate covariance matrix
  dlib::matrix&lt;double&gt; cov(normal.nc(), normal.nc());
  dlib::set_all_elements(cov, 0);
  for (long r = 0; r &lt; normal.nr(); ++r) {
    auto row = dlib::rowm(normal, r);
    cov += dlib::trans(row - mu) * (row - mu);
  }
  cov *= 1.0 / normal.nr();
  double cov_det = dlib::det(cov);  // matrix determinant
  dlib::matrix&lt;double&gt; cov_inv = dlib::inv(cov);  // inverse matrix
  //  define probability function
  auto first_part = 1. / std::pow(2. * M_PI, normal.nc() / 2.) /
                    std::sqrt(cov_det);
  auto prob = [&amp;](const dlib::matrix&lt;double&gt;&amp; sample) {
    dlib::matrix&lt;double&gt; s = sample - mu;
    dlib::matrix&lt;double&gt; exp_val_m =
        s * (cov_inv * dlib::trans(s));
    double exp_val = -0.5 * exp_val_m(0, 0);
    double p = first_part * std::exp(exp_val);
    return p;
  };
  // change this parameter to see the decision boundary
  double prob_threshold = 0.001;
  auto detect = [&amp;](auto samples) {
    for (long r = 0; r &lt; samples.nr(); ++r) {
      auto row = dlib::rowm(samples, r);
      auto p = prob(row);
      if (p &gt;= prob_threshold) {
        // Do something with anomalies
      } else {
        // Do something with normal
      }
    }
  };
  detect(normal);
  detect(test);
}</pre>			<p>The<a id="_idIndexMarker651"/> idea of this approach is to define a function that returns the probability of appearing, given a sample in a dataset. To implement such a function, we calculate the statistical characteristics of the training dataset. In the first step, we calculate the mean values of each feature and store them in the one-dimensional matrix. Then, we calculate the covariance matrix for the training samples using the formula for the correlation matrix that was given in the prior theoretical section named <em class="italic">Density estimation approach</em>. Next, we determine the correlation matrix determinant and inverse version. We define a lambda function named <strong class="source-inline">prob</strong> to calculate the probability of a single sample using the formula provided in the <em class="italic">Using multivariate Gaussian </em><span class="No-Break"><em class="italic">distribution</em></span><span class="No-Break"> section.</span></p>
			<p>For large datasets, the computational complexity of calculating the covariance matrix can become a significant factor in the overall runtime of an ML model. Furthermore, optimizing the calculation of the covariance matrix for large datasets requires a combination of techniques, including sparsity, parallelization, approximation, and <span class="No-Break">efficient algorithms.</span></p>
			<p>We also define a probability threshold to separate anomalies. It determines the boundary between normal and anomalous behavior, and it plays a crucial role in the classification of samples as anomalies or normal. Engineers must carefully consider their application’s requirements and adjust the threshold accordingly to achieve the desired level of sensitivity. For example, in security applications where false alarms are costly, a higher threshold might be preferred to minimize false positives. In contrast, in medical diagnoses where missing a potential anomaly could have serious consequences, a lower threshold might be more appropriate to ensure that no true anomalies <span class="No-Break">go undetected.</span></p>
			<p>Then, we<a id="_idIndexMarker652"/> iterate over all the examples (including the training and testing datasets) to find out how the algorithm separates regular samples from anomalies. In the following graph, we can see the result of this separation. The dots labeled as <strong class="source-inline">1 cls</strong> <span class="No-Break">are anomalies:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer227">
					<img alt="Figure 5.10 – Anomaly detection w﻿ith Dlib multivariate Gaussian distribution" src="image/B19849_05_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Anomaly detection w<a id="_idTextAnchor294"/>ith Dlib multivariate Gaussian distribution</p>
			<p>We see that this approach found fewer anomalies than the previous one, so you should be aware that some methods will not work well with your data and it will make sense to try different methods. In the following section, we will see how to use a multivariate Gaussian model from the <strong class="source-inline">mlpack</strong> library for the <span class="No-Break">same task.</span></p>
			<h3>Multivariate Gaussian model with mlpack</h3>
			<p>We <a id="_idIndexMarker653"/>already discussed in the previous chapter the <strong class="source-inline">GMM</strong> and the <strong class="source-inline">EMFit</strong> classes that exist in the <strong class="source-inline">mlpack</strong> library. The <strong class="bold">Expectation-Maximization with Fit</strong> (<strong class="bold">EMFit</strong>) algorithm is an ML technique <a id="_idIndexMarker654"/>used to estimate the <a id="_idIndexMarker655"/>parameters of a <strong class="bold">gaussian mixture model</strong> (<strong class="bold">GMM</strong>). It works by iteratively optimizing the parameters to fit the data. We can use them not only for solving clustering tasks but also for anomaly detection. There will only the one difference: we have to specify only one cluster for the training. So, GMM class initialization will look like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
GMM gmm(/*gaussians*/ 1, /*dimensionality*/ 2);</pre>			<p>The initializations for the <strong class="source-inline">KMeans</strong> and the <strong class="source-inline">EMFit</strong> algorithms will be the same as in the <span class="No-Break">previous example:</span></p>
			<pre class="source-code">
KMeans&lt;&gt; kmeans;
size_t max_iterations = 250;
double tolerance = 1e-10;
EMFit&lt;KMeans&lt;&gt;, NoConstraint&gt; em(max_iterations,
                                 tolerance,
                                 kmeans);
gmm.Train(normal,
          /*trials*/ 3,
          /*use_existing_model*/ false,
          em);</pre>			<p>The values for <strong class="source-inline">max_iterations</strong> and the convergence tolerance variables influence the training process by determining how long the algorithm runs and when it stops. A higher number of trials may lead to more accurate results but also increase computational time. The convergence tolerance determines how close the parameters must be to their previous values before the algorithm stops. If the tolerance is too low, the algorithm may never converge, while if it is too high, it may converge to a <span class="No-Break">suboptimal solution.</span></p>
			<p>Then, we can use the <strong class="source-inline">Probability</strong> method of the <strong class="source-inline">gmm</strong> object to test some new data. The only new action we have to take is to define a probability threshold that will be used to check if a new sample belongs to the original data distribution or if it’s an anomaly. This can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
double prob_threshold = 0.001;</pre>			<p>Having this<a id="_idIndexMarker656"/> threshold, the usage of the <strong class="source-inline">Probability</strong> method will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
auto detect = [&amp;](const arma::mat&amp; samples) {
  for (size_t c = 0; c &lt; samples.n_cols; ++c) {
    auto sample = samples.col(c);
    double x = sample.at(0, 0);
    double y = sample.at(1, 0);
    auto p = gmm.Probability(sample);
    if (p &gt;= prob_threshold) {
      plot_clusters[0].first.push_back(x);
      plot_clusters[0].second.push_back(y);
    } else {
      plot_clusters[1].first.push_back(x);
      plot_clusters[1].second.push_back(y);
    }
  }
};</pre>			<p>Here, we defined a lambda function that can be applied to any dataset defined as a matrix. In this function, we have used a simple loop over all samples in which we applied the <strong class="source-inline">Probability</strong> method for a single sample and compared the returned value with the threshold. If the probability value is too low, we mark the sample as an anomaly by adding its coordinates to the <strong class="source-inline">plotting_cluster[1]</strong> object. To plot the anomaly detection result, we used the same approach as was described in the previous chapter. The following code shows how to use the function <span class="No-Break">we defined:</span></p>
			<pre class="source-code">
arma::mat normal;
arma::mat test;
Clusters plot_clusters;
detect(normal);
detect(test);
PlotClusters(plot_clusters, "Density Estimation Tree", file_name);</pre>			<p>We <a id="_idIndexMarker657"/>applied the <strong class="source-inline">detect</strong> function to both sets of data: the <strong class="source-inline">normal</strong> one that was used for the training and the new one, <strong class="source-inline">test</strong>. You can see the anomaly detection result in the <span class="No-Break">following graph:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer228">
					<img alt="Figure 5.11 – Anomaly detection with ﻿mlpack multivariate Gaussian distribution" src="image/B19849_05_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Anomaly detection with mlpack multivariate Gaussian distribution</p>
			<p>The two outliers were detected. You can try to change the probability threshold to see how the decision <a id="_idIndexMarker658"/>boundary will be changed and what objects will be classified <span class="No-Break">as anomalies.</span></p>
			<p>In the following section, we will see how to use the KDE algorithm implementation from the <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library.</span></p>
			<h3>KDE with mlpack</h3>
			<p>The <a id="_idIndexMarker659"/>KDE algorithm in the <strong class="source-inline">mlpack</strong> library is implemented in the <strong class="source-inline">KDE</strong> class. This class can be specialized with several template parameters; the most important ones are <strong class="source-inline">KernelType</strong>, <strong class="source-inline">MetricType</strong>, and <strong class="source-inline">TreeType</strong>. Let’s use the Gaussian kernel, the Euclidean distance as the metric, and KD-Tree for the tree type. A tree data structure is used to optimize the algorithm’s computational complexity. For each query point, the algorithm will apply a kernel function to each reference point, so the computational complexity can be <em class="italic">O(N^2)</em> in the naive implementation for <em class="italic">N</em> query points and <em class="italic">N</em> reference points. The tree optimization avoids many similar calculations, because kernel function values decrease with distance, but it also introduces some level of approximation. The following code snippet shows how to define a <strong class="source-inline">KDE</strong> object for <span class="No-Break">our sample:</span></p>
			<pre class="source-code">
using namespace mlpack;
...
KDE&lt;GaussianKernel,
    EuclideanDistance,
    arma::mat,
    KDTree&gt;
  kde(/*rel error*/ 0.0, /*abs error*/ 0.01, GaussianKernel());</pre>			<p>Due to the approximations used in the algorithm, the API allows us to define relative and absolute error tolerances. Relative and absolute error tolerances control the level of approximation in the KDE estimate. A higher tolerance allows for more approximation, which can reduce computational complexity but also decrease accuracy. Conversely, a lower tolerance requires more computation but can result in a more <span class="No-Break">accurate estimate.</span></p>
			<p>The <strong class="bold">relative error tolerance</strong> parameter <a id="_idIndexMarker660"/>specifies the maximum relative error allowed between the true density and the estimated density at any point. It is used to determine the optimal bandwidth for <span class="No-Break">the kernel.</span></p>
			<p>The <strong class="bold">absolute error tolerance</strong> parameter<a id="_idIndexMarker661"/> sets the maximum absolute error allowed between the true density and the estimated density over the entire domain. It can be used to<a id="_idIndexMarker662"/> ensure that the estimated density is within a certain range of the <span class="No-Break">true density.</span></p>
			<p>For our sample, we defined only the absolute one. The next step is to train our algorithm object with the normal data without anomalies; it can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::mat normal;
...
kde.Train(normal);</pre>			<p>You can see that different algorithms in the <strong class="source-inline">mlpack</strong> library use mostly the same API. Then, we <a id="_idIndexMarker663"/>can define a function to classify the given data as normal or as an anomaly. The following code shows <span class="No-Break">its definition:</span></p>
			<pre class="source-code">
double density_threshold = 0.1;
Clusters plot_clusters;
auto detect = [&amp;](const arma::mat&amp; samples) {
  arma::vec estimations;
  kde.Evaluate(samples, estimations);
  for (size_t c = 0; c &lt; samples.n_cols; ++c) {
    auto sample = samples.col(c);
    double x = sample.at(0, 0);
    double y = sample.at(1, 0);
    auto p = estimations.at(c);
    if (p &gt;= density_threshold) {
      plot_clusters[0].first.push_back(x);
      plot_clusters[0].second.push_back(y);
    } else {
      plot_clusters[1].first.push_back(x);
      plot_clusters[1].second.push_back(y);
    }
  }
};</pre>			<p>We defined a lambda function that takes the data matrix and passes it to the <strong class="source-inline">Evaluate</strong> method of the <strong class="source-inline">kde</strong> object. This method evaluated and assigned density value estimation for every sample in the given data matrix. Then, we just compared those estimations with the <strong class="source-inline">density_threshold</strong> value to decide if the sample was normal or an anomaly. Samples with low-density values were classified <span class="No-Break">as anomalies.</span></p>
			<p>To<a id="_idIndexMarker664"/> select an optimal threshold, you need to balance the trade-off between sensitivity and specificity based on your specific use case. If you prioritize detecting all anomalies, you may want to set a lower threshold to increase the number of true positives, even if it means accepting more false positives. Conversely, if you prioritize minimizing false alarms, you might choose a higher threshold, which could miss some anomalies but reduce the number of false positives. In practice, selecting an optimal density threshold often involves experimenting with different values and evaluating the results using metrics such as precision, recall, and F1 score. Additionally, domain knowledge and expert input can help guide the <span class="No-Break">selection process.</span></p>
			<p>This function also prepares data for plotting in the same manner as we did earlier. The following code shows how to plot two datasets with normal and <span class="No-Break">outlier data:</span></p>
			<pre class="source-code">
arma::mat normal;
arma::mat test;
detect(normal);
detect(test);
PlotClusters(plot_clusters, "Density Estimation Tree", file_name);</pre>			<p>You can see the anomaly detection result in the <span class="No-Break">following graph:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer229">
					<img alt="Figure 5.12 – Anomaly detection with KDE from ﻿mlpack" src="image/B19849_05_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Anomaly detection with KDE from mlpack</p>
			<p>We can see <a id="_idIndexMarker665"/>that using the KDE method, we can find the two outliers, as we detected in the previous sections. The points labeled with <strong class="source-inline">1 cls</strong> are outliers. By changing the density threshold, you can see how the decision boundary will <span class="No-Break">be changed.</span></p>
			<p>In the following section, we will see how to use the DET algorithm implementation from the <span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library.</span></p>
			<h3>DET with mlpack</h3>
			<p>The DET method<a id="_idIndexMarker666"/> from the <strong class="source-inline">mlpack</strong> library is implemented in the <strong class="source-inline">DTree</strong> class. To start working with it, we have to make a copy of the training normal data because an object of the <strong class="source-inline">DTree</strong> class will change the input data order. The data ordering is changed because <strong class="source-inline">mlpack</strong> creates a tree data structure directly on the given data object. The following code snippet shows how to define such <span class="No-Break">an object:</span></p>
			<pre class="source-code">
arma::mat data_copy = normal;
DTree&lt;&gt; det(data_copy);</pre>			<p>Due to the input data reordering the algorithms, the API is required to provide a mapping of the indices that will show the relation of new indices to the old ones. Such mapping can be initialized <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::Col&lt;size_t&gt; data_indices(data_copy.n_cols);
for (size_t i = 0; i &lt; data_copy.n_cols; i++) {
  data_indices[i] = i;
}</pre>			<p>Here, we just stored the original relation of the data indices to the input data. Later, this mapping will be updated by the algorithm. Now, can build the DET by calling the <strong class="source-inline">Grow</strong> method, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
size_t max_leaf_size = 5;
size_t min_leaf_size = 1;
det.Grow(data_copy, data_indices, false, max_leaf_size, 
  min_leaf_size);</pre>			<p>The two main parameters of the <strong class="source-inline">Grow</strong> method are <strong class="source-inline">max_leaf_size</strong> and <strong class="source-inline">min_leaf_size</strong>. They should be turned manually after a series of experiments or with some prior knowledge of the dataset characteristics. It can be tricky to estimate those parameters with automated techniques such as cross-validation because, for anomaly detection tasks, we don’t usually have enough data marked as anomalies. So, the values for this example were <span class="No-Break">chosen manually.</span></p>
			<p>Having an initialized DET, we can use the <strong class="source-inline">ComputeValue</strong> method to estimate a density for some given data sample. If we choose a density threshold value, we can detect anomalies just by comparison with this value. We used the same approach in other <a id="_idIndexMarker667"/>algorithms too. The following code snippet shows how to use a threshold to distinguish between normal and anomalous data and build a data structure for <span class="No-Break">result plotting:</span></p>
			<pre class="source-code">
double density_threshold = 0.01;
Clusters plot_clusters;
auto detect = [&amp;](const arma::mat&amp; samples) {
  for (size_t c = 0; c &lt; samples.n_cols; ++c) {
    auto sample = samples.col(c);
    double x = sample.at(0, 0);
    double y = sample.at(1, 0);
    auto p = det.ComputeValue(sample);
    if (p &gt;= density_threshold) {
      plot_clusters[0].first.push_back(x);
      plot_clusters[0].second.push_back(y);
    } else {
      plot_clusters[1].first.push_back(x);
      plot_clusters[1].second.push_back(y);
    }
  }
};</pre>			<p>We defined a <strong class="source-inline">detect</strong> function that simply iterates over the input data matrix columns and applies the <strong class="source-inline">ComputeValue</strong> method for every given sample to get the density estimate. Then, this function compares a value with <strong class="source-inline">density_threshold</strong>, and if the density is big enough, puts the sample in the first plotting cluster. Otherwise, the sample comes to<a id="_idIndexMarker668"/> the second plotting cluster. We can apply this function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
detect(normal);
detect(test);
PlotClusters(plot_clusters, "Density Estimation Tree", file_name);</pre>			<p>Here, <strong class="source-inline">normal</strong> and <strong class="source-inline">test</strong> are matrices with the normal and anomalous data samples, correspondingly. The following graph shows the detection <span class="No-Break">result plotting:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer230">
					<img alt="Figure 5.13 – Anomaly detection with DET algorithm" src="image/B19849_05_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Anomaly detection with DET algorithm</p>
			<p>You may notice that this method classified more data points as anomalies compared to previous methods. To change this detection result, you can play with three parameters: the density threshold and leaf <strong class="source-inline">min</strong> and <strong class="source-inline">max</strong> sizes. This method with more complex turning may <a id="_idIndexMarker669"/>be useful for datasets where you don’t know the data distribution rule (the kernel form) or it’s hard to write code for it. The same goes for when you have norma<a id="_idTextAnchor295"/>l<a id="_idTextAnchor296"/> data with several clusters with <span class="No-Break">different distributions.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor297"/>Summary</h1>
			<p>In this chapter, we examined anomalies in data. We discussed several approaches to anomaly detection and looked at two kinds of anomalies: outliers and novelties. We considered the fact that anomaly detection is primarily a UL problem, but despite this, some algorithms require labeled data, while others are semi-supervised. The reason for this is that, generally, there is a tiny number of positive examples (that is, anomalous samples) and a large number of negative examples (that is, standard samples) in anomaly <span class="No-Break">detection tasks.</span></p>
			<p>In other words, we don’t usually have enough positive samples to train algorithms. That is why some solutions use labeled data to improve algorithm generalization and precision. On the contrary, <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>) usually requires a large number of positive and negative examples, and their distribution needs to <span class="No-Break">be balanced.</span></p>
			<p>Also, notice that the task of detecting anomalies does not have a single formulation and that it is often interpreted differently, depending on the nature of the data and the goal of the concrete task. Moreover, choosing the correct anomaly detection method depends primarily on the task, data, and available a priori information. We also learned that different libraries can give slightly different results, even for the <span class="No-Break">same algorithms.</span></p>
			<p>In the following chapter, we will discuss dimension reduction methods. Such methods help us to reduce the dimensionality of data with high dimensionality into a new representation of data with lower dimensionality while<a id="_idTextAnchor298"/><a id="_idTextAnchor299"/> preserving essential information from the <span class="No-Break">original data.</span></p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor300"/>Further reading</h1>
			<ul>
				<li>Anomaly detection learning <span class="No-Break">resources: </span><a href="https://github.com/yzhao062/anomaly-detection-resources"><span class="No-Break">https://github.com/yzhao062/anomaly-detection-resources</span></a></li>
				<li><em class="italic">Outlier Detection with One-Class SVMs: An Application to Melanoma </em><span class="No-Break"><em class="italic">Prognosis</em></span><span class="No-Break">: </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/"><span class="No-Break">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041295/</span></a></li>
				<li>Isolation <span class="No-Break">forest: </span><a href="https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf"><span class="No-Break">https://feitonyliu.files.wordpress.com/2009/07/liu-iforest.pdf</span></a></li>
				<li>Ram, Parikshit &amp; Gray, Alexander. (2011). <em class="italic">Density estimation trees</em>. <em class="italic">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. 627-635. <span class="No-Break">10.1145/2020408.2020507: </span><a href="https://www.researchgate.net/publication/221654618_Density_estimation_trees"><span class="No-Break">https://www.researchgate.net/publication/221654618_Density_estimation_trees</span></a></li>
				<li>Tutorial on <span class="No-Break">KDE: </span><a href="https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf"><span class="No-Break">https://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf</span></a></li>
			</ul>
		</div>
	</body></html>