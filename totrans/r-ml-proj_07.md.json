["```py\ninstall.packages(\"h2o\")\n```", "```py\nlibrary(h2o)\n```", "```py\n# including the required libraries\nlibrary(tidyverse)\nlibrary(h2o)\nlibrary(rio)\nlibrary(doParallel)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(ggthemes)\nlibrary(knitr)\nlibrary(caret)\nlibrary(caretEnsemble)\nlibrary(plotly)\nlibrary(lime)\nlibrary(plotROC)\nlibrary(pROC)\n```", "```py\nlocalH2O = h2o.init(ip = 'localhost', port = 54321, nthreads = -1,max_mem_size = \"8G\")\n# Detecting the available number of cores\nno_cores <- detectCores() - 1\n# utilizing all available cores\ncl<-makeCluster(no_cores)\nregisterDoParallel(cl)\n```", "```py\nH2O is not running yet, starting it now...\nNote:  In case of errors look at the following log files:\n    /tmp/RtmpKZvQ3m/h2o_sunil_started_from_r.out\n    /tmp/RtmpKZvQ3m/h2o_sunil_started_from_r.err\njava version \"1.8.0_191\"\nJava(TM) SE Runtime Environment (build 1.8.0_191-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\nStarting H2O JVM and connecting: ..... Connection successful!\nR is connected to the H2O cluster:\n    H2O cluster uptime:         4 seconds 583 milliseconds\n    H2O cluster timezone:       Asia/Kolkata\n    H2O data parsing timezone:  UTC\n    H2O cluster version:        3.20.0.8\n    H2O cluster version age:    2 months and 27 days \n    H2O cluster name:           H2O_started_from_R_sunil_jgw200\n    H2O cluster total nodes:    1\n    H2O cluster total memory:   7.11 GB\n    H2O cluster total cores:    4\n    H2O cluster allowed cores:  4\n    H2O cluster healthy:        TRUE\n    H2O Connection ip:          localhost\n    H2O Connection port:        54321\n    H2O Connection proxy:       NA\n    H2O Internal Security:      FALSE\n    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n    R Version:                  R version 3.5.1 (2018-07-02)\n```", "```py\n# setting the working directory where the data file is location\nsetwd(\"/home/sunil/Desktop/book/chapter 7\")\n# loading the Rdata file and reading it into the dataframe called cc_fraud\ncc_fraud<-get(load(\"creditcard.Rdata\"))\n# performing basic EDA on the dataset\n# Viewing the dataframe to confirm successful load of the dataset\nView(cc_fraud)\n```", "```py\nprint(str(cc_fraud))\n```", "```py\n'data.frame':     284807 obs. of  31 variables:\n $ Time  : num  0 0 1 1 2 2 4 7 7 9 ...\n $ V1    : num  -1.36 1.192 -1.358 -0.966 -1.158 ...\n $ V2    : num  -0.0728 0.2662 -1.3402 -0.1852 0.8777 ...\n $ V3    : num  2.536 0.166 1.773 1.793 1.549 ...\n $ V4    : num  1.378 0.448 0.38 -0.863 0.403 ...\n $ V5    : num  -0.3383 0.06 -0.5032 -0.0103 -0.4072 ...\n $ V6    : num  0.4624 -0.0824 1.8005 1.2472 0.0959 ...\n $ V7    : num  0.2396 -0.0788 0.7915 0.2376 0.5929 ...\n $ V8    : num  0.0987 0.0851 0.2477 0.3774 -0.2705 ...\n $ V9    : num  0.364 -0.255 -1.515 -1.387 0.818 ...\n $ V10   : num  0.0908 -0.167 0.2076 -0.055 0.7531 ...\n $ V11   : num  -0.552 1.613 0.625 -0.226 -0.823 ...\n $ V12   : num  -0.6178 1.0652 0.0661 0.1782 0.5382 ...\n $ V13   : num  -0.991 0.489 0.717 0.508 1.346 ...\n $ V14   : num  -0.311 -0.144 -0.166 -0.288 -1.12 ...\n $ V15   : num  1.468 0.636 2.346 -0.631 0.175 ...\n $ V16   : num  -0.47 0.464 -2.89 -1.06 -0.451 ...\n $ V17   : num  0.208 -0.115 1.11 -0.684 -0.237 ...\n $ V18   : num  0.0258 -0.1834 -0.1214 1.9658 -0.0382 ...\n $ V19   : num  0.404 -0.146 -2.262 -1.233 0.803 ...\n $ V20   : num  0.2514 -0.0691 0.525 -0.208 0.4085 ...\n $ V21   : num  -0.01831 -0.22578 0.248 -0.1083 -0.00943 ...\n $ V22   : num  0.27784 -0.63867 0.77168 0.00527 0.79828 ...\n $ V23   : num  -0.11 0.101 0.909 -0.19 -0.137 ...\n $ V24   : num  0.0669 -0.3398 -0.6893 -1.1756 0.1413 ...\n $ V25   : num  0.129 0.167 -0.328 0.647 -0.206 ...\n $ V26   : num  -0.189 0.126 -0.139 -0.222 0.502 ...\n $ V27   : num  0.13356 -0.00898 -0.05535 0.06272 0.21942 ...\n $ V28   : num  -0.0211 0.0147 -0.0598 0.0615 0.2152 ...\n $ Amount: num  149.62 2.69 378.66 123.5 69.99 ...\n $ Class : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n```", "```py\nprint(table(cc_fraud$Class))\n```", "```py\n     0      1\n284315    492\n```", "```py\n# Printing the Histograms for Multivariate analysis\ntheme_set(theme_economist_white())\n# visualization showing the relationship between variable V1 and the class\nggplot(cc_fraud,aes(x=\"\",y=V1,fill=Class))+geom_boxplot()+labs(x=\"V1\",y=\"\")\n```", "```py\n# visualization showing the distribution of transaction amount with\n# respect to the class, it may be observed that the amount are discretized\n# into 50 bins for plotting purposes\nggplot(cc_fraud,aes(x = Amount)) + geom_histogram(color = \"#D53E4F\", fill = \"#D53E4F\", bins = 50) + facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\nggplot(cc_fraud, aes(x =Time,fill = Class))+ geom_histogram(bins = 30)+\n  facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\nggplot(cc_fraud, aes(x =V2, fill=Class))+ geom_histogram(bins = 30)+\n  facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\nggplot(cc_fraud, aes(x =V3, fill=Class))+ geom_histogram(bins = 30)+\n  facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\nggplot(cc_fraud, aes(x =V4,fill=Class))+ geom_histogram(bins = 30)+\n  facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\nggplot(cc_fraud, aes(x=V6, fill=Class)) + geom_density(alpha=1/3) + scale_fill_hue()\n```", "```py\nggplot(cc_fraud, aes(x=V7, fill=Class)) + geom_density(alpha=1/3) + scale_fill_hue()\n```", "```py\nggplot(cc_fraud, aes(x=V8, fill=Class)) + geom_density(alpha=1/3) + scale_fill_hue()\n```", "```py\n# visualizationshowing the V7 variable with respect to the class\nggplot(cc_fraud, aes(x=V9, fill=Class)) + geom_density(alpha=1/3) + scale_fill_hue()\n```", "```py\n# observe we are plotting the data quantiles\nggplot(cc_fraud, aes(x =\"\",y=V10, fill=Class))+ geom_violin(adjust = .5,draw_quantiles = c(0.25, 0.5, 0.75))+labs(x=\"V10\",y=\"\")\n```", "```py\ncc_fraud %>%\n  ggplot(aes(x = Class)) +\n  geom_bar(color = \"chocolate\", fill = \"chocolate\", width = 0.2) +\n  theme_bw()\n```", "```py\nprint(summary(cc_fraud$Time))\n```", "```py\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n      0   54202   84692   94814  139320  172792\n```", "```py\n# creating a new variable called day based on the seconds \n# represented in Time variable\n cc_fraud=cc_fraud %>% mutate(Day = case_when(.$Time > 3600 * 24 ~ \"day2\",.$Time < 3600 * 24 ~ \"day1\"))\n#visualizing the dataset post creating the new variable\nView(cc_fraud%>%head())\n```", "```py\nView(cc_fraud%>%tail())\n```", "```py\nprint(table(cc_fraud[,\"Day\"]))\n```", "```py\n  day1   day2\n144786 140020\n```", "```py\ncc_fraud$Time_day <- if_else(cc_fraud$Day == \"day2\", cc_fraud$Time - 86400, cc_fraud$Time)\nprint(tapply(cc_fraud$Time_day,cc_fraud$Day,summary,simplify = FALSE))\n```", "```py\n$day1\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n      0   38432   54689   52948   70976   86398\n\n$day2\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n      1   37843   53425   51705   68182   86392\n```", "```py\ncc_fraud<-cc_fraud%>%mutate_if(is.character,as.factor)\n```", "```py\ncc_fraud=cc_fraud %>% \n  mutate(Time_Group = case_when(.$Time_day <= 38138~ \"morning\" ,\n                                .$Time_day <= 52327~  \"afternoon\",\n                                .$Time_day <= 69580~\"evening\",\n                                .$Time_day > 69580~\"night\"))\n#Visualizing the data post creating the new variable\nView(head(cc_fraud))\n```", "```py\nView(tail(cc_fraud))\n```", "```py\n#visualizing the transaction count by day\ncc_fraud %>%drop_na()%>%\n  ggplot(aes(x = Day)) +\n  geom_bar(fill = \"chocolate\",width = 0.3,color=\"chocolate\") +\n  theme_economist_white()\n```", "```py\ncc_fraud$Class <- factor(cc_fraud$Class)\ncc_fraud %>%drop_na()%>%\n  ggplot(aes(x = Time_Group)) +\n  geom_bar(color = \"#238B45\", fill = \"#238B45\") +\n  theme_bw() +\n  facet_wrap( ~ Class, scales = \"free\", ncol = 2)\n```", "```py\n# getting the summary of amount with respect to the class\nprint(tapply(cc_fraud$Amount  ,cc_fraud$Class,summary))\n```", "```py\n$`0`\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.\n    0.00     5.65    22.00    88.29    77.05 25691.16\n$`1`\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n   0.00    1.00    9.25  122.21 105.89 2125.87\n```", "```py\n# converting R dataframe to H2O dataframe\ncc_fraud_h2o <- as.h2o(cc_fraud)\n#splitting the data into 60%, 20%, 20% chunks to use them as training,\n#vaidation and test datasets\nsplits <- h2o.splitFrame(cc_fraud_h2o,ratios = c(0.6, 0.2), seed = 148)  \n# creating new train, validation and test h2o dataframes\ntrain <- splits[[1]]\nvalidation <- splits[[2]]\ntest <- splits[[3]]\n# getting the target and features name in vectors\ntarget <- \"Class\"\nfeatures <- setdiff(colnames(train), target)\n```", "```py\nmodel_one = h2o.deeplearning(x = features, training_frame = train,\n                             AE = TRUE,\n                             reproducible = TRUE,\n                             seed = 148,\n                             hidden = c(10,10,10), epochs = 100,\n activation = \"Tanh\",\n                             validation_frame = test)\n```", "```py\n |===========================================================================================================================| 100%\n```", "```py\nh2o.saveModel(model_one, path=\"model_one\", force = TRUE)\nmodel_one<-h2o.loadModel(\"/home/sunil/model_one/DeepLearning_model_R_1544970545051_1\")\nprint(model_one)\n```", "```py\nModel Details:\n==============\nH2OAutoEncoderModel: deeplearning\nModel ID:  DeepLearning_model_R_1544970545051_1\nStatus of Neuron Layers: auto-encoder, gaussian distribution, Quadratic loss, 944 weights/biases, 20.1 KB, 2,739,472 training samples, mini-batch size 1\n  layer units  type dropout       l1       l2 mean_rate rate_rms momentum mean_weight weight_rms mean_bias bias_rms\n1     1    34 Input  0.00 %       NA       NA        NA       NA       NA          NA         NA        NA       NA\n2     2    10  Tanh  0.00 % 0.000000 0.000000  0.610547 0.305915 0.000000   -0.000347   0.309377 -0.028166 0.148318\n3     3    10  Tanh  0.00 % 0.000000 0.000000  0.181705 0.103598 0.000000    0.022774   0.262611 -0.056455 0.099918\n4     4    10  Tanh  0.00 % 0.000000 0.000000  0.133090 0.079663 0.000000    0.000808   0.337259  0.032588 0.101952\n5     5    34  Tanh      NA 0.000000 0.000000  0.116252 0.129859 0.000000    0.006941   0.357547  0.167973 0.688510\nH2OAutoEncoderMetrics: deeplearning\n Reported on training data. Training Set Metrics:\n=====================\nMSE: (Extract with `h2o.mse`) 0.0003654009\nRMSE: (Extract with `h2o.rmse`) 0.01911546\nH2OAutoEncoderMetrics: deeplearning\n Reported on validation data. Validation Set Metrics:\n=====================\nMSE: (Extract with `h2o.mse`) 0.0003508435\nRMSE: (Extract with `h2o.rmse`) 0.01873082\n```", "```py\ntest_autoencoder <- h2o.predict(model_one, test)\n```", "```py\n|===========================================================================================================================| 100%\n```", "```py\ntrain_features <- h2o.deepfeatures(model_one, train, layer = 2) %>%\n  as.data.frame() %>%\n  mutate(Class = as.vector(train[, 31]))\n# printing the reduced data represented in layer2\nprint(train_features%>%head(3))\n```", "```py\nDF.L2.C1  DF.L2.C2     DF.L2.C3    DF.L2.C4   DF.L2.C5 \n-0.12899115 0.1312075  0.115971952 -0.12997648 0.23081912\n-0.10437942 0.1832959  0.006427409 -0.08018725 0.05575977\n-0.07135827 0.1705700 -0.023808057 -0.11383244 0.10800857\nDF.L2.C6   DF.L2.C7    DF.L2.C8  DF.L2.C9  DF.L2.C10  Class0.1791547 0.10325721  0.05589069 0.5607497 -0.9038150     0\n0.1588236 0.11009450 -0.04071038 0.5895413 -0.8949729     0\n0.1676358 0.10703990 -0.03263755 0.5762191 -0.8989759     0\n```", "```py\nggplot(train_features, aes(x = DF.L2.C1, y = DF.L2.C2, color = Class)) +\n  geom_point(alpha = 0.1,size=1.5)+theme_bw()+\n  scale_fill_brewer(palette = \"Accent\")\n```", "```py\nggplot(train_features, aes(x = DF.L2.C3, y = DF.L2.C4, color = Class)) +\n  geom_point(alpha = 0.1,size=1.5)+theme_bw()+\n  scale_fill_brewer(palette = \"Accent\")\n```", "```py\n# let's consider the third hidden layer. This is again a random choice\n# in fact we could have taken any layer among the 10 inner layers\ntrain_features <- h2o.deepfeatures(model_one, validation, layer = 3) %>%\n  as.data.frame() %>%\n  mutate(Class = as.factor(as.vector(validation[, 31]))) %>%\n  as.h2o()\n```", "```py\n|===========================================================================================================================| 100% |===========================================================================================================================| 100%\n```", "```py\nfeatures_two <- setdiff(colnames(train_features), target)\n```", "```py\nmodel_two <- h2o.deeplearning(y = target,\n                              x = features_two,\n                              training_frame = train_features,\n                              reproducible = TRUE,\n                              balance_classes = TRUE,\n                              ignore_const_cols = FALSE,\n                              seed = 148,\n                              hidden = c(10, 5, 10),\n                              epochs = 100,\n                              activation = \"Tanh\")\n```", "```py\nh2o.saveModel(model_two, path=\"model_two\", force = TRUE)\nmodel_two <- h2o.loadModel(\"/home/sunil/model_two/DeepLearning_model_R_1544970545051_2\")\nprint(model_two)\n```", "```py\nModel Details:\n==============\nH2OBinomialModel: deeplearning\nModel ID:  DeepLearning_model_R_1544970545051_2\nStatus of Neuron Layers: predicting Class, 2-class classification, bernoulli distribution, CrossEntropy loss, 247 weights/biases, 8.0 KB, 2,383,962 training samples, mini-batch size 1\n  layer units    type dropout       l1       l2 mean_rate rate_rms momentum mean_weight weight_rms mean_bias bias_rms\n1     1    10   Input  0.00 %       NA       NA        NA       NA       NA          NA         NA        NA       NA\n2     2    10    Tanh  0.00 % 0.000000 0.000000  0.001515 0.001883 0.000000   -0.149216   0.768610 -0.038682 0.891455\n3     3     5    Tanh  0.00 % 0.000000 0.000000  0.003293 0.004916 0.000000   -0.251950   0.885017 -0.307971 0.531144\n4     4    10    Tanh  0.00 % 0.000000 0.000000  0.002252 0.001780 0.000000    0.073398   1.217405 -0.354956 0.887678\n5     5     2 Softmax      NA 0.000000 0.000000  0.007459 0.007915 0.000000   -0.095975   3.579932  0.223286 1.172508\nH2OBinomialMetrics: deeplearning\n Reported on training data.\n  Metrics reported on temporary training frame with 9892 samples MSE:  0.1129424\nRMSE:  0.336069\nLogLoss:  0.336795\nMean Per-Class Error:  0.006234916\nAUC:  0.9983688\nGini:  0.9967377\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n          0    1    Error      Rate\n0      4910   62 0.012470  =62/4972\n1         0 4920 0.000000   =0/4920\nTotals 4910 4982 0.006268  =62/9892\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold    value idx\n1                       max f1  0.009908 0.993739 153\n2                       max f2  0.009908 0.997486 153\n3                 max f0point5  0.019214 0.990107 142\n4                 max accuracy  0.009908 0.993732 153\n5                max precision  1.000000 1.000000   0\n6                   max recall  0.009908 1.000000 153\n7              max specificity  1.000000 1.000000   0\n8             max absolute_mcc  0.009908 0.987543 153\n9   max min_per_class_accuracy  0.019214 0.989541 142\n10 max mean_per_class_accuracy  0.009908 0.993765 153\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)\n```", "```py\ntest_3 <- h2o.deepfeatures(model_one, test, layer = 3)\nprint(test_3%>%head())\n```", "```py\n|===========================================================================================================================| 100%\n```", "```py\ntest_pred=h2o.predict(model_two, test_3,type=\"response\")%>%\n  as.data.frame() %>%\n  mutate(actual = as.vector(test[, 31]))\n```", "```py\n|===========================================================================================================================| 100%\n```", "```py\ntest_pred%>%head()\n  predict        p0           p1 actual\n1       0 1.0000000 1.468655e-23      0\n2       0 1.0000000 2.354664e-23      0\n3       0 1.0000000 5.987218e-09      0\n4       0 1.0000000 2.888583e-23      0\n5       0 0.9999988 1.226122e-06      0\n6       0 1.0000000 2.927614e-23      0\n# summarizing the predictions\nprint(h2o.predict(model_two, test_3) %>%\n  as.data.frame() %>%\n  dplyr::mutate(actual = as.vector(test[, 31])) %>%\n  group_by(actual, predict) %>%\n  dplyr::summarise(n = n()) %>%\n  mutate(freq = n / sum(n)))\n```", "```py\n|===========================================================================================================================| 100%\n# A tibble: 4 x 4\n# Groups:   actual [2]\n  actual predict     n   freq\n  <chr>  <fct>   <int>  <dbl>\n1 0      0       55811 0.986\n2 0      1         817 0.0144\n3 1      0          41 0.414\n4 1      1          58 0.586\n```", "```py\nh2o.shutdown()\n```"]