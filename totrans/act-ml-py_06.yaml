- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Active Learning to Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will dive into using active learning techniques for computer
    vision tasks. Computer vision involves analyzing visual data such as images and
    videos to extract useful information. It relies heavily on machine learning models
    such as convolutional neural networks. However, these models require large labeled
    training sets, which can be expensive and time-consuming to obtain. Active ML
    provides a solution by interactively querying the user to label only the most
    informative examples. This chapter demonstrates how to implement uncertainty sampling
    for diverse computer vision tasks. By the end, you will have the tools to efficiently
    train computer vision models with optimized labeling effort. The active ML methods
    presented open up new possibilities for building robust vision systems with fewer
    data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing active ML for an image classification project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying active ML to an object detection project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using active ML for an instance segmentation project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will need to install the Ultralytics, PyYAML, and Roboflow
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: Ultralytics is a popular open source Python library for building high-performance
    computer vision and deep learning models. It provides implementations of state-of-the-art
    object detection and image segmentation models including YOLO that can be trained
    on custom datasets.
  prefs: []
  type: TYPE_NORMAL
- en: PyYAML is a Python library used for reading and writing YAML files. YAML is
    a human-readable data serialization format. PyYAML allows loading YAML data from
    files or strings into Python data types such as dictionaries and lists. It can
    also dump Python objects back into YAML strings.
  prefs: []
  type: TYPE_NORMAL
- en: Roboflow, as presented in earlier chapters, is a platform that helps with preparing
    and managing datasets for computer vision models. It provides tools to annotate
    images, create training/test splits, and export labeled datasets in formats that
    are usable by deep learning frameworks such as PyTorch. Roboflow also integrates
    with libraries such as Ultralytics to streamline training pipelines. The main
    goal is to simplify the dataset management aspects of developing CV models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install these packages, we can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you will need a Roboflow account in order to get a Roboflow API
    key. You can create an account here: [https://app.roboflow.com/](https://app.roboflow.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing active ML for an image classification project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will guide you through the implementation of active ML techniques
    for an image classification project. Image classification has various applications
    in computer vision, ranging from identifying products for an e-commerce website
    to detecting patterns of deforestation on geospatial tiles. However, creating
    accurate image classifiers requires extensive datasets of labeled images, which
    can be expensive and time-consuming to gather, as mentioned in [*Chapter 1*](B21789_01.xhtml#_idTextAnchor015),
    *Introducing Active Machine Learning*. Active ML offers a solution to this labeling
    bottleneck by interactively requesting the oracle to label only the most informative
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: We will build an image classification model that will be capable of accurately
    classifying various images obtained from the CIFAR-10 dataset. This dataset is
    widely recognized in the field of computer vision and contains a diverse collection
    of 60,000 images, each belonging to one of 10 different classes. We will start
    with a small *labeled* set of only 2,000 images from CIFAR-10, then employ an
    active ML strategy to select the best images to present to an oracle for labeling.
    Specifically, we will use uncertainty sampling to query the examples the model
    is least certain about. We use uncertainty sampling here as it is simpler and
    less computationally expensive than other methods we have discussed previously.
    For instance, query-by-committee requires training multiple models, which is computationally
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: As more labels are acquired, model accuracy improves with fewer training examples.
    This demonstrates how active learning can create high-performing computer vision
    models with significantly lower data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN for the CIFAR dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation will cover initializing a **convolutional neural network**
    (**CNN**) classifier, training our model with a small labeled set, selecting unlabeled
    images for the next labeling step using active ML, acquiring new labels, retraining
    the model, and tracking model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Quick reminder
  prefs: []
  type: TYPE_NORMAL
- en: A CNN classifier takes an image as input, extracts feature maps using convolutions,
    integrates the features in fully connected layers, and outputs predicted class
    probabilities based on what it learned during training. The convolutions allow
    it to automatically learn relevant visual patterns, making CNNs very effective
    for image classification tasks. You can find the PyTorch official tutorial on
    building a neural network model at [https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a simple image classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we use this model because it is a small CNN that runs quickly and
    efficiently. This is helpful for running simple proofs of concept. However, next,
    we could use one of the pretrained models (such as `torchvision`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find all the `torchvision` pretrained models on the library’s model
    page: [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we load the CIFAR-10 dataset with the appropriate transform function.
    The transform function defines a series of data processing and augmentation operations
    that are automatically applied when fetching samples from a PyTorch dataset. In
    the following code, we convert the images to tensors and normalize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This print shows us that the length of the full training dataset is 50,000 images.
    We are using the CIFAR-10 train dataset because we set the Boolean value of `train=True`.
    Later on, we will use the test set from CIFAR-10 and will then set `train=False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create a small dataset of 2,000 labeled images. The purpose here
    is to simulate the existence of a small labeled set of images, while the remaining
    images are unlabeled. Our objective is to identify and select the most informative
    images for labeling next with active ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have created a small labeled dataset and now need to initialize our
    training PyTorch data loader. A **PyTorch data loader** is used to load and iterate
    over datasets for training neural networks. It takes the dataset that contains
    the actual images and labels and is responsible for batching up these samples
    and feeding them to the model. The data loader allows you to specify a batch size,
    which determines how many samples are batched together – this is usually set to
    something like 64 or 128\. Additionally, the data loader will shuffle the data
    by default if you are using it for a training set. This randomization of the order
    of samples helps the model generalize better during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to initialize our model. We know that CIFAR-10 has 10 classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A good practice is to visualize the data with which we are working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4**.1* depicts a sample of CIFAR-10 dataset images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A random visualization of some CIFAR-10 dataset images](img/B21789_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A random visualization of some CIFAR-10 dataset images
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also good to take a look at the labels, so let’s print the first five
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the following list of labels as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see that this is correct when cross-referencing these labels with the
    first five images in *Figure 4**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: What is unnormalizing?
  prefs: []
  type: TYPE_NORMAL
- en: '**Unnormalizing** an image means reversing any normalization that was previously
    applied to the image pixel values in order to restore the original pixel value
    distribution (from the 0–1 range to the original 0–255 range).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our data loader, we can start the training loop; we first
    define our loss and optimizer. The **loss function** measures how well the model’s
    predictions match the true labels for a batch of images. It calculates the error
    between the predicted and true labels. Common loss functions for classification
    include cross-entropy loss and negative log-likelihood loss. These loss functions
    will output a high number if the model predicts incorrect labels, and a low number
    if the predictions are accurate. The goal during training is to minimize the loss
    by updating the model parameters. A good resource for learning about the loss
    functions available for use in PyTorch can be found here: [https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **optimizer** is responsible for this parameter updating. It uses the loss
    value to perform backpropagation and update the model’s weights and biases to
    reduce the loss. **Stochastic gradient descent** (**SGD**) is a popular optimization
    algorithm, where the parameters are updated proportionally to the gradient of
    the loss function. The learning rate controls the size of the updates. Other optimizers
    such as **Adam** and **RMSProp** are also commonly used for deep learning models
    (to learn about the optimizer functions available for use in PyTorch, you can
    visit this link: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will train our model for 100 epochs. Epochs represent the number of passes
    through the full training dataset during the training of the model. We define
    a `train` function as follows to run our training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now an initial trained model on our small dataset and we want to use
    it to select the next images to label. But first, let’s evaluate this model on
    the CIFAR-10 test set. We define an evaluation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use this function with our trained model once we define our test
    set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The test set’s length is 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use our evaluation function with this test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: So we have now tested our first trained model, which was trained on the 2,000
    images of our initial small labeled set. The model’s accuracy on the test set
    is 40.08%. We aim to improve this accuracy by labeling more images. This is where
    our active ML selection strategy comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Applying uncertainty sampling to improve classification performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will choose the most informative images to label next from our dataset –
    namely, the frames where the **model is least confident**, a method discussed
    in [*Chapter 2*](B21789_02.xhtml#_idTextAnchor027), *Designing Query* *Strategy
    Frameworks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define a function to get the model’s uncertainty scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define our data loader for the unlabeled set. We will use a batch
    size of 1 as we will loop through all the images to get the uncertainty scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We collect the confidence scores for our set of **unlabeled** images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'These values represent the **least confidence scores** of the model’s predictions.
    The higher the scores, the less confident the model is. Therefore, next, we want
    to know the indices of the images where the scores are highest. We decide that
    we want to select 200 images (queries):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sort by uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the original indices of the most uncertain samples and print the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the indices of the images selected using our active ML least-confident
    strategy. These are the images that would be sent to our oracles to be labeled
    and then used to train the model again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at five of these selected images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.2 – Five of the chosen images to be labeled next](img/B21789_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Five of the chosen images to be labeled next
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the images that now need to be labeled. Since this is a demo, we already
    have the labels, so let’s retrain our model with these newly labeled images. First,
    we need to add those to our labeled set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This returns 2,200, which is correct because we first selected 2,000 images
    from our dataset and then queried 200 with our active ML sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start our training from scratch again for 100 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the evaluation on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We have improved the accuracy on the test set from 40.08% to 41.54% by adding
    images selected with our active ML strategy to the training dataset. We could
    also fine-tune the model that was originally trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We have an interesting result here: the fine-tuned model is performing worse
    than the model trained from scratch using the larger dataset. Overall, the model’s
    performance improves when the selected images chosen by the active ML are added.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be applied to real-world problems. It is important to note,
    however, that this is a basic demonstration of how to use the least confident
    sampling method for classification. In a real project, you would need to have
    oracles label the selected images. Additionally, you would likely need to query
    more than 200 images and use a larger pretrained model, as mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: While the previous example demonstrated active ML for image classification,
    the same principles can be applied to other computer vision tasks such as object
    detection, as we’ll see next.
  prefs: []
  type: TYPE_NORMAL
- en: Applying active ML to an object detection project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will guide you through the implementation of active ML techniques
    for an object detection project. An object detection project refers to developing
    a computer vision model to detect and localize objects within images or videos.
    The dataset is a collection of images (video frames) containing examples of the
    objects you want to detect, among other things. The dataset needs to have labels
    in the form of bounding boxes around the objects. Popular datasets for this purpose
    include **COCO** ([https://cocodataset.org/](https://cocodataset.org/)), **PASCAL
    VOC** ([http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)),
    and **OpenImages** ([https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)).
    The model architecture uses a neural network designed for object detection such
    as Faster R-CNN, YOLO, and so on. This type of architecture can automatically
    identify and localize real-world objects within visual data. The end result is
    a model that can detect and draw boxes around objects such as cars, people, furniture,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The object detection project faces the same problem as classification projects:
    creating datasets is difficult and time-consuming. In fact, it is even more challenging
    for object detection tasks because it involves manually drawing bounding boxes
    around the objects. Once again, active ML provides a solution to this labeling
    bottleneck by sending the most informative images to the oracles for labeling.
    We will build an object detection model that will be capable of localizing brain
    tumors. This dataset we will use is from Roboflow Universe ([https://universe.roboflow.com/](https://universe.roboflow.com/))
    and is called *Brain Tumor Computer Vision Project*. To download this dataset,
    we use the Roboflow API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This downloads the dataset locally. The dataset is downloaded as a folder with
    the structure shown in *Figure 4**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp](img/B21789_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and training our model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we need to fix the `data.yaml` file to work properly in Google Colab
    and organize the data for our active ML demo. The `data.yaml` file is used in
    the `ultralytics` training to specify where the different sets (`train`, `valid`,
    and `test`) are placed. We assume here that the original training set is our unlabeled
    set of images, the validation set is our testing data, and the test set is our
    labeled data because it has the fewest examples. So, first, we define a function
    to rename the folders accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.4 – Structure of the dataset after renaming the subfolders for our
    demo](img/B21789_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Structure of the dataset after renaming the subfolders for our
    demo
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.4* shows the structure that we now have in our brain tumor dataset
    after renaming the folders for our demo. We then modify the `data.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here, we renamed the subfolder paths in the `data.yaml` file, which is the file
    that we will use for our training. We do not want to use the `val` folder for
    now in our training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take a look at our subfolders to determine the number of images in
    one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now begin our initial training on our `labeled` dataset. For this training,
    we will utilize a widely used Python computer vision package called `ultralytics`
    ([https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics))
    and employ the `yolov8` model. The `yolov8` model is capable of performing tasks
    such as detection and tracking, instance segmentation, image classification, and
    pose estimation. We will train our model for 10 epochs only for our demo purposes.
    We use the `detect` task type because we want to train the model for object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the training is done, we evaluate it on the test set. Here is how we can
    evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s analyze these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`precision(B)` measures how many of the predicted bounding boxes are correct.
    A value of 0.60 means 60% of the predicted boxes match the ground truth boxes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall(B)` measures how many of the ground truth boxes were correctly detected.
    A value of 0.48 means the model detected 48% of the true boxes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mAP50(B)` is the mean average precision at the **intersection over union**
    (**IoU**) threshold of 50%, which measures the model’s precision across different
    confidence thresholds. A prediction is considered correct if the IoU with ground
    truth is at least 50%. A value of 0.50 means the model has 50% mAP at this IoU
    threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mAP50-95(B)` is the mean average precision at IoU thresholds between 50% and
    95% and is a more strict metric that expects higher overlap with the ground truth
    to be considered correct. The lower value of 0.23 indicates performance drops
    at higher IoU thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fitness` combines precision and recall. A model that scores high on precision
    but low on recall would have poor fitness. Similarly, high recall but low precision
    also results in low fitness. A high fitness score requires strong performance
    on both precision and recall metrics. This encourages the model to improve both
    the accuracy and completeness of detections during training. The specific fitness
    value of 0.25 here indicates there is significant room for improvement in precision,
    recall, or both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metrics indicate reasonably good precision but lower recall, meaning the
    model struggles to detect all ground truth boxes. The high precision but lower
    mAP shows many detections are offset from the ground truth. Overall, the metrics
    show room for improvement in the alignment and completeness of detections.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is thus to select the most informative images to label using our
    active ML approach.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an active ML strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `ultralytics` package, which is highly useful for enabling the
    selection of informative images. This can help us improve the metrics we just
    discussed. This package provides the confidence score for each bounding box prediction,
    which we will utilize here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the model to each image in the unlabeled set using a confidence threshold
    of `0.15`. This means that any predictions with a confidence score below 0.15
    will be discarded. You can choose this value based on your specific needs and
    use case. It is important to keep in mind that choosing a low confidence score
    threshold allows for the selection of images where the model lacks confidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at some of these images and the predicted bounding boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.5 – Samples of model predictions on images from the unlabeled set](img/B21789_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Samples of model predictions on images from the unlabeled set
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 4**.5* that the model is detecting tumors in the unlabeled
    brain images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s collect all the confidence scores of the predicted bounding boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll only keep the minimum confidence value for each image. If there is no
    predicted bounding box, we add a confidence score of `10` (a high value to put
    these images at the end of the list of potential images). Confidence scores are
    values ranging from 0 to 1, with 1 being high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We have 6,930 confidence scores, which is correct because we have 6,930 unlabeled
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we select 500 images where the confidence scores are the lowest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We now get the selected images with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We move these selected images (and their corresponding label files) to our
    labeled set – this mimics the step where we would have an oracle label these images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check that we have correctly moved the images and label files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: These two `print` commands both return 1,490, which is what we expected because
    we had 990 labeled images and then added 500 new image/label pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train our model again with this updated dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we evaluate this model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we get the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Comparing these metrics with the previous metrics we got, we can see that the
    precision improved from 0.60 to 0.65, the recall from 0.48 to 0.51, the mAP50
    from 0.50 to 0.54, the mAP50-95 from 0.22 to 0.27, and the fitness from 0.25 to
    0.29\. So, adding the 500 most informative images to our labeled set improved
    our metrics across the board.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a similar method for instance segmentation, which we will cover in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using active ML for a segmentation project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will reuse what we did for the object detection task, but
    instead of using an object detection dataset, we will use an instance segmentation
    dataset with the `segment` task of `yolov8`.
  prefs: []
  type: TYPE_NORMAL
- en: '`person`, `car`, and so on. Instance segmentation separates individual instances
    of those classes – person 1 versus person 2, or car 1 versus car 2.'
  prefs: []
  type: TYPE_NORMAL
- en: The combination of localization, classification, and separation of instances
    enables precise analysis of images down to the pixel level. This makes instance
    segmentation useful for applications such as autonomous driving, medical imaging,
    and robotics, where understanding scenes at a fine-grained level is required.
    Some popular instance segmentation algorithms and models are **Mask R-CNN** ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)),
    **Panoptic FPN** ([https://arxiv.org/abs/1901.02446](https://arxiv.org/abs/1901.02446)),
    and **YOLACT** ([https://arxiv.org/abs/1904.02689](https://arxiv.org/abs/1904.02689)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s download the `strawberry` dataset from Roboflow Universe
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Then, we work through the same steps as we followed for the object detection
    dataset in the preceding section. We rename the subfolders for our demo use case
    and update the YAML file. We end up with the structure shown in *Figure 4**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders
    for our demo](img/B21789_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders
    for our demo
  prefs: []
  type: TYPE_NORMAL
- en: 'For this dataset, after renaming the folders and updating the YAML file, the
    code returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are now training for instance segmentation, we update the training code
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is completed, we evaluate the model using the same code as
    in the previous project and obtain the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The metrics with `(B)` represent the metrics for object detection, while the
    metrics with `(M)` refer to instance segmentation, where `M` stands for masks.
    The metrics are the same between the two types; the only difference is that the
    `M` metrics are computed on the pixels from the masks rather than those from the
    bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Following the same logic, we then select the images to label in order to improve
    our metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is slightly different this time when we run the model on each image
    in the unlabeled set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We have to specify that we are performing a segmentation task and choose a higher
    confidence threshold to avoid memory issues in Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the model’s predictions on some of the unlabeled images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This returns the image depicted in *Figure 4**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Samples of model predictions on images from the unlabeled set](img/B21789_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Samples of model predictions on images from the unlabeled set
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.7*, we can see that the model is correctly detecting most of
    the strawberries. The object detection is represented by the green bounding boxes
    in the images, while the segmentation is indicated by the white overlaying masks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then follow the steps discussed in the preceding section on object detection,
    where we selected the 500 images to label next, and we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We move these images to our labeled set, and thus go from 184 images in the
    labeled set to 684\. We run the training on the updated labeled set, followed
    by the evaluation, and obtain these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s compare those to the metrics we had before the addition of the 500 most
    informative images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that all metrics have improved.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, this chapter has demonstrated how active ML can be applied to
    optimize the training of computer vision models. As we have seen, computer vision
    tasks such as image classification, object detection, and instance segmentation
    require large labeled datasets to train **convolutional neural networks** (**CNNs**).
    Manually collecting and labeling this much data is expensive and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Active ML provides a solution to this challenge by intelligently selecting the
    most informative examples to be labeled by a human oracle. Strategies such as
    uncertainty sampling query the model to find the data points it is least certain
    about. By labeling only these useful data points, we can train our models with
    significantly less data-labeling effort required.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered implementing active ML approach for diverse computer
    vision applications. By interactively querying the model and refining the training
    data, we can rapidly improve model performance at a fraction of the labeling cost.
    These techniques make it feasible to develop computer vision systems even with
    limited data.
  prefs: []
  type: TYPE_NORMAL
- en: The active ML implementations presented offer new possibilities for building
    performant and robust computer vision models without needing massive datasets.
    With these strategies, you can optimize and target the data collection and training
    efforts for efficient results. Going forward, active ML will become an essential
    tool for developing real-world computer vision systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to leverage active ML for big data
    projects that use large amounts of data, such as videos.
  prefs: []
  type: TYPE_NORMAL
