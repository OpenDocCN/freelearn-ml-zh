- en: '*Chapter 2*: Architectures for Streaming and Real-Time Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming architectures are an essential component of solutions for real-time
    machine learning and streaming analytics. Even if you have a model or other analytics
    tools that can treat data in real time, update, and respond straight away, this
    will be of no use if there is no architecture to support your solution.
  prefs: []
  type: TYPE_NORMAL
- en: The first important consideration is making sure that your models and analytics
    can function on each data point; there needs to be an update function and/or a
    predict function that can update the solution on each new observation being received
    by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important consideration for real-time and streaming architectures is
    data ingress: how to make sure that data can be received on an observation per
    observation basis, rather than the more traditional batch approach with daily
    database updates, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, it will be important that you understand how to make different
    software systems communicate. For example, data has to flow very fast from your
    data generating process, maybe go through a data storage solution, a data quality
    tool, or a security layer, and then be received by your analytics program. The
    analytics program will do its work and send the result back to the source, or
    maybe forward the treated data points to a visualization solution, an alerting
    system, or similar.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will get an introduction to architectures for streaming
    and real-time machine learning. The central focus of this book will remain on
    the analytics and machine learning part of the pipeline. The goal of this chapter
    is to give you enough elements to imagine and implement rough working architectures,
    while some of the highly-specialized parts on performance, availability, and security
    will be left out.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining your analytics as a function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding microservices architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Communicating between services through APIs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demystifying the HTTP protocol
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a simple API on AWS
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Big data tools for real-time streaming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calling a big data environment in real time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link of the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the green **Code** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Download ZIP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you download the ZIP file, you unzip it in your local environment, and
    you will be able to access the code through your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: Python environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with Python environments, I would advise you to
    check out Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with the Jupyter Notebook and JupyterLab, which are both great for
    executing notebooks. It also comes with Spyder and VSCode for editing scripts
    and programs.
  prefs: []
  type: TYPE_NORMAL
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup to do.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code in the book will generally use Colab and Kaggle Notebooks with Python
    version 3.7.13, and you can set up your own environment to mimic this.
  prefs: []
  type: TYPE_NORMAL
- en: Defining your analytics as a function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to get started with architecture, let's build an idea from the ground
    up using the different building blocks that are necessary to make this a minimal
    working product.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that you need to have for this is an understanding of the type
    of real-time analytics that you want to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s go with the same example as in the previous chapter: a real-time
    business rule that prints an alert when the temperature or acidity of our production
    line is out of the acceptable limits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, this alert was coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the previous chapter, you used an iteration over a DataFrame to test out
    this code. In reality, you will always need an idea of architecture so that you
    can make your code actually receive data in real time from a data generating process.
    This building block will be covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following schematic drawing, you see a high-level architectural schema
    for our streaming solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A high-level architectural schema for a streaming solution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – A high-level architectural schema for a streaming solution
  prefs: []
  type: TYPE_NORMAL
- en: In this schematic drawing, you clearly see that writing code will give you some
    of the key components of your solution. However, you need to build an architecture
    around this to make the solution come to life. The darker pieces are still missing
    from the example implementation.
  prefs: []
  type: TYPE_NORMAL
- en: While the goal of this book is not to give a full in-depth course on architecture,
    you will discover some tools and building blocks here that will allow you to deliver
    an MVP real-time use case. To get your building blocks cleanly organized, you
    will need to choose an architectural structure for your solutions. Microservices
    are an architectural pattern that will allow you to build clean, small building
    blocks and have them communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding microservices architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of **microservices** is important to understand when working on
    architectures. Although there are other ways to architecture software projects,
    microservices are quite popular for a good reason. They help teams be flexible
    and effective, and help to keep software flexible and clearly structured.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind microservices is in the name: software is represented as many
    small services that operate individually. When looking at the overall architecture,
    each of the microservices is inside a small, *black box* with clearly defined
    inputs and outputs. Processes are put in place to call the right black box at
    the right time.'
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architecture is loosely coupled. This means that there is no fixed
    communication between the different microservices. Instead, each microservice
    can be called, or not called, by any other services or code.
  prefs: []
  type: TYPE_NORMAL
- en: If a change needs to be made to one of the microservices, the scope of the change
    is fairly local, thereby not affecting other microservices. As input and output
    are predefined, this also helps in keeping the foundational structure of the program
    in order, without it being fixed in any way.
  prefs: []
  type: TYPE_NORMAL
- en: To allow different microservices to communicate, an often-chosen solution is
    to use **Application Programming Interfaces** (**APIs**). Let's deep dive into
    those now.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating between services through APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A central component in microservice architectures is the use of APIs. An API
    is a part that allows you to connect two microservices (or other pieces of code)
    together.
  prefs: []
  type: TYPE_NORMAL
- en: APIs are much like websites. Just like a website, an API is built behind a website-like
    link or an IP address. When you go to a website, the server of the website sends
    you the code that represents the website. Your internet browser then interprets
    this code and shows you a web page.
  prefs: []
  type: TYPE_NORMAL
- en: When you call an API, the API will receive your request. The request triggers
    your code to be run on the server and generates a response that is sent back to
    you. If something goes wrong (maybe your request was not as expected or an error
    occurs), you may not receive any response, or receive an error code such as `request
    not authorized` or `internal server error`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a flow chart that covers this. A computer or user
    sends an HTTP request, and the API server sends back the response according to
    the code that runs on the API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – A high-level architectural schema for a streaming solution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – A high-level architectural schema for a streaming solution
  prefs: []
  type: TYPE_NORMAL
- en: You can call APIs with a lot of different tools. Sometimes, you can even use
    your internet browser, otherwise, tools such as cURL do the job on the command
    line. You can use tools such as Postman or Insomnia for calling APIs with a user
    interface. All the communication is covered in fixed rules and practices, which,
    together, are called the HTTP protocol, which we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the HTTP protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interaction between services (or websites) uses the HTTP protocol. When working
    with APIs and building communicating microservices, it is important to understand
    the basics of the HTTP protocol.
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing to know is how to send and format requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: The GET request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest HTTP request is the `GET` request. You use this when you need to
    get something from a server or a service. For example, when going to a website,
    your browser sends a `GET` request to the website's IP address to obtain the website's
    layout code.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `GET` request can simply be sent from Python using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the `requests` library in Python to send a `GET` request to the
    Google home page. This is technically the same process as going to your internet
    browser and going to the Google home page. You'll obtain all the code that is
    needed for your web browser to show you the Google home page. Although many of
    you are very familiar with the look of the Google home page in your browser, it
    is much less recognizable in this code response. It is important to understand
    that it is actually exactly the same thing, just in a different format.
  prefs: []
  type: TYPE_NORMAL
- en: The POST request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `POST` request is another request that you'll encounter very often. It allows
    you to send some data with your request. This is often necessary, especially in
    analytics APIs, as the analytics are likely to happen on this data. By adding
    the data in the body of the `POST` request, you make sure that your analytics
    code received your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax in Python will be something like the following code block. For now,
    this code doesn''t work as you have not built a server that is able to do something
    with this data. However, just keep in mind that the `POST` request allows you
    to send your data point to an API with the goal of obtaining a response:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: JSON format for communication between systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common format for interaction between services is the **JavaScript
    Object Notation** (**JSON**) format. It is a data type that very strongly resembles
    the dictionary format in Python. In effect, it is a key-value object that is surrounded
    by accolades.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a JSON payload is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This data format is fairly easy to understand and very commonly used. It is,
    therefore, important to understand how it works. You'll see its use later on in
    the chapter as well.
  prefs: []
  type: TYPE_NORMAL
- en: RESTful APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While API development is out of scope for this book, it will be useful to have
    some pointers and best practices. The most used API structure is the **Representational
    State Transfer** (**REST**) API.
  prefs: []
  type: TYPE_NORMAL
- en: The REST API works just like other APIs, but it follows a certain set of style
    rules that make it recognizable as a REST API, also called the RESTful API.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are six guiding constraints in REST APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: Client-server architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statelessness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cacheability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layered system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code on demand (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniform interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to go further on this, some further reading resources are provided
    at the end of the chapter. Now that we have learned about the HTTP protocol, let's
    build an API on **Amazon Web Services** (**AWS**).
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple API on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to do something practical, let's build a super simple API on AWS. This
    will allow you to understand how different services can communicate together.
    It can also serve as a good testing environment for putting the examples in the
    rest of the book to the test.
  prefs: []
  type: TYPE_NORMAL
- en: You will use the following components of the AWS framework.
  prefs: []
  type: TYPE_NORMAL
- en: API Gateway in AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an AWS service that handles API requests for you. You specify the type
    of request that you expect to receive, and you specify the action that should
    be taken upon reception of a request. When building an API using API Gateway,
    this will automatically generate an IP address or link to which you can send your
    API requests.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda in AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lambda is a serverless execution environment for code. This means that you can
    write Python code, plug it to the API Gateway, and not think about how to set
    up servers, firewalls, and all that. This is great for decoupling systems, and
    it is fast enough for many real-time systems.
  prefs: []
  type: TYPE_NORMAL
- en: Data-generating process on a local machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the last component, you will build a separate data-generating process in
    Python. You can execute this code in a notebook. Every time a new data point is
    generated, the code will call the API with the analytics service and reply with
    an alert if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'A schematic overview of this architecture can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Detailed architecture schema for AWS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Detailed architecture schema for AWS
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement the example, we will use the following step-by-step instructions.
    If you have an AWS account, you can skip *Step 0*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 0 – Creating an account on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you do not yet have an account on AWS, it is easy to create one. You will
    have to set it up with a credit card, but the services that we will use here all
    have a free tier. As long as you shut down the resources at the end of your test,
    you are unlikely to incur any fees. However, be careful, because mistakes happen,
    and if you use a lot of resources on AWS, you will end up paying.
  prefs: []
  type: TYPE_NORMAL
- en: To set up an account, you can simply follow the steps on [aws.amazon.com](http://aws.amazon.com).
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Setting up a Lambda function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Upon receipt of the `POST` request, a Lambda function has to be called to execute
    our alert and send back the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to **Lambda** in the **Services** menu and click on **Create function**.
    You will see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Creating a Lambda function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Creating a Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to select **Python** and to give the appropriate name to your function.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have finished creating the function, it is time to code it. You can
    use the following code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This code has two functions. The `super_simple_alert` function takes a `datapoint`
    and returns an answer (an alarm in string format). The `lambda_handler` function
    is the code that deals with the incoming API calls. The event contains the `datapoint`,
    so the event is passed to the `super_simple_alert` function in order to analyze
    whether an alert should be launched. This is stored in the `answer` variable.
    Finally, the `lambda_handler` function returns a Python dictionary with the status
    code `200` and a body that contains the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The window should now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – The Lambda function window'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – The Lambda function window
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Set up API Gateway
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a first step, let's set up API Gateway to receive a `POST` request. The `POST`
    request will contain a body in which there is JSON that has a value for temperature
    and pH, just like in the alerting example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up API Gateway, you have to go to the **API Gateway** menu, which is
    accessible through the **Services** menu. The **Management** console looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – The AWS Management console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – The AWS Management console
  prefs: []
  type: TYPE_NORMAL
- en: 'You should end up on the **API Gateway** menu, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The API Gateway menu'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – The API Gateway menu
  prefs: []
  type: TYPE_NORMAL
- en: When you are in the **API Gateway** menu, you can go to **Create API** to set
    up your first API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside **Create API**, do the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **REST API**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the **REST** protocol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the API as a new API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an API name, for example, `streamingAPI`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will obtain an empty API configuration menu, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Adding a method in API Gateway'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Adding a method in API Gateway
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to add a `POST` method, so go to `POST` method. The following menu
    will appear for setting up the `POST` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – The POST setup'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 – The POST setup
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Deploy the API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Still in the API Gateway menu, click on `test` to deploy to. You can use the
    default setup for this stage, but it is important to take the URL that is on top
    here to be able to call your API from your data generation process. You will need
    to set the settings as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – More details for the API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_02_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – More details for the API
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Calling your API from another Python environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, you can call your API from another Python environment, such as a notebook
    on your own computer, or from a Google Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following code to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You will obtain the following answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Code block 2-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can imagine how a real-time data-generating process would simply call
    the API at each new data point and alerts would be generated right away!
  prefs: []
  type: TYPE_NORMAL
- en: More architectural considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although this is a great first try at building an API, you should be aware that
    there is much more to think about when you want to build this in a reliable and
    secure way. There is a reason that data science and software engineering are different
    jobs, and it takes time to learn all the skills necessary to manage an API from
    A to Z. In general, this will not be asked of a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the things that were not covered in this example are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance: scaling, load balancing, and latency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDoS attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and hacking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial aspects of API invocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency on a cloud provider versus being cloud provider agnostic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, there are some resources for further reading, which
    you can check out.
  prefs: []
  type: TYPE_NORMAL
- en: Other AWS services and other services in general that have the same functionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The current example used API Gateway and a Lambda function to build an API.
    The advantages of this method are the easiness of access and setup, which makes
    it great as a method to present in this book. However, you should be aware that
    there are many other tools and technologies for building APIs.
  prefs: []
  type: TYPE_NORMAL
- en: AWS is one of the most used cloud providers, and most things that can be done
    on AWS can be done on the other cloud providers' platforms as well. Examples of
    other big players are Google's GCP and Microsoft's Azure. Even on AWS, there are
    many alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: You can also build APIs in local environments. When doing this, you'll again
    have a large choice of tools and providers. Now that you have seen how to build
    an API using standard programming in Python and using a microservices approach,
    you will next see some alternatives using the big data environment. Big data environments
    generally have a steeper learning curve and may often be made for a specific use
    case, but they can be very powerful and absolutely necessary when working with
    high volume and velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Big data tools for real time streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many big data tools that do real-time streaming analytics. They can
    be great alternatives for *regular* real-time systems, especially when volumes
    are large and high speeds are required.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, the term **big data** is generally used to regroup tools that
    solve problems that are too complex to fit in memory The problems solved have
    three core characteristics: volume, variety, and velocity.'
  prefs: []
  type: TYPE_NORMAL
- en: Big data tools are generally known for doing a lot of work in parallel computing.
    When writing non-optimized, regular Python code, the code will often pass data
    points one by one. Big data solutions solve this by treating data points in parallel
    on multiple servers. This approach makes big data tools faster whenever there
    is a lot of data, but slower when there is little data (due to the overhead of
    managing the different workers).
  prefs: []
  type: TYPE_NORMAL
- en: Big data tools are often relatively specific; they should only be used for use
    cases that have vast amounts of data. It does not make sense to start working
    on big data tools for every problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Numerous such solutions are made for working with streaming data. Let''s have
    a look at some commonly used tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Streaming**: Spark Streaming is an addition to Spark, one of the main
    tools for big data nowadays. Spark Streaming can be plugged into sources such
    as Kafka, Flume, and Amazon Kinesis, thereby making streaming data accessible
    in a Spark environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka**: Kafka is an open source tool managed by Apache. It is a framework
    that is made for delivering real-time data feeds. It is used by many companies
    to deliver data pipelines and streaming analytics. Even some cloud providers have
    integrated Kafka into their solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Flume**: Apache Flume is another open source tool managed by Apache,
    which also focuses on streaming data. Flume is specifically used for treating
    large amounts of log data in a big data environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Beam**: Another tool in the Apache streaming family is Apache Beam.
    This tool can handle both batch and streaming data. It is best known for building
    ETL and data processing pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Storm**: Apache Storm is a stream processing computation framework
    that allows doing distributed computation. It is used to process data streams
    with Hadoop in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache NiFi**: Apache NiFi is a tool that focuses on ETL. It gives its users
    the possibility to automate and manage data flows between systems. It can work
    together with Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud DataFlow**: Google Cloud DataFlow is a tool proposed by Google
    Cloud Platform. It is developed specifically for tackling streaming use cases.
    It allows users to execute Apache Beam pipelines in a fully managed service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Kinesis**: Amazon Kinesis is strongly based on open source Apache
    Kafka, which was discussed earlier. The advantage of using Kinesis over Kafka
    is that it comes with a lot of things that are managed for you, whereas if you
    use Kafka directly, you spend more effort on managing the service. Of course,
    in return, you must use the AWS platform to access it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Stream Analytics**: Azure Stream Analytics is the main streaming analytics
    service proposed on Microsoft''s cloud platform, Azure. It is a real-time analytics
    service that is based on Trill.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IBM Streams**: IBM Streams is a streaming analytics tool that is proposed
    on the IBM cloud. Just like Kinesis, it is based on the open source Kafka project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling a big data environment in real time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your real-time analytics service is managed by a big data or specific streaming
    tool, you cannot always follow the API method for connecting your real-time process
    to your analytics process.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you'll need to look into the documentation of the tool of your
    choice and make sure that you understand how to make the connections work. At
    this point, you are often going to need a specialized profile to work with you,
    as this level of architecture and data engineering is generally considered out
    of scope for most data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: A general difference between the microservice system and the big data system
    is that in a microservice approach, we are generally considering that there must
    be a response coming from the API that is taken into account by the calling service.
  prefs: []
  type: TYPE_NORMAL
- en: In big data environments, it is much more common for a service such as a website
    to send data to a big data environment but not need a response. You could imagine
    a website that writes out every interaction by a user to a fixed location as JSON
    files. The big data streaming tool is then plugged onto this data storage location
    to read in the data in a streaming fashion and converts this into an analysis,
    a visualization, or something else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a minimal example that will show how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a JSON file called `example.json`, in which you write only the
    following data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code block 2-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now write a very short piece of Spark Streaming code that reads this
    data in a streaming way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In short, this code starts by creating a `spark` session. Once the session is
    created, a schema is defined for the `example.json` file. As it has only one key
    (called `value`), the schema is quite short. The data type for the value is `string`.
  prefs: []
  type: TYPE_NORMAL
- en: You then see that the data is imported using the `.readStream` method, which
    actually does a lot of the heavy lifting in streaming for you. If you'd like to
    go further with this example, you could write all kinds of analytical Spark functions
    using the `streamingDF` library and you will have streaming analytics using the
    well-known big data tool **PySpark**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have started to discover the field of architecture. You
    have built your own API on AWS, and you have seen the basic foundation of communication
    between systems. You should now understand that data is key in communication between
    systems and that good communication between systems is essential for delivering
    value through analytics.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true in the case of real-time and streaming analytics. The
    high speed and often large size of data can easily pose problems if architectural
    bottlenecks are not identified early enough in the project.
  prefs: []
  type: TYPE_NORMAL
- en: There are other topics that you must remember to take into account, including
    security, availability, and compliance. Those topics are best left to someone
    who makes it their full-time responsibility to take care of such data architecture
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we'll go back to the core of this book, as you'll
    discover how to build analytics use cases on streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Microservices Architecture*: [https://cloud.google.com/learn/what-is-microservices-architecture](https://cloud.google.com/learn/what-is-microservices-architecture)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API: [https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces](https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HTTP: [https://developer.mozilla.org/en-US/docs/Web/HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top 10 real-time data streaming tools*: [https://ipspecialist.net/top-10-real-time-data-streaming-tools/](https://ipspecialist.net/top-10-real-time-data-streaming-tools/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark Streaming: [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kafka: [https://kafka.apache.org/](https://kafka.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flume: [https://flume.apache.org/](https://flume.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beam: [https://beam.apache.org/](https://beam.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storm: [https://storm.apache.org/](https://storm.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NiFi: [https://nifi.apache.org/](https://nifi.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Cloud Dataflow: [https://cloud.google.com/dataflow](https://cloud.google.com/dataflow)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Kinesis: [https://aws.amazon.com/kinesis/](https://aws.amazon.com/kinesis/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azure Stream Analytics: [https://azure.microsoft.com/en-us/services/stream-analytics/](https://azure.microsoft.com/en-us/services/stream-analytics/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IBM Streams: [https://www.ibm.com/docs/en/streams](https://www.ibm.com/docs/en/streams)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Capturing Web Page Scroll Progress with Amazon Kinesis*, by AWS: [https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
