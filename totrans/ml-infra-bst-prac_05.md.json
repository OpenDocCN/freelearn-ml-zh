["```py\n# importing libraries\nfrom pygerrit2 import GerritRestAPI\n# A bit of config - repo\ngerrit_url = \"https://gerrit.onap.org/r\"\n# since we use a public OSS repository\nauth = None\n# this line gets sets the parameters for the HTML API\nrest = GerritRestAPI(url=gerrit_url, auth = auth)\n# the main query where we ask the endpoint to provide us the list and details of all changes\n# each change is essentially a review that has been submitted to the repository\nchanges = rest.get(\"/changes/?q=status:merged&o=ALL_FILES&o=ALL_REVISIONS&o=DETAILED_LABELS&start=0\",\nheaders={'Content-Type': 'application/json'})\n```", "```py\n# import the atlassian module to be able to connect to JIRA\nfrom atlassian import Jira\njira_instance = Jira(\n    #Url of the JIRA server\n    url=\"https://miroslawstaron.atlassian.net/\",\n    #  user name\n    username='email@domain.com',\n    # token\n    password='your_token',\n    cloud=True\n)\n# get all the tasks/ issues for the project\njql_request = 'project = MLBPB'\nissues = jira_instance.jql(jql_request)\n```", "```py\n# First create a Github instance:\n# using an access token\ng = Github(token, per_page=100)\n# get the repo for this book\nrepo = g.get_repo(\"miroslawstaron/machine_learning_best_practices\")\n# get all commits\ncommits = repo.get_commits()\n```", "```py\n# print the number of commits\nprint(f'Number of commits in this repo: {commits.totalCount}')\n# print the last commit\nprint(f'The last commit message: {commits[0].commit.message}')\n```", "```py\n# print the names of all files in the commit\n# 0 means that we are looking at the latest commit\nprint(commits[0].file)\n```", "```py\n# get one of the files from the commit\nfileOne = commits[0].files[0]\n# get the file from the second commit\nfileTwo = commits[1].files[0]\n# to get the content of the file, we need to get the sha of the commit\n# otherwise we only get the content from the last commit\nfl = repo.get_contents(fileOne.filename, ref=commits[0].sha)\nfr = repo.get_contents(fileTwo.filename, ref=commits[1].sha)\n# read the file content, but decoded into strings\n# otherwise we would get the content in bytes\nlinesOne = fl.decoded_content\nlinesTwo = fr.decoded_content\n```", "```py\n# calculate the diff using difflib\n# for which we use a library difflib\nimport difflib\n# print diff lines by iterating the list of lines\n# returned by the difflib library\nfor line in difflib.unified_diff(str(linesOne),\n                                 str(linesTwo),\n                                 fromfile=fileOne.filename,\n                                 tofile=fileTwo.filename):\n  print(line)\n```", "```py\nimport logging\n# create a logging file\n# including the format of the log messages\nlogging.basicConfig(filename='./information_quality_gerrit.log',\n                    filemode='w',\n                    format='%(asctime)s;%(name)s;%(levelname)s;%(message)s',\n                    level=logging.DEBUG)\n# specifying the name of the logger,\n# which will tell us that the message comes from this program\n# and not from any other modules or components imported\nlogger = logging.getLogger('Gerrit data export pipeline')\n# the first log message to indicate the start of the execution\n# it is important to add this, since the same log-file can be re-used\n# the re-use can be done by other components to provide one single point of logging\nlogger.info('Configuration started')\n```", "```py\n# A bit of config – repo\ngerrit_url = \"https://gerrit.onap.org/r\"\nfileName = \"./gerrit_reviews.csv\"\n# since we use a public oss repository, we don't need to authenticate\nauth = None\n# this line gets sets the parameters for the HTML API\nrest = GerritRestAPI(url=gerrit_url, auth = auth)\nlogger.info('REST API set-up complete')\n# a set of parameters for the JSON API to get changes in batches of 500\nstart = 0                       # which batch we start from – usually 0\nlogger.info('Connecting to Gerrit server and accessing changes')\ntry:\n    # the main query where we ask the endpoing to provide us the list and details of all changes\n    # each change is essentially a review that has been submitted to the repository\n    changes = rest.get(\"/changes/?q=status:merged&o=ALL_FILES&o=ALL_REVISIONS&o=DETAILED_LABELS&start={}\".format(start),\n                       headers={'Content-Type': 'application/json'})\nexcept Exception as e:\n    logger.error('ENTITY ACCESS – Error retrieving changes: {}'.format)\nlogger.info(…) statement as well as error messages with the logger.error(…) statement.\n\t\t\tThe content of this log file can be quite substantial, so we need to filter the messages based on their importance. That’s why we distinguish between errors and information.\n\t\t\tThe following is a fragment of such a log file. The first line contains the information message (boldface `INFO`) to show that the machine learning pipeline has been started:\n\n```", "```py\n\n\t\t\tWe filter these messages in the last part of our measurement system – the information quality measurement and monitoring system. This last part reads through the log files and collects the error messages, categorizes them, and then visualizes them:\n\n```", "```py\n\n\t\t\tThe bold-faced lines categorize the error messages found – in other words, they quantify the quality dimensions. This quantification is important as we need to understand how many problems of each kind were found in the machine learning pipeline.\n\t\t\tThe next step is to visualize the information quality, and for that, we need a quality analysis model. Then, we can use this quality model to visualize the quality dimensions:\n\n```", "```py\n\n\t\t\tThe visualization can be done in several ways, but in the majority of cases, it is enough to visualize it in a tabular form, which is easy to overview and comprehend. The most important for this visualization is that it communicates whether there are (or not) any information quality problems:\n\n```", "```py\n\n\t\t\tThe result of this code fragment is the visual representation shown in *Figure 4**.5*. This example can be found in this book’s GitHub repository:\n\t\t\t![Figure 4.5 – Results from the quality checks, visualized in a tabular form](img/B19548_04_5.jpg)\n\n\t\t\tFigure 4.5 – Results from the quality checks, visualized in a tabular form\n\t\t\tThis rudimentary way of checking the information’s quality illustrates my next best practice.\n\t\t\tBest practice # 28\n\t\t\tUse simple logging to trace any problems in your machine learning pipeline to monitor information quality.\n\t\t\tIt’s generally a good practice to design and develop robust software systems. Logging is one of the mechanisms that’s used to detect potential problems. Logging is also a very good software engineering practice for systems that are not interactive, such as machine learning-based ones. Therefore, extracting the information from logs can help us understand the quality of the information that is used in a machine learning-based system.\n\t\t\tNoise\n\t\t\tData quality in machine learning systems has one additional and crucial attribute – noise. Noise can be defined as data points that contribute negatively to the ability of machine learning systems to identify patterns in the data. These data points can be outliers that make the datasets skew toward one or several classes in classification problems. The outliers can also cause prediction systems to over- or under-predict because they emphasize patterns that do not exist in the data.\n\t\t\tAnother type of noise is contradictory entries, where two (or more) identical data points are labeled with different labels. We can illustrate this with the example of product reviews on Amazon, which we saw in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038). Let’s import them into a new Python script with `dfData = pd.read_csv('./book_chapter_4_embedded_1k_reviews.csv')`. In this case, this dataset contains a summary of the reviews and the score. We focus on these two columns and we define noise as different scores for the same summary review. For example, if one person provides a score of 5 for the review with the tag “Awesome!” and another person provides a score of 4 for another review with the tag “Awesome!,” the same data point becomes noisy as it is annotated with two different labels – two different scores.\n\t\t\tSo, first, we must check whether there are any duplicate entries:\n\n```", "```py\n\n\t\t\tThis code checks whether the number of data points is the same as the number of unique data points; if not, then we risk having noisy entries. We can check whether there are duplicate entries by using the following code:\n\n```", "```py\n\n\t\t\tNow, we can remove all duplicate entries using the following command, though a simple solution would be to remove them (`dfClean = dfData[~dfData.Summary.isin(lstDuplicated)]`). A better solution is to check whether they are noisy entries or just duplicates. We can do this using the following code fragment:\n\n```", "```py\n\n\t\t\tAfter running this fragment of code, the dataset does not contain any contradictory entries and therefore no class noise. Although it is possible to adopt a different strategy – for example, instead of removing noisy data points, we could change them to one of the classes – such an approach changes the pattern in the data, and therefore is not fully representative of the data. We simply do not know which of the classes is more correct than the others, especially if there are duplicate data points with two different labels.\n\t\t\tBest practice # 29\n\t\t\tThe best strategy to reduce the impact of noise on machine learning classifiers is to remove the noisy data points.\n\t\t\tAlthough we can correct noisy data points by changing their label or reducing the impact of these attributes on the predictions, the best strategy is to remove these data points. Removing is better as it does not change the patterns in the data. Imagine that we relabel noisy entries – this creates a pattern in the data that does not exist, which causes the algorithms to mispredict future data points.\n\t\t\tRemoving noise from the data is the only one way to handle noise. Another method is to increase the number of features so that we can distinguish between data points. We can analyze data and identify whether there is a risk of noise, and then we can check whether  it is possible to add one more feature to the dataset to distinguish between entries labeled differently. However, this is outside the scope of this chapter.\n\t\t\tSummary\n\t\t\tData for machine learning systems is crucial – without data, there can be no machine learning systems. In most machine learning literature, the process of training models usually starts with the data in tabular form. In software engineering, however, this is an intermediate step. The data is collected from source systems and needs to be processed.\n\t\t\tIn this chapter, we learned how to access data from modern software engineering systems such as Gerrit, GitHub, JIRA, and Git. The code included in this chapter illustrates how to collect data that can be used for further steps in the machine learning pipeline – feature extraction. We’ll focus on this in the next chapter.\n\t\t\tCollecting data is not the only preprocessing step that is required to design and develop a reliable software system. Quantifying and monitoring information (and data) quality is equally important. We need to check that the data is fresh (timely) and that there are no problems in preprocessing that data.\n\t\t\tOne of the aspects that is specific to machine learning systems is the presence of noise in the data. In this chapter, we learned how to treat class noise in the data and how to reduce the impact of the noise on the final machine learning algorithm.\n\t\t\tIn the next chapter, we dive deeper into concepts related to data - clearning it from noise and quantifying its properties.\n\t\t\tReferences\n\n\t\t\t\t*   *Vaswani, A. et al., Attention is all you need. Advances in neural information processing systems,* *2017\\. 30.*\n\t\t\t\t*   *Dastin, J., Amazon scraps secret AI recruiting tool that showed bias against women. In Ethics of Data and Analytics. 2018, Auerbach Publications.* *p. 296-299.*\n\t\t\t\t*   *Staron, M., D. Durisic, and R. Rana,* *Improving measurement certainty by using calibration to find systematic measurement error—a case of lines-of-code measure. In Software Engineering: Challenges and Solutions. 2017, Springer.* *p. 119-132.*\n\t\t\t\t*   *Staron, M. and W. Meding, Software Development Measurement Programs. Springer. https://doi. org/10.1007/978-3-319-91836-5, 2018\\. 10:* *p. 3281333.*\n\t\t\t\t*   *Fenton, N. and J. Bieman, Software metrics: a rigorous and practical approach. 2014:* *CRC press.*\n\t\t\t\t*   *Li, N., M. Shepperd, and Y. Guo, A systematic review of unsupervised learning techniques for software defect prediction. Information and Software Technology, 2020\\. 122:* *p. 106287.*\n\t\t\t\t*   *Staron, M. et al. Robust Machine Learning in Critical Care—Software Engineering and Medical Perspectives. In* *2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN).* *2021\\. IEEE.*\n\t\t\t\t*   *Zhang, J. et al., CoditT5: Pretraining for Source Code and Natural Language Editing. arXiv preprint* *arXiv:2208.05446, 2022.*\n\t\t\t\t*   *Staron, M. et al. Using machine learning to identify code fragments for manual review. In 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA).* *2020\\. IEEE.*\n\t\t\t\t*   *Ochodek, M., S. Kopczyńska, and M. Staron, Deep learning model for end-to-end approximation of COSMIC functional size based on use-case names. Information and Software Technology, 2020\\. 123:* *p. 106310.*\n\t\t\t\t*   *Cichy, C. and S. Rass, An overview of data quality frameworks. IEEE Access, 2019\\. 7:* *p. 24634-24648.*\n\t\t\t\t*   *Lee, Y.W. et al., AIMQ: a methodology for information quality assessment. Information & management, 2002\\. 40(2):* *p. 133-146.*\n\t\t\t\t*   *Staron, M. and W. Meding. Ensuring reliability of information provided by measurement systems. In International Workshop on Software Measurement.* *2009\\. Springer.*\n\t\t\t\t*   *Pandazo, K. et al. Presenting software metrics indicators: a case study. In Proceedings of the 20th international conference on Software Product and Process Measurement (**MENSURA). 2010.*\n\t\t\t\t*   *Staron, M. et al. Improving Quality of Code Review Datasets–Token-Based Feature Extraction Method. In International Conference on Software Quality.* *2021\\. Springer.*\n\n```"]