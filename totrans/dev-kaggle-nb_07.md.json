["```py\ntrain_corr['target'].sort_values(ascending=False)[1:16] \n```", "```py\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n            result.append(token)\n    return result \n```", "```py\n%%time\npreprocessed_comments = train_subsample['comment_text'].map(preprocess) \n```", "```py\n%%time\ndictionary = gensim.corpora.Dictionary(preprocessed_comments)\ndictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=75000) \n```", "```py\n%%time\nbow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_comments]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus] \n```", "```py\n%%time\nlda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20,\n                                    id2word=dictionary, passes=2, workers=2) \n```", "```py\ntopics = lda_model.print_topics(num_words=5)\nfor i, topic in enumerate(topics[:10]):\n    print(\"Train topic {}: {}\".format(i, topic)) \n```", "```py\nfor index, score in sorted(lda_model[bd5], key=lambda tup: -1*tup[1]):\n    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5))) \n```", "```py\nimport spacy\nnlp = spacy.load('en_core_web_sm') \n```", "```py\nselected_text = train.loc[train['comment_text'].str.contains(\"Trump\") | train['comment_text'].str.contains(\"Obama\")] \nselected_text[\"len\"] = selected_text['comment_text'].apply(lambda x: len(x))\nselected_text = selected_text.loc[selected_text.len < 100]\nselected_text.shape \n```", "```py\nfor sentence in selected_text[\"comment_text\"].head(5):\n    print(\"\\n\")\n    doc = nlp(sentence)\n    for ent in doc.ents:\n        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n    displacy.render(doc, style=\"ent\",jupyter=True) \n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nlabels = nlp.get_pipe(\"ner\").labels\nfor label in labels:\n    print(f\"{label} - {spacy.explain(label)}\") \n```", "```py\nfor sentence in selected_text[\"comment_text\"].head(5):\n    print(\"\\n\")\n    tokens = twt().tokenize(sentence)\n    tags = nltk.pos_tag(tokens, tagset = \"universal\")\n    for tag in tags:\n        print(tag, end=\" \") \n```", "```py\nfor sentence in selected_text[\"comment_text\"].head(5):\n    print(\"\\n\")\n    doc = nlp(sentence)\n    for token in doc:\n        print(token.text, token.pos_, token.ent_type_, end=\" | \") \n```", "```py\nimport re\ndef visualize_pos(sentence):\n    colors = {\"PRON\": \"blueviolet\",\n              \"VERB\": \"lightpink\",\n              \"NOUN\": \"turquoise\",\n              \"PROPN\": \"lightgreen\",\n              \"ADJ\" : \"lime\",\n              \"ADP\" : \"khaki\",\n              \"ADV\" : \"orange\",\n              \"AUX\" : \"gold\",\n              \"CONJ\" : \"cornflowerblue\",\n              \"CCONJ\" : \"magenta\",\n              \"SCONJ\" : \"lightmagenta\",\n              \"DET\" : \"forestgreen\",\n              \"NUM\" : \"salmon\",\n              \"PRT\" : \"yellow\",\n              \"PUNCT\": \"lightgrey\"}\n\n    pos_tags = [\"PRON\", \"VERB\", \"NOUN\", \"PROPN\", \"ADJ\", \"ADP\",\n                \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"SCONJ\",  \"DET\", \"NUM\", \"PRT\", \"PUNCT\"]\n\n    # Fix for issues in the original code\n    sentence = sentence.replace(\".\", \" .\")\n    sentence = sentence.replace(\"'\", \"\")\n    # Replace nltk tokenizer with spacy tokenizer and POS tagging\n    doc = nlp(sentence)\n    tags = []\n    for token in doc:\n        tags.append((token.text, token.pos_))\n\n    # Get start and end index (span) for each token\n    span_generator = twt().span_tokenize(sentence)\n    spans = [span for span in span_generator]\n    # Create dictionary with start index, end index, \n# pos_tag for each token\n    ents = []\n    for tag, span in zip(tags, spans):\n        if tag[1] in pos_tags:\n            ents.append({\"start\" : span[0], \n                         \"end\" : span[1], \n                         \"label\" : tag[1] })\n    doc = {\"text\" : sentence, \"ents\" : ents}\n    options = {\"ents\" : pos_tags, \"colors\" : colors}\n\n    displacy.render(doc, \n                    style = \"ent\", \n                    options = options, \n                    manual = True,\n                   ) \n```", "```py\ndef build_vocabulary(texts):\n    \"\"\"\n    Build the vocabulary from the corpus\n    Credits to: [9] [10]\n    Args:\n        texts: list of list of words\n    Returns:\n        dictionary of words and their count\n    \"\"\"\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\nexcept KeyError:\n                vocab[word] = 1\nreturn vocab \n```", "```py\n# populate the vocabulary\ndf = pd.concat([train ,test], sort=False)\nvocabulary = build_vocabulary(df['comment_text']) \n```", "```py\n# display the first 10 elements and their count\nprint({k: vocabulary[k] for k in list(vocabulary)[:10]}) \n```", "```py\ndef load_embeddings(file):\n    \"\"\"\n    Load the embeddings\n    Credits to: [9] [10]\n    Args:\n        file: embeddings file\n    Returns:\n        embedding index\n    \"\"\"\ndef get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index\n%%time\nGLOVE_PATH = '../input/glove840b300dtxt/'\nprint(\"Extracting GloVe embedding started\")\nembed_glove = load_embeddings(os.path.join(GLOVE_PATH,'glove.840B.300d.txt'))\nprint(\"Embedding completed\") \n```", "```py\ndef embedding_matrix(word_index, embeddings_index):\n    '''\n    Create the embedding matrix\n    credits to: [9] [10] \n    Args:\n        word_index: word index (from vocabulary)\n        embedding_index: embedding index (from embeddings file)\n    Returns:\n        embedding matrix\n    '''\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    EMBED_SIZE = all_embs.shape[1]\n    nb_words = min(MAX_FEATURES, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n    for word, i in tqdm(word_index.items()):\n        if i >= MAX_FEATURES:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix \n```", "```py\ndef check_coverage(vocab, embeddings_index):\n    '''\n    Check the vocabulary coverage by the embedding terms\n    credits to: [9] [10]\n    Args:\n        vocab: vocabulary\n        embedding_index: embedding index (from embeddings file)\n    Returns:\n        list of unknown words; also prints the vocabulary coverage of embeddings and \n        the % of comments text covered by the embeddings\n    '''\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\nfor word in tqdm(vocab.keys()):\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\nprint('Found embeddings for {:.3%} of vocabulary'.format(len(known_words)/len(vocab)))\n    print('Found embeddings for {:.3%} of all text'.format(nb_known_words/(nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n    return unknown_words \n```", "```py\nprint(\"Verify the initial vocabulary coverage\")\noov_glove = check_coverage(vocabulary, embed_glove) \n```", "```py\ndef add_lower(embedding, vocab):\n    '''\n    Add lower case words\n    credits to: [9] [10]\n    Args:\n        embedding: embedding matrix\n        vocab: vocabulary\n    Returns:\n        None\n        modify the embeddings to include the lower case from vocabulary\n    '''\n    count = 0\nfor word in tqdm(vocab):\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\nprint(f\"Added {count} words to embedding\") \n```", "```py\ntrain['comment_text'] = train['comment_text'].apply(lambda x: x.lower())\ntest['comment_text'] = test['comment_text'].apply(lambda x: x.lower())\nprint(\"Check coverage for vocabulary with lower case\")\noov_glove = check_coverage(vocabulary, embed_glove)\nadd_lower(embed_glove, vocabulary) # operates on the same vocabulary\noov_glove = check_coverage(vocabulary, embed_glove) \n```", "```py\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n...\n} \n```", "```py\ndef known_contractions(embed):\n    '''\n    Add know contractions\n    credits to: [9] [10]\n    Args:\n        embed: embedding matrix\n    Returns:\n        known contractions (from embeddings)\n    '''\n    known = []\n    for contract in tqdm(contraction_mapping):\n        if contract in embed:\n            known.append(contract)\n    return known \n```", "```py\ndef clean_contractions(text, mapping):\n    '''\n    Clean the contractions\n\n    credits to: [9] [10]\n    Args:\n        text: current text\n        mapping: contraction mappings\n    Returns: modify the comments to use the base form from contraction mapping\n    '''\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text \n```", "```py\npunct_mapping = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−βø³π'₹´°£€\\×™√²—–&'\npunct_mapping += '©^®` <→°€™' ♥←×§″′Â█½à…\"✶\"–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—'─▒: ¼⊕▼▪†■’▀¨▄♫⭐é¯♦¤▲è¸¾Ã‘'∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\ndef unknown_punct(embed, punct):\n    '''\n    Find the unknown punctuation\n    credits to: [9] [10] \n    Args:\n        embed: embedding matrix\n        punct: punctuation\n    Returns:\n        unknown punctuation\n    '''\n    unknown = ''\nfor p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\nreturn unknown \n```", "```py\npuncts = {\"‘\": \"'\", \"´\": \"'\", \"°\": \"\", \"€\": \"euro\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"pound\",\n          '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '…': ' '}\ndef clean_special_chars(text, punct, mapping):\n    '''\n    Clean special characters\n    credits to: [9] [10]\n    Args:\n        text: current text\n        punct: punctuation\n        mapping: punctuation mapping\n    Returns:\n        cleaned text\n    '''\nfor p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ') \n    return text \n```", "```py\nmore_puncts = {'▀': '.', '▄': '.', 'é': 'e', 'è': 'e', 'ï': 'i','⭐': 'star', 'ᴀ': 'A',  'ᴀɴᴅ': 'and', '»': ' '}\ntrain['comment_text'] = train['comment_text'].apply(lambda x: clean_special_chars(x, punct_mapping, more_puncts))\ntest['comment_text'] = test['comment_text'].apply(lambda x: clean_special_chars(x, punct_mapping, more_puncts))\n%%time\ndf = pd.concat([train ,test], sort=False)\nvocab = build_vocabulary(df['comment_text'])\nprint(\"Check coverage after additional punctuation replacement\")\noov_glove = check_coverage(vocab, embed_glove) \n```", "```py\n logger.info('Fitting tokenizer')\n    tokenizer = Tokenizer() \n    tokenizer.fit_on_texts(list(train[COMMENT_TEXT_COL]) + list(test[COMMENT_TEXT_COL]))\n    word_index = tokenizer.word_index\n    X_train = tokenizer.texts_to_sequences(list(train[COMMENT_TEXT_COL]))\n    X_test = tokenizer.texts_to_sequences(list(test[COMMENT_TEXT_COL]))\n    X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n    X_test = pad_sequences(X_test, maxlen=MAX_LEN) \n```", "```py\ndef build_embedding_matrix(word_index, path):\n    '''\n     Build embeddings\n    '''\n    logger.info('Build embedding matrix')\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, EMB_MAX_FEAT))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\nexcept:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n\n    del embedding_index\n    gc.collect()\n    return embedding_matrix\ndef build_embeddings(word_index):\n    '''\n     Build embeddings\n    '''\n    logger.info('Load and build embeddings')\n    embedding_matrix = np.concatenate(\n        [build_embedding_matrix(word_index, f) for f in EMB_PATHS], axis=-1) \n    return embedding_matrix \n```", "```py\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\n    '''\n        Build model\n    '''\n    logger.info('Build model')\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n\n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n    return model \n```", "```py\ndef run_model(X_train, y_train, y_aux_train, embedding_matrix, word_index, loss_weight):\n    '''\n        Run model\n    '''\n    logger.info('Run model')\n\n    checkpoint_predictions = []\n    weights = []\n    for model_idx in range(NUM_MODELS):\n        model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n        for global_epoch in range(NUM_EPOCHS):\n            model.fit(\n                X_train, [y_train, y_aux_train],\n                batch_size=BATCH_SIZE, epochs=1, verbose=1,\n                callbacks=[LearningRateScheduler(lambda epoch: 1.1e-3 * (0.55 ** global_epoch))]\n            )\n            with open('temporary.pickle', mode='rb') as f:\n                X_test = pickle.load(f) # use temporary file to reduce memory\n            checkpoint_predictions.append(model.predict(X_test, batch_size=1024)[0].flatten())\n            del X_test\n            gc.collect()\n            weights.append(2 ** global_epoch)\n        del model\n        gc.collect()\n\n    preds = np.average(checkpoint_predictions, weights=weights, axis=0)\n    return preds\ndef submit(sub_preds):\n    logger.info('Prepare submission')\n    submission = pd.read_csv(os.path.join(JIGSAW_PATH,'sample_submission.csv'), index_col='id')\n    submission['prediction'] = sub_preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv', index=False) \n```"]