<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer234">
			<h1 id="_idParaDest-166"><em class="italic"><a id="_idTextAnchor165"/>Chapter 10</em>: Training Deep Neural Networks on Azure</h1>
			<p>In the previous chapter, we learned how to train and score classical ML models using non-parametric tree-based ensemble methods. While these methods work well on many small- and medium-sized datasets that contain categorical variables, they don't generalize well on large datasets.</p>
			<p>In this chapter, we will train complex parametric models using <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) for even better generalization with very large datasets. This will help you understand <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>), how to train and use them, and when they perform better than traditional models.</p>
			<p>First, we will provide a short and practical overview of why and when DL works well and focus on understanding the general principles and rationale rather than the theoretical approach. This will help you to assess which use cases and datasets need DL and how it works in general.</p>
			<p>Then, we will look at one of the popular application domains for DL – computer vision. We will train a simple <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) model for image classification using the Azure Machine Learning service and additional Azure infrastructure. We will compare the performance to a model that has been fine-tuned on a pre-trained <strong class="bold">residual neural network</strong> (<strong class="bold">ResNet</strong>) model. This will set you up to train your models from scratch, fine-tune existing models for your application domain, and overcome situations where not enough training data is available.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introduction to Deep Learning</li>
				<li>Training a CNN for image classification</li>
			</ul>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create decision tree-based ensemble classifiers:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2 </strong></li>
			</ul>
			<p>Similar to the previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning.</p>
			<p>All the code examples in this chapter can be found in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter10">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter10</a>.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/>Introduction to Deep Learning</h1>
			<p>Deep learning has revolutionized the<a id="_idIndexMarker1242"/> ML domain recently and is constantly outperforming classical statistical approaches, and even humans, in various tasks such as image classification, object detection, segmentation, speech transcription, text translation, text understanding, sales forecasting, and much more. In contrast to classical models, DL models use many millions of parameters, parameter sharing, optimization techniques, and implicit feature extraction to outperform all previously hand-crafted feature detectors and ML models when trained with enough data.</p>
			<p>In this section, we will help you understand the basics of neural networks and the path to training deeper models with more parameters, better generalization, and hence better performance. This will help you understand how DL-based approaches work, as well as why and when they make sense for certain domains and datasets. If you are already an expert in DL, feel free to skip this section and go directly to the practical examples in the <em class="italic">Training a CNN for image classification</em> section.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>Why Deep Learning?</h2>
			<p>Many traditional optimization, classification, and forecasting processes have worked well over the past<a id="_idIndexMarker1243"/> decades using classical ML approaches, such as k-nearest neighbor, linear and<a id="_idIndexMarker1244"/> logistic regression, naïve Bayes, <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), tree-based ensemble models, and others. They worked well on various types of data (transactional, time series, operational, and so on) and data types (binary, numerical, and categorical) for small- to mid-sized datasets.</p>
			<p>However, in some domains, data generation has exploded, and classical ML models couldn't achieve better performance even with an increasing amount of training data. This especially affected the <a id="_idIndexMarker1245"/>domains of computer vision and NLP around late 2010. That's when researchers had a breakthrough with neural networks – also called <strong class="bold">multilayer perceptrons</strong> (<strong class="bold">MLPs</strong>) – a technique that <a id="_idIndexMarker1246"/>was used in the late 80s to capture the vast number of features in a large image dataset by using multiple nested layers.</p>
			<p>The following chart captures this idea very well. While traditional ML approaches work very well on small- and medium-sized datasets, their performance usually does not improve with more training data. However, DL models are massive parametric models that can capture a vast number of details from training data. Hence, we can see that their prediction performance increases as the amount of data increases:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B17928_10_01.jpg" alt="Figure 10.1 – The effectiveness of DL versus traditional ML " width="1034" height="359"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – The effectiveness of DL versus traditional ML</p>
			<p>Traditional models often use pre-engineered features and are optimized for datasets of various data types and ranges. In the previous chapter, we saw that gradient-boosted trees perform extremely well on categorical data. However, in domains that contain highly structured data or data of variable lengths, many traditional models reach their limits. This is especially true for pixel information in two- and three-dimensional images and videos, as well as waveforms in audio data and characters and character sequences in free-text data. ML <a id="_idIndexMarker1247"/>models used to process such data using complex manually tuned feature extractors, such as <strong class="bold">histogram of oriented gradients</strong> (<strong class="bold">HoG</strong>) filters, <strong class="bold">scale-invariant feature transform</strong> (<strong class="bold">SIFT</strong>) features, or <strong class="bold">local binary patterns</strong> (<strong class="bold">LBPs</strong>) – just to name<a id="_idIndexMarker1248"/> a few filters in the <a id="_idIndexMarker1249"/>computer vision domain.</p>
			<p>What makes this data so <a id="_idIndexMarker1250"/>complicated is that no obvious linear relationship between the input data (for example, a single pixel) and the output exists – in most cases, seeing a single pixel of an image won't help determine the brand of a car in that image. Therefore, there was an increasing need to train larger and more capable parametric models that used raw, unprocessed data as input to capture these relationships from the input pixel to make a final prediction.</p>
			<p>It's important to understand that the need for deeper models with many more parameters comes from the vastly increasing amount of highly structured training data in specific domains, such as vision, audio, and language. These new models often have millions of parameters to capture the massive amounts of raw and augmented training data, as well as developing an internal generalized conceptual representation of the training data. Keep this in mind when choosing an ML approach for your use case.</p>
			<p>A quick look at your training data often helps to determine whether a DL-based model is suitable for the task – given that DL models have millions of parameters to train. If your data is stored in a SQL database or CSV or Excel files, then you should probably look into classical ML approaches, such as parametric statistical (linear regression, SVM, and so on) or non-parametric (decision tree-based ensembles) approaches. If your data is so big that it doesn't fit into <a id="_idIndexMarker1251"/>memory or is stored in a <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>), blob storage, or a file storage server, then you could use a DL-based approach.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>From neural networks to deep learning</h2>
			<p>The foundation of neural <a id="_idIndexMarker1252"/>networks and hence today's DL-based<a id="_idIndexMarker1253"/> approaches – the <strong class="bold">perceptron</strong> – is a concept that is over half a century old and was developed in the 1950s. In this section, we will take a look at the basics, and work our way back to <strong class="bold">MLPs</strong> – also called <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>) – and <strong class="bold">CNNs</strong> in the 1980s, and <a id="_idIndexMarker1254"/>then to <strong class="bold">DNNs</strong> and <a id="_idIndexMarker1255"/>DL in the last decade. This will <a id="_idIndexMarker1256"/>help you understand <a id="_idIndexMarker1257"/>the foundational concepts of neural networks and hence DL, as well as how model architectures and training techniques have evolved over the last century into the state-of-the-art techniques we are using today.</p>
			<h3>The perceptron – a classifier from the 50s</h3>
			<p>Perceptrons are the foundational building blocks of today's neural networks and are modeled on cells in the human<a id="_idIndexMarker1258"/> brain (so-called <strong class="bold">neurons</strong>). They are simple non-linear functions consisting of two components: a weighted sum of all the inputs and an activation function<a id="_idIndexMarker1259"/> that fires if the output is larger than the specified threshold. While this analogy of a neuron is a great way to model how a brain works, it is a poor model to understand how the input signal is transformed into its output.</p>
			<p>Rather than neurons in the brain, we prefer a much simpler, non-biological approach to explain the perceptron, MLPs, and CNNs – namely, a simple geometric approach. When simplified, this method requires you to only understand the two-dimensional line equation. Once you understand the basics in two dimensions, the concept can be extended to multiple dimensions, where the line becomes a plane or hyperplane in a higher-dimensional feature space.</p>
			<p>If we look at a single perceptron, it describes a <strong class="bold">weighted sum</strong> of its inputs plus constant bias<a id="_idIndexMarker1260"/> with an <strong class="bold">activation function</strong>. Let's break down the two components of the perceptron. Do you know what is also described as a weighted sum of its inputs plus bias? Right, the line equation:</p>
			<p><img src="image/Formula_10.1.png" alt="" width="101" height="21"/></p>
			<p>In the preceding equation, <em class="italic">x</em> is the input, <em class="italic">k</em> is the weight, and <em class="italic">b</em> is the bias term. You have probably seen this equation at some point in your math curriculum. A property of this equation is that when you're inserting a point's <em class="italic">x</em> and <em class="italic">y</em> coordinates into the line equation, it yields 0 = 0 for all the points that lie on the line. We can use this information to derive the vector form of the line equation, as follows:</p>
			<p><img src="image/Formula_10.2.png" alt="" width="208" height="44"/></p>
			<p>Hence, <img src="image/Formula_10.3.png" alt="" width="52" height="21"/> is <em class="italic">0</em> when the point lies on the line. What happens if we insert the coordinates of a point that does not lie on the line? A good guess is that the result will be either positive or negative but certainly not 0. A property of the vector line equation is that the sign of this result describes which side of the line the point lies on. Hence, the point lies either on the left or the right-hand side of the line when <img src="image/Formula_10.4.png" alt="" width="54" height="21"/> is positive or negative but not null.</p>
			<p>To determine the side of the <a id="_idIndexMarker1261"/>line, we can apply the sign function to <img src="image/Formula_10.5.png" alt="" width="54" height="22"/>. The sign function is<a id="_idIndexMarker1262"/> often also referred to as the step function, as its output is either <em class="italic">1</em> or <em class="italic">-1</em>, hence positive or negative. The sign or step function here is our activation function and hence the second component of the perceptron. The output of the perceptron, <img src="image/Formula_10.6.png" alt="" width="16" height="25"/>, can be written as follows:</p>
			<p><img src="image/Formula_10.7.png" alt="" width="231" height="44"/></p>
			<p>In the following chart, we can see two points, a line, and their shortest distance to the line. Both points are not lying on the line, so the line separates both points from each other. If we insert both points' coordinates into the vector line equation, then one point would result in a positive value <img src="image/Formula_10.8.png" alt="" width="54" height="22"/>, whereas the other point would result in a negative value <img src="image/Formula_10.9.png" alt="" width="53" height="23"/>:</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/B17928_10_02.jpg" alt="Figure 10.2 – A simple binary classifier " width="908" height="373"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – A simple binary classifier</p>
			<p>The result would tell us which side of the line the point lies on. This line is a geometric description of the perceptron, which is a very simple classifier. The trained perceptron is defined through the line equation (or a hyperplane in multiple dimensions), which separates a space into left and right. This line is the decision boundary for a classification, and a point is <a id="_idIndexMarker1263"/>an observation. By inserting a point into the line equation and applying the step function, we return the resulting class of the observation, which is left or right, -1 or +1, or class A or B. This describes a binary classifier.</p>
			<p>And how do we find the decision boundary? To find the optimal decision boundary, we can follow an iterative training process while using labeled training samples. First, we must initialize a random decision boundary, then compute the distance from each sample to the decision boundary and move the decision boundary into the direction that minimizes the total sum of distances. The optimal vector to move the decision boundary is if we move it along the negative gradient, such that the distance between the point and the line reaches a minimum. By using a learning rate factor, we iterate this process a few times and end up with a perfectly aligned <a id="_idIndexMarker1264"/>decision boundary, if the training samples are linearly separable. This process is called <strong class="bold">gradient descent</strong>, where we iteratively modify the classifier weights (decision boundaries, in this example) to find the optimal boundary with minimal error.</p>
			<h3>The multilayer perceptron</h3>
			<p>A perceptron<a id="_idIndexMarker1265"/> describes a simple classifier whose decision boundary is a line (or hyperplane) that's been defined through the weighted inputs. However, instead of using a single classifier, we can simply increase the number of neurons, which will result in multiple decision boundaries, as shown in the following chart:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B17928_10_03.jpg" alt="Figure 10.3 – Combining multiple perceptrons " width="794" height="372"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Combining multiple perceptrons</p>
			<p>Each neuron describes a decision boundary and hence will have separate weights and a separate output – left or right of the decision boundary. By stacking multiple neurons in layers, we can create<a id="_idIndexMarker1266"/> classifiers whose inputs are the output of the previous ones. This allows us to combine the results from multiple decision boundaries into a single output – for example, finding all the samples that are enclosed by the decision boundaries of three neurons, as shown in the preceding chart.</p>
			<p>While a single layer of perceptrons describes a linear combination of inputs and outputs, researchers began to stack these perceptrons into multiple sequential layers, where each layer was followed by an activation function. This is called MLP, or an ANN. Using the geometric model as an analogy, you could simply stack multiple decision boundaries on complex geometric objects to create more complex decision boundaries. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Another analogy is that the classifier's decision boundary is always a straight hyperplane, but the input samples are transformed to be linearly separated through the decision boundary.</p>
			<p>The same geometric analogy helps us understand the layers in DL models. While the first layers of a network describe very low-level geometric features, such as straight edges and lines, the higher levels describe complicated nested combinations of these low-level features; for example, four lines build a square, five squares build a more complex shape, and a <a id="_idIndexMarker1267"/>combination of those shapes looks like a human face. We just built a face detector using a three-layer neural network.</p>
			<p>The Google DeepDream experiment is a fantastic example of this analogy. In the following figure, we can visualize how three layers of different depths in a pre-trained DNN represent features in an image of a cloudy sky. The layers are extracted from the beginning, middle, and end of a DNN and transform the input image to minimize the loss of each layer. Here, we can see how the earlier layer focuses mostly on lines and edges (left), whereas the middle layer sees abstract shapes (middle), and the last layer activates on very specific high-level features in the image (right):</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/B17928_10_04.jpg" alt="Figure 10.4 – DeepDream – minimizing loss for the layers of a DNN " width="928" height="455"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – DeepDream – minimizing loss for the layers of a DNN</p>
			<p>Next, let's look at CNNs.</p>
			<h3>CNNs</h3>
			<p>Using multiple high-dimensional hyperplane equations, where each output feeds into each input of the following layer, requires a <a id="_idIndexMarker1268"/>very large number of parameters. While a high number of parameters is required to model a massive amount of complex training data, a so-called fully connected neural network is not the best way to describe these connections. So, what's the problem?</p>
			<p>In a fully connected network, each output is fed to each neuron of the consecutive layer as input. In each neuron, we require a weight for each input, so we need as many weights as there are input dimensions. This number quickly explodes when we start stacking multiple layers of perceptrons. Another problem is that the network cannot generalize because it learns all the individual weights separately for each dimension.</p>
			<p>In the 1980s, CNNs were<a id="_idIndexMarker1269"/> invented to solve these problems. Their purpose was to reduce the number of connections and parameters on a single layer to a fixed set of parameters, independent of the number of input dimensions. The parameters of a layer are now shared within all the inputs. The idea of this approach comes from signal processing, where filters are applied to a signal through a convolution operation. Convolution means applying a single set of weights, such as a window function, to multiple regions of the input and later summing up all the signal responses of the filter for each location.</p>
			<p>This was the same idea for the convolution layers of CNNs. By using a fixed-sized filter that is convolved with the input, we can greatly reduce the number of parameters for each layer and add more nested layers to the network. By using a so-called pooling layer, we can also reduce the image size and apply filters to a downscaled version of the input. Let's take a look at the popular layers that are used for building CNNs:</p>
			<ul>
				<li><strong class="bold">Fully connected (FC)</strong>: The FC layer is a layer of fully connected neurons, as described in the <a id="_idIndexMarker1270"/>previous section about perceptrons – it connects every output from the previous layer with a neuron. In DNN, FC layers are often used at the end of the network to combine all the spatially distributed activations of the previous convolution layers. The FC layers also have the largest number of parameters in a model (usually around 90%).</li>
				<li><strong class="bold">Convolution</strong>: A convolution<a id="_idIndexMarker1271"/> layer consists of spatial (often two-dimensional) filters that are convolved along the spatial dimensions and summed up along the depth dimension of the input. Due to weight sharing, they are much more efficient than fully connected layers and have a lot fewer parameters.</li>
				<li><strong class="bold">Pooling</strong>: Convolution layers are <a id="_idIndexMarker1272"/>often followed by a pooling layer to reduce the spatial dimension of the volume for the next filter – this is the equivalent of a subsampling operation. The pooling operation itself has no learnable<a id="_idIndexMarker1273"/> parameters. Most of the time, <strong class="bold">max pooling</strong> layers are used in DL models due to their simple gradient computation. Another <a id="_idIndexMarker1274"/>popular choice is <strong class="bold">avg pooling</strong>, which is mostly used as a classifier at the end of a network.</li>
				<li><strong class="bold">Normalization</strong>: In modern <a id="_idIndexMarker1275"/>DNNs, normalization layers are often used to stabilize gradients throughout the network. Due to the unbounded behavior of some activation functions, filter responses have to be normalized. A commonly <a id="_idIndexMarker1276"/>used normalization technique is <strong class="bold">batch normalization</strong>.</li>
			</ul>
			<p>Now that we understand the main components of CNNs, we can look into how these models were stacked even deeper to improve generalization and hence improve the prediction's performance.</p>
			<h3>From CNNs to DL</h3>
			<p>The perceptron from<a id="_idIndexMarker1277"/> the 50s, as well as ANNs and CNNs from the 80s, build the foundation for all the DL models that are used today. By stabilizing the gradients during the training process, researchers could overcome the exploding and vanishing gradients problem and build deeper models. This was achieved by using additional normalization layers, rectified linear activation, auxiliary losses, and residual connections.</p>
			<p>Deeper models have more learnable parameters – often well over 100 million parameters – so they can find higher-level patterns and learn more complex transformations. However, to train deeper models, you must also use more training data. Therefore, companies and researchers built massive labeled datasets (such as ImageNet) to feed these models with training data.</p>
			<p>This development process was facilitated by the availability of cheap parallelizable compute in the form of GPUs and cloud computing. Training these deep models quickly went from months to days to hours within a couple of years. Today, we can train a typical DNN in under an hour with a highly parallelized compute infrastructure.</p>
			<p>A lot of research also went into new techniques for stacking layers, from very deep networks with skip connections, as in ResNet152, to networks with parallel layer groups, as in GoogLeNet. A combination of both layer types led to extremely efficient network architectures such as SqueezeNet and Inception. New layer types such as LSTM, GRU, and attention enabled <a id="_idIndexMarker1278"/>significantly better prediction performance, while the GAN and transform models created entirely new ways to train and optimize models.</p>
			<p>All these advances helped make DL what it has become today – a ubiquitous ML technique that, given enough training data, can outperform traditional ML models and often even humans in most prediction tasks. Today, DL is applied to almost any domain where there is sufficient data at hand.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor170"/>DL versus traditional ML</h2>
			<p>Let's look at the main <a id="_idIndexMarker1279"/>differences between classical ML- and DL-based approaches and find out what DL models can do with so many more parameters and how they benefit from them.</p>
			<p>If we look at the image or audio processing domain before 2012, we will see that ML models were not usually trained on the raw data itself. Instead, the raw data went through a manually crafted feature extractor and converted into a lower-dimensional feature space. When dealing with images of 256 x 256 x 3 dimensions (RGB) – which corresponds to a 196,608-dimensional feature space – and converting these into, say, a 2,048-dimensional feature embedding as input for the ML models, we greatly reduce the computational requirements for these models. The feature extractors for image and audio features often use a convolution operator and a specific filter (such as an edge detector, blob detector, spike/dip detector, and so on). However, the filter is usually constructed manually.</p>
			<p>The classical ML models that have been developed in the past 50+ years are still the ones we are successfully using today. Among those are tree-based ensemble techniques, linear and logistic regression, SVMs, and MLPs. The MLP model is also known as a fully connected neural network with hidden layers and still serves as a classification or regression head in some of the early DL architectures.</p>
			<p>The following diagram shows the typical pipeline of a classical ML approach in the computer vision domain:</p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B17928_10_05.jpg" alt="Figure 10.5 – Traditional ML classifier " width="1046" height="426"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Traditional ML classifier</p>
			<p>First, the raw data is converted into a lower-dimensional feature embedding using hand-crafted image filters (SIFT, SURF, HoG, LBPs, Haar filters, and so on). Then, feature embedding is used to train an ML model; for example, a multi-layer, fully connected neural network or <a id="_idIndexMarker1280"/>decision-tree classifier, as shown in the preceding diagram.</p>
			<p>When it is difficult for a human being to express a relationship between an input image and an output label in simple rules, then it is most likely also difficult for a classical computer vision and ML approach to find such rules. DL-based approaches perform a lot better in these cases. The reason for this is that DL models are trained on raw input data instead of manually extracted features. Since convolution layers are the same as randomized and trained image filters, these filters for feature extraction are implicitly learned by the network.</p>
			<p>The following diagram shows a DL approach to image classification, which is similar to the previous diagram for the classical ML approach:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B17928_10_06.jpg" alt="Figure 10.6 – DL-based classifier " width="852" height="427"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – DL-based classifier</p>
			<p>As we can see, the raw input data of the image is fed directly to the network, which outputs the final image label. This is why we often refer to a DL model as an end-to-end model – because it creates an <a id="_idIndexMarker1281"/>end-to-end transformation between the input data (literally, the raw pixel values) and the model's output.</p>
			<p>As shown in the preceding diagram, the DL-based model is an end-to-end model that learns both the feature extractor and the classifier in a single model. However, we often refer to the last fully connected layer.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Look at the type of data before choosing your ML model. If you are dealing with images, video, audio, time series, language, or text, you may wish to use a DL model or feature extractor for embedding, clustering, classification, or regression. If you are working with operational or business data, then a classic ML approach would be a better fit.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/>Using traditional ML with DL-based feature extractors</h2>
			<p>In many cases, especially when<a id="_idIndexMarker1282"/> you have small datasets, not enough compute resources, or knowledge to train end-to-end DL models, you can also reuse a pre-trained DL model as a feature extractor. This can be done by loading a pre-trained model and performing a forward pass until the classification/regression head. It returns a multi-dimensional embedding (a so-called latent space representation) that you can directly plug into a classical ML model.</p>
			<p>Here is an example of such a hybrid approach. We are using the <strong class="source-inline">IncpetionV3</strong> model as a feature extractor, pre-trained on the <strong class="source-inline">imagenet</strong> data. The DL model is only used to transform the raw input<a id="_idIndexMarker1283"/> image data into a lower-dimensional feature representation. Then, an SVM model is trained on top of the image features. Let's look at the source code for this example:</p>
			<p class="source-code">import numpy as np </p>
			<p class="source-code">from tensorflow.keras.applications import InceptionV3</p>
			<p class="source-code">def extract_features(img_data, IMG_SIZE):    </p>
			<p class="source-code">    IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)    </p>
			<p class="source-code">    model = InceptionV3(input_shape=IMG_SHAPE,</p>
			<p class="source-code">                        include_top=False,</p>
			<p class="source-code">                        weights='imagenet',</p>
			<p class="source-code">                        pooling='avg')</p>
			<p class="source-code">    predictions = model.predict(img_data)</p>
			<p class="source-code">    return np.squeeze(predictions)</p>
			<p class="source-code">labels = [] # loaded previously</p>
			<p class="source-code">features = extract_features(image_data)</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p>
			<p class="source-code">    features, labels)</p>
			<p class="source-code">from sklearn.svm import SVC</p>
			<p class="source-code">clf = SVC(kernel='linear', C=1)</p>
			<p class="source-code">clf.fit(X_train, y_train)</p>
			<p>In the preceding code, we used TensorFlow to load the <strong class="source-inline">InceptionV3</strong> model with the ImageNet-based <a id="_idIndexMarker1284"/>weights but without any classification or regression head. This is achieved by setting the <strong class="source-inline">include_top</strong> property to <strong class="source-inline">False</strong>. Then, we squeezed the output of the prediction – the image's latent representation – into a single vector. Finally, we trained an SVM on the image features using scikit-learn and a default train/test split.</p>
			<p>We started with the classical approach, where feature extraction and ML were separated into two steps. However, the filters in the classical approach were hand-crafted and applied directly to the raw input data. In a DL approach, we implicitly learn the feature extraction.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor172"/>Training a CNN for image classification</h1>
			<p>Now that we have a good <a id="_idIndexMarker1285"/>understanding of why and when to use DL models, we can start to implement one and run it using Azure Machine Learning. We will start with a task that DL performed very well with over the past years – computer vision, or more precisely, image classification. If you feel that this is too easy for you, you can replace the actual training script with any other computer vision technique and follow along with the steps in this section:</p>
			<ol>
				<li>First, we will power up an <a id="_idIndexMarker1286"/>Azure Machine Learning compute instance, which will serve as our Jupyter Notebook authoring environment. First, we will write a training script and execute it in the authoring environment to verify that it works properly, checkpoints the model, and logs the training and validation metrics. We will train the model for a few epochs to validate the setup, the code, and the resulting model. </li>
				<li>Next, we will try to improve the algorithm by adding data augmentation to the training script. While this seems like an easy task, I want to reiterate that this is necessary and strongly recommended for any DL-based ML approach. Image data can easily be augmented to improve generalization and therefore model scoring performance. However, through this technique, training the model will take even longer than before because more training data is being used for each epoch.</li>
				<li>Now, we must move the training script from the authoring environment to a GPU cluster – a remote compute environment. We will do all this – upload the data, generate the training scripts, create the cluster, execute the training script on the cluster, and retrieve the trained model – from within the authoring environment in the Azure Machine Learning service. If you are already training ML models yourself on your server, then this section will show you how to move your training scripts to a remote execution environment and how to benefit from dynamically scalable compute (both vertically and horizontally, hence larger and more machines), auto-scaling, cheap data storage, and much more.</li>
				<li>Once you have successfully trained a CNN from scratch, you will want to move on to the next level in terms of model performance and complexity. A good and recommended approach is to fine-tune pre-trained DL models rather than train them from scratch. Using this approach, we can often also use a pre-trained model from a specific task, drop the classification head (usually the last one or two layers) from the <a id="_idIndexMarker1287"/>model, and reuse the feature extractor for another task by training our classification head on top of it. This is called transfer learning and is widely used for training state-of-the-art models for various domains.</li>
			</ol>
			<p>Now, let's open a Jupyter notebook and start training a CNN image classifier.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>Training a CNN from scratch in your notebook</h2>
			<p>Let's train a CNN on <a id="_idIndexMarker1288"/>Jupyter on the Azure Machine Learning service. First, we want to simply train a model in the current authoring environment, which means we must use the compute (CPU and memory) from the compute instance. This is a standard Python/Jupyter environment, so it is no different from training an ML model on your local machine. So, let's go ahead and create a new compute instance in our Azure Machine Learning service workspace, and then open the Jupyter environment:</p>
			<ol>
				<li value="1">Before we begin creating our CNN model, we need some training data. As we train the ML model on the authoring computer, the data needs to be on the same machine. For this example, we will use the MNIST image dataset:<p class="source-code">import os</p><p class="source-code">import urllib</p><p class="source-code">os.makedirs('./data/mnist', exist_ok=True)</p><p class="source-code">BASE_URL = 'http://yann.lecun.com/exdb/mnist/'</p><p class="source-code">urllib.request.urlretrieve(</p><p class="source-code">    BASE_URL + 'train-images-idx3-ubyte.gz',</p><p class="source-code">    filename='./data/mnist/train-images.gz')</p><p class="source-code">urllib.request.urlretrieve(</p><p class="source-code">    BASE_URL + 'train-labels-idx1-ubyte.gz',</p><p class="source-code">    filename='./data/mnist/train-labels.gz')</p><p class="source-code">urllib.request.urlretrieve(</p><p class="source-code">    BASE_URL + 't10k-images-idx3-ubyte.gz',</p><p class="source-code">    filename='./data/mnist/test-images.gz')</p><p class="source-code">urllib.request.urlretrieve(</p><p class="source-code">    BASE_URL + t10k-labels-idx1-ubyte.gz',</p><p class="source-code">    filename='./data/mnist/test-labels.gz')</p></li>
			</ol>
			<p>In the preceding code, we<a id="_idIndexMarker1289"/> loaded the training and testing data and put it in the <strong class="source-inline">data</strong> directory of the current environment where the code executes. In the next section, we will learn how to make the data available on any compute instance in the ML workspace.</p>
			<ol>
				<li value="2">Next, we must load the data, parse it, and store it in multi-dimensional NumPy arrays. We will use a helper function, <strong class="source-inline">load</strong>, which is defined in the accompanying<a id="_idIndexMarker1290"/> source code for this chapter. After that, we must preprocess the training data by normalizing the pixel values to a range between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:<p class="source-code">DIR = './data/mnist/'</p><p class="source-code">X_train = load(DIR + 'train-images.gz', False) / 255.0</p><p class="source-code">X_test = load(DIR + 'test-images.gz', False) / 255.0</p><p class="source-code">y_train = load(DIR + 'train-labels.gz', True) \</p><p class="source-code">              .reshape(-1)</p><p class="source-code">y_test = load(DIR + 'test-labels.gz', True) \</p><p class="source-code">             .reshape(-1)</p></li>
			</ol>
			<p>Using the <strong class="source-inline">reshape</strong> method, we checked that the training and testing labels are one-dimensional vectors with a single label per training and testing sample.</p>
			<p>Once we have the training <a id="_idIndexMarker1291"/>data, it is time to decide which Python framework to use to train the neural network models. While you are not limited to any specific framework in Azure Machine Learning, it is recommended you use either TensorFlow (with Keras) or PyTorch to train neural networks and DL models. TensorFlow and Keras are great choices when you're training and deploying standard production models.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">PyTorch is a great choice for tinkering with exotic models and custom layers and debugging customized models. In my opinion, PyTorch is a bit easier to get started with, whereas TensorFlow is more complex and mature and has a bigger ecosystem. In this chapter, we will use TensorFlow due to its large ecosystem, Keras integration, great documentation, and good support in the Azure Machine Learning service.</p>
			<ol>
				<li value="3">Having chosen an ML framework, we<a id="_idIndexMarker1292"/> can start to construct a simple CNN. Let's use <strong class="source-inline">keras</strong> to construct a sequential model:<p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Conv2D, \</p><p class="source-code">    MaxPooling2D, Flatten, Dense</p><p class="source-code">model = Sequential()</p><p class="source-code">model.add(Conv2D(filters=16,</p><p class="source-code">                 kernel_size=3,</p><p class="source-code">                 padding='same',</p><p class="source-code">                 activation='relu',</p><p class="source-code">                 input_shape=(28,28,1)))</p><p class="source-code">model.add(MaxPooling2D(pool_size=2))</p><p class="source-code">model.add(Conv2D(filters=32,</p><p class="source-code">                 kernel_size=3,</p><p class="source-code">                 padding='same',</p><p class="source-code">                 activation='relu'))</p><p class="source-code">model.add(MaxPooling2D(pool_size=2))</p><p class="source-code">model.add(Flatten())</p><p class="source-code">model.add(Dense(256, activation='relu'))</p><p class="source-code">model.add(Dense(10, activation='softmax'))</p></li>
			</ol>
			<p>In the preceding code, we took advantage of the <strong class="source-inline">keras.Sequential</strong> model API to construct a simple CNN model. We went with the default initialization of the weights and solely specified the model structure here. You can also see the typical combination of a feature extractor until the <strong class="source-inline">Flatten</strong> layer, and the MLP classification head outputting 10 probabilities using the <strong class="source-inline">softmax</strong> activation function at the end.</p>
			<p>Let's take a quick<a id="_idIndexMarker1293"/> look at the model, which has, in total, <strong class="source-inline">409034</strong> parameters, as shown in the following diagram. Please note that we specifically constructed a simple CNN from a tiny image size of <strong class="source-inline">28x28</strong> grayscale images. The following diagram shows the compact structure of the model defined. Here, we can observe that the largest number of parameters is the fully connected layer after the feature extractor, which contains 98% of the parameters of the total model:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B17928_10_07.jpg" alt="Figure 10.7 – DL model architecture " width="530" height="550"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – DL model architecture</p>
			<ol>
				<li value="4">After defining the <a id="_idIndexMarker1294"/>model structure, we need to define the <strong class="source-inline">loss</strong> metric that we are trying to optimize and specify an optimizer. The optimizer is responsible for computing the changes for all the weights per training iteration, given the total and backpropagated loss. With Keras and TensorFlow, we can easily choose a state-of-the-art optimizer and use a default metric for classification:<p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='adam',</p><p class="source-code">              metrics=['accuracy'])</p></li>
			</ol>
			<p>In the preceding code, we defined a <strong class="source-inline">categorical_crossentropy</strong> loss and the <strong class="source-inline">adam</strong> optimizer to train the CNN. We also tracked another metric besides the loss – <strong class="source-inline">accuracy</strong>. This makes it easier to estimate and measure the performance of the CNN during training.</p>
			<ol>
				<li value="5">Before we start training, we must <a id="_idIndexMarker1295"/>define a model checkpoint. This is important as it allows us to pause and resume training at any given time after an epoch. Using Keras, it is quite simple to implement this, as follows:<p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">checkpoint_path = "./mnist_cnn.bin"</p><p class="source-code">checkpoint_cb = ModelCheckpoint(checkpoint_path)</p></li>
				<li>Finally, we can start the training locally by invoking the <strong class="source-inline">fit</strong> method on the Keras model. We must supply the training data as well as the batch size and the number of epochs (iterations) for training. We must also pass the previously created <strong class="source-inline">callback</strong> model checkpoint so that we can save the model after each epoch:<p class="source-code">model.fit(X_train,</p><p class="source-code">          y_train,</p><p class="source-code">          batch_size=16,</p><p class="source-code">          epochs=10,</p><p class="source-code">          callbacks=[checkpoint_cb])</p></li>
				<li>Finally, we can use the trained model of the last epoch to compute the final score on the test set:<p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">model = load_model(checkpoint_path)</p><p class="source-code">scores = model.evaluate(X_test, y_test, verbose=1)</p><p class="source-code">print('Test loss:', scores[0])</p><p class="source-code">print('Test accuracy:', scores[1])</p></li>
			</ol>
			<p>In the preceding code, we can <a id="_idIndexMarker1296"/>see that training a CNN on a compute instance in Azure Machine Learning is straightforward and similar to training a model on the local machine. The only difference is that we have to be sure that all the required libraries (and required versions) have been installed and that the data is available.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/>Generating more input data using augmentation</h2>
			<p>DL models usually have many<a id="_idIndexMarker1297"/> millions of parameters to represent the model with the training set distribution. Hence, when dealing with DL, be it in custom vision using Cognitive Services, Azure Machine Learning Studio, or custom models in ML service workspaces, you should always implement data augmentation.</p>
			<p>Data augmentation is a way of creating more training data by slightly modifying the available data and providing the modified data to the ML algorithm. Depending on the use case, this could include mirroring, translating, scaling, or skewing images, as well as changing the brightness, luminosity, or color information of images. These modifications strongly improve the generalization of the model, such as enabling better scale, translation, rotation, and transformation invariance.</p>
			<p>The benefit of using TensorFlow and Keras is that data augmentation is a built-in capability. First, we can create an <strong class="source-inline">ImageDataGenerator</strong> object, which stores all our modifications and can generate iterators through the augmented dataset. The data augmentation techniques for this generator can be configured when the generator is being initialized. However, we want to use the generator to simply iterate through the training images without augmentation and add augmentation once we have connected all the pieces. Let's take a look:</p>
			<ol>
				<li value="1">Let's implement an image data generator in Keras using the <strong class="source-inline">ImageDataGenerator</strong> object:<p class="source-code">from tensorflow.keras.preprocessing.image import \</p><p class="source-code">    ImageDataGenerator</p><p class="source-code">datagen = ImageDataGenerator()</p></li>
				<li>Now, we can return a data iterator from the image data generator by passing the original training image data and labels to the generator. Before we sample images from the generator, we need to compute the training set statistics that will be required for further augmentations. Similar to the scikit-learn <strong class="source-inline">BaseTransformer</strong> interface, we need to call the <strong class="source-inline">fit</strong> method on the generator:<p class="source-code">datagen.fit(x_train)</p></li>
				<li>Next, we must<a id="_idIndexMarker1298"/> create an iterator by using the <strong class="source-inline">flow</strong> method:<p class="source-code">it = datagen.flow(X_train, y_train, batch_size=16)</p></li>
				<li>If instead of loading the images into NumPy arrays beforehand, we wanted to read individual images from a folder, we could use a different generator function to do so, as shown in the following snippet:<p class="source-code">it = datagen.flow_from_directory(</p><p class="source-code">         directory='./data/mnist',</p><p class="source-code">         target_size=(28, 28),</p><p class="source-code">         batch_size=16,</p><p class="source-code">         class_mode='categorical')</p></li>
			</ol>
			<p>However, in our example, the training images have been combined into a single file, so we don't need to load the image data ourselves.</p>
			<ol>
				<li value="5">The iterator can now be used to loop through the data generator and yield new training samples with each iteration. To do so, we need to replace the <strong class="source-inline">fit</strong> function with the <strong class="source-inline">fit_generator</strong> function, which expects an iterator instead of a training dataset:<p class="source-code">model.fit_generator(it,</p><p class="source-code">                    steps_per_epoch=256,</p><p class="source-code">                    epochs=10,</p><p class="source-code">                    callbacks=[checkpoint_cb])</p></li>
			</ol>
			<p>As we can see, we can pass the same arguments for <strong class="source-inline">epoch</strong> and <strong class="source-inline">callback</strong> to the <strong class="source-inline">fit_generator</strong> function as we did to the <strong class="source-inline">fit</strong> function. The only difference is that now, we need to fix several steps per epoch so that the iterator yields new images. Once we have added augmentation methods to the generator, we could theoretically generate unlimited modifications of each training image per epoch. Hence, with this argument, we can define<a id="_idIndexMarker1299"/> how many batches of data we wish to train each epoch with, which should roughly correspond to the number of training samples divided by the batch size.</p>
			<p>Finally, we can configure the data augmentation techniques. The default image data generator supports a variety of augmentations through different arguments:</p>
			<ul>
				<li>Translation or shifts</li>
				<li>Horizontal or vertical flips</li>
				<li>Rotations</li>
				<li>Brightness</li>
				<li>Zoom</li>
			</ul>
			<p>Let's go back to the image data generator and activate data augmentation techniques. Here is an example generator that is often used for data augmentation in image processing:</p>
			<p class="source-code">datagen = ImageDataGenerator(</p>
			<p class="source-code">              featurewise_center=True,</p>
			<p class="source-code">              featurewise_std_normalization=True,</p>
			<p class="source-code">              rotation_range=20,</p>
			<p class="source-code">              width_shift_range=0.2,</p>
			<p class="source-code">              height_shift_range=0.2,</p>
			<p class="source-code">              horizontal_flip=True)</p>
			<p>By using this data generator, we can train the model with augmented image data and further improve the performance of the CNN. As we saw previously, this is a crucial and strongly recommended <a id="_idIndexMarker1300"/>step in any DL training pipeline.</p>
			<p>Let's move all the code that we have developed so far into a file called <strong class="source-inline">scripts/train.py</strong>. We will use this file in the next section to schedule and run it on a GPU cluster.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>Training on a GPU cluster using Azure Machine Learning</h2>
			<p>Now that we have a<a id="_idIndexMarker1301"/> training script ready, verified that the script works, and added data augmentation, we can move this training script to a <a id="_idIndexMarker1302"/>more performant execution environment. In DL, many operations, such as convolutions, pooling, and general tensor operators, can benefit from parallel execution. Therefore, we will execute the training script on a GPU cluster and track its status in the authoring environment.</p>
			<p>One benefit of using Azure Machine Learning is that we can set up and run everything in Python from the authoring environment – that is, the Jupyter notebook running on the Azure Machine Learning compute instance:</p>
			<ol>
				<li value="1">First, we must configure our Azure Machine Learning workspace, which is a single statement without arguments on the compute instance:<p class="source-code">from azureml.core.workspace import Workspace</p><p class="source-code">ws = Workspace.from_config()</p></li>
				<li>Next, we must load or create a GPU cluster with autoscaling for the training process:<p class="source-code">from azureml.core.compute import ComputeTarget, \</p><p class="source-code">    AmlCompute</p><p class="source-code">from azureml.core.compute_target import \</p><p class="source-code">    ComputeTargetException</p><p class="source-code">cluster_name = "gpu-cluster"</p><p class="source-code">vm_size = "STANDARD_NC6"</p><p class="source-code">max_nodes = 3</p><p class="source-code">try:</p><p class="source-code">    compute_target = ComputeTarget(ws, </p><p class="source-code">        name=cluster_name)</p><p class="source-code">    print('Found existing compute target.')</p><p class="source-code">except ComputeTargetException:</p><p class="source-code">    print('Creating a new compute target...')</p><p class="source-code">    compute_config = \</p><p class="source-code">        AmlCompute.provisioning_configuration(</p><p class="source-code">            vm_size=vm_size, max_nodes=max_nodes)</p><p class="source-code">    # create the cluster and wait for completion</p><p class="source-code">    compute_target = ComputeTarget.create(ws, </p><p class="source-code">        cluster_name, compute_config)</p><p class="source-code">compute_target.wait_for_completion(show_output=True)</p></li>
			</ol>
			<p>As shown in the <a id="_idIndexMarker1303"/>preceding code snippet, creating <a id="_idIndexMarker1304"/>a GPU cluster with autoscaling only requires a couple of lines of code within Jupyter with Azure Machine Learning. But how did we choose the VM size and the number of nodes for the GPU cluster?</p>
			<p>In general, you can decide between the NC, ND, and NV types from the N-series VMs in Azure. A later version number (for example, v2 or v3) usually means updated hardware, hence a newer CPU and GPU, and better memory. You can think of the different N-series versions in terms of applications (<em class="italic">NC</em>, where <em class="italic">C</em> means compute; <em class="italic">ND</em>, where <em class="italic">D</em> means deep learning; and <em class="italic">NV</em>, where <em class="italic">V</em> means video). The following table will help you compare the different N-series VM types and their GPU<a id="_idIndexMarker1305"/> configurations. Most <a id="_idIndexMarker1306"/>machines can be scaled up to four GPUs per VM. </p>
			<p>The following table shows an Azure VM N-series comparison:</p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/B17928_10_08.jpg" alt="Figure 10.8 – Azure VM N-series costs " width="1215" height="414"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Azure VM N-series costs</p>
			<p>The prices in the preceding table represent pay-as-you-go prices for Linux VMs in the West US 2 region for December 2021. Please note that these prices may have changed by the time you are reading this, but it should give you an indication of the different options and configurations to choose from.</p>
			<p>To get a better understanding of the costs and performance, we can look at a typical workload for training a ResNet50 model on the ImageNet dataset. The following table, provided by Nvidia, shows that it makes sense to choose the latest GPU models as their performance increase is much better and the costs are more efficient than the older GPU models:</p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/B17928_10_09.jpg" alt="Figure 10.9 – GPU costs " width="1257" height="190"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – GPU costs</p>
			<p>As shown in the preceding table, the performance increase that's visible in the lower training duration for the same task pays off and results in a much lower cost for the overall task.</p>
			<p>Hence, the <strong class="source-inline">STANDARD_NC6</strong> model is a great starting point, from a pricing perspective, for experimenting with GPUs, CNNs, and DNNs in Azure. The only thing that we have to <a id="_idIndexMarker1307"/>make sure of is that our model can fit into the available GPU memory of the VM. A common way to calculate this is to compute the number of<a id="_idIndexMarker1308"/> parameters for the model, times 2 for storing gradients (times 1 when performing only inferencing), times the batch size, and times 4 for the single-precision size in bytes (or times 2 for half-precision).</p>
			<p>In our example, the CNN architecture requires 1.6 MB to store the trainable parameters (weights and biases). To also store backpropagated losses for a batch size of 16, we would require around 51.2 MB (1.6 MB x 16 x 2) of GPU memory to perform the whole end-to-end training on a single GPU. This also fits easily in our 12 GB of GPU memory in the smallest NC instance.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">While these numbers seem small for our test case, you will often deal with larger models (with up to 100 million parameters) and larger image sizes. To put that into perspective, ResNet152, when trained on image dimensions of 224 x 224 x 3, has approximately 60 million parameters and a size of 240 MB. On the <strong class="source-inline">STANDARD_NC6</strong> instance, we could train, at most, at a batch size of 24, according to our equation.</p>
			<p>By adding more GPUs or nodes to the cluster, we must introduce a different framework to take advantage of the distributed setup. We will discuss this in more detail in <a href="B17928_12_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 12</em></a>, <em class="italic">Distributed Machine Learning on Azure</em>. However, we can add more nodes with autoscaling to the cluster so that multiple people <a id="_idIndexMarker1309"/>can submit multiple jobs <a id="_idIndexMarker1310"/>simultaneously. The number of maximum nodes can be computed as <em class="italic">simultaneous models/node * number of peak models to be trained simultaneously</em>. In our test scenario, we will go with a cluster size of <strong class="source-inline">3</strong> so that we can schedule a few models at the same time.</p>
			<ol>
				<li value="3">Now that we have decided on a VM size and GPU configuration, we can continue with the training process. Next, we need to make sure that the cluster can access the training data. To do so, we will use the default datastore on the Azure Machine Learning workspace:<p class="source-code">ds = ws.get_default_datastore()</p><p class="source-code">ds.upload(src_dir='./data/mnist',</p><p class="source-code">          target_path='mnist',</p><p class="source-code">          show_progress=True)</p></li>
			</ol>
			<p>In the preceding code, we copied the training data from the local machine to the default datastore – the blob storage account. As we discussed in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>, there are also other ways to upload your data to blob storage or another storage system.</p>
			<p>Mounting blob storage to a machine, or even a cluster, is usually not a straightforward task. Yes, you could have a NAS and mount it as a network drive on every node in the cluster, but this is tedious to set up and scale. Using the Azure Machine Learning datastore API, we can simply request a reference to the datastore, which can be used to mount the correct folder on every machine that needs to access the data:</p>
			<p class="source-code">ds_data = ds.as_mount()</p>
			<p>The preceding command returns a <strong class="source-inline">Datastore Mount</strong> object, which doesn't look particularly powerful. However, if we pass this reference as a parameter to the training script, it can automatically mount the datastore and read the content from the datastore on each training compute in Azure Machine Learning. If you have <a id="_idIndexMarker1311"/>ever played with mount<a id="_idIndexMarker1312"/> points or <strong class="source-inline">fstab</strong>, you will understand that this one-liner can speed up your daily workflow.</p>
			<ol>
				<li value="4">Now, we can create an Azure Machine Learning configuration. Let's create <strong class="source-inline">ScriptRunConfiguration</strong> so that we can schedule the training script on the cluster:<p class="source-code">from azureml.core import ScriptRunConfig</p><p class="source-code">script_params={</p><p class="source-code">    '--data-dir': ds_data</p><p class="source-code">}</p><p class="source-code">src = src = ScriptRunConfig(</p><p class="source-code">    source_directory='./scripts',</p><p class="source-code">    script='train.py',</p><p class="source-code">    compute_target=compute_target,</p><p class="source-code">    environment=tf_env)</p></li>
				<li>To read the data from the specified default datastore, we need to parse the argument in the <strong class="source-inline">train.py</strong> script. Let's go back to the script and replace the file-loading code with the following code block:<p class="source-code">import argparse</p><p class="source-code">parser = argparse.ArgumentParser()</p><p class="source-code">parser.add_argument('--data-dir', type=str)</p><p class="source-code">args = parser.parse_args()</p><p class="source-code">DIR = args.data_dir</p><p class="source-code">X_train = load(DIR + 'train-images.gz', False) / 255.0</p><p class="source-code">X_test = load(DIR + 'test-images.gz', False) / 255.0</p><p class="source-code">y_train = load(DIR + 'train-labels.gz', True) \</p><p class="source-code">              .reshape(-1)</p><p class="source-code">y_test = load(DIR + 'test-labels.gz', True) \</p><p class="source-code">              .reshape(-1)</p></li>
				<li>This leaves us with scheduling<a id="_idIndexMarker1313"/> and running the script <a id="_idIndexMarker1314"/>on the GPU cluster. However, before doing so, we want to make sure that all the runs are tracked in the Azure Machine Learning service. Therefore, we must also add <strong class="source-inline">Run</strong> to the <strong class="source-inline">train.py</strong> file and reuse the Keras callback for Azure Machine Learning from <a href="B17928_03_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing the Azure Machine Learning Workspace</em>. Here is what the training script will look like:<p class="source-code">from azureml.core import Run</p><p class="source-code"># Get the run configuration</p><p class="source-code">run = Run.get_context()</p><p class="source-code"># Create an Azure Machine Learning monitor callback</p><p class="source-code">azureml_cb = AzureMlKerasCallback(run)</p><p class="source-code">callbacks = [azureml_cb, checkpoint_cb]</p><p class="source-code">model.fit_generator(it,</p><p class="source-code">                    steps_per_epoch=256,</p><p class="source-code">                    epochs=10,</p><p class="source-code">                    callbacks=callbacks)</p><p class="source-code"># Load the best model</p><p class="source-code">model = load_model(checkpoint_path)</p><p class="source-code"># Score trained model</p><p class="source-code">scores = model.evaluate(X_test, y_test, verbose=1)</p><p class="source-code">print('Test loss:', scores[0])</p><p class="source-code">run.log('Test loss', scores[0])</p><p class="source-code">print('Test accuracy:', scores[1])</p><p class="source-code">run.log('Test accuracy', scores[1])</p></li>
			</ol>
			<p>As we can see, we <a id="_idIndexMarker1315"/>added the <strong class="source-inline">Run</strong> configuration<a id="_idIndexMarker1316"/> and the Keras callback to track all the metrics during the epochs. We also collected the final test set metric and reported it to the Azure Machine Learning service. You can find the complete runnable example in the code provided with this book.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor176"/>Improving your performance through transfer learning</h2>
			<p>In many cases, you won't have a<a id="_idIndexMarker1317"/> dataset containing hundreds of millions of labeled training samples, and that's completely understandable. So, how can you still benefit from all the previous work and benchmarks? Shouldn't a feature extractor trained on recognizing animals also perform well on recognizing faces? The classifier would certainly be different, but the visual features that are extracted from the images should be similar.</p>
			<p>This is the idea behind <strong class="bold">fine-tuning</strong> pre-trained<a id="_idIndexMarker1318"/> models or, more generally speaking, <strong class="bold">transfer learning</strong>. To fine-tune, we can simply reuse a feature extractor from a pre-trained DL model (for example, pre-trained on the ImageNet dataset, the <strong class="source-inline">faces</strong> dataset, the <strong class="source-inline">CoCo</strong> dataset, and so on) and attach a custom classifier to the end of the model. Transfer learning means that we can transfer the features from a model from one task to another task: for example, from classification to object detection. It may be a bit confusing at first regarding whether we would want to reuse features for a different task. However, if a model has been taught to identify patterns of geographical shapes in images, this same feature extractor could certainly be reused for any image-related task in the same domain.</p>
			<p>One useful property of transfer learning is that the initial learning task doesn't necessarily need to be a supervised ML task, so it is not necessary to have annotated training data to train the feature extractor. A popular unsupervised ML technique is called auto-encoders, where an <a id="_idIndexMarker1319"/>ML model tries to generate a similar-looking output, given input, using a feature extractor and an upsampling network. By minimizing the error between the generated output and the input, the feature extractor learns to efficiently represent the input data in latent space. Auto-encoders are popular for pre-training network architectures before the pre-trained weights for the actual ML task are used. </p>
			<p>We need to make sure that the pre-trained model was trained on a dataset in the same domain. Images of biological cells look very different from faces, and clouds look very different from buildings. In general, the ImageNet dataset covers a broad spectrum of photograph-style images for many standard visual features, such as buildings, cars, animals, and more. Therefore, it is a good choice to use a pre-trained model for many computer vision tasks.</p>
			<p>Transfer learning is not only tied to image data and modeling data for computer vision. Transfer learning has proven valuable in any domain where datasets are sufficiently similar, such as for human voices or written text. Hence, whenever you are implementing a DL model, do your research on what datasets could be used for transfer learning and to ultimately improve the model's performance.</p>
			<p>Let's bring the theory into practice and dive into some examples. We saw a similar example earlier in this chapter, where we piped the output of the feature extractor to an SVM. In this section, we <a id="_idIndexMarker1320"/>want to achieve something similar, but the result will be a single end-to-end model. Therefore, in this example, we will build a network architecture for the new model consisting of a pre-trained feature extractor and a newly initialized classification head:</p>
			<ol>
				<li value="1">First, we must define the number of output classes and the input shape and load the base model from Keras:<p class="source-code">from tensorflow.keras.applications.resnet50 \</p><p class="source-code">    import ResNet50</p><p class="source-code">num_classes = 10</p><p class="source-code">input_shape = (224, 224, 3)</p><p class="source-code"># create the base pre-trained model</p><p class="source-code">base_model = ResNet50(input_shape=input_shape, </p><p class="source-code">                      weights='imagenet',</p><p class="source-code">                      include_top=False,</p><p class="source-code">                      pooling='avg')</p></li>
			</ol>
			<p>In the preceding code, most of the magic for pre-training happens thanks to Keras. First, we specified the image dataset that will be used to train this model using the <strong class="source-inline">weights</strong> argument, which will automatically initialize the model weights with the pre-trained <strong class="source-inline">imagenet</strong> weights. With the third argument, <strong class="source-inline">include_top=False</strong>, we told Keras to only load the feature extractor part of the model. Using the <strong class="source-inline">pooling</strong> argument, we also specified how the last pooling operation should be performed. In this case, we chose average pooling.</p>
			<ol>
				<li value="2">Next, we must freeze the layers of the model by setting their <strong class="source-inline">trainable</strong> property to <strong class="source-inline">False</strong>. To do so, we can simply loop over all the layers in the model:<p class="source-code">for layer in base_model.layers:</p><p class="source-code">    layer.trainable = False</p></li>
				<li>Finally, we can <a id="_idIndexMarker1321"/>attach any network architecture to the model that we want. In this case, we will attach the same classifier head that we used in the CNN network from the previous section. Finally, we must construct the final model class by using the new architecture and output as the classifier output layer:<p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.layers import Flatten, Dense</p><p class="source-code">clf = base_model.output</p><p class="source-code">clf = Dense(256, activation='relu')(clf)</p><p class="source-code">clf = Dense(10, activation='softmax')(clf)</p><p class="source-code">model = Model(base_model.input, clf)</p></li>
			</ol>
			<p>That's it! You have successfully built a new end-to-end model that combines a pre-trained ResNet50 feature extractor on ImageNet with your custom classifier. You can now use this Keras model and plug it into your preferred optimizer and send it off to the GPU cluster. The output of the training process will be a single model that can be managed and deployed as any other custom model.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You are not limited to always freezing all the layers of the original network. A common approach is to also unfreeze later layers in the network, decrease the learning rate by at least a factor of 10, and continue training. By repeating this procedure, we could even retrain (or fine-tune) all the layers of the network in a step-by-step approach with a decreasing learning rate.</p>
			<p>Independently of your choice and use case, you should add transfer learning to your standard repertoire for training DL models. Treat it like other popular preprocessing and training techniques, such <a id="_idIndexMarker1322"/>as data augmentation, which should always be used when you're training DL models.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Summary</h1>
			<p>In this chapter, we learned when and how to use DL to train an ML model on Azure. We used both a compute instance and a GPU cluster from within the Azure Machine Learning service to train a model using Keras and TensorFlow.</p>
			<p>First, we found out that DL works very well on highly structured data with non-obvious relationships from the raw input data to the resulting prediction. Good examples include image classification, speech-to-text, and translation. We also saw that DL models are parametric models with a large number of parameters, so we often need a large amount of labeled or augmented input data. In contrast to traditional ML approaches, the extra parameters are used to train a fully end-to-end model, also including feature extraction from the raw input data.</p>
			<p>Training a CNN using the Azure Machine Learning service is not difficult. We saw many approaches, from prototyping in Jupyter to augmenting the training data, to running the training on a GPU cluster with autoscaling. The difficult part in DL is preparing and providing enough high-quality training data, finding a descriptive error metric, and optimizing between costs and performance. We provided an overview of how to decide on the best VM and GPU size and configuration for your job, something that I recommend you do before starting your first GPU cluster.</p>
			<p>In the next chapter, we will go one step further and look into hyperparameter tuning and automated ML, a feature in the Azure Machine Learning service that lets you train and optimize stacked models automatically.</p>
		</div>
	</div>
</div>
</body></html>