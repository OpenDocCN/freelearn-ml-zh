<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer302">
<h1 class="chapter-number" id="_idParaDest-171"><a id="_idTextAnchor225"/>12</h1>
<h1 id="_idParaDest-172"><a id="_idTextAnchor226"/>Working with H2O AutoML and Apache Spark</h1>
<p>In <a href="B17298_10.xhtml#_idTextAnchor196"><em class="italic">Chapter 10</em></a>, <em class="italic">Working with Plain Old Java Objects (POJOs)</em>, and <a href="B17298_11.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a>,<em class="italic"> Working with Model Object, Optimized (MOJO)</em>, we explored how to build and deploy our <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) models as POJOs and MOJOs in production systems and use them to make predictions. In the majority of real-world problems, you will often need to deploy your entire ML pipeline in production so that you can deploy as well as train models on the fly. Your system will also be gathering and storing new data that you can later use to retrain your models. In such a scenario, you will eventually need to integrate your H2O server into your business product and coordinate the ML effort.</p>
<p>Apache Spark is one of the more commonly used technologies in the domain of ML. It is an analytics engine used for large-scale data processing using cluster computing. It is completely open source and widely supported by the Apache Software Foundation.</p>
<p>Considering the popularity of Spark in the field of data processing, H2O.ai developed an elegant software solution that combines the benefits of both Spark and AutoML into a single one-stop solution for ML pipelines. This software product is called H2O Sparkling Water.</p>
<p>In this chapter, we will learn more about H2O Sparkling Water. First, we will understand what Spark is and how it works and then move on to understanding how H2O Sparkling Water operates H2O AutoML in conjunction with Spark to solve fast data processing needs.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Exploring Apache Spark</li>
<li>Exploring H2O Sparkling Water</li>
</ul>
<p>By the end of this chapter, you should have a general idea of how we can incorporate H2O AI along with Apache Spark using H2O Sparkling Water and how you can benefit from the best of both these worlds.</p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor227"/>Technical requirements</h1>
<p>For this chapter, you will require the following:</p>
<ul>
<li>The latest version of your preferred web browser.</li>
<li>An <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) of your choice or a Terminal.</li>
<li>All experiments conducted in this chapter have been performed on a Terminal. You are free to follow along using the same setup or perform the same experiments using any IDE of your choice.</li>
</ul>
<p>So, let’s start by understanding what exactly Apache Spark is.</p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor228"/>Exploring Apache Spark</h1>
<p>Apache Spark<a id="_idIndexMarker1135"/> started as a project in UC Berkeley AMPLab in 2009. It was then open sourced under a BSD license in 2010. Three years later, in 2013, it was donated to the Apache Software Foundation and became a top-level project. A year later, it was used by Databricks in a data sorting competition where it set a new world record. Ever since then, it has been picked up and used widely for in-memory distributed data analysis in the big data industry. </p>
<p>Let’s see what the various components of Apache Spark are and their respective functionalities.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor229"/>Understanding the components of Apache Spark</h2>
<p><strong class="bold">Apache Spark</strong> is an open source data processing engine. It is used to process<a id="_idIndexMarker1136"/> data in real time, as well as in batches using cluster computing. All data processing tasks<a id="_idIndexMarker1137"/> are performed in memory, making task executions very fast. Apache Spark’s data processing capabilities coupled with H2O’s AutoML functionality can make your ML system perform more efficiently and powerfully. But before we dive deep into H2O Sparkling Water, let’s understand what Apache Spark is and what it consists of.</p>
<p>Let’s start by understanding what the<a id="_idIndexMarker1138"/> various components of the Spark ecosystem are:</p>
<div>
<div class="IMG---Figure" id="_idContainer285">
<img alt="Figure 12.1 – Apache Spark components " height="402" src="image/B17298_12_001.jpg" width="845"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Apache Spark components</p>
<p>The various components of the Spark ecosystem are as follows:</p>
<ul>
<li><strong class="bold">Spark Core</strong>: The Spark Core<a id="_idIndexMarker1139"/> component is the most vital component of the Spark ecosystem. It is responsible for basic functions such as input-output operations and scheduling and monitoring jobs. All the other components are built on top of this component. This component supports the Scala, Java, Python, and R programming languages using specific interfaces. The Spark Core component itself is written in the Scala programming language.</li>
<li><strong class="bold">Spark SQL</strong>: The Spark SQL component<a id="_idIndexMarker1140"/> is used to leverage the power of SQL queries to run data queries on data stored in Spark nodes.</li>
<li><strong class="bold">Spark Streaming</strong>: The Spark Streaming<a id="_idIndexMarker1141"/> component is used to batch as well as stream data in the same application.</li>
<li><strong class="bold">Spark MLlib</strong>: Spark MLlib is the ML library used by Spark<a id="_idIndexMarker1142"/> to develop and deploy scalable ML pipelines. It is also used to perform ML analytics tasks such as feature extraction, feature engineering, dimensionality reduction, and so on.</li>
<li><strong class="bold">GraphX</strong>: The GraphX component is a library<a id="_idIndexMarker1143"/> that is used to perform data analytics on graph-based data. It is used to perform graph data construction and traversals.</li>
<li><strong class="bold">Spark R</strong>: The Spark R component is an R package that provides a front-end shell for users to communicate with Spark via the R programming language. All data processing done by R is carried out on a single node. This makes R not ideal for processing large amounts of data. The Spark R component helps users perform these data operations on huge datasets in a distributed manner by using the underlying Spark cluster.</li>
</ul>
<h2 id="_idParaDest-176"><a id="_idTextAnchor230"/>Understanding the Apache Spark architecture</h2>
<p>Apache Spark has a well-defined<a id="_idIndexMarker1144"/> architecture. As mentioned previously, Spark is run on a cluster system. Within this cluster, you will have one node that is assigned as the master while the others act as workers. All this work is performed by independent processes in the worker nodes and the combined effort is coordinated by the Spark context. </p>
<p>Refer to the following diagram to get a better understanding of the Apache Spark archit<a id="_idTextAnchor231"/>ecture:</p>
<div>
<div class="IMG---Figure" id="_idContainer286">
<img alt="Figure 12.2 – Apache Spark architecture " height="867" src="image/B17298_12_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Apache Spark architecture</p>
<p>The Spark architecture comprises the following components:</p>
<ul>
<li><strong class="bold">Spark Cluster Manager</strong>: The Spark cluster manager<a id="_idIndexMarker1145"/> is responsible for managing<a id="_idIndexMarker1146"/> the allocation of resources to nodes and monitoring their health. It is responsible for maintaining the cluster of machines on which the Spark application is running. When you start a Spark application, the cluster manager will start up different nodes in the cluster, depending on the specified configuration, and restart any services that fail in the middle of execution.</li>
</ul>
<p>The Spark cluster manager comes in three types:</p>
<ul>
<li><strong class="bold">Standalone</strong>: This is a simple cluster<a id="_idIndexMarker1147"/> manager that comes bundled with Spark and is very easy to set up and use.</li>
<li><strong class="bold">Hadoop YARN</strong>: <strong class="bold">Yet Another Resource Negotiator</strong> (<strong class="bold">YARN</strong>) is a resource manager<a id="_idIndexMarker1148"/> that comes with<a id="_idIndexMarker1149"/> the Hadoop ecosystem. Spark, being a data processing<a id="_idIndexMarker1150"/> system, can integrate with many data storage systems. <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>) is one of the most commonly used distributed filesystems in the big data industry and using Spark with HDFS has been a common setup in companies. Since YARN comes with the Hadoop ecosystem, you can use the same resource manager to manage your Spark resources.</li>
<li><strong class="bold">Kubernetes</strong>: Kubernetes is an open source container<a id="_idIndexMarker1151"/> orchestration system for automating<a id="_idIndexMarker1152"/> deployment operations, scaling services, and other forms of server management. Kubernetes is also capable of managing Spark cluster resources.</li>
</ul>
<ul>
<li><strong class="bold">Spark Driver</strong>: The Spark driver is the main program<a id="_idIndexMarker1153"/> of the Spark<a id="_idIndexMarker1154"/> application. It is responsible for controlling the execution of the application and keeps track of the different states of the nodes, as well as the tasks that have been assigned to each node. The program can be any script that you run or even the Spark interface.</li>
<li><strong class="bold">Spark Executors</strong>: The Spark executors<a id="_idIndexMarker1155"/> are the actual processes that perform<a id="_idIndexMarker1156"/> the computation task on the worker nodes. They are pretty simple processes whose aim is to take the assigned task, compute it, and then send back the results to the Spark Context.</li>
<li><strong class="bold">SparkContext</strong>: The Spark Context, as its name suggests, keeps<a id="_idIndexMarker1157"/> track of the context<a id="_idIndexMarker1158"/> of the execution. Any command that the Spark driver executes goes through this context. The Spark Context communicates with the Spark cluster manager to coordinate the execution activities with the correct executor.</li>
</ul>
<p>The Spark driver program is the primary function that manages the parallel execution of operations on a cluster. The driver program does so using a data structure called a <strong class="bold">Resilient Distributed Dataset</strong> (<strong class="bold">RDD</strong>). </p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor232"/>Understanding what a Resilient Distributed Dataset is</h2>
<p>Apache Spark is built <a id="_idIndexMarker1159"/>on the foundation of the <strong class="bold">RDD</strong>. It is a fault-tolerant record of data that resides on multiple nodes and is immutable. Everything that you do in Spark is done using an RDD. Since it is immutable, any transformation that you do eventually creates a new RDD. RDDs are partitioned into logical sets that are then distributed among the Spark nodes for execution. Spark handles all this distribution internally.</p>
<p>Let’s understand how Spark uses RDDs to perform data processing at scale. Refer to the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer287">
<img alt="Figure 12.3 – Linear RDD transformations " height="825" src="image/B17298_12_003.jpg" width="1367"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Linear RDD transformations</p>
<p>So, RDDs are immutable, which means that once the dataset has been created, it cannot be modified. So, if you want to make changes in the dataset, then Spark will create a new RDD from the existing RDD and keeps track of the changes. Here, you have your initial data stored in <strong class="bold">RDD 1</strong>, so you must assume you need to drop a column and convert the type of another column from a string into a number. Spark will create <strong class="bold">RDD 2</strong>, which will contain these changes, as well as make note of the changes it has made. Eventually, as you further transform the data, Spark will contain many RDDs.</p>
<p>You may be wondering what happens if you need to perform many transformations on the data; will Spark create that many RRDs and eventually run out of memory? Remember, RDDs are resilient and immutable, so if you have created <strong class="bold">RDD 3</strong> from <strong class="bold">RDD 2,</strong> then you will only need to keep <strong class="bold">RDD2</strong> and the data transformation process from <strong class="bold">RDD 2</strong> to <strong class="bold">RDD 3.</strong> You will no longer need <strong class="bold">RDD 1</strong> so that can be removed to free up space. Spark does all the memory management for you. It will remove any RDDs that are not needed.</p>
<p>That was a very simplified<a id="_idIndexMarker1160"/> explanation for a simple problem. What if you are creating multiple RDDs that contain different transformations from the same RDD? This can be seen in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer288">
<img alt="Figure 12.4 – Branched RDD transformations " height="996" src="image/B17298_12_004.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Branched RDD transformations</p>
<p>In this case, you will need<a id="_idIndexMarker1161"/> to keep all the RDDs. This is where Spark’s <strong class="bold">lazy evaluation</strong> comes into play. Lazy evaluation is an evaluation technique where evaluation expressions are delayed until the resultant value is needed. Let’s understand this better by looking into RDD operations. There are two types of operations:</p>
<ul>
<li><strong class="bold">Transformations</strong>: Transformations<a id="_idIndexMarker1162"/> are operations that produce<a id="_idIndexMarker1163"/> a new RDD from an existing RDD that contains changes in the dataset. These operations mostly consist of converting raw datasets into a refined final dataset that can be used to extract evaluation metrics or other processes. This mostly involves data manipulation operations such as union operations or groupby operations.</li>
<li><strong class="bold">Actions</strong>: Actions are operations that take<a id="_idIndexMarker1164"/> an RDD as input but don’t generate <a id="_idIndexMarker1165"/>a new RDD as output. The output value derived from the action operation is sent back to the driver. This mostly involves operations such as count, which returns the number of elements in the RDD, or aggregate, which performs aggregate operations on the contents of the RDD and sends the result back.</li>
</ul>
<p>Transformation operations are lazy. When performing transformation operations on an RDD, Spark will keep a note of what needs to be done but won’t do it immediately. It will only start the transformation process when it gets an action operation, hence the name lazy evaluation.</p>
<p>Let’s understand the whole<a id="_idIndexMarker1166"/> process with a simple example. Let’s assume you have an RDD that contains a raw dataset of all the employees of a company and you want to calculate the average salary of all the senior ML engineers. Your transformation operations are to filter all the ML engineers into <strong class="bold">RDD 2</strong> and then further filter by seniority into <strong class="bold">RDD 3</strong> When you pass this transformation operation to Spark, it won’t create <strong class="bold">RDD 3</strong> It will just keep a note of it. When it gets the action operation – that is, to calculate the average salary – that is when the lazy evaluation kicks in and Spark starts performing the transformation and, eventually, the action.</p>
<p>Lazy evaluation helps Spark understand what required transformation operations are needed to perform the action operation and find the most efficient way of doing the transformation while keeping the space complexity in mind.</p>
<p class="callout-heading">Tip</p>
<p class="callout">Spark is a very sophisticated and powerful technology. It provides plenty of flexibility and can be configured to best suit different kinds of data processing needs. In this chapter, we have just explored the tip of the iceberg of Apache Spark. If you are interested in understanding the capabilities of Spark to their fullest extent, I highly encourage you to explore the Apache Spark<a id="_idIndexMarker1167"/> documentation, which can be found at <a href="https://spark.apache.org/">https://spark.apache.org/</a>.</p>
<p>Now that we have a basic idea of how Spark works, let’s understand how H2O Sparkling Water combines both H2O and Spark.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor233"/>Exploring H2O Sparkling Water</h1>
<p><strong class="bold">Sparkling Water</strong> is an H2O product that combines<a id="_idIndexMarker1168"/> the fast and scalable ML of H2O with the analytics capabilities of Apache Spark. The combination of both these technologies allows users to make SQL queries for data munging, feed the results to H2O for model training, build and deploy models to production, and then use them for predictions.</p>
<p>H2O Sparkling Water is designed in a way that you can run H2O in regular Spark applications. It has provisions to run the H2O server inside of Spark executors so that the H2O server has access to all the data stored in executors for performing any ML-based computations. </p>
<p>The transparent integration<a id="_idIndexMarker1169"/> between H2O and Spark provides the following benefits:</p>
<ul>
<li>H2O algorithms, including AutoML, can be used in Spark workflows</li>
<li>Application-specific data structures can be transformed and supported between H2O and Spark</li>
<li>You can use Spark RDDs as datasets in H2O ML algorithms</li>
</ul>
<p>Sparkling Water supports two types of backends:</p>
<ul>
<li><strong class="bold">Internal Backend</strong>: In this type of setup, the H2O<a id="_idIndexMarker1170"/> application is launched inside the Spark executor once the H2O context is initialized. H2O then starts its service by initializing its key-value store and memory manager inside each of the executors. It is easy to deploy H2O Sparkling Water as an internal backend, but if Spark’s cluster manager decides to shut down any of the executors, then the H2O server running in the executor is also shut down. The internal backend is a default setup used by H2O Sparkling Water. The architecture of the internally running H2O Sparkling Water looks as follows:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer289">
<img alt="Figure 12.5 – Sparkling Water internal backend architecture " height="1069" src="image/B17298_12_005.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Sparkling Water internal backend architecture</p>
<p>As you can see, the H2O service<a id="_idIndexMarker1171"/> resides inside each of the Spark executors.</p>
<ol>
<li><strong class="bold">External Backend</strong>: In this type of setup, the H2O service<a id="_idIndexMarker1172"/> is deployed separately from the Spark executors and the communication between the H2O servers and the Spark executors is handled by the Spark driver. The architecture of H2O Sparkling Water as an external backend works as follows:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer290">
<img alt="Figure 12.6 – Sparkling Water external backend architecture " height="1214" src="image/B17298_12_006.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Sparkling Water external backend architecture</p>
<p>As you can see, the H2O cluster<a id="_idIndexMarker1173"/> is run separately from the Spark executor. The separation has benefits since the H2O clusters are no longer affected by the shutting down of Spark Executors. However, this adds the overhead of the H2O driver needing to coordinate the communication between the H2O cluster and the Spark Executors.</p>
<p>Sparkling Water, despite being built on<a id="_idIndexMarker1174"/> top of Spark, uses an H2OFrame when performing computations using the H2O server in the Sparkling Water cluster. Thus, there is a lot of data exchange and interchange between the Spark RDD and the H2OFrame.</p>
<p>DataFrames are converted between different types as follows:</p>
<ul>
<li><strong class="bold">H2OFrame into RDD</strong>: When converting an H2OFrame<a id="_idIndexMarker1175"/> into an RDD, instead of recreating the data into a different type, Sparkling Water creates a wrapper around the H2OFrame that acts like an RDD API. This wrapper interprets all RDD-based operations into identical H2OFrame operations.</li>
<li><strong class="bold">RDD into H2OFrame</strong>: Converting an RDD into an H2OFrame<a id="_idIndexMarker1176"/> involves evaluating the data in the RDD and then converting it into an H2OFrame. The data in the H2OFrame, however, is heavily compressed. Data being shared between H2O and Spark depends on the type of backend used for deployment.</li>
<li><strong class="bold">Data Sharing in Internal Sparkling Water Backend</strong>: In the internal Sparkling<a id="_idIndexMarker1177"/> Water backend, since<a id="_idIndexMarker1178"/> the H2O service is launched inside the Spark Executor, both the Spark service<a id="_idIndexMarker1179"/> and the H2O service inside the executor use the same <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) and as such, the data is accessible to both the services. The following diagram shows the process of data sharing in the internal Sparkling Water backend:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer291">
<img alt="Figure 12.7 – Data sharing in the internal Sparkling Water backend " height="940" src="image/B17298_12_007.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Data sharing in the internal Sparkling Water backend</p>
<p>Since both services <a id="_idIndexMarker1180"/>are on the same executor, you need<a id="_idIndexMarker1181"/> to consider memory when converting DataFrames between the two types. You will need to allocate enough memory for both Spark and H2O to perform their respective operations. Spark will need the minimum memory of your dataset, plus additional memory for any transformations that you wish to perform. Also, converting RDDs into H2OFrames will lead to duplication of data, so it’s recommended that a 4x bigger dataset should be used for H2O. </p>
<ul>
<li><strong class="bold">Data Sharing in External Sparkling Water Backend</strong>: In the external<a id="_idIndexMarker1182"/> Sparkling Water <a id="_idIndexMarker1183"/>backend, the H2O service is launched in a cluster that is separate from the Spark Executor. So, there is an added overhead of transferring the data from one cluster to another over the network. The following diagram should help you understand this:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="Figure 12.8 – Data sharing in the external Sparkling Water backend " height="284" src="image/B17298_12_008.jpg" width="570"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Data sharing in the external Sparkling Water backend</p>
<p>Since both services<a id="_idIndexMarker1184"/> reside in their own<a id="_idIndexMarker1185"/> cluster (if you have allocated enough memory to the respective clusters), you don’t need to worry about memory constraints.</p>
<p class="callout-heading">Tip</p>
<p class="callout">H2O Sparkling Water can be run on different types of platforms in various kinds of ways. If you are interested in learning<a id="_idIndexMarker1186"/> more about the various ways in which you can deploy H2O Sparkling Water, as well as getting more information about its backends, then feel free to check out <a href="https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml">https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml</a>. </p>
<p>Now that we know how H2O Sparkling Water works, let’s see how we can download and install it.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor234"/>Downloading and installing H2O Sparkling Water</h2>
<p>H2O Sparkling Water has some specific requirements that need to be satisfied before you can install it on your system. The requirements<a id="_idIndexMarker1187"/> for installing H2O Sparkling Water version 3.36 are as follows:</p>
<ul>
<li><strong class="bold">Operating System</strong>: H2O Sparkling Water is only supported for Linux, macOS, and Windows.</li>
<li><strong class="bold">Java Version</strong>: H2O Sparkling Water supports all Java versions above Java 1.8.</li>
<li><strong class="bold">Python Version</strong>: If you plan to use the Python version of Sparkling Water, known as PySparkling, then you will need a Python<a id="_idIndexMarker1188"/> version abo<a id="_idTextAnchor235"/>ve 3.6 installed on your system.</li>
<li><strong class="bold">H2O Version</strong>: H2O Sparkling Water version 3.36.1 requires the same version of H2O installed on your system. However, H2O Sparkling Water comes prepackaged with a compatible H2O version, so you don’t need to separately install H2O to use H2O Sparkling Water.</li>
<li><strong class="bold">Spark Version</strong>: H2O Sparkling Water version 3.36.1 strictly supports Spark 3.2. Any Spark version above or below version 3.2 may cause issues with installation or how H2O Sparkling Water works. Spark 3.2 has its own dependencies, which are as follows:<ul><li><strong class="bold">Java Version</strong>: Spark 3.2 strictly supports Java 8 and Java 11</li><li><strong class="bold">Scala Version</strong>: Spark 3.2 strictly runs on Scala 2.12/2.13</li><li><strong class="bold">R Version</strong>: Spark 3.2 supports any R version above 3.5</li><li><strong class="bold">Python Version</strong>: Spark 3.2 supports any Python version above 3.6</li></ul></li>
<li><strong class="bold">Environment Variables</strong>: You will need to set the <strong class="source-inline">SPARK_HOME</strong> environment variable to point to your local Spark 3.2 installation.</li>
</ul>
<p>Now, let’s set up our system<a id="_idIndexMarker1189"/> so that we can download<a id="_idIndexMarker1190"/> and install H2O Sparkling Water. Follow these steps to set up H2O Sparkling Water:</p>
<ol>
<li value="1">We will start by installing<a id="_idIndexMarker1191"/> Java 11, which is needed for both Spark and H2O Sparkling<a id="_idIndexMarker1192"/> Water. Even though Spark supports Java 8 as well, it is preferable to use Java 11 since it is the newer version with improvements and security patches. You can download and install Java 11 by executing the following command:<p class="source-code"><strong class="bold">sudo apt-get install openjdk-11-jdk</strong></p></li>
<li>Optionally, if you wish to use the PySparkling Python interpreter, then install Python version 3.10. You can do so by executing the following command:<p class="source-code"><strong class="bold">sudo apt install python3</strong></p></li>
<li>Now that we have the basic languages installed, let’s go ahead and download and install Spark version 3.2. You can download the specific version for Spark from the Apache<a id="_idIndexMarker1193"/> Software Foundation official download page (https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz) or by directly running the following command in your Terminal:<p class="source-code"><strong class="bold">wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz</strong></p></li>
</ol>
<p>If you are using the <strong class="bold">Maven project</strong>, then you can directly specify the Spark<a id="_idIndexMarker1194"/> core Maven dependency, as follows:</p>
<p class="source-code">&lt;dependency&gt;</p>
<p class="source-code">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</p>
<p class="source-code">    &lt;artifactId&gt;spark-core_2.13&lt;/artifactId&gt;</p>
<p class="source-code">    &lt;version&gt;3.1.2&lt;/version&gt;</p>
<p class="source-code">&lt;/dependency&gt;</p>
<p>You can find the<a id="_idIndexMarker1195"/> Maven repository for Spark at <a href="https://mvnrepository.com/artifact/org.apache.spark/spark-core">https://mvnrepository.com/artifact/org.apache.spark/spark-core</a>.</p>
<ol>
<li value="4">Then, you can extract the <strong class="source-inline">.tar</strong> file by executing the following command in your Terminal:<p class="source-code"><strong class="bold">sudo tar xzvf spark-*</strong></p></li>
<li>Now that we have extracted<a id="_idIndexMarker1196"/> the Spark binaries, let’s set our environment<a id="_idIndexMarker1197"/> variables, as follows:<p class="source-code">export SPARK_HOME="/path/to/spark/installation"</p></li>
<li>We must also set the <strong class="source-inline">MASTER</strong> environment variable to <strong class="source-inline">local[*]</strong> to launch a local Spark cluster:<p class="source-code">export MASTER="local[*]"</p></li>
<li>Now that we have all the dependencies of H2O Sparkling Water installed and ready, let’s go ahead and download<a id="_idIndexMarker1198"/> H2O Sparkling Water. You can download the latest version from <a href="https://h2o.ai/products/h2o-sparkling-water/">https://h2o.ai/products/h2o-sparkling-water/</a>. Upon clicking the <strong class="bold">Download Latest</strong> button, you should be redirected to the H2O Sparkling Water repository website, where you can download the H2O Sparkling Water version <em class="italic">3.36</em> ZIP file.</li>
<li>Once the download has finished, you can unzip the ZIP file by executing the following command in your Terminal:<p class="source-code">unzip sparkling-water-*</p></li>
<li>You can see if everything is working fine by starting the H2O Sparkling Water shell by executing the following command inside your Sparkling Water installation folder:<p class="source-code">bin/sparkling-shell</p></li>
<li>By doing this, you can see if Sparkling Water has integrated with Spark by starting an H2O cloud inside the Spark cluster. You can do so by executing the following commands inside <strong class="source-inline">sparkling-shell</strong>:<p class="source-code">import ai.h2o.sparkling._</p><p class="source-code">val h2oContext = H2OContext.getOrCreate()</p></li>
</ol>
<p>You should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer293">
<img alt="Figure 12.9 – Successfully starting up H2O Sparkling Water " height="441" src="image/B17298_12_009.jpg" width="622"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Successfully starting up H2O Sparkling Water</p>
<p>Now that we have<a id="_idIndexMarker1199"/> successfully downloaded and installed both Spark<a id="_idIndexMarker1200"/> and H2O Sparkling Water and ensured that both are working correctly, there is some general recommended tuning<a id="_idIndexMarker1201"/> that you must do, as per H2O.ai’s documentation. Let’s take a look:</p>
<ul>
<li>Increase the available memory for the Spark driver as well as the Spark Executors from the default value of <strong class="bold">1G</strong> to <strong class="bold">4G</strong>. You can do so by passing the following <strong class="source-inline">config</strong> parameter when starting the Sparkling shell:<p class="source-code">bin/sparkling-shell --conf spark.executor.memory=4g spark.driver.memory=4g</p></li>
</ul>
<p>If you are using YARN or your cluster manager, then use <strong class="source-inline">config spark.yarn.am.memory</strong> instead of <strong class="source-inline">spark.driver.memory</strong>. You can also set these values as default configuration properties by setting the values in the <strong class="source-inline">spark-defaults.conf</strong> file. This can be found among your Spark installation files.</p>
<ul>
<li>Along with cluster <a id="_idIndexMarker1202"/>memory, it is also recommended to increase the PermGen size of your Spark nodes. The default PermGen<a id="_idIndexMarker1203"/> size often proves to be very small and can lead to <strong class="source-inline">OutOfMemoryError</strong>. <strong class="bold">PermGen</strong> is a special heap space that is separate from the main memory heap. It is used by the JVM to keep track of loaded class metadata. You can set this value using the <strong class="source-inline">spark.driver.extraJavaOptions</strong> and <strong class="source-inline">spark.executor.extraJavaOptions</strong> configuration options, as follows:<p class="source-code">bin/sparkling-shell --conf spark.driver.extraJavaOptions -XX:MaxPermSize=384 -XX:PermSize=384m spark.executor.extraJavaOptions -XX:MaxPermSize=384 -XX:PermSize=384m</p></li>
<li>It is also recommended to keep your cluster homogeneous – that is, both the Spark driver and Executor have the same amount of resources allocated to them.</li>
<li>The following configurations are also recommended to speed up and stabilize the creation of H2O services on top of the Spark cluster:<ul><li>Increase the number of seconds to wait for a task launched in data-local mode so that H2O tasks are processed locally with data. You can set this as follows:<p class="source-code">bin/sparkling-shell --conf spark.locality.wait=3000</p></li><li>Enforcing Spark starts scheduling jobs only when it is allocated 100% of its resources:<p class="source-code">bin/sparkling-shell --conf spark.scheduler.minRegisteredResourcesRatio=1</p></li><li>Don’t retry failed tasks:<p class="source-code">bin/sparkling-shell --conf spark.task.maxFailures=1</p></li><li>Set the interval between<a id="_idIndexMarker1204"/> each executor’s heartbeats to the driver to less than Spark’s network timeout – that is <strong class="source-inline">spark.network.timeout</strong> – whose default value is <em class="italic">120 seconds</em>. So, set the heartbeat value to around <em class="italic">10 seconds</em>:<p class="source-code">bin/sparkling-shell --conf spark.executor.heartbeatInterval=10s</p></li></ul></li>
</ul>
<p>Now that we have appropriately configured Spark and H2O Sparkling Water, let’s see how we can use these technologies to solve an ML problem using both Spark and H2O AutoML.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor236"/>Implementing Spark and H2O AutoML using H2O Sparkling Water</h2>
<p>For this experiment, we will<a id="_idIndexMarker1205"/> be using<a id="_idIndexMarker1206"/> the Concrete<a id="_idIndexMarker1207"/> Compressive Strength<a id="_idIndexMarker1208"/> dataset. You can<a id="_idIndexMarker1209"/> find this dataset at https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength.</p>
<p>Here are more details on the dataset: I-Cheng Yeh, <em class="italic">Modeling of strength of high performance concrete using artificial neural networks</em>, Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998).</p>
<p>Let’s start by understanding the problem statement we will be working with. </p>
<h3>Understanding the problem statement</h3>
<p>The Concrete Compressive Strength dataset<a id="_idIndexMarker1210"/> is a dataset that consists of <em class="italic">1,030</em> data points consisting of the following features:</p>
<ul>
<li><strong class="bold">Cement</strong>: This feature denotes the amount of cement added in kg in m3 of the mixture</li>
<li><strong class="bold">Blast Furnace Slag</strong>: This feature denotes the amount of slag added in kgs in m3 of the mixture</li>
<li><strong class="bold">Fly Ash</strong>: This feature denotes the amount of fly ash added in kgs in m3 of the mixture</li>
<li><strong class="bold">Water</strong>: This feature denotes the amount of water added in kgs in m3 of the mixture</li>
<li><strong class="bold">Superplasticizer</strong>: This feature denotes the amount of superplasticizer added in kgs in m3 of the mixture</li>
<li><strong class="bold">Coarse Aggregate</strong>: This feature denotes the amount of coarse aggregate – in other words, stone – added in kgs in m3 of the mixture</li>
<li><strong class="bold">Fine Aggregate</strong>: This feature denotes the amount of fine aggregate – in other words, sand – added in kgs in m3 of the mixture</li>
<li><strong class="bold">Age</strong>: This feature denotes the age of the cement</li>
<li><strong class="bold">Concrete compressive strength</strong>: This feature denotes the compressive strength of the concrete in <strong class="bold">Megapascals</strong> (<strong class="bold">MPa</strong>)</li>
</ul>
<p>The ML problem<a id="_idIndexMarker1211"/> is to use all the features to predict the compressive strength of the concrete. </p>
<p>The content of the dataset is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer294">
<img alt="Figure 12.10 – Concrete Compressive Strength dataset sample " height="434" src="image/B17298_12_010.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Concrete Compressive Strength dataset sample</p>
<p>So, let’s see how we can solve <a id="_idIndexMarker1212"/>this problem using H2O Sparkling Water. Fi<a id="_idTextAnchor237"/>rst, we shall learn how to train models using H2O AutoML and Spark.</p>
<h3>Running AutoML training in Sparkling Water</h3>
<p>Once you have successfully<a id="_idIndexMarker1213"/> installed both Spark 3.2 and H2O Sparkling Water, as well as<a id="_idIndexMarker1214"/> set the correct environment variables (<strong class="source-inline">SPARK_HOME</strong> and <strong class="source-inline">MASTER</strong>), you can start the model training process.</p>
<p>Follow these steps:</p>
<ol>
<li value="1">Start the Sparkling shell by executing the command inside the H2O Sparkling Water extracted folder:<p class="source-code">./bin/sparkling-shell</p></li>
</ol>
<p>This should start a Scala shell in your Terminal. The output should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer295">
<img alt="Figure 12.11 – Scala shell for H2O Sparkling Water " height="751" src="image/B17298_12_011.jpg" width="1586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Scala shell for H2O Sparkling Water</p>
<p>You can also perform <a id="_idIndexMarker1215"/>the same experiment in Python <a id="_idIndexMarker1216"/>using the <strong class="source-inline">PySparkling</strong> shell. You can start the <strong class="source-inline">PySparkling</strong> shell by executing the following command:</p>
<p class="source-code">./bin/PySparkling</p>
<p>You should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer296">
<img alt="Figure 12.12 – Python shell for H2O Sparkling Water " height="239" src="image/B17298_12_012.jpg" width="683"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Python shell for H2O Sparkling Water</p>
<ol>
<li value="2">Now, we need to start an H2O cluster<a id="_idIndexMarker1217"/> inside the Spark environment. We can<a id="_idIndexMarker1218"/> do this by creating an H2OContext and then executing its <strong class="source-inline">getOrCreate()</strong>function. So, execute the following code in your Sparkling shell to import the necessary dependencies and execute the H2O context code:<p class="source-code">import ai.h2o.sparkling._</p><p class="source-code">import java.net.URI</p><p class="source-code">val h2oContext = H2OContext.getOrCreate()</p></li>
</ol>
<p>In the PySparkling shell, the code will be as follows:</p>
<p class="source-code">from PySparkling import *</p>
<p class="source-code">h2oContext = H2OContext.getOrCreate()</p>
<p>You should get an output similar to the following that states that your H2O context has been created:</p>
<div>
<div class="IMG---Figure" id="_idContainer297">
<img alt="Figure 12.13 – H2O context created successfully " height="209" src="image/B17298_12_013.jpg" width="789"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – H2O context created successfully</p>
<ol>
<li value="3">Now, we must ensure that our Concrete Compressive Strength dataset can be downloaded on every node using Spark’s built-in file I/O system. So, execute the following commands to import your dataset:<p class="source-code">import org.apache.spark.SparkFiles</p><p class="source-code">spark.sparkContext.addFile("/home/salil/Downloads/Concrete_Data.csv")</p></li>
</ol>
<p>In the PySparkling shell, we must<a id="_idIndexMarker1219"/> import the dataset<a id="_idIndexMarker1220"/> using H2O’s <strong class="source-inline">import</strong> function. The Python code will be as follows:</p>
<p class="source-code">import h2o</p>
<p class="source-code">h2oFrame = h2o.import_file("/home/salil/Downloads/Concrete_Data.csv")</p>
<ol>
<li value="4">Once added, we must parse the dataset into a Spark Dataframe by executing the following commands in the Scala shell:<p class="source-code">val sparkDataFrame = spark.read.option("header", "true").option("inferSchema", "true").csv(SparkFiles.get("Concrete_Data.csv"))</p></li>
</ol>
<p>In the PySparkling shell, the equivalent code will be as follows:</p>
<p class="source-code">sparkDataFrame = hc.asSparkFrame(h2oFrame)</p>
<ol>
<li value="5">Now, <strong class="source-inline">sparkDataFrame</strong> contains the dataset as a Spark DataFrame. So, let’s perform a train-test split on it to split the DataFrame into testing and training DataFrames. You can do so by executing the following commands in the Sparkling shell:<p class="source-code">val Array(trainingDataFrame, testingDataFrame) = sparkDataFrame.randomSplit(Array(0.7, 0.3), seed=123)</p></li>
</ol>
<p>In the PySparkling shell, execute the following command:</p>
<p class="source-code">[trainingDataFrame, testingDataFrame] = sparkDataFrame.randomSplit([0.7, 0.3], seed=123)</p>
<ol>
<li value="6">We now have <strong class="source-inline">trainingDataFrame</strong> and <strong class="source-inline">testingDataFrame</strong> ready for training and testing, respectively. Let’s create an H2OAutoML instance to auto-train models on <strong class="source-inline">trainingDataFrame</strong>. Execute the following commands to instantiate an H2O AutoML object:<p class="source-code">import ai.h2o.sparkling.ml.algos.H2OAutoML</p><p class="source-code">val aml = new H2OAutoML()</p></li>
</ol>
<p>In PySparkling, when initializing the H2O AutoML object, we also set the label column. The code for this is as follows:</p>
<p class="source-code">from PySparkling.ml import H2OAutoML</p>
<p class="source-code">aml = H2OAutoML(labelCol=" Concrete compressive strength ")</p>
<ol>
<li value="7">Let’s see how we can<a id="_idIndexMarker1221"/> set the label of the dataset<a id="_idIndexMarker1222"/> so that the AutoML object is aware of which columns from the DataFrame are to be predicted in the Scala shell. Execute the following command:<p class="source-code">aml.setLabelCol("Concrete compressive strength")</p></li>
</ol>
<p>H2O will treat all the columns of the DataFrame as features unless explicitly specified. It will, however, ignore columns that are set as <strong class="bold">labels</strong>, <strong class="bold">fold columns</strong>, <strong class="bold">weights</strong>, or any other explicitly set ignored columns.</p>
<p>H2O AutoML distinguishes between regression and classification problems depending on the type of the response<a id="_idIndexMarker1223"/> column. If the response column is a string, then H2O AutoML assumes it is a <strong class="bold">classification problem</strong>, whereas if the response column is numerical, then H2O AutoML assumes that it is a <strong class="bold">regression problem</strong>. You can override this default behavior by explicitly<a id="_idIndexMarker1224"/> specifying this during instantiation by either instantiating the <strong class="source-inline">ai.h2o.sparkling.ml.algos.classification.H2OAutoMLClassifier</strong> object or the <strong class="source-inline">ai.h2o.sparkling.ml.algos.regression.H2OAutoMLRegressor</strong> object instead of <strong class="source-inline">ai.h2o.sparkling.ml.algos.H2OautoML</strong>, as we did in this example.</p>
<ol>
<li value="8">Now, let’s limit AutoML model training to only 10 models. Execute the following command:<p class="source-code">aml.setMaxModels(10)</p></li>
</ol>
<p>The equivalent Python syntax for this code is the same, so execute this same command in your PySparkling shell.</p>
<ol>
<li value="9">Once we have our AutoML object all set up, the only thing remaining is to trigger the training. To do so, execute the following command:<p class="source-code">val model = aml.fit(trainingDataFrame)</p></li>
</ol>
<p>The equivalent code<a id="_idIndexMarker1225"/> for Python<a id="_idIndexMarker1226"/> is as follows:</p>
<p class="source-code">model = aml.fit(trainingDataFrame)</p>
<p>Once training is finished, you should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer298">
<img alt="Figure 12.14 – H2O AutoML result in H2O Sparkling Water " height="445" src="image/B17298_12_014.jpg" width="552"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.14 – H2O AutoML result in H2O Sparkling Water</p>
<p>As you can see, we got a stacked ensemble model as the leader model with the model key below it. Below <strong class="bold">Model Key</strong> is <strong class="bold">Model summary</strong>, which contains the training and cross-validation metrics.</p>
<p>As we did in <a href="B17298_02.xhtml#_idTextAnchor038"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with H2O Flow (H2O’s Web UI)</em>, we have not set the sort metric<a id="_idIndexMarker1227"/> for the <strong class="source-inline">aml</strong> object, so by default, H2O AutoML will<a id="_idIndexMarker1228"/> use the default metrics. This will be <strong class="source-inline">deviance</strong> since it is a <strong class="bold">regression problem</strong>. You can explicitly<a id="_idIndexMarker1229"/> set the sort metric using <strong class="source-inline">automl.setSortMetric()</strong>and pass in the sort metric of your choice.</p>
<ol>
<li value="10">You can also get a detailed view of the model by using the <strong class="source-inline">getModelDetails()</strong>function. Execute the following command:<p class="source-code">model.getModelDetails()</p></li>
</ol>
<p>This command will work on both the PySparkling and Scala shells and will output very detailed JSON about the model’s metadata.</p>
<ol>
<li value="11">You can also view the AutoML leaderboard by executing the following command:<p class="source-code">val leaderboard = aml.getLeaderboard()</p><p class="source-code">leaderboard.show(false)</p></li>
</ol>
<p>The equivalent Python code for the PySparkling shell is as follows:</p>
<p class="source-code">leaderboard = aml.getLeaderboard("ALL")</p>
<p class="source-code">leaderboard.show(truncate = False)</p>
<p>You should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer299">
<img alt="Figure 12.15 – H2O AutoML leaderboard in H2O Sparkling Water " height="290" src="image/B17298_12_015.jpg" width="1456"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.15 – H2O AutoML leaderboard in H2O Sparkling Water</p>
<p>This will display the leaderboard<a id="_idIndexMarker1230"/> containing all the models<a id="_idIndexMarker1231"/> that have been trained and ranked based on the sort metric. </p>
<ol>
<li value="12">Making predictions using H2O Sparkling Water is also very easy. The prediction functionality is wrapped behind a simple, easy-to-use wrapper function called <strong class="source-inline">transform</strong>. Execute the following code to make predictions on the testing DataFrame:<p class="source-code">model.transform(testingDataFrame).show(false)</p></li>
</ol>
<p>In the PySparkling shell, it is slightly different. Here, you must execute the following code:</p>
<p class="source-code">model.transform(testingDataFrame).show(truncate = False)</p>
<p>You should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer300">
<img alt="Figure 12.16 – Prediction results combined with the testing DataFrame " height="882" src="image/B17298_12_016.jpg" width="1596"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.16 – Prediction results combined with the testing DataFrame</p>
<p>The output of the <strong class="source-inline">transform</strong><a id="_idIndexMarker1232"/> function shows the entire <strong class="bold">testDataFrame</strong> with two additional columns on the right-hand<a id="_idIndexMarker1233"/> side called <strong class="bold">detailed_prediction</strong> and <strong class="bold">prediction</strong>.</p>
<ol>
<li value="13">Now, let’s download this model as a MOJO so that we can use it for the next experiment, where we shall see how H2O Sparkling Water loads and uses MOJO models. Execute the following command:<p class="source-code">model.write.save("model_dir")</p></li>
</ol>
<p>This command is the same for both the Scala and Python shells and should download the model MOJO in your specified path. If you are using the Hadoop filesystem as your Spark data storage engine, then the command uses HDFS by default.</p>
<p>Now that we know how to import <a id="_idIndexMarker1234"/>a dataset, train models, and make<a id="_idIndexMarker1235"/> predictions using H2O Sparkling Water, let’s take it one step further and see how we can reuse existing model binaries, also called MOJOs, by loading them into H2O Sparkling Water and making predictions on them.</p>
<h3>Making predictions using model MOJOs in H2O Sparkling Water</h3>
<p>When you train models<a id="_idIndexMarker1236"/> using H2O Sparkling Water, the models<a id="_idIndexMarker1237"/> that are generated are always of the MOJO type. H2O Sparkling Water can load model MOJOs generated by H2O-3 and is also backward compatible with the different versions of H2O-3. You do not need to create an H2O context to use model MOJOs for predictions, but you do need a scoring environment. Let’s understand this by completing an experiment.</p>
<p>Follow these steps:</p>
<ol>
<li value="1">To make predictions using imported model MOJOs, you need a scoring environment. We can create a scoring environment in two ways; let’s take a look:<ol><li>Use Sparkling Water prepared scripts, which set all the dependencies that are needed to load MOJOs and make predictions on the Spark classpath. Refer to the following commands:</li></ol></li>
</ol>
<p>The following command is for a Scala shell:</p>
<p class="source-code">./bin/spark-shell --jars jars/sparkling-water-assembly-scoring_2.12-3.36.1.3-1-3.2-all.jar</p>
<p>The following command is for a Python shell:</p>
<p class="source-code">./bin/pyspark --py-files py/h2o_PySparkling_scoring_3.2-3.36.1.3-1-3.2.zip</p>
<ol>
<li value="2">Use Spark directly and set the dependencies manually.</li>
</ol>
<ol>
<li value="2">Once we have our scoring environment set up, we can load the model MOJOs. Model MOJOs loaded into Sparkling Water are immutable. So, making any configuration changes is not possible once you have loaded the model. However, you can set the configurations before you load the model. You can do so by using the <strong class="source-inline">H2OMOJOSettings()</strong>function. Refer to the following example:<p class="source-code">import ai.h2o.sparkling.ml.models._</p><p class="source-code">val modelConfigurationSettings = H2OMOJOSettings(convertInvalidNumbersToNa = true, convertUnknownCategoricalLevelsToNa = true)</p></li>
</ol>
<p>For PySparkling, refer<a id="_idIndexMarker1238"/> to the following<a id="_idIndexMarker1239"/> code:</p>
<p class="source-code">from PySparkling.ml import *</p>
<p class="source-code">val modelConfigurationSettings = H2OMOJOSettings(convertInvalidNumbersToNa = true, convertUnknownCategoricalLevelsToNa = true)</p>
<ol>
<li value="3">Once you have set the configuration settings, you can load the model MOJO using the <strong class="source-inline">createFromMojo()</strong> function from the <strong class="source-inline">H2OMOJOModel</strong> library. So, execute the following code to load the model MOJO that you created in the <em class="italic">Running AutoML training in Sparkling Water</em> section and pass the configuration settings:<p class="source-code">val loadedModel = H2OMOJOModel.createFromMojo("model_dir/model_mojo", modelConfigurationSettings)</p></li>
</ol>
<p>The Python equivalent is as follows:</p>
<p class="source-code">loadedModel = H2OMOJOModel.createFromMojo("model_dir/ model_mojo", modelConfigurationSettings)</p>
<p>If you specify the model MOJO path as a relative path and if HDFS is enabled, Sparkling Water will check the HDFS home directory; otherwise, it will search for it from the current directory. You can also pass an absolute path to your model MOJO file.</p>
<p>You can also manually specify where you want to load your model MOJO. For the HDFS filesystem, you can use the following command:</p>
<p class="source-code">loadedModel = H2OMOJOModel.createFromMojo("hdfs:///user/salil/ model_mojo")</p>
<p>For a local filesystem, you can use the following command:</p>
<p class="source-code">loadedModel = H2OMOJOModel.createFromMojo("file:///Users/salil/some_ model_mojo")</p>
<ol>
<li value="4">Once successfully loaded, you <a id="_idIndexMarker1240"/>can simply use the model<a id="_idIndexMarker1241"/> to make predictions, as we did in the <em class="italic">Running AutoML training in Sparkling Water</em> section. So, execute the following command to make predictions using your recently loaded model MOJO:<p class="source-code">val predictionResults = loadedModel.transform(testingDataframe)</p></li>
<li>The prediction results are stored as another Spark DataFrame. So, to view the prediction values, we can just display the prediction results by executing the following command:<p class="source-code">predictionResults.show()</p></li>
</ol>
<p>You should get an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer301">
<img alt="Figure 12.17 – Prediction results from the model MOJO " height="452" src="image/B17298_12_017.jpg" width="1493"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.17 – Prediction results from the model MOJO</p>
<p>As you can see, we had specifically set <strong class="source-inline">withDetailedPredictionCol</strong> to <strong class="source-inline">False</strong> when loading the MOJO. That is why we can’t see the detailed <strong class="source-inline">_prediction_column</strong> in the prediction results.</p>
<p class="callout-heading">Tip</p>
<p class="callout">There are plenty of configurations that you can set up when loading H2O model MOJOs into Sparkling Water. There are also additional methods<a id="_idIndexMarker1242"/> available for MOJO models that can help gather more information about your model MOJO. All these details can be found on H2O’s official documentation page at <a href="https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/deployment/load_mojo.xhtml#loading-and-usage-of-h2o-3-mojo-model">https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/deployment/load_mojo.xhtml#loading-and-usage-of-h2o-3-mojo-model</a>.</p>
<p>Congratulations – you have just<a id="_idIndexMarker1243"/> learned how to use<a id="_idIndexMarker1244"/> Spark and H2O AutoML together using H2O Sparkling Water.</p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor238"/>Summary</h1>
<p>In this chapter, we learned how to use H2O AutoML with Apache Spark using an H2O system called H2O Sparkling Water. We started by understanding what Apache Spark is. We investigated the various components that make up the Spark software. Then, we dived deeper into its architecture and understood how it uses a cluster of computers to perform data analysis. We investigated the Spark cluster manager, the Spark driver, Executor, and also the Spark Context. Then, we dived deeper into RDDs and understood how Spark uses them to perform lazy evaluations on transformation operations on the dataset. We also understood that Spark is smart enough to manage its resources efficiently and remove any unused RDDs during operations.</p>
<p>Building on top of this knowledge of Spark, we started exploring what H2O Sparkling Water is and how it uses Spark and H2O together in a seamlessly integrated system. We then dove deeper into its architecture and understood its two types of backends that can be used to deploy the system. We also understood how it handles data interchange between Spark and H2O.</p>
<p>Once we had a clear idea of what H2O Sparkling Water was, we proceeded with the practical implementation of using the system. We learned how to download and install the system and the strict dependencies it needs to run smoothly. We also explored the various configuration tunings that are recommended by H2O.ai when starting H2O Sparkling Water. Once the system was up and running, we performed an experiment where we used the Concrete Compressive Strength dataset to make predictions on the compressive strength of concrete using H2O Sparkling Water. We imported the dataset into a Spark cluster, performed AutoML using H2O AutoML, and used the leading model to make predictions. Finally, we learned how to export and import model MOJOs into H2O Sparkling Water and use them to make predictions.</p>
<p>In the next chapter, we shall explore a few case studies conducted by H2O.ai and understand the real-world implementation of H2O by businesses and how H2O helped them solve their ML problems.</p>
</div>
<div>
<div id="_idContainer303">
</div>
</div>
</div></body></html>