<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Machine Learning Definitions and Concepts</h1>
            </header>

            <article>
                
<p>This chapter offers a high-level definition and explanation of the machine learning concepts needed to use the Amazon Machine Learning (Amazon ML) service and fully understand how it works. The chapter has three specific goals:</p>
<ul>
<li>Listing the main techniques to improve the quality of predictions used when dealing with raw data. You will learn how to deal with the most common types of data problems. Some of these techniques are available in Amazon ML, while others aren't.</li>
<li>Presenting the predictive analytics workflow and introducing the concept of cross validation or how to split your data to train and test your models.</li>
<li>Showing how to detect poor performance of your model and presenting strategies to improve these performances.</li>
</ul>
<p>The reader will learn the following:</p>
<ul>
<li>How to spot common problems and anomalies within a given dataset</li>
<li>How to extract the most information out of a dataset in order to build robust models</li>
<li>How to detect and improve upon poor predictive performance</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What's an algorithm? What's a model?</h1>
            </header>

            <article>
                
<p>Before we dive into data munging, let's take a moment to explain the difference between an algorithm and a model, two terms we've been using up until now without a formal definition.</p>
<p>Consider the simple linear regression example we saw in <span class="ChapterrefPACKT"><a href="767f8b14-c3f2-4e45-bfcd-bb45ae2b9e65.xhtml" target="_blank">Chapter 1</a></span>, <em>Introduction to Machine Learning and Predictive Analytics —</em> the linear regression equation with one predictor:</p>
<div class="CDPAlignCenter CDPAlign"><img height="18" src="assets/image_02_001.png" width="89"/></div>
<p>Here, <em>x</em> is the variable, <em>ŷ</em> the prediction, not the real value, and <em>(a,b)</em> the parameters of the linear regression model:</p>
<ul>
<li>The conceptual or theoretical model is the representation of the data that is the most adapted to the actual dataset. It is chosen at the beginning by the data scientist. In this case, the conceptual model is the linear regression model, where the prediction is a linear combination of a variable. Other conceptual models include decision trees, naive bayes, neural networks, and so on. All these models have parameters that need to be tuned to the actual data.</li>
<li>The algorithm is the computational process that will calculate the optimal parameters of the conceptual model. In our simple linear regression case, the algorithm will calculate the optimal parameters <em>a</em> and <em>b</em>. Here optimal means that it gives the best predictions given the available dataset.</li>
<li>Finally, the predictive model corresponds to the conceptual model associated with the optimal parameters found for the available dataset.</li>
</ul>
<p>In reality, no one explicitly distinguishes between the conceptual model and the predictive model. Both are called the model.</p>
<p>In short, the algorithm is the method of learning, and the model is what results form the learning phase. The model is the conceptual model (trees, svm, linear) trained by the algorithm on your training dataset.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dealing with messy data</h1>
            </header>

            <article>
                
<p>As the dataset grows, so do inconsistencies and errors. Whether as a result of human error, system failure, or data structure evolutions, real-world data is rife with invalid, absurd, or missing values. Even when the dataset is spotless, the nature of some variables need to be adapted to the model. We look at the most common data anomalies and characteristics that need to be corrected in the context of Amazon ML linear models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classic datasets versus real-world datasets</h1>
            </header>

            <article>
                
<p>Data scientists and machine-learning practitioners often use classic datasets to demonstrate the behavior of certain models. The <strong>Iris</strong> dataset, composed of 150 samples of three types of iris flowers, is one of the most commonly used to demonstrate or to teach predictive analytics. It has been around since 1936!</p>
<p>The <strong>Boston housing</strong> dataset and the <strong>Titanic</strong> dataset are other very popular datasets for predictive analytics. For text classification, the <strong>Reuters</strong> or the <strong>20 newsgroups</strong> text datasets are very common, while image recognition datasets are used to benchmark deep learning models. These classic datasets are used to establish baselines when evaluating the performances of algorithms and models. Their characteristics are well known, and data scientists know what performances to expect.</p>
<p>These classic datasets can be downloaded:</p>
<ul>
<li><strong>Iris</strong>: <a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">http://archive.ics.uci.edu/ml/datasets/Iris</a></li>
<li><strong>Boston housing</strong>: <a href="https://archive.ics.uci.edu/ml/datasets/Housing" target="_blank">https://archive.ics.uci.edu/ml/datasets/Housing</a></li>
<li><strong>Titanic dataset</strong>: <a href="https://www.kaggle.com/c/titanic" target="_blank">https://www.kaggle.com/c/titanic</a> or <a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/" target="_blank">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/</a></li>
<li><strong>Reuters</strong>: <a href="https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection" target="_blank">https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection</a></li>
<li><strong>20 newsgroups</strong>: <a href="http://scikit-learn.org/stable/datasets/twenty_newsgroups.html" target="_blank">http://scikit-learn.org/stable/datasets/twenty_newsgroups.html</a></li>
<li><strong>Image recognition and deep learning</strong>: <a href="http://deeplearning.net/datasets/" target="_blank">http://deeplearning.net/datasets/</a></li>
</ul>
<p>However, classic datasets can be weak equivalents of real datasets, which have been extracted and aggregated from a diverse set of sources: databases, APIs, free form documents, social networks, spreadsheets, and so on. In a real-life situation, the data scientist must often deal with messy data that has missing values, absurd outliers, human errors, weird formatting, strange inputs, and skewed distributions.</p>
<p>The first task in a predictive analytics project is to clean up the data. In the following section, we will look at the main issues with raw data and what strategies can be applied. Since we will ultimately be using a linear model for our predictions, we will process the data with that in mind.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Assumptions for multiclass linear models</h1>
            </header>

            <article>
                
<p>For a linear model to offer reliable predictions, predictors must satisfy a certain number of conditions. These conditions are known as the <em>Assumptions of Multiple Linear Regression</em> (<span class="URLPACKT"><a href="http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/" target="_blank">http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/</a></span>):</p>
<ul>
<li><strong>Linear relationship</strong>: The predictors should have some level of linear relationship with the outcome</li>
<li><strong>Multivariate normality</strong>: The predictors should follow a Gaussian distribution</li>
<li><strong>No or little multicollinearity</strong>: The predictors should not be correlated to one another</li>
<li><strong>Homoscedasticity</strong>: The variance of each predictor should remain more or less constant across the whole range of values</li>
</ul>
<p>Of course, these assumptions are seldom verified. But there are ways to transform the data to approach these optimal conditions.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Missing values</h1>
            </header>

            <article>
                
<p>Data aggregation, extraction, and consolidation is often not perfect and sometimes results in missing values. There are several common strategies to deal with missing values in datasets:</p>
<ul>
<li>Removing all the rows with missing values from the dataset. This is simple to apply, but you may end up throwing away a big chunk of information that would have been valuable to your model.</li>
<li>Using models that are, by nature, not impacted by missing values such as decision tree-based models: random forests, boosted trees. Unfortunately, the linear regression model, and by extension the SGD algorithm, does not work with missing values (<span class="URLPACKT"><a href="http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf" target="_blank">http://facweb.cs.depaul.edu/sjost/csc423/documents/missing_values.pdf</a></span>).</li>
<li>Imputing the missing data with replacement values; for example, replacing missing values with the median, the average, or the harmonic mean of all the existing values, or using clustering or linear regression to predict the missing values. It may be interesting to add the information that these values were missing in the first place to the dataset.</li>
</ul>
<p>In the end, the right strategy will depend on the type of missing data and of course, the context. While replacing missing blood pressure numbers in a patient medical record by some average may not be acceptable in a healthcare context, replacing missing age values by the average age in the Titanic dataset is definitely adapted to a data science competition.</p>
<p>However, Amazon ML's documentation is not 100% clear on the strategy used to deal with missing values:</p>
<div class="packt_quote">If the target attribute is present in the record, but a value for another numeric attribute is missing, then Amazon ML overlooks the missing value. In this case, Amazon ML <span class="underline">creates a substitute attribute</span> and sets it to 1 to indicate that this attribute is missing.</div>
<p>In the case of missing values, a new column is created with a Boolean flag to indicate that the value was missing in the first place. But it is not clear whether the whole row or sample is dismissed or overlooked or if just the cell is removed. There is no mention of any type of imputation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Normalization</h1>
            </header>

            <article>
                
<p>Machine learning algorithms incrementally update the model parameters by minimizing the error between the real value and the one predicted with the last iteration's parameters. To measure this prediction error we introduce the concept of loss functions. A loss function is a measure of the prediction error. For a certain algorithm, using different loss functions will create variants of the algorithm. Most common loss functions use the <strong>L2</strong> or the <strong>L1</strong> norm to measure the error:</p>
<p><strong>●  </strong><strong>L2 norm:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img height="42" src="assets/B05028_2_02_eqn.png" width="163"/></div>
<p><strong>●  </strong><strong>L1 norm:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img height="40" src="assets/B05028_2_03_eqn.png" width="150"/></div>
<p>Where <em>y<sub>i</sub></em> and <em>ŷ</em> are the real and predicted values of the samples.</p>
<p>The measure of the prediction error can end up being skewed when the different predictors differ by an order of magnitude. The large predictors obfuscate the importance of the smaller valued ones, thus making it difficult to infer the relative importance of each predictor in the model. This impacts how the respective weights of the linear model converge to their optimal value and as a consequence the performance of the algorithm. Predictors with the highest magnitude will end up dominating the model even if the predictor has little predictive power with regard to the real outcome value. Normalizing the data is a way to mitigate that problem by forcing the predictors to all be on the same scale.</p>
<p>There are two common types of normalization; data can be normalized or standardized:</p>
<ul>
<li>The <strong>min-max normalization</strong>, or <strong>normalization</strong>, which sets all values between <em>[0,1]</em>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="34" src="assets/image_02_006.png" width="140"/></div>
<ul>
<li>The <strong>z-score normalization</strong>, or <strong>standardization</strong>, which normalizes with respect to the standard deviation. All predictors will have a mean of 0 and a standard deviation of 1:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="36" src="assets/image_02_007.png" width="114"/></div>
<div class="packt_infobox">
<p>The tree-based methods (decision trees, random forests, boosted trees) are the only machine learning models whose performance is not improved by normalization or standardization. All other distance/variance-based predictive algorithms may benefit from normalization. It has been shown that standardization is particularly useful for SGD, as it ensures that all the weights will be adapted at the same speed.</p>
<p><span class="URLPACKT"><em>Efficient BackProp Yann A. LeCun et al. in Neural Networks</em>: <em>Tricks of the Trade pp. 9-48, Springer Verlag</em></span></p>
</div>
<p>Amazon ML offers z-score standardization as part of the available data transformations.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Imbalanced datasets</h1>
            </header>

            <article>
                
<p>Dealing with imbalanced datasets is a very common classification problem.</p>
<p>Consider a Binary classification problem. Your goal is to predict a positive versus a negative class. The ratio between the two classes is highly skewed in favor of the positive class. This situation is frequently encountered in the following instance:</p>
<ul>
<li>In a medical context where the positive class corresponds to the presence of cancerous cells in people out of a large random population</li>
<li>In a marketing context where the positive class corresponds to prospects buying an insurance while the majority of people are not buying it</li>
</ul>
<p>In both these cases, we want to detect the samples in the minority class, but they are overwhelmingly outnumbered by the samples in the majority (negative) class. Most predictive models will be highly biased toward the majority class.</p>
<p>In the presence of highly imbalanced classes, a very simplistic model that always predicts the majority class and never the minority one will have excellent accuracy but would never detect the important and valuable class. Consider for instance a dataset composed of 1,000 samples, with 50 positive samples that we want to detect or predict and 950 negative ones of little interest. That simplistic model has an accuracy rate of 95% which is obviously a decent accuracy even though that model is totally useless. This problem is known as the <strong>Accuracy paradox</strong> (<a href="https://en.wikipedia.org/wiki/Accuracy_paradox" target="_blank"><span class="URLPACKT">https://en.wikipedia.org/wiki/Accuracy_paradox</span></a>).</p>
<p>A straightforward solution would be to gather more data, with a focus on collecting samples of the minority class and in order to balance out the two classes. But that's not always a possibility.</p>
<p>There are many other strategies to deal with imbalanced datasets. We will briefly look at some of the most common ones. One approach is to resample the data by under sampling or oversampling the available data:</p>
<ul>
<li><strong>Undersampling</strong> consists in discarding most samples in the majority class in order to tilt back the minority/majority class ratio toward <em>50/50</em>. The obvious problem with that strategy is that a lot of data is discarded and along with that, meaningful signal for the model. This technique can be useful in the presence of large enough datasets.</li>
<li><strong>Oversampling</strong> consists in duplicating samples that belong to the minority class. Contrary to under sampling, there is no loss of data with that strategy. However, oversampling adds extra weight to certain patterns from the minority class, which may not bring useful information to the model. Oversampling adds noise to the model. Oversampling is useful when the dataset is small and you can't afford to leave some data out.</li>
</ul>
<p>Under sampling and oversampling are two simple and easy-to-implement methods that are useful in establishing a baseline. Another widely-used method consists in creating synthetic samples from the existing data. A popular sample creation technique is the <strong>SMOTE</strong> method, which stands for <strong>Synthetic Minority Over-Sampling Technique</strong>. SMOTE works by selecting similar samples (with respect to some distance measure) from the minority class and adding perturbations on the selected attributes. SMOTE then creates new minority samples within clusters of existing minority samples. SMOTE is less of a solution in the presence of high-dimensional datasets.</p>
<div class="packt_tip">The <strong>imbalanced library</strong> in Python (<span class="URLPACKT"><a href="http://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank">http://</a><a href="http://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank">github.com/scikit-learn-contrib/imbalanced-learn</a>)</span> or the <strong>unbalanced package</strong> in R (<span class="URLPACKT"><a href="https://cran.r-project.org/web/packages/unbalanced/index.html" target="_blank">https://cran.r-project.org/web/packages/unbalanced/index.html</a>)</span> both offer a large set of advanced techniques on top of the ones mentioned.</div>
<p>Note that the choice of the metric used to assess the performances of the model is particularly important in the context of an imbalanced dataset. The accuracy rate, which is defined as the ratio of correctly predicted samples to the total number of samples is the most straightforward metric in classification problems. But as we have seen, this accuracy rate is not a good indicator of the model's predictive power in the presence of a highly skewed class distribution.</p>
<p>In such a context, two metrics are recommended:</p>
<ul>
<li><strong>Cohen's kappa:</strong> A robust measure of the agreement between real and predicted classes (<a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank">https://en.wikipedia.org/wiki/Cohen%27s_kappa</a>)</li>
<li><strong>The F1 score</strong>: The harmonic mean between Precision and Recall (<span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/F1_score" target="_blank">https://en.wikipedia.org/wiki/F1_score</a>)</span></li>
</ul>
<p>The F1 score is the metric used by Amazon ML to assess the quality of a classification model. We give the definition of the F1-score under the <em>Evaluating the performance of your model</em> section at the end of this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Addressing multicollinearity</h1>
            </header>

            <article>
                
<p>The following is the definition of multicollinearity according to Wikipedia (<span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/Multicollinearity" target="_blank">https://en.wikipedia.org/wiki/Multicollinearity</a></span>):</p>
<p><strong>Multicollinearity</strong> is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy.</p>
<p>Basically, let's say you have a model with three predictors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="16" src="assets/B05028_2_04_eqn.png" width="178"/></div>
<p>And one of the predictors is a linear combination (perfect multicollinearity) or is approximated by a linear combination (near multicollinearity) of two other predictors.</p>
<p>For instance: <img height="13" src="assets/B05028_2_05_eqn.png" width="119"/></p>
<p>Here, <img height="8" src="assets/B05028_2_06_eqn.png" width="8"/> is some noise variable.</p>
<p>In that case, changes in <img height="9" src="assets/B05028_2_07_eqn.png" width="15"/> and <img height="8" src="assets/B05028_2_08_eqn.png" width="13"/> will drive changes in <img height="9" src="assets/B05028_2_09_eqn.png" width="14"/>, and as a consequence, <img height="9" src="assets/B05028_2_10_eqn.png" width="17"/> will be tied to <img height="8" src="assets/B05028_2_11_eqn.png" width="14"/> and <img height="9" src="assets/B05028_2_12_eqn.png" width="16"/>. The information already contained in <img height="8" src="assets/B05028_2_07_eqn.png" width="13"/> and <img height="9" src="assets/B05028_2_08_eqn.png" width="14"/> will be shared with the third predictor <img height="9" src="assets/B05028_2_09_eqn.png" width="14"/>, which will cause high uncertainty and instability in the model. Small changes in the predictors' values will bring large variations in the coefficients. The regression may no longer be reliable. In more technical terms, the standard errors of the coefficient would increase, which would lower the significance of otherwise important predictors.</p>
<p>There are several ways to detect multicollinearity. Calculating the correlation matrix of the predictors is a first step, but that would only detect collinearity between pairs of predictors.</p>
<p>A widely used detection method for multicollinearity is to calculate the <strong>variance inflation factor</strong> or <strong>VIF</strong> for each of the predictors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" src="assets/B05028_2_13_eqn.png" width="85"/></div>
<p>Here, <img height="15" src="assets/B05028_2_14_eqn.png" width="15"/> is the coefficient of determination of the regression equation in step one, with <img height="8" src="assets/B05028_2_15_eqn.png" width="12"/> on the left-hand side and all other predictor variables (all the other <img height="13" src="assets/B05028_2_16_eqn.png" width="74"/> variables) on the right-hand side.</p>
<p>A large value for one of the VIFs is an indication that the variance (the square of the standard error) of a particular coefficient is larger than it would be if that predictor was completely uncorrelated with all the other predictors. For instance, a VIF of 1.8 indicates that the variance of a predictor is 80% larger than what it would be in the uncorrelated case.</p>
<p>Once the attributes with high collinearity have been identified, the following options will reduce collinearity:</p>
<ul>
<li>Removing the high collinearity predictors from the dataset</li>
<li>Using <strong>Partial Least Squares Regression (PLS)</strong> or <strong>Principal Components Analysis (PCA)</strong>, regression methods (<span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a></span>); PLS and PCA will reduce the number of predictors to a smaller set of uncorrelated variables</li>
</ul>
<p>Unfortunately, detection and removal of multicollinear variables is not available from the Amazon ML platform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Detecting outliers</h1>
            </header>

            <article>
                
<p>Given a variable, outliers are values that are very distant from other values of that variable. Outliers are quite common, and often caused by human or measurement errors. Outliers can strongly derail a model.</p>
<p>To demonstrate, let's look at two simple datasets and see how their mean is influenced by the presence of an outlier.</p>
<p>Consider the two datasets with few samples each: <em>A = [1,2,3,4]</em> and <em>B = [1,2,3,4, 100]</em>. The 5<sup>th</sup> value in the B dataset,  100, is obviously an outlier: <em>mean(A) = 2.5</em>, while <em>mean(B) = 22</em>. An outlier can have a large impact on a metric. Since most machine learning algorithms are based on distance or variance measurements, outliers can have a high impact on the performance of a model.</p>
<p>Multiple linear regression is sensitive to outlier effects, as shown in the following graph where adding a single outlier point derails the solid regression line into the dashed one:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="445" src="assets/image_02_024.png" width="445"/></div>
<p>Removing the samples associated with the outliers is the simplest solution.</p>
<p>Another solution can be to apply <strong>quantile binning</strong> to the predictor by splitting the values into <em>N</em> ordered intervals or bins, each approximately containing an equal number of samples. This will transform a numeric (continuous) predictor into a categorical one. For example, [1,2,3,4,5,6,7,8,9,10,11,100] split into three equally sized bins becomes [<em>1,1,1,1,2,2,2,2,3,3,3,3</em>]; the outlier value 100 has been included in the third bin and hidden.</p>
<p>The downside of quantile binning is that some granularity of information is lost in the process, which may degrade the performance of the model.</p>
<p><strong>Quantile binning</strong> is available as a data transformation process in Amazon ML and is also used to quantify non-linearities in the original dataset.</p>
<div class="packt_tip">In fact, <strong>Quantile Binning</strong> (<strong>QB</strong>) is applied by default by Amazon ML to all continuous variables that do not exhibit a straightforward linear relation to the outcome. In all our trials, and contrary to our prior assumptions, we have found that QB is a very efficient data transformation in the Amazon ML context.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Accepting non-linear patterns</h1>
            </header>

            <article>
                
<p>A linear regression model implies that the outcome can be estimated by a linear combination of the predictors. This, of course, is not always the case, as features often exhibit nonlinear patterns.</p>
<p>Consider the following graph, where <em>Y</em> axis depends on <em>X</em> axis but the relationship displays an obvious quadratic pattern. Fitting a line (<em>y = aX + b</em>) as a prediction model of <em>Y</em> as a function of <em>X</em> does not work:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="401" src="assets/image_02_025.png" width="401"/></div>
<p>Some models and algorithms are able to naturally handle non-linearities, for example, tree-based models or support vector machines with non-linear kernels. Linear regression and SGD are not.</p>
<p><strong>Transformations</strong>: One way to deal with these nonlinear patterns in the context of linear regression is to transform the predictors. In the preceding simple example, adding the square of the predictor X to the model would give a much better result. The model would now be of the following form:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/B05028_2_28_eqn.png"/></div>
<p>And as shown in the following diagram, the new quadratic model fits the data much better:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="420" src="assets/image_02_027.png" width="420"/></div>
<p>We are not restricted to the quadratic case, and a power function of higher order can be used to transform existing attributes and create new predictors. Other useful transformations could include taking the logarithm, exponential, sine and cosine, and so on. The <strong>Boxcox</strong> <strong>transformation</strong> (<a href="http://onlinestatbook.com/2/transformations/box-cox.html" target="_blank"><span class="URLPACKT">http://onlinestatbook.com/2/transformations/box-cox.html</span></a>) is worth citing at this point. It's an efficient data transformation that reduces skewness and kurtosis of a variable distribution. It reshapes the variable distribution into one closer to a Gaussian distribution.</p>
<p><strong>Splines</strong> are an excellent and more powerful alternative to polynomial interpolation. Splines are piece-wise polynomials that join smoothly. At their simplest level, splines consists of lines that are connected together at different points. Splines are not available in Amazon ML.</p>
<p><strong>Quantile binning</strong> is the Amazon ML solution to non-linearities. By splitting the data into N bins, you remove any non-linearities in the bin's intervals. Although binning has several drawbacks (<span class="URLPACKT"><a href="http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous" target="_blank">http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous</a></span>), the main one being that information is discarded in the process, it has been shown to generate excellent prediction performance in the Amazon ML platform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Adding features?</h1>
            </header>

            <article>
                
<p>In general, adding new features that are correlated in some ways to the outcome brings information and improves a model. However, adding too many features with little predictive power may end up bringing confusion to that same model and in the end degrading its performance.</p>
<p>Feature selection by removal of the least interesting features is worth trying when the sample size is small compared to the number of features; it leads to too few observations or too many features. There are different strategies (<span class="URLPACKT"><a href="http://machinelearningmastery.com/an-introduction-to-feature-selection/" target="_blank">http://machinelearningmastery.com/an-introduction-to-feature-selection/</a></span>) to identify and remove weak features. Selecting features based on their correlation with the outcome and discarding features with little or no correlation with the outcome will usually improve your model.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Preprocessing recapitulation</h1>
            </header>

            <article>
                
<p>The following table recapitulates the different issues that one can find in raw data and whether Amazon ML offers ways to deal with them:</p>
<table class="a">
<tbody>
<tr>
<td/>
<td><strong>Linear model sensitivity</strong></td>
<td><strong>Available on Amazon ML</strong></td>
</tr>
<tr>
<td><strong>Missing values</strong></td>
<td>Yes</td>
<td>Dealt with automatically</td>
</tr>
<tr>
<td><strong>Standardization</strong></td>
<td>Yes</td>
<td>z-score standardization</td>
</tr>
<tr>
<td><strong>Outliers</strong></td>
<td>Yes</td>
<td>Quantile binning</td>
</tr>
<tr>
<td><strong>Multicollinearity</strong></td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td><strong>Imbalanced datasets</strong></td>
<td>Yes</td>
<td>Uses the right metric F1 Score<br/>
No sampling strategy (may exist in background)</td>
</tr>
<tr>
<td><strong>Non linearities</strong></td>
<td>Yes</td>
<td>Quantile binning</td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The predictive analytics workflow</h1>
            </header>

            <article>
                
<p>We have been talking about training the model. What does that mean in practice?</p>
<p>In supervised learning, the dataset is usually split into three non-equal parts: training, validation, and test:</p>
<div class="CDPAlignCenter CDPAlign"><img height="59" src="assets/image_02_028.png" width="330"/></div>
<ul>
<li>The <strong>training</strong> set on which you train your model. It has to be big enough to give the model as much information on the data as possible. This subset of the data is used by the algorithm to estimate the best parameters of the model. In our case, the SGD algorithm will use that training subset to find the optimal weights of the linear regression model.</li>
<li>The <strong>validation</strong> set is used to assess the performance of a trained model. By measuring the performance of the trained model on a subset that has not been used in its training, we have an objective assessment of its performance. That way we can train different models with different meta parameters and see which one is performing the best on the validation set. This is also called model selection. Note that this creates a feedback loop, since the validation dataset now has an influence on your model selection. Another model may have performed worse on that particular validation subset but overall better on new data.</li>
<li>The <strong>test</strong> set corresponds to data that is set aside until you have fully optimized your features and model. The test subset is also called the <strong>held-out</strong> dataset.</li>
</ul>
<p>In real life, your model will face previously unforeseen data, since the ultimate <em>raison d'etre</em> for a model is to predict unseen data. Therefore, it is important to assess the performance of the model on data it has never encountered before. The held-out dataset is a proxy for yet unseen data. It is paramount to leave this dataset aside until the end. It should never be used to optimize the model or the data attributes. </p>
<p>These three subsets should be large enough to represent the real data accurately. More precisely, the distribution of all the variables should be equivalent in the three subsets. If the original dataset is ordered in some way, it is important to make sure that the data is shuffled prior to the train, validation, test split.</p>
<p>As mentioned previously, the model you choose based on its performance on the validation set may have had a positive bias toward that particular dataset. In order to minimize such a dependency, it is common to train and evaluate several models with the same parameter settings and to average the performance of the model over several training validation dataset pairs. This reduces the model selection dependence with regard to the specific distribution of variables in the validation dataset.</p>
<p>This third-split method is basic and as we've seen, the model could end up being dependent on some specificities of the validation subset. Cross-validation is a standard method to reduce that dependency and improve our model selection. Cross validation consists in carrying out several training/validation split and averaging the model performance on the different validation subsets. The most frequent cross validation technique is k-fold cross validation which consists in splitting the dataset in K-chunks, recursively using each part as validation and the k-1 other parts as training. Other cross validation techniques include Monte-Carlo cross validation where the different training and validation sets are randomly sampled from the initial dataset. We will implement Monte Carlo cross validation in a later chapter. Cross validation is not a feature included in the Amazon ML service and needs to be implemented programatically. In Amazon ML, the training and evaluation of a model is done on one training-validation split only.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training and evaluation in Amazon ML</h1>
            </header>

            <article>
                
<p>In the context of Amazon ML, the model is linear regression and the algorithm the <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) algorithm. This algorithm has one main meta parameter called the learning rate and often noted <img height="6" src="assets/B05028_2_17_eqn.png" width="9"/>, which dictates how much of a new sample is taken into account for each iterative update of the weights. A larger learning rate makes the algorithm converge faster but stabilizes further from the optimal weights, while a smaller learning induces a slower convergence but a more precise set of regression coefficients.</p>
<p>Given a training and a validation dataset, this is how Amazon ML tunes and select the best model:</p>
<ul>
<li>Amazon trains several models, each with a different learning rate</li>
<li>For a given a learning rate:
<ul>
<li>The training dataset allows the SGD to train the model by finding the best regression coefficients</li>
<li>The model is used on the validation dataset to make predictions</li>
</ul>
</li>
<li>By comparing the quality of the predictions of the different models on that validation set, Amazon ML is able to select the best model and the associated best learning rate</li>
<li>The held-out set is used as final confirmation that the model is reliable</li>
</ul>
<p>Usual splitting ratios for the training, validation, and held-out subsets are as follows:</p>
<ul>
<li>Training : <em>70%</em> validation and held-out 15% each</li>
</ul>
<ul>
<li>Training : <em>60%</em> validation and <span>held-out</span> 20% each</li>
</ul>
<div class="packt_tip"><strong>Shuffling</strong>: It is important to make sure that the predictors and the outcome follow the same distribution in all three subsets. Shuffling the data before splitting it is an important part of creating reliable training, validation, and <span>held-out</span> subsets.</div>
<p>It is important to define the data transformations on the training dataset and apply the transformation parameters on the validation and held-out subsets so that the validation and <span>held-out subsets do not leak information back in the training set.</span></p>
<p>Take standardization as an example: the standard deviation and the mean of the predictors should be calculated on the training dataset. And these values then applied to standardize the validation and <span>held-out</span> sets. If you use the whole original dataset to calculate the mean and SGD, you leak information from the <span>held-out</span> set into the training set.</p>
<p>A common Supervised Predictive Analytics workflow follows these steps - Let's assume we have an already extracted dataset and that we have chosen a metric to assess the quality of our predictions:</p>
<ol>
<li>Building the dataset
<ul>
<li>Cleaning up and transforming the data to handle noisy data issues</li>
<li>Creating new predictors</li>
<li>Shuffling and splitting the data into a training, a validation and a held-out set</li>
</ul>
</li>
<li>Selecting the best model
<ul>
<li>Choosing a model (linear, tree-based, bayesian , ...)</li>
<li>Repeat for several values of the meta parameters:
<ul>
<li>Train the model on the training set</li>
<li>Assess the model performance on the validation set</li>
</ul>
</li>
</ul>
</li>
<li>Repeat steps 1 and 2 with new data, new predictors, and other model parameters until you are satisfied with the performances of your model. Keep the best model.</li>
<li>Final test of the model on the held-out subset.</li>
</ol>
<div class="packt_infobox">In the context of Amazon ML, there is no possibility to choose a model (step 2) other than a linear regression one (logistic regression for classification).</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Identifying and correcting poor performances</h1>
            </header>

            <article>
                
<p>A performant predictive model is one that produces reliable and satisfying predictions on new data. There are two situations where the model will fail to consistently produce good predictions, and both depend on how the model is trained. A poorly trained model will result in underfitting, while an overly trained model will result in overfitting.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Underfitting</h1>
            </header>

            <article>
                
<p><strong>Underfitting</strong> means that the model was poorly trained. Either the training dataset did not have enough information to infer strong predictions, or the algorithm that trained the model on the training dataset was not adequate for the context. The algorithm was not well parameterized or simply inadequate for the data.</p>
<p>If we measure the prediction error not only on the validation set but also on the training set, the prediction error will be large if the model is underfitting. Which makes sense: if the model cannot predict the training, it won't be able to predict the outcomes in the validation set it has not seen before. Underfitting basically means your model is not working.</p>
<p>Common strategies to palliate this problem include:</p>
<ul>
<li>Getting more data samples – If the problem comes from a dataset that is too small or does not contain sufficient information, getting more data may improve the model performance.</li>
<li>Adding more features, raw or via feature engineering – by taking the log, squaring, binning, using splines or power functions. Adding many features and seeing how that improves the predictions.</li>
<li>Choosing another model – Support Vector Machine, Random Forest, Boosted trees, Bayes classifiers all have different strengths in different contexts.</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Overfitting</h1>
            </header>

            <article>
                
<p><strong>Overfitting</strong> occurs when the model was so well trained that it fits the training data too perfectly and cannot handle new data. </p>
<p>Say you have a unique predictor of an outcome and that the data follows a quadratic pattern:</p>
<ol>
<li>You fit a linear regression on that data <img height="9" src="assets/B05028_2_18_eqn.png" width="53"/>, the predictions are weak. Your model is underfitting the data. There is a high error level on both the training error and the validation dataset.</li>
<li>You add the square of the predictor in the model <img height="16" src="assets/B05028_2_19_eqn.png" width="101"/> and find that your model makes good predictions. The error on both the training and the validation datasets are equivalent and lower than for the simpler model.</li>
<li>If you increase the number and power of polynomial features so that the model is now <img height="16" src="assets/B05028_2_20_eqn.png" width="191"/>, you end up fitting the training data too closely. The model has a very low prediction error on the training dataset but is unable to predict anything on new data. The prediction error on the validation dataset remains high.</li>
</ol>
<p>This is a case of overfitting.</p>
<p>The following graph shows an example of an overfitting model with regard to the previous quadratic dataset, by setting a high order for the polynomial regression (<em>n = 16</em>). The polynomial regression fits the training data so well it would be incapable of any predictions on new data whereas the quadratic model (<em>n = 2</em>) would be more robust:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="474" src="assets/image_02_035.png" width="474"/></div>
<p>The best way to detect overfitting is, therefore, to compare the prediction errors on the training and validation sets. A significant gap between the two errors implies overfitting. A way to prevent this overfitting from happening is to add constraints on the model. In machine learning, we use regularization.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regularization on linear models</h1>
            </header>

            <article>
                
<p>The Stochastic Gradient Descent algorithm (SGD) finds the optimal weights <em>{w<sub>i</sub>}</em> of the model by minimizing the error between the true and the predicted values on the N training samples:</p>
<div class="CDPAlignCenter CDPAlign"><img height="92" src="assets/B05028_2_22_eqn.png" width="201"/></div>
<p>Where <img height="14" src="assets/image_02_038.png" width="151"/> are the predicted values, <em>ŷ<sub>i</sub></em> the real values to be predicted; we have <em>N</em> samples, and each sample has <em>n</em> dimensions.</p>
<p>Regularization consists of adding a term to the previous equation and to minimize the regularized error:</p>
<div class="CDPAlignCenter CDPAlign"><img height="45" src="assets/B05028_2_24_eqn.png" width="227"/></div>
<p>The <img height="9" src="assets/image_02_042.png" width="7"/> parameter helps quantify the amount of regularization, while <em>R(w)</em> is the regularization term dependent on the regression coefficients.</p>
<p>There are two types of weight constraints usually considered:</p>
<ul>
<li>L2 regularization as the sum of the squares of the coefficients:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="40" src="assets/B05028_2_25_eqn.png" width="107"/></div>
<ul>
<li>L1 regularization as the sum of the absolute value of the coefficients:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="41" src="assets/B05028_2_26_eqn.png" width="103"/></div>
<p>The constraint on the coefficients introduced by the regularization term <em>R(w)</em> prevents the model from overfitting the training data. The coefficients become tied together by the regularization and can no longer be tightly leashed to the predictors. Each type of regularization has its characteristic and gives rise to different variations on the SGD algorithm, which we now introduce:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">L2 regularization and Ridge</h1>
            </header>

            <article>
                
<p>L2 regularization prevents the weights <em>{w<sub>i</sub>}</em> from being too spread. The smaller weights that rise up for non-correlated though potentially meaningful, features will not become insignificant when compared to the weights associated to the important correlated features. L2 regularization will enforce similar scaling of the weights. A direct consequence of L2 regularization is to reduce the negative impact of collinearity, since the weights can no longer diverge from one another.</p>
<p>The Stochastic Gradient Descent algorithm with L2 regularization is known as the <strong>Ridge algorithm</strong>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">L1 regularization and Lasso</h1>
            </header>

            <article>
                
<p>L1 regularization usually entails some loss of predictive power of the model. </p>
<p>One of the properties of L1 regularization is to force the smallest weights to 0 and thereby reduce the number of features taken into account in the model. This is a desired behavior when the number of features (<em>n</em>) is large compared to the number of samples (<em>N</em>). L1 is better suited for datasets with many features. </p>
<p>The Stochastic Gradient Descent algorithm with L1 regularization is known as the <strong>Least Absolute Shrinkage and Selection Operator</strong> <strong>(Lasso)</strong> algorithm.</p>
<p>In both cases the hyper-parameters of the model are as follows:</p>
<ul>
<li>The learning rate <img src="assets/B05028_2_17_eqn.png"/> of the SGD algorithm</li>
<li>A parameter <img height="9" src="assets/image_02_042.png" width="7"/> to tune the amount of regularization added to the model</li>
</ul>
<p>A third type of regularization called <strong>ElasticNet</strong> consists in adding both a L2 and a L1 regularization term to the model. This brings up the best of both regularization schemas at the expense of an extra hyper-parameter.</p>
<p>In other contexts, although experts have different opinions (<span class="URLPACKT"><a href="https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization" target="_blank">https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization</a></span>) on which type of regularization is more effective, the consensus seems to favor L2 over L1 regularization.</p>
<p>L2 and L1 regularization are both available in Amazon ML while ElasticNet is not. The amount of regularization available is limited to three values for  <img height="11" src="assets/image_02_042.png" width="8"/>: mild (1<em>0<sup>-6</sup></em>), medium (<em>10<sup>-4</sup></em>), and aggressive (<em>10<sup>-2</sup></em>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating the performance of your model</h1>
            </header>

            <article>
                
<p>Evaluating the predictive performance of a model requires defining a measure of the quality of its predictions. There are several available metrics both for regression and classification. The metrics used in the context of Amazon ML are the following ones:</p>
<ul>
<li><strong>RMSE for regression</strong>: The root mean squared error is defined by the square of the difference between the true outcome values and their predictions:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="43" src="assets/B05028_2_27_eqn.png" width="165"/></div>
<ul>
<li><strong>F-1 Score and ROC-AUC for classification</strong>: Amazon ML uses logistic regression for binary classification problems. For each prediction, logistic regression returns a value between 0 and 1. This value is interpreted as a probability of the sample belonging to one of the two classes. A probability lower than 0.5 indicates belonging to the first class, while a probability higher than 0.5 indicates a belonging to the second class. The decision is therefore highly dependent on the value of the threshold. A value which we can modify.</li>
<li>Denoting one class positive and the other negative, we have four possibilities depicted in the following table:</li>
</ul>
<table class="a0">
<tbody>
<tr>
<td/>
<td><strong>Predicted Yes</strong></td>
<td><strong>Predicted No</strong></td>
</tr>
<tr>
<td><strong>Real value: Yes</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN) (or type II error)</td>
</tr>
<tr>
<td><strong>Real value: No</strong></td>
<td>False Positive (FP)</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
<ul>
<li>This matrix is called a <strong>confusion matrix</strong> (<span class="URLPACKT"><a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank">https://en.wikipedia.org/wiki/Confusion_matrix</a></span>) . It defines four indicators of the performance of a classification model:
<ul>
<li>TP: How many Yes were correctly predicted Yes</li>
<li>FP: How many No were wrongly predicted Yes</li>
<li>FN: How many Yes were wrongly predicted No</li>
<li>TN: How many No were correctly predicted No</li>
</ul>
</li>
<li>From these four indicators, we can define the following metrics:
<ul>
<li><strong>Recall:</strong> This denotes the amount of predicted positives actually positive. Recall is also called <strong>True Positive Rate</strong> (<strong>TPR</strong>) or sensitivity. It is the probability of detection:</li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>Recall = (TP / TP + FN)</em></p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Precision</strong> as the fraction of the real positives over all the positive predicted values:</li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>Precision = (TP / TP + FP)</em></p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>False Positive Rate</strong> is the number of falsely predicted positives over all the true negatives. It's the probability of false alarm:</li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>FPR = FP / FP + TN</em></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Finally, the <strong>F1-score</strong> is defined as the weighted average of the recall and the precision, and is given by the following:</li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>F1-score = 2 TP / ( 2 TP + FP + FN)</em></p>
<ul>
<li style="list-style-type: none">
<ul>
<li>A F1 score is always between 0 and 1, with 1 the best value and 0 the worst one.</li>
</ul>
</li>
</ul>
<p>As noted previously, these scores are all dependent on the initial threshold used to interpret the result of the logistic regression in order to decide when a prediction belongs to one class or the other. We can choose to vary that threshold. This is where the ROC-AUC comes in.</p>
<p>If you plot the True Positive Rate (Recall) against the False Positive Rate for different values of the decision threshold, you obtain a graph like the following, called the <strong>Receiver Operating Characteristic</strong> or <strong>ROC curve</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="333" src="assets/image_02_054-1.png" width="364"/></div>
<ul>
<li>The diagonal line indicates an equal probability of belonging to one class or another. The closer the curve is to the upper-left corner, the better your model performances are.</li>
<li>The ROC curve has been widely used since WWII, when it was first invented to detect enemy planes in radar signals.</li>
<li>Once you have the ROC curve, you can calculate the <strong>Area Under the Curve</strong> or <strong>AUC</strong>.</li>
<li>The AUC will give you a unique score for your model taking into account all the possible values for the probability threshold from 0 to 1. The higher the AUC the better.</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we focused on two important elements of a predictive analytics project: the data and the evaluation of the predictive power of the model. We first listed the most common problems encountered with raw data, their impact on the linear regression model, and ways to solve them. The reader should now be able to identify and deal with missing values, outliers, imbalanced datasets, and normalization.</p>
<p>We also introduced the two most frequent problems in predictive analytics: underfitting and overfitting. L1 and L2 regularization is an important element in the Amazon ML platform, which helps overcome overfitting and make models more robust and able to handle previously unseen data.</p>
<p>We are now ready to dive into the Amazon Machine Learning platform in the next chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>