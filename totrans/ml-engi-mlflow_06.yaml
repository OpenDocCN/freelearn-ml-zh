- en: '*Chapter 4*: Experiment Management in MLflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will give you practical experience with stock predictions
    by creating different models and comparing metrics of different runs in MLflow.
    You will be guided in terms of how to use the MLflow experiment method so that
    different machine learning practitioners can share metrics and improve on the
    same model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the experiments module
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the experiment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding experiments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing different models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning your model with hyperparameter optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this stage, we currently have a baseline pipeline that acts based on a naïve
    heuristic. In this chapter, we will add to our set of skills the ability to experiment
    with multiple models and tune one specific model using MLflow.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We will be delving into our **Psystock** company use case of a stock trading
    machine learning platform introduced in [*Chapter 2*](B16783_02_Final_SB_epub.xhtml#_idTextAnchor030),
    *Your Machine Learning Project*. In this chapter, we will add to our platform
    to compare multiple models and run experiments in the benchmark to be able to
    create a predictor for a specific stock and ticker.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In data science functions, a common methodology is to develop a model for a
    specific model that involves the following three steps: creating baseline models
    with different model types, identifying the best performant model, and predicting
    with the best model.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Docker Compose installed. Please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line and installed as described in [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a bash terminal (Linux or Windows).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your machine learning installed locally and described
    in [*Chapter 3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your* *Data
    Science Workbench*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with the experiments module
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started with the technical modules, you will need to get started with
    the environment prepared for this chapter in the following folder: [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able, at this stage, to execute the `make` command to build up
    your workbench with the dependencies needed to follow along with this chapter.
    You need next to type the following command to move to the right directory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To start the environment, you need to run the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The entry point to start managing experimentation in **MLflow** is the experiments
    interface illustrated in *Figure 4.1*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The Experiments interface in MLflow'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The Experiments interface in MLflow
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: On the left pane (1), you can manage and create experiments, and on the right
    (2), you can query details of a specific experiment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new experiment, you need to click on the **+** button on the left
    pane and add the details of your experiment, as illustrated by *Figure 4.2*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Creating new experiments'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_02.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Creating new experiments
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced at a high level the tracking server and the experiment management
    features, we will now proceed to use the features available on our workbench to
    tackle the challenges of the current chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Defining the experiment
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the machine learning problem framing methodology, we will now define
    the main components of our stock price prediction problem as defined for the chapter:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_Table_(1).jpg)![Table 4.1 – Machine learning problem framing
    recap'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_Table_(2).jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 – Machine learning problem framing recap
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The **F-score** metric in machine learning is a measure of accuracy for binary
    classifiers and provides a good balance and trade-off between misclassifications
    (false positives or false negatives). Further details can be found on the Wikipedia
    page: [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As specified in our machine learning problem framing, we will use as input data
    the market observations for the period January-December 2020, as provided by the
    Yahoo data API.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code excerpt, which uses the `pandas_datareader` module available
    in our workbench, allows us to easily retrieve the data that we want. The complete
    working notebook is available at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For this particular problem, we will retrieve data from 2014 up to the end
    of 2020, as represented in the table provided in *Figure 4.3*. The table provides
    value information about High, Low, Open, and Close for the BTC stock of the trading
    section. This data will be used to train the models in the current chapter:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Listing the data retrieved from the source (Yahoo Finance)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'This data can easily be plotted by plotting one of the variables just to illustrate
    the continuous nature of the data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To illustrate a bit more about the nature of the data, we can plot an excerpt
    of the data:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source
    (Yahoo Finance)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source
    (Yahoo Finance)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Having defined precisely what we will be experimenting with in this section,
    we will move to add new models to enable us to run experiments and compare among
    them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The data for the required range was conveniently saved in a file under `Chapter04/gradflow/notebooks/training_data.csv`,
    for the period ranging from 2014 to 2020 inclusive, so it can be easily retrieved
    during the modeling phase.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Adding experiments
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, in this section, we will use the experiments module in **MLflow** to track
    the different runs of different models and post them in our workbench database
    so that the performance results can be compared side by side.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The experiments can actually be done by different model developers as long as
    they are all pointing to a shared MLflow infrastructure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: To create our first, we will pick a set of model families and evaluate our problem
    on each of the cases. In broader terms, the major families for classification
    can be tree-based models, linear models, and neural networks. By looking at the
    metric that performs better on each of the cases, we can then direct tuning to
    the best model and use it as our initial model in production.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Our choice for this section includes the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic Classifier**: Part of the family of linear-based models and a commonly
    used baseline.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Xgboost**: This belongs to the family of tree boosting algorithms where many
    weak tree classifiers are assembled to produce a stronger model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras**: This type of model belongs to the neural network''s family and is
    generally indicated for situations where there is a lot of data available and
    relations are not linear between the features.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps to set up a new model are quite common and there will be overlapping
    and repeated code for each of the models. We will start next with a logistic regression-based
    classifier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up a logistic-based classifier
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this sub-section, we will implement a logistic regression classifier in `scikit-learn`
    and train a model with our input data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the `Chapter04/gradflow/notebooks/mlflow_run_logistic_regression.ipynb`
    file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '`SKLearn` model, `LogisticRegression`, and the metrics functionality, `f1_score`,
    that will enable us to calculate the performance:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`training_data.csv` file:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data is split into training and testing using the `train_test_split` function,
    which takes one-third of the data for testing, with the remainder being used for
    training.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mlflow.set_experiment` method. This will create an experiment if it does not
    exist or associate your current run with an experiment. We use `mlflow.sklearn.autolog()`
    to enable the automated capabilities of MLflow to capture the metrics of our experiment:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`with`. The `mlflow.start_run` function is used to take care of registering
    your run with a specific `run_name` so that it can be identified and encloses
    the `fit` model, with evaluation code used to calculate the performance metrics
    of the `f1_score` experiment:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Additionally, we need to log our specific metric, `f1_experiment_score`, with
    the `mlflow.log_metric` function. The main reason for adding our specific method
    is that for each model, the autologging functionality in **MLflow** uses the default
    metric used by each underlying framework and generally, these metrics don't match.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After executing all the steps relating to model development, we can now navigate
    to our run and visualize the log of the experiment. In *Figure 4.5*, you can see
    the specific parameters associated with logistic regression, durations, and all
    the parameters used on your run:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Logistic regression model details'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Logistic regression model details
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'For `SKLearn` models, **MLflow** automatically logs confusion matrices and
    precision and recall curves that are very useful in detecting how well the model
    performed on training data. For instance, the *Figure 4.6* report will be stored
    in the artifacts of your run:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Confusion matrix metrics'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Confusion matrix metrics
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: MLflow provides built-in metrics for Sklearn, providing better visibility of
    the model produced during training without the developer needing to produce extra
    code.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up an XGBoost-based classifier
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now implement a gradient tree-based algorithm using the `XGBoost` library.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the `Chapter04/gradflow/notebooks/mlflow_run_xgboost.ipynb`
    file:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing dependencies**: The XGBoost library is imported alongside the metrics
    function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`training_data.csv` file.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "`Baseline_Predictions`, and we need to give MLflow the instruction to automatically\
    \ \Llog the model through `mlflow.xgboost.autolog`:"
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`f1_score`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After executing all the steps relating to model development, we can now navigate
    to our run and visualize the log of the experiment. In *Figure 4.7*, you can see
    the specific parameters associated with `xgboost_model_baseline`, durations, and
    all the parameters used on your run:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – XGBoost classifier details in MLflow'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – XGBoost classifier details in MLflow
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'For XGBoost models, **MLflow** automatically logs feature information and importance.
    We can see in *Figure 4.8* the ranking of our features in the model stored in
    the *Artifacts* section of the workbench:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – XGBoost feature importance on MLflow'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_08.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – XGBoost feature importance on MLflow
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The feature importance graph in *Figure 4.8* allows the developer to have some
    insights into the internals of the model ascertained from the data. In this particular
    case, it seems that the second and seventh days of the 14 days in the input vector
    are the top two meaningful features. We will next implement a deep learning-based
    model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up a deep learning-based classifier
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will implement a neural network algorithm to solve our classification
    problem.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the Chapter04/gradflow/notebooks/mlflow_run_keras.ipynb
    file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow`, as we are using it as a backend for `keras`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Retrieving data**: Refer to Step 2 in the *Steps for setting up an XGBoost-based
    classifier* section.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Baseline_Predictions`, and we need to give MLflow the instruction to automatically
    log the model through `mlflow.tensorflow.autolog`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Sklearn` or XGBoost classifiers, so we need to define the layers and architecture
    of the network. In this particular case, the `Sequential` architecture and the
    model need to be compiled as required by Tensorflow:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`run_name` and fitting the model followed by calculating the `f1_score` metrics:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For `keras` models, **MLflow** automatically logs a myriad of neural network-related
    data, namely, regarding optimizers and epoch and batch sizes, as well as other
    relevant information that can be seen in *Figure 4.9*:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Keras classifier model details'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_09.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Keras classifier model details
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, **TensorFlow** logs can be hooked into a TensorBoard. This is
    a TensorFlow built-in tool to provide visualizations and metrics for the machine
    learning workflow. Interfaces are created so that the model developer can leverage
    the native TensorFlow instrumentation and specialized visualization tooling.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Having set up our classifiers in our platform, in the next section, we are ready
    to compare the performance of the different classifiers developed using MLflow.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different models
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have run the experiments in this section for each of the models covered
    and verified all the different artifacts. Just by looking at our baseline experiment
    table, and by selecting the common custom metric, `f1_experiment_score`, we can
    see that the best performing model is the logistic regression-based model, with
    an F-score of 0.66:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_10.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Comparing different model performance in terms of the goal metric
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics can also be compared side by side, as shown in the excerpt in *Figure
    4.11*. On the left side, we have the `SKlearn` model, and on the right the XGBoost
    model, with the custom metrics of `f1_experiment_score`. We can see that the metrics
    provided by both are different and, hence, the reason for custom metrics when
    we have different models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Metrics of the Sklearn model'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_11.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Metrics of the Sklearn model
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: After comparing the metrics, it becomes clear that the best model is logistic
    regression. To improve the model, in the next section, we will optimize its parameters
    with state-of-the-art techniques and use MLflow experiment features to achieve
    that.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Tuning your model with hyperparameter optimization
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models have many parameters that allow the developer to improve
    performance and control the model that they are using, providing leverage to better
    fit the data and production use cases. Hyperparameter optimization is the systematic
    and automated process of identifying the optimal parameters for your machine learning
    model and is critical for the successful deployment of such a system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we identified the best family (in other words, `LogisticRegression`)
    model for our problem, so now it''s time to identify the right parameters for
    our model with MLflow. You can follow along in the following notebook in the project
    repository, Chapter04/gradflow/notebooks/hyperopt_optimization_logistic_regression_mlflow.ipynb:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`hyperopt` library, which contains multiple algorithms to help us carry out
    model tuning:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`f1_score` metric in our model. The way optimization works in `hyperopt` is
    through minimization, but in our case, we want the maximum possible `f1_score`
    metric. So, the way we define our loss (the function to minimize) is as the inverse
    of our `f1_score` metric, as in `loss = 1-fscore`, so the minimization of this
    function will represent the best `f1_score` metric. For each run of the model''s
    parameters, we will enclose it in an `mlflow.start_run(nested=True)` in such a
    way that each optimization iteration will be logged as a sub run of the main job,
    providing multiple advantages in terms of comparing metrics across runs:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`best` variable. The core function is the minimization represented by `fmin(fn
    = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials
    = bayes_trials)`, where we provide the parameter space and objective function
    as previously defined:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: After running the experiment for a few minutes, we can now review the experiments
    in `Hyperopt_Optimization` experiment:![Figure 4.12 – Listing all the nested runs
    of the hyperparameter tuning
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16783_04_12.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.12 – Listing all the nested runs of the hyperparameter tuning
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By clicking on the `training_f1_score` and the solver:![](img/B16783_04_13.jpg)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 4.13 – Listing all the nested runs of the hyperparameter tuning
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can easily compare in the same interface the different solvers and implications
    for our performance metric, providing further insights into our modeling phase:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以轻松地在同一界面中比较不同的求解器和对我们性能指标的影响，从而进一步了解我们的建模阶段：
- en: '![Figure 4.14 – Listing all the nested runs of the hyperparameter tuning'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 – 列出超参数调优的所有嵌套运行'
- en: '](img/B16783_04_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16783_04_14.jpg](img/B16783_04_14.jpg)'
- en: Figure 4.14 – Listing all the nested runs of the hyperparameter tuning
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 列出超参数调优的所有嵌套运行
- en: We concluded this section by optimizing the parameters of the most performant
    model for our current problem. In the next chapter of the book, we will be using
    the information provided by the best model to delve into the life cycle of the
    model management in **MLflow**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过优化当前问题的最有效模型的参数来结束本节。在本书的下一章中，我们将使用最佳模型提供的信息，深入探讨**MLflow**中模型管理的生命周期。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the experiments component of MLflow. We got to
    understand the logging metrics and artifacts in MLflow. We detailed the steps
    to track experiments in MLflow.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了MLflow的实验组件。我们了解了MLflow中的日志指标和工件。我们详细说明了在MLflow中跟踪实验的步骤。
- en: In the final sections, we explored the use case of hyperparameter optimization
    using the concepts learned in the chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几节中，我们探讨了使用本章学到的概念进行超参数优化的用例。
- en: In the next chapter, we will focus on managing models with MLflow using the
    models developed in this chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于使用本章开发的模型，利用MLflow来管理模型。
- en: Further reading
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To consolidate your knowledge further, you can consult the documentation available
    at the following links:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步巩固你的知识，你可以查阅以下链接中的文档：
- en: https://www.mlflow.org/docs/latest/tracking.html
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mlflow.org/docs/latest/tracking.html](https://www.mlflow.org/docs/latest/tracking.html)'
- en: h[ttps://en.wikipedia.org/wiki/Hyperparameter_optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Hyperparameter_optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)'
