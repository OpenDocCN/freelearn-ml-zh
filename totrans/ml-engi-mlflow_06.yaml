- en: '*Chapter 4*: Experiment Management in MLflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will give you practical experience with stock predictions
    by creating different models and comparing metrics of different runs in MLflow.
    You will be guided in terms of how to use the MLflow experiment method so that
    different machine learning practitioners can share metrics and improve on the
    same model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the experiments module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing different models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning your model with hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this stage, we currently have a baseline pipeline that acts based on a naïve
    heuristic. In this chapter, we will add to our set of skills the ability to experiment
    with multiple models and tune one specific model using MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: We will be delving into our **Psystock** company use case of a stock trading
    machine learning platform introduced in [*Chapter 2*](B16783_02_Final_SB_epub.xhtml#_idTextAnchor030),
    *Your Machine Learning Project*. In this chapter, we will add to our platform
    to compare multiple models and run experiments in the benchmark to be able to
    create a predictor for a specific stock and ticker.
  prefs: []
  type: TYPE_NORMAL
- en: 'In data science functions, a common methodology is to develop a model for a
    specific model that involves the following three steps: creating baseline models
    with different model types, identifying the best performant model, and predicting
    with the best model.'
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Docker Compose installed. Please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line and installed as described in [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your machine learning installed locally and described
    in [*Chapter 3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your* *Data
    Science Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with the experiments module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started with the technical modules, you will need to get started with
    the environment prepared for this chapter in the following folder: [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able, at this stage, to execute the `make` command to build up
    your workbench with the dependencies needed to follow along with this chapter.
    You need next to type the following command to move to the right directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the environment, you need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The entry point to start managing experimentation in **MLflow** is the experiments
    interface illustrated in *Figure 4.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '2'
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The Experiments interface in MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The Experiments interface in MLflow
  prefs: []
  type: TYPE_NORMAL
- en: On the left pane (1), you can manage and create experiments, and on the right
    (2), you can query details of a specific experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new experiment, you need to click on the **+** button on the left
    pane and add the details of your experiment, as illustrated by *Figure 4.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Creating new experiments'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Creating new experiments
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced at a high level the tracking server and the experiment management
    features, we will now proceed to use the features available on our workbench to
    tackle the challenges of the current chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the machine learning problem framing methodology, we will now define
    the main components of our stock price prediction problem as defined for the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_Table_(1).jpg)![Table 4.1 – Machine learning problem framing
    recap'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_Table_(2).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 – Machine learning problem framing recap
  prefs: []
  type: TYPE_NORMAL
- en: 'The **F-score** metric in machine learning is a measure of accuracy for binary
    classifiers and provides a good balance and trade-off between misclassifications
    (false positives or false negatives). Further details can be found on the Wikipedia
    page: [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As specified in our machine learning problem framing, we will use as input data
    the market observations for the period January-December 2020, as provided by the
    Yahoo data API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code excerpt, which uses the `pandas_datareader` module available
    in our workbench, allows us to easily retrieve the data that we want. The complete
    working notebook is available at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For this particular problem, we will retrieve data from 2014 up to the end
    of 2020, as represented in the table provided in *Figure 4.3*. The table provides
    value information about High, Low, Open, and Close for the BTC stock of the trading
    section. This data will be used to train the models in the current chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Listing the data retrieved from the source (Yahoo Finance)
  prefs: []
  type: TYPE_NORMAL
- en: 'This data can easily be plotted by plotting one of the variables just to illustrate
    the continuous nature of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To illustrate a bit more about the nature of the data, we can plot an excerpt
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source
    (Yahoo Finance)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source
    (Yahoo Finance)
  prefs: []
  type: TYPE_NORMAL
- en: Having defined precisely what we will be experimenting with in this section,
    we will move to add new models to enable us to run experiments and compare among
    them.
  prefs: []
  type: TYPE_NORMAL
- en: The data for the required range was conveniently saved in a file under `Chapter04/gradflow/notebooks/training_data.csv`,
    for the period ranging from 2014 to 2020 inclusive, so it can be easily retrieved
    during the modeling phase.
  prefs: []
  type: TYPE_NORMAL
- en: Adding experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, in this section, we will use the experiments module in **MLflow** to track
    the different runs of different models and post them in our workbench database
    so that the performance results can be compared side by side.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments can actually be done by different model developers as long as
    they are all pointing to a shared MLflow infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: To create our first, we will pick a set of model families and evaluate our problem
    on each of the cases. In broader terms, the major families for classification
    can be tree-based models, linear models, and neural networks. By looking at the
    metric that performs better on each of the cases, we can then direct tuning to
    the best model and use it as our initial model in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our choice for this section includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic Classifier**: Part of the family of linear-based models and a commonly
    used baseline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Xgboost**: This belongs to the family of tree boosting algorithms where many
    weak tree classifiers are assembled to produce a stronger model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras**: This type of model belongs to the neural network''s family and is
    generally indicated for situations where there is a lot of data available and
    relations are not linear between the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps to set up a new model are quite common and there will be overlapping
    and repeated code for each of the models. We will start next with a logistic regression-based
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up a logistic-based classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this sub-section, we will implement a logistic regression classifier in `scikit-learn`
    and train a model with our input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the `Chapter04/gradflow/notebooks/mlflow_run_logistic_regression.ipynb`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SKLearn` model, `LogisticRegression`, and the metrics functionality, `f1_score`,
    that will enable us to calculate the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`training_data.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data is split into training and testing using the `train_test_split` function,
    which takes one-third of the data for testing, with the remainder being used for
    training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mlflow.set_experiment` method. This will create an experiment if it does not
    exist or associate your current run with an experiment. We use `mlflow.sklearn.autolog()`
    to enable the automated capabilities of MLflow to capture the metrics of our experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`with`. The `mlflow.start_run` function is used to take care of registering
    your run with a specific `run_name` so that it can be identified and encloses
    the `fit` model, with evaluation code used to calculate the performance metrics
    of the `f1_score` experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Additionally, we need to log our specific metric, `f1_experiment_score`, with
    the `mlflow.log_metric` function. The main reason for adding our specific method
    is that for each model, the autologging functionality in **MLflow** uses the default
    metric used by each underlying framework and generally, these metrics don't match.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After executing all the steps relating to model development, we can now navigate
    to our run and visualize the log of the experiment. In *Figure 4.5*, you can see
    the specific parameters associated with logistic regression, durations, and all
    the parameters used on your run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Logistic regression model details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Logistic regression model details
  prefs: []
  type: TYPE_NORMAL
- en: 'For `SKLearn` models, **MLflow** automatically logs confusion matrices and
    precision and recall curves that are very useful in detecting how well the model
    performed on training data. For instance, the *Figure 4.6* report will be stored
    in the artifacts of your run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Confusion matrix metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Confusion matrix metrics
  prefs: []
  type: TYPE_NORMAL
- en: MLflow provides built-in metrics for Sklearn, providing better visibility of
    the model produced during training without the developer needing to produce extra
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up an XGBoost-based classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now implement a gradient tree-based algorithm using the `XGBoost` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the `Chapter04/gradflow/notebooks/mlflow_run_xgboost.ipynb`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing dependencies**: The XGBoost library is imported alongside the metrics
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`training_data.csv` file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "`Baseline_Predictions`, and we need to give MLflow the instruction to automatically\
    \ \Llog the model through `mlflow.xgboost.autolog`:"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`f1_score`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing all the steps relating to model development, we can now navigate
    to our run and visualize the log of the experiment. In *Figure 4.7*, you can see
    the specific parameters associated with `xgboost_model_baseline`, durations, and
    all the parameters used on your run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – XGBoost classifier details in MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – XGBoost classifier details in MLflow
  prefs: []
  type: TYPE_NORMAL
- en: 'For XGBoost models, **MLflow** automatically logs feature information and importance.
    We can see in *Figure 4.8* the ranking of our features in the model stored in
    the *Artifacts* section of the workbench:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – XGBoost feature importance on MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – XGBoost feature importance on MLflow
  prefs: []
  type: TYPE_NORMAL
- en: The feature importance graph in *Figure 4.8* allows the developer to have some
    insights into the internals of the model ascertained from the data. In this particular
    case, it seems that the second and seventh days of the 14 days in the input vector
    are the top two meaningful features. We will next implement a deep learning-based
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for setting up a deep learning-based classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will implement a neural network algorithm to solve our classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete notebook for this model is available in the book''s repository
    and can be used to follow along in the Chapter04/gradflow/notebooks/mlflow_run_keras.ipynb
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow`, as we are using it as a backend for `keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Retrieving data**: Refer to Step 2 in the *Steps for setting up an XGBoost-based
    classifier* section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Baseline_Predictions`, and we need to give MLflow the instruction to automatically
    log the model through `mlflow.tensorflow.autolog`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Sklearn` or XGBoost classifiers, so we need to define the layers and architecture
    of the network. In this particular case, the `Sequential` architecture and the
    model need to be compiled as required by Tensorflow:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`run_name` and fitting the model followed by calculating the `f1_score` metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For `keras` models, **MLflow** automatically logs a myriad of neural network-related
    data, namely, regarding optimizers and epoch and batch sizes, as well as other
    relevant information that can be seen in *Figure 4.9*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Keras classifier model details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Keras classifier model details
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, **TensorFlow** logs can be hooked into a TensorBoard. This is
    a TensorFlow built-in tool to provide visualizations and metrics for the machine
    learning workflow. Interfaces are created so that the model developer can leverage
    the native TensorFlow instrumentation and specialized visualization tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Having set up our classifiers in our platform, in the next section, we are ready
    to compare the performance of the different classifiers developed using MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have run the experiments in this section for each of the models covered
    and verified all the different artifacts. Just by looking at our baseline experiment
    table, and by selecting the common custom metric, `f1_experiment_score`, we can
    see that the best performing model is the logistic regression-based model, with
    an F-score of 0.66:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16783_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Comparing different model performance in terms of the goal metric
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics can also be compared side by side, as shown in the excerpt in *Figure
    4.11*. On the left side, we have the `SKlearn` model, and on the right the XGBoost
    model, with the custom metrics of `f1_experiment_score`. We can see that the metrics
    provided by both are different and, hence, the reason for custom metrics when
    we have different models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Metrics of the Sklearn model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Metrics of the Sklearn model
  prefs: []
  type: TYPE_NORMAL
- en: After comparing the metrics, it becomes clear that the best model is logistic
    regression. To improve the model, in the next section, we will optimize its parameters
    with state-of-the-art techniques and use MLflow experiment features to achieve
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning your model with hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models have many parameters that allow the developer to improve
    performance and control the model that they are using, providing leverage to better
    fit the data and production use cases. Hyperparameter optimization is the systematic
    and automated process of identifying the optimal parameters for your machine learning
    model and is critical for the successful deployment of such a system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we identified the best family (in other words, `LogisticRegression`)
    model for our problem, so now it''s time to identify the right parameters for
    our model with MLflow. You can follow along in the following notebook in the project
    repository, Chapter04/gradflow/notebooks/hyperopt_optimization_logistic_regression_mlflow.ipynb:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hyperopt` library, which contains multiple algorithms to help us carry out
    model tuning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`f1_score` metric in our model. The way optimization works in `hyperopt` is
    through minimization, but in our case, we want the maximum possible `f1_score`
    metric. So, the way we define our loss (the function to minimize) is as the inverse
    of our `f1_score` metric, as in `loss = 1-fscore`, so the minimization of this
    function will represent the best `f1_score` metric. For each run of the model''s
    parameters, we will enclose it in an `mlflow.start_run(nested=True)` in such a
    way that each optimization iteration will be logged as a sub run of the main job,
    providing multiple advantages in terms of comparing metrics across runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`best` variable. The core function is the minimization represented by `fmin(fn
    = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials
    = bayes_trials)`, where we provide the parameter space and objective function
    as previously defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After running the experiment for a few minutes, we can now review the experiments
    in `Hyperopt_Optimization` experiment:![Figure 4.12 – Listing all the nested runs
    of the hyperparameter tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16783_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.12 – Listing all the nested runs of the hyperparameter tuning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By clicking on the `training_f1_score` and the solver:![](img/B16783_04_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 4.13 – Listing all the nested runs of the hyperparameter tuning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can easily compare in the same interface the different solvers and implications
    for our performance metric, providing further insights into our modeling phase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Listing all the nested runs of the hyperparameter tuning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16783_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – Listing all the nested runs of the hyperparameter tuning
  prefs: []
  type: TYPE_NORMAL
- en: We concluded this section by optimizing the parameters of the most performant
    model for our current problem. In the next chapter of the book, we will be using
    the information provided by the best model to delve into the life cycle of the
    model management in **MLflow**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the experiments component of MLflow. We got to
    understand the logging metrics and artifacts in MLflow. We detailed the steps
    to track experiments in MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the final sections, we explored the use case of hyperparameter optimization
    using the concepts learned in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on managing models with MLflow using the
    models developed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To consolidate your knowledge further, you can consult the documentation available
    at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: https://www.mlflow.org/docs/latest/tracking.html
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: h[ttps://en.wikipedia.org/wiki/Hyperparameter_optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
