- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Privacy and Responsible AI Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we talked about how to build a data governance program
    for our organization and how to identify types of sensitive data. Our work does
    not stop there. Although in some cases we can safely exclude sensitive information,
    other times we cannot. So, our **machine learning** (**ML**) models that solve
    problems might need to contain personal data. Sometimes that data can be relevant
    and useful, or it can create unintended correlations that make the model biased.
    This is the issue that we will tackle in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will talk about how to recognize sensitive information and how to mitigate
    it if it is not relevant to the model training process by using techniques such
    as differential privacy. We will explore how to protect individual information
    even from aggregated data or the model results. To help us with that, we will
    see how we can use the SmartNoise **software development** **kit** (**SDK**).
  prefs: []
  type: TYPE_NORMAL
- en: We will also discuss fairness and how you can recognize bias in your model’s
    predictions. Here, we will apply the responsible AI principles we have learned
    together with the Fairlearn library and the Responsible AI dashboard. Together
    with bias comes model interpretability. We will analyze together how to calculate
    which features affect the prediction of your model for global or individual predictions
    by generating feature-importance values with model explainers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will wrap up by explaining **federated learning** (**FL**) and secure
    multi-party computation to protect sensitive data in cross-organizational ML scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering and protecting sensitive data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing differential privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigating fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with model interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring FL and secure multi-party computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to protect your data against bias
    and privacy without compromising the quality of your predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter is available in this repository under the `ch5` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-Model-Security-in-Azure/](https://github.com/PacktPublishing/Machine-Learning-Model-Security-in-Azure/)'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use the libraries, you need to be familiar with Python. In this book, we
    will use notebooks from the Azure Machine Learning environment to run the examples,
    but if you prefer to use your own development environment and tools, that is fine.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Python
  prefs: []
  type: TYPE_NORMAL
- en: 'New to Python and ML? Take a look at this learning path to learn the basics
    of Python: [https://learn.microsoft.com/en-us/training/paths/beginner-python/](https://learn.microsoft.com/en-us/training/paths/beginner-python/).'
  prefs: []
  type: TYPE_NORMAL
- en: Running a notebook in Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of running a notebook in Azure Machine Learning is very straightforward.
    All you need to do is import or create a workbook in the interface, attach a compute
    target, and then run the cells. Let us see the steps together:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **Notebooks** section and upload or create your file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Azure Machine Learning notebooks](img/B21076_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Azure Machine Learning notebooks
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the notebook file and attach a running compute target from the **Compute**
    dropdown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Attaching a compute target to a notebook](img/B21076_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Attaching a compute target to a notebook
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook as usual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing the SmartNoise SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SmartNoise SDK ([https://smartnoise.org/](https://smartnoise.org/)) is a
    differential privacy toolkit that you can use in ML or analytics. Here, we’ll
    see how we can install the library in order to use it later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install SmartNoise SQL, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To install SmartNoise Synthesizers, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: SmartNoise documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the complete documentation here: [https://docs.smartnoise.org/](https://docs.smartnoise.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Fairlearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fairlearn can be installed with `pip` from PyPI using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Fairlearn documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the complete documentation here: [https://fairlearn.org/](https://fairlearn.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering and protecting sensitive data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although having good governance and working with multiple tools that work with
    data can help us with sensitive data discovery classification and profiling, more
    often than not, the data used in our ML experiments comes from outside sources,
    or maybe we are simply not developing for our own organization. In that case,
    we need to train ourselves on what sensitive data is and how to do a quick cleanup
    if we need to use Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying sensitive data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sensitive data refers to any information that, if exposed, could cause harm,
    privacy breaches, or lead to identity theft, monetary loss, or other adverse consequences
    for individuals or organizations. This data requires special protection due to
    its nature and the potential risks associated with its disclosure.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many categories of sensitive data, many of which are outlined ahead,
    together with examples that we need to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Personally identifiable information** (**PII**): Information that can be
    used to identify an individual, such as full name, date of birth, social security
    number, driver’s license number, passport number, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial information**: Credit card numbers, bank account details, financial
    transaction records, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health information**: Medical records, health insurance information, mental
    health records, and other health-related data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Passwords and authentication data**: Usernames, passwords, security questions,
    or any other credentials used to access systems or accounts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Biometric data**: Fingerprints, retinal scans, facial recognition data, and
    other biometric identifiers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidential business information**: Trade secrets, **intellectual property**
    (**IP**), financial reports, customer lists, proprietary algorithms, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Government classified information**: Information classified by governments
    for national security reasons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personal communications**: Private messages, emails, and other communications
    that individuals expect to be confidential'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social and demographic information**: Race, ethnicity, religion, sexual orientation,
    and other sensitive demographic data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geolocation data**: Precise location data of individuals or assets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of that data we can exclude from our ML process. For example, if we are
    training a model to predict diabetes, we don’t need the patient data but the symptom
    data. In this case, we can safely exclude that information from our dataset by
    using the techniques we will explore in the next section. But if we are training
    a model to recognize faces, we need the actual biometric data.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see some techniques that help us clean up sensitive data so that it is
    not known to the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data anonymization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data anonymization is a process of removing or obfuscating PII from a dataset
    to protect the privacy of individuals while still maintaining the data’s overall
    usefulness for analysis and research purposes. The goal is to ensure that the
    data cannot be linked back to specific individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common techniques used in data anonymization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removing direct identifiers**: The most straightforward method is to remove
    direct identifiers such as names, social security numbers, phone numbers, email
    addresses, and so on from the dataset. We can use a unique identification number
    that identifies each record to maintain uniqueness or correlation, but anything
    that can identify a person is removed completely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudonymization**: The process of replacing sensitive data with pseudonyms
    or randomly generated identifiers is called pseudonymization. The objective is
    to obscure the original identity of individuals or entities in a dataset while
    allowing data processing and analysis to continue using the pseudonymized data.
    Unlike full anonymization, pseudonymization retains the structure and format of
    the original data, making it useful for certain purposes while still protecting
    privacy. This way, the original data is still present in the dataset, but the
    linkage to specific individuals is broken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice – combining pseudonymization with other techniques
  prefs: []
  type: TYPE_NORMAL
- en: Pseudonymization is a very good privacy-enhancing technique, but it’s not guaranteed.
    If additional information or external datasets can be combined with the pseudonymized
    data, identification of the individual may still be possible. Therefore, it’s
    good to combine pseudonymization with other security measures to effectively protect
    sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data masking or tokenization**: This technique is very similar to pseudonymization,
    but instead of encrypting or replacing sensitive data, data masking or tokenization
    replaces the values with randomly generated tokens or symbols. For example, if
    there is a credit card number present, all digits except the last four will be
    replaced by a star symbol. Data masking or tokenization might result in data that
    is not useful to the ML process. Therefore, you might want to remove the data
    altogether.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalization or aggregation**: This involves grouping data into broader
    categories to reduce the level of detail while still preserving overall patterns
    and trends. This technique is often used in data anonymization to protect individual
    privacy. The goal of generalization or aggregation is to strike a balance between
    data utility and privacy. While it reduces the risk of directly identifying individuals,
    it still provides valuable insights for analysis and research. However, it’s important
    to carefully consider the level of generalization to avoid potential re-identification
    risks, especially when combined with other available information. Here are some
    examples of generalization or aggregation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Age ranges**: Instead of using exact ages, data can be generalized into age
    ranges such as *18-24*, *25-34*, *35-44*, and so on. This maintains the information
    about age groups without revealing precise ages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geographical aggregation**: Instead of using precise addresses, data can
    be aggregated at the city, state, or country level. For instance, *New York* could
    represent data from various neighborhoods within the city.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Income brackets**: Instead of exact income values, data can be grouped into
    income brackets such as *Low Income*, *Middle Income*, and *High Income*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time intervals**: Temporal data can be aggregated into time intervals, such
    as days, weeks, or months, rather than using exact timestamps. For example, *Q1
    2023* could represent data from *January* to *March*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education levels**: Instead of specific degrees, data can be grouped by education-level
    categories such as *high school diploma*, *bachelor’s degree*, *master’s degree*,
    and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product categories**: Sales data can be aggregated at the product category
    level rather than listing individual products; for example, *electronics*, *clothing*,
    and *furniture*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer segments**: Data can be grouped into segments based on customer
    behavior or characteristics, such as *frequent shoppers*, *new customers*, or
    *high-spending customers*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction amount ranges**: Instead of exact transaction amounts, financial
    data can be grouped into ranges such as *$0-$50*, *$50-$100*, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health conditions**: Medical data can be aggregated into broader health condition
    categories instead of specifying individual diagnoses; for example, *cardiovascular*,
    *respiratory*, and *neurological*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web browsing patterns**: Internet browsing data can be aggregated based on
    website categories (for example, news, entertainment, shopping) rather than recording
    every visited website.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing, combining, or masking sensitive information is not the only option.
    When we need to use sensitive data to train models, there are other techniques
    to either protect the data or manipulate the data in a way that still protects
    any sensitive information without limiting our model’s potential.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore how to protect the privacy of individual data without removing
    it from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Differential privacy is a concept that has the purpose of protecting the privacy
    of individual data contributors while still allowing useful statistical analysis.
    The basic idea behind differential privacy is to add noise or random perturbations
    to the data in such a way that the statistical properties of the dataset stay
    the same, but it is much more difficult to identify individual information within
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The level of privacy protection in differential privacy is controlled by a
    parameter called epsilon (ε). A smaller value of epsilon indicates a higher level
    of privacy, but it might also lead to a decrease in data utility (usefulness of
    the data for analysis). Striking a balance between privacy and utility is a key
    challenge in implementing differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 5.3 – Epsilon (\uFEFFƐ) value relationship with privacy and accuracy](img/B21076_05_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Epsilon (Ɛ) value relationship with privacy and accuracy
  prefs: []
  type: TYPE_NORMAL
- en: A library that we can use to add noise to the data is the SmartNoise SDK. SmartNoise
    is an open source SDK designed to implement differential privacy in various data
    analysis and ML workflows. It is developed by OpenDP and aims to make it easier
    for data analysts, researchers, and data scientists to apply differential privacy
    techniques to their data without extensive knowledge of the underlying mathematical
    complexities.
  prefs: []
  type: TYPE_NORMAL
- en: The SmartNoise SDK provides a set of tools and utilities that can be integrated
    into existing data analysis and ML pipelines to ensure privacy-preserving computations.
    It offers an abstraction layer for adding differential privacy to computations,
    allowing data analysts to easily specify privacy parameters (such as epsilon)
    and apply privacy-preserving mechanisms without dealing directly with the intricacies
    of differential privacy algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different components you can use, and these are the official recommendations
    from the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Use **OpenDP** directly when working with Jupyter notebooks and reproducible
    research or if you require fine-grained control over processing and privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use **SmartNoise SQL** if you are working with large datasets or data cubes
    over tabular data stored in SQL databases or Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use **SmartNoise Synthesizers** if you are still in the research process and
    you want to see what the result will look like with other collaborators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we will see an example by using SmartNoise SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install SmartNoise SQL by running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: data_path = 'mockdata.csv'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: mockdata = pd.read_csv(data_path)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: actualdata = mockdata[['age','diabetic']].groupby(\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[''diabetic'']).mean().to_markdown()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(actualdata)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us see what happens when we add noise to the data. First, we need to declare
    the epsilon variable. We will execute this code multiple times, first with a low
    epsilon value for greater privacy (`0.05`) and second with a high epsilon value
    for accuracy (`0.90`), and compare the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see in the following table a comparison of multiple executions of the
    preceding code side by side, and the average age for non-diabetic and diabetic
    patients is almost the same (around 47 years old), but when we use the SmartNoise
    SDK library with different epsilon values, the results start to vary from 1% to
    15% depending on the epsilon value used. This percentage might seem high; however,
    it is up to us to determine the balance between privacy and accuracy. The only
    thing the library guarantees is to maintain statistical uniformity:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Actual average** **age** | **Execution 1****Average age** | **Execution
    2****Average age** | **Execution 3****Average age** | **Execution 4****Average
    age** |'
  prefs: []
  type: TYPE_TB
- en: '| Epsilon value | N/A | 0.05 | 0.05 | 0.90 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| Non- diabetic | 47.4101 | 54.2823275862069 | 53.209829867674856 | 47.72727272727273
    | 47.38953488372093 |'
  prefs: []
  type: TYPE_TB
- en: '| Diabetic | 47.4741 | 42.19132149901381 | 44.00204081632653 | 47.36438923395445
    | 47.616977225672876 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Dataset results
  prefs: []
  type: TYPE_NORMAL
- en: You can see how to run this notebook in the book repository mentioned previously
    in the *Technical requirements* section. This way, we can protect the privacy
    of the data without compromising the results of our model. What happens, though,
    when the data we are trying to protect actually affects the predictions, resulting
    in a negative impact?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can ensure our data is private and the model also provides
    fair results for different sensitive groups.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mitigating fairness in ML models is an essential step to ensure that the model
    does not exhibit bias or discrimination against certain groups of individuals.
    Even though we can remove PII from our datasets, predictions might favor different
    groups based on characteristics such as race, gender, age, or religion. If the
    training data is not diverse and representative of the population you aim to serve,
    bias can creep into the model if the data does not adequately represent all groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to learn to identify bias in our models. This is easy by conducting
    an analysis of the metrics of the model. Suppose you suspect that your load approval
    model favors people above a certain age to get their loan application approved.
    You can start by looking at the metrics for the complete dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Selection Rate** | **Accuracy** | **Recall** | **Precision** |'
  prefs: []
  type: TYPE_TB
- en: '| Complete dataset | 0.337 | 0.8895 | 0.8385650224215246 | 0.8323442136498517
    |'
  prefs: []
  type: TYPE_TB
- en: Table 5.2 – Dataset metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, calculate the same metrics by age group. We can do this by using a library
    such as Fairlearn. The result from the split looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Selection Rate** | **Accuracy** | **Recall** | **Precision** |'
  prefs: []
  type: TYPE_TB
- en: '| Age 30 or younger | 0.299282 | 0.890668 | 0.818519 | 0.815498 |'
  prefs: []
  type: TYPE_TB
- en: '| Age over 30 | 0.698413 | 0.878307 | 0.922481 | 0.901515 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.3 – Metrics per age group
  prefs: []
  type: TYPE_NORMAL
- en: Metrics explained
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics are specific to the type of model. The preceding metrics are for classification
    models. You can find a list of metrics for different models and charts with an
    explanation here: [https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2).'
  prefs: []
  type: TYPE_NORMAL
- en: From these metrics, you should be able to discern that a larger proportion of
    older individuals are predicted to be approved for the loan. Accuracy should be
    more or less equal for the two groups, but a closer inspection of precision and
    recall indicates some disparity in how well the model predicts for each age group.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, consider **Recall**. This metric indicates the proportion
    of positive cases that were correctly identified by the model. In other words,
    of all the individuals who should get approval for their loan application and
    they actually do, how many did the model find? The model seems to do a better
    job in the older age group.
  prefs: []
  type: TYPE_NORMAL
- en: So, what do we do now? Do we try to correct the data and model to predict equally
    between the two groups? The short answer is *not yet*. We need to consider the
    context first and evaluate why the model exhibits this behavior, and whether this
    is justified. Remember that metrics are just metrics, and it is up to us to interpret
    them. Maybe our model favors the older age group because they are more financially
    stable. We need to investigate more before we reach a decision because this identified
    bias might be reasonable. Suppose we had a face recognition application that favored
    light-skinned people over dark-skinned people; this would be a clear bias and
    would need to be corrected. The context and the model’s purpose will define our
    next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Fairlearn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fairlearn is an open source project to help ML engineers and data scientists
    improve the fairness of AI systems. It can assist by providing fairness-related
    metrics that can be compared between groups and for the overall population. To
    calculate those metrics and conduct an investigation, the Fairlearn SDK is immensely
    helpful. It breaks down the metrics from each sensitive group, and then you can
    use the fairness dashboard to complete your assessment, as looking at the data
    visually always helps.
  prefs: []
  type: TYPE_NORMAL
- en: Fairlearn SDK and Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'This feature is in Preview at the time of writing, so you can see limitations
    and applications in the Azure Machine Learning environment here: [https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml?view=azureml-api-1](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-fairness-aml?view=azureml-api-1).
    The Fairlearn project documentation can be found here: [https://fairlearn.org/](https://fairlearn.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see an example process of working with the visual dashboard.
    You can generate the UI in your notebooks, and it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Fairness dashboard main page](img/B21076_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Fairness dashboard main page
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also register the model and upload the dashboard data to your workspace
    to conduct your assessment, as seen in the following screenshot. The process is
    much easier with the help of the wizard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to choose sensitive features; for example, the **Age** column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Step 1: Choosing your sensitive features](img/B21076_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5 – Step 1: Choosing your sensitive features'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to choose a metric to evaluate against in order to examine
    any possible bias. Depending on the model algorithm, you might get different metrics
    here to choose from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Step 2: Choosing a primary metric to measure performance](img/B21076_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6 – Step 2: Choosing a primary metric to measure performance'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third and final step, you set the parity constraints against which you
    want to measure fairness; for example, demographic parity or selection rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Step 3: Choose how you want to measure fairness](img/B21076_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7 – Step 3: Choose how you want to measure fairness'
  prefs: []
  type: TYPE_NORMAL
- en: 'The service runs an analysis based on the parameters and returns results that
    we can use to determine if the model favors one or more groups based on sensitive
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Reviewing the results](img/B21076_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Reviewing the results
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The integration with the Fairlearn open source package is supported at the time
    of writing only on the `azureml v1` Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that complete fairness may not always be achievable, and trade-offs
    may exist between different fairness goals. We need to ensure that the model performs
    adequately for all sensitive groups in our datasets. The key is to make informed
    decisions about fairness trade-offs and continuously strive to improve the model’s
    fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Another part of responsible AI is the ability to explain how a model makes predictions.
    Let us see some techniques in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Working with model interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model interpretability in ML refers to the ability to understand and explain
    how a particular model makes predictions or decisions. Interpretable models provide
    clear insights into the features or variables that are most influential in the
    model’s decision-making process. This is particularly important in domains where
    the decision-making process needs to be transparent and understandable, such as
    healthcare, finance, and legal systems.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can never explain 100% why a model makes a prediction, you can
    use explainers to understand which features affect the results. Explainers can
    help us provide global explanations; for example, which features affect the overall
    behavior of the model or local explanations that provide us with information on
    what influenced an individual prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us explore some methods we can use to achieve model interpretability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature importance** (**FI**) determines the influence of each feature in
    influencing the model’s predictions. Techniques such as the following can be used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permutation FI** (**PFI**): This method involves randomly shuffling the values
    of each feature and measuring the impact on the model’s performance. Features
    with the highest drop in performance after shuffling are considered more important.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SHapley Additive exPlanations (SHAP) values**: SHAP values provide a unified
    measure of FI based on cooperative game theory. They assign contributions to each
    feature in a prediction, considering all possible feature combinations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-agnostic Explanations** (**LIME**): LIME creates
    local interpretable models around a specific prediction by perturbing the data
    and observing the impact on the prediction. It helps explain individual predictions
    for any model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial dependence plots** (**PDP**) and **individual conditional expectation**
    (**ICE**) test the feature influence on the model by using different techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDP plots the average effect of a single feature on the model’s predictions
    while keeping other features constant.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ICE plots multiple individual PDPs, one for each instance, providing a more
    granular view of how the feature affects different instances.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rule-based models**: Decision trees and linear models are inherently interpretable
    as their structure can be easily visualized and understood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proxy models**: Train a simpler, interpretable model to approximate the behavior
    of a more complex model. This allows for better understanding without sacrificing
    too much accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualizations**: Visualizations such as heatmaps, saliency maps, and activation
    maps can help understand how the model processes and weighs different input features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer-wise Relevance Propagation** (**LRP**): LRP is a technique used in
    **neural networks** (**NNs**) to understand which input features contribute most
    to a specific output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method we choose depends on our model and the flavor used to create it.
    Let us see the options we have for model interpretability in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Responsible AI dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To work with interpretability in Azure Machine Learning, you can use the Interpret-Community
    package for the v1 SDK, or it is recommended to use the newer version, which is
    part of the new Responsible AI dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us explore the capabilities available:'
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability in Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Learn more about the Interpret-Community package in Azure Machine Learning
    and how common explainers work in the following documentation: [https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?view=azureml-api-2#supported-model-interpretability-techniques](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability?view=azureml-api-2#supported-model-interpretability-techniques).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can generate a dashboard in any model in the MLflow format generated with
    the scikit-learn by going to your registered model under the **Models** menu,
    choosing the **Responsible AI** tab, and clicking on the **Create dashboard**
    button, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Creating a Responsible AI dashboard](img/B21076_05_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Creating a Responsible AI dashboard
  prefs: []
  type: TYPE_NORMAL
- en: MLflow models
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow is an open source platform designed to manage the end-to-end ML life
    cycle by providing a consistent and easy-to-use framework, making collaboration
    and reproducibility more accessible for data science and ML teams. Find out more
    here: [https://mlflow.org/](https://mlflow.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as the dashboard is generated, you can click on it on the list to view
    more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Opening the dashboard](img/B21076_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Opening the dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'The dashboard has a lot of information to help not only with explainability
    but also metrics about fairness, data distribution, and individual predictions.
    Along with the data, there are numerous visualizations you can take advantage
    of to analyze your model, as seen in the following screenshot. Make sure you adhere
    to responsible AI development principles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Reviewing the metrics](img/B21076_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Reviewing the metrics
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to model interpretability, you can see FIs for the complete dataset
    and for individual predictions. You can tweak the class importance weights and
    the chart to suit your needs and understand the model better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the next screenshot, we can see global FIs for a sample diabetes dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Reviewing FIs](img/B21076_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Reviewing FIs
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability is a balance between simplicity and accuracy. Highly
    interpretable models might sacrifice some predictive performance, while very complex
    models might be difficult to explain comprehensively. The choice of interpretability
    method depends on the specific use case, audience, and the trade-off between model
    complexity and transparency.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an AI dashboard using the SDK
  prefs: []
  type: TYPE_NORMAL
- en: 'For further resources to build the AI dashboard using the SDK using YAML and
    Python, see here: [https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-insights-sdk-cli?view=azureml-api-2&tabs=yaml](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-insights-sdk-cli?view=azureml-api-2&tabs=yaml).'
  prefs: []
  type: TYPE_NORMAL
- en: All the techniques we have seen so far focus on having one dataset that is trained
    on one compute resource to generate the resulting model. In the next section,
    we will see some techniques that focus on splitting model training and datasets
    between different compute sources.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring FL and secure multi-party computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**FL** is an ML approach that enables the training of models across multiple
    devices or servers without centrally aggregating the raw data. In traditional
    ML, data is usually collected and sent to a central compute server for training,
    which raises privacy and security concerns, especially when dealing with sensitive
    or personal information.'
  prefs: []
  type: TYPE_NORMAL
- en: In FL, the training process happens locally on the devices or nodes (for example
    smartphones, edge devices, or compute instances) that generate or store the data.
    These nodes collaborate by sharing only model updates (gradients) rather than
    the raw data itself. The central compute server aggregates these updates to create
    an improved global model. This process is repeated iteratively, with each node
    contributing to the model’s improvement while keeping its data private.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of FL are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy**: As the raw data remains on the local nodes, there is no need to
    share sensitive information with a central server, reducing the risk of data leaks
    and breaches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced data transmission**: FL decreases the amount of data that needs to
    be sent over the network, which can be beneficial when dealing with large datasets
    or bandwidth constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decentralization**: The model training process can be distributed across
    a large number of nodes, enabling scalability and robustness in a distributed
    environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local adaptation**: Nodes can update the global model while taking into account
    their local data distribution, leading to models that are more relevant and tailored
    to specific local characteristics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FL is especially useful in scenarios where data privacy and security are crucial,
    such as healthcare and financial services, and processing can be distributed as
    in **Internet of Things** (**IoT**) applications. It gives us the opportunity
    to leverage the collective knowledge from distributed data sources without compromising
    the privacy of individual users.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see a quick introduction to FL and its applications with Azure Machine
    Learning.
  prefs: []
  type: TYPE_NORMAL
- en: FL with Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working in the cloud, FL can be easier than you might think. With on-demand
    compute and processing power come a lot of benefits. Especially with the `azureml`
    SDK v2, FL features are built in.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to integrate is by using designer pipelines. The designer has
    a drag-and-drop interface. With the new version of the SDK come several exciting
    features. We can create our own component very easily, and each component that
    you drag and drop in the pipeline can be run on a different compute target.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see a pipeline created in the designer
    from the pre-built samples. By clicking on a component, we can easily change the
    compute target from the **Pipeline interface** button by going to the **Run settings**
    option and choosing **Use other** **compute target**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Changing the compute target in Azure Machine Learning](img/B21076_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Changing the compute target in Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: If we extend the functionality and train multiple models at the same time using
    different datasets, we have an FL implementation and multi-party computation.
    For compute, we can also leverage Azure confidential computing to make the implementation
    even more secure, but we will talk about securing Azure compute in Azure Machine
    Learning later in the book. This can also be extended to using multiple workspaces
    connected to the same or different data stores.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about using FL with Azure Machine Learning is that you can still
    combine and apply all the metrics and techniques we have outlined in this chapter
    since we are still working within the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: FL recipes and examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Find out more on how to implement Azure Machine Learning here: [https://github.com/Azure-Samples/azure-ml-federated-learning](https://github.com/Azure-Samples/azure-ml-federated-learning).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Protecting sensitive data is a multi-faceted problem. There are ways and techniques
    to mitigate fairness and protect privacy work ethically and responsibly with AI,
    but the balance between prediction accuracy and data protection is very sensitive.
    If you add the complexity of choosing the right combination of techniques based
    on your data and algorithms, it can seem daunting.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned to identify different types of sensitive data and
    common techniques to remove or mask them. However, it is not always possible to
    completely eliminate them as they are useful for the model training process. In
    this case, there are several libraries available to help. We can use the SmartNoise
    SDK to introduce noise to our data and protect privacy, work with the Fairlearn
    SDK to mitigate fairness, and use the Responsible AI dashboard together with explainers
    to interpret our models. We ended this chapter by introducing the concept of FL
    and how to apply it using Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: We can talk about data until the end of the book, but the truth is, there is
    only so much you can do in data processing. In the next chapter, we will focus
    on working with access to the data, the workspace, and the roles required for
    each part.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Run a Federated Learning Demo in 5* *mins*: [https://github.com/Azure-Samples/azure-ml-federated-learning/blob/main/docs/quickstart.md](https://github.com/Azure-Samples/azure-ml-federated-learning/blob/main/docs/quickstart.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Federated Learning with Azure Machine Learning, NVIDIA FLARE and MONAI* –
    Build session: [https://build.microsoft.com/en-US/sessions/5bd5120f-5239-450d-8a57-373efb43c0cf?source=sessions](https://build.microsoft.com/en-US/sessions/5bd5120f-5239-450d-8a57-373efb43c0cf?source=sessions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Medical Imaging with Azure Machine Learning* *Demos*: [https://github.com/Azure/medical-imaging](https://github.com/Azure/medical-imaging)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Securing and Monitoring Your AI Environment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to the cloud, there is more to security than data. In this part,
    you will learn how to secure identity and access and all the cloud infrastructure
    around your Azure Machine Learning resources. Then, you will learn how to automate
    those processes using MLOps practices and how to set up system monitoring to detect
    and resolve any security issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21076_06.xhtml#_idTextAnchor141)*, Managing and Securing Access*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21076_07.xhtml#_idTextAnchor160)*, Managing and Securing your
    Azure Machine Learning Workspace*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21076_08.xhtml#_idTextAnchor176)*, Managing and Securing the
    MLOps Life Cycle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21076_09.xhtml#_idTextAnchor189)*, Logging, Monitoring, and
    Threat Detection*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
