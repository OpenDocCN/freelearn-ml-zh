<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer015">
<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor015"/>1 </h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Comprehending Google Cloud Services</h1>
<p>In Part 1 of this book, we will be building a foundation by focusing on Google Cloud and Python, the essential platform and tool for our learning journey, respectively.</p>
<p>In this chapter, we will dive into <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) and discuss the Google Cloud services that are closely related to <strong class="bold">Google Cloud Machine Learning</strong>. Mastering these services will provide us with a solid background.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding the GCP global infrastructure</li>
<li>Getting started with GCP</li>
<li>GCP organization structure</li>
<li>GCP Identity and Access Management</li>
<li>GCP compute spectrum </li>
<li>GCP storage and database services </li>
<li>GCP big data and analytics services</li>
<li>GCP artificial intelligence services</li>
</ul>
<p>Let’s get started.</p>
<h1 id="_idParaDest-17"><a id="_idTextAnchor017"/>Understanding the GCP global infrastructure </h1>
<p><strong class="bold">Google</strong> is one of the <a id="_idIndexMarker000"/>biggest cloud <a id="_idIndexMarker001"/>service providers in the world. With the physical computing infrastructures such as computers, hard disk drives, routers, and switches in Google’s worldwide data centers, which are connected by Google’s global backbone network, Google provides a full spectrum of cloud services in GCP, including compute, network, database, security, and advanced services such as big data, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), and many, many more.</p>
<p>Within Google’s global cloud infrastructure, there are many data center groups. Each data center group is called a <strong class="bold">GCP region</strong>. These<a id="_idIndexMarker002"/> regions are located worldwide, in Asia, Australia, Europe, North America, and South America. These regions are connected by Google’s global backbone network for performance optimization and resiliency. Each GCP region is a <a id="_idIndexMarker003"/>collection of <strong class="bold">zones</strong> that are isolated from each other. Each zone has one or more data centers and is identified by a name that combines a letter identifier with the region’s name. For example, zone <em class="italic">US-Central1-a</em> is a zone in the <em class="italic">US-Central1</em> region, which is physically located in Council Bluffs, Iowa, the United State of America. In the GCP global infrastructure, there are also many <strong class="bold">edge locations</strong> or <strong class="bold">points of</strong> <strong class="bold">presence</strong> (<strong class="bold">POPs</strong>) where <a id="_idIndexMarker004"/>Google’s global networks connect to the internet. More details about GCP regions, zones, and edge locations <a id="_idIndexMarker005"/>can be found at <a href="https://cloud.google.com/about/locations">https://cloud.google.com/about/locations</a>.</p>
<p>GCP provides on-demand cloud resources at a global scale. These resources can be used together to build solutions that <a id="_idIndexMarker006"/>help meet business goals and satisfy technology requirements. For example, if a company needs 1,000 TB of storage in Tokyo, its IT professional can log into their GCP account console and provision the storage in the <em class="italic">Asia-northeast1</em> region at any time. Similarly, a 3,000 TB database can be provisioned in Sydney and a 4,000-node cluster in Frankfurt at any time, with just a few clicks. And finally, if a company wants to set up a global website, such as <a href="http://zeebestbuy.com">zeebestbuy.com</a>, with the lowest latencies for their global users, they can build three web servers in the global regions of London, Virginia, and Singapore, and utilize Google’s global DNS service to distribute the web traffic along these three web servers. Depending on the user’s web browser location, DNS will route the traffic to the nearest web server.</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Getting started with GCP</h1>
<p>Now that we have<a id="_idIndexMarker007"/> learned about Google’s global cloud infrastructure and the on-demand resource provisioning concept of cloud computing, we can’t wait to dive into Google Cloud and provision resources in the cloud! </p>
<p>In this section, we will build cloud resources by doing the following:</p>
<ul>
<li>Creating a free-tier GCP account</li>
<li>Provisioning a virtual computer instance in Google Cloud</li>
<li>Provisioning our first storage in Google Cloud</li>
</ul>
<p>Let’s go through each of these steps in detail.</p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>Creating a free-tier GCP account</h2>
<p>Google provides<a id="_idIndexMarker008"/> a free-tier account type for us to get started on GCP. More details can be found at <a href="https://cloud.google.com/free/docs/gcp-free-tier">https://cloud.google.com/free/docs/gcp-free-tier</a>. </p>
<p>Once you have signed up for a GCP free-tier account, it’s time to plan our first resources in Google Cloud – a computer and a storage folder in the cloud. We will provision them as needed. How exciting!</p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>Provisioning our first computer in Google Cloud</h2>
<p>We will start with the<a id="_idIndexMarker009"/> simplest idea: provisioning<a id="_idIndexMarker010"/> a computer in the cloud. Think about a home computer for a moment. It has a <strong class="bold">Central Processing Unit</strong> (<strong class="bold">CPU</strong>), <strong class="bold">Random Access Memory</strong> (<strong class="bold">RAM</strong>), <strong class="bold">hard disk drives</strong> (<strong class="bold">HDDs</strong>), and a <strong class="bold">network interface card</strong> (<strong class="bold">NIC</strong>) for <a id="_idIndexMarker011"/>connecting to the relevant <strong class="bold">Internet Service Provider</strong> (<strong class="bold">ISP</strong>) equipment (such as<a id="_idIndexMarker012"/> cable modems and routers). It also has an <a id="_idIndexMarker013"/>operating system (Windows or Linux), and it may have a database such as MySQL for some family data management, or Microsoft Office for home office usage. </p>
<p>To provision a computer in Google Cloud, we will need to do the same planning for its hardware, such as the number of CPUs, RAM, and the size of HDDs, as well as for its software, such as the operating system (Linux or Windows) and database (<strong class="bold">MySQL</strong>). We may also need to plan the network for the computer, such as an external IP address, and whether the IP address needs to be static or dynamic. For example, if we plan to provision a web server, then our computer will need a static external IP address. And from a security point of view, we will need to set up the network firewalls so that only specific computers at home or work may access our computer in the cloud.  </p>
<p>GCP offers a cloud service for consumers to provision a<a id="_idIndexMarker014"/> computer in the cloud: <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>). With the GCE service, we can build flexible, self-managed <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) in the <a id="_idIndexMarker015"/>Google Cloud. GCE offers different hardware and software options based on consumers’ needs, so you can use customized VM types and select the appropriate operating system for the VM instances.  </p>
<p>Following the instructions at <a href="https://cloud.google.com/compute/docs/instances/create-start-instance">https://cloud.google.com/compute/docs/instances/create-start-instance</a>, you can create a VM in GCP. Let’s pause here and go to the GCP console to provision our first computer.</p>
<p>How do we access the <a id="_idIndexMarker016"/>computer? If the VM has a Windows operating system, you can use <strong class="bold">Remote Desktop</strong> to access it. For a Linux VM, you can use <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) to log in. More details are available at <a href="https://cloud.google.com/compute">https://cloud.google.com/compute</a>. </p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor021"/>Provisioning our first storage in Google Cloud </h2>
<p>When we open the <a id="_idIndexMarker017"/>computer case and look inside our home computer, we can see its hardware components – that is, its CPU, RAM, HDD, and NIC. The hard disks within a PC are limited in size and performance. <em class="italic">EMC</em>, a company founded in 1979 by Richard Egan and Roger Marino, expanded PC hard disks outside of the PC case to a separate computer network storage platform called <em class="italic">Symmetrix</em> in 1990. Symmetrix has its<a id="_idIndexMarker018"/> own CPU/RAM and provides huge storage capacities. It is connected to the computer through fiber cables and serves as the <strong class="bold">storage array</strong> of the computer. On<a id="_idIndexMarker019"/> the other hand, <em class="italic">SanDisk</em>, founded in 1988 by Eli Harari, Sanjay Mehrotra, and Jack Yuan, produced the first Flash-based <strong class="bold">solid-state drive</strong> (<strong class="bold">SSD</strong>) in a 2.5-inch<a id="_idIndexMarker020"/> hard drive, called <em class="italic">Cruzer</em>, in 2000. Cruzer provides<a id="_idIndexMarker021"/> portable storage via a USB connection to a computer. By thinking out of the box and extending either to Symmetrix or Cruzer, EMC and Sandisk extended the hard disk concept out of the box. These are great examples of start-up ideas!</p>
<p>And then comes the great idea of cloud computing – the concept of storage is further extended to cloud-block <a id="_idIndexMarker022"/>storage, cloud <strong class="bold">network-attached storage</strong> (<strong class="bold">NAS</strong>), and cloud object storage. Let’s look at these in more detail:</p>
<ul>
<li><strong class="bold">Cloud block storage</strong> is a form of<a id="_idIndexMarker023"/> software-based storage that can be attached to a VM in the cloud, just like a hard disk is attached to our PC at home. In Google Cloud, cloud block storage is called <strong class="bold">persistent disks</strong> (<strong class="bold">PD</strong>). Instead of buying a physical hard disk and installing it on the PC to use it, PDs can be <a id="_idIndexMarker024"/>created instantly and attached to a VM in the cloud, with only a couple of clicks. </li>
<li><strong class="bold">Cloud network-attached storage</strong> (<strong class="bold">Cloud NAS</strong>) is a form of software-based storage that can be shared<a id="_idIndexMarker025"/> among many cloud VMs through a virtual cloud network. In GCP, cloud NAS is called <strong class="bold">Filestore</strong>. Instead of buying<a id="_idIndexMarker026"/> a physical file server, installing it on a network, and sharing it with multiple PCs at home, a Filestore instance can be created instantly and shared by many cloud VMs, with only a couple of clicks. </li>
<li><strong class="bold">Cloud object storage</strong> is a form of <a id="_idIndexMarker027"/>software-based storage that can be used to store objects (files, images, and so on) in the cloud. In GCP, cloud object storage<a id="_idIndexMarker028"/> is called <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>). Different from PD, which is a cloud block storage type that’s used by a VM (it can be shared in read-only mode among multiple VMs), and Filestore, which is a cloud NAS type shared by many VMs, GCS is a cloud object type used for storing immutable objects. Objects are stored in GCS buckets. In GCP, bucket creation and deletion, object uploading, downloading, and <a id="_idIndexMarker029"/>deletion can all be done from the GCP console, with just a couple of clicks!</li>
</ul>
<p>GCS provides different storage classes based on the object accessing patterns. More details can be found at <a href="https://cloud.google.com/storage">https://cloud.google.com/storage</a>.</p>
<p>Following the instructions at <a href="https://cloud.google.com/storage/docs/creating-buckets">https://cloud.google.com/storage/docs/creating-buckets</a>, you can create a storage folder/bucket and upload objects into it. Let’s pause here and go to the GCP console to provision our first storage bucket and upload some objects into it.</p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor022"/>Managing resources using GCP Cloud Shell</h2>
<p>So far, we have discussed <a id="_idIndexMarker030"/>provisioning VMs and buckets/objects in the cloud from the GCP console. There is another tool that can help us create, manage, and delete resources: GCP Cloud Shell. Cloud Shell is a command-line interface that can easily be accessed from your console browser. After you click the <strong class="bold">Cloud Shell</strong> button on the GCP console, you will get a Cloud Shell – a command-line user interface on a VM, in your web browser, with all the cloud resource management commands already installed.</p>
<p>The following tools are provided by Google for customers to create and manage cloud resources using the command line:</p>
<ul>
<li>The <strong class="source-inline">gcloud</strong> tool is the main command-line interface for GCP products and services such as GCE.</li>
<li>The <strong class="source-inline">gsutil</strong> tool is for GCS services.</li>
<li>The <strong class="source-inline">bq</strong> tool is for BigQuery services.</li>
<li>The <strong class="source-inline">kubectl</strong> tool is for Kubernetes services.</li>
</ul>
<p>Please refer to <a href="https://cloud.google.com/shell/docs/using-cloudshell-command">https://cloud.google.com/shell/docs/using-cloudshell-command</a> for more information about GCP<a id="_idIndexMarker031"/> Cloud Shell and commands, as well as how to create a VM and a storage bucket using Cloud Shell commands.</p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor023"/>GCP networking – virtual private clouds </h2>
<p>Think about home<a id="_idIndexMarker032"/> computers again – they are all connected via a network, wired or wireless, so that they can connect to the internet. Without networking, a computer is almost useless. Within GCP, a cloud <a id="_idIndexMarker033"/>network unit is called a <strong class="bold">virtual private cloud</strong> (<strong class="bold">VPC</strong>). A VPC is a software-based logical network resource. Within a GCP project, a limited number of VPCs can be provisioned. After launching VMs in the cloud, you can connect them within a VPC, or isolate them from each other in separate VPCs. Since GCP VPCs are global and can span multiple regions in the world, you can provision a VPC, as well as the resources within it, anywhere in the world. Within a VPC, a public subnet has VMs with external IP addresses that are accessible from the internet and can access the internet; a private subnet contains VMs that do not have external IP addresses. VPCs can be peered with each other, within a GCP project, or outside a GCP project. </p>
<p>VPCs can be provisioned using the GCP console or GCP Cloud Shell. Please refer to <a href="https://cloud.google.com/vpc/">https://cloud.google.com/vpc/</a> for details. Let’s pause here and go to the GCP console to provision our VPC and subnets, and then launch some VMs into those subnets.</p>
<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>GCP organization structure</h1>
<p>Before we discuss the GCP <a id="_idIndexMarker034"/>cloud services further, we need to spend some time talking about the GCP organization structure, which is quite different from that of the <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) cloud and the Microsoft Azure cloud. </p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>The GCP resource hierarchy</h2>
<p>As shown in the following<a id="_idIndexMarker035"/> diagram, within a GCP cloud domain, at the top is the GCP organization, followed by folders, then projects. As a common practice, we can map a company’s organizational hierarchy to a GCP structure: a company maps to a GCP organization, its departments (sales, engineering, and more) are mapped to folders, and the functional projects from the departments are mapped to projects under the folders. Cloud resources such as VMs, <strong class="bold">databases</strong> (<strong class="bold">DBs</strong>), and so on are under the projects.</p>
<p>In a GCP organization hierarchy, <em class="italic">each project is a separate compartment, and each resource belongs to exactly one project</em>. Projects can have multiple owners and users. They are managed and billed separately, although multiple projects may be associated with the same billing account:</p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.1 – Sample GCP organization structure " height="826" src="image/Figure_1.1.jpg" width="1452"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Sample GCP organization structure</p>
<p>In the preceding diagram, there are two organizations: one for production and one for testing (sandbox). Under<a id="_idIndexMarker036"/> each organization, there are multiple layers of folders (note that the number of folder layers and the number of folders at each layer may be limited), and under each folder, there are multiple projects, each of which contains multiple resources.</p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>GCP projects</h2>
<p>GCP projects are the<a id="_idIndexMarker037"/> logical separations of GCP resources. Projects are used to fully isolate<a id="_idIndexMarker038"/> resources based on Google Cloud’s <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions:</p>
<ul>
<li><strong class="bold">Billing isolation</strong>: Use different projects to separate spending units</li>
<li><strong class="bold">Quotas and limits</strong>: Set at the project level and separated by workloads</li>
<li><strong class="bold">Administrative complexity</strong>: Set at the project level for access separation</li>
<li><strong class="bold">Blast radius</strong>: Misconfiguration issues are limited within a project</li>
<li><strong class="bold">Separation of duties</strong>: Business units and data sensitivity are separate</li>
</ul>
<p>In summary, the GCP organization structure provides a hierarchy for managing Google Cloud resources, with projects being the logical isolation and separation. In the next section, we will discuss resource permissions within the GCP organization by looking at IAM.</p>
<h1 id="_idParaDest-27"><a id="_idTextAnchor027"/>GCP Identity and Access Management</h1>
<p>Once we have reviewed<a id="_idIndexMarker039"/> the GCP organization structure and the GCP resources of VMs, storage, and network, we must look at the access management of these resources within the GCP organization: IAM. GCP IAM manages cloud identities<a id="_idIndexMarker040"/> using the <strong class="bold">AAA</strong> model: <strong class="bold">authentication</strong>, <strong class="bold">authorization</strong>, and <strong class="bold">auditing</strong> (or <strong class="bold">accounting</strong>).</p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>Authentication</h2>
<p>The first <em class="italic">A</em> in the <em class="italic">AAA</em> model is <strong class="bold">authentication</strong>, which<a id="_idIndexMarker041"/> involves verifying the cloud identity that is<a id="_idIndexMarker042"/> trying to access the cloud. Instead of the traditional way of just asking for a username and password, <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>) is used, an authentication method that requires users to verify their<a id="_idIndexMarker043"/> identity using multiple independent methods. For security reasons, all user authentications, including GCP console access and <a id="_idIndexMarker044"/>any other <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>) implementations, must be done while enforcing MFA. Usernames and passwords are simply ineffective in protecting user access these days. </p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor029"/>Authorization</h2>
<p><strong class="bold">Authorization</strong> is represented<a id="_idIndexMarker045"/> by the second <em class="italic">A</em> in the <em class="italic">AAA</em> model. It is the process of granting or denying a user<a id="_idIndexMarker046"/> access to cloud resources once the user has been authenticated into the cloud account. The amount of information and the number of services the user can access depend on the user’s authorization level. Once a user’s identity has been verified and the user has been authenticated into GCP, the user must pass the authorization rules to access the cloud resources and data. Authorization determines the resources that the user can and cannot access. </p>
<p>Authorization defines <em class="italic">who can do what on which resource</em>. The following diagram shows the <strong class="bold">authorization</strong> concept in<a id="_idIndexMarker047"/> GCP. As you can see, there are three parties in the authorization process: the first layer in the figure is identity – this specifies <em class="italic">who</em> can be a user account, a group of users, or an application (<strong class="bold">Service Account</strong>). The third layer specifies <em class="italic">which</em> cloud resources, such as GCS buckets, GCE VMs, VPCs, service accounts, or other GCP resources. A <strong class="bold">Service Account</strong> can be an identity as well as a resource:</p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.2 – GCP IAM authentication " height="280" src="image/Figure_1.2.jpg" width="474"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – GCP IAM authentication</p>
<p>The middle layer is <strong class="bold">IAM Role</strong>, also known as the <em class="italic">what</em>, which refers to specific privileges or actions that the<a id="_idIndexMarker048"/> identity has against the resources. For example, when a group is provided the privilege of a compute viewer, then the group will have read-only access to get and list GCE resources, without being able to write/change them. GCP supports three types of IAM roles: <strong class="bold">primitive</strong> (<strong class="bold">basic</strong>), <strong class="bold">predefined</strong>, and <strong class="bold">custom</strong>. Let’s take a look:  </p>
<ul>
<li><strong class="bold">Primitive (basic) roles</strong>, include the Owner, Editor, and Viewer roles, which existed in GCP before the introduction<a id="_idIndexMarker049"/> of IAM.  These roles have thousands of permissions across all Google Cloud services and confer significant privileges. Therefore, in production environments, it is recommended to not grant basic roles unless there is no alternative. Instead, grant the most limited predefined roles or custom roles that meet your needs.  </li>
<li><strong class="bold">Predefined roles</strong> provide granular access to specific services following role-based permission needs. Predefined <a id="_idIndexMarker050"/>roles are created and maintained by Google. Google automatically updates its permissions as necessary, such as when Google Cloud adds new features or services.</li>
<li><strong class="bold">Custom roles</strong> provide <a id="_idIndexMarker051"/>granular access according to the user-specified list of permissions. These roles should be used sparingly as the user is responsible for maintaining the associated permissions.</li>
</ul>
<p>In GCP, authentication is<a id="_idIndexMarker052"/> implemented using IAM policies, which bind identities to IAM roles. Here is a sample IAM policy:</p>
<pre class="source-code">{
  "bindings": [
    {
      "members": [
        "user:jack@example.com"
      ],
      "role": "roles/resourcemanager.organizationAdmin"
    },
    {
      "members": [
        "user:jack@example.com",
        "user:joe@example.com"
      ],
      "role": "roles/resourcemanager.projectCreator"
    }
  ],
  "etag": "BwUjMhCsNvY=",
  "version": 1
}</pre>
<p>In the preceding example, Jack (<strong class="source-inline">jack@example.com</strong>) is granted the Organization Admin predefined role (<strong class="source-inline">roles/resourcemanager.organizationAdmin</strong>) and thus has permissions for organizations, folders, and limited project operations. Both Jack and Joe (<strong class="source-inline">joe@example.com</strong>) can create projects since they have been granted the Project Creator role (<strong class="source-inline">roles/resourcemanager.projectCreator</strong>). Together, these two role bindings<a id="_idIndexMarker053"/> provide fine-grained GCP resource access to Jack and Joe, though Jack has more privileges.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>Auditing or accounting</h2>
<p>The third <em class="italic">A</em> in the <em class="italic">AAA</em> model<a id="_idIndexMarker054"/> refers to <strong class="bold">auditing</strong> or <strong class="bold">accounting</strong>, which is the process of keeping track of a user’s <a id="_idIndexMarker055"/>activity while accessing GCP resources, including the amount of time spent in the <a id="_idIndexMarker056"/>network, the services they’ve accessed, and the amount of data transferred during their login session. Auditing data is used for trend analysis, access recording, compliance auditing, breach detection, forensics and investigations, accounts billing, cost allocations, and capacity planning. With the Google Cloud Audit Logs service, you can keep track of users/groups and their activities and ensure the activity records are genuine. Auditing logs are very helpful for cloud security. For example, tracing back to events of a cybersecurity incident can be very valuable to forensics analyses and case investigations.</p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor031"/>Service account</h2>
<p>In GCP, a service account is a <a id="_idIndexMarker057"/>specialized account that can be used by GCP services and other applications running on GCE instances or elsewhere<a id="_idIndexMarker058"/> to interact with GCP <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>). They are like <em class="italic">programmatic access users</em> by which you can give access to GCP services. Service accounts exist in GCP projects but can be given permissions at the organization and folder levels, as well as to different projects. By leveraging service account credentials, applications can authorize themselves to a set of APIs and perform actions within the permissions that have been granted to the service account. For example, an application running on a GCE instance can use the instance’s service account to interact with other Google services (such as a Cloud SQL Database instance) and their underlying APIs.</p>
<p>When we created our first VM, a default service account was created for that VM at the same time. You can define the permissions<a id="_idIndexMarker059"/> for this VM’s service account by defining its <strong class="bold">access scopes</strong>. Once defined, all the applications running on this VM will have the same permission to access <a id="_idIndexMarker060"/>other GCP resources, such as a GCS bucket. When the number of VMs has increased significantly, this will generate a lot of service accounts. That’s why we often create a service account and assign it to a VM or other resources that need to have the same GCP permissions.</p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>GCP compute services</h1>
<p>Previously, we looked at the GCE <a id="_idIndexMarker061"/>service and created our VM instances in the cloud. Now, let’s look at the whole GCP compute spectrum, which includes <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>), <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), Cloud Run, <strong class="bold">Google App Engine</strong> (<strong class="bold">GAE</strong>), and Cloud Functions, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.3 – GCP compute services " height="301" src="image/Figure_1.3.jpg" width="1119"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – GCP compute services</p>
<p>The GCP compute spectrum provides a broad range of business use cases. Based on the business model, we can choose GCE, GKE, GAE, Cloud Run, or Cloud Functions to match the requirements. We will discuss each of them briefly in the next few sections.</p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>GCE virtual machines</h2>
<p>We discussed the concepts<a id="_idIndexMarker062"/> surrounding GCE and provisioned VMs using the cloud console and Cloud Shell. In this section, we will discuss GCP GCE VM images and the pricing model.</p>
<p>Compute Engine images provide the base operating environment for applications that run in <strong class="bold">Compute Engines</strong> (that is, VMs), and they are critical to ensuring that your application deploys and scales quickly and reliably. You can also use golden/trusted images to archive application versions for disaster recovery or rollback scenarios. GCE images are also crucial in security since they can be used to deploy all the VMs in a company.</p>
<p>GCE offers different pricing models for VMs: pay-as-you-go, preemptive, committed usage, and sole-tenant host. </p>
<p>Pay-as-you-go is for business cases that need to provision VMs on the fly. If the workload is foreseeable, we want to use committed usage for the discounted price. If the workload can be restarted, we want to further leverage the <em class="italic">preemptive</em> model and bid for the VM prices. If licenses tied to the host exist, the <em class="italic">sole-tenant host</em> type fits our needs. For more details about GCE VM <a id="_idIndexMarker063"/>pricing, check out <a href="https://cloud.google.com/compute/vm-instance-pricing">https://cloud.google.com/compute/vm-instance-pricing</a>.</p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Load balancers and managed instance groups</h2>
<p>A single computer may be down due to hardware or software failures, and it also does not provide any scaling when computing <a id="_idIndexMarker064"/>power demands are changed along the timeline. To ensure high availability and scalability, GCP provides <strong class="bold">load balancers</strong> (<strong class="bold">LBs</strong>) and <strong class="bold">managed instance groups</strong> (<strong class="bold">MIGs</strong>). LBs and MIGs allow you to create <a id="_idIndexMarker065"/>homogeneous groups of instances so that load balancers can direct traffic to more than one VM instance. MIG also offers features such as auto-scaling and auto-healing. Auto-scaling lets you deal with spikes in traffic by configuring the appropriate minimum and maximum instances in the autoscaling policy and scaling the number of VM instances up or down based on specific signals, while auto-healing performs health checking and, if necessary, automatically recreates unhealthy instances. </p>
<p>Let’s look at an example to explain this idea:</p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.4 – GCP load balancers and managed instance groups " height="652" src="image/Figure_1.4.jpg" width="1204"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – GCP load balancers and managed instance groups</p>
<p>As shown in the preceding diagram, <a href="http://www.zeebestbuy.com">www.zeebestbuy.com</a> is a global e-commerce company. Every year, when <em class="italic">Black Friday</em> comes, their website is so heavily loaded that a single computer cannot accommodate the traffic – many more web servers (running on VM instances) are needed to distribute the traffic load. After <em class="italic">Black Friday</em>, the traffic goes back to normal, and not that many instances are needed. On the GCP platform, we use LBs and MIGs to solve this problem. As shown in the preceding diagram, we build three web servers globally (N. Virginia in the US, Singapore, and London in the UK), and GCP DNS can distribute the user traffic to these three locations based on the user’s browser location and the latency to the three sites. At each site, we set up an LB and a MIG:  the desired capacity, as well as the minimum and maximum capacities, can be set appropriately based on the normal and peak traffic. When <em class="italic">Black Friday</em> comes, the LB and MIG work together to elastically launch new VM instances (web servers) to handle the increased traffic. After the <em class="italic">Black Friday</em> sale ends, they will stop/delete the VM instances to reflect the decreased traffic. </p>
<p>MIG uses a launch<a id="_idIndexMarker066"/> template, which is like a launch configuration, and specifies instance configuration information, including the ID of the VM image, the instance type, the scaling thresholds, and other parameters that are used to launch VM instances. LB uses health checks to monitor the instances. If an instance is not responding within the configured threshed times, new instances will be launched based on the launch template.</p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>Containers and Google Kubernetes Engine</h2>
<p>Just like the transformation from physical machines into VMs, the transformation from VMs into containers is<a id="_idIndexMarker067"/> revolutionary. Instead of launching a VM to run an application, we package the application into a standard unit that contains everything to run the application or service in the same way on different VMs. We build the package into a Docker image; a container is a running instance of a Docker image. While a hypervisor virtualizes the hardware into VMs, a Docker image virtualizes an operating system into application containers.</p>
<p>Due to loose coupling and modular portability, more and more applications are being containerized. Quickly, a <a id="_idIndexMarker068"/>question arose: how can all these containers/Docker images be managed? There is where <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) comes in, a container management system developed by Google. A GKE cluster usually consists of at least one control plane and multiple worker machines called nodes, which work<a id="_idIndexMarker069"/> together to manage/orchestrate the containers. A Kubernetes Pod is a group of containers that are deployed together and work together to complete a task. For example, an app server pod contains three separate containers: the app server itself, a monitoring container, and a logging container. Working together, they form the application or service of a business use case. </p>
<p>Following the instructions at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster">https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster</a>, you can create a GKE zonal cluster. Let’s pause here and use GCP Cloud Shell to create a GKE cluster.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>GCP Cloud Run </h2>
<p>GCP Cloud Run is a managed compute <a id="_idIndexMarker070"/>platform that enables you to run stateless containers that can be invoked via HTTP requests on either a fully managed environment or in your GKE cluster. Cloud Run is serverless, which means that all infrastructure management tasks are the responsibility of Google, leaving the user to focus on application development. With Cloud Run, you can build your applications in any language using whatever frameworks and tools you want, and then deploy them in seconds without having to manage the server infrastructure.</p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>GCP Cloud Functions</h2>
<p>Different from the GCE and GKE <a id="_idIndexMarker071"/>services, which deploy VMs or containers to run applications, respectively, Cloud Functions is a serverless compute service that allows you to submit your code (written in JavaScript, Python, Go, and so on). Google Cloud will run the code in the backend and deliver the results to you. You do not know and do not care about where the code was run – you are only charged for the time your code runs on GCP. </p>
<p>Leveraging Cloud Functions, a piece of code can be triggered within a few milliseconds based on certain events. For example, after an object is uploaded to a GCS bucket, a message can be <a id="_idIndexMarker072"/>generated and sent to GCP Pub/Sub, which will cause Cloud Functions to process the object. Cloud Functions can also be triggered based on HTTP endpoints you define or by events in Firebase mobile applications.</p>
<p>With Cloud Functions, Google takes care of the backend infrastructure for running the code and lets you focus on the code development only.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>GCP storage and database service spectrum</h1>
<p>Previously, we examined the GCS service and created our storage bucket in the cloud, as well as the persistent disks and Filestore instances for our cloud VM instances. Now, let’s look at the whole GCP<a id="_idIndexMarker073"/> storage and<a id="_idIndexMarker074"/> database service spectrum, which includes Cloud Storage, Cloud SQL, Cloud Spanner, Cloud Firestore, Bigtable, and BigQuery, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.5 – GCP storage and database services " height="306" src="image/Figure_1.5.jpg" width="1424"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – GCP storage and database services</p>
<p>Here, Cloud Storage stores objects, Cloud SQL and Cloud Spanner are the relational databases, Cloud Firestore and Bigtable are NoSQL databases.BigQuery is a data warehouse as well as a bigdata analytical/visualization tool. We will discuss BigQuery in the <em class="italic">GCP big data and analytics services</em> section.</p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>GCP storage</h2>
<p>We have already discussed GCP storage, including <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>), persistent disks, and<a id="_idIndexMarker075"/> Filestore. GCS is a common<a id="_idIndexMarker076"/> choice for GCP ML jobs to store their training data, models, checkpoints, and logs. In the next few sections, we will discuss more GCP storage databases and services.</p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Google Cloud SQL</h2>
<p>Cloud SQL is a fully managed <a id="_idIndexMarker077"/>GCP relational database service for MySQL, PostgreSQL, and SQL Server. With Cloud SQL, you run the same relational databases you are familiar with on-premises, without the hassle of self-management, such as backup and restore, high availability, and more. As a managed service, it is the responsibility of Google to manage the database backups, export and import, ensure high availability and failover, perform patch maintenance and updates, and perform monitoring and logging.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Google Cloud Spanner</h2>
<p>Google Cloud Spanner is a GCP fully managed relational database with unlimited global scale, strong consistency, and <a id="_idIndexMarker078"/>up to 99.999% availability. Like a relational database, Cloud Spanner has schemas, SQL, and strong consistency. Also, like a non-relational database, Cloud Spanner offers high availability, horizontal scalability, and configurable replications. Cloud Spanner has been used for mission-critical business use cases, such as online trading systems for transactions and financial management. </p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Cloud Firestore </h2>
<p>Cloud Firestore is a fast, fully managed, serverless, cloud-native NoSQL document database. Cloud Firestore supports ACID transactions and allows you to run sophisticated queries against NoSQL data without performance degradation. It stores, syncs and query data for mobile apps and web apps at global scale. Firestore integrates with Firebase and other GCP services seamlessly  and thus accelerates serverless application development.</p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Google Cloud Bigtable</h2>
<p>Cloud Bigtable is Google’s fully managed <a id="_idIndexMarker079"/>NoSQL big data database service. Bigtable stores data in tables that are sorted using key/value maps. Bigtable can store trillions of rows and millions of columns, enabling applications to store petabytes of data. Bigtable provides extreme scalability and automatically handles database tasks such as restarts, upgrades, and replication. Bigtable is ideal for storing very large amounts of semi-structured or non-structured data, with sub-10 milliseconds latency and extremely high read and write throughput. Many of Google’s core products such as Search, Analytics, Maps, and Gmail use Cloud Bigtable. </p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>GCP big data and analytics services</h1>
<p>Distinguished from storage <a id="_idIndexMarker080"/>and database services, the big data and analytics services focus on the big data processing pipeline: from data ingestion, storing, and processing to visualization, it helps you create a complete cloud-based big data infrastructure:</p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.6 – GCP big data and analytics services " height="295" src="image/Figure_1.6.jpg" width="1091"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – GCP big data and analytics services</p>
<p>As shown in the preceding diagram, the GCP big data and analytics services include Cloud Dataproc, Cloud Dataflow, BigQuery, and Cloud Pub/Sub. </p>
<p>Let’s examine each of them briefly.</p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Google Cloud Dataproc</h2>
<p>Based on the concept of Map-Reduce <a id="_idIndexMarker081"/>and the architecture of Hadoop systems, <strong class="bold">Google</strong> <strong class="bold">Cloud Dataproc</strong> is a managed GCP service for <a id="_idIndexMarker082"/>processing large datasets. <strong class="bold">Dataproc</strong> provides organizations with the flexibility to provision and configure data processing clusters of varying sizes on demand. Dataproc integrates well with other GCP services. It can operate directly on Cloud Storage files or use Bigtable to analyze data, and it can be integrated with <strong class="bold">Vertex AI</strong>, <strong class="bold">BigQuery</strong>, <strong class="bold">Dataplex</strong>, and other <strong class="bold">GCP</strong> services.</p>
<p>Dataproc helps users process, transform, and understand vast quantities of data. You can use Dataproc to run Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. You can also use Dataproc for data lake modernization, ETL processes, and more.</p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Google Cloud Dataflow</h2>
<p>Cloud Dataflow is a GCP-managed<a id="_idIndexMarker083"/> service for developing and executing a wide variety of data processing patterns, including <strong class="bold">Extract, Transform, Load</strong> (<strong class="bold">ETL</strong>), batch, and streaming jobs. Cloud Dataflow is a serverless data <a id="_idIndexMarker084"/>processing service that runs jobs written with Apache Beam libraries. Cloud Dataflow executes jobs that consist of a pipeline – a sequence of steps that reads data, transforms it into different formats, and writes it out. A dataflow <a id="_idIndexMarker085"/>pipeline consists of a series of pipes, which is a way to connect components, where data moves from one component to the next via a pipe. When jobs are executed on Cloud Dataflow, the service spins up a cluster of VMs, distributes the job tasks to the VMs, and dynamically scales the cluster based on job loads and their performance.</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Google Cloud BigQuery</h2>
<p>BigQuery is a Google fully managed <a id="_idIndexMarker086"/>enterprise data warehouse <a id="_idIndexMarker087"/>service that is highly scalable, fast, and optimized for data analytics. It has the following features:</p>
<ul>
<li>BigQuery supports ANSI-standard SQL queries, including joins, nested and repeated fields, analytic and aggregation functions, scripting, and a variety of spatial functions via geospatial analytics. </li>
<li>With BigQuery, you do not physically manage the infrastructure assets. BigQuery’s serverless architecture lets you use SQL queries to answer big business questions with zero infrastructure overhead. With BigQuery’s scalable, distributed analysis engine, you can query petabytes of data in minutes.</li>
<li>BigQuery integrates seamlessly with other GCP data services. You can query data stored in BigQuery or run queries on data where it lives using external tables or federated queries, including GCS, Bigtable, Spanner, or Google Sheets stored in Google Drive. </li>
<li>BigQuery helps you manage and analyze your data with built-in features such as ML, geospatial analysis, and business intelligence. We will discuss BigQuery ML later in this book.</li>
</ul>
<p>Google BigQuery is used in many<a id="_idIndexMarker088"/> business cases due to it being SQL-friendly, having a serverless structure, and having built-in integration with other GCP services.</p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Google Cloud Pub/Sub</h2>
<p>GCP Pub/Sub is a widely used cloud<a id="_idIndexMarker089"/> service for decoupling many GCP services – it implements an event/message queue <a id="_idIndexMarker090"/>pipe to integrate services and parallelize tasks. With the Pub/Sub service, you can create event producers, called publishers, and event consumers, called subscribers. Using Pub/Sub, the publishers communicate with subscribers asynchronously by broadcasting events – a publisher can have multiple subscribers and a subscriber can subscribe to multiple publishers:</p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.7 – Google Cloud Pub/Sub services" height="229" src="image/Figure_1.7.jpg" width="789"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Google Cloud Pub/Sub services</p>
<p>The preceding diagram shows the example we discussed in the <em class="italic">GCP Cloud Functions</em> section: after an object is uploaded to a GCS bucket, a request/message can be generated and sent to GCP Pub/Sub, which can trigger an email notification and a cloud function to process the object. When the number of parallel object uploads is huge, Cloud Pub/Sub will help buffer/queue the requests/messages and decouple the GCS service from other cloud services such as Cloud Functions.</p>
<p>So far, we have covered<a id="_idIndexMarker091"/><a id="_idIndexMarker092"/> various GCP services, including compute, storage, databases, and data analytics (big data). Now, let’s take a look at various GCP <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) services.</p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>GCP artificial intelligence services</h1>
<p>The AI services in Google Cloud are<a id="_idIndexMarker093"/> some of its best services. Google Cloud’s AI services include the following:</p>
<ul>
<li><strong class="bold">BigQuery ML</strong> (<strong class="bold">BQML</strong>) </li>
<li>TensorFlow and Keras </li>
<li>Google Vertex AI</li>
<li>Google ML API</li>
</ul>
<p>Google BQML is built from Google Cloud BQ, which serves as a serverless big data warehouse and analytical platform. BQML trains ML models from the datasets already stored in BQ, using SQL-based languages. TensorFlow introduces the concepts of tensors and provides a framework for ML development, whereas Keras provides a high-level structure using TensorFlow. We will discuss BQML, TensorFlow, and Keras in more detail in part three of this book, along with Google Cloud Vertex AI and the Google Cloud ML API, which we will briefly introduce next.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Google Vertex AI</h2>
<p><strong class="bold">Google Vertex AI</strong> (<a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform">https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform</a>) aims<a id="_idIndexMarker094"/> to <a id="_idIndexMarker095"/>provide a<a id="_idIndexMarker096"/> fully managed, scalable, secure, enterprise-level ML development infrastructure. Within the Vertex AI environment, data scientists can complete all of their ML projects from end to end: data preparation and feature engineering; model training, validation, and tuning; model deployment and monitoring, and so on. It provides a unified API, client library, and user interface. </p>
<p>Vertex AI provides end-to-end ML <a id="_idIndexMarker097"/>services, including, but not limited to, the following:</p>
<ul>
<li>Vertex AI data labeling and dataset</li>
<li>Vertex AI Feature Store</li>
<li>Vertex AI Workbench and notebooks</li>
<li>Vertex AI training </li>
<li>Vertex AI models and endpoints</li>
<li>Vertex AI Pipelines</li>
<li>Vertex AI Metadata</li>
<li>Vertex AI Experiments and TensorBoard</li>
</ul>
<p>We will examine each of<a id="_idIndexMarker098"/> these in detail in the third part of this book.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Google Cloud ML APIs</h2>
<p><strong class="bold">Google Cloud ML APIs</strong> provide application<a id="_idIndexMarker099"/> interfaces to<a id="_idIndexMarker100"/> customers with Google’s pre-trained ML models, which are trained with Google’s data. The following are a few AI APIs:</p>
<ul>
<li><strong class="bold">Google Cloud sight APIs</strong>, which include the<a id="_idIndexMarker101"/> Google Cloud Vision API and Cloud Video API. The pre-trained models of the sight APIs use ML to understand your images with industry-leading prediction accuracy. They can be used to detect objects/faces/scenes, read handwriting, and build valuable image/video metadata.</li>
<li><strong class="bold">Google Cloud language APIs</strong>, which includes the Natural Language Processing API and Translation API. These <a id="_idIndexMarker102"/>powerful pre-trained models of the  Language API empower developers to easily <a id="_idIndexMarker103"/>apply <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) to their applications, alongside features such as sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis. The Translation API allows you to detect a language and translate it into the target language.</li>
<li><strong class="bold">Google Cloud conversation APIs</strong>, which include the Speech-to-Text, Text-to-Speech, and Dialogflow APIs. The <a id="_idIndexMarker104"/>pre-trained models of the Conversation APIs accurately convert speech into text, text into speech, and enable developers to develop business applications for call centers, online voice ordering systems, and so on using Google’s cutting-edge AI technologies.</li>
</ul>
<p>AI is the ability of a<a id="_idIndexMarker105"/> computer (or a robot controlled by a computer) to perform tasks that are usually done by humans because they require human intelligence. In the history of human beings, from vision development (related to the Cambrian explosion) to language development, to tool development, a fundamental question is, how did we humans evolve and how can we teach a computer to learn to see, speak, and use tools? The GCP AI service spectrum includes vision services(image recognition, detection, segmentation, and so on), language services(text, speech, translation, and so on), and many more. We will learn more about these services later in this book. We are certain that many more AI services, including hand detection tools, will be added to the spectrum in the future.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Summary</h1>
<p>In this chapter, we started by creating a GCP free-tier account and provisioning our VM and storage bucket in the cloud. Then, we looked at the GCP organization’s structure, resource hierarchy, and IAM. Finally, we looked at the GCP services that are related to ML, including compute, storage, big data and analytics, and AI, to have a solid understanding of each GCP service.</p>
<p>To help you with your hands-on GCP skills, we have provided examples in <a href="B18333_11.xhtml#_idTextAnchor184"><em class="italic">Appendix 1</em></a>, <em class="italic">Practicing with Basic GCP Services</em>, where we have provided labs for provisioning basic GCP resources, step by step.</p>
<p>In the next chapter, we will build another foundation: Python programming. We will focus on Python basic skill development and Python data library usage.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Further reading<a href="https://cloud.google.com/compute/docs/instances/create-start-instance"/></h1>
<p><a href="https://cloud.google.com/compute/docs/instances/create-start-instance">To learn more about the topics t</a>hat were covered in this chapter, take a look at the following resources:</p>
<ul>
<li><a href="https://cloud.google.com/compute/">https://cloud.google.com/compute/</a></li>
<li><a href="https://cloud.google.com/storage/">https://cloud.google.com/storage/</a></li>
<li><a href="https://cloud.google.com/vpc">https://cloud.google.com/vpc</a></li>
<li><a href="https://cloud.google.com/products/databases">https://cloud.google.com/products/databases</a></li>
<li><a href="https://cloud.google.com/products/security-and-identity">https://cloud.google.com/products/security-and-identity</a></li>
<li><a href="https://cloud.google.com/solutions/smart-analytics">https://cloud.google.com/solutions/smart-analytics</a></li>
<li><a href="https://cloud.google.com/products/ai">https://cloud.google.com/products/ai</a></li>
<li><a href="B18333_11.xhtml#_idTextAnchor184"><em class="italic">Appendix 1</em></a>, <em class="italic">Practicing with Basic GCP Services</em></li>
</ul>
</div>
<div>
<div id="_idContainer016">
</div>
</div>
</div>
</body></html>