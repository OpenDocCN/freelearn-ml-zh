["```py\n> # load library\n> library(twitteR)\n> # set credentials\n> consumerSecret = \"XXXXXXXXXXXXX\"\n> consumerKey = \"XXXXXXXXXXXXXXXXXXXXXXXXXx\"\n\n```", "```py\nNo to it:\n```", "```py\n> twitterUser <- getUser(\"jack\")\n> # extract jack's tweets\n> tweets <- userTimeline(twitterUser, n = 300)\n> tweets\n\n```", "```py\n> # get tweet attributes\n> tweets[[1]]$getClass()\n>\n> # get retweets count\n> tweets[[1]]$retweetCount\n>\n> # get favourite count\n> tweets[[1]]$favoriteCount\n\n```", "```py\nlibrary(twitteR)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tm)\nlibrary(wordcloud)\n\nconsumerSecret = \"XXXXXXXXX\"\nconsumerKey = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\nsetup_twitter_oauth(consumer_key = consumerKey,consumer_secret = consumerSecret)\n\n```", "```py\n# trending tweets\ntrendingTweets = searchTwitter(\"#ResolutionsFor2016\",n=1000)\ntrendingTweets.df = twListToDF(trendingTweets)\ntrendingTweets.df$text <- sapply(trendingTweets.df$text,function(x) iconv(x,to='UTF-8'))\n\n```", "```py\ntrendingTweets.df$tweetSource = sapply(trendingTweets.df$statusSource,function(sourceSystem) enodeSource(sourceSystem))\n\n```", "```py\n# transformations\ntweetCorpus <- Corpus(VectorSource(trendingTweets.df$text))\ntweetCorpus <- tm_map(tweetCorpus, tolower)\ntweetCorpus <- tm_map(tweetCorpus, removePunctuation)\ntweetCorpus <- tm_map(tweetCorpus, removeNumbers)\n\n# remove URLs\nremoveURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\ntweetCorpus <- tm_map(tweetCorpus, removeURL) \n\n# remove stop words\ntwtrStopWords <- c(stopwords(\"english\"),'resolution','resolutions','resolutionsfor','resolutionsfor2016','2016','new','year','years','newyearresolution')\ntweetCorpus <- tm_map(tweetCorpus, removeWords, twtrStopWords)\n\ntweetCorpus <- tm_map(tweetCorpus, PlainTextDocument)\n\n```", "```py\n# Term Document Matrix\n> twtrTermDocMatrix <- TermDocumentMatrix(tweetCorpus, control = list(minWordLength = 1))\n\n```", "```py\n# Terms occuring in more than 30 times\n> which(apply(twtrTermDocMatrix,1,sum)>=30)\n\n```", "```py\n# print the frequent terms from termdocmatrix\n> (frequentTerms<-findFreqTerms(twtrTermDocMatrix,lowfreq = 10))\n\n```", "```py\n# calculate frequency of each term\nterm.freq <- rowSums(as.matrix(twtrTermDocMatrix))\n\n# picking only a subset\nsubsetterm.freq <- subset(term.freq, term.freq >= 10)\n\n# create data frame from subset of terms\nfrequentTermsSubsetDF <- data.frame(term = names(subsetterm.freq), freq = subsetterm.freq)\n\n# create data frame with all terms\nfrequentTermsDF <- data.frame(term = names(term.freq), freq = term.freq)\n\n# sort by subset DataFrame frequency\nfrequentTermsSubsetDF <- frequentTermsSubsetDF[with(frequentTermsSubsetDF, order(-frequentTermsSubsetDF$freq)), ]\n\n# sort by complete DataFrame frequency\nfrequentTermsDF <- frequentTermsDF[with(frequentTermsDF, order(-frequentTermsDF$freq)), ]\n\n# words by frequency from subset data frame\nggplot(frequentTermsSubsetDF, aes(x = reorder(term,freq), y = freq)) + geom_bar(stat = \"identity\") +xlab(\"Terms\") + ylab(\"Frequency\") + coord_flip()\n\n```", "```py\n# wordcloud\n> wordcloud(words=frequentTermsDF$term, freq=frequentTermsDF$freq,random.order=FALSE)\n\n```", "```py\n# top retweets\n> head(subset(trendingTweets.df$text, grepl(\"trillionaire\",trendingTweets.df$text) ),n=1)\n\n```", "```py\n# Associatons\n(fitness.associations <- findAssocs(twtrTermDocMatrix,\"fitness\",0.25))\n\nfitnessTerm.freq <- rowSums(as.matrix(fitness.associations$fitness))\n\nfitnessDF <- data.frame(term=names(fitnessTerm.freq),freq=fitnessTerm.freq)\n\nfitnessDF <- fitnessDF[with(fitnessDF, order(-fitnessDF$freq)), ]\nggplot(fitnessDF,aes(x=reorder(term,freq),y=freq))\n+geom_bar(stat = \"identity\") +xlab(\"Terms\")\n+ ylab(\"Associations\")\n+ coord_flip()\n\n```", "```py\n# Source by retweet count\ntrendingTweetsSubset.df <- subset(trendingTweets.df, trendingTweets.df$retweetCount >= 5000 )\n\nggplot(trendingTweetsSubset.df, aes(x =tweetSource, y =retweetCount/100)) + geom_bar(stat = \"identity\") +xlab(\"Source\") + ylab(\"Retweet Count\")\n\n```", "```py\n# remove sparse terms\ntwtrTermDocMatrix2 <- removeSparseTerms(twtrTermDocMatrix, sparse = 0.98)\n\ntweet_matrix <- as.matrix(twtrTermDocMatrix2)\n\n# cluster terms\ndistMatrix <- dist(scale(tweet_matrix))\n\nfit <- hclust(distMatrix,method=\"single\")\nplot(fit)\n\n```", "```py\n# set user handle\natISS <- getUser(\"ISS_Research\")\n\n# extract iss_research tweets\ntweets <- userTimeline(atISS, n = 1000)\n\ntweets.df=twListToDF(tweets)\n\ntweets.df$text <- sapply(tweets.df$text,function(x) iconv(x,to='UTF-8'))\n\n#Document Term Matrix\ntwtrDTM <- DocumentTermMatrix(twtrCorpus, control = list(minWordLength = 1))\n\n```", "```py\ndocument-term matrix, unlike last time where we prepared a *term-document matrix*.\n```", "```py\n#topic modeling\n\n# find 8 topics\nldaTopics <- LDA(twtrDTM, k = 8) \n\n#first 6 terms of every topic\nldaTerms <- terms(ldaTopics, 6) \n\n# concatenate terms\n(ldaTerms <- apply(ldaTerms, MARGIN = 2, paste, collapse = \", \"))\n\n```", "```py\n# first topic identified for every tweet\nfirstTopic <- topics(ldaTopics, 1)\n\ntopics <- data.frame(date=as.Date(tweets.df$created), firstTopic)\n\nqplot(date, ..count.., data=topics, geom=\"density\",fill=ldaTerms[firstTopic], position=\"stack\")+scale_fill_grey()\n\n```"]