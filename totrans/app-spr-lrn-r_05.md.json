["```py\n    library(rattle.data)\n    ```", "```py\n    data(weatherAUS)\n    ```", "```py\n    df <- weatherAUS\n    ```", "```py\n    str(df)\n    ```", "```py\n    df$RISK_MM <- NULL\n    ```", "```py\n    temp_df<-as.data.frame(\n      sort(\n      round(\n      sapply(df, function(y) sum(length(which(is.na(y)))))/dim(df)[1],2)\n      )\n    )\n    colnames(temp_df) <- \"NullPerc\"\n    ```", "```py\n    print(temp_df)\n    ```", "```py\n                  NullPerc\n    Date              0.00\n    Location          0.00\n    MinTemp           0.01\n    MaxTemp           0.01\n    WindSpeed9am      0.01\n    Temp9am           0.01\n    Rainfall          0.02\n    WindSpeed3pm      0.02\n    Humidity9am       0.02\n    Temp3pm           0.02\n    RainToday         0.02\n    RainTomorrow      0.02\n    WindDir3pm        0.03\n    Humidity3pm       0.03\n    WindGustDir       0.07\n    WindGustSpeed     0.07\n    WindDir9am        0.07\n    Pressure9am       0.10\n    Pressure3pm       0.10\n    Cloud9am          0.38\n    Cloud3pm          0.41\n    Evaporation       0.43\n    Sunshine          0.48\n    ```", "```py\n    cols_to_drop <-tail(rownames(temp_df),4)\n    ```", "```py\n    df_new<- na.omit(df[,!names(df) %in% cols_to_drop])\n    ```", "```py\n    print(\"Shape of data after dropping columns:\")\n    print(dim(df_new))\n    ```", "```py\n    Shape of data after dropping columns:\n    112925     19\n    ```", "```py\n    temp_df<-as.data.frame(sort(round(sapply(df_new, function(y) sum(length(which(is.na(y)))))/dim(df)[1],2)))\n    colnames(temp_df) <- \"NullPerc\"\n    ```", "```py\n    print(temp_df)\n    ```", "```py\n                  NullPerc\n    Date                 0\n    Location             0\n    MinTemp              0\n    MaxTemp              0\n    Rainfall             0\n    WindGustDir          0\n    WindGustSpeed        0\n    WindDir9am           0\n    WindDir3pm           0\n    WindSpeed9am         0\n    WindSpeed3pm         0\n    Humidity9am          0\n    Humidity3pm          0\n    Pressure9am          0\n    Pressure3pm          0\n    Temp9am              0\n    Temp3pm              0\n    RainToday            0\n    RainTomorrow         0\n    ```", "```py\n    library(lubridate)\n    ```", "```py\n    df_new$day <- day(df_new$Date)\n    df_new$month <- month(df_new$Date)\n    df_new$dayofweek <- wday(df_new$Date)\n    df_new$quarter <- quarter(df_new$Date)\n    ```", "```py\n    str(df_new[,c(\"day\",\"month\",\"dayofweek\",\"quarter\")])\n    ```", "```py\n    df_new$Date <- NULL\n    ```", "```py\n    'data.frame':\t112925 obs. of  4 variables:\n     $ day      : int  1 2 3 4 5 6 7 8 9 10 ...\n     $ month    : num  12 12 12 12 12 12 12 12 12 12 ...\n     $ dayofweek: num  2 3 4 5 6 7 1 2 3 4 ...\n     $ quarter  : int  4 4 4 4 4 4 4 4 4 4 ...\n    ```", "```py\n    location_dist <- df_new %>%    group_by(Location) %>%     summarise(Rain  = sum(ifelse(RainTomorrow ==\"Yes\",1,0)), cnt=n()) %>%    mutate(pct = Rain/cnt) %>%    arrange(desc(pct))\n    ```", "```py\n    print(paste(\"#Distinct locations:\",dim(location_dist)[1]))\n    ```", "```py\n    \"#Distinct locations: 44\"\n    ```", "```py\n    print(summary(location_dist))\n    ```", "```py\n        Location        Rain             cnt            pct         \n     Adelaide     : 1   Min.   : 102.0   Min.   : 670   Min.   :0.06687  \n     Albury       : 1   1st Qu.: 427.8   1st Qu.:2330   1st Qu.:0.18380  \n     AliceSprings : 1   Median : 563.5   Median :2742   Median :0.21833  \n     BadgerysCreek: 1   Mean   : 568.6   Mean   :2566   Mean   :0.21896  \n     Ballarat     : 1   3rd Qu.: 740.5   3rd Qu.:2884   3rd Qu.:0.26107  \n     Bendigo      : 1   Max.   :1031.0   Max.   :3117   Max.   :0.36560  \n     (Other)      :38  \n    ```", "```py\n    location_dist$Location <- as.character(location_dist$Location)\n    ```", "```py\n    location_list <- c(head(location_dist$Location,5),tail(location_dist$Location,5))\n    ```", "```py\n    print(\"Final list of locations - \")\n    print(location_list)\n    ```", "```py\n    [1] \"Final list of locations - \"\n     [1] \"Portland\"      \"Walpole\"       \"Dartmoor\"      \"Cairns\"       \n     [5] \"NorfolkIsland\" \"Moree\"         \"Mildura\"       \"AliceSprings\" \n     [9] \"Uluru\"         \"Woomera\" \n    ```", "```py\n    df_new$Location <- as.character(df_new$Location)\n    ```", "```py\n    df_new$new_location <- factor(ifelse(df_new$Location %in% location_list,df_new$Location,\"Others\"))\n    ```", "```py\n    df_new$Location <- NULL\n    ```", "```py\n    temp <- df_new %>% mutate(loc = as.character(new_location)) %>%    group_by(as.character(loc)) %>%    summarise(Rain  = sum(ifelse(RainTomorrow ==\"Yes\",1,0)), cnt=n()) %>%    mutate(pct = Rain/cnt) %>%    arrange(desc(pct))\n    ```", "```py\n    print(temp)\n    ```", "```py\n    # A tibble: 11 x 4\n       `as.character(loc)`  Rain   cnt    pct\n       <chr>               <dbl> <int>  <dbl>\n      Portland             1031  2820 0.366 \n      Walpole               864  2502 0.345 \n      Dartmoor              770  2294 0.336 \n      Cairns                910  2899 0.314 \n      NorfolkIsland         883  2864 0.308 \n      Others              19380 86944 0.223 \n      Moree                 336  2629 0.128 \n      Mildura               315  2897 0.109 \n      AliceSprings          227  2744 0.0827\n      Uluru                 110  1446 0.0761\n      Woomera               193  2886 0.0669\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_index <- sample(seq_len(nrow(df_new)),floor(0.7 * nrow(df_new)))\n    ```", "```py\n    train <- df_new[train_index,]\n    test <- df_new[-train_index,]\n    ```", "```py\n    model <- glm(RainTomorrow ~ MinTemp + Rainfall + WindGustSpeed +         WindSpeed3pm +Humidity3pm + Pressure3pm +        RainToday +  Temp3pm + Temp9am,         data=train,        family=binomial(link='logit'))\n    ```", "```py\n    summary(model)\n    ```", "```py\n    Call:\n    glm(formula = RainTomorrow ~ MinTemp + Rainfall + WindGustSpeed + \n        WindSpeed3pm + Humidity3pm + Pressure3pm + RainToday + Temp3pm + \n        Temp9am, family = binomial(link = \"logit\"), data = train)\n    Deviance Residuals: \n        Min       1Q   Median       3Q      Max  \n    -2.9323  -0.5528  -0.3235  -0.1412   3.2047  \n    Coefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n    (Intercept)    6.543e+01  1.876e+00  34.878  < 2e-16 ***\n    MinTemp        9.369e-05  5.056e-03   0.019    0.985    \n    Rainfall       7.496e-03  1.404e-03   5.337 9.44e-08 ***\n    WindGustSpeed  5.817e-02  1.153e-03  50.434  < 2e-16 ***\n    WindSpeed3pm  -4.331e-02  1.651e-03 -26.234  < 2e-16 ***\n    Humidity3pm    7.363e-02  9.868e-04  74.614  < 2e-16 ***\n    Pressure3pm   -7.162e-02  1.821e-03 -39.321  < 2e-16 ***\n    RainTodayYes   4.243e-01  2.751e-02  15.425  < 2e-16 ***\n    Temp3pm        3.930e-02  5.171e-03   7.599 2.98e-14 ***\n    Temp9am       -4.605e-02  6.270e-03  -7.344 2.07e-13 ***\n    ---\n    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n    (Dispersion parameter for binomial family taken to be 1)\n        Null deviance: 83718 on 79046 degrees of freedom\n    Residual deviance: 56557 on 79037 degrees of freedom\n    AIC: 56577\n    Number of Fisher Scoring iterations: 5\n    ```", "```py\n    print(\"Distribution of labels in the data-\")\n    print(table(df_new$RainTomorrow)/dim(df_new)[1])\n    ```", "```py\n    \"Distribution of labels in the data-\"\n           No       Yes \n    0.7784459 0.2215541 \n    ```", "```py\n    print(\"Training data results -\")\n    pred_train <-factor(ifelse(predict(model,\n                               newdata=train, type=\"response\")> 0.5,\"Yes\",\"No\"))\n    ```", "```py\n    train_metrics <- confusionMatrix(pred_train,  \n                                         train$RainTomorrow,positive=\"Yes\")\n    print(train_metrics)\n    ```", "```py\n    [1] \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  58233  8850\n           Yes  3258  8706\n\n                   Accuracy : 0.8468          \n                     95% CI : (0.8443, 0.8493)\n        No Information Rate : 0.7779          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4998          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4959          \n                Specificity : 0.9470          \n             Pos Pred Value : 0.7277          \n             Neg Pred Value : 0.8681          \n                 Prevalence : 0.2221          \n             Detection Rate : 0.1101          \n       Detection Prevalence : 0.1514          \n          Balanced Accuracy : 0.7215          \n\n           'Positive' Class : Yes             \n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <-factor(ifelse(predict(model,newdata=test,type = \"response\") > 0.5,\"Yes\",\"No\"))\n    ```", "```py\n    test_metrics <- confusionMatrix(pred_test, \n                                    test$RainTomorrow,positive=\"Yes\")\n    print(test_metrics)\n    ```", "```py\n    [1] \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25066  3754\n           Yes  1349  3709\n\n                   Accuracy : 0.8494          \n                     95% CI : (0.8455, 0.8532)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5042          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4970          \n                Specificity : 0.9489          \n             Pos Pred Value : 0.7333          \n             Neg Pred Value : 0.8697          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1095          \n       Detection Prevalence : 0.1493          \n          Balanced Accuracy : 0.7230          \n\n           'Positive' Class : Yes    \n    ```", "```py\n    model <- glm(RainTomorrow~., data=train ,family=binomial(link='logit'))\n    ```", "```py\n    print(\"Training data results-\")\n    pred_train <-factor(ifelse(predict(model,newdata=train,type = \"response\") >= 0.5,\"Yes\",\"No\"))\n    ```", "```py\n    train_metrics <- confusionMatrix(pred_train, train$RainTomorrow,positive=\"Yes\")\n    print(train_metrics)\n    ```", "```py\n    \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  58189  8623\n           Yes  3302  8933\n\n                   Accuracy : 0.8491          \n                     95% CI : (0.8466, 0.8516)\n        No Information Rate : 0.7779          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5104          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.5088          \n                Specificity : 0.9463          \n             Pos Pred Value : 0.7301          \n             Neg Pred Value : 0.8709          \n                 Prevalence : 0.2221          \n             Detection Rate : 0.1130          \n       Detection Prevalence : 0.1548          \n          Balanced Accuracy : 0.7276          \n\n           'Positive' Class : Yes             \n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <-factor(ifelse(predict(model,newdata=test,type = \"response\") > 0.5,\"Yes\",\"No\"))\n    ```", "```py\n    test_metrics <- confusionMatrix(pred_test, test$RainTomorrow,positive=\"Yes\")\n    print(test_metrics)\n    ```", "```py\n    \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25057  3640\n           Yes  1358  3823\n\n                   Accuracy : 0.8525          \n                     95% CI : (0.8486, 0.8562)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5176          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.5123          \n                Specificity : 0.9486          \n             Pos Pred Value : 0.7379          \n             Neg Pred Value : 0.8732          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1128          \n       Detection Prevalence : 0.1529          \n          Balanced Accuracy : 0.7304          \n\n           'Positive' Class : Yes\n    ```", "```py\n    \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25057  3640\n           Yes  1358  3823\n\n                   Accuracy : 0.8525          \n                     95% CI : (0.8486, 0.8562)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5176          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.5123          \n                Specificity : 0.9486          \n             Pos Pred Value : 0.7379          \n             Neg Pred Value : 0.8732          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1128          \n       Detection Prevalence : 0.1529          \n          Balanced Accuracy : 0.7304          \n\n           'Positive' Class : Yes             \n    ```", "```py\n    library(rpart)\n    library(rpart.plot)\n    ```", "```py\n    tree_model <- rpart(RainTomorrow~.,data=train)\n    ```", "```py\n    plotcp(tree_model)\n    ```", "```py\n    rpart.plot(tree_model,uniform=TRUE, main=\"Predicting RainFall\")\n    ```", "```py\n    print(\"Training data results -\")\n    pred_train <- predict(tree_model,newdata = train,type = \"class\")\n    confusionMatrix(pred_train, train$RainTomorrow,positive=\"Yes\")\n    ```", "```py\n    \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  59667 11215\n           Yes  1824  6341\n\n                   Accuracy : 0.835           \n                     95% CI : (0.8324, 0.8376)\n        No Information Rate : 0.7779          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4098          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.36119         \n                Specificity : 0.97034         \n             Pos Pred Value : 0.77661         \n             Neg Pred Value : 0.84178         \n                 Prevalence : 0.22210         \n             Detection Rate : 0.08022         \n       Detection Prevalence : 0.10329         \n          Balanced Accuracy : 0.66576         \n\n           'Positive' Class : Yes             \n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <- predict(tree_model,newdata = test,type = \"class\")\n    confusionMatrix(pred_test, test$RainTomorrow,positive=\"Yes\")\n    ```", "```py\n    [1] \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25634  4787\n           Yes   781  2676\n\n                   Accuracy : 0.8356          \n                     95% CI : (0.8317, 0.8396)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4075          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.35857         \n                Specificity : 0.97043         \n             Pos Pred Value : 0.77408         \n             Neg Pred Value : 0.84264         \n                 Prevalence : 0.22029         \n             Detection Rate : 0.07899         \n       Detection Prevalence : 0.10204         \n          Balanced Accuracy : 0.66450         \n\n           'Positive' Class : Yes\n    ```", "```py\ncontrol = rpart.control(\n    minsplit = 20, \n    minbucket = round(minsplit/3), \n    cp = 0.01, \n    maxcompete = 4, \n    maxsurrogate = 5, \n    usesurrogate = 2, xval = 10, \n    surrogatestyle = 0, \n    maxdepth = 30\n)\n```", "```py\n    \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25068  3926\n           Yes  1347  3537\n\n                   Accuracy : 0.8444          \n                     95% CI : (0.8404, 0.8482)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4828          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4739          \n                Specificity : 0.9490          \n             Pos Pred Value : 0.7242          \n             Neg Pred Value : 0.8646          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1044          \n       Detection Prevalence : 0.1442          \n          Balanced Accuracy : 0.7115          \n\n           'Positive' Class : Yes \n    ```", "```py\n    library(randomForest)\n    ```", "```py\n    rf_model <- randomForest(RainTomorrow ~ . , data = train, ntree = 100,                                             importance = TRUE, \n                                                maxnodes=60)\n    ```", "```py\n    print(\"Training data results -\")\n    pred_train <- predict(rf_model,newdata = train,type = \"class\")\n    confusionMatrix(pred_train, train$RainTomorrow,positive=\"Yes\")\n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <- predict(rf_model,newdata = test,type = \"class\")\n    confusionMatrix(pred_test, test$RainTomorrow,positive=\"Yes\")\n    ```", "```py\n    varImpPlot(rf_model)\n    ```", "```py\n    [1] \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  59630 10133\n           Yes  1861  7423\n\n                   Accuracy : 0.8483          \n                     95% CI : (0.8457, 0.8508)\n        No Information Rate : 0.7779          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.472           \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.42282         \n                Specificity : 0.96974         \n             Pos Pred Value : 0.79955         \n             Neg Pred Value : 0.85475         \n                 Prevalence : 0.22210         \n             Detection Rate : 0.09391         \n       Detection Prevalence : 0.11745         \n          Balanced Accuracy : 0.69628         \n\n           'Positive' Class : Yes             \n\n    [1] \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction    No   Yes\n           No  25602  4369\n           Yes   813  3094\n\n                   Accuracy : 0.847           \n                     95% CI : (0.8432, 0.8509)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4629          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.41458         \n                Specificity : 0.96922         \n             Pos Pred Value : 0.79191         \n             Neg Pred Value : 0.85423         \n                 Prevalence : 0.22029         \n             Detection Rate : 0.09133         \n       Detection Prevalence : 0.11533         \n          Balanced Accuracy : 0.69190         \n\n           'Positive' Class : Yes             \n\n    ```", "```py\n    target<- \"RainTomorrow\"\n    categorical_columns <- c(\"RainToday\",\"WindGustDir\",\"WindDir9am\",\n    \"WindDir3pm\", \"new_location\")\n    numeric_columns <- setdiff(colnames(train),c(categorical_columns,target))\n    ```", "```py\n    df_new <- df_new %>% mutate_if(is.factor, as.character)\n    ```", "```py\n    dummies <- dummyVars(~ RainToday + WindGustDir + WindDir9am + \n                         WindDir3pm + new_location, data = df_new)\n    df_all_ohe <- as.data.frame(predict(dummies, newdata = df_new))\n    ```", "```py\n    df_final <- cbind(df_new[,numeric_columns],df_all_ohe)\n    ```", "```py\n    y <- ifelse(df_new[,target] == \"Yes\",1,0)\n    ```", "```py\n    set.seed(2019)\n    train_index <- sample(seq_len(nrow(df_final)),floor(0.7 * nrow(df_final)))\n    xgb.train <- df_final[train_index,]\n    y_train<- y[train_index]\n    xgb.test <- df_final[-train_index,]\n    y_test <- y[-train_index]\n    ```", "```py\n    xgb <- xgboost(data = data.matrix(xgb.train), \n                   label = y_train, \n                   eta = 0.01,\n                   max_depth = 6, \n                   nround=200, \n                   subsample = 1,\n                   colsample_bytree = 1,\n                   seed = 1,\n                   eval_metric = \"logloss\",\n                   objective = \"binary:logistic\",\n                   nthread = 4\n    )\n    ```", "```py\n    print(\"Training data results -\")\n    pred_train <- factor(ifelse(predict(xgb,data.matrix(xgb.train),type=\"class\")>0.5,1,0))\n    confusionMatrix(pred_train,factor(y_train),positive='1')\n    ```", "```py\n    \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 58967  8886\n             1  2524  8670\n\n                   Accuracy : 0.8557          \n                     95% CI : (0.8532, 0.8581)\n        No Information Rate : 0.7779          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5201          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4938          \n                Specificity : 0.9590          \n             Pos Pred Value : 0.7745          \n             Neg Pred Value : 0.8690          \n                 Prevalence : 0.2221          \n             Detection Rate : 0.1097          \n       Detection Prevalence : 0.1416          \n          Balanced Accuracy : 0.7264          \n\n           'Positive' Class : 1               \n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <- factor(ifelse(predict(xgb,data.matrix(xgb.test),\n    type=\"class\")>0.5,1,0))\n    confusionMatrix(pred_test,factor(y_test),positive='1')\n    ```", "```py\n    [1] \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 25261  3884\n             1  1154  3579\n\n                   Accuracy : 0.8513          \n                     95% CI : (0.8475, 0.8551)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.5017          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4796          \n                Specificity : 0.9563          \n             Pos Pred Value : 0.7562          \n             Neg Pred Value : 0.8667          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1056          \n       Detection Prevalence : 0.1397          \n          Balanced Accuracy : 0.7179          \n\n           'Positive' Class : 1\n    ```", "```py\n    print(\"Training data results -\")\n    pred_train <- factor(ifelse(predict(xgb,data.matrix(xgb.train),\n    type=\"class\")>0.53,1,0))\n    confusionMatrix(pred_train,factor(y_train),positive='1')\n    ```", "```py\n    [1] \"Training data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 59626  9635\n             1  1865  7921\n\n                   Accuracy : 0.8545        \n                     95% CI : (0.852, 0.857)\n        No Information Rate : 0.7779        \n        P-Value [Acc > NIR] : < 2.2e-16     \n\n                      Kappa : 0.4999        \n     Mcnemar's Test P-Value : < 2.2e-16     \n\n                Sensitivity : 0.4512        \n                Specificity : 0.9697        \n             Pos Pred Value : 0.8094        \n             Neg Pred Value : 0.8609        \n                 Prevalence : 0.2221        \n             Detection Rate : 0.1002        \n       Detection Prevalence : 0.1238        \n          Balanced Accuracy : 0.7104        \n\n           'Positive' Class : 1             \n    ```", "```py\n    print(\"Test data results -\")\n    pred_test <- factor(ifelse(predict(xgb,data.matrix(xgb.test),\n    type=\"class\")>0.53,1,0))\n    confusionMatrix(pred_test,factor(y_test),positive='1')\n    ```", "```py\n    1] \"Test data results -\"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 25551  4210\n             1   864  3253\n\n                   Accuracy : 0.8502         \n                     95% CI : (0.8464, 0.854)\n        No Information Rate : 0.7797         \n        P-Value [Acc > NIR] : < 2.2e-16      \n\n                      Kappa : 0.4804         \n     Mcnemar's Test P-Value : < 2.2e-16      \n\n                Sensitivity : 0.43588        \n                Specificity : 0.96729        \n             Pos Pred Value : 0.79014        \n             Neg Pred Value : 0.85854        \n                 Prevalence : 0.22029        \n             Detection Rate : 0.09602        \n       Detection Prevalence : 0.12152        \n          Balanced Accuracy : 0.70159        \n\n           'Positive' Class : 1  \n    ```", "```py\n    standardizer <- preProcess(x_train, method='range',rangeBounds=c(0,1))\n    ```", "```py\n    x_train_scaled <- predict(standardizer, newdata=x_train)\n    x_test_scaled <- predict(standardizer, newdata=x_test)\n    ```", "```py\n    predictors <-  dim(x_train_scaled)[2]\n    ```", "```py\n    dl_model <-  keras_model_sequential()  %>% \n      layer_dense(units = 250, activation = 'relu', \n    input_shape =c(predictors)) %>% \n      layer_dense(units = 250, activation = 'relu' ) %>% \n      layer_dense(units = 250, activation = 'relu') %>% \n      layer_dense(units = 1, activation = 'sigmoid') \n    ```", "```py\n    dl_model %>% compile(\n      loss = 'binary_crossentropy',\n      optimizer = optimizer_adam(),\n      metrics = c('accuracy')\n    )\n    summary(dl_model)\n    ```", "```py\n    _____________________________________________________________\n    Layer (type)                 Output Shape               Param #\n    =============================================================\n    dense_34 (Dense)             (None, 250)                16750\n    _____________________________________________________________\n    dense_35 (Dense)             (None, 250)                62750\n    _____________________________________________________________\n    dense_36 (Dense)             (None, 250)                62750\n    _____________________________________________________________\n    dense_37 (Dense)             (None, 1)                  251  \n    =============================================================\n    Total params: 142,501\n    Trainable params: 142,501\n    Non-trainable params: 0\n    ```", "```py\n    history <- dl_model %>% fit(\n      as.matrix(x_train_scaled), as.matrix(y_train), \n      epochs = 10, batch_size = 32, \n      validation_split = 0.2\n    )\n    ```", "```py\n    Train on 63237 samples, validate on 15810 samples\n    Epoch 1/10\n    63237/63237 [==============================] - 7s 104us/step – \n    loss: 0.3723 - acc: 0.8388 - val_loss: 0.3639 - val_acc: 0.8426\n    Epoch 2/10\n    63237/63237 [==============================] - 6s 102us/step – \n    loss: 0.3498 - acc: 0.8492 - val_loss: 0.3695 - val_acc: 0.8380\n    Epoch 3/10\n    63237/63237 [==============================] - 6s 97us/step – \n    loss: 0.3434 - acc: 0.8518 - val_loss: 0.3660 - val_acc: 0.8438\n    Epoch 4/10\n    63237/63237 [==============================] - 6s 99us/step – \n    loss: 0.3390 - acc: 0.8527 - val_loss: 0.3628 - val_acc: 0.8395\n    Epoch 5/10\n    63237/63237 [==============================] - 6s 97us/step – \n    loss: 0.3340 - acc: 0.8551 - val_loss: 0.3556 - val_acc: 0.8440\n    Epoch 6/10\n    63237/63237 [==============================] - 7s 119us/step – \n    loss: 0.3311 - acc: 0.8574 - val_loss: 0.3612 - val_acc: 0.8414\n    Epoch 7/10\n    63237/63237 [==============================] - 7s 107us/step – \n    loss: 0.3266 - acc: 0.8573 - val_loss: 0.3536 - val_acc: 0.8469\n    Epoch 8/10\n    63237/63237 [==============================] - 7s 105us/step – \n    loss: 0.3224 - acc: 0.8593 - val_loss: 0.3575 - val_acc: 0.8471\n    Epoch 9/10\n    63237/63237 [==============================] - 7s 105us/step – \n    loss: 0.3181 - acc: 0.8607 - val_loss: 0.3755 - val_acc: 0.8444\n    Epoch 10/10\n    63237/63237 [==============================] - 7s 104us/step – \n    loss: 0.3133 - acc: 0.8631 - val_loss: 0.3601 - val_acc: 0.8468\n    ```", "```py\n    print(\"Training data results - \")\n    pred_train <- factor(ifelse(predict(dl_model, \n    as.matrix(x_train_scaled))>0.5,1,0))\n    confusionMatrix(pred_train,factor(y_train),positive='1')\n    ```", "```py\n    \"Training data results - \"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 59281  8415\n             1  2351  9000\n\n                   Accuracy : 0.8638          \n                     95% CI : (0.8614, 0.8662)\n        No Information Rate : 0.7797          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.547           \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.5168          \n                Specificity : 0.9619          \n             Pos Pred Value : 0.7929          \n             Neg Pred Value : 0.8757          \n                 Prevalence : 0.2203          \n             Detection Rate : 0.1139          \n       Detection Prevalence : 0.1436          \n          Balanced Accuracy : 0.7393          \n\n           'Positive' Class : 1               \n    ```", "```py\n    #Predict on Test Data\n    pred_test <- factor(ifelse(predict(dl_model,   \n                               as.matrix(x_test_scaled))>0.5,1,0))\n    confusionMatrix(pred_test,factor(y_test),positive='1')\n    ```", "```py\n    \"Test data results - \"\n    Confusion Matrix and Statistics\n              Reference\n    Prediction     0     1\n             0 25028  3944\n             1  1246  3660\n\n                   Accuracy : 0.8468          \n                     95% CI : (0.8429, 0.8506)\n        No Information Rate : 0.7755          \n        P-Value [Acc > NIR] : < 2.2e-16       \n\n                      Kappa : 0.4965          \n     Mcnemar's Test P-Value : < 2.2e-16       \n\n                Sensitivity : 0.4813          \n                Specificity : 0.9526          \n             Pos Pred Value : 0.7460          \n             Neg Pred Value : 0.8639          \n                 Prevalence : 0.2245          \n             Detection Rate : 0.1080          \n       Detection Prevalence : 0.1448          \n          Balanced Accuracy : 0.7170          \n           'Positive' Class : 1               \n    ```"]