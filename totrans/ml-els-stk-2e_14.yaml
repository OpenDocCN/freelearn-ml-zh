- en: '*Chapter 11*: Classification Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：分类分析'
- en: 'When we speak about the field of machine learning and specifically the types
    of machine learning algorithms, we tend to invoke a taxonomy of three different
    classes of algorithms: supervised learning, unsupervised learning, and reinforcement
    learning. The third one falls outside of the scope of both this book and the current
    features available in the Elastic Stack, while the second one has been our topic
    of investigation throughout the chapters on anomaly detection, as well as the
    previous chapter on outlier detection. In this chapter, we will finally start
    dipping our toes into the world of supervised learning. The Elastic Stack provides
    two flavors of supervised learning: classification and regression. This chapter
    will be dedicated to understanding the former, while the subsequent chapter will
    tackle the latter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论机器学习领域以及具体的机器学习算法类型时，我们往往会提到三种不同类别的算法的分类法：监督学习、无监督学习和强化学习。第三种超出了本书的范围以及当前
    Elastic Stack 可用的功能，而第二种在异常检测章节以及之前的离群值检测章节中一直是我们的研究主题。在本章中，我们最终将开始涉足监督学习的世界。Elastic
    Stack 提供两种监督学习类型：分类和回归。本章将致力于理解前者，而随后的章节将解决后者。
- en: The goal of supervised learning is to take a labeled dataset and extract the
    patterns from it, encode the knowledge obtained from the dataset within a structure
    we call a model, and then use this trained model to make predictions on previously
    unseen examples of data. This is common for both classification and regression.
    The former is used to predict discrete labels or classes while the former for
    continuous values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的目标是接受一个标记的数据集，从中提取模式，将数据集获得的知识编码在我们称之为模型的结构中，然后使用这个训练好的模型对之前未见过的数据实例进行预测。这在分类和回归中都很常见。前者用于预测离散标签或类别，而后者用于连续值。
- en: Classification problems are all around us. A histopathologist looking at patient
    samples is tasked with classifying each as either malignant or benign, an assembly
    line worker looking at machine parts is tasked with classifying each as faulty
    or functional, a business analyst looking at customer churn data is trying to
    predict whether a customer will renew or cancel a subscription for a service and
    so forth.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题无处不在。一位病理学家在观察患者样本时，被要求将每个样本分类为恶性或良性，一位装配线工人观察机器部件时，被要求将每个部件分类为故障或功能正常，一位业务分析师在观察客户流失数据时，试图预测客户是否会续订或取消一项服务的订阅，等等。
- en: The process of learning about classification and how it works in the Elastic
    Stack will also bring us into close contact with a number of other topics, the
    understanding of which is relevant to any practitioner who wishes to make full
    use of classification to solve problems. These topics include feature engineering,
    the division of a dataset into a training and a test set, understanding how to
    measure the performance of a classifier and why the same performance metric applied
    to the training set measures a different thing than when it is applied to the
    testing set, how to use feature importance to understand how each feature contributed
    to the class label the model assigned to a data point, as well as many others
    whose discussion will make the bulk of this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 了解分类及其在 Elastic Stack 中的工作原理的过程，也会让我们接触到许多其他主题，对这些主题的理解对于任何希望充分利用分类来解决问题的从业者来说都是相关的。这些主题包括特征工程、将数据集划分为训练集和测试集、理解如何衡量分类器的性能以及为什么相同的性能指标在应用于训练集时与应用于测试集时测量的结果不同，如何使用特征重要性来理解每个特征如何贡献到模型分配给数据点的类别标签，以及许多其他将在本章中讨论的内容。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: 'Classification: from data to a trained model'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类：从数据到训练好的模型
- en: Taking your first steps with classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用分类
- en: 'Classification under the hood: gradient boosted decision trees'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类内部机制：梯度提升决策树
- en: Hyperparameters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数
- en: Interpreting results
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释结果
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The material in this chapter requires Elasticsearch 7.9+. The examples have
    been tested using Elasticsearch version 7.10.1, but should work on any version
    of Elasticsearch later than 7.9\. Please note that running the examples in this
    chapter requires a Platinum license. In case a particular example or section requires
    a later version of Elasticsearch, this will be mentioned in the text.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容需要Elasticsearch 7.9+。示例已使用Elasticsearch版本7.10.1进行测试，但应适用于任何高于7.9版本的Elasticsearch。请注意，运行本章中的示例需要铂金许可证。如果某个特定示例或部分需要Elasticsearch的更高版本，文本中将会提及。
- en: 'Classification: from data to a trained model'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类：从数据到训练模型
- en: The process of training a classification model from a source dataset is a multi-step
    affair that involves many steps. In this section, we will take a bird's eye view
    (depicted in *Figure 11.1*) of this whole process, which begins with a labeled
    training dataset (*Figure 11.1* part A.).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从源数据集训练分类模型的过程是一个多步骤的过程，涉及许多步骤。在本节中，我们将从*图11.1*（图中所示）的角度概述整个过程，该过程从标记的训练数据集开始（*图11.1*部分A）。
- en: '![Figure 11.1 – An overview of the supervised learning process that takes a
    labeled dataset and outputs a trained model'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – 监督学习过程概述，该过程接受标记数据集并输出训练模型'
- en: '](img/B17040_11_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_1.jpg]'
- en: Figure 11.1 – An overview of the supervised learning process that takes a labeled
    dataset and outputs a trained model
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 监督学习过程概述，该过程接受标记数据集并输出训练模型
- en: This training dataset is usually split into a training part, which will be fed
    into the training algorithm (*Figure 11.1* part B.). The output of the training
    algorithm is a trained model (*Figure 11.1* part C.). The trained model is then
    used to classify the testing dataset (*Figure 11.1*, part D.), originally set
    aside from the whole dataset. The performance of the model on the testing dataset
    is captured in a set of evaluation metrics that can be used to determine whether
    a model generalizes well enough to previously unseen examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练数据集通常被分为训练部分，这部分将被输入到训练算法中（*图11.1*部分B）。训练算法的输出是一个训练模型（*图11.1*部分C）。然后，使用训练模型对测试数据集进行分类（*图11.1*，部分D），该测试数据集最初是从整个数据集中分离出来的。模型在测试数据集上的性能被捕获在一系列评估指标中，这些指标可以用来确定模型是否足够泛化，以适应之前未见过的示例。
- en: Each of these steps will be further clarified in practical walk-throughs presented
    in this chapter. To relate the practical examples to the more theoretical aspects
    of machine learning, we will also present a conceptual understanding of what supervised
    learning is. This will lead us into a discussion about feature engineering – what
    are features and how do they impact the performance of a classifier? Finally,
    we will look at how to evaluate the performance of a classifier and how splitting
    the data into a training and a testing set helps us measure two different kinds
    of performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将进一步阐述这些步骤，通过本章提供的实际操作演示。为了将实际示例与机器学习的更理论方面联系起来，我们还将介绍对监督学习的概念理解。这将引导我们进入关于特征工程——什么是特征以及它们如何影响分类器性能的讨论。最后，我们将探讨如何评估分类器的性能以及如何通过将数据分为训练集和测试集来帮助我们衡量两种不同的性能。
- en: lassification models learn from data
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型从数据中学习
- en: A common maxim in the machine learning community is that a machine learning
    classifier is a software artifact that learns from data. What exactly does it
    mean for a software artifact to learn from data? One way to think about this is
    that learning happens when the model becomes better and better at making classifications
    (in the case of classification models) as it sees or experiences more and more
    data. But a machine learning model, after all, is not a living being, so what
    exactly do we mean when we say that a model learns from data?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习社区中有一个常见的说法，即机器学习分类器是一种从数据中学习的软件工件。软件工件从数据中学习究竟意味着什么呢？一种思考方式是，当模型在看到或经历越来越多的数据时，它越来越擅长进行分类（在分类模型的情况下），这时学习就发生了。但毕竟，机器学习模型不是生物体，那么当我们说模型从数据中学习时，我们究竟指的是什么呢？
- en: The answer to this question will lead us to the important concept of **decision
    boundaries**. Let's examine this concept through a fictional two-dimensional dataset
    that records the weights and circumferences of pumpkins. Suppose that our dataset
    contains measurements from different variants of pumpkins and our goal is to teach
    our classification model to classify pumpkins into variants based on their weight
    and circumference.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案将引导我们到重要的**决策边界**概念。让我们通过一个记录南瓜重量和周长的虚构二维数据集来检验这个概念。假设我们的数据集包含不同变种南瓜的测量值，我们的目标是教会我们的分类模型根据南瓜的重量和周长将南瓜分类到不同的变种中。
- en: If we plot our pumpkin dataset in two dimensions, it might look something like
    *Figure 11.2*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将南瓜数据集绘制在二维空间中，它可能看起来像*图11.2*。
- en: '![Figure 11.2 – A fictional two-dimensional dataset depicting circumference
    and weight measurements from two different types of pumpkins represented by the
    circles and squares'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.2 – 一个虚构的二维数据集，描述了两种不同类型南瓜的周长和重量测量，这些南瓜分别由圆圈和正方形表示'
- en: '](img/B17040_11_2.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_2.jpg](img/B17040_11_2.jpg)'
- en: Figure 11.2 – A fictional two-dimensional dataset depicting circumference and
    weight measurements from two different types of pumpkins represented by the circles
    and squares
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – 一个虚构的二维数据集，描述了两种不同类型南瓜的周长和重量测量，这些南瓜分别由圆圈和正方形表示
- en: The weight is on the horizontal axis and the circumference on the vertical.
    If a data point is represented by a circle, it represents a pumpkin of the first
    variant. If by a square, it represents a pumpkin of another variant. Take a moment
    to look at the distribution of squares and circles in *Figure 11.2* and imagine
    you had to create a simple rule to distinguish the square variant pumpkins from
    the circle variant pumpkins and record it in such a way so that when another person
    looks at your drawing at a later time, they can use it to determine which of the
    two variants a new pumpkin they have just measured belongs to.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重量位于水平轴上，周长位于垂直轴上。如果一个数据点由一个圆表示，它代表一个第一变种的南瓜。如果由一个正方形表示，它代表另一个变种的南瓜。花点时间看看*图11.2*中正方形和圆的分布，并想象你需要创建一个简单的规则来区分方形变种南瓜和圆形变种南瓜，并以此方式记录下来，以便当另一个人在以后的时间查看你的绘图时，他们可以使用它来确定他们刚刚测量的新南瓜属于两个变种中的哪一个。
- en: A very simple way you might go about this task is to examine the two-dimensional
    representation in front of you and draw with a pen, a line that approximately
    separates the square variant from the circle variant, for example, as in *Figure
    11.3*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个任务的一个非常简单的方法是检查你面前的二维表示，并用笔画一条线，大致将方形变种与圆形变种分开，例如，如图*11.3*所示。
- en: '![Figure 11.3 – A decision boundary separates two classes of pumpkins – represented
    as data points in a two-dimensional space – from each other.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.3 – 一个决策边界将两个南瓜类别——在二维空间中以数据点表示——彼此分开。'
- en: '](img/B17040_11_3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_3.jpg](img/B17040_11_3.jpg)'
- en: Figure 11.3 – A decision boundary separates two classes of pumpkins – represented
    as data points in a two-dimensional space – from each other.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 一个决策边界将两个南瓜类别（在二维空间中以数据点表示）彼此分开。
- en: A few data points will end up on the wrong sides of the line, but this is just
    the inevitable nature of designing classifiers – very few classifications end
    up being perfect! What you have just recorded is a decision boundary – a line
    or a hyperdimensional plane (if we are operating on a dataset with multiple variables)
    that separates the members of one class from another. Now, anyone who wants to
    classify newly measured pumpkins can pick up your diagram and plot the measurements
    of their own pumpkin on the diagram to see which side of the decision boundary
    their data point falls on, as shown in *Figure 11.4*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据点最终会落在线的错误一侧，但这只是设计分类器的必然性质——很少的分类最终是完美的！你刚刚记录的是决策边界——一条线或超维平面（如果我们正在处理具有多个变量的数据集），它将一个类别的成员与另一个类别的成员分开。现在，任何想要对新的测量南瓜进行分类的人都可以拿起你的图表，并在图表上绘制他们自己南瓜的测量值，以查看他们的数据点落在决策边界的哪一侧，如图*11.4*所示。
- en: '![Figure 11.4 – The new data point represented by the triangle falls on the
    right side of the decision boundary and is thus likely a part of the square variant'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.4 – 由三角形表示的新数据点落在决策边界的右侧，因此很可能是方形变种的一部分'
- en: '](img/B17040_11_4.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_4.jpg](img/B17040_11_4.jpg)'
- en: Figure 11.4 – The new data point represented by the triangle falls on the right
    side of the decision boundary and is thus likely a part of the square variant
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 由三角形表示的新数据点落在决策边界的右侧，因此很可能是正方形变体的一个部分
- en: This is a conceptual approximation of the process that classification algorithms
    take. They take a set of training data (our original pumpkin measurements and
    the variants to which each measured pumpkin belongs) and then apply various tests
    and transformations to learn a decision boundary. This process of learning from
    the training data is called **training** and it applies not only to classification
    algorithms but to regression ones as well. This boundary becomes encoded in a
    **trained model** and can then be used to make predictions on future, previously
    unseen data points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对分类算法所采取过程的近似概念。它们接受一组训练数据（我们的原始南瓜测量值以及每个测量南瓜所属的变体）并应用各种测试和转换来学习决策边界。从训练数据中学习的过程称为**训练**，这不仅适用于分类算法，也适用于回归算法。这个边界被编码在**训练模型**中，然后可以用来对未来的、之前未见过的数据点进行预测。
- en: One thing to note from the above fictional example is that we got fairly lucky.
    The two-dimensional plot of our data of pumpkin weight versus pumpkin circumferences
    yielded a picture where we could easily draw a decision boundary that, while imperfect,
    nevertheless gives good results for the majority of data points. However, in the
    real world, very few datasets will lend themselves this neatly to machine learning
    analysis, and as we will learn, the attributes that we choose to represent our
    data points can have a big impact on how separable a dataset is and thus how well
    a classification model will perform on it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述虚构的例子中要注意的一点是，我们相当幸运。我们的数据（南瓜重量与南瓜周长的二维图）产生了一幅图，我们可以轻松地画出决策边界，虽然并不完美，但对于大多数数据点来说，结果仍然很好。然而，在现实世界中，很少有数据集会如此整洁地适合机器学习分析，而且正如我们将学到的，我们选择用来表示数据点的属性可以极大地影响数据集的可分离性，从而影响分类模型在其上的表现。
- en: Suppose that instead of using the **features** weight and circumference for
    our pumpkins, we had chosen the percentage of chemical compound X that was discovered
    in the pumpkin and the amount of water that was used to water it during growth.
    In this case, the two-dimensional plot of the data points (shown in Figure 11.5)
    using these features does not make it easy to draw any kind of decision boundary
    at all!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不是使用南瓜的**特征**重量和周长，而是选择了在南瓜中发现化学化合物 X 的百分比以及生长过程中用于灌溉的水量。在这种情况下，使用这些特征的数据点的二维图（如图
    11.5 所示）根本无法画出任何类型的决策边界！
- en: '![Figure 11.5 – These features do not provide a meaningful separation of the
    two classes'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5 – 这些特征并没有为两个类别提供有意义的分离](img/B17040_11_5.jpg)'
- en: '](img/B17040_11_5.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17040_11_5.jpg](img/B17040_11_5.jpg)'
- en: Figure 11.5 – These features do not provide a meaningful separation of the two
    classes
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 这些特征并没有为两个类别提供有意义的分离
- en: As we will see in the next section, **feature engineering** is a large topic
    on its own and a major pre-processing step and warrants consideration when embarking
    on any machine learning project, whether in the Elastic Stack or outside of it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在下一节中看到的，**特征工程**是一个庞大的主题，也是预处理的主要步骤之一，在开始任何机器学习项目时都应予以考虑，无论是在 Elastic
    Stack 中还是在其他地方。
- en: Feature engineering
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'In the previous section, we illustrated the concept of decision boundaries
    that are the product of the classifier learning from data using a fictional pumpkin
    dataset. Let''s examine the next topic, the process of selecting and manipulating
    features in such a way that they are suitable for classification, with the help
    of a more realistic example: that of malware classification. With millions of
    new malware variants being released into the wild almost every day, it is difficult
    for traditional rule-based approaches to classify benign binaries from malicious
    ones and stay effective. Because we are dealing with a large volume of data and
    varied inputs impossible to capture with inflexible rules, machine learning provides
    a perfect solution for this. How exactly do we need to preprocess the training
    data – the malicious and benign binaries in this case – in order for the machine
    learning algorithm to be able to understand them. This question and the answer
    to it brings us to a whole subfield of study in machine learning known as feature
    engineering.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过一个虚构的南瓜数据集说明了决策边界的概念，这是分类器从数据中学习的结果。现在让我们通过一个更现实的例子来探讨下一个主题，即如何选择和操纵特征以便它们适合分类：恶意软件分类。由于几乎每天都有数百万新的恶意软件变种被发布到野外，传统的基于规则的分类方法很难将良性二进制文件与恶意二进制文件区分开来并保持有效性。因为我们处理的是大量数据和各种输入，这些输入无法用僵化的规则来捕捉，因此机器学习为这个问题提供了一个完美的解决方案。我们究竟需要如何预处理训练数据——在这个例子中是恶意和良性二进制文件——以便机器学习算法能够理解它们。这个问题及其答案将我们引向机器学习中的一个子领域，称为特征工程。
- en: The process of feature engineering involves taking the knowledge that domain
    experts have about the problem and applying it to the training data. For example,
    a malware analyst might be able to tell us that the strings in normal binaries
    are often longer than the strings in malicious binaries and this can be a helpful
    feature in distinguishing the two. As part of the feature engineering process,
    we would then develop a way to calculate the average string length in each of
    the binaries in our training data and use that as a feature.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的过程涉及将领域专家对问题的了解应用于训练数据。例如，一个恶意软件分析师可能能够告诉我们，正常二进制文件中的字符串通常比恶意二进制文件中的字符串长，这可以作为区分两者的一个有用特征。作为特征工程过程的一部分，我们随后开发了一种方法来计算训练数据中每个二进制文件的平均字符串长度，并将其用作特征。
- en: It is good to set aside some time to understand what knowledge domain experts
    have of the problem and what features have been used to achieve state-of-the-art
    results. This is important, because, as we saw in our fictional pumpkin example
    above, the selection of features determines whether or not our model will be able
    to learn the distinguishing characteristics of the two classes. For example, suppose
    that we trained our malware classification model to use the size of the binary
    in bytes and the presence of the letter a in the name of the binary as features.
    If it turned out that both malicious and benign binaries exhibit similar sizes
    in bytes and both contain the letter a in their names, our resulting classifier
    would be useless because it would look at features that make no difference when
    it comes to distinguishing malicious binaries from benign.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最好留出一些时间来了解知识领域专家对问题的了解以及为了实现最先进的结果所使用的特征。这很重要，因为正如我们在上面的虚构南瓜例子中所看到的，特征的选择决定了我们的模型是否能够学会区分两个类别的特征。例如，假设我们训练我们的恶意软件分类模型使用二进制文件的大小（以字节为单位）和二进制文件名称中字母a的存在作为特征。如果结果是恶意和良性二进制文件在字节大小上表现出相似性，并且两者名称中都包含字母a，那么我们得到的分类器将毫无用处，因为它会关注那些在区分恶意二进制文件和良性二进制文件时没有差别的特征。
- en: Feature engineering often involves iteration. One should start with a guess
    of what features might produce good results, train a model, and evaluate it on
    the test set and then iterate gradually adding, reducing, or manipulating features
    until a desired quality of results is reached. Later on in the chapter, we will
    take a look at exactly how to measure the performance of the model and how you
    can achieve this using machine learning in the Elastic Stack.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程通常涉及迭代。我们应该从一个可能产生良好结果的特征的猜测开始，训练一个模型，并在测试集上评估它，然后逐渐添加、减少或操纵特征，直到达到期望的结果质量。在章节的后面部分，我们将探讨如何精确地衡量模型的性能，以及如何使用Elastic
    Stack中的机器学习来实现这一点。
- en: Evaluating the model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: The final topic that we will cover in this introductory section on classification
    is that of evaluation. How do we know how well our model performed? There are
    various ways to quantify **model performance** and we will take a look at these
    techniques and what they mean in a later section of this chapter in detail. An
    important concept related to measuring the performance of the model is what data
    the performance is measured on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节分类的介绍中，我们将讨论的最后一个主题是评估。我们如何知道我们的模型表现如何？有各种方法可以量化**模型性能**，我们将在本章的后续部分详细探讨这些技术和它们的意义。与衡量模型性能相关的一个重要概念是性能是在什么数据上衡量的。
- en: In order to measure how well a model does, we need to use the model to make
    predictions on a labeled dataset so that we can compare the class labels that
    the model has predicted with the ground truth labels and calculate how many mistakes
    the model made. One of these datasets is the training data, but if we were to
    use this data to estimate the performance of the model, we would be essentially
    showing the model the same data as we used to train it. While doing this would
    give us a good estimate of the **training error**, it would not tell us anything
    about how well the model will generalize to make predictions on previously unseen
    examples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量一个模型的表现如何，我们需要使用该模型在标记的数据集上进行预测，以便我们可以将模型预测的类别标签与真实标签进行比较，并计算模型犯了多少错误。这些数据集之一是训练数据，但如果我们使用这些数据来估计模型的表现，我们实际上是在向模型展示我们在训练时使用的数据。虽然这样做会给我们一个很好的**训练误差**估计，但它不会告诉我们模型在预测先前未见过的例子时表现如何。
- en: In order to estimate the performance of the model on data the model has not
    previously seen, we have to set aside a portion of the training data. This portion,
    also often known as the **testing dataset**, will not be used for training the
    model. Instead, after the training process has been completed, we will use the
    model to make predictions on the testing dataset and see how many data points
    in the testing dataset are misclassified. This measure will give us an idea of
    how well the model will generalize to make predictions on previously unseen data
    points. The number of mistakes the model makes when making predictions on this
    dataset is known as the **generalization error**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计模型在模型之前未见过的数据上的表现，我们必须将一部分训练数据留出。这部分数据，也常被称为**测试数据集**，不会用于模型的训练。相反，在训练过程完成后，我们将使用模型在测试数据集上进行预测，并查看测试数据集中有多少数据点被错误分类。这个度量将给我们一个关于模型在预测先前未见过的数据点时表现如何的印象。模型在预测这个数据集时犯的错误数量被称为**泛化误差**。
- en: Several metrics such as **accuracy,** **precision**, and **recall** (some of
    which we saw in [*Chapter 10*](B17040_10_Epub_AM.xhtml#_idTextAnchor177), *Outlier
    Detection*), will be used to measure these errors. Refer back to the section *Evaluating
    outlier detection with the Evaluate API* in[*Chapter 10*](B17040_10_Epub_AM.xhtml#_idTextAnchor177)
    for a quick reminder of the basic concepts.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将几个指标如**准确率**、**精确度**和**召回率**（其中一些我们在[*第10章*](B17040_10_Epub_AM.xhtml#_idTextAnchor177)，*异常检测*中见过）用于衡量这些误差。请参考[*第10章*](B17040_10_Epub_AM.xhtml#_idTextAnchor177)中*使用Evaluate
    API评估异常检测*的部分，以快速回顾基本概念。
- en: We will discuss each of the ideas explored above in more detail in later chapters,
    but for now, let's turn to see how these concepts play out in practice. We will
    use the Wisconsin Breast Cancer public dataset to create a sample classification
    data frame analytics job.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中更详细地讨论上述探索的每个想法，但就目前而言，让我们看看这些概念在实际中是如何发挥作用的。我们将使用威斯康星州乳腺癌公共数据集来创建一个示例分类数据帧分析作业。
- en: Taking your first steps with classification
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始进行分类的第一步
- en: 'In this section, we will be creating a sample classification job using the
    public Wisconsin Breast Cancer dataset. The original dataset is available here:
    ([https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original))).
    For this exercise, we will be using a slightly sanitized version of the dataset,
    which will remove the necessity for data cleaning (an important step in the lifecycle
    of a machine learning project, but not one we have space to discuss in this book)
    and allow us to focus on the basics of creating a classification job:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用公开的威斯康星州乳腺癌数据集创建一个示例分类作业。原始数据集在此处可用：([https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original))).
    对于这个练习，我们将使用数据集的一个稍微清洗过的版本，这将消除数据清洗（机器学习项目生命周期中的重要步骤，但不是本书中我们有机会讨论的内容）的需要，并使我们能够专注于创建分类作业的基本知识：
- en: Download the sanitized dataset file `breast-cancer-wisconsin-outlier.csv` from
    the `Chapter 11 - Classification Analysis` folder in the book's GitHub repository
    ([https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2011%20-%20Classification%20Analysis](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2011%20-%20Classification%20Analysis))
    and store it locally on your machine. In your Kibana instance, navigate to the
    `Machine` `Learning` app from the left-hand side menu and click on the **Data
    Visualizer** tab. This will take you to the **File** **Uploader**. Click on **Upload
    file** and select the CSV you downloaded.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从本书 GitHub 存储库中的“第 11 章 - 分类分析”文件夹下载清洗过的数据集文件`breast-cancer-wisconsin-outlier.csv`（[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2011%20-%20Classification%20Analysis](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2011%20-%20Classification%20Analysis)），并将其存储在您的本地机器上。在您的
    Kibana 实例中，从左侧菜单导航到`Machine` `Learning` 应用程序，并点击**数据可视化器**标签。这将带您进入**文件** **上传器**。点击**上传文件**并选择您下载的
    CSV 文件。
- en: Once the upload is successful (check your index pattern under **Discover** and
    briefly browse a few documents to make sure everything is alright with the dataset),
    navigate back to the **Machine Learning** app and instead of selecting the **Data
    Visualizer** tab, select **Data Frame Analytics**. This should bring up a view
    like the one displayed in *Figure 11.6*.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上传成功后（在**发现**下检查您的索引模式，并简要浏览一些文档以确保数据集一切正常），返回到**机器学习**应用程序，而不是选择**数据可视化器**标签，选择**数据帧分析**。这应该会显示类似于*图
    11.6*中显示的视图。
- en: '![Figure 11.6 – This data frame analytics jobs overview is currently empty'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.6 – 此数据帧分析作业概览目前为空'
- en: '](img/B17040_11_6.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.7 – 数据帧分析向导帮助创建三种不同类型的作业。对于我们的当前案例，选择分类](img/B17040_11_6.jpg)'
- en: Figure 11.6 – This data frame analytics jobs overview is currently empty
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.6 – 当前此数据帧分析作业概览为空
- en: Click on the blue **Create job** button. This will take you to the **Data frame
    analytics** wizard, which is a bit like the **Transforms** wizard we introduced
    in [*Chapter 9*](B17040_09_Epub_AM.xhtml#_idTextAnchor162), *Introducting Data
    Frame Analytics*, and allows you to easily create a classification, regression,
    or outlier detection job. In this case, we will be selecting classification in
    the dropdown in *Figure 11.7*.![Figure 11.7 – The Data frame analytics wizard
    assists with creating three different types of jobs. For our current case, select
    Classification
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击蓝色**创建作业**按钮。这将带您进入**数据帧分析**向导，它有点类似于我们在[*第 9 章*](B17040_09_Epub_AM.xhtml#_idTextAnchor162)，“介绍数据帧分析”中介绍的**转换**向导，并允许您轻松创建分类、回归或异常检测作业。在这种情况下，我们将在*图
    11.7*中的下拉菜单中选择分类。
- en: '](img/B17040_11_7.jpg)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.7 – 数据帧分析向导帮助创建三种不同类型的作业。对于我们的当前案例，选择分类](img/B17040_11_7.jpg)'
- en: Figure 11.7 – The Data frame analytics wizard assists with creating three different
    types of jobs. For our current case, select Classification
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.7 – 数据帧分析向导帮助创建三种不同类型的作业。对于我们的当前案例，选择分类
- en: Next, we will move on to selecting the dependent variable. If you recall from
    our discussion earlier, the goal of classification is to learn to predict which
    class a given, previously unseen data point belongs to. The variable that denotes
    this class is called the `Class` variable and hence we select that from the dropdown.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将继续选择因变量。如果您还记得我们之前的讨论，分类的目标是学会预测一个给定的、之前未见过的数据点属于哪个类别。表示这个类别的变量被称为`Class`变量，因此我们从下拉菜单中选择它。
- en: Once we make the selection, the wizard will display a list of included and excluded
    fields. Pay attention here. If there is an existing field in your dataset `Class`.
    If the sample number is less than say 30, then predict malignant. If not, predict
    benign. Therefore, it is good to be prudent, examine one's data in advance, and
    then delete all of the variables that are not expected to carry meaningful information
    that would help predict the dependent variable.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们做出选择，向导将显示一个包含和排除字段列表。请注意这里。如果您的数据集中存在名为`Class`的字段。如果样本数量少于30，则预测为恶性。如果不是，则预测为良性。因此，谨慎行事，提前检查数据，然后删除所有预期不会携带有助于预测因变量的有意义信息的变量是很好的。
- en: Even if the variable that denotes the sample number does not carry such proxying
    artifacts, we still do not expect it to convey much meaningful information that
    would help in deducing the dependent variable and as such, it would just bloat
    our job's memory and it is better to exclude it right from the start.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使表示样本数量的变量不携带这样的代理效应，我们也不期望它能传达很多有助于推断因变量的有意义信息，因此，它只会增加我们任务的内存负担，最好从一开始就排除它。
- en: As you can see in *Figure 11.8*, we have excluded the variable `Sample_code_number`
    from the job by unticking the box.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您在*图11.8*中看到的，我们已经通过取消勾选复选框排除了变量`Sample_code_number`。
- en: '![Figure 11.8 – Exclude Sample_code_number from the classification job to avoid
    introducing proxying effects'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图11.8 – 从分类任务中排除Sample_code_number以避免引入代理效应'
- en: '](img/B17040_11_8.jpg)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片/B17040_11_8.jpg]'
- en: Figure 11.8 – Exclude Sample_code_number from the classification job to avoid
    introducing proxying effects
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.8 – 从分类任务中排除Sample_code_number以避免引入代理效应
- en: 'After this, we are ready to move on to the next part of the configuration,
    which involves selecting the **training percentage**. Recall that at the beginning
    of this section, we discussed how the performance of the classifier can be evaluated
    on two distinct datasets derived from the parent dataset: the training and the
    testing dataset. Making this split manually within Elasticsearch can be tedious,
    in particular, if one is dealing with a large dataset. To make this easier, the
    Data Frame Analytics job wizard includes a configuration option that allows us
    to set how much of the dataset we want to set aside for training the model and
    how much of the dataset we want to set aside for testing the model. This quantity
    is set in the wizard by using the slider displayed in *Figure 11.9*.![Figure 11.9
    – The training percent slider'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此之后，我们就可以继续配置的下一部分，这涉及到选择**训练百分比**。回想一下，在本节的开头，我们讨论了分类器的性能如何可以在从父数据集派生出的两个不同的数据集上评估：训练数据集和测试数据集。在Elasticsearch中手动进行这种拆分可能会很繁琐，尤其是当处理大型数据集时。为了使这个过程更容易，DataFrame
    Analytics作业向导包括一个配置选项，允许我们设置我们想要为训练模型保留的数据集比例以及我们想要为测试模型保留的数据集比例。这个数量是通过向导中显示的滑块来设置的，如*图11.9*所示。![图11.9
    – 训练百分比滑块
- en: '](img/B17040_11_9.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片/B17040_11_9.jpg]'
- en: Figure 11.9 – The training percent slider
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.9 – 训练百分比滑块
- en: How should one determine the training percent? As in many phases of building
    a machine learning program, the answer to this will largely depend on the size
    of your dataset and whether you care more about getting an accurate estimate for
    the performance on the training dataset or on the testing dataset. We will discuss
    the difference between these two performance measures in more detail later, but
    for now, suffice it to say that the performance of the model on the testing dataset
    gives you an estimate of how well the model will perform once you start applying
    it to previously unseen examples, so in most cases, it definitely pays to dedicate
    a portion of the data to get an estimate of how well your model will perform once
    you deploy it into production.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该如何确定训练百分比？正如在构建机器学习程序的许多阶段一样，这个答案将很大程度上取决于您数据集的大小以及您是否更关心对训练数据集还是测试数据集的性能进行准确估计。我们将在稍后更详细地讨论这两种性能指标之间的差异，但就目前而言，可以说，模型在测试数据集上的性能为您提供了模型一旦开始应用于先前未见过的示例时的性能估计，因此，在大多数情况下，将一部分数据用于估计模型部署到生产环境后的性能是非常值得的。
- en: The other aspect is the size of the dataset. As a rule of thumb, if your dataset
    contains more than 100,000 documents, start with a smaller training percentage
    – around 10% or 15% and then iterate depending on the quality of the results.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个方面是数据集的大小。一般来说，如果你的数据集包含超过100,000个文档，开始时使用较小的训练百分比——大约10%或15%，然后根据结果的质量进行迭代。
- en: Since in this case, the whole dataset contains just under 700 documents, we
    will set the training percent as 60%.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于在这种情况下，整个数据集只有不到700个文档，我们将训练百分比设置为60%。
- en: After selecting the training percent, the wizard guides us to move onto **Additional
    options** as shown in *Figure 11.10*. We will leave these settings as defaults
    for now but will return to discuss them in later, more advanced examples.![Figure
    11.10 – Additional options for classification Data Frame Analytics jobs
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择训练百分比后，向导将引导我们进入**附加选项**，如图*图11.10*所示。我们现在将保留这些默认设置，但将在后续的更高级示例中返回讨论它们。![Figure
    11.10 – Additional options for classification Data Frame Analytics jobs
- en: '](img/B17040_11_10.jpg)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_11_10.jpg]'
- en: Figure 11.10 – Additional options for classification Data Frame Analytics jobs
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.10 – Additional options for classification Data Frame Analytics jobs
- en: Finally, after clicking **Continue**, we will set the job ID and leave everything
    else as the default. Tick the **Start immediately** tickbox and finally create
    and start the job by clicking the **Create** button.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在点击**继续**后，我们将设置作业ID，并将其他所有内容保留为默认设置。勾选**立即开始**复选框，最后通过点击**创建**按钮创建并启动作业。
- en: Return back to the **Data frame analytics** main page. You should see the job
    you just created in the job management panel as displayed in *Figure 11.11*.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回到**数据帧分析**主页面。你应该能在作业管理面板中看到你刚刚创建的作业，如图*图11.11*所示。
- en: '![Figure 11.11 – The Data frame analytics job overview panel shows a summary
    of the current Data frame analytics jobs'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 11.11 – The Data frame analytics job overview panel shows a summary
    of the current Data frame analytics jobs'
- en: '](img/B17040_11_11.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_11_11.jpg]'
- en: Figure 11.11 – The Data frame analytics job overview panel shows a summary of
    the current Data frame analytics jobs
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.11 – The Data frame analytics job overview panel shows a summary of the
    current Data frame analytics jobs
- en: Once the job status is displayed as **stopped** and the **Progress** bar display
    indicates that all phases of the job have been completed, use the **Actions**
    menu to navigate to the results. This is achieved by clicking on **View** in the
    menu.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦作业状态显示为**停止**，并且**进度**条显示表明作业的所有阶段都已完成，使用**操作**菜单导航到结果。这是通过在菜单中点击**查看**来实现的。
- en: The results page displays a host of information that is vital to understanding
    how the classifier we trained on our dataset performed. A sample results page
    is shown in *Figure 11.12*.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果页面显示了大量的信息，这些信息对于理解我们在数据集上训练的分类器表现至关重要。一个示例结果页面如图*图11.12*所示。
- en: As you look at this figure, make a note of the `analyzed_fields` stanza, you
    will see that there is a field called `Outlier`. This was a duplicate field that
    we created from the `Class` field in [*Chapter 10*](B17040_10_Epub_AM.xhtml#_idTextAnchor177),
    *Outlier Detection*. This means that it was also the proxy field that was making
    our results look better than they really are.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你查看这个图表时，注意一下`analyzed_fields`部分，你会看到一个名为`Outlier`的字段。这是我们根据[*第10章*](B17040_10_Epub_AM.xhtml#_idTextAnchor177)，*异常值检测*中的`Class`字段创建的重复字段。这意味着它也是一个使我们的结果看起来比实际情况更好的代理字段。
- en: Let's re-create a duplicate job but exclude both the field `Outlier` and the
    field `Sample_code_number`. After this job has finished, the new results on the
    testing dataset look like *Figure 11.14*.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们重新创建一个重复的作业，但排除字段`Outlier`和字段`Sample_code_number`。在这个作业完成后，测试数据集上的新结果看起来像*图11.14*。
- en: '![Figure 11.14 – The confusion matrix displays the results from the classification
    job after excluding the Outlier variable'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.14 – The confusion matrix displays the results from the classification
    job after excluding the Outlier variable'
- en: '](img/B17040_11_14.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_14.jpg]'
- en: Figure 11.14 – The confusion matrix displays the results from the classification
    job after excluding the Outlier variable
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 混淆矩阵显示了在排除异常值变量后的分类作业结果
- en: As we can see from the results in *Figure 11.14*, excluding the `Outlier` variable
    along with the `Sample_code_number` variable has resulted in a true positive rate
    of 98% as well as some false positives and false negatives, which is a more realistic
    result than the perfect classification score we had before.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.14*所示，从结果中我们可以看到，排除`Outlier`变量和`Sample_code_number`变量后，真正阳性率达到了98%，以及一些假阳性假阴性，这比我们之前完美的分类评分更接近实际情况。
- en: 'Before we go any further into classification problems, it will be good to understand
    exactly what goes under the hood in the Elastic Machine Learning Stack when we
    train our classification model. That is what the next section will be dedicated
    to: understanding how decision trees work under the hood.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨分类问题之前，了解当我们训练分类模型时，Elastic机器学习堆栈内部到底发生了什么，将是非常有益的。这就是下一节将致力于解决的问题：理解决策树是如何在内部工作的。
- en: 'Classification under the hood: gradient boosted decision trees'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部工作原理：梯度提升决策树
- en: The ultimate goal for a classification task is to solve a problem that requires
    us to take previously unseen data points and try to infer which of the several
    possible classes they belong to. We achieve this by taking a labeled training
    dataset that contains a representative number of data points, extracting relevant
    features that allow us to learn a decision boundary, and then encode the knowledge
    about this decision boundary into a classification model. This model then makes
    **decisions** about which class a given data point belongs to. How does the model
    learn to do this? This is the question that we will try to answer in this section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，最终目标是解决一个需要我们尝试推断未见数据点属于几个可能类别之一的问题。我们通过使用包含代表性数据点的标记训练数据集，提取允许我们学习决策边界的相关特征，然后将关于这个决策边界的知识编码到分类模型中来实现这一点。然后，该模型对给定数据点属于哪个类别做出**决定**。模型是如何学习做到这一点的？这是我们将在本节中尝试回答的问题。
- en: In accordance with our habits throughout the book, let's start by exploring
    conceptually what tools humans use to navigate a set of complicated decisions.
    A familiar tool that many of us have used before to help make decisions when several,
    possibly complex factors are involved, is a flowchart. *Figure 11.15* displays
    a sample flowchart that one might construct to be able to decide, given the weather
    conditions of the day, what type of jacket to wear.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们在整本书中的习惯，让我们首先从概念上探索人类用来导航一系列复杂决策的工具。许多人在涉及多个可能复杂的因素时，都曾使用过的一个熟悉工具是流程图。*图11.15*显示了一个示例流程图，人们可能会构建这个流程图，以便在知道当天的天气条件下，决定穿哪种夹克。
- en: '![Figure 11.15 – A basic flowchart'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.15 – 一个基本的流程图'
- en: '](img/B17040_11_15.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_11_15.jpg)'
- en: Figure 11.15 – A basic flowchart
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 一个基本的流程图
- en: At each stage, a flowchart asks a question (such as how warm or cold it is or
    whether or not it is raining outside) and based on our answer, redirects us to
    another part of the flowchart and a new set of questions. Ultimately, by answering
    questions in the flowchart and following the flow, we come to a decision or a
    class label.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，流程图都会提出一个问题（例如天气是暖和还是寒冷，或者是否在下雨）并根据我们的回答，将我们引导到流程图的另一部分和一组新问题。最终，通过在流程图中回答问题并遵循流程，我们得出一个决定或类别标签。
- en: The model that is produced as part of the training process in the Elastic Stack,
    conceptually, does something very similar. In the machine learning field, the
    algorithm is known as a decision tree. Although the version used in the Elastic
    ML stack is much more complicated than what is described here, the basic concepts
    apply.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Elastic Stack训练过程的一部分产生的模型，在概念上，做的是非常相似的事情。在机器学习领域，这个算法被称为决策树。尽管Elastic ML堆栈中使用的版本比这里描述的要复杂得多，但基本概念是适用的。
- en: Let's take a closer look at decision trees.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看决策树。
- en: Introduction to decision trees
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树简介
- en: How is our decision tree constructed? We begin by splitting the dataset into
    two groups using the value of a certain feature and a certain threshold. The way
    we determine this feature and this threshold is by looking through all of the
    available features (or fields if we want to use the Elasticsearch terminology)
    and then find the one feature threshold pair that produces the purest split. What
    do we mean by the purest split? In order to understand the concept of node purity
    and how it affects the construction of the decision tree and the subsequent usage
    of the decision tree to classify a new previously unseen datapoint, we have to
    take a step back and examine the overall picture.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何构建决策树的？我们首先通过某个特征和某个阈值将数据集分为两组。我们确定这个特征和这个阈值的方法是查看所有可用的特征（或者如果我们想使用Elasticsearch术语，则是字段），然后找到产生最纯分割的那个特征阈值对。我们所说的最纯分割是什么意思？为了理解节点纯度的概念以及它如何影响决策树的构建和随后的决策树对新未见数据点的分类使用，我们必须退后一步，审视整体情况。
- en: As you may recall, our goal is to essentially design a decision flowchart that
    we can traverse when we need to classify a new, unknown data point. The way we
    determine a classification in a flowchart is by looking at the final node to which
    our traversal leads. In decision trees, this final node is called the leaf node
    and it consists of all of the data points in our training data that have ended
    up there as a result of successive splits performed using a certain feature and
    a certain threshold. Since the classification procedure is usually imperfect,
    it is inevitable that terminal leaves will not contain data points belonging to
    one class only, but instead will be mixed. The amount of this mixing can be quantified
    by various measures and is known as the purity of the node or leaf. A leaf or
    node that contains only data points of one class is the purest one.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆，我们的目标是设计一个决策流程图，当我们需要分类一个新未知数据点时可以遍历它。我们在流程图中确定分类的方式是通过查看我们的遍历最终到达的最终节点。在决策树中，这个最终节点被称为叶节点，它由所有在训练数据中由于使用特定特征和特定阈值进行的连续分裂而最终到达那里的数据点组成。由于分类过程通常是不完美的，终端叶节点不可能只包含属于一个类别的数据点，而将是混合的。这种混合的程度可以通过各种度量来量化，称为节点的纯度或叶节点的纯度。只包含一个类别的数据点的叶节点或节点是最纯的。
- en: Having pure nodes in a decision tree is great because it means that once we
    have reached the terminal node in our "flowchart" we can be fairly confident that
    the datapoint we are currently trying to classify belongs to the same class as
    the data points already in the terminal node. In practice, optimizing for pure
    nodes may lead to too much splitting in the decision tree (after all, we could
    gauge this metric by simply creating as many leaf nodes as there are data points
    – in this way, each leaf would contain exactly one data point of exactly one class
    and thus have perfect purity), so the leaf nodes will always be mixed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中拥有纯节点是非常好的，因为这意味着一旦我们达到了“流程图”中的终端节点，我们可以相当自信地认为我们目前正在尝试分类的数据点属于与终端节点中已有的数据点相同的类别。在实践中，优化纯节点可能会导致决策树过度分裂（毕竟，我们可以通过简单地创建与数据点数量一样多的叶节点来衡量这个指标——这样，每个叶节点将恰好包含一个确切类别的确切数据点，从而具有完美的纯度），因此叶节点总是混合的。
- en: Computing the proportion of data points in the leaf node that belongs to a given
    class gives us an estimate of the probability that the datapoint we are trying
    to classify belongs to that class. For example, suppose we have a decision tree
    to classify data points into class A and class B. One of the terminal nodes has
    80 examples from class A and 20 examples from class B. During classification,
    if our new data point ends up in this node, the probability of it being class
    A is then 0.8\.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 计算叶节点中属于给定类别的数据点的比例，可以给我们一个估计，即我们正在尝试分类的数据点属于该类别的概率。例如，假设我们有一个决策树来将数据点分类到类别A和类别B。其中一个终端节点有80个来自类别A的示例和20个来自类别B的示例。在分类过程中，如果我们的新数据点最终到达这个节点，那么它属于类别A的概率就是0.8。
- en: This is of course a simplified representation of what actually happens under
    the hood, but should hopefully give you a conceptual understanding of how decision
    trees work.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然是对实际发生情况的一种简化表示，但应该能帮助你从概念上理解决策树是如何工作的。
- en: Gradient boosted decision trees
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升决策树
- en: Often a single decision tree on its own will not produce a strong classifier.
    That is why, over the years, data scientists and machine learning practitioners
    have discovered that tree-based classifiers can be powerful if combined with special
    training schemes that iteratively improve them. One such scheme is known as boosting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，单个决策树本身不会产生强大的分类器。这就是为什么多年来，数据科学家和机器学习从业者发现，如果结合特殊的训练方案，基于树的分类器可以非常强大。其中一种方案被称为提升。
- en: Without going into too much technical detail, the process of boosting trains
    a succession of decision trees, and each decision tree improves upon the previous
    one. The boosting procedure does this by taking the data points that were misclassified
    by the previous iteration of the decision tree and re-training a new decision
    tree to improve classification on these previously misclassified points.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入技术细节，提升的过程是训练一系列决策树，并且每个决策树都会改进前一个决策树。提升过程通过选择前一个决策树分类错误的那些数据点，并重新训练一个新的决策树来改进这些先前分类错误点的分类。
- en: Hyperparameters
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: In the previous section, we took a conceptual overview of how decision trees
    are constructed. In particular, we established that one of the criteria for determining
    where a decision tree should be split (in other words, when a new path should
    be added to our conceptual flowchart) is by looking at the purity of the resulting
    nodes. We also noted that allowing the algorithm to exclusively focus on the purity
    of the nodes as a criterion for constructing the decision tree would quickly lead
    to trees that **overfit** the training data. These decision trees are so tuned
    to the training data that they are not only capturing the most salient features
    for classifying a given data point but are even modeling the noise in the data
    as though it is a real signal. Therefore, while this kind of a decision tree that
    is allowed to optimize for specific metrics without restrictions will perform
    really well on the training data, it will neither perform well on the testing
    dataset nor generalize well to classifying previously unseen data points, which
    is the ultimate goal in training the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们概述了决策树是如何构建的。特别是，我们确定确定决策树应该在哪里分裂（换句话说，何时应该向我们的概念流程图添加新路径）的一个标准是查看结果节点的纯度。我们还指出，如果让算法仅以节点纯度为标准构建决策树，将很快导致过度拟合训练数据的树。这些决策树调整得如此之好，以至于它们不仅捕捉了用于分类给定数据点的最显著特征，甚至将数据中的噪声建模为真实信号。因此，虽然这种允许无限制优化特定指标的决策树在训练数据上表现良好，但它既不会在测试数据集上表现良好，也不会很好地泛化到分类先前未见过的数据点，这是训练模型的目标。
- en: To mitigate against these pitfalls, the training procedure for generating a
    decision tree from a training dataset has several hyperparameters. A **hyperparameter**
    is a kind of knob or advanced configuration setting that one can twist and adjust
    until one finds the optimal settings for training one's model. These knobs control
    things such as the number of trees that are trained in our boosting sequence,
    how deep each tree grows, how many features are used to train the tree, and so
    forth.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些陷阱，从训练数据集生成决策树的训练过程有几个超参数。**超参数**是一种可以旋转和调整的旋钮或高级配置设置，直到找到训练模型的最佳设置。这些旋钮控制着诸如在提升序列中训练的树的数量、每棵树的生长深度、用于训练树的特性数量等等。
- en: 'The hyperparameters that are exposed by the Data Frame Analytics API for classification
    are the following: `eta`, `feature_bag_fraction`, `gamma`, `lambda`, and `max_trees`.
    We will take a look at each of these in turn, but before we dive into examining
    what they mean and how they affect the resulting decision trees that are trained
    from our dataset, let''s take a moment to discuss **hyperparameter optimization**.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框分析API公开的用于分类的超参数如下：`eta`、`feature_bag_fraction`、`gamma`、`lambda`和`max_trees`。我们将依次查看这些参数，但在深入探讨它们的意义以及它们如何影响从我们的数据集中训练出的决策树之前，让我们花一点时间来讨论**超参数优化**。
- en: If you are an advanced user and have trained **gradient boosted trees** with
    other frameworks, you probably have an idea what is the optimal value for each
    of these for your particular training dataset and problem. However, how do you
    approach setting these values if you are starting from scratch. A systematic process
    for finding the best values for hyperparameters is called hyperparameter optimization.
    In this process, the training dataset is split into groups or cross-validation
    folds. To make sure that each cross-validation fold is representative of the whole
    training dataset, the sampling procedure that generates these folds makes sure
    that the proportion of members from each class is approximately the same in each
    cross-validation fold as it is in the whole dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个高级用户，并且已经使用其他框架训练了**梯度提升树**，你可能对你的特定训练数据集和问题中每个参数的最佳值有一个概念。然而，如果你是从零开始，你如何设置这些值呢？寻找超参数最佳值的系统过程称为超参数优化。在这个过程中，训练数据集被分成组或交叉验证折。为了确保每个交叉验证折代表整个训练数据集，生成这些折的采样过程确保每个交叉验证折中每个类别的成员比例与整个数据集中的比例大致相同。
- en: Once the dataset has been stratified into these folds, we can proceed to the
    next part of hyperparameter optimization. We have five hyperparameters that can
    each take a large range of values. How do we devise a systematic method for finding
    the best combination of hyperparameters? One option is to create a multidimensional
    grid where each point on the grid corresponds to a particular combination of hyperparameter
    values. For example, we could have `eta` 0.5, `feature_bag_fraction` at 0.8, `gamma`
    at 0.7, `lambda` at 0.6, and `max_trees` at 50\. Once we have selected these parameters,
    we take our K cross-validation folds and train a model on K-1 of the folds while
    leaving the Kth fold set aside for testing. We then perform this procedure K times
    for the same set of parameters so that each time we leave out a different Kth
    fold for testing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被分层到这些折中，我们就可以继续进行超参数优化的下一部分。我们有五个超参数，每个都可以取很大的值范围。我们如何设计一个系统性的方法来找到最佳的超参数组合？一个选项是创建一个多维网格，其中网格上的每个点对应于一组特定的超参数值。例如，我们可以有`eta`
    0.5，`feature_bag_fraction`为0.8，`gamma`为0.7，`lambda`为0.6，以及`max_trees`为50。一旦我们选择了这些参数，我们就在K个交叉验证折中训练一个模型，同时将K个折保留用于测试。然后我们针对同一组参数重复此过程K次，以确保每次我们都会留出一个不同的K个折用于测试。
- en: We then repeat this procedure for the next set of possible hyperparameter values
    until we find a combination of values that performs the best on the held-out testing
    set. As you can probably imagine, repeating this procedure even for a handful
    of hyperparameter combinations can be quite expensive in terms of time and compute.
    Thus, in practice, we use various optimization techniques to arrive at a good-enough
    set of hyperparameters while keeping in mind the potential cost of computation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们重复此过程，直到找到在保留的测试集上表现最佳的超参数值组合。正如你可能想象的那样，即使对于少量超参数组合重复此过程，在时间和计算上也可能相当昂贵。因此，在实践中，我们使用各种优化技术来获得足够好的超参数集，同时考虑到计算的成本。
- en: If you are curious which hyperparameters are selected for your model using hyperparameter
    optimization, you can navigate to the **Data Frame Analytics** page in Kibana
    and click on **Manage Jobs**. This will take you to the **Data Frame Analytics**
    job management page. Find the ID of the classification job that you wish to examine
    and click on the downward arrow on the left-hand side beside the job ID. This
    will display a dropdown with the details about the job. Click on the **Job stats**
    panel as shown in *Figure 11.17*. This will display information about the job
    along with analysis statistics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解使用超参数优化为你的模型选择了哪些超参数，你可以导航到Kibana中的**数据帧分析**页面并点击**管理作业**。这将带你到**数据帧分析**作业管理页面。找到你想要检查的分类作业的ID，然后在作业ID旁边的左侧点击向下箭头。这将显示一个包含作业详细信息的下拉菜单。点击如图11.17所示的**工作统计**面板。这将显示关于作业的信息以及分析统计。
- en: '![Figure 11.16 – The Job stats panel displays basic information about the classification
    job as well as information about the hyperparameters determined by hyperparameter
    optimization'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.16 – 工作统计面板显示关于分类工作以及由超参数优化确定的超参数的基本信息'
- en: '](img/B17040_11_16.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_16.jpg](img/B17040_11_16.jpg)'
- en: Figure 11.16 – The Job stats panel displays basic information about the classification
    job as well as information about the hyperparameters determined by hyperparameter
    optimization
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 – 工作统计面板显示关于分类工作以及由超参数优化确定的超参数的基本信息
- en: 'Finally, before we close off this section of the chapter, let''s take a brief
    tour of the five aforementioned hyperparameters: `eta`, `feature_bag_fraction`,
    `gamma`, `lambda`, and `max_trees` to get a feel for what they mean and what aspect
    of the decision tree training process they control. Before we dive into the process
    of figuring out what each of these means, let''s take a brief look at how gradient
    boosted decision trees are constructed. This will help set the definitions of
    these hyperparameters in context and make it clearer how each of these affects
    the final form of the sequence of decision trees that are produced as a result
    of gradient boosting.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们关闭本章的这一部分之前，让我们简要地浏览一下前面提到的五个超参数：`eta`、`feature_bag_fraction`、`gamma`、`lambda`和`max_trees`，以了解它们的意义以及它们控制决策树训练过程的哪个方面。在我们深入探讨每个这些参数的含义之前，让我们简要地看看梯度提升决策树是如何构建的。这将有助于在上下文中设定这些超参数的定义，并使每个这些如何影响梯度提升产生的决策树序列的最终形式变得更加清晰。
- en: As you might recall from previous sections, the basic building block of gradient
    boosted decision trees is a simple decision tree. The decision tree is constructed
    by recursively dividing the dataset into smaller and smaller sections based on
    thresholds from certain features. For example, if we were classifying data points
    representing various measurements from iris flowers, we might decide to split
    the dataset on petal length. All data points with petal lengths of less than 2
    cm are split into the left node, the rest into the right. The way we pick this
    feature – petal length in this case – is by going through all of the possible
    features in the dataset and examining the purity of the nodes that result in making
    a split using that feature. As you might imagine, in multidimensional datasets,
    it might take a very long time computationally to go through each of the features
    and test to see how pure the nodes that result from splitting on that feature
    are. In order to reduce the computation time required, we can choose to only test
    a fraction of the features and this fraction is what is controlled by the `feature_bag_fraction`
    parameter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能从之前的章节中回忆起来，梯度提升决策树的基本构建块是一个简单的决策树。决策树是通过递归地将数据集根据某些特征的阈值划分为越来越小的部分来构建的。例如，如果我们正在对代表各种测量值的鸢尾花数据点进行分类，我们可能会决定根据花瓣长度来分割数据集。所有花瓣长度小于2厘米的数据点被分割到左节点，其余的到右节点。我们选择这个特征——在这个例子中是花瓣长度——的方法是遍历数据集中的所有可能特征，并检查使用该特征进行分割产生的节点的纯度。正如您可能想象的那样，在多维数据集中，遍历每个特征并测试使用该特征分割产生的节点纯度可能需要非常长的时间。为了减少所需的计算时间，我们可以选择只测试特征的一部分，而这个比例是由`feature_bag_fraction`参数控制的。
- en: After we have trained our first decision tree based on the training dataset,
    the process of boosting demands that we take the data points that were misclassified
    by the first decision tree and construct a subsequent iteration that aims to improve
    upon the first decision tree. Thus by the end of this procedure, we will have
    a sequence of decision trees, which in some literature is also called a forest.
    The rate at which this forest grows, in other words, how long the final sequence
    of decision trees will be, is controlled by the parameter `eta`. The smaller the
    value of eta, the larger the forest will be, and also the better this forest will
    generalize to previously unseen data points.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们根据训练数据集训练出第一个决策树之后，提升过程要求我们选取第一个决策树错误分类的数据点，并构建一个后续迭代，目的是改进第一个决策树。因此，到这个过程的结束时，我们将有一系列决策树，在某些文献中这也可以称为森林。换句话说，这个森林的增长速度，即最终决策树序列的长度，是由参数`eta`控制的。`eta`的值越小，森林就会越大，而且这个森林对之前未见过的数据点的泛化能力也会越好。
- en: In our section introducing decision trees, we discussed how, when left to simply
    optimize to fit the training dataset, the tree can grow until it fits the training
    data perfectly. Although this might sound desirable, in reality allowing a tree
    to overfit the training dataset will result in a final model that generalizes
    poorly. Thus to control the growth of an individual decision tree, we have two
    hyperparameters, `gamma` and `lambda`. The higher the value of `gamma`, the more
    the training process will prefer smaller trees, which will help mitigate overfitting.
    Similarly to gamma, the higher the value of lambda, the smaller the decision trees
    will be.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍决策树的章节中，我们讨论了当仅优化以适应训练数据集时，树可以生长到完美适应训练数据。虽然这可能听起来很理想，但现实中允许树过度拟合训练数据集将导致最终模型泛化能力差。因此，为了控制单个决策树的生长，我们有两个超参数，`gamma`和`lambda`。`gamma`的值越高，训练过程就越倾向于生成更小的树，这有助于减轻过拟合。与`gamma`类似，`lambda`的值越高，决策树就会越小。
- en: Although in practice you can always rely on the hyperparameter optimization
    process to pick good values for each of these parameters, it is good to be aware
    at least on a conceptual level what each of these means and how they affect the
    final trained model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在实践中，你可以始终依赖超参数优化过程来为这些参数中的每一个选择好的值，但在概念层面上至少了解每个参数的含义以及它们如何影响最终训练好的模型是很好的。
- en: Interpreting results
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果解释
- en: In the last section, we took a look at the theoretical underpinnings of decision
    trees and took a conceptual tour of how they are constructed. In this section,
    we will return to the classification example we examined earlier in the chapter
    and take a closer look at the format of the results as well as how to interpret
    them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了决策树的理论基础，并从概念上了解了它们的构建方式。在本节中，我们将回到本章早期考察的分类示例，并更仔细地查看结果的格式以及如何解释它们。
- en: Earlier in the chapter, we created a trained model to predict whether a given
    breast tissue sample was malicious or benign (as a reminder, in this dataset malignant
    is denoted by class 2 and benign by class 4). A snippet of the classification
    results for this model is shown in *Figure 11.18*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们创建了一个训练好的模型来预测给定的乳腺组织样本是恶性还是良性（作为提醒，在这个数据集中恶性用类别2表示，良性用类别4表示）。该模型的分类结果片段显示在*图11.18*中。
- en: '![Figure 11.17 – Classification results for a sample data point in the Wisconsin
    breast cancer dataset'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.17 – 威斯康星乳腺癌数据集中一个样本数据点的分类结果'
- en: '](img/B17040_11_17.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_17.jpg](img/B17040_11_17.jpg)'
- en: Figure 11.17 – Classification results for a sample data point in the Wisconsin
    breast cancer dataset
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 11.17 – 威斯康星乳腺癌数据集中一个样本数据点的分类结果
- en: 'With this trained model, we can take previously unseen data points and make
    predictions. What form do these predictions take? In the simplest form, a data
    point is assigned a class label (the field `ml.Class_prediction` in *Figure 11.18*
    shows an example of this). In our example case, this label can take one of two
    values: 2 (malignant) and 4 (benign).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个训练好的模型，我们可以对之前未见过的数据点进行预测。这些预测是什么形式？在最简单的情况下，一个数据点被分配一个类别标签（*图11.18*中的`ml.Class_prediction`字段展示了这一点的例子）。在我们的示例案例中，这个标签可以取两个值之一：2（恶性）和4（良性）。
- en: 'However, this way of classifying data points masks a certain issue in the process
    of prediction: that of how confident we are in our predictions. You can see the
    importance of quantifying the uncertainty associated with our predictions if you
    think about something as quotidian as daily weather forecasts. One can predict
    that it will rain on Wednesday with an 80% probability and on Thursday with a
    16% probability. Both days have been assigned the label "rainy," but you would
    be less likely to forget your umbrella on Wednesday than on Thursday. This all
    goes to say that when evaluating our machine learning classifiers, we not only
    care about the label that has been assigned to a given data point but also how
    confident the model is about that label. In the Elastic Stack, there are two metrics
    that measure how confident our classification model is about the assigned class
    labels: class probability and class score. We will discuss both of these in greater
    detail below.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种对数据点进行分类的方式掩盖了预测过程中的一个问题：我们对预测的信心程度。如果你考虑像日常天气预报这样的事情，你会看到量化与我们的预测相关的不确定性的重要性。一个人可以预测周三有80%的概率下雨，周四有16%的概率下雨。这两天都被分配了“雨天”的标签，但你在周三比在周四更不可能忘记带伞。这一切都说明，当我们评估我们的机器学习分类器时，我们不仅关心分配给特定数据点的标签，还关心模型对这个标签的信心程度。在Elastic
    Stack中，有两个度量标准可以衡量我们的分类模型对分配的类别标签的信心程度：类别概率和类别得分。我们将在下面更详细地讨论这两个度量标准。
- en: In addition to knowing how confident a model is about a given prediction, it
    can often be very useful to know which features of the data point were important
    in nudging the classification of the point towards one class over another. The
    contribution of a datapoint's feature to its predicted label is summarized by
    its feature importance value.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了知道模型对给定预测的信心程度外，了解哪些数据点的特征在推动该点分类向某一类而非另一类转变中起到了重要作用，通常非常有用。数据点特征对其预测标签的贡献可以通过其特征重要性值来总结。
- en: Class probability
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别概率
- en: As mentioned above, it is usually not enough to just examine the label that
    a machine learning classifier has assigned to a data point. We also need to know
    what the probability of the assignment was. In *Figure 11.18*, the probabilities
    for both class 2 and class 4 are shown in the nested structure under the field
    `ml.top_classes`. As we can see from *Figure 11.18*, the class label assigned
    to the data point is 4 and the probability of the data point belonging to this
    class is 0.814 (rounded). The model is confident that this data point indeed belongs
    to class 4\.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，仅仅检查机器学习分类器分配给数据点的标签通常是不够的。我们还需要知道分配的概率。在*图11.18*中，类别2和类别4的概率显示在`ml.top_classes`字段下的嵌套结构中。从*图11.18*中我们可以看出，分配给数据点的类别标签是4，该数据点属于这个类别的概率是0.814（四舍五入）。模型对此数据点确实属于类别4有信心。
- en: Class score
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别得分
- en: While for many cases, assigning the class label based on the class that receives
    the higher probability is a good enough rule, it is not a good choice for all
    datasets. For example, for datasets where classes are highly imbalanced, a better
    option might be to use the class score. This value is recorded as `ml.prediction_score`
    in *Figure 11.18* and as a more precise `ml.top_classes.class_score` in the nested
    breakdown of class labels and class probabilities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多情况，根据获得更高概率的类别分配类别标签是一个足够好的规则，但并不是所有数据集都适用。例如，对于类别高度不平衡的数据集，更好的选择可能是使用类别得分。这个值在*图11.18*中记录为`ml.prediction_score`，在类别标签和类别概率的嵌套分解中记录为更精确的`ml.top_classes.class_score`。
- en: The class score is computed from the class probability, but in such a way that
    takes into whether one wants to maximize the accuracy or the minimum recall. In
    other words, how tolerant one is to misclassifications in classes that are less
    represented in the training dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类别得分是从类别概率计算得出的，但这种方式会考虑到是否希望最大化准确度或最小化召回率。换句话说，一个人对训练数据集中代表性较弱的类别中的误分类有多大的容忍度。
- en: Tip
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For a detailed explanation of how class scores are computed, please see the
    Elastic documentation here: [https://www.elastic.co/guide/en/machine-learning/current/dfa-classification.html#dfa-classification-class-score](https://www.elastic.co/guide/en/machine-learning/current/dfa-classification.html#dfa-classification-class-score)
    and for a more detailed walk-through, the Jupyter notebook here: [https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb](https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何计算类别得分的详细解释，请参阅此处 Elastic 文档：[https://www.elastic.co/guide/en/machine-learning/current/dfa-classification.html#dfa-classification-class-score](https://www.elastic.co/guide/en/machine-learning/current/dfa-classification.html#dfa-classification-class-score)，以及更详细的说明，请参阅此处的
    Jupyter 笔记本：[https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb](https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb)。
- en: Feature importance
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征重要性
- en: When examining the predictions of a machine learning model, not only do we care
    about the predicted class label, the probability of that class label, and also,
    potentially, the class score, we usually also want to know what were the features
    that contributed to the model making a certain decision. This is captured with
    feature importance. Each field used in the training process (the fields that were
    selected as **Included** during the configuration of the **Classification** job
    in the section *Taking your first steps with classification*) can be assigned
    a potential feature importance value, but usually, we only want to know about
    the most important features, that is, the fields that had the highest feature
    importance values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查机器学习模型的预测时，我们不仅关心预测的类别标签、该类别标签的概率，以及可能还有类别得分，我们通常还想知道哪些特征导致了模型做出特定的决策。这通过特征重要性来体现。在训练过程中使用的每个字段（在
    *使用分类开始您的第一步* 部分的 *分类* 作业配置中选择为 **包含** 的字段）可以分配一个潜在的特征重要性值，但通常我们只想了解最重要的特征，即具有最高特征重要性值的字段。
- en: Thus, to avoid cluttering the Elasticsearch results index that is written to
    our cluster with the results of each machine learning job, the **Classification**
    job configuration allows us to select the number of top feature importance values
    that will be written for each classified datapoint. This configuration is set
    to 4 in the example shown in *Figure 11.19*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免将每个机器学习作业的结果写入我们集群的 Elasticsearch 结果索引中造成混乱，**分类**作业配置允许我们选择为每个分类数据点写入的顶级特征重要性值的数量。在
    *图 11.19* 中显示的示例中，此配置设置为 4。
- en: '![Figure 11.18 – This configuration will write out 4 feature importance values
    for each document'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.18 – 此配置将为每个文档输出 4 个特征重要性值'
- en: '](img/B17040_11_18.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_11_18.jpg)'
- en: Figure 11.18 – This configuration will write out 4 feature importance values
    for each document
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 – 此配置将为每个文档输出 4 个特征重要性值
- en: Once the classification job has finished, each document in the results index
    will have, in addition to the predicted class, the class probability breakdown
    and the class score, and entries for each of the top four feature importance values
    for a given data point. An abridged snippet of the feature importance values for
    a sample data point is shown in *Figure 11.20*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分类作业完成，结果索引中的每个文档除了预测类别外，还将包含类别概率分解、类别得分，以及针对给定数据点的四个最高特征重要性值的条目。一个样本数据点的特征重要性值的简略片段显示在
    *图 11.20* 中。
- en: '![Figure 11.19 – Two feature importance values for a sample datapoint'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.19 – 一个样本数据点的两个特征重要性值'
- en: '](img/B17040_11_19.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_11_19.jpg)'
- en: Figure 11.19 – Two feature importance values for a sample datapoint
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 一个样本数据点的两个特征重要性值
- en: This datapoint is assigned class label 4\. Among the features that contributed
    to this predicted label were the value that this data point had for the field
    `Bare_Nuclei` and the value that it had for `Marginal_Adhesion`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据点被分配类别标签 4。在导致此预测标签的特征中，包括此数据点在字段 `Bare_Nuclei` 中的值和它在 `Marginal_Adhesion`
    中的值。
- en: In addition to examining the feature importance values for each individual data
    point, we can also examine which features are significant for classifications
    in the dataset as a whole. This chart is displayed in *Figure 11.21* and is available
    in the Data Frame Analytics results view (you can access this view by going to
    the Data Frame Analytics jobs management page, selecting the job for which you
    have configured how many feature importance values should be written out, and
    then clicking on **View**).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检查每个数据点的特征重要性值之外，我们还可以检查哪些特征对于整个数据集的分类是显著的。此图表显示在*图11.21*中，并在数据帧分析结果视图中可用（您可以通过访问数据帧分析作业管理页面，选择您已配置应输出多少特征重要性值的作业，然后点击**查看**来访问此视图）。
- en: '![Figure 11.20 – The total feature importance values for the whole dataset'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.20 – 整个数据集的总特征重要性值'
- en: '](img/B17040_11_20.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_11_20.jpg]'
- en: Figure 11.20 – The total feature importance values for the whole dataset
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 – 整个数据集的总特征重要性值
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have taken a deep dive into supervised learning. We have
    examined what supervised learning means, what role is played by training data
    in constructing the model, what it means to train a supervised learning model,
    what features are and how they should be engineered to obtain optimal performance,
    as well as how a model is evaluated and what various model performance measures
    mean.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了监督学习。我们考察了监督学习的含义，训练数据在构建模型中扮演的角色，训练监督学习模型的意义，特征是什么以及它们应该如何工程化以获得最佳性能，以及如何评估模型以及各种模型性能指标的含义。
- en: After learning about the basics of supervised learning in general, we took a
    closer look at classification and examined how one can create and run classification
    jobs in the Elastic Stack as well as how one can evaluate the trained models that
    are produced by these jobs. In addition to looking at basic concepts such as confusion
    matrices, we also examined situations where it is good to be skeptical about results
    that seem to be too good to be true and the potential underlying reasons why classification
    results can sometimes appear perfect and why this does not necessarily mean that
    the underlying trained model is any good.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解监督学习的基本知识之后，我们更深入地研究了分类，并考察了如何在Elastic Stack中创建和运行分类作业，以及如何评估这些作业产生的训练模型。除了查看基本概念，如混淆矩阵之外，我们还考察了在结果看似过于完美时应该持怀疑态度的情况，以及分类结果有时看似完美背后的潜在原因，以及这并不一定意味着底层训练模型有任何优点。
- en: 'Moreover, we took a deeper look at the engine that drives the classification
    functionality in the Elastic Stack: gradient boosted decision trees. To learn
    more about how decision trees work under the hood, we took a conceptual look at
    how an individual decision tree is constructed, what we mean by purity in the
    context of decision trees, as well as how unconstrained decision trees can lead
    to trained models that overfit on the training dataset and generalize poorly.
    In order to tune the process of growing a decision tree from a dataset, the training
    procedure exposes several advanced configuration parameters or hyperparameters.
    By default, these are set by a procedure known as hyperparameter optimization,
    but advanced users may also choose to tune these manually.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还更深入地研究了驱动Elastic Stack中分类功能的引擎：梯度提升决策树。为了了解决策树在底层是如何工作的，我们概念性地考察了单个决策树的构建方式，以及我们在决策树上下文中所说的纯度是什么，以及无约束的决策树如何导致训练数据集上的过度拟合和泛化不良的训练模型。为了调整从数据集生长决策树的过程，训练过程暴露了几个高级配置参数或超参数。默认情况下，这些是通过称为超参数优化的过程设置的，但高级用户也可以选择手动调整这些参数。
- en: In the final section of this chapter, we returned to our original breast cancer
    classification dataset to further examine the results format and the meaning of
    class probability, class score, and how feature importance can be used to determine
    which features contribute to a data point being assigned one class over another.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们回到了原始的乳腺癌分类数据集，进一步考察了结果格式以及类概率、类得分以及如何使用特征重要性来确定哪些特征导致数据点被分配为某一类而不是另一类。
- en: In the next chapter, we will build upon the foundation laid in this chapter
    to learn how decision trees can be used to solve problems where the dependent
    variable we want to predict is not a discrete value as in the case of classification
    but a continuous value.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在此基础上构建，学习如何使用决策树来解决依赖变量不是离散值（如分类情况）而是连续值的问题。
- en: Further reading
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information about how the class score is calculated, please take a
    look at the text and code examples in this Jupyter notebook: [https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb](https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何计算课程分数的更多信息，请参阅本 Jupyter 笔记本中的文本和代码示例：[https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb](https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb)。
