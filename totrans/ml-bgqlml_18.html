<html><head></head><body>
		<div id="_idContainer205">
			<h1 id="_idParaDest-194"><em class="italic"><a id="_idTextAnchor196"/>Chapter 14</em>: BigQuery ML Tips and Best Practices</h1>
			<p>BigQuery ML has the great advantage of democratizing the use of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) for data and business analysts. In fact, BigQuery ML enables users without any programming experience to implement advanced ML algorithms. Even though BigQuery ML is designed to simplify and automatize the creation of a ML model, there are some best practices and tips that should be adopted during the development life cycle of a ML algorithm to obtain an effective performance from it.</p>
			<p>Having a background in data science can help us in further improving the performance of our ML models and in avoiding pitfalls during the implementation of a use case. In this chapter, we'll learn how to choose the right technique for each specific business scenario and will learn about the tools we can leverage to improve the performance of ML models. </p>
			<p>Following a typical ML development life cycle, we'll go through the following topics:</p>
			<ul>
				<li>Choosing the right BigQuery ML algorithm</li>
				<li>Preparing the datasets</li>
				<li>Understanding feature engineering</li>
				<li>Tuning hyperparameters</li>
				<li>Using BigQuery ML for online predictions</li>
			</ul>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor197"/>Choosing the right BigQuery ML algorithm</h1>
			<p>In this section, we'll learn why it is <a id="_idIndexMarker626"/>so important to define a clear business objective before implementing a ML model, and we'll understand which BigQuery ML algorithm is suitable for each specific use case.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A <strong class="bold">data scientist</strong> is a professional in charge of the collection, analysis, and understanding of large amounts of <a id="_idIndexMarker627"/>data. This role typically requires a mix of skills, such as matching statistics and coding.</p>
			<p class="callout">A <strong class="bold">data analyst</strong> is different from <a id="_idIndexMarker628"/>a data scientist. A data analyst is more focused on industry knowledge and business processes rather than on coding and programming skills. People in this role have huge experience in data manipulation and visualization and are able to present relevant business insights derived from data.</p>
			<p>In order to<a id="_idIndexMarker629"/> get meaningful results in ML, it is necessary to define a clear business objective. Before starting on the actual implementation of the ML model, data analysts and data scientists should clearly define the business goal they wish to achieve.</p>
			<p>One of the most famous techniques to set up clear goals is known as the <strong class="bold">Specific, Measurable, Attainable, Relevant, and Time-based</strong> (<strong class="bold">SMART</strong>) framework. In this paradigm, each letter represents a<a id="_idIndexMarker630"/> specific feature that our final goal should satisfy, as outlined here: </p>
			<ul>
				<li><strong class="bold">Specific</strong>: It is necessary to define a clear and precise business objective.</li>
				<li><strong class="bold">Measurable</strong>: In order to understand if a BigQuery ML model satisfies our criteria, we need to select <a id="_idIndexMarker631"/>one or more <strong class="bold">Key Performance Indicators</strong> (<strong class="bold">KPIs</strong>) such as<a id="_idIndexMarker632"/> the <strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>), the <strong class="bold">Area Under the Curve</strong> (<strong class="bold">AUC</strong>)<a id="_idIndexMarker633"/> value, or the <strong class="bold">Mean</strong><strong class="bold"><a id="_idIndexMarker634"/></strong><strong class="bold"> Absolute Error</strong> (<strong class="bold">MAE</strong>).</li>
				<li><strong class="bold">Attainable</strong>: We need to analyze the complexity of the use case that we want to solve and set the right expectations—for example, we cannot expect that our BigQuery ML model will predict the right values 100% of the time.</li>
				<li><strong class="bold">Relevant</strong>: We need to focus our efforts on the most important use cases, as it may be that some business scenarios can bring a limited business advantage to our company.</li>
				<li><strong class="bold">Time-based</strong>: As data analysts and data scientists, we have limited resources in terms of time. Focusing on the right goals is fundamental in order to generate value for our company.</li>
			</ul>
			<p>We can apply the SMART framework to the ML field to help us in choosing the right use cases and the right BigQuery ML algorithm to use.</p>
			<p>As an example, in<a id="_idIndexMarker635"/> the following table, you can visualize the SMART framework applied to the use case that we have developed in <a href="B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>,  <em class="italic">Predicting Numerical Values with Linear Regression</em>:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B16722_14_001.jpg" alt="Figure 14.1 – The SMART framework applied to a ML use case&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – The SMART framework applied to a ML use case</p>
			<p>Once we have defined our business objective using the SMART approach, we are ready to select the best BigQuery ML algorithms that can help us in addressing the business scenario.</p>
			<p>According to the business objective that we want to achieve and the training dataset that we can leverage, we can identify the BigQuery ML algorithms that can potentially solve our use case.</p>
			<p>In the following table, we can see a summary of all the BigQuery ML techniques that we can use to develop our ML models:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B16722_14_002.jpg" alt="Figure 14.2 – The BigQuery ML algorithms at a glance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – The BigQuery ML algorithms at a glance</p>
			<p>In order to achieve our goal, we<a id="_idIndexMarker636"/> can navigate to the table represented in <em class="italic">Figure 14.2</em> and clearly find the BigQuery ML algorithm that can solve our use case. For example, for the business scenario that we've analyzed in <em class="italic">Figure 14.1</em>, we can assert the following:</p>
			<ul>
				<li>The BigQuery Public dataset that we used in <a href="B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Predicting Numerical Values with Linear Regression</em>, is a labeled dataset because, for each record, it includes the trip duration of the past rides.</li>
				<li>The expected outcome is a continuous real number. In fact, the goal of the use case is to predict the trip duration, expressed in minutes.</li>
			</ul>
			<p>By using the table presented in <em class="italic">Figure 14.2</em>, we notice that we can use one of the following BigQuery ML algorithms to address our prediction use case:</p>
			<ul>
				<li><strong class="bold">Linear regression</strong></li>
				<li><strong class="bold">Boosted Tree – XGBoost – Regression</strong></li>
				<li><strong class="bold">Deep Neural Network – Regression</strong></li>
			</ul>
			<p>In this section, we learned how we can set a clear business objective. Then, we understood which BigQuery <a id="_idIndexMarker637"/>ML technique we can use to address our business goal. In the next section, we'll focus on preparing the datasets to get effective ML models.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor198"/>Preparing the datasets</h1>
			<p>In this section, we'll learn <a id="_idIndexMarker638"/>about which techniques we can apply to ensure that the data we will use to build our ML model is correct and produces the desired results. After that, we'll discover the strategies that we can use to segment the datasets into training, validation, and test sets.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor199"/>Working with high-quality data</h2>
			<p>In this section, we'll<a id="_idIndexMarker639"/> understand the characteristics that our datasets should have in order to develop effective BigQuery ML models.</p>
			<p>Since ML models learn from data, it's very important to feed our ML algorithms with high-quality data, especially during<a id="_idIndexMarker640"/> the training phase. Since <strong class="bold">data quality</strong> is a very broad topic, it would require a specific book to analyze it in detail. For this reason, we will focus only on main data quality concepts in relation to the building of a ML model.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="bold">Data quality</strong> is a discipline that includes processes, professionals, technologies, and best practices to identify and correct anomalies, errors, and defects in data.</p>
			<p class="callout">This practice is fundamental in supporting business decisions with trusted and affordable data insights.</p>
			<p>We can measure the quality of a dataset according to different data quality dimensions.</p>
			<p>In the following diagram, you can see the different dimensions used to measure the quality of the data:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B16722_14_003.jpg" alt="Figure 14.3 – The data quality dimensions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – The data quality dimensions</p>
			<p>We can evaluate the quality of a dataset based on the following dimensions: <strong class="bold">Accuracy</strong>, <strong class="bold">Completeness</strong>, <strong class="bold">Consistency</strong>, <strong class="bold">Timeliness</strong>, <strong class="bold">Validity</strong>, and <strong class="bold">Uniqueness</strong>.</p>
			<p>In the following<a id="_idIndexMarker641"/> sections, we'll explain each data quality dimension and why it is important for the realization of a ML model.</p>
			<h3>Accuracy</h3>
			<p><strong class="bold">Accuracy</strong> refers to the information that <a id="_idIndexMarker642"/>is available in our dataset. If a numerical value is wrong and doesn't reflect reality, this will affect the effectiveness of the BigQuery ML model built on top of it.</p>
			<p>Discovering information that is inaccurate is not an easy task, but we can apply some data quality checks to identify relevant issues in the data—for example, we can execute queries to identify and eventually remove records that contain incorrect values.</p>
			<p>In the following table, you<a id="_idIndexMarker643"/> can see a typical example of inaccurate data, with the presence of a negative value to express the age of a person:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B16722_14_004.jpg" alt="Figure 14.4 – An example of a table with inaccurate values in the Age column &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – An example of a table with inaccurate values in the Age column </p>
			<p>Using <strong class="source-inline">MAX</strong> and <strong class="source-inline">MIN</strong> operators in <strong class="source-inline">SELECT</strong> queries can be a good way to find wrong values in the columns. These <a id="_idIndexMarker644"/>records are called outliers because they present very different values from the other records in the same column. Executing some preliminary <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>) queries to <a id="_idIndexMarker645"/>extract the maximum and the minimum values of the features and the label can be very useful in helping identify the most relevant errors in the dataset. </p>
			<p>For example, in <em class="italic">Figure 14.4</em>, Alice's age will be identified as an outlier in the <strong class="bold">Age</strong> column. In these cases, we can think about filtering out records with non-realistic values.</p>
			<h3>Completeness</h3>
			<p><strong class="bold">Completeness</strong> is focused on the presence of values in the columns of our dataset. If we train the BigQuery ML model<a id="_idIndexMarker646"/> on datasets with many missing or <strong class="source-inline">NULL</strong> fields, these fields will affect the performance of our model.</p>
			<p>In the following table, you <a id="_idIndexMarker647"/>can see an example of incomplete data:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B16722_14_005.jpg" alt="Figure 14.5 – An example of a table with incomplete values in the Age column &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – An example of a table with incomplete values in the Age column </p>
			<p>In order to prevent the presence of incomplete records, the most effective solution is to apply specific filters in the query to exclude records with missing values. A typical completeness check is based on adding a <strong class="source-inline">WHERE</strong> clause when the feature or the label of the model should not be empty and should be different from <strong class="source-inline">NULL</strong>.</p>
			<p>In the following snippet of code, you can see an example of a <strong class="source-inline">SELECT</strong> statement with completeness checks applied to the <strong class="source-inline">&lt;TEXT_FIELD&gt;</strong> placeholder:</p>
			<p class="source-code">SELECT *</p>
			<p class="source-code">FROM &lt;TABLE_NAME&gt;</p>
			<p class="source-code">WHERE</p>
			<p class="source-code">    &lt;TEXT_FIELD&gt; IS NOT NULL AND &lt;TEXT_FIELD&gt; &lt;&gt; '' </p>
			<p>In the preceding example, we're applying completeness checks on the table represented by the <strong class="source-inline">&lt;TABLE_NAME&gt;</strong> placeholder. The quality check verifies that the <strong class="source-inline">&lt;TEXT_FIELD&gt;</strong> field is not equal to <strong class="source-inline">NULL</strong> and is not empty.</p>
			<p>If a record presents incomplete fields, we can choose to exclude this record from the dataset or to replace missing <a id="_idIndexMarker648"/>values with default ones to fill the gaps.</p>
			<h3>Consistency</h3>
			<p>Achieving <strong class="bold">consistency</strong> in data is<a id="_idIndexMarker649"/> one of the most complex tasks to perform. In enterprise contexts, the same data is usually stored in multiple locations and may be expressed with different formats or units of measurement. When a column presents a value incompatible with the values in other columns of a dataset, the data is inconsistent. </p>
			<p>For example, we can imagine a table with a column that contains a temperature expressed in °C and another column with the same temperature in °F. If the two values should represent the same temperature but are not well calculated, the table will present an inconsistency.</p>
			<p>In the following table, you can see that the second record presents an inconsistency in the temperature values:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B16722_14_006.jpg" alt="Figure 14.6 – An example of a table with inconsistent values in the Temperature columns &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.6 – An example of a table with inconsistent values in the Temperature columns </p>
			<p>In <em class="italic">Figure 14.6</em>, the second record—which corresponds to the temperature measured in <strong class="bold">Paris</strong>—presents an inconsistency between the °C and the °F scale.</p>
			<p>To check the consistency of the data, we should usually apply validation checks on multiple fields that can reside in the same table or in different tables.</p>
			<h3>Timeliness</h3>
			<p><strong class="bold">Timeliness</strong> is particularly important <a id="_idIndexMarker650"/>when we need to use a BigQuery ML model. When we train a ML model, we need to be sure that all the features used during the training stage will be available when the ML model is executed.</p>
			<p>In the business scenario of <a href="B16722_04_Final_ASB_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Predicting Numerical Values with Linear Regression</em>, we used the start and stop stations of a bike rental company to predict the trip duration. In this case, we've trained the BigQuery ML model leveraging the start and stop station, but if the stop station is not available at the prediction time, the ML model becomes inapplicable and worthless.</p>
			<p>To avoid this common error, we need to check that all the features used to train the model will also be available during the prediction phase when the ML model will generate the predictions.</p>
			<h3>Validity</h3>
			<p>The <strong class="bold">validity</strong> of a value is<a id="_idIndexMarker651"/> strictly related to the expected format that a field should have. If a column contains date values expressed in the format <em class="italic">DD/MM/YYYY</em> and one of the records presents the format <em class="italic">DD-MM-YYYY</em>, the validity of the record is compromised.</p>
			<p>In the following table, you will notice that the second record presents a non-valid value for the <strong class="bold">Birth Date</strong> field:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B16722_14_007.jpg" alt="Figure 14.7 – An example of a table with invalid values in the Birth Date column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.7 – An example of a table with invalid values in the Birth Date column</p>
			<p>To meet this dimension, we <a id="_idIndexMarker652"/>must check that all the values of a column are stored in the same format and in a homogeneous way. In order to check the exact format of each field, we can apply regular expressions to the values in the columns. </p>
			<h3>Uniqueness</h3>
			<p><strong class="bold">Unique</strong> information means that there is<a id="_idIndexMarker653"/> exactly one record in a table to represent a specific item. Data duplication can happen due to several reasons, such as bugs in the data ingestion process, or uncontrolled interruptions and restarts during data loading.</p>
			<p>In order to prevent these<a id="_idIndexMarker654"/> errors, we need to know the fields that compose the <strong class="bold">Primary Key</strong> (<strong class="bold">PK</strong>) of our records, and we need to check that the <strong class="bold">PK</strong> matches one—and only one—record.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In a table, the <strong class="bold">PK</strong> is the minimum set of columns that uniquely identify a row.</p>
			<p>The presence of duplicated records can lead the ML model to learn from occurrences that are not actual but generated by technical errors.</p>
			<p>Now that we've discovered all the data quality dimensions to check before implementing our BigQuery ML model, let's take a look the techniques we can use to segment the datasets into training, validation, and test sets.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor200"/>Segmenting the datasets</h2>
			<p>In this section, we'll learn how to <a id="_idIndexMarker655"/>easily segment the datasets to support the training, validation, and test phases of a BigQuery ML model.</p>
			<p>For most BigQuery ML algorithms, we need to split the initial dataset into three different sets, as follows:</p>
			<ul>
				<li>The <strong class="bold">training</strong> dataset represents the <a id="_idIndexMarker656"/>sample of data that we'll use to train our BigQuery ML model.</li>
				<li>The <strong class="bold">validation</strong> dataset is different<a id="_idIndexMarker657"/> from the training dataset and we can use it to evaluate the model's performance. We perform validation on completely new data that is different from the sample data used in the training stage. In this phase, we can also tune the hyperparameters of the model.</li>
				<li>The <strong class="bold">test</strong> set is the dataset<a id="_idIndexMarker658"/> used to finally apply the model and verify its results and performance. </li>
			</ul>
			<p>Splitting the initial <a id="_idIndexMarker659"/>dataset into these three subsets can be cumbersome, but if you have enough data, you can apply the following rule of thumb:</p>
			<ul>
				<li>80% of the data for the training set.</li>
				<li>10% for the validation set.</li>
				<li>The remaining 10% for the test set.</li>
				<li>If we're working on large amounts of data, we can reduce the percentage of observations used for the validation and test sets. </li>
			</ul>
			<p>In the following screenshot, you can see a graphical representation of the optimal splitting strategy:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B16722_14_008.jpg" alt="Figure 14.8 – The 80/10/10 splitting strategy for ML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.8 – The 80/10/10 splitting strategy for ML</p>
			<p>In order to achieve the best results, the split procedure should be as random as possible.</p>
			<p>In the following code block, you can see how to apply this rule of thumb by using the <strong class="source-inline">MOD</strong> function:</p>
			<p class="source-code">SELECT</p>
			<p class="source-code">  CASE</p>
			<p class="source-code">    WHEN MOD(&lt;RECORD_KEY&gt;, 10) &lt; 8 THEN 'training'</p>
			<p class="source-code">    WHEN MOD(&lt;RECORD_KEY&gt;, 10) = 8 THEN 'evaluation'</p>
			<p class="source-code">    WHEN MOD(&lt;RECORD_KEY&gt;, 10) = 9 THEN 'test'</p>
			<p class="source-code">  END AS dataframe</p>
			<p class="source-code">FROM</p>
			<p class="source-code">  &lt;TABLE_NAME&gt;</p>
			<p>In the example, the records stored in the table represented by the placeholder <strong class="source-inline">&lt;TABLE_NAME&gt;</strong>, are split into three different sets according to the value of the field dataframe. In this case, the MOD <a id="_idIndexMarker660"/>function returns a value from 0 to 10. Using this function allows us to group the records into three different sets. By leveraging the clauses <strong class="source-inline">MOD(&lt;RECORD_KEY&gt;, 10)&lt;8, MOD(&lt;RECORD_KEY&gt;, 10) = 8 and MOD(&lt;RECORD_KEY&gt;, 10) = 9</strong>, we can split the records into the <strong class="source-inline">'training', 'evaluation'</strong> and <strong class="source-inline">'test'</strong> sets.</p>
			<p>Now that we've understood how to segment the datasets according to our needs for training, validation, and testing, let's look at the understanding of the feature engineering techniques.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor201"/>Understanding feature engineering</h1>
			<p>In this section, we'll understand<a id="_idIndexMarker661"/> which techniques we can use to improve the features of a BigQuery ML model before the training stage.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="bold">Feature engineering</strong> is the practice of applying preprocessing functions on raw data, to extract features useful for training a ML model. Creating preprocessed features can significantly improve the performance of a ML model.</p>
			<p>By design, BigQuery ML automatically applies feature engineering during the training phase when we use the <strong class="source-inline">CREATE MODEL</strong> function, but it also allows us to apply preprocessing transformations as well.</p>
			<p>In order to automatically apply the feature engineering operations during the training and the prediction stage, we can include all the pre-processing functions into the <strong class="source-inline">TRANSFORM</strong> clause when we train the BigQuery ML model.</p>
			<p>As we can see from the following code example, we need to use the <strong class="source-inline">TRANSFORM</strong> clause before the <strong class="source-inline">OPTIONS</strong> clause, and after the <strong class="source-inline">CREATE MODEL</strong> statement:</p>
			<p class="source-code">CREATE MODEL &lt;MODEL_NAME&gt;</p>
			<p class="source-code">TRANSFORM(&lt;TRANSFORM_CLAUSES&gt;)</p>
			<p class="source-code">OPTIONS</p>
			<p class="source-code">  (&lt;OPTION_CLAUSES&gt;) AS</p>
			<p class="source-code">&lt;TRAINING_TABLE&gt;</p>
			<p>In BigQuery ML, there are two<a id="_idIndexMarker662"/> different types of feature engineering function, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Scalar</strong> functions apply on <a id="_idIndexMarker663"/>a single record</li>
				<li><strong class="bold">Analytic</strong> functions <a id="_idIndexMarker664"/>calculate the results on all the rows</li>
			</ul>
			<p>In the following list, you can take a look at the most important feature engineering functions that you can apply in BigQuery ML:</p>
			<ul>
				<li><strong class="source-inline">ML.BUCKETIZE</strong>: This is used to convert a numerical expression into a categorical field—for example, you can use this function to convert the age of a person into <em class="italic">Young</em>, <em class="italic">Middle</em>, or <em class="italic">Old</em> buckets.</li>
				<li><strong class="source-inline">ML.FEATURE_CROSS</strong>: This is used to combine two features into a unique feature. For example, if in a dataset we have the gender and the place of birth of a person, we can combine these two features to simplify our BigQuery ML model. This technique is particularly indicated when we've correlated features and we want to include both the information in our ML model.</li>
				<li><strong class="source-inline">ML.QUANTILE_BUCKETIZE</strong>: This is very similar to the <strong class="source-inline">ML.BUCKETIZE</strong> function. In this case, the function is analytic and applies on all records in the dataset. The splitting of records into buckets is based on the quantiles of an entire set of records.<p class="callout-heading">Important note</p><p class="callout">The <strong class="bold">Quantile</strong> is the specific portion<a id="_idIndexMarker665"/> of a dataset. For example, it identifies how many values are above or below a certain threshold value.</p></li>
				<li><strong class="source-inline">ML.MIN_MAX_SCALER</strong>: This is an analytic function that returns a value from zero to one according to the distribution of the values in the entire record set.</li>
				<li><strong class="source-inline">ML.STANDARD_SCALER</strong>: This is an analytic function that allows us to use the standard deviation and mean of the record set.</li>
			</ul>
			<p>For an entire list of <a id="_idIndexMarker666"/>feature engineering and preprocessing functions, you can visit the official BigQuery documentation at the following link: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions</a>.</p>
			<p>Now that we have learned about feature engineering, let's move on to hyperparameter tuning.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor202"/>Tuning hyperparameters</h1>
			<p>In this section, we'll discover the <a id="_idIndexMarker667"/>most important hyperparameters that we can tune in BigQuery ML.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="bold">Hyperparameter tuning</strong> is the practice of choosing the best set of parameters to train a specific ML model. A hyperparameter influences and controls the learning process during the ML training stage.</p>
			<p>By design, BigQuery ML uses default hyperparameters to train a model, but advanced users can manually change them to influence the training process.</p>
			<p>In BigQuery ML, we can specify the hyperparameters in the <strong class="source-inline">OPTIONS</strong> clause as optional parameters. The most relevant hyperparameters, depending on the model, that we can change before starting the training of a BigQuery ML model are listed here:</p>
			<ul>
				<li><strong class="source-inline">L1_REG</strong>: This is a regularization parameter that we can use to prevent overfitting by keeping the weights of the model close to zero.</li>
				<li><strong class="source-inline">L2_REG</strong>: This is a second regularization parameter that we can use to prevent overfitting.</li>
				<li><strong class="source-inline">MAX_ITERATIONS</strong>: This represents the maximum number of iterations that BigQuery ML will perform to train the model.</li>
				<li><strong class="source-inline">LEARN_RATE</strong>: This is a parameter that affects how much the model changes according to the error of the previous iteration.</li>
				<li><strong class="source-inline">MIN_REL_PROGRESS</strong>: This is the<a id="_idIndexMarker668"/> minimum improvement that is necessary to continue the training after an iteration.</li>
				<li><strong class="source-inline">NUM_CLUSTERS</strong>: This is used for K-Means algorithms and represents the number of clusters that the model will create.</li>
				<li><strong class="source-inline">HIDDEN_UNITS</strong>: This is <a id="_idIndexMarker669"/>used in <strong class="bold">Deep Neural Networks</strong> (<strong class="bold">DNNs</strong>) and represents the number of hidden layers in a network.</li>
			</ul>
			<p>For a complete list of all the hyperparameters that you can apply with BigQuery ML, we suggest visiting the official documentation at <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create</a>.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor203"/>Using BigQuery ML for online predictions</h1>
			<p>In this section, we'll understand how <a id="_idIndexMarker670"/>we can use a BigQuery ML model in a synchronous and online manner.</p>
			<p>BigQuery ML represents a huge opportunity to democratize ML techniques for business and data analysts. When BigQuery ML is trained and ready to use, we can invoke it directly in BigQuery using a SQL query or we can export it into TensorFlow format.</p>
			<p>The requirements of each use case drive the prediction type that we should adopt, as outlined here:</p>
			<ul>
				<li>We use <strong class="bold">online prediction</strong> when we want to <a id="_idIndexMarker671"/>enable request-response applications and when getting an immediate prediction is critical.</li>
				<li>We adopt <strong class="bold">batch prediction</strong> to process<a id="_idIndexMarker672"/> large volumes of data when we don't need immediate predictions—for example, scheduling daily or weekly jobs that calculate predictions on the data collected since the last job execution.</li>
			</ul>
			<p>While using BigQuery<a id="_idIndexMarker673"/> SQL statements is more suitable for batch predictions on a large number of records stored in a BigQuery table, the possibility of exporting BigQuery ML models into TensorFlow opens new opportunities in terms of applications.</p>
			<p>In the following diagram, you can see the life cycle of a BigQuery ML model from the training phase to the deployment phase:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B16722_14_009.jpg" alt="Figure 14.9 – Using a BigQuery ML model for online predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.9 – Using a BigQuery ML model for online predictions</p>
			<p>From the preceding diagram, we see that in the first step <strong class="bold">(1)</strong> of the BigQuery ML model life cycle we train the ML model by leveraging the training dataset stored in BigQuery. In this first step, the ML model goes through the three main stages of the development cycle, as follows:</p>
			<ul>
				<li><strong class="bold">Train</strong>: In this stage, the BigQuery ML model learns from the training data.</li>
				<li><strong class="bold">Evaluate</strong>: In this step, we evaluate the KPIs of the model and we can tune the hyperparameters. </li>
				<li><strong class="bold">Test</strong>: In this last phase, we finally test the BigQuery ML model on the test dataset to get the predictions.</li>
			</ul>
			<p>When the BigQuery ML model <a id="_idIndexMarker674"/>satisfies our expectations in terms of performance, we can do the following:</p>
			<ul>
				<li>Use the model in BigQuery <strong class="bold">(2a)</strong> through SQL by leveraging data that is already stored in tables. This approach typically works well for batch predictions—for example, we can periodically run the BigQuery SQL statement to execute the model every day or every week by generating the new predictions.</li>
				<li>Export the model into the TensorFlow <strong class="source-inline">SavedFormat</strong> <strong class="bold">(2b)</strong>, as we described in <a href="B16722_13_Final_ASB_ePub.xhtml#_idTextAnchor184"><em class="italic">Chapter 13</em></a>, <em class="italic">Running TensorFlow Models with BigQuery ML</em>. This second approach is particularly suitable for running the ML model outside of BigQuery on other TensorFlow-compatible platforms. The same approach can be adopted by exporting the ML model into the XGBoost Booster format.</li>
			</ul>
			<p>When we've exported the BigQuery ML model, we can deploy the ML algorithm to one of the following:</p>
			<ul>
				<li><strong class="bold">Google Cloud AI Platform Prediction</strong> <strong class="bold">(3a)</strong>: With this Google Cloud module, we can deploy trained ML models in the cloud and leverage the cloud infrastructure to serve the model and generate online predictions. This cloud service automatically provisions and manages the infrastructure resources to run the ML model and can scale up according to the number of requests that come from <a id="_idIndexMarker675"/>client applications. The deployment on Google Cloud AI Platform Prediction automatically generates a <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) endpoint that can be used to invoke the <a id="_idIndexMarker676"/>model through <strong class="bold">HyperText Transfer Protocol</strong> (<strong class="bold">HTTP</strong>) requests. <p>This kind of approach is particularly useful when we have multiple client applications that should interact with our ML model and we don't want to have to worry about the infrastructure maintenance of the service. As a prerequisite, we need to consider that we can only use this kind of approach if an internet connection is available. In fact, to invoke the REST <strong class="bold">Application Programming Interface</strong> (<strong class="bold">API</strong>) exposed by the<a id="_idIndexMarker677"/> cloud platform, we need to perform HTTP requests from the client applications to the cloud.</p></li>
				<li>An <strong class="bold">on-premise machine</strong> where it can <a id="_idIndexMarker678"/>be deployed using containers <strong class="bold">(3b)</strong>. TensorFlow models, in fact, can be deployed using containers by leveraging a <strong class="bold">TensorFlow Serving Docker</strong> container. To understand the deployment<a id="_idIndexMarker679"/> steps of a TensorFlow model in a <strong class="bold">Docker</strong> container, you can check out the documentation at <a href="https://www.tensorflow.org/tfx/serving/docker">https://www.tensorflow.org/tfx/serving/docker</a>.<p class="callout-heading">Important note</p><p class="callout">A <strong class="bold">container</strong> is a virtualization <a id="_idIndexMarker680"/>mechanism that runs on top of a single operating system. <strong class="bold">Docker</strong> is a container engine used to deploy containerized applications. The container engine allocates the hardware resources for each application and manages the scalability of the virtual infrastructure.</p></li>
			</ul>
			<p>We can use this <a id="_idIndexMarker681"/>approach when the ML model needs to run under specific conditions, such as during an absence of internet connectivity, or on sensitive data. In fact, when we deploy the on-premise machine ML model, the cloud infrastructure is no longer involved in the predictions and there is no data transfer between the on-premise environment and the cloud.</p>
			<p>In both deployment scenarios, the ML model can be invoked by using HTTP requests, passing the input parameters in the request payload. At the end of the ML model execution, the predictions are returned into the response payload.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor204"/>Summary</h1>
			<p>In this chapter, we've learned the most important tips and best practices that we can apply during the implementation of a ML use case with BigQuery ML. </p>
			<p>We've analyzed the importance of data preparation; we started looking at the data quality aspects; then, we've learned how we can easily split the data to get balanced training, validation, and test sets.</p>
			<p>We then looked at how we can further improve a ML model's performance using BigQuery ML functions for feature engineering.</p>
			<p>After that, we focused our attention on tuning hyperparameters. When we train a model, BigQuery ML allows us to choose different parameters, and these variables influence the training stage.</p>
			<p>Finally, we have understood why it is so important to deploy BigQuery ML models on other platforms so that we get online predictions and satisfy near-real-time business scenarios.</p>
			<p>Congratulations on finishing reading the book! You should now be able to use BigQuery ML for your business scenarios and use cases. I suggest you continue to keep constantly informed about this topic, which is so interesting and is evolving so quickly.</p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor205"/>Further resources</h1>
			<ul>
				<li><strong class="bold">BigQuery ML Create Model</strong>: <a href="https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike">https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike</a></li>
				<li><strong class="bold">BigQuery ML preprocessing functions</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">CREATE MODEL</strong><strong class="bold"> statement for importing TensorFlow models</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">ML EVALUATE</strong><strong class="bold"> function</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate</a></li>
				<li><strong class="bold">BigQuery ML </strong><strong class="source-inline">ML PREDICT</strong><strong class="bold"> function</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict">https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict</a></li>
				<li><strong class="bold">Exporting a BigQuery ML model for online prediction</strong>: <a href="https://cloud.google.com/bigquery-ml/docs/export-model-tutorial">https://cloud.google.com/bigquery-ml/docs/export-model-tutorial</a></li>
			</ul>
		</div>
	</body></html>