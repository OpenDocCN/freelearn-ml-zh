- en: Using Decision Trees to Make a Medical Diagnosis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行医疗诊断
- en: Now that we know how to handle data in all shapes and forms, be it numerical,
    categorical, text, or image data, it is time to put our newly gained knowledge
    to good use.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何处理各种形状和形式的数据，无论是数值、分类、文本还是图像数据，现在是时候将我们新获得的知识付诸实践了。
- en: In this chapter, we will learn how to build a machine learning system that can
    make a medical diagnosis. We aren't all doctors, but we've probably all been to
    one at some point in our lives. Typically, a doctor would gain as much information
    as possible about a patient's history and symptoms to make an informed diagnosis.
    We will mimic a doctor's decision-making process with the help of what is known
    as **decision trees**. We will also cover the Gini coefficient, information gain,
    and variance reduction, along with overfitting and pruning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何构建一个可以进行医疗诊断的机器学习系统。我们可能不是医生，但我们在生活中某个时刻可能都去看过医生。通常，医生会尽可能多地获取有关患者病史和症状的信息，以便做出明智的诊断。我们将借助所谓的**决策树**来模拟医生的决策过程。我们还将涵盖基尼系数、信息增益和方差减少，以及过拟合和剪枝。
- en: A decision tree is a simple yet powerful supervised learning algorithm that
    resembles a flow chart; we will talk more about this in just a minute. Other than
    in medicine, decision trees are commonly used in fields such as astronomy (for
    example, for filtering noise from the Hubble Space Telescope images or to classify
    star-galaxy clusters), manufacturing and production (for example, by Boeing to
    discover flaws in the manufacturing process), and object recognition (for example,
    for recognizing 3D objects).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一个简单但强大的监督学习算法，类似于流程图；我们将在下一分钟内更多地讨论这一点。除了在医学领域，决策树还广泛应用于天文学（例如，从哈勃太空望远镜图像中过滤噪声或对星系团进行分类）、制造和生产（例如，波音公司用于发现制造过程中的缺陷）以及物体识别（例如，用于识别3D物体）等领域。
- en: 'Specifically, we want to learn about the following in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们特别想了解以下内容：
- en: Building simple decision trees from data and using them for either classification
    or regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据中构建简单的决策树，并使用它们进行分类或回归
- en: Deciding which decision to make next using the Gini coefficient, information
    gain, and variance reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基尼系数、信息增益和方差减少来决定下一步要做的决策
- en: Pruning a decision tree and its benefits
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝决策树及其好处
- en: But first, let's talk about what decision trees actually are.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们来谈谈决策树实际上是什么。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接中找到本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05)。
- en: 'Here is a summary of the software and hardware requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是软件和硬件要求的总结：
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 OpenCV 版本 4.1.x（4.1.0或4.1.1都可以正常工作）。
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Python 版本 3.6（任何 3.x 版本的 Python 都可以）。
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要 Anaconda Python 3 来安装 Python 和所需的模块。
- en: You can use any OS—macOS, Windows, and Linux-based OSes, along with this book.
    We recommend you have at least 4 GB RAM in your system.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows和基于Linux的操作系统，以及这本书。我们建议您的系统至少有4 GB的RAM。
- en: You don't need to have a GPU to run the code provided along with this book.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不需要GPU来运行本书提供的代码。
- en: Understanding decision trees
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: A decision tree is a simple yet powerful model for supervised learning problems.
    As the name suggests, we can think of it as a tree in which information flows
    along different branches—starting at the trunk and going all of the way to the
    individual leaves, making decisions about which branch to take at each junction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一个简单但强大的监督学习模型。正如其名所示，我们可以将其想象为一棵树，信息沿着不同的分支流动——从树干开始，一直延伸到单个叶子，在每个节点做出关于选择哪个分支的决定。
- en: 'This is basically a decision tree! Here is a simple example of a decision tree:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是一个决策树！下面是一个简单的决策树示例：
- en: '![](img/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png)'
- en: A decision tree is made of a hierarchy of questions or tests about the data
    (also known as **decision nodes**) and their possible consequences.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树由关于数据（也称为**决策节点**）及其可能后果的一系列问题或测试的层次结构组成。
- en: 'One of the true difficulties with building decision trees is how to pull out
    suitable features from the data. To make this clear, let''s use a concrete example.
    Let''s say we have a dataset consisting of a single email:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的真正困难之一是如何从数据中提取合适的特征。为了使这一点更清楚，让我们用一个具体的例子来说明。假设我们有一个由单个电子邮件组成的数据集：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This email can be vectorized in much the same way as we did in the previous
    chapter, using scikit-learn''s `CountVectorizer`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这封电子邮件可以像我们在上一章中做的那样，使用scikit-learn的`CountVectorizer`进行向量化：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the previous chapter, we know that we can have a look at the feature names
    in `X` using the following function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一章，我们知道我们可以使用以下函数查看`X`中的特征名称：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For the sake of clarity, we focus on the first five words only, which are sorted
    alphabetically. Then, the corresponding number of occurrences can be found as
    follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们只关注前五个词，这些词按字母顺序排序。然后，可以找到相应的出现次数如下：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This tells us that four out of five words show up just once in the email, but
    the word, `account` (the last one listed in `Out[3]`), actually shows up twice.
    In the last chapter, we typed `X.toarray()` to convert the sparse array, `X`, into
    a human-readable array. The result is a 2D array, where rows correspond to the
    data samples, and columns correspond to the feature names described in the preceding
    command. Since there is only one sample in the dataset, we limit ourselves to
    row 0 of the array (that is, the first data sample) and the first five columns
    in the array (that is, the first five words).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，五分之四的词在电子邮件中只出现了一次，但“account”这个词（在`Out[3]`中列出的最后一个词）实际上出现了两次。在上一章中，我们输入了`X.toarray()`来将稀疏数组`X`转换为人类可读的数组。结果是二维数组，其中行对应于数据样本，列对应于前一个命令中描述的特征名称。由于数据集中只有一个样本，我们只限于数组的第0行（即第一个数据样本）和数组的前五列（即前五个词）。
- en: So, how do we check whether the email is from a Nigerian prince?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何检查这封电子邮件是否来自尼日利亚王子？
- en: 'One way to do this is to look at whether the email contains both the words, `nigerian` and `prince`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的一种方法就是看看电子邮件中是否同时包含“nigerian”和“prince”这两个词：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: What do we find to our surprise? The word `prince` does not occur in the email.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 令我们惊讶的是，我们发现“prince”这个词在电子邮件中并没有出现。
- en: Does this mean the message is legit?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着这条消息是合法的？
- en: No, of course not. Instead of `'prince'`, the email went with the words, `head
    of state`, effectively circumventing our all-too-simple spam detector.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不，当然不是。电子邮件中没有使用“王子”，而是用了“国家元首”这样的词，有效地绕过了我们过于简单的垃圾邮件检测器。
- en: Similarly, how do we even start to model the second decision in the tree: *wants
    me to send him money?* There is no straightforward feature in the text that answers
    this question. Hence, this is a problem of feature engineering, of combining the
    words that actually occur in the message in such a way that allows us to answer
    this question. Sure, a good sign would be to look for strings such as `US$` and `money`,
    but then we still wouldn't know the context in which these words were mentioned.
    For all we know, perhaps they were part of the sentence: *Don't worry, I don't
    want you to send me any money.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们甚至不知道如何开始建模树中的第二个决策：*想要我给他汇款吗？* 文本中没有直接回答这个问题的特征。因此，这是一个特征工程问题，即以这种方式组合消息中实际出现的单词，以便我们能够回答这个问题。当然，一个好的迹象是寻找像“US$”和“money”这样的字符串，但这样我们仍然不知道这些词被提及的上下文。据我们所知，也许它们是句子的一部分：*别担心，我不想让你给我汇款。*
- en: To make matters worse, it turns out that the order in which we ask these questions
    can actually influence the final outcome. For example, what if we asked the last
    question first: *do I actually know a Nigerian prince?* Suppose we had a Nigerian
    prince for an uncle, then finding the words *Nigerian prince* in the email might
    no longer be suspicious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，结果发现我们提问的顺序实际上可以影响最终结果。例如，如果我们先问最后一个问题：*我实际上认识一个尼日利亚王子吗？* 假设我们有一个尼日利亚王子作为叔叔，那么在电子邮件中找到“尼日利亚王子”这个词可能就不再可疑了。
- en: As you can see, this seemingly simple example got quickly out of hand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个看似简单的例子很快就变得难以控制了。
- en: Luckily, the theoretical framework behind decision trees helps us to find both
    the right decision rules as well as which decisions to tackle next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，决策树背后的理论框架帮助我们找到正确的决策规则以及下一步要解决的问题。
- en: However, to understand these concepts, we have to dig a little deeper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了理解这些概念，我们必须深入挖掘。
- en: Building our first decision tree
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的第一个决策树
- en: I think we are ready for a more complex example. As promised earlier, let's
    now move into the medical domain.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我们已经准备好了一个更复杂的例子。如之前所承诺的，现在让我们进入医疗领域。
- en: 'Let''s consider an example where several patients have suffered from the same
    illness, such as a rare form of basorexia. Let''s further assume that the true
    causes of the disease remain unknown to this day and that all of the information
    that is available to us consists of a bunch of physiological measurements. For
    example, we might have access to the following information:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中几位患者患了同一种疾病，比如一种罕见的厌食症。让我们进一步假设，直到今天，这种疾病的真正原因仍然未知，而我们所能获得的所有信息都是一些生理测量值。例如，我们可能可以访问以下信息：
- en: A patient's blood pressure (`BP`)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的血压（`BP`）
- en: A patient's cholesterol level (`cholesterol`)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的胆固醇水平（`cholesterol`）
- en: A patient's gender (`sex`)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的性别（`sex`）
- en: A patient's age (`age`)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的年龄（`age`）
- en: A patient's blood sodium concentration (`Na`)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的血钠浓度（`Na`）
- en: A patient's blood potassium concentration (`K`)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者的血钾浓度（`K`）
- en: Based on all of ...
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于所有...
- en: Generating new data
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成新数据
- en: 'Before proceeding with the further steps, let''s quickly understand one very
    crucial step for every machine learning engineer—data generation. We know that
    all machine learning and deep learning techniques require a huge amount of data—in
    simple terms: the bigger, the better. But what if you don''t have enough data?
    Well, you can end up with a model that doesn''t have enough accuracy. The common
    technique employed (if you are not able to generate any new data) is to use the
    majority of the data for training. The major downside of this is that you have
    a model that is not generalized or, in other terms, suffers from overfitting.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一步之前，让我们快速了解每个机器学习工程师都非常关键的一步——数据生成。我们知道所有机器学习和深度学习技术都需要大量的数据——简单来说：越大越好。但如果你没有足够的数据呢？好吧，你可能会得到一个不够准确的模型。如果你无法生成任何新数据，常用的技术是将大部分数据用于训练。这种方法的重大缺点是，你得到的模型没有泛化能力，或者说，遭受了过拟合。
- en: One solution to deal with the preceding issue is to generate new data or, as
    it is commonly referred to, synthetic data. The key point to note here is that
    the synthetic data should have similar features to your real data. The more similar
    they are to the real data, the better it is for you as an ML engineer. This technique
    is referred to as **data augmentation**, where we use various techniques such
    as rotation and mirror images to generate new data that is based upon the existing
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解决前面问题的一个方案是生成新的数据，或者通常所说的合成数据。这里需要注意的是，合成数据应该具有与真实数据相似的特征。它们与真实数据越相似，对作为机器学习工程师的你来说就越好。这种技术被称为**数据增强**，其中我们使用各种技术，如旋转和镜像图像，来生成基于现有数据的新数据。
- en: Since we are dealing with a hypothetical case here, we can write simple Python
    code to generate random data—since there are no set features for us here. In a
    real-world case, you would use data augmentation to generate realistic-looking
    new data samples. Let's see how to approach this for our case.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里处理的是一个假设案例，我们可以编写简单的Python代码来生成随机数据——因为我们这里没有固定的特征。在现实世界的案例中，你会使用数据增强来生成看起来真实的新数据样本。让我们看看我们如何处理我们的案例。
- en: Here, the dataset is actually a list of dictionaries, where every dictionary
    constitutes a single data point that contains a patient's blood work, age, and
    gender, as well as the drug that was prescribed. So, we know that we want to create
    new dictionaries and we know the keys to use in this dictionary. The next thing
    to focus on is the data type of the values in the dictionaries.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据集实际上是一个字典列表，其中每个字典代表一个单独的数据点，包含一个患者的血液检查结果、年龄和性别，以及所开具的药物。因此，我们知道我们想要创建新的字典，也知道在这个字典中要使用的键。接下来要关注的是字典中值的类型。
- en: We start with `age`, which is an integer, then we have the gender, which is
    either `M` or `F`. Similarly, for other values, we can infer the data types and,
    in some cases, using common sense, we can even infer the range of the values to
    use.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`年龄`开始，它是一个整数，然后是性别，它可以是`M`或`F`。类似地，对于其他值，我们可以推断数据类型，在某些情况下，运用常识，我们甚至可以推断出应该使用的值的范围。
- en: It is very important to note that common sense and deep learning don't go well
    together most of the time. This is because you want your model to understand when
    something is an outlier. For example, we know that it's highly unlikely for someone
    to have an age of 130 but a generalized model should understand that this value
    is an outlier and should not be taken into account. This is why you should always
    have a small portion of data with such illogical values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是要注意，常识和深度学习大多数时候并不兼容。这是因为你希望你的模型能够理解何时某个值是异常值。例如，我们知道一个人年龄为130岁是非常不可能的，但一个通用的模型应该理解这个值是异常值，不应该被考虑在内。这就是为什么你应该始终保留一小部分具有这种不合理值的数据。
- en: 'Let''s see how we can generate some synthetic data for our case:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何为我们的案例生成一些合成数据：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can call the preceding function using `entries = generateBasorexiaData (5)` if
    we want to generate five new entries.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想生成五个新的条目，可以使用`entries = generateBasorexiaData (5)`调用前面的函数。
- en: Now that we know how to generate the data, let's have a look at what we can
    do with this data. Can we figure out the doctor's reasoning for prescribing drugs
    `A`, `B`, `C`, or `D`? Can we see a relationship between a patient's blood values
    and the drug that the doctor prescribed?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何生成数据，让我们看看我们可以用这些数据做什么。我们能弄清楚医生为什么开药`A`、`B`、`C`或`D`吗？我们能看到患者的血液值和医生所开药物之间的关系吗？
- en: Chances are, it is as difficult a question to answer for you as it is for me.
    Although the dataset might look random at first glance, I have, in fact, put in
    some clear relationships between a patient's blood values and the prescribed drug.
    Let's see whether a decision tree can uncover these hidden relationships.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，这个问题对你来说和对我来说一样难以回答。尽管数据集乍一看可能看起来是随机的，但实际上，我已经在患者血液值和所开药物之间建立了一些明确的关系。让我们看看决策树能否揭示这些隐藏的关系。
- en: Understanding the task by understanding the data
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过理解数据来理解任务
- en: What is always the first step in tackling a new machine learning problem?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解决一个新的机器学习问题时的第一步总是什么？
- en: 'You are absolutely right: getting a sense of the data. The better we understand
    the data, the better we understand the problem we are trying to solve. In our
    future endeavors, this will also help us to choose an appropriate machine learning
    algorithm.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你说得完全正确：对数据的感知。我们越了解数据，就越能理解我们试图解决的问题。在我们的未来努力中，这也有助于我们选择合适的机器学习算法。
- en: The first thing to realize is that the `drug` column is actually not a feature
    value like all of the other columns. Since it is our goal to predict which drug
    will be prescribed based on a patient's blood values, the `drug` column effectively becomes the target
    label. In other words, the inputs to our machine learning algorithm will be the
    blood values, age, and gender of a ...
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要意识到的是，`drug`列实际上并不是像其他列那样的特征值。由于我们的目标是根据患者的血液值预测将被开处的药物，`drug`列实际上变成了目标标签。换句话说，我们机器学习算法的输入将是血液值、年龄和性别
    ...
- en: Preprocessing the data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: For our data to be understood by the decision tree algorithm, we need to convert
    all categorical features (`sex`, `BP`, and `cholesterol`) into numerical features.
    What is the best way to do that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的数据能够被决策树算法理解，我们需要将所有分类特征（`sex`、`BP`和`cholesterol`）转换为数值特征。最好的方法是什么？
- en: 'Exactly: we use scikit-learn''s `DictVectorizer`. Like we did in the previous
    chapter, we feed the dataset that we want to convert to the `fit_transform` method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正确：我们使用scikit-learn的`DictVectorizer`。就像我们在上一章中做的那样，我们将要转换的数据集输入到`fit_transform`方法中：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, `data_pre` contains the preprocessed data. If we want to look at the
    first data point (that is, the first row of `data_pre`), we match the feature
    names with the corresponding feature values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`data_pre`包含预处理后的数据。如果我们想查看第一个数据点（即`data_pre`的第一行），我们将特征名称与相应的特征值进行匹配：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From this, we can see that the three categorical variables—blood pressure (`BP`),
    cholesterol level (`cholesterol`), and gender (`sex`)—have been encoded using
    one-hot coding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以看到三个分类变量——血压(`BP`)、胆固醇水平(`cholesterol`)和性别(`sex`)——已经使用独热编码进行编码。
- en: 'To make sure that our data variables are compatible with OpenCV, we need to
    convert everything into floating point values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的数据变量与OpenCV兼容，我们需要将所有内容转换为浮点值：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, all that''s left to do is to split the data into training and tests sets,
    like we did in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*. Remember that we always want to keep the training
    and test sets separate. Since we only have 20 data points to work with in this
    example, we should probably reserve more than 10 percent of the data for testing.
    A 15-5 split seems appropriate here. We can be explicit and order the `split` function
    to yield exactly five test samples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，剩下的工作就是像我们在[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)中做的那样，将数据分为训练集和测试集。记住，我们总是希望保持训练集和测试集的分离。由于在这个例子中我们只有20个数据点可以工作，我们可能需要为测试保留超过10%的数据。在这里，15-5的分割似乎很合适。我们可以明确地命令`split`函数产生恰好五个测试样本：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Constructing the tree
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建树形图
- en: 'Building the decision tree with OpenCV works in much the same way as in [Chapter
    3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First Steps in Supervised Learning*.
    Recall that all of the machine learning functions reside in OpenCV 3.1''s `ml` module:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenCV构建决策树与[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)中提到的监督学习入门步骤非常相似。回想一下，所有的机器学习函数都位于OpenCV
    3.1的`ml`模块中：
- en: 'We can create an empty decision tree using the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码创建一个空的决策树：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To train the decision tree on the training data, we use the `train` method. That''s
    why we converted the data to float before—so that we could use it in the `train`
    method:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在训练数据上训练决策树，我们使用`train`方法。这就是为什么我们之前将数据转换为浮点数的原因——这样我们就可以在`train`方法中使用它：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we have to specify whether the data samples in `X_train` occupy the rows
    (using `cv2.ml.ROW_SAMPLE`) or the columns (`cv2.ml.COL_SAMPLE`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须指定`X_train`中的数据样本是占据行（使用`cv2.ml.ROW_SAMPLE`）还是列（`cv2.ml.COL_SAMPLE`）。
- en: Then, we can predict the labels of the ...
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以预测...的标签。
- en: Visualizing a trained decision tree
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化训练好的决策树
- en: OpenCV's implementation of decision trees is good enough if you are just starting
    out and don't care too much about what's going on under the hood. However, in
    the following sections, we will switch to scikit-learn. Its implementation allows
    us to customize the algorithm and makes it a lot easier to investigate the inner
    workings of the tree. Its usage is also much better documented.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始使用OpenCV的决策树实现，并且不太关心底层的工作原理，那么它的实现已经足够好了。然而，在接下来的几节中，我们将转向scikit-learn。它的实现允许我们自定义算法，使得调查树的内部工作原理变得容易得多。它的文档也更好。
- en: 'In scikit-learn, decision trees can be used for both classification and regression.
    They reside in the `tree` module:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，决策树可以用于分类和回归。它们位于`tree`模块中：
- en: 'Let''s first import the `tree` module from `sklearn`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先从`sklearn`导入`tree`模块：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similar to OpenCV, we then create an empty decision tree using the `DecisionTreeClassifier` constructor:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与OpenCV类似，我们接下来使用`DecisionTreeClassifier`构造函数创建一个空的决策树：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The tree can then be trained using the `fit` method:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，可以使用`fit`方法训练树：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then compute the accuracy score on both the training and the test sets
    using the `score` method:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`score`方法在训练集和测试集上计算准确度得分：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, here''s the cool thing: if you want to know what the tree looks like,
    you can do so using GraphViz to create a PDF file (or any other supported file
    type) from the tree structure. For this to work, you need to install GraphViz
    first. Worry not as it is already present in the environment we created at the
    beginning of this book.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有个有趣的事情：如果你想了解树的外观，你可以使用GraphViz从树结构创建一个PDF文件（或任何其他支持的文件类型）。为了使这成为可能，你首先需要安装GraphViz。不用担心，因为它已经包含在我们这本书开头创建的环境中。
- en: 'Then, back in Python, you can export the tree in GraphViz format to a file, `tree.dot`, using
    scikit-learn''s `export_graphviz` exporter:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，回到Python中，你可以使用scikit-learn的`export_graphviz`导出器将树以GraphViz格式导出到一个文件，例如`tree.dot`：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, back on the command line, you can use GraphViz to turn `tree.dot` into
    (for example) a PNG file:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，回到命令行，你可以使用GraphViz将`tree.dot`转换为（例如）一个PNG文件：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Alternatively, you can also specify `-Tpdf` or any other supported image format.
    The result for the preceding tree looks like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以指定`-Tpdf`或任何其他支持的图像格式。前一个树的结果看起来像这样：
- en: '![](img/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png)'
- en: What does this all mean? Let's break the diagram down step by step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切意味着什么？让我们一步一步地分解这个图表。
- en: Investigating the inner workings of a decision tree
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探究决策树的内部工作原理
- en: We established earlier that a decision tree is basically a flow chart that makes
    a series of decisions about the data. The process starts at the root node (which
    is the node at the very top), where we split the data into two groups (only for
    binary trees), based on some decision rule. Then, the process is repeated until
    all remaining samples have the same target label, at which point we have reached
    a leaf node.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经建立了一个决策树基本上是一个做出一系列数据决策的流程图。这个过程从根节点开始（这是最顶端的节点），根据某些决策规则将数据分成两组（仅适用于二叉树），然后重复这个过程，直到所有剩余的样本都具有相同的目标标签，此时我们就到达了一个叶节点。
- en: In the spam filter example earlier, decisions were made by asking true/false
    questions. For example, we asked whether an email contained a certain word. If
    it did, we followed the edge labeled true and asked the next question. However,
    this works not just for categorical features, ...
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的垃圾邮件过滤器示例中，决策是通过提出真/假问题来做出的。例如，我们询问一封电子邮件是否包含某个特定的单词。如果包含，我们就沿着标记为 `true`
    的边前进，并提出下一个问题。然而，这不仅仅适用于分类特征，...
- en: Rating the importance of features
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估特征的重要性
- en: What I haven't told you yet is how you pick the features along which to split
    the data. The preceding root node split the data according to *Na <= 0.72,* but
    who told the tree to focus on sodium first? Also, where does the number 0.72 come
    from anyway?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我还没有告诉你的事情是，你如何选择用于分割数据的特征。前面的根节点根据 *Na <= 0.72* 来分割数据，但谁告诉这棵树首先关注钠呢？此外，0.72
    这个数字又是从哪里来的呢？
- en: 'Apparently, some features might be more important than others. In fact, scikit-learn
    provides a function to rate feature importance, which is a number between *0*
    and *1* for each feature, where *0* means *not used at all in any decisions made* and
    *1* means *perfectly predicts the target*. The feature importances are normalized
    so that they all sum to 1:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，某些特征可能比其他特征更重要。实际上，scikit-learn 提供了一个评估特征重要性的函数，该函数为每个特征提供一个介于 *0* 和 *1*
    之间的数值，其中 *0* 表示在所有决策中**完全没有使用**，而 *1* 表示**完美预测目标**。特征重要性被归一化，以便它们的总和为 1：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we remind ourselves of the feature names, it will become clear which feature
    seems to be the most important. A plot might be most informative:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回想一下特征名称，就会变得清楚哪个特征似乎是最重要的。一个图表可能最有信息量：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will result in the following bar plot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下条形图：
- en: '![](img/448d58c4-1960-44f1-847b-e1e2552d81d9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/448d58c4-1960-44f1-847b-e1e2552d81d9.png)'
- en: Now, it becomes evident that the most important feature for knowing which drug
    to administer to patients was actually whether the patient had a normal cholesterol
    level. Age, sodium levels, and potassium levels were also important. On the other
    hand, gender and blood pressure did not seem to make any difference at all. However,
    this does not mean that gender or blood pressure are uninformative. It only means
    that these features were not picked by the decision tree, likely because another
    feature would have led to the same splits.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显，最重要的特征是知道给患者服用哪种药物，实际上是否患者有正常的胆固醇水平。年龄、钠水平和钾水平也很重要。另一方面，性别和血压似乎没有任何区别。然而，这并不意味着性别或血压是无信息的。这只意味着这些特征没有被决策树选中，可能是因为另一个特征会导致相同的分割。
- en: But, hold on. If cholesterol level is so important, why was it not picked as
    the first feature in the tree (that is, in the root node)? Why would you choose
    to split on the sodium level first? This is where I need to tell you about that
    ominous `gini` label in the diagram earlier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，等等。如果胆固醇水平如此重要，为什么它没有被选为树中的第一个特征（即根节点）？为什么你会选择首先根据钠水平进行分割？这就是我需要告诉你之前图中那个不祥的
    `gini` 标签的地方。
- en: Feature importances tell us which features are important for classification,
    but not which class label they are indicative of. For example, we only know that
    the cholesterol level is important, but we don't know how that led to different
    drugs being prescribed. In fact, there might not be a simple relationship between
    features and classes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性告诉我们哪些特征对分类很重要，但并不告诉我们它们指示的是哪个类标签。例如，我们只知道胆固醇水平很重要，但我们不知道它是如何导致开处不同药物的。实际上，特征和类别之间可能没有简单的关联。
- en: Understanding the decision rules
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策规则
- en: 'To build the perfect tree, you would want to split the tree at the most informative
    feature, resulting in the purest daughter nodes. However, this simple idea leads
    to some practical challenges:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建完美的树，你希望在最有信息量的特征处分割树，从而得到最纯净的女儿节点。然而，这个简单的想法带来了一些实际挑战：
- en: It's not actually clear what most informative means. We need a concrete value,
    a score function, or a mathematical equation that can describe how informative
    a feature is.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上并不清楚什么是最具信息量的。我们需要一个具体的值，一个评分函数，或者一个可以描述特征信息量的数学方程。
- en: To find the best split, we have to search over all of the possibilities at every
    decision node.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了找到最佳的分割点，我们必须在每个决策节点搜索所有可能性。
- en: 'Fortunately, the decision tree algorithm actually does these two steps for
    you. Two of the most commonly used criteria that scikit-learn supports are the
    following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，决策树算法实际上为你做了这两步。scikit-learn支持的两个最常用的标准如下：
- en: '`criterion=''gini''`: The Gini impurity is a measure of misclassification,
    with the aim ...'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion=''gini''`：基尼不纯度是误分类的度量，目的是... '
- en: Controlling the complexity of decision trees
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制决策树的复杂性
- en: If you continue to grow a tree until all leaves are pure, you will typically arrive
    at a tree that is too complex to interpret. The presence of pure leaves means
    that the tree is 100 percent correct on the training data, as was the case with
    our tree shown earlier. As a result, the tree is likely to perform very poorly
    on the test dataset, as was the case with our tree shown earlier. We say the tree overfits the
    training data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续生长树直到所有叶子都是纯净的，你通常会得到一个过于复杂的树，难以解释。纯净叶子的存在意味着树在训练数据上100%正确，就像我们之前展示的树一样。因此，该树在测试数据集上的表现可能非常差，就像我们之前展示的树一样。我们说树对训练数据过拟合了。
- en: 'There are two common ways to avoid overfitting:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合有两种常见的方法：
- en: '**Pre-pruning**: This is the process of stopping the creation of the tree early.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预剪枝**：这是在早期停止树创建的过程。'
- en: '**Post-pruning** **(or just pruning)**: This is the process of first building the
    tree but then removing or collapsing nodes that contain only a little information.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后剪枝**（或简称**剪枝**）：这是首先构建树，然后移除或合并只包含少量信息的节点的过程。'
- en: 'There are several ways to pre-prune a tree, all of which can be achieved by
    passing optional arguments to the `DecisionTreeClassifier` constructor:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以预剪枝一棵树，所有这些都可以通过传递可选参数给`DecisionTreeClassifier`构造函数来实现：
- en: Limiting the maximum depth of the tree via the `max_depth` parameter
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`max_depth`参数限制树的最大深度
- en: Limiting the maximum number of leaf nodes via `max_leaf_nodes`
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`max_leaf_nodes`限制最大叶节点数
- en: Requiring a minimum number of points in a node to keep splitting it via `min_samples_split`
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`min_samples_split`要求节点中至少有足够多的点以继续分割
- en: Often pre-pruning is sufficient to control overfitting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常预剪枝足以控制过拟合。
- en: Try it out on our toy dataset! Can you get the score on the test set to improve
    at all? How does the tree layout change as you start playing with the earlier
    parameters?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的玩具数据集上试一试！你能否提高测试集上的分数？当你开始调整早期参数时，树布局是如何变化的？
- en: In more complicated real-world scenarios, pre-pruning is no longer sufficient
    to control overfitting. In such cases, we want to combine multiple decision trees
    into what is known as a **random forest**. We will talk about this in [Chapter
    10](10a94a5b-e700-4fae-b9f6-f6b77a1580f6.xhtml), *Ensemble Methods for Classification*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的现实世界场景中，预剪枝不再足够用来控制过拟合。在这种情况下，我们希望将多个决策树组合成所谓的**随机森林**。我们将在第10章[集成分类方法](10a94a5b-e700-4fae-b9f6-f6b77a1580f6.xhtml)中讨论这一点。
- en: Using decision trees to diagnose breast cancer
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树诊断乳腺癌
- en: 'Now that we have built our first decision tree, it''s time to turn our attention to
    a real dataset: the Breast Cancer Wisconsin dataset ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了我们的第一个决策树，是时候将我们的注意力转向一个真实的数据集：威斯康星乳腺癌数据集 ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)))。
- en: 'This dataset is a direct result of medical imaging research and is considered
    a classic today. The dataset was created from digitized images of healthy (benign)
    and cancerous (malignant) tissues. Unfortunately, I wasn''t able to find any public-domain
    examples from the original study, but the images look similar to the following
    screenshot:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是医学影像研究的一个直接结果，并且现在被认为是经典的。这个数据集是从健康（良性）和癌变（恶性）组织的数字化图像中创建的。不幸的是，我无法找到原始研究中任何公有领域的示例，但图像看起来与以下截图相似：
- en: '![](img/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png)'
- en: The goal of the research was to classify tissue ...
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的目的是对组织进行分类...
- en: Loading the dataset
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'The full dataset is part of scikit-learn''s example datasets. We can import
    it using the following commands:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集是scikit-learn示例数据集的一部分。我们可以使用以下命令导入它：
- en: 'First, let''s load the dataset using the `load_breast_cancer` function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用`load_breast_cancer`函数加载数据集：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As in the previous examples, all data is contained in a 2D feature matrix, `data.data`,
    where the rows represent data samples and the columns are the feature values:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，所有数据都包含在一个2D特征矩阵`data.data`中，其中行表示数据样本，列是特征值：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With a look at the provided feature names, we recognize some that we mentioned
    earlier:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看提供的特征名称，我们认出了一些我们之前提到过的：
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since this is a binary classification task, we expect to find exactly two target
    names:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个二元分类任务，我们预计会找到恰好两个目标名称：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s reserve some 20 percent of all data samples for testing:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们保留所有数据样本的20%用于测试：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You could certainly choose a different ratio, but most commonly people use
    something like 70-30, 80-20, or 90-10\. It all depends a bit on the dataset size
    but, in the end, should not make too much of a difference. Splitting the data
    80-20 should result in the following set sizes:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你当然可以选择不同的比例，但最常见的是使用70-30、80-20或90-10。这取决于数据集的大小，但最终不应有太大的差异。将数据分割为80-20应产生以下集合大小：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Building the decision tree
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建决策树
- en: 'As shown earlier, we can create a decision tree using scikit-learn''s `tree`
    module. For now, let''s not specify any optional arguments:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用scikit-learn的`tree`模块创建一个决策树。现在，让我们不要指定任何可选参数：
- en: 'We will start off by creating a decision tree:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建一个决策树：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Do you remember how to train the decision tree? We will use the `fit` function
    for that:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还记得如何训练决策树吗？我们将使用`fit`函数来完成：
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Since we did not specify any pre-pruning parameters, we would expect this ...
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们没有指定任何预剪枝参数，我们预计这...
- en: Using decision trees for regression
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行回归
- en: 'Although we have so far focused on using decision trees in classification tasks,
    you can also use them for regression. But you will need to use scikit-learn again,
    as OpenCV does not provide this flexibility. We will therefore only briefly review
    its functionality here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们迄今为止一直专注于在分类任务中使用决策树，但你也可以将其用于回归。但你需要再次使用scikit-learn，因为OpenCV不提供这种灵活性。因此，我们在这里将只简要回顾其功能：
- en: 'Let''s say we wanted to use a decision tree to fit a sin wave. To make things
    interesting, we will also add some noise to the data points using NumPy''s random
    number generator:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想使用决策树来拟合一个sin波。为了使事情更有趣，我们还将使用NumPy的随机数生成器对数据点添加一些噪声：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then create 100 randomly spaced *x* values between 0 and 5 and calculate
    the corresponding sin values:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建100个在0到5之间的随机间隔的*x*值，并计算相应的sin值：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then add noise to every other data point in `y` (using `y[::2]`), scaled
    by `0.5` so we don''t introduce too much jitter:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在`y`中的每个其他数据点添加噪声（使用`y[::2]`），并按`0.5`的比例缩放，这样我们不会引入太多的抖动：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You can then create a regression tree like any other tree before.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以创建一个像其他树一样的回归树。
- en: 'A small difference is that the `gini` and `entropy` split criteria do not apply
    to regression tasks. Instead, scikit-learn provides two different split criteria:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小的不同之处在于，`gini`和`entropy`分割标准不适用于回归任务。相反，scikit-learn提供了两种不同的分割标准：
- en: '**mse (also known as variance reduction)**: This criterion calculates the **Mean
    Squared Error** (**MSE**) between ground truth and prediction and splits the node
    that leads to the smallest MSE.'
  id: totrans-174
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mse（也称为方差减少）**：此标准计算真实值和预测值之间的**平均平方误差**（**MSE**），并分割导致最小MSE的节点。'
- en: '**mae**: This criterion calculates the **Mean Absolute Error** (**MAE**) between ground truth
    and prediction and splits the node that leads to the smallest MAE.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mae**：此标准计算真实值和预测值之间的**平均绝对误差**（**MAE**），并分割导致最小MAE的节点。'
- en: 'Using the MSE criterion, we will build two trees. Let''s first build a tree
    with depth 2:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MSE标准，我们将构建两个树。让我们首先构建一个深度为2的树：
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will build a decision tree with a maximum depth of 5:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个最大深度为5的决策树：
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can then use the decision tree like a linear regressor from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)中的线性回归器一样使用决策树。
- en: 'For this, we create a test set with *x* values densely sampled in the whole
    range from 0 through 5:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了这个目的，我们创建了一个测试集，其中*x*值在整个0到5的范围内密集采样：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The predicted *y* values can then be obtained with the `predict` method:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`predict`方法获得预测的*y*值：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we plot all of these together, we can see how the decision trees differ:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将所有这些放在一起绘制，我们可以看到决策树之间的差异：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will produce the following plot:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![](img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)'
- en: Here, the thick red line represents the regression tree with depth 2\. You can
    see how the tree tries to approximate the data using these crude steps. The thinner
    blue line belongs to the regression tree with depth 5; the added depth has allowed
    the tree to make many finer-grained approximations. Therefore, this tree can approximate
    the data even better. However, because of this added power, the tree is also more
    susceptible to fitting noisy values, as can be seen especially from the spikes
    on the right-hand side of the plot.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，粗红色的线代表深度为2的回归树。你可以看到树是如何通过这些粗略的步骤来近似数据的。较细的蓝色线属于深度为5的回归树；增加的深度使得树能够进行更多更精细的近似。因此，这棵树可以更好地近似数据。然而，由于这种增加的力量，树也更易受到拟合噪声值的影响，尤其是在图表右侧的尖峰中可以明显看出。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned all about decision trees and how to apply them to
    both classification and regression tasks. We talked a bit about data generation,
    overfitting, and ways to avoid this phenomenon by tweaking pre-pruning and post-pruning
    settings. We also learned about how to rate the quality of a node split using
    metrics such as the Gini impurity and information gain. Finally, we applied decision
    trees to medical data to detect cancerous tissues. We will come back to decision
    trees towards the end of this book when we will combine multiple trees into what
    is known as a random forest. But for now, let's move on to a new topic.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了所有关于决策树的知识以及如何将它们应用于分类和回归任务。我们简要讨论了数据生成、过拟合以及通过调整预剪枝和后剪枝设置来避免这种现象的方法。我们还学习了如何使用基尼不纯度和信息增益等指标来评估节点分裂的质量。最后，我们将决策树应用于医学数据以检测癌组织。我们将在本书的末尾回到决策树，届时我们将多个树组合成所谓的随机森林。但现在，让我们继续探讨新的主题。
- en: 'In the next chapter, we will introduce another staple of the machine learning
    world: support vector ...'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍机器学习世界中的另一个基本概念：支持向量...
