- en: Using Decision Trees to Make a Medical Diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to handle data in all shapes and forms, be it numerical,
    categorical, text, or image data, it is time to put our newly gained knowledge
    to good use.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to build a machine learning system that can
    make a medical diagnosis. We aren't all doctors, but we've probably all been to
    one at some point in our lives. Typically, a doctor would gain as much information
    as possible about a patient's history and symptoms to make an informed diagnosis.
    We will mimic a doctor's decision-making process with the help of what is known
    as **decision trees**. We will also cover the Gini coefficient, information gain,
    and variance reduction, along with overfitting and pruning.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is a simple yet powerful supervised learning algorithm that
    resembles a flow chart; we will talk more about this in just a minute. Other than
    in medicine, decision trees are commonly used in fields such as astronomy (for
    example, for filtering noise from the Hubble Space Telescope images or to classify
    star-galaxy clusters), manufacturing and production (for example, by Boeing to
    discover flaws in the manufacturing process), and object recognition (for example,
    for recognizing 3D objects).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we want to learn about the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building simple decision trees from data and using them for either classification
    or regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding which decision to make next using the Gini coefficient, information
    gain, and variance reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning a decision tree and its benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But first, let's talk about what decision trees actually are.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any OS—macOS, Windows, and Linux-based OSes, along with this book.
    We recommend you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a simple yet powerful model for supervised learning problems.
    As the name suggests, we can think of it as a tree in which information flows
    along different branches—starting at the trunk and going all of the way to the
    individual leaves, making decisions about which branch to take at each junction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is basically a decision tree! Here is a simple example of a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree is made of a hierarchy of questions or tests about the data
    (also known as **decision nodes**) and their possible consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the true difficulties with building decision trees is how to pull out
    suitable features from the data. To make this clear, let''s use a concrete example.
    Let''s say we have a dataset consisting of a single email:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This email can be vectorized in much the same way as we did in the previous
    chapter, using scikit-learn''s `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the previous chapter, we know that we can have a look at the feature names
    in `X` using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of clarity, we focus on the first five words only, which are sorted
    alphabetically. Then, the corresponding number of occurrences can be found as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that four out of five words show up just once in the email, but
    the word, `account` (the last one listed in `Out[3]`), actually shows up twice.
    In the last chapter, we typed `X.toarray()` to convert the sparse array, `X`, into
    a human-readable array. The result is a 2D array, where rows correspond to the
    data samples, and columns correspond to the feature names described in the preceding
    command. Since there is only one sample in the dataset, we limit ourselves to
    row 0 of the array (that is, the first data sample) and the first five columns
    in the array (that is, the first five words).
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we check whether the email is from a Nigerian prince?
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to do this is to look at whether the email contains both the words, `nigerian` and `prince`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What do we find to our surprise? The word `prince` does not occur in the email.
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean the message is legit?
  prefs: []
  type: TYPE_NORMAL
- en: No, of course not. Instead of `'prince'`, the email went with the words, `head
    of state`, effectively circumventing our all-too-simple spam detector.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, how do we even start to model the second decision in the tree: *wants
    me to send him money?* There is no straightforward feature in the text that answers
    this question. Hence, this is a problem of feature engineering, of combining the
    words that actually occur in the message in such a way that allows us to answer
    this question. Sure, a good sign would be to look for strings such as `US$` and `money`,
    but then we still wouldn't know the context in which these words were mentioned.
    For all we know, perhaps they were part of the sentence: *Don't worry, I don't
    want you to send me any money.*
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, it turns out that the order in which we ask these questions
    can actually influence the final outcome. For example, what if we asked the last
    question first: *do I actually know a Nigerian prince?* Suppose we had a Nigerian
    prince for an uncle, then finding the words *Nigerian prince* in the email might
    no longer be suspicious.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this seemingly simple example got quickly out of hand.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the theoretical framework behind decision trees helps us to find both
    the right decision rules as well as which decisions to tackle next.
  prefs: []
  type: TYPE_NORMAL
- en: However, to understand these concepts, we have to dig a little deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Building our first decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think we are ready for a more complex example. As promised earlier, let's
    now move into the medical domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example where several patients have suffered from the same
    illness, such as a rare form of basorexia. Let''s further assume that the true
    causes of the disease remain unknown to this day and that all of the information
    that is available to us consists of a bunch of physiological measurements. For
    example, we might have access to the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: A patient's blood pressure (`BP`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's cholesterol level (`cholesterol`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's gender (`sex`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's age (`age`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's blood sodium concentration (`Na`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's blood potassium concentration (`K`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on all of ...
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before proceeding with the further steps, let''s quickly understand one very
    crucial step for every machine learning engineer—data generation. We know that
    all machine learning and deep learning techniques require a huge amount of data—in
    simple terms: the bigger, the better. But what if you don''t have enough data?
    Well, you can end up with a model that doesn''t have enough accuracy. The common
    technique employed (if you are not able to generate any new data) is to use the
    majority of the data for training. The major downside of this is that you have
    a model that is not generalized or, in other terms, suffers from overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: One solution to deal with the preceding issue is to generate new data or, as
    it is commonly referred to, synthetic data. The key point to note here is that
    the synthetic data should have similar features to your real data. The more similar
    they are to the real data, the better it is for you as an ML engineer. This technique
    is referred to as **data augmentation**, where we use various techniques such
    as rotation and mirror images to generate new data that is based upon the existing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are dealing with a hypothetical case here, we can write simple Python
    code to generate random data—since there are no set features for us here. In a
    real-world case, you would use data augmentation to generate realistic-looking
    new data samples. Let's see how to approach this for our case.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the dataset is actually a list of dictionaries, where every dictionary
    constitutes a single data point that contains a patient's blood work, age, and
    gender, as well as the drug that was prescribed. So, we know that we want to create
    new dictionaries and we know the keys to use in this dictionary. The next thing
    to focus on is the data type of the values in the dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: We start with `age`, which is an integer, then we have the gender, which is
    either `M` or `F`. Similarly, for other values, we can infer the data types and,
    in some cases, using common sense, we can even infer the range of the values to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to note that common sense and deep learning don't go well
    together most of the time. This is because you want your model to understand when
    something is an outlier. For example, we know that it's highly unlikely for someone
    to have an age of 130 but a generalized model should understand that this value
    is an outlier and should not be taken into account. This is why you should always
    have a small portion of data with such illogical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can generate some synthetic data for our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can call the preceding function using `entries = generateBasorexiaData (5)` if
    we want to generate five new entries.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to generate the data, let's have a look at what we can
    do with this data. Can we figure out the doctor's reasoning for prescribing drugs
    `A`, `B`, `C`, or `D`? Can we see a relationship between a patient's blood values
    and the drug that the doctor prescribed?
  prefs: []
  type: TYPE_NORMAL
- en: Chances are, it is as difficult a question to answer for you as it is for me.
    Although the dataset might look random at first glance, I have, in fact, put in
    some clear relationships between a patient's blood values and the prescribed drug.
    Let's see whether a decision tree can uncover these hidden relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the task by understanding the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is always the first step in tackling a new machine learning problem?
  prefs: []
  type: TYPE_NORMAL
- en: 'You are absolutely right: getting a sense of the data. The better we understand
    the data, the better we understand the problem we are trying to solve. In our
    future endeavors, this will also help us to choose an appropriate machine learning
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to realize is that the `drug` column is actually not a feature
    value like all of the other columns. Since it is our goal to predict which drug
    will be prescribed based on a patient's blood values, the `drug` column effectively becomes the target
    label. In other words, the inputs to our machine learning algorithm will be the
    blood values, age, and gender of a ...
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our data to be understood by the decision tree algorithm, we need to convert
    all categorical features (`sex`, `BP`, and `cholesterol`) into numerical features.
    What is the best way to do that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Exactly: we use scikit-learn''s `DictVectorizer`. Like we did in the previous
    chapter, we feed the dataset that we want to convert to the `fit_transform` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `data_pre` contains the preprocessed data. If we want to look at the
    first data point (that is, the first row of `data_pre`), we match the feature
    names with the corresponding feature values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From this, we can see that the three categorical variables—blood pressure (`BP`),
    cholesterol level (`cholesterol`), and gender (`sex`)—have been encoded using
    one-hot coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure that our data variables are compatible with OpenCV, we need to
    convert everything into floating point values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, all that''s left to do is to split the data into training and tests sets,
    like we did in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*. Remember that we always want to keep the training
    and test sets separate. Since we only have 20 data points to work with in this
    example, we should probably reserve more than 10 percent of the data for testing.
    A 15-5 split seems appropriate here. We can be explicit and order the `split` function
    to yield exactly five test samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Constructing the tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building the decision tree with OpenCV works in much the same way as in [Chapter
    3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First Steps in Supervised Learning*.
    Recall that all of the machine learning functions reside in OpenCV 3.1''s `ml` module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create an empty decision tree using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the decision tree on the training data, we use the `train` method. That''s
    why we converted the data to float before—so that we could use it in the `train`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have to specify whether the data samples in `X_train` occupy the rows
    (using `cv2.ml.ROW_SAMPLE`) or the columns (`cv2.ml.COL_SAMPLE`).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can predict the labels of the ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing a trained decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV's implementation of decision trees is good enough if you are just starting
    out and don't care too much about what's going on under the hood. However, in
    the following sections, we will switch to scikit-learn. Its implementation allows
    us to customize the algorithm and makes it a lot easier to investigate the inner
    workings of the tree. Its usage is also much better documented.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, decision trees can be used for both classification and regression.
    They reside in the `tree` module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the `tree` module from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to OpenCV, we then create an empty decision tree using the `DecisionTreeClassifier` constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree can then be trained using the `fit` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then compute the accuracy score on both the training and the test sets
    using the `score` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, here''s the cool thing: if you want to know what the tree looks like,
    you can do so using GraphViz to create a PDF file (or any other supported file
    type) from the tree structure. For this to work, you need to install GraphViz
    first. Worry not as it is already present in the environment we created at the
    beginning of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, back in Python, you can export the tree in GraphViz format to a file, `tree.dot`, using
    scikit-learn''s `export_graphviz` exporter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, back on the command line, you can use GraphViz to turn `tree.dot` into
    (for example) a PNG file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can also specify `-Tpdf` or any other supported image format.
    The result for the preceding tree looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png)'
  prefs: []
  type: TYPE_IMG
- en: What does this all mean? Let's break the diagram down step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the inner workings of a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We established earlier that a decision tree is basically a flow chart that makes
    a series of decisions about the data. The process starts at the root node (which
    is the node at the very top), where we split the data into two groups (only for
    binary trees), based on some decision rule. Then, the process is repeated until
    all remaining samples have the same target label, at which point we have reached
    a leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: In the spam filter example earlier, decisions were made by asking true/false
    questions. For example, we asked whether an email contained a certain word. If
    it did, we followed the edge labeled true and asked the next question. However,
    this works not just for categorical features, ...
  prefs: []
  type: TYPE_NORMAL
- en: Rating the importance of features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What I haven't told you yet is how you pick the features along which to split
    the data. The preceding root node split the data according to *Na <= 0.72,* but
    who told the tree to focus on sodium first? Also, where does the number 0.72 come
    from anyway?
  prefs: []
  type: TYPE_NORMAL
- en: 'Apparently, some features might be more important than others. In fact, scikit-learn
    provides a function to rate feature importance, which is a number between *0*
    and *1* for each feature, where *0* means *not used at all in any decisions made* and
    *1* means *perfectly predicts the target*. The feature importances are normalized
    so that they all sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we remind ourselves of the feature names, it will become clear which feature
    seems to be the most important. A plot might be most informative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/448d58c4-1960-44f1-847b-e1e2552d81d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, it becomes evident that the most important feature for knowing which drug
    to administer to patients was actually whether the patient had a normal cholesterol
    level. Age, sodium levels, and potassium levels were also important. On the other
    hand, gender and blood pressure did not seem to make any difference at all. However,
    this does not mean that gender or blood pressure are uninformative. It only means
    that these features were not picked by the decision tree, likely because another
    feature would have led to the same splits.
  prefs: []
  type: TYPE_NORMAL
- en: But, hold on. If cholesterol level is so important, why was it not picked as
    the first feature in the tree (that is, in the root node)? Why would you choose
    to split on the sodium level first? This is where I need to tell you about that
    ominous `gini` label in the diagram earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances tell us which features are important for classification,
    but not which class label they are indicative of. For example, we only know that
    the cholesterol level is important, but we don't know how that led to different
    drugs being prescribed. In fact, there might not be a simple relationship between
    features and classes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the decision rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the perfect tree, you would want to split the tree at the most informative
    feature, resulting in the purest daughter nodes. However, this simple idea leads
    to some practical challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: It's not actually clear what most informative means. We need a concrete value,
    a score function, or a mathematical equation that can describe how informative
    a feature is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To find the best split, we have to search over all of the possibilities at every
    decision node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fortunately, the decision tree algorithm actually does these two steps for
    you. Two of the most commonly used criteria that scikit-learn supports are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''gini''`: The Gini impurity is a measure of misclassification,
    with the aim ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the complexity of decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you continue to grow a tree until all leaves are pure, you will typically arrive
    at a tree that is too complex to interpret. The presence of pure leaves means
    that the tree is 100 percent correct on the training data, as was the case with
    our tree shown earlier. As a result, the tree is likely to perform very poorly
    on the test dataset, as was the case with our tree shown earlier. We say the tree overfits the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to avoid overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-pruning**: This is the process of stopping the creation of the tree early.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-pruning** **(or just pruning)**: This is the process of first building the
    tree but then removing or collapsing nodes that contain only a little information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several ways to pre-prune a tree, all of which can be achieved by
    passing optional arguments to the `DecisionTreeClassifier` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the maximum depth of the tree via the `max_depth` parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting the maximum number of leaf nodes via `max_leaf_nodes`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requiring a minimum number of points in a node to keep splitting it via `min_samples_split`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often pre-pruning is sufficient to control overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Try it out on our toy dataset! Can you get the score on the test set to improve
    at all? How does the tree layout change as you start playing with the earlier
    parameters?
  prefs: []
  type: TYPE_NORMAL
- en: In more complicated real-world scenarios, pre-pruning is no longer sufficient
    to control overfitting. In such cases, we want to combine multiple decision trees
    into what is known as a **random forest**. We will talk about this in [Chapter
    10](10a94a5b-e700-4fae-b9f6-f6b77a1580f6.xhtml), *Ensemble Methods for Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: Using decision trees to diagnose breast cancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have built our first decision tree, it''s time to turn our attention to
    a real dataset: the Breast Cancer Wisconsin dataset ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is a direct result of medical imaging research and is considered
    a classic today. The dataset was created from digitized images of healthy (benign)
    and cancerous (malignant) tissues. Unfortunately, I wasn''t able to find any public-domain
    examples from the original study, but the images look similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of the research was to classify tissue ...
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full dataset is part of scikit-learn''s example datasets. We can import
    it using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the dataset using the `load_breast_cancer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous examples, all data is contained in a 2D feature matrix, `data.data`,
    where the rows represent data samples and the columns are the feature values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'With a look at the provided feature names, we recognize some that we mentioned
    earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is a binary classification task, we expect to find exactly two target
    names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s reserve some 20 percent of all data samples for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You could certainly choose a different ratio, but most commonly people use
    something like 70-30, 80-20, or 90-10\. It all depends a bit on the dataset size
    but, in the end, should not make too much of a difference. Splitting the data
    80-20 should result in the following set sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Building the decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown earlier, we can create a decision tree using scikit-learn''s `tree`
    module. For now, let''s not specify any optional arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start off by creating a decision tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Do you remember how to train the decision tree? We will use the `fit` function
    for that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Since we did not specify any pre-pruning parameters, we would expect this ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using decision trees for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have so far focused on using decision trees in classification tasks,
    you can also use them for regression. But you will need to use scikit-learn again,
    as OpenCV does not provide this flexibility. We will therefore only briefly review
    its functionality here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we wanted to use a decision tree to fit a sin wave. To make things
    interesting, we will also add some noise to the data points using NumPy''s random
    number generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create 100 randomly spaced *x* values between 0 and 5 and calculate
    the corresponding sin values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add noise to every other data point in `y` (using `y[::2]`), scaled
    by `0.5` so we don''t introduce too much jitter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: You can then create a regression tree like any other tree before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A small difference is that the `gini` and `entropy` split criteria do not apply
    to regression tasks. Instead, scikit-learn provides two different split criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**mse (also known as variance reduction)**: This criterion calculates the **Mean
    Squared Error** (**MSE**) between ground truth and prediction and splits the node
    that leads to the smallest MSE.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mae**: This criterion calculates the **Mean Absolute Error** (**MAE**) between ground truth
    and prediction and splits the node that leads to the smallest MAE.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the MSE criterion, we will build two trees. Let''s first build a tree
    with depth 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will build a decision tree with a maximum depth of 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can then use the decision tree like a linear regressor from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we create a test set with *x* values densely sampled in the whole
    range from 0 through 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted *y* values can then be obtained with the `predict` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot all of these together, we can see how the decision trees differ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the thick red line represents the regression tree with depth 2\. You can
    see how the tree tries to approximate the data using these crude steps. The thinner
    blue line belongs to the regression tree with depth 5; the added depth has allowed
    the tree to make many finer-grained approximations. Therefore, this tree can approximate
    the data even better. However, because of this added power, the tree is also more
    susceptible to fitting noisy values, as can be seen especially from the spikes
    on the right-hand side of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned all about decision trees and how to apply them to
    both classification and regression tasks. We talked a bit about data generation,
    overfitting, and ways to avoid this phenomenon by tweaking pre-pruning and post-pruning
    settings. We also learned about how to rate the quality of a node split using
    metrics such as the Gini impurity and information gain. Finally, we applied decision
    trees to medical data to detect cancerous tissues. We will come back to decision
    trees towards the end of this book when we will combine multiple trees into what
    is known as a random forest. But for now, let's move on to a new topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will introduce another staple of the machine learning
    world: support vector ...'
  prefs: []
  type: TYPE_NORMAL
