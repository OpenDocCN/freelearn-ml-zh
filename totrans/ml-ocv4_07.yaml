- en: Using Decision Trees to Make a Medical Diagnosis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to handle data in all shapes and forms, be it numerical,
    categorical, text, or image data, it is time to put our newly gained knowledge
    to good use.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to build a machine learning system that can
    make a medical diagnosis. We aren't all doctors, but we've probably all been to
    one at some point in our lives. Typically, a doctor would gain as much information
    as possible about a patient's history and symptoms to make an informed diagnosis.
    We will mimic a doctor's decision-making process with the help of what is known
    as **decision trees**. We will also cover the Gini coefficient, information gain,
    and variance reduction, along with overfitting and pruning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is a simple yet powerful supervised learning algorithm that
    resembles a flow chart; we will talk more about this in just a minute. Other than
    in medicine, decision trees are commonly used in fields such as astronomy (for
    example, for filtering noise from the Hubble Space Telescope images or to classify
    star-galaxy clusters), manufacturing and production (for example, by Boeing to
    discover flaws in the manufacturing process), and object recognition (for example,
    for recognizing 3D objects).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we want to learn about the following in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Building simple decision trees from data and using them for either classification
    or regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding which decision to make next using the Gini coefficient, information
    gain, and variance reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning a decision tree and its benefits
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But first, let's talk about what decision trees actually are.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any OS—macOS, Windows, and Linux-based OSes, along with this book.
    We recommend you have at least 4 GB RAM in your system.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding decision trees
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a simple yet powerful model for supervised learning problems.
    As the name suggests, we can think of it as a tree in which information flows
    along different branches—starting at the trunk and going all of the way to the
    individual leaves, making decisions about which branch to take at each junction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'This is basically a decision tree! Here is a simple example of a decision tree:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: A decision tree is made of a hierarchy of questions or tests about the data
    (also known as **decision nodes**) and their possible consequences.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the true difficulties with building decision trees is how to pull out
    suitable features from the data. To make this clear, let''s use a concrete example.
    Let''s say we have a dataset consisting of a single email:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This email can be vectorized in much the same way as we did in the previous
    chapter, using scikit-learn''s `CountVectorizer`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the previous chapter, we know that we can have a look at the feature names
    in `X` using the following function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For the sake of clarity, we focus on the first five words only, which are sorted
    alphabetically. Then, the corresponding number of occurrences can be found as
    follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This tells us that four out of five words show up just once in the email, but
    the word, `account` (the last one listed in `Out[3]`), actually shows up twice.
    In the last chapter, we typed `X.toarray()` to convert the sparse array, `X`, into
    a human-readable array. The result is a 2D array, where rows correspond to the
    data samples, and columns correspond to the feature names described in the preceding
    command. Since there is only one sample in the dataset, we limit ourselves to
    row 0 of the array (that is, the first data sample) and the first five columns
    in the array (that is, the first five words).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we check whether the email is from a Nigerian prince?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to do this is to look at whether the email contains both the words, `nigerian` and `prince`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: What do we find to our surprise? The word `prince` does not occur in the email.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean the message is legit?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: No, of course not. Instead of `'prince'`, the email went with the words, `head
    of state`, effectively circumventing our all-too-simple spam detector.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, how do we even start to model the second decision in the tree: *wants
    me to send him money?* There is no straightforward feature in the text that answers
    this question. Hence, this is a problem of feature engineering, of combining the
    words that actually occur in the message in such a way that allows us to answer
    this question. Sure, a good sign would be to look for strings such as `US$` and `money`,
    but then we still wouldn't know the context in which these words were mentioned.
    For all we know, perhaps they were part of the sentence: *Don't worry, I don't
    want you to send me any money.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: To make matters worse, it turns out that the order in which we ask these questions
    can actually influence the final outcome. For example, what if we asked the last
    question first: *do I actually know a Nigerian prince?* Suppose we had a Nigerian
    prince for an uncle, then finding the words *Nigerian prince* in the email might
    no longer be suspicious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this seemingly simple example got quickly out of hand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the theoretical framework behind decision trees helps us to find both
    the right decision rules as well as which decisions to tackle next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: However, to understand these concepts, we have to dig a little deeper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Building our first decision tree
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think we are ready for a more complex example. As promised earlier, let's
    now move into the medical domain.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example where several patients have suffered from the same
    illness, such as a rare form of basorexia. Let''s further assume that the true
    causes of the disease remain unknown to this day and that all of the information
    that is available to us consists of a bunch of physiological measurements. For
    example, we might have access to the following information:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A patient's blood pressure (`BP`)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's cholesterol level (`cholesterol`)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's gender (`sex`)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's age (`age`)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's blood sodium concentration (`Na`)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A patient's blood potassium concentration (`K`)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on all of ...
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before proceeding with the further steps, let''s quickly understand one very
    crucial step for every machine learning engineer—data generation. We know that
    all machine learning and deep learning techniques require a huge amount of data—in
    simple terms: the bigger, the better. But what if you don''t have enough data?
    Well, you can end up with a model that doesn''t have enough accuracy. The common
    technique employed (if you are not able to generate any new data) is to use the
    majority of the data for training. The major downside of this is that you have
    a model that is not generalized or, in other terms, suffers from overfitting.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: One solution to deal with the preceding issue is to generate new data or, as
    it is commonly referred to, synthetic data. The key point to note here is that
    the synthetic data should have similar features to your real data. The more similar
    they are to the real data, the better it is for you as an ML engineer. This technique
    is referred to as **data augmentation**, where we use various techniques such
    as rotation and mirror images to generate new data that is based upon the existing
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Since we are dealing with a hypothetical case here, we can write simple Python
    code to generate random data—since there are no set features for us here. In a
    real-world case, you would use data augmentation to generate realistic-looking
    new data samples. Let's see how to approach this for our case.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Here, the dataset is actually a list of dictionaries, where every dictionary
    constitutes a single data point that contains a patient's blood work, age, and
    gender, as well as the drug that was prescribed. So, we know that we want to create
    new dictionaries and we know the keys to use in this dictionary. The next thing
    to focus on is the data type of the values in the dictionaries.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: We start with `age`, which is an integer, then we have the gender, which is
    either `M` or `F`. Similarly, for other values, we can infer the data types and,
    in some cases, using common sense, we can even infer the range of the values to
    use.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to note that common sense and deep learning don't go well
    together most of the time. This is because you want your model to understand when
    something is an outlier. For example, we know that it's highly unlikely for someone
    to have an age of 130 but a generalized model should understand that this value
    is an outlier and should not be taken into account. This is why you should always
    have a small portion of data with such illogical values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can generate some synthetic data for our case:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can call the preceding function using `entries = generateBasorexiaData (5)` if
    we want to generate five new entries.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to generate the data, let's have a look at what we can
    do with this data. Can we figure out the doctor's reasoning for prescribing drugs
    `A`, `B`, `C`, or `D`? Can we see a relationship between a patient's blood values
    and the drug that the doctor prescribed?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Chances are, it is as difficult a question to answer for you as it is for me.
    Although the dataset might look random at first glance, I have, in fact, put in
    some clear relationships between a patient's blood values and the prescribed drug.
    Let's see whether a decision tree can uncover these hidden relationships.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the task by understanding the data
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is always the first step in tackling a new machine learning problem?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'You are absolutely right: getting a sense of the data. The better we understand
    the data, the better we understand the problem we are trying to solve. In our
    future endeavors, this will also help us to choose an appropriate machine learning
    algorithm.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to realize is that the `drug` column is actually not a feature
    value like all of the other columns. Since it is our goal to predict which drug
    will be prescribed based on a patient's blood values, the `drug` column effectively becomes the target
    label. In other words, the inputs to our machine learning algorithm will be the
    blood values, age, and gender of a ...
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our data to be understood by the decision tree algorithm, we need to convert
    all categorical features (`sex`, `BP`, and `cholesterol`) into numerical features.
    What is the best way to do that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Exactly: we use scikit-learn''s `DictVectorizer`. Like we did in the previous
    chapter, we feed the dataset that we want to convert to the `fit_transform` method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, `data_pre` contains the preprocessed data. If we want to look at the
    first data point (that is, the first row of `data_pre`), we match the feature
    names with the corresponding feature values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From this, we can see that the three categorical variables—blood pressure (`BP`),
    cholesterol level (`cholesterol`), and gender (`sex`)—have been encoded using
    one-hot coding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure that our data variables are compatible with OpenCV, we need to
    convert everything into floating point values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, all that''s left to do is to split the data into training and tests sets,
    like we did in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*. Remember that we always want to keep the training
    and test sets separate. Since we only have 20 data points to work with in this
    example, we should probably reserve more than 10 percent of the data for testing.
    A 15-5 split seems appropriate here. We can be explicit and order the `split` function
    to yield exactly five test samples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Constructing the tree
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building the decision tree with OpenCV works in much the same way as in [Chapter
    3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First Steps in Supervised Learning*.
    Recall that all of the machine learning functions reside in OpenCV 3.1''s `ml` module:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create an empty decision tree using the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To train the decision tree on the training data, we use the `train` method. That''s
    why we converted the data to float before—so that we could use it in the `train`
    method:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we have to specify whether the data samples in `X_train` occupy the rows
    (using `cv2.ml.ROW_SAMPLE`) or the columns (`cv2.ml.COL_SAMPLE`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can predict the labels of the ...
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing a trained decision tree
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV's implementation of decision trees is good enough if you are just starting
    out and don't care too much about what's going on under the hood. However, in
    the following sections, we will switch to scikit-learn. Its implementation allows
    us to customize the algorithm and makes it a lot easier to investigate the inner
    workings of the tree. Its usage is also much better documented.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, decision trees can be used for both classification and regression.
    They reside in the `tree` module:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the `tree` module from `sklearn`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similar to OpenCV, we then create an empty decision tree using the `DecisionTreeClassifier` constructor:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The tree can then be trained using the `fit` method:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then compute the accuracy score on both the training and the test sets
    using the `score` method:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, here''s the cool thing: if you want to know what the tree looks like,
    you can do so using GraphViz to create a PDF file (or any other supported file
    type) from the tree structure. For this to work, you need to install GraphViz
    first. Worry not as it is already present in the environment we created at the
    beginning of this book.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, back in Python, you can export the tree in GraphViz format to a file, `tree.dot`, using
    scikit-learn''s `export_graphviz` exporter:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, back on the command line, you can use GraphViz to turn `tree.dot` into
    (for example) a PNG file:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Alternatively, you can also specify `-Tpdf` or any other supported image format.
    The result for the preceding tree looks like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: What does this all mean? Let's break the diagram down step by step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the inner workings of a decision tree
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We established earlier that a decision tree is basically a flow chart that makes
    a series of decisions about the data. The process starts at the root node (which
    is the node at the very top), where we split the data into two groups (only for
    binary trees), based on some decision rule. Then, the process is repeated until
    all remaining samples have the same target label, at which point we have reached
    a leaf node.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: In the spam filter example earlier, decisions were made by asking true/false
    questions. For example, we asked whether an email contained a certain word. If
    it did, we followed the edge labeled true and asked the next question. However,
    this works not just for categorical features, ...
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Rating the importance of features
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What I haven't told you yet is how you pick the features along which to split
    the data. The preceding root node split the data according to *Na <= 0.72,* but
    who told the tree to focus on sodium first? Also, where does the number 0.72 come
    from anyway?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Apparently, some features might be more important than others. In fact, scikit-learn
    provides a function to rate feature importance, which is a number between *0*
    and *1* for each feature, where *0* means *not used at all in any decisions made* and
    *1* means *perfectly predicts the target*. The feature importances are normalized
    so that they all sum to 1:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we remind ourselves of the feature names, it will become clear which feature
    seems to be the most important. A plot might be most informative:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will result in the following bar plot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/448d58c4-1960-44f1-847b-e1e2552d81d9.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Now, it becomes evident that the most important feature for knowing which drug
    to administer to patients was actually whether the patient had a normal cholesterol
    level. Age, sodium levels, and potassium levels were also important. On the other
    hand, gender and blood pressure did not seem to make any difference at all. However,
    this does not mean that gender or blood pressure are uninformative. It only means
    that these features were not picked by the decision tree, likely because another
    feature would have led to the same splits.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: But, hold on. If cholesterol level is so important, why was it not picked as
    the first feature in the tree (that is, in the root node)? Why would you choose
    to split on the sodium level first? This is where I need to tell you about that
    ominous `gini` label in the diagram earlier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances tell us which features are important for classification,
    but not which class label they are indicative of. For example, we only know that
    the cholesterol level is important, but we don't know how that led to different
    drugs being prescribed. In fact, there might not be a simple relationship between
    features and classes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the decision rules
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the perfect tree, you would want to split the tree at the most informative
    feature, resulting in the purest daughter nodes. However, this simple idea leads
    to some practical challenges:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: It's not actually clear what most informative means. We need a concrete value,
    a score function, or a mathematical equation that can describe how informative
    a feature is.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To find the best split, we have to search over all of the possibilities at every
    decision node.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fortunately, the decision tree algorithm actually does these two steps for
    you. Two of the most commonly used criteria that scikit-learn supports are the
    following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''gini''`: The Gini impurity is a measure of misclassification,
    with the aim ...'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the complexity of decision trees
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you continue to grow a tree until all leaves are pure, you will typically arrive
    at a tree that is too complex to interpret. The presence of pure leaves means
    that the tree is 100 percent correct on the training data, as was the case with
    our tree shown earlier. As a result, the tree is likely to perform very poorly
    on the test dataset, as was the case with our tree shown earlier. We say the tree overfits the
    training data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to avoid overfitting:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-pruning**: This is the process of stopping the creation of the tree early.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-pruning** **(or just pruning)**: This is the process of first building the
    tree but then removing or collapsing nodes that contain only a little information.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several ways to pre-prune a tree, all of which can be achieved by
    passing optional arguments to the `DecisionTreeClassifier` constructor:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the maximum depth of the tree via the `max_depth` parameter
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting the maximum number of leaf nodes via `max_leaf_nodes`
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requiring a minimum number of points in a node to keep splitting it via `min_samples_split`
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often pre-pruning is sufficient to control overfitting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Try it out on our toy dataset! Can you get the score on the test set to improve
    at all? How does the tree layout change as you start playing with the earlier
    parameters?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In more complicated real-world scenarios, pre-pruning is no longer sufficient
    to control overfitting. In such cases, we want to combine multiple decision trees
    into what is known as a **random forest**. We will talk about this in [Chapter
    10](10a94a5b-e700-4fae-b9f6-f6b77a1580f6.xhtml), *Ensemble Methods for Classification*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Using decision trees to diagnose breast cancer
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have built our first decision tree, it''s time to turn our attention to
    a real dataset: the Breast Cancer Wisconsin dataset ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is a direct result of medical imaging research and is considered
    a classic today. The dataset was created from digitized images of healthy (benign)
    and cancerous (malignant) tissues. Unfortunately, I wasn''t able to find any public-domain
    examples from the original study, but the images look similar to the following
    screenshot:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: The goal of the research was to classify tissue ...
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The full dataset is part of scikit-learn''s example datasets. We can import
    it using the following commands:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the dataset using the `load_breast_cancer` function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As in the previous examples, all data is contained in a 2D feature matrix, `data.data`,
    where the rows represent data samples and the columns are the feature values:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With a look at the provided feature names, we recognize some that we mentioned
    earlier:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since this is a binary classification task, we expect to find exactly two target
    names:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s reserve some 20 percent of all data samples for testing:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You could certainly choose a different ratio, but most commonly people use
    something like 70-30, 80-20, or 90-10\. It all depends a bit on the dataset size
    but, in the end, should not make too much of a difference. Splitting the data
    80-20 should result in the following set sizes:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Building the decision tree
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown earlier, we can create a decision tree using scikit-learn''s `tree`
    module. For now, let''s not specify any optional arguments:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start off by creating a decision tree:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Do you remember how to train the decision tree? We will use the `fit` function
    for that:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Since we did not specify any pre-pruning parameters, we would expect this ...
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using decision trees for regression
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have so far focused on using decision trees in classification tasks,
    you can also use them for regression. But you will need to use scikit-learn again,
    as OpenCV does not provide this flexibility. We will therefore only briefly review
    its functionality here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we wanted to use a decision tree to fit a sin wave. To make things
    interesting, we will also add some noise to the data points using NumPy''s random
    number generator:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then create 100 randomly spaced *x* values between 0 and 5 and calculate
    the corresponding sin values:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We then add noise to every other data point in `y` (using `y[::2]`), scaled
    by `0.5` so we don''t introduce too much jitter:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You can then create a regression tree like any other tree before.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A small difference is that the `gini` and `entropy` split criteria do not apply
    to regression tasks. Instead, scikit-learn provides two different split criteria:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '**mse (also known as variance reduction)**: This criterion calculates the **Mean
    Squared Error** (**MSE**) between ground truth and prediction and splits the node
    that leads to the smallest MSE.'
  id: totrans-174
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mae**: This criterion calculates the **Mean Absolute Error** (**MAE**) between ground truth
    and prediction and splits the node that leads to the smallest MAE.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the MSE criterion, we will build two trees. Let''s first build a tree
    with depth 2:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will build a decision tree with a maximum depth of 5:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can then use the decision tree like a linear regressor from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we create a test set with *x* values densely sampled in the whole
    range from 0 through 5:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了这个目的，我们创建了一个测试集，其中*x*值在整个0到5的范围内密集采样：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The predicted *y* values can then be obtained with the `predict` method:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`predict`方法获得预测的*y*值：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we plot all of these together, we can see how the decision trees differ:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们将所有这些放在一起绘制，我们可以看到决策树之间的差异：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will produce the following plot:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![](img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png)'
- en: Here, the thick red line represents the regression tree with depth 2\. You can
    see how the tree tries to approximate the data using these crude steps. The thinner
    blue line belongs to the regression tree with depth 5; the added depth has allowed
    the tree to make many finer-grained approximations. Therefore, this tree can approximate
    the data even better. However, because of this added power, the tree is also more
    susceptible to fitting noisy values, as can be seen especially from the spikes
    on the right-hand side of the plot.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，粗红色的线代表深度为2的回归树。你可以看到树是如何通过这些粗略的步骤来近似数据的。较细的蓝色线属于深度为5的回归树；增加的深度使得树能够进行更多更精细的近似。因此，这棵树可以更好地近似数据。然而，由于这种增加的力量，树也更易受到拟合噪声值的影响，尤其是在图表右侧的尖峰中可以明显看出。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned all about decision trees and how to apply them to
    both classification and regression tasks. We talked a bit about data generation,
    overfitting, and ways to avoid this phenomenon by tweaking pre-pruning and post-pruning
    settings. We also learned about how to rate the quality of a node split using
    metrics such as the Gini impurity and information gain. Finally, we applied decision
    trees to medical data to detect cancerous tissues. We will come back to decision
    trees towards the end of this book when we will combine multiple trees into what
    is known as a random forest. But for now, let's move on to a new topic.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了所有关于决策树的知识以及如何将它们应用于分类和回归任务。我们简要讨论了数据生成、过拟合以及通过调整预剪枝和后剪枝设置来避免这种现象的方法。我们还学习了如何使用基尼不纯度和信息增益等指标来评估节点分裂的质量。最后，我们将决策树应用于医学数据以检测癌组织。我们将在本书的末尾回到决策树，届时我们将多个树组合成所谓的随机森林。但现在，让我们继续探讨新的主题。
- en: 'In the next chapter, we will introduce another staple of the machine learning
    world: support vector ...'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍机器学习世界中的另一个基本概念：支持向量...
