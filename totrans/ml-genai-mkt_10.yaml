- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Enhancing Brand Presence with Few-Shot Learning and Transfer Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过少样本学习和迁移学习增强品牌影响力
- en: This chapter explores the capabilities of **few-shot learning** (**FSL**) and
    its value in enhancing brand presence through tailored marketing strategies. Building
    on the insights from **zero-shot learning** (**ZSL**) covered in the previous
    chapter, we now focus on how FSL, by utilizing a limited set of examples, enables
    rapid and effective adaptation of AI models to new tasks. This approach is particularly
    valuable in marketing, where the ability to swiftly adjust content to align with
    evolving consumer preferences and market trends is crucial.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了**少样本学习**（**FSL**）的能力及其通过定制营销策略增强品牌影响力的价值。在上一章中介绍了**零样本学习**（**ZSL**）的见解之后，我们现在关注FSL如何通过利用少量示例，使AI模型能够快速有效地适应新任务。这种方法在市场营销中尤其有价值，因为能够迅速调整内容以适应不断变化的消费者偏好和市场趋势至关重要。
- en: Initially, we will introduce some of the fundamental concepts of FSL in the
    context of meta-learning as an underlying technique that facilitates quick learning
    from small datasets. We will then explore the synergy between FSL and transfer
    learning using practical examples; while FSL is effective at quickly adapting
    to new tasks with few examples, transfer learning complements this by utilizing
    pre-trained models that are fine-tuned for specific tasks, reducing the need for
    extensive model retraining.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍FSL在元学习背景下的基本概念，作为促进从小数据集快速学习的技术。然后，我们将通过实际示例探索FSL与迁移学习之间的协同作用；虽然FSL在快速适应新任务时效果显著，但迁移学习通过利用针对特定任务微调的预训练模型来补充这一点，从而减少了大量模型重新训练的需求。
- en: 'This chapter highlights how combining FSL with related strategies can optimize
    marketing efforts, making them more responsive and efficient. This chapter equips
    you with the understanding to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章强调了将FSL与相关策略相结合如何优化营销工作，使其更具响应性和效率。本章将为您提供以下理解：
- en: Grasp the core concepts of FSL and its similarities to and differences from
    transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解FSL的核心概念及其与迁移学习的相似之处和不同之处
- en: Recognize the practical benefits of employing FSL to adapt quickly to new marketing
    conditions with limited data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到在有限数据下快速适应新营销条件的FSL的实际好处
- en: Apply FSL and transfer learning techniques to real-world marketing scenarios
    to enhance brand messaging and consumer engagement
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将FSL和迁移学习技术应用于现实世界的营销场景，以增强品牌信息和消费者参与度
- en: Navigating FSL
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索FSL导航
- en: In the marketing domain, the agility to quickly tune content strategies to meet
    the evolving needs of a brand is invaluable. FSL stands out for its capacity to
    effectively learn and perform tasks with limited input data. While ZSL is designed
    to work without any specific examples of the new classes during inference, relying
    on a generalized, abstract understanding of the task derived from previously learned
    tasks, FSL uses a small number of examples to adapt to new tasks. This adaptation
    often relies on a more direct application of learned patterns and can be fine-tuned
    with data, making it particularly effective when some example data is available.
    This efficiency enables marketers to rapidly test new strategies, such as personalizing
    email campaigns for different customer segments or quickly adapting social media
    content to reflect emerging trends, without the long lead times associated with
    gathering and training on extensive datasets. For instance, a marketing manager
    might use FSL to generate tailored marketing copy that aligns with a company’s
    rebranding initiative, as we will discuss in the *Applying FSL to improve brand
    consistency section*, even when there are limited examples of such rebranded content
    available.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在市场营销领域，能够快速调整内容策略以满足品牌不断变化的需求是非常宝贵的。FSL因其能够有效地从有限的数据中学习和执行任务而脱颖而出。虽然ZSL旨在推理时无需任何新类别的具体示例，而是依赖于从先前学习任务中得出的对任务的通用、抽象理解，但FSL使用少量示例来适应新任务。这种适应通常依赖于对学习到的模式更直接的应用，并且可以通过数据微调，这使得在有示例数据的情况下特别有效。这种效率使得营销人员能够快速测试新的策略，例如为不同的客户细分市场个性化电子邮件活动或快速调整社交媒体内容以反映新兴趋势，而无需花费大量时间收集和训练大量数据集。例如，一位营销经理可能会使用FSL来生成符合公司品牌重塑计划的定制营销文案，正如我们将在*将FSL应用于提高品牌一致性部分*中讨论的那样，即使这种重塑内容的示例有限。
- en: As we further explore the utility of FSL for enhancing brand presence, it’s
    essential to build on our conceptual understanding of **Generative AI** (**GenAI**)
    introduced in the previous chapter. FSL hinges on the idea that intelligent systems
    can learn new concepts or tasks with only a small number of training examples,
    drawing heavily from prior knowledge and the application of sophisticated meta-learning
    algorithms. This discussion will extend your foundational knowledge of this concept,
    bridging the gap between high-level concepts and practical applications that are
    useful for understanding the hands-on examples given later in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进一步探索FSL在增强品牌存在感方面的效用，构建对上一章中引入的概念**生成式AI（GenAI**）的概念理解至关重要。FSL基于这样一个观点，即智能系统可以通过仅使用少量训练示例来学习新的概念或任务，大量借鉴先前知识和应用复杂的元学习算法。这次讨论将扩展你对这一概念的基础知识，弥合高级概念与实际应用之间的差距，这些实际应用对于理解本章后面给出的实际例子非常有用。
- en: At its core, FSL is often facilitated by **meta-learning**, an approach that
    enables models to quickly adapt to new tasks by using learnings from a variety
    of prior tasks. However, meta-learning is just one possible approach, and others
    such as metric-based learning, whereby models are trained to compare new instances
    against a few labeled examples using a learned metric or distance function, can
    also be used. Given its broad applicability and effectiveness as an FSL approach,
    let’s explore meta-learning further in the next section.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，FSL通常通过**元学习**得到促进，这是一种使模型能够通过使用先前各种任务的学习来快速适应新任务的方法。然而，元学习只是众多可能方法中的一种，其他方法，如基于度量的学习，其中模型被训练去比较新实例与少数标记示例，使用学习到的度量或距离函数，也可以被使用。鉴于其作为FSL方法的广泛适用性和有效性，让我们在下一节进一步探讨元学习。
- en: Understanding FSL through meta-learning
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过元学习理解FSL
- en: FSL often involves meta-learning, or “learning to learn,” whereby models are
    designed to quickly adapt to new tasks based on learnings from a wide array of
    previous tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: FSL（元学习）通常涉及元学习，或称为“学习如何学习”，其中模型被设计成能够根据从大量先前任务中学习到的知识快速适应新任务。
- en: 'This is particularly crucial in marketing, where consumer behavior and preferences
    can be dynamic, requiring models that can pivot quickly without extensive retraining.
    Meta-learning frameworks in FSL train models on a variety of tasks, enabling them
    to develop a generalized understanding or an internal representation that can
    efficiently map to new tasks with minimal additional input. Some of the key components
    of meta-learning are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这在营销领域尤为重要，因为消费者的行为和偏好可能是动态的，需要能够快速调整而无需大量重新训练的模型。FSL中的元学习框架在多种任务上训练模型，使它们能够发展出一种通用的理解或内部表示，可以高效地将新任务映射到最小额外的输入。元学习的一些关键组成部分包括：
- en: '**Task variety**: Meta-learning algorithms are exposed to a wide variety of
    learning tasks during the training phase. This exposure helps the model learn
    more robust features that are not overly specific to one task but are general
    enough to be applicable across a spectrum of future tasks. This is analogous to
    a marketing team working across different campaign types like email or social
    media and learning to identify core elements that predict success regardless of
    the specific product or audience.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务多样性**：元学习算法在训练阶段接触到广泛的学习任务。这种接触帮助模型学习到更稳健的特征，这些特征不是过于特定于某一任务，但足够通用，可以适用于一系列未来的任务。这类似于一个营销团队在不同的活动类型（如电子邮件或社交媒体）中工作，并学会识别无论具体产品或受众如何都能预测成功的核心元素。'
- en: '**Rapid adaptation**: The primary goal of meta-learning is to enable rapid
    learning on new tasks. This is achieved through the model’s ability to fine-tune
    itself to new conditions with minimal training data. For instance, if a model
    trained in a meta-learning framework is faced with a new product launch, it can
    quickly adjust its parameters to optimize marketing content for the product’s
    target demographic based on its prior knowledge of similar products.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速适应**：元学习的主要目标是实现对新任务的快速学习。这是通过模型能够使用最少量的训练数据对自己的条件进行微调来实现的。例如，如果一个在元学习框架中训练的模型面临一个新的产品发布，它可以快速调整其参数，根据其对类似产品的先前知识来优化产品的营销内容。'
- en: '**Optimization techniques**: Meta-learning involves special training schemes
    such as episodic training, whereby the model undergoes simulated training episodes.
    Each episode involves learning from a small set of data and then testing on a
    new set of data from the same task. This trains the model to generalize well from
    small data samples.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at these fundamentals in action in the following example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Implementing model-agnostic meta-learning in marketing
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider an example meta-learning model using a simple adaptation of **model-agnostic
    meta-learning** (**MAML**) and how it can be applied to optimize marketing strategies.
    MAML works by optimizing a model’s parameters so that a small number of gradient
    updates will lead to fast learning on a new task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Learn more about MAML**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'MAML is a widely recognized meta-learning algorithm introduced by Finn et al.
    in 2017 in the paper *Model-Agnostic Meta-Learning for Fast Adaptation of Deep
    Networks*. For a deeper dive into the topic, including interactive examples, check
    out the resource: https://interactive-maml.github.io/.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simplified implementation, we’ll simulate a single marketing task as
    a training input as a proxy for the multiple tasks that would be considered in
    a full MAML deployment. We will then train a simple neural network model on the
    task and then use the result of our meta-training to improve the model’s adaptability
    on a different task. The following diagram provides an illustration of the mechanics
    of a meta-learning framework, as well as what it might look like when applied
    to multiple marketing tasks as examples:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Key components of a possible MAML framework in the context of
    marketing campaigns'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.10](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.10)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-agnostic techniques are designed to be broadly applicable across a range
    of model architectures and tasks. To better understand the mechanics of the MAML
    approach, we will break down its application into several functions that are simplifications
    of what would be involved with a full implementation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have a function, `generate_task`, that creates simulated marketing
    tasks with its own set of coefficients representing campaign data, which, in this
    case, is represented as a quadratic relationship with noise. The randomly generated
    coefficients for each task dictate how input data (such as email campaign length,
    budget, or reach) affect the outcome (such as engagement or conversions). Then
    the `task` function computes the campaign’s success metrics based on the input
    and the coefficients. Note that we provide seed values so that your output should
    be consistent with what’s shown here, but your results may still vary due to randomness
    from parallel processing or GPU usage:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we have a function, `train_model_on_task`, that trains the model on one
    of the tasks generated by the previous function by performing short bursts of
    training on the task and using the `Adam` optimizer to update the model’s weights.
    Then TensorFlow `GradientTape` is used for automatic differentiation, calculating
    the gradients needed to minimize the loss, which measures how well the model’s
    predictions match the actual outcomes of the campaign:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有一个函数`train_model_on_task`，它通过在任务上执行短时间段的训练并使用`Adam`优化器更新模型权重来在之前函数生成的任务之一上训练模型。然后使用TensorFlow的`GradientTape`进行自动微分，计算用于最小化损失的梯度，该损失衡量模型的预测与活动实际结果之间的匹配程度：
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we have the `meta_train_model` function, which is at the heart of meta-learning.
    This handles the meta-training process by optimizing the model’s ability to quickly
    adapt to new tasks after minimal exposure. In practice, the model is first trained
    on a task, and then it is evaluated on new data from the same task.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们有`meta_train_model`函数，这是元学习的核心。这个函数通过优化模型在最小接触后快速适应新任务的能力来处理元训练过程。在实践中，模型首先在一个任务上接受训练，然后在新数据上对该任务进行评估。
- en: 'The loss calculated from this evaluation guides adjustment of the model’s initial
    parameters so that just a few updates lead to significant improvements on new
    tasks. Note that in a full implementation, we would likely use a `reset_weights`
    argument so that after each task the model’s weights are reset to their state
    before the task-specific training and a `meta_optimizer` that updates the model’s
    initial parameters based on the task performance. This ensures that the model
    does not overfit to a particular task and retains its ability to generalize across
    tasks:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次评估计算出的损失指导调整模型的初始参数，以便仅通过几次更新就能在新任务上带来显著的改进。请注意，在完整实现中，我们可能会使用`reset_weights`参数，以便在每个任务之后将模型的权重重置到任务特定训练之前的状态，并使用`meta_optimizer`根据任务性能更新模型的初始参数。这确保了模型不会过度拟合到特定任务，并保持其在任务之间的泛化能力：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To illustrate this simplified example in action, we can first initialize a
    very simple neural network with two layers to apply our meta-training function.
    Here, there is first a dense layer with 10 neurons and a sigmoid activation and
    then a final dense layer with just one neuron to capture the model’s prediction
    for the success of a marketing campaign based on the input feature:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了说明这个简化的示例在实际中的应用，我们可以首先初始化一个非常简单的神经网络，包含两层，以应用我们的元训练函数。在这里，首先是一个包含10个神经元的密集层，具有sigmoid激活，然后是一个最终只有一个神经元的密集层，用于捕捉模型基于输入特征的营销活动成功预测：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following summary:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下总结：
- en: '![](img/B30999_10_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_10_02.png)'
- en: 'Figure 10.2: Simple neural network model architecture'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：简单的神经网络模型架构
- en: 'Let’s now generate a proxy for a complex task using your quadratic function
    and plot the model’s performance on this task before training:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用你的二次函数生成一个复杂任务的代理，并在训练之前绘制模型在这个任务上的性能：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following plot shows the performance of the model before undergoing meta-training.
    As we can see, the model is initially unable to accurately predict the true values,
    indicating the need for further training and optimization:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了在经过元训练之前模型的性能。正如我们所见，模型最初无法准确预测真实值，这表明需要进一步的训练和优化：
- en: '![](img/B30999_10_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_10_03.png)'
- en: 'Figure 10.3: Model performance before meta-training on the task'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：在任务上进行元训练前的模型性能
- en: 'Now we can perform meta-training using our algorithm and then plot the model’s
    performance again on the same task after meta-training:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用我们的算法进行元训练，然后再次绘制模型在相同任务上的性能：
- en: '[PRE5]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This yields the following plot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![](img/B30999_10_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_10_04.png)'
- en: 'Figure 10.4: Model performance after meta-training on the task'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：在任务上进行元训练后的模型性能
- en: As shown in *Figures 10.3* and *10.4*, the model initially fails to grasp any
    quadratic relationship in the data. After meta-training, however, the predictions
    improve. While this is a simplified implementation including only a simple task,
    this implementation framework gives a baseline for the mechanics of MAML as a
    tool for preparing your model for FSL.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.3*和*10.4*所示，模型最初无法掌握数据中的任何二次关系。然而，在元训练之后，预测得到了改善。虽然这是一个只包括简单任务的简化实现，但这个实现框架为MAML作为准备模型进行FSL的工具的机制提供了一个基准。
- en: Overcoming challenges in FSL
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服FSL中的挑战
- en: FSL aims to make the most out of minimal data, but this can come with inherent
    challenges such as overfitting and poor generalization. **Overfitting** occurs
    when a model, trained on a very small dataset, learns the noise and irrelevant
    details to the extent that it negatively impacts the performance on new data.
    To mitigate this, regularization techniques such as L2 regularization and dropout
    can be used to simplify the model complexity of neural networks, helping prevent
    the model from learning overly complex patterns that do not generalize well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: FSL旨在充分利用最少的数据，但这可能伴随着固有的挑战，如过拟合和泛化能力差。**过拟合**发生在模型在非常小的数据集上训练时，学习到了噪声和不相关的细节，以至于它对新数据的性能产生了负面影响。为了减轻这一点，可以使用如L2正则化和dropout等正则化技术来简化神经网络模型的复杂度，帮助防止模型学习过于复杂的模式，这些模式泛化能力不佳。
- en: '**Regularization techniques to prevent overfitting**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**防止过拟合的正则化技术**'
- en: 'Two common regularization techniques in neural networks are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中两种常见的正则化技术是：
- en: L2 regularization (weight decay) adds a penalty to the loss function based on
    the squared magnitude of the model’s weights. This discourages large weights,
    leading to simpler models that generalize better.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化（权重衰减）在损失函数中添加了一个基于模型权重平方大小的惩罚。这阻止了权重过大，导致更简单且泛化能力更好的模型。
- en: 'Learn more here, including how to compare learning curves to evaluate the impact
    of regularization: [https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization](https://developers.google.com/machine-learning/crash-course/overfitting/regularization).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里了解更多，包括如何比较学习曲线以评估正则化的影响：[https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization](https://developers.google.com/machine-learning/crash-course/overfitting/regularization)。
- en: Dropout randomly deactivates (“drops out”) neurons during training to prevent
    the model from becoming too reliant on specific pathways.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout随机地在训练过程中停用（“丢弃”）神经元，以防止模型过度依赖特定的路径。
- en: 'Learn more in the seminal paper *Dropout: a simple way to prevent neural networks
    from overfitting* by Srivastava et al.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在Srivastava等人撰写的开创性论文《Dropout: a simple way to prevent neural networks from
    overfitting》中了解更多。'
- en: 'For instance, we could add these types of regularization while building a neural
    network model suitable for a regression task. As an illustration, the model is
    designed with a moderately complex architecture and includes several layers with
    both types of regularization:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在构建适合回归任务的神经网络模型时，我们可以添加这些类型的正则化。作为一个例子，该模型设计了一个适度的复杂架构，并包含具有两种类型正则化的多层：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This yields the following summary:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下总结：
- en: '![](img/B30999_10_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_10_05.png)'
- en: Figure 10.5\. Model architecture summary
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：模型架构摘要
- en: 'The model embeds L2 regularization within its dense layers to curb the weight
    size and encourage simpler models. A dropout of 0.3 is also applied after each
    dense layer, reducing overfitting by deactivating random neuron outputs during
    training. This regularization pattern is applied across the hidden layers to promote
    the model’s ability to generalize. The following figure provides a visualization
    of how dropout randomly deactivates neurons during training:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在其密集层中嵌入L2正则化，以控制权重大小并鼓励更简单的模型。在每个密集层之后也应用了0.3的dropout，通过在训练期间停用随机神经元输出，减少了过拟合。这种正则化模式应用于隐藏层，以促进模型泛化能力。以下图展示了dropout在训练期间如何随机停用神经元：
- en: '![](img/B30999_10_06.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_10_06.png)'
- en: 'Figure 10.6: Dropout neural net model. Left: A standard neural net with two
    hidden layers. Right: An example of a thinned net produced by applying dropout
    to the network on the left. Crossed units have been dropped. (source: https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：Dropout神经网络模型。左：一个带有两个隐藏层的标准神经网络。右：对左侧网络应用dropout后产生的稀疏网络示例。交叉的单元已被丢弃。（来源：https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf）
- en: As alluded to earlier, **generalization** is another critical challenge. In
    the field of image processing, data augmentation techniques like image rotation,
    cropping, or color adjustment are often employed to artificially enhance the dataset’s
    size and variability, helping the model learn more general features that perform
    better on new tasks. In NLP applications, data augmentation techniques such as
    those presented in *Chapter 5* can be employed, along with others such as synonym
    replacement, in order to increase robustness and aid in better generalization
    across different contexts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**泛化**是另一个关键挑战。在图像处理领域，数据增强技术，如图像旋转、裁剪或颜色调整，通常被用来人为地增强数据集的大小和变异性，帮助模型学习更通用的特征，这些特征在新任务上表现更好。在NLP应用中，可以采用如第5章中所述的数据增强技术，以及其他如同义词替换等技术，以增加鲁棒性并帮助在不同上下文中更好地泛化。
- en: A significant challenge in FSL is also the difficulty of achieving robust performance
    with limited training data. Depending on the complexity of the pattern being learned,
    models may require larger datasets to generalize well and therefore struggle to
    learn useful patterns without it, leading to poor performance on new tasks. Transfer
    learning can be a powerful, complementary strategy to address this challenge.
    By starting with a model pre-trained on a large and diverse dataset, you can leverage
    the learned features and fine-tune the model on your specific FSL task. This approach
    enables rapid adaptation to new tasks with minimal data while retaining a broad
    knowledge base, complementing FSL by providing a rich initial set of features
    that need only slight adjustments rather than learning from scratch. This powerful
    approach has its limitations and challenges as well; however, these are topics
    we will discuss in the following section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在FSL（功能磁共振成像）中，一个显著的挑战也是如何在有限的训练数据下实现稳健的性能。根据学习模式的复杂性，模型可能需要更大的数据集来良好地泛化，因此如果没有这样的数据集，它们将难以学习有用的模式，导致在新任务上的表现不佳。迁移学习可以作为一种强大的、补充的策略来应对这一挑战。通过从在大规模和多样化的数据集上预训练的模型开始，你可以利用学习到的特征，并在你特定的FSL任务上微调模型。这种方法可以在最小数据量的情况下快速适应新任务，同时保留广泛的知识库，通过提供需要仅做轻微调整而不是从头开始学习的丰富初始特征集来补充FSL。这种强大的方法也有其局限性和挑战，然而，这些是我们将在下一节讨论的主题。
- en: '**Optimizing FSL techniques**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化FSL技术**'
- en: 'Key strategies for enhancing the efficacy of FSL models:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 提高FSL模型有效性的关键策略：
- en: '**Overfitting prevention**: Implement regularization techniques to help the
    model focus on simpler, more general patterns, rather than memorizing specific
    noise in the training data.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止过拟合**：实施正则化技术，帮助模型专注于更简单、更一般的模式，而不是记住训练数据中的特定噪声。'
- en: '**Enhancing generalization**: Employ data augmentation to expand the training
    dataset. For text, approaches like paraphrasing or injecting synonyms help the
    model learn language variations.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强泛化**：使用数据增强来扩展训练数据集。对于文本，像释义或注入同义词这样的方法有助于模型学习语言变体。'
- en: '**Leveraging transfer learning**: Utilize a model pre-trained on a comprehensive
    dataset and then fine-tune it for your specific FSL task.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用迁移学习**：利用在大规模数据集上预训练的模型，然后针对你特定的FSL任务进行微调。'
- en: Navigating transfer learning
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导航迁移学习
- en: Transfer learning can enhance how marketing professionals leverage AI by enabling
    the more effective use of pre-trained models on new tasks with only minor adjustments.
    While FSL uses a set of examples from the new task for quick adaptation, transfer
    learning focuses on repurposing an existing model without needing additional examples
    from the new domain.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习可以通过在新任务上仅进行少量调整来更有效地使用预训练模型来增强营销专业人士对AI的利用。虽然FSL使用新任务的一组示例进行快速适应，但迁移学习侧重于重新利用现有模型，而无需从新领域获取额外的示例。
- en: This approach capitalizes on the knowledge that models gain from large-scale
    data in previous tasks, applying it to enhance marketing efforts in completely
    different areas without the overhead of retraining the model from scratch. Put
    differently, FSL improves model adaptability using very limited data examples,
    whereas transfer learning excels in environments where the relationship between
    past and current tasks is strong but the availability of large enough labeled
    datasets for training a base model for the new task is difficult or costly to
    acquire.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法利用了模型从前一任务中从大规模数据中获取的知识，将其应用于完全不同的领域的营销努力，而无需从头开始重新训练模型。换句话说，FSL通过使用非常有限的数据示例来提高模型的适应性，而迁移学习在历史任务和当前任务之间关系强烈但为新任务训练基础模型所需的大规模标记数据集难以获得或成本高昂的环境中表现卓越。
- en: An additional advantage of transfer learning over FSL can come from a cost perspective.
    For example, when using paid API services for state-of-the-art GenAI models, pricing
    can be related to the input size used to generate the response. In the context
    of NLP applications, this is often captured by the token count. When fine-tuning
    via transfer learning, the base model only needs to be adjusted once. This means
    that the token count paid for during the fine-tuning phase does not recur with
    every usage of the model, as opposed to FSL, which may require the same relevant
    examples to be fed in each time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习相较于FSL的另一个优势可能来自成本角度。例如，当使用付费API服务最先进的GenAI模型时，定价可能与生成响应所使用的输入大小相关。在NLP应用中，这通常由令牌计数来衡量。通过迁移学习进行微调时，基础模型只需调整一次。这意味着在微调阶段支付的令牌计数不会随着模型每次使用而重复，与FSL不同，FSL可能需要每次都输入相同的相关示例。
- en: With transfer learning, the initial cost of fine-tuning the model can be offset
    by the lower costs for each subsequent inference, as no additional examples need
    to be fed into the model. Transfer learning therefore becomes cost-effective when
    the number of inferences after fine-tuning exceeds a certain threshold, making
    it cheaper in the long run compared to FSL.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习，模型微调的初始成本可以通过每次推理的较低成本来抵消，因为不需要向模型输入额外的示例。因此，当微调后的推理次数超过某个阈值时，迁移学习变得具有成本效益，从长远来看比FSL更便宜。
- en: 'As an example, here is a breakdown to illustrate the potential cost differences
    between FSL and transfer learning:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是一个分解，说明了FSL和迁移学习之间潜在成本差异：
- en: '**FSL API costs**:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FSL API费用**：'
- en: '**Base cost**: Cost for tokens used in the prompt (Cost per token × Number
    of tokens in prompt)'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础成本**：提示中使用的令牌的成本（每个令牌的成本 × 提示中的令牌数）'
- en: '**Additional cost per example**: Cost per token × Number of tokens per example'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个示例的额外成本**：每个令牌的成本 × 每个示例的令牌数'
- en: '**Transfer learning API costs**:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习API费用**：'
- en: '**Initial fine-tuning cost**: One-time cost for adjusting the pre-trained model
    using a larger set of example data'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始微调成本**：使用更大示例数据集调整预训练模型的一次性成本'
- en: '**Cost per inference**: Cost for each inference using the fine-tuned model'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每次推理的成本**：使用微调模型进行每次推理的成本'
- en: '**When to use transfer learning over FSL**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**何时使用迁移学习而非FSL**'
- en: 'Compared to FSL, transfer learning can be more advantageous under the following
    circumstances:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与FSL相比，在以下情况下迁移学习可能更有优势：
- en: '**High frequency of usage**: The more frequently the model is used, the quicker
    the initial fine-tuning cost is amortized.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用频率高**：模型使用得越频繁，初始微调成本就越快摊销。'
- en: '**Stable task requirements**: New tasks must be similar enough for the fine-tuned
    model to perform well without further adjustment.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定的任务需求**：新任务必须足够相似，以便微调后的模型无需进一步调整就能表现良好。'
- en: '**Complex pattern adaption**: While requiring more extensive data, it can adapt
    deeper model layers to learn more complex patterns.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂模式适应**：虽然需要更多的数据，但它可以适应更深的模型层来学习更复杂的模式。'
- en: The mechanics of transfer learning
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的工作原理
- en: At a high level, transfer learning and its fine-tuning processes are built upon
    similar foundations established by techniques like ZSL and FSL. Both ZSL and FSL
    are designed to apply pre-existing knowledge to new tasks. However, they differ
    primarily in the approach to adaptation. ZSL hypothesizes about *unseen tasks*
    based on learned abstract concepts without any specific examples, whereas FSL
    uses a *handful of examples* to guide the adaptation. Transfer learning extends
    these concepts by utilizing a base of extensively trained models that can be specifically
    fine-tuned to a new task, providing a practical balance between the broad generalization
    of ZSL and the rapid adaptation characteristic of FSL. Transfer learning is often
    the preferred approach when a robust, pre-trained model exists and there are examples
    from a highly related domain, but a lack of examples specifically related to the
    new task.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，迁移学习和其微调过程建立在类似于ZSL和FSL等技术建立的基础之上。ZSL和FSL都旨在将现有知识应用于新任务。然而，它们在适应方法上主要存在差异。ZSL基于学习到的抽象概念对*未见任务*进行假设，而不需要任何具体示例，而FSL使用*少量示例*来引导适应。迁移学习通过利用可以针对新任务进行特定微调的广泛训练的模型基础，扩展了这些概念，在ZSL的广泛泛化能力和FSL的快速适应特性之间提供了实用的平衡。当存在一个健壮的预训练模型，并且有来自高度相关领域的示例，但缺乏与新任务具体相关的示例时，迁移学习通常是首选的方法。
- en: At a more technical level, fine-tuning in transfer learning involves subtle
    yet significant adjustments to the model’s parameters that are already well-trained
    on large, diverse datasets. This fine-tuning adapts the learned features to the
    specifics of a new, related task, often involving adjustments to the deeper, more
    discriminative layers of the model. Such modifications enable the model to maintain
    its generalization capabilities while optimizing for performance on specific tasks
    within the new domain. This is different from FSL, where fine-tuning aims to quickly
    adapt the model using very few examples by making minimal adjustments, often only
    to the model’s final layers or via prompt engineering where there can be no adjustments
    to the trainable parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在更技术层面上，迁移学习中的微调涉及对已经在大型、多样化的数据集上经过良好训练的模型参数进行微妙但重要的调整。这种微调使学习到的特征适应于新、相关任务的具体情况，通常涉及调整模型的更深、更具判别性的层。这些修改使模型能够在保持其泛化能力的同时，优化在新领域特定任务上的性能。这与FSL不同，FSL的微调旨在通过最小调整（通常仅限于模型的最终层或通过提示工程，其中无法调整可训练参数）使用非常少的示例快速适应模型。
- en: 'The following figure illustrates the differences between ZSL, FSL, and transfer
    learning:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了ZSL、FSL和迁移学习之间的差异：
- en: '![](img/B30999_10_07.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_10_07.png)'
- en: 'Figure 10.7: Key differences between typical ZSL, FSL, and transfer learning
    implementations and their mechanics'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：典型的ZSL、FSL和迁移学习实现及其机制之间的关键差异
- en: Transfer learning using Keras
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras的迁移学习
- en: In this section, in order to provide variety beyond applications just in the
    field of NLP, we will present a framework for how transfer learning can be implemented
    in an image classification task. This example will present a framework for how
    you can adapt a pre-trained image recognition model to a new marketing-relevant
    challenge of your choice, such as identifying desired product features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，为了在自然语言处理领域之外提供多样性，我们将展示一个框架，说明如何将迁移学习应用于图像分类任务。本例将展示如何将预训练的图像识别模型适应于您选择的新的与营销相关的新挑战，例如识别所需的产品特征。
- en: The first step in the transfer learning process involves finding a pre-trained
    model that has been extensively trained on a large and diverse dataset. Here,
    we will use the VGG16 model, known for its robust performance in image recognition.
    VGG16 is a convolutional neural network that achieves this high performance through
    training on the ImageNet dataset using over a million images in order to classify
    them into a thousand image categories, as detailed by Simonyan and Zisserman in
    their 2014 paper, *Very deep convolutional networks for large-scale image recognition*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习过程的第一步是寻找一个在大型和多样化的数据集上经过广泛训练的预训练模型。在这里，我们将使用VGG16模型，该模型以其在图像识别中的稳健性能而闻名。VGG16是一个卷积神经网络，通过在ImageNet数据集上使用超过一百万张图像进行训练，以将它们分类为一千个图像类别，如Simonyan和Zisserman在他们2014年的论文《非常深的卷积网络用于大规模图像识别》中详细描述的那样。
- en: 'First, we’ll summarize the full architecture of this model using `model.summary`
    to give us a better understanding of its layers and components:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B30999_10_08.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Model architecture summary'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown here, the VGG16 architecture is a deep neural network consisting of
    multiple layers designed to process and transform the image into a form that is
    increasingly abstract and useful for classification tasks. Here’s a simple breakdown
    based on the preceding summary:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This accepts images of size 224 x 224 pixels, which is a standard
    dimension for many image processing tasks. Each image has three color channels
    (Red, Green, and Blue), which explains the `3` in the shape given in *Figure 10.8*.
    The `None` given in each layer at the start is effectively a placeholder for the
    batch size, allowing for any number of images to be processed.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional layers (Conv2D)**: These perform the core of the work in the
    network. VGG16 has multiple convolutional layers stacked on top of each other,
    each with a set number of filters that detect different features in the image
    at various levels of granularity. For example, the first convolutional layer might
    detect edges, while deeper layers might identify more complex patterns like textures
    or specific objects. The progression in the number of filters from 64 to 512 reflects
    an increase in the complexity of features being detected in the image as it moves
    through the network.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max pooling layers (MaxPooling2D)**: These are interspersed with the convolutional
    layers and serve to reduce the spatial dimensions (width and height) of the input
    for the next convolutional layer. For instance, a pooling layer following a 224
    x 224 convolutional layer output reduces it to 112 x 112, thus reducing the computation
    required and helping in the detection of dominant features that are invariant
    to small shifts and rotations in the input image.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully connected layers (Dense)**: Near the end of the network, the flattened
    output from the convolutional layers is fed into these densely connected layers.
    These are used to combine all features from the prior convolutional layers to
    eventually classify the image into one of the 1,000 categories.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer (predictions)**: The final layer outputs the probabilities of
    the image belonging to each of the 1,000 classes trained in the ImageNet dataset.
    The class with the highest probability is taken as the prediction of the model.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a visualization of this summary taken from a medical research
    article by Yang et al. ([https://www.nature.com/articles/s41598-021-99015-3](https://www.nature.com/articles/s41598-021-99015-3)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_09.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Visualization of the architecture of the VGG16 deep neural network'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now adapt this model for transfer learning:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the model into memory again, but this time with the option `include_top=False`
    in order to truncate the model after the last convolution layer. This will move
    the top layers responsible for classification into predefined categories and allow
    the model to serve as a feature extractor, which means it outputs a complex feature
    map that represents the input image’s key characteristics but doesn’t output a
    specific category prediction:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将再次将模型加载到内存中，但这次使用`include_top=False`选项，以便在最后一个卷积层之后截断模型。这将移动负责分类的顶层到预定义的类别中，并允许模型作为特征提取器，这意味着它输出一个复杂的特征图，代表输入图像的关键特征，但不输出特定的类别预测：
- en: '[PRE8]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can then adapt this feature-rich model to our new task of product classification
    by adding our own classification layers that will be trained on top of the feature
    extractor:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在此基础上，通过添加自己的分类层来适应我们的新任务——产品分类，这些分类层将在特征提取器之上进行训练：
- en: '[PRE9]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, the model can be fine-tuned for transfer learning using new, task-specific
    data. This involves setting the pre-trained base model layers to non-trainable,
    thereby freezing the weights of the pre-trained layers, which is generally advised
    to avoid having the initial phase of training forget the majority of the base
    model’s initial feature extraction weights:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，可以使用新的、特定于任务的数据进行迁移学习来微调模型。这涉及到将预训练的基础模型层设置为不可训练的，从而冻结预训练层的权重，这通常建议避免训练的初始阶段忘记基础模型的大部分初始特征提取权重：
- en: '[PRE10]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Self-guided exercise: Fine-tuning VGG16 on your data**'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**自我引导练习：根据您的数据微调VGG16**'
- en: 'To apply transfer learning using the VGG16 model to classify images of product
    features, follow these steps:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要使用VGG16模型进行迁移学习并分类产品特征图像，请按照以下步骤操作：
- en: '**Collect images**: Gather images of products with features your company aims
    to classify. Sources can include online catalogs, social media, or direct captures
    of physical inventory.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集图像**：收集您公司旨在分类的产品特征图像。来源可以包括在线目录、社交媒体或直接捕捉的实物库存。'
- en: '**Prepare the data**: Resize images to 224 x 224 pixels to match the VGG16
    input size and normalize pixel values to range `[0, 1]`.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备数据**：将图像调整大小为224 x 224像素，以匹配VGG16的输入大小，并将像素值归一化到范围 `[0, 1]`。'
- en: '**Train the model**: Use the compiled model and fit your images (`new_data`)
    and their binary labels `[0, 1]`, for instance using:'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型**：使用编译好的模型，并拟合您的图像（`new_data`）及其二进制标签 `[0, 1]`，例如使用：'
- en: '[PRE11]'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Evaluate and fine-tune**: Based on performance, consider adjusting your learning
    rate or gradually unfreezing layers in the base VGG16 model'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估和微调**：根据性能，考虑调整学习率或逐渐解冻基础VGG16模型中的层。'
- en: Transfer learning using API services
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用API服务进行迁移学习
- en: 'Transfer learning through API services offers a practical solution for marketing
    professionals who wish to utilize cutting-edge AI without the complexities of
    a local implementation. Some of the practical benefits of using an available API
    service for transfer learning are the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过API服务进行迁移学习为希望利用前沿AI而无需处理本地实现复杂性的市场营销专业人士提供了一个实用的解决方案。使用可用的API服务进行迁移学习的实际好处包括以下几方面：
- en: '**Access to advanced models**: Continuous updates and optimizations of API-accessible
    models ensure the use of cutting-edge technology that is more advanced than what
    may be publicly available.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问高级模型**：API可访问模型的持续更新和优化确保了使用比公开可用的技术更先进的前沿技术。'
- en: '**Computational efficiency**: Offloading the computational demands to cloud-based
    services removes the need for sophisticated hardware that may be inaccessible
    for individuals or small businesses.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算效率**：将计算需求卸载到基于云的服务中，消除了对可能对个人或小型企业不可及的复杂硬件的需求。'
- en: '**Simplified parameter management**: This refers to the automatic handling
    of learning rates, gradient adjustments, and other complex training parameters
    that can affect model performance.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化参数管理**：这指的是自动处理学习率、梯度调整以及其他可能影响模型性能的复杂训练参数。'
- en: To apply transfer learning effectively, it’s crucial that the fine-tuning dataset
    mirrors the unique characteristics of your brand. This could include customer
    feedback or vetted marketing copy and product descriptions. If a proprietary dataset
    is unavailable, publicly accessible data such as reviews or social media posts
    about similar products or services could be used. The key is to compile this data
    into a single file, typically in CSV format, in a location that is accessible
    to the API. Each entry then needs to be clearly labeled according to its expected
    label or output.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example input in CSV format that gives the model a deeper
    understanding of what sustainability means in the context of our specific brand,
    setting the stage for what will be discussed in the hands-on example in the next
    section:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can initiate the model fine-tuning using OpenAI’s API with the GPT-4
    model, replacing `openai.api_key` with your actual key and `path/to/your/dataset.csv`
    with a path to the dataset’s location. Before executing this code, remember to
    consult the latest OpenAI API documentation ([https://beta.openai.com/docs/](https://beta.openai.com/docs/))
    for any updates or changes in the API usage:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Overcoming challenges in transfer learning
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While transfer learning can be a powerful approach for fine-tuning pre-trained
    models to better perform better on new tasks, it is not without its challenges
    and limitations. The following are a couple of key challenges to consider when
    applying transfer learning:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**Model drift**: In the context of transfer learning, model drift refers to
    the gradual decline in a model’s accuracy as the data it was fine-tuned on becomes
    less representative of the current environment or trends.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, imagine using a model pre-trained on marketing data from 2019 to
    predict consumer preferences for 2024\. Initially, the model may perform well
    after being fine-tuned via transfer learning with more recent 2020 data. However,
    as consumer behavior shifts due to reasons such as new social media platforms
    like TikTok gaining in popularity or significant changes in economic conditions,
    the model may start recommending outdated messaging strategies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: To address model drift, one needs to continuously update the model with recent
    data from the target domain and monitor its performance closely. This can be achieved
    by continuous learning techniques that allow the model to adjust dynamically as
    new data comes in. Further resources on continuous learning are given in the info
    box in this section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain mismatch**: Domain mismatch in transfer learning occurs when the source
    domain (where the model was initially trained) and the target domain (where the
    model is fine-tuned) differ significantly. In transfer learning, this means the
    pre-trained model’s knowledge may not generalize well to the new domain, leading
    to poor performance.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, consider a model pre-trained on English-language customer reviews
    from an e-commerce platform, which is then fine-tuned to analyze reviews in Japanese
    for a local market. Even after fine-tuning, the model may struggle to capture
    cultural nuances and linguistic differences, leading to incorrect sentiment analysis.
    This may be because the pre-trained model’s reliance on patterns learned from
    English data makes it less effective when applied to Japanese reviews where the
    expressions and cultural references are different.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: To address this, a more appropriate approach would be to fine-tune the model
    with a carefully curated dataset of Japanese customer reviews that specifically
    captures the specific linguistic and cultural nuances of the local market.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Staying ahead of model drift**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the accuracy of your transfer learning models, especially as consumer
    behavior evolves, continuous learning is key.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Continuous learning is often implemented within the practices of MLOps. Learn
    more about MLOps principles and best practices at [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/ai-ml).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced the fundamentals and techniques behind how FSL and transfer
    learning can adapt pre-trained models to better address new marketing tasks, we
    will now demonstrate the importance of FSL through a hands-on marketing example.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Applying FSL to improve brand consistency
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous exploration of ZSL in *Chapter 9*, we demonstrated how a pre-trained
    model like GPT-4 could generate marketing copy for an e-commerce brand launching
    eco-friendly products. This ZSL approach, while powerful, primarily relied on
    the model’s ability to infer context and content from generalized pre-training
    using prompts emphasizing terms like *sustainable* and *eco-friendly*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: As the examples in this section will show, while ZSL provides a solid foundation
    for generating brand-relevant content, it often lacks the precision required for
    capturing the deeper, more nuanced aspects of a brand’s ethos. This is particularly
    true for brands whose identity is heavily tied to specific practices or principles
    that may not be well represented in the generalized training of the **large language
    model** (**LLM**).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps for effective FSL in marketing campaigns**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt refinement and execution**: Begin with a basic prompt like one that
    would be used for ZSL, but now add a few examples of desired outputs or contexts
    that reflect the brand’s unique aspects.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output analysis**: Evaluate the content to ensure it maintains the necessary
    brand consistency. Note deviations from these expectations as they provide insights
    into areas needing further examples.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative refinement**: Based on the feedback and performance of the initial
    outputs, refine the examples and prompts iteratively, continuing until you achieve
    your desired marketing KPIs (see *Chapter 2*).'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continuing our scenario from the section on *Zero-shot learning for marketing
    copy* in *Chapter 9*, we revisit our journey with the sustainability-focused e-commerce
    brand that we introduced earlier. Now, in response to evolving market trends and
    corporate directives, the company known for its eco-friendly kitchenware is undergoing
    a rebranding to align with the growing consumer demand for social responsibility.
    Recent insights reveal that consumers are increasingly interested in supporting
    brands that not only offer sustainable products but also demonstrate a tangible
    commitment to fair labor practices and positive community impact. This shift is
    particularly pronounced among affluent younger consumers who value ethical production
    and are willing to pay a premium for products that make a real difference in the
    lives of workers and communities. Furthermore, this demographic wants their products
    not just to have eco-friendly packaging but for them to be deemed “zero waste.”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate how GPT-4 can effectively capture our newly defined sustainability
    campaign goals by enhancing output content through prompt engineering. This includes
    the subtle clarifications around sustainability mentioned earlier:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: A commitment to fair labor practices
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero waste packaging
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Active community engagement
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will exemplify this through a case study aimed at boosting an email marketing
    campaign. This involves iterative refinement of prompts, closely tied to monitoring
    key performance indicators previously discussed in *Chapter 2*, to ensure alignment
    with our rebranded marketing objectives.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking with ZSL and FSL
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before integrating FSL into our email marketing campaign, it’s important to
    benchmark the effectiveness of ZSL at capturing the refined brand strategy for
    FSL is introduced. Here, we will compare a baseline response generated using ZSL
    that’s devoid of specific examples that align with our rebranded sustainability
    focus. We’ll define a function to execute the API completion and then illustrate
    how ZSL performs without the nuanced context of our updated campaign goals:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us the following response:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, while the response is clear, it is too generic and lacks the
    specificity and context needed to meet the specific goals of our rebranding initiative.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the value FSL can provide, here, we revisit the exact same base prompt
    as earlier, but also include context specific to our updated campaign goals via
    FSL:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding text, the aspects of the LLM-generated content that organically
    relate to our specific goals—environmental sustainability, fair labor practices,
    and community engagement—are highlighted for clarity. This comparison with ZSL
    underscores the capability of FSL to produce content that is more tailored and
    relevant to the strategic nuances of our rebrand by leveraging the initial contextual
    examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Developing an email marketing campaign
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s explore the application of FSL in enhancing the personalization
    and relevance of email marketing campaigns, significantly boosting both customer
    engagement and conversion rates. The following is a concise guide on how to utilize
    FSL to optimize email campaigns effectively:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial email creation**: Generate the initial batch of emails using FSL,
    specifically focusing on your core marketing goals—in this case, sustainability
    and ethical practices.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collect initial metrics and responses**: Analyze the initial reactions from
    customers to understand what aspects of the content resonate the most. This analysis
    is crucial in identifying successful elements and areas that may require additional
    refinement.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative refinement**: Refine the content of the emails based on the engagement
    metrics and feedback received from customers.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continued feedback integration**: Continuously integrate new insights to
    keep the email marketing campaign dynamically aligned with evolving customer preferences
    and market trends.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let’s examine what these steps entail.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Initial email creation'
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in employing FSL in an email marketing campaign is to clearly
    define the campaign’s objectives. For our eco-friendly e-commerce brand, the objective
    might be to launch a new line of eco-friendly kitchenware that aligns with the
    rebrand goals. Let’s consider our objective for this scenario to be the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With the campaign’s objective clearly defined, the next step involves creating
    prompts that instruct the AI on the content that’s appropriate for the email message.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '**Structuring the initial email for your marketing campaign**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The following general structure is helpful in creating an effective initial
    message for your email marketing campaign that increases engagement and conversion
    with your audience:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**: Greet the customer and introduce the new product line enthusiastically.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Body**: Provide detailed information about the core aspects of the product
    – in this case, the sustainability and ethical manufacturing aspects.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call to action**: Include a call to action that encourages the recipient
    to take specific steps such as making a purchase or visiting a website.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Closing**: Conclude with a warm closing that reinforces the brand’s commitment
    to the core aspects of the product.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll define a starting prompt for the model, which will be used to generate
    the initial batch of emails. Similar to the earlier product description example,
    each element of the `fsl_prompts` is designed to progressively build a narrative
    around the brand’s commitment to sustainability, ethical practices, and community
    involvement, as required by the corporate rebrand, in the following way:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The first pair of prompts ensures the content appropriately emphasizes the eco-friendly
    and zero-waste aspect of the kitchenware
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second set of prompts addresses the context expected when discussing the
    ethical labor practices involved in the production process
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final two prompts tie the product to community engagement efforts, linking
    purchases to initiatives that are important to socially responsible consumers
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To ensure that the generated content aligns with our campaign goals, we’ll
    also adjust our base prompt to be more specific to an email campaign and increase
    the `max_tokens` to reflect the longer content:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This produces the following email content:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 2: Collect and analyze initial metrics and responses'
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the deployment of our email campaign with the newly created content,
    it is crucial to assess its effectiveness using KPIs. For this evaluation, we
    assume we already have representative data from the first seven days post-launch
    that will allow us to examine the metrics of open rate, click-through rate, conversion
    rate, and unsubscribe rate. This analysis helps us identify areas for improvement
    by visualizing these KPIs over time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the following graph, we can observe the trends in open rates, click-through
    rates, conversion rates, and unsubscribe rates over the first week following the
    campaign launch. This allows us to quickly identify patterns, which can be critical
    for diagnosing issues with the campaign’s content or delivery:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_10.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Email campaign KPIs during the first week after launch'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '**Benchmarking your email campaign performance**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of your marketing campaign metrics can be tricky,
    especially without industry benchmarks as a reference. To quickly assess whether
    your KPIs are on target, start by comparing them to established benchmarks.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a recent comparison of email marketing benchmarks: [https://www.webfx.com/blog/marketing/email-marketing-benchmarks/](https://www.webfx.com/blog/marketing/email-marketing-benchmarks/).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate and review the average performance across these metrics:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This yields the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Based on these KPIs, the following are some observations and possible explanations
    that we may consider addressing in our next iteration of the email campaign:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '**Low open rates**: This may suggest that the email subject lines are not sufficiently
    engaging.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Click-through rates**: Some recipients view the emails but do not click through
    at expected rates, possibly due to less compelling content or offers.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modest conversion rates**: This indicates a need for stronger calls to action
    or improvements in the effectiveness of landing pages.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rising unsubscribe rate**: This could point to issues with the frequency
    or relevance of the content.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to these KPIs, assume we’ve collected feedback from email recipients
    who responded directly to this email with their questions and concerns. We can
    also gather information, for example, by employing sentiment analysis techniques
    discussed in *Chapter 5*, allowing us to determine the core topics and sentiments
    from product reviews and social media posts from customers who read our emails.
    Here’s an overview of the positive feedback that we will assume to have received:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Appreciation for **sustainable practices**: “I love that your kitchenware is
    made from recycled materials.”'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Approval of **ethical manufacturing**: “It’s reassuring to see a brand commit
    to fair wages and safe working conditions.”'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enthusiasm for the **product range**: “The stainless steel cookware looks fantastic!”'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the highlights of the negative feedback:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Desire for more **product variety**: “I wish that you advertised more color
    options for the product lines.”'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Questions about **product care**: “I’m concerned about how I can care for these
    products to ensure they last longer.”'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concerns over **price points**: “The products are great, but they’re a bit
    pricey compared to non-eco-friendly alternatives.”'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the positive comments validate our campaign’s core messages, the criticisms
    and questions provide actionable insights for our future email messaging. Using
    this feedback, in the next section, we will hone in on how the negative critiques
    can be exploited to refine our email marketing strategies and improve our KPIs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Iterative refinement'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After reviewing the initial metrics and customer feedback from the previous
    sections, it’s clear that while some aspects of the campaign resonate well, there
    are key areas needing improvement. The next step is to refine our FSL prompts
    based on the initial metrics, address the specific criticisms from customer feedback,
    better meet customer expectations, and enhance the overall effectiveness of the
    campaign. The following are the new elements of the prompt, with commentary included
    after each element of the prompt, highlighting their value to our task:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This prompt sets the tone for the email, capturing the key elements from our
    previous FSL attempt but also emphasizing the importance of an engaging subject
    line in direct response to the low open rate observed in the initial KPI metrics.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This response caters to feedback desiring more variety. It emphasizes the range
    of colors available, aiming to attract customers looking to personalize their
    purchases.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Addressing customer concerns about maintenance, this prompt reassures customers
    that the products are not only sustainable but also easy to care for, enhancing
    their appeal by ensuring longevity.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This prompt directly tackles the issue of price sensitivity noted in the feedback.
    It reframes the cost as an investment in quality and sustainability, arguing for
    long-term value and environmental impact.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After executing the API recall, we get the following result:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After deploying these updates emails on April 7, assume we collect the next
    seven days of KPI data and observe the following updated trends:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This gives us the following plot, where we observe significant changes in the
    KPIs after FSL refinements:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_11.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Email campaign KPIs after campaign changes and improvements were
    made in response to customer feedback'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this, we have the following average KPIs:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'By comparing these metrics to the previous ones, we see significant improvements
    across all areas. While attributing the exact origins of these improvements is
    not possible without techniques such as A/B testing (*Chapter 6*) or causal inference
    (*Chapter 3*), one can still speculate what may be behind these improved KPIs:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '**Open rate** improved significantly, which may reflect the more engaging subject
    lines, including “Revolutionize your Kitchen…”'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Click-through rate** saw an increase, which may be attributed to the inclusion
    of requested features like product variety and detailed care instructions, which
    increased customer engagement'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversion rate** increased, indicating that the email content was not only
    more engaging but also more effective at convincing customers of the value and
    relevance of the products, especially after addressing price concerns'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsubscribe rate** decreased, reflecting higher content satisfaction'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: Continued feedback integration'
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in utilizing FSL for email marketing campaigns is to establish
    a robust system for continuous feedback integration. This approach ensures that
    the campaign remains dynamic and responsive to the evolving needs and preferences
    of customers, as well as broader market trends. By integrating continuous feedback,
    the campaign not only maintains relevance but also strengthens customer engagement
    and promotes brand loyalty over time.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Effective feedback integration requires mechanisms that allow customers to share
    their thoughts and experiences easily. This includes embedding quick survey links
    directly in emails, enabling direct email responses, and engaging with customers
    through social media interactions related to the campaign. These channels facilitate
    the flow of information from customers back to marketers, providing valuable insights
    that can be used to refine and improve the campaign continuously.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '**Example feedback collection strategies**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are additional ways to collect direct customer feedback:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '**Live chat feedback**: Implement live chat on your website to allow real-time
    interactions and immediate feedback.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive content**: Use quizzes and polls in your emails or on digital
    platforms to make feedback collection engaging.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback QR codes**: QR codes on products or ads can link to feedback forms
    so customers can easily respond on their devices.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To effectively analyze and utilize ongoing feedback, setting up a real-time
    feedback dashboard can be immensely beneficial. This dashboard can serve as a
    central hub for monitoring and analyzing feedback alongside standard marketing
    KPIs, providing a comprehensive view of campaign performance. To build such a
    dashboard, consider using software solutions like Tableau, Microsoft Power BI,
    or Google Data Studio, which offer powerful tools and intuitive interfaces for
    creating interactive and real-time data visualizations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps for creating your own feedback dashboard:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '**Select a dashboard tool**: Choose a platform that best fits your technical
    capabilities and budget. Tableau, Microsoft Power BI, and Google Data Studio are
    popular choices due to their robust features and scalability.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Integrate data sources**: Connect the dashboard tool to the data sources
    that gather your marketing KPIs and feedback. This may include email campaign
    management tools, social media analytics, and customer feedback systems.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Design the dashboard**: Create visualizations that clearly display the key
    metrics you need to track. Customize the dashboard to highlight trends, spikes,
    and dips in campaign performance.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Schedule regular reviews**: Set up regular intervals, bi-weekly or monthly,
    for example, to review the dashboard insights. Use these sessions to assess the
    effectiveness of recent changes and to plan further strategic adjustments based
    on the data insights.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Keeping your emails out of the spam folder**'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To maximize the effectiveness of your email campaigns, it’s essential to prevent
    your messages from landing in the spam folder. Here are three key strategies for
    achieving this:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Set up email authentication**: Implement standards like SPF, DKIM, and DMARC
    to authenticate your emails. This helps establish trust with email providers to
    reduce the risk of your emails being marked as spam.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintain list hygiene**: Regularly clean your email list by removing inactive
    subscribers and ensuring that all recipients have opted in to receive your emails.
    This not only boosts engagement but also helps protect your sender reputation.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize email content**: Other optimization strategies include personalizing
    your emails using FSL, limiting the number of links, and maintaining a balanced
    image-to-text ratio.'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the potential of FSL and its promise for refining AI-driven
    marketing strategies to enhance brand presence. Building on the principles introduced
    through ZSL, we explored how FSL leverages a limited dataset to enable rapid adaptation
    of AI models to new tasks. This is crucial in the fast-paced marketing domain,
    where aligning quickly with evolving consumer preferences and market trends can
    have a significant impact on a brand’s relevance and engagement.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: While FSL focuses on quick adaptability using minimal examples, transfer learning
    complements this by applying pre-trained models fine-tuned for specific tasks,
    thereby minimizing the need for extensive retraining. The chapter emphasized practical
    strategies combining these methodologies to optimize your marketing efforts. Through
    approaches like the MAML approach, we demonstrated how you can use meta-learning
    frameworks for marketing.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: As we proceed, the next chapter will introduce the concept of **retrieval-augmented
    generation** (**RAG**). We will explore how RAG can dynamically produce content
    that reflects the latest available data by integrating generative models with
    advanced information retrieval techniques. This approach not only increases the
    relevance of the content generated but also enhances precision in consumer targeting,
    making your marketing efforts significantly more effective. The upcoming discussion
    will cover the technical setup of a knowledge retrieval system and practical applications
    of RAG in marketing, through which we aim to provide you with robust tools for
    writing precisely targeted marketing content that resonates with your current
    and potential customers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
