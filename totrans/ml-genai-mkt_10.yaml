- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enhancing Brand Presence with Few-Shot Learning and Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explores the capabilities of **few-shot learning** (**FSL**) and
    its value in enhancing brand presence through tailored marketing strategies. Building
    on the insights from **zero-shot learning** (**ZSL**) covered in the previous
    chapter, we now focus on how FSL, by utilizing a limited set of examples, enables
    rapid and effective adaptation of AI models to new tasks. This approach is particularly
    valuable in marketing, where the ability to swiftly adjust content to align with
    evolving consumer preferences and market trends is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we will introduce some of the fundamental concepts of FSL in the
    context of meta-learning as an underlying technique that facilitates quick learning
    from small datasets. We will then explore the synergy between FSL and transfer
    learning using practical examples; while FSL is effective at quickly adapting
    to new tasks with few examples, transfer learning complements this by utilizing
    pre-trained models that are fine-tuned for specific tasks, reducing the need for
    extensive model retraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter highlights how combining FSL with related strategies can optimize
    marketing efforts, making them more responsive and efficient. This chapter equips
    you with the understanding to:'
  prefs: []
  type: TYPE_NORMAL
- en: Grasp the core concepts of FSL and its similarities to and differences from
    transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize the practical benefits of employing FSL to adapt quickly to new marketing
    conditions with limited data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply FSL and transfer learning techniques to real-world marketing scenarios
    to enhance brand messaging and consumer engagement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating FSL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the marketing domain, the agility to quickly tune content strategies to meet
    the evolving needs of a brand is invaluable. FSL stands out for its capacity to
    effectively learn and perform tasks with limited input data. While ZSL is designed
    to work without any specific examples of the new classes during inference, relying
    on a generalized, abstract understanding of the task derived from previously learned
    tasks, FSL uses a small number of examples to adapt to new tasks. This adaptation
    often relies on a more direct application of learned patterns and can be fine-tuned
    with data, making it particularly effective when some example data is available.
    This efficiency enables marketers to rapidly test new strategies, such as personalizing
    email campaigns for different customer segments or quickly adapting social media
    content to reflect emerging trends, without the long lead times associated with
    gathering and training on extensive datasets. For instance, a marketing manager
    might use FSL to generate tailored marketing copy that aligns with a company’s
    rebranding initiative, as we will discuss in the *Applying FSL to improve brand
    consistency section*, even when there are limited examples of such rebranded content
    available.
  prefs: []
  type: TYPE_NORMAL
- en: As we further explore the utility of FSL for enhancing brand presence, it’s
    essential to build on our conceptual understanding of **Generative AI** (**GenAI**)
    introduced in the previous chapter. FSL hinges on the idea that intelligent systems
    can learn new concepts or tasks with only a small number of training examples,
    drawing heavily from prior knowledge and the application of sophisticated meta-learning
    algorithms. This discussion will extend your foundational knowledge of this concept,
    bridging the gap between high-level concepts and practical applications that are
    useful for understanding the hands-on examples given later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, FSL is often facilitated by **meta-learning**, an approach that
    enables models to quickly adapt to new tasks by using learnings from a variety
    of prior tasks. However, meta-learning is just one possible approach, and others
    such as metric-based learning, whereby models are trained to compare new instances
    against a few labeled examples using a learned metric or distance function, can
    also be used. Given its broad applicability and effectiveness as an FSL approach,
    let’s explore meta-learning further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding FSL through meta-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FSL often involves meta-learning, or “learning to learn,” whereby models are
    designed to quickly adapt to new tasks based on learnings from a wide array of
    previous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is particularly crucial in marketing, where consumer behavior and preferences
    can be dynamic, requiring models that can pivot quickly without extensive retraining.
    Meta-learning frameworks in FSL train models on a variety of tasks, enabling them
    to develop a generalized understanding or an internal representation that can
    efficiently map to new tasks with minimal additional input. Some of the key components
    of meta-learning are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task variety**: Meta-learning algorithms are exposed to a wide variety of
    learning tasks during the training phase. This exposure helps the model learn
    more robust features that are not overly specific to one task but are general
    enough to be applicable across a spectrum of future tasks. This is analogous to
    a marketing team working across different campaign types like email or social
    media and learning to identify core elements that predict success regardless of
    the specific product or audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid adaptation**: The primary goal of meta-learning is to enable rapid
    learning on new tasks. This is achieved through the model’s ability to fine-tune
    itself to new conditions with minimal training data. For instance, if a model
    trained in a meta-learning framework is faced with a new product launch, it can
    quickly adjust its parameters to optimize marketing content for the product’s
    target demographic based on its prior knowledge of similar products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization techniques**: Meta-learning involves special training schemes
    such as episodic training, whereby the model undergoes simulated training episodes.
    Each episode involves learning from a small set of data and then testing on a
    new set of data from the same task. This trains the model to generalize well from
    small data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at these fundamentals in action in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing model-agnostic meta-learning in marketing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider an example meta-learning model using a simple adaptation of **model-agnostic
    meta-learning** (**MAML**) and how it can be applied to optimize marketing strategies.
    MAML works by optimizing a model’s parameters so that a small number of gradient
    updates will lead to fast learning on a new task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learn more about MAML**'
  prefs: []
  type: TYPE_NORMAL
- en: 'MAML is a widely recognized meta-learning algorithm introduced by Finn et al.
    in 2017 in the paper *Model-Agnostic Meta-Learning for Fast Adaptation of Deep
    Networks*. For a deeper dive into the topic, including interactive examples, check
    out the resource: https://interactive-maml.github.io/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simplified implementation, we’ll simulate a single marketing task as
    a training input as a proxy for the multiple tasks that would be considered in
    a full MAML deployment. We will then train a simple neural network model on the
    task and then use the result of our meta-training to improve the model’s adaptability
    on a different task. The following diagram provides an illustration of the mechanics
    of a meta-learning framework, as well as what it might look like when applied
    to multiple marketing tasks as examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Key components of a possible MAML framework in the context of
    marketing campaigns'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.10](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.10)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model-agnostic techniques are designed to be broadly applicable across a range
    of model architectures and tasks. To better understand the mechanics of the MAML
    approach, we will break down its application into several functions that are simplifications
    of what would be involved with a full implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have a function, `generate_task`, that creates simulated marketing
    tasks with its own set of coefficients representing campaign data, which, in this
    case, is represented as a quadratic relationship with noise. The randomly generated
    coefficients for each task dictate how input data (such as email campaign length,
    budget, or reach) affect the outcome (such as engagement or conversions). Then
    the `task` function computes the campaign’s success metrics based on the input
    and the coefficients. Note that we provide seed values so that your output should
    be consistent with what’s shown here, but your results may still vary due to randomness
    from parallel processing or GPU usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we have a function, `train_model_on_task`, that trains the model on one
    of the tasks generated by the previous function by performing short bursts of
    training on the task and using the `Adam` optimizer to update the model’s weights.
    Then TensorFlow `GradientTape` is used for automatic differentiation, calculating
    the gradients needed to minimize the loss, which measures how well the model’s
    predictions match the actual outcomes of the campaign:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then we have the `meta_train_model` function, which is at the heart of meta-learning.
    This handles the meta-training process by optimizing the model’s ability to quickly
    adapt to new tasks after minimal exposure. In practice, the model is first trained
    on a task, and then it is evaluated on new data from the same task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss calculated from this evaluation guides adjustment of the model’s initial
    parameters so that just a few updates lead to significant improvements on new
    tasks. Note that in a full implementation, we would likely use a `reset_weights`
    argument so that after each task the model’s weights are reset to their state
    before the task-specific training and a `meta_optimizer` that updates the model’s
    initial parameters based on the task performance. This ensures that the model
    does not overfit to a particular task and retains its ability to generalize across
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To illustrate this simplified example in action, we can first initialize a
    very simple neural network with two layers to apply our meta-training function.
    Here, there is first a dense layer with 10 neurons and a sigmoid activation and
    then a final dense layer with just one neuron to capture the model’s prediction
    for the success of a marketing campaign based on the input feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Simple neural network model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now generate a proxy for a complex task using your quadratic function
    and plot the model’s performance on this task before training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot shows the performance of the model before undergoing meta-training.
    As we can see, the model is initially unable to accurately predict the true values,
    indicating the need for further training and optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Model performance before meta-training on the task'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can perform meta-training using our algorithm and then plot the model’s
    performance again on the same task after meta-training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Model performance after meta-training on the task'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figures 10.3* and *10.4*, the model initially fails to grasp any
    quadratic relationship in the data. After meta-training, however, the predictions
    improve. While this is a simplified implementation including only a simple task,
    this implementation framework gives a baseline for the mechanics of MAML as a
    tool for preparing your model for FSL.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming challenges in FSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FSL aims to make the most out of minimal data, but this can come with inherent
    challenges such as overfitting and poor generalization. **Overfitting** occurs
    when a model, trained on a very small dataset, learns the noise and irrelevant
    details to the extent that it negatively impacts the performance on new data.
    To mitigate this, regularization techniques such as L2 regularization and dropout
    can be used to simplify the model complexity of neural networks, helping prevent
    the model from learning overly complex patterns that do not generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization techniques to prevent overfitting**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two common regularization techniques in neural networks are:'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization (weight decay) adds a penalty to the loss function based on
    the squared magnitude of the model’s weights. This discourages large weights,
    leading to simpler models that generalize better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learn more here, including how to compare learning curves to evaluate the impact
    of regularization: [https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization](https://developers.google.com/machine-learning/crash-course/overfitting/regularization).'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout randomly deactivates (“drops out”) neurons during training to prevent
    the model from becoming too reliant on specific pathways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learn more in the seminal paper *Dropout: a simple way to prevent neural networks
    from overfitting* by Srivastava et al.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, we could add these types of regularization while building a neural
    network model suitable for a regression task. As an illustration, the model is
    designed with a moderately complex architecture and includes several layers with
    both types of regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5\. Model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: 'The model embeds L2 regularization within its dense layers to curb the weight
    size and encourage simpler models. A dropout of 0.3 is also applied after each
    dense layer, reducing overfitting by deactivating random neuron outputs during
    training. This regularization pattern is applied across the hidden layers to promote
    the model’s ability to generalize. The following figure provides a visualization
    of how dropout randomly deactivates neurons during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Dropout neural net model. Left: A standard neural net with two
    hidden layers. Right: An example of a thinned net produced by applying dropout
    to the network on the left. Crossed units have been dropped. (source: https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: As alluded to earlier, **generalization** is another critical challenge. In
    the field of image processing, data augmentation techniques like image rotation,
    cropping, or color adjustment are often employed to artificially enhance the dataset’s
    size and variability, helping the model learn more general features that perform
    better on new tasks. In NLP applications, data augmentation techniques such as
    those presented in *Chapter 5* can be employed, along with others such as synonym
    replacement, in order to increase robustness and aid in better generalization
    across different contexts.
  prefs: []
  type: TYPE_NORMAL
- en: A significant challenge in FSL is also the difficulty of achieving robust performance
    with limited training data. Depending on the complexity of the pattern being learned,
    models may require larger datasets to generalize well and therefore struggle to
    learn useful patterns without it, leading to poor performance on new tasks. Transfer
    learning can be a powerful, complementary strategy to address this challenge.
    By starting with a model pre-trained on a large and diverse dataset, you can leverage
    the learned features and fine-tune the model on your specific FSL task. This approach
    enables rapid adaptation to new tasks with minimal data while retaining a broad
    knowledge base, complementing FSL by providing a rich initial set of features
    that need only slight adjustments rather than learning from scratch. This powerful
    approach has its limitations and challenges as well; however, these are topics
    we will discuss in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing FSL techniques**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key strategies for enhancing the efficacy of FSL models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting prevention**: Implement regularization techniques to help the
    model focus on simpler, more general patterns, rather than memorizing specific
    noise in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing generalization**: Employ data augmentation to expand the training
    dataset. For text, approaches like paraphrasing or injecting synonyms help the
    model learn language variations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leveraging transfer learning**: Utilize a model pre-trained on a comprehensive
    dataset and then fine-tune it for your specific FSL task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning can enhance how marketing professionals leverage AI by enabling
    the more effective use of pre-trained models on new tasks with only minor adjustments.
    While FSL uses a set of examples from the new task for quick adaptation, transfer
    learning focuses on repurposing an existing model without needing additional examples
    from the new domain.
  prefs: []
  type: TYPE_NORMAL
- en: This approach capitalizes on the knowledge that models gain from large-scale
    data in previous tasks, applying it to enhance marketing efforts in completely
    different areas without the overhead of retraining the model from scratch. Put
    differently, FSL improves model adaptability using very limited data examples,
    whereas transfer learning excels in environments where the relationship between
    past and current tasks is strong but the availability of large enough labeled
    datasets for training a base model for the new task is difficult or costly to
    acquire.
  prefs: []
  type: TYPE_NORMAL
- en: An additional advantage of transfer learning over FSL can come from a cost perspective.
    For example, when using paid API services for state-of-the-art GenAI models, pricing
    can be related to the input size used to generate the response. In the context
    of NLP applications, this is often captured by the token count. When fine-tuning
    via transfer learning, the base model only needs to be adjusted once. This means
    that the token count paid for during the fine-tuning phase does not recur with
    every usage of the model, as opposed to FSL, which may require the same relevant
    examples to be fed in each time.
  prefs: []
  type: TYPE_NORMAL
- en: With transfer learning, the initial cost of fine-tuning the model can be offset
    by the lower costs for each subsequent inference, as no additional examples need
    to be fed into the model. Transfer learning therefore becomes cost-effective when
    the number of inferences after fine-tuning exceeds a certain threshold, making
    it cheaper in the long run compared to FSL.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, here is a breakdown to illustrate the potential cost differences
    between FSL and transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FSL API costs**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Base cost**: Cost for tokens used in the prompt (Cost per token × Number
    of tokens in prompt)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional cost per example**: Cost per token × Number of tokens per example'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning API costs**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Initial fine-tuning cost**: One-time cost for adjusting the pre-trained model
    using a larger set of example data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost per inference**: Cost for each inference using the fine-tuned model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When to use transfer learning over FSL**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to FSL, transfer learning can be more advantageous under the following
    circumstances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High frequency of usage**: The more frequently the model is used, the quicker
    the initial fine-tuning cost is amortized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stable task requirements**: New tasks must be similar enough for the fine-tuned
    model to perform well without further adjustment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex pattern adaption**: While requiring more extensive data, it can adapt
    deeper model layers to learn more complex patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mechanics of transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, transfer learning and its fine-tuning processes are built upon
    similar foundations established by techniques like ZSL and FSL. Both ZSL and FSL
    are designed to apply pre-existing knowledge to new tasks. However, they differ
    primarily in the approach to adaptation. ZSL hypothesizes about *unseen tasks*
    based on learned abstract concepts without any specific examples, whereas FSL
    uses a *handful of examples* to guide the adaptation. Transfer learning extends
    these concepts by utilizing a base of extensively trained models that can be specifically
    fine-tuned to a new task, providing a practical balance between the broad generalization
    of ZSL and the rapid adaptation characteristic of FSL. Transfer learning is often
    the preferred approach when a robust, pre-trained model exists and there are examples
    from a highly related domain, but a lack of examples specifically related to the
    new task.
  prefs: []
  type: TYPE_NORMAL
- en: At a more technical level, fine-tuning in transfer learning involves subtle
    yet significant adjustments to the model’s parameters that are already well-trained
    on large, diverse datasets. This fine-tuning adapts the learned features to the
    specifics of a new, related task, often involving adjustments to the deeper, more
    discriminative layers of the model. Such modifications enable the model to maintain
    its generalization capabilities while optimizing for performance on specific tasks
    within the new domain. This is different from FSL, where fine-tuning aims to quickly
    adapt the model using very few examples by making minimal adjustments, often only
    to the model’s final layers or via prompt engineering where there can be no adjustments
    to the trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates the differences between ZSL, FSL, and transfer
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Key differences between typical ZSL, FSL, and transfer learning
    implementations and their mechanics'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning using Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, in order to provide variety beyond applications just in the
    field of NLP, we will present a framework for how transfer learning can be implemented
    in an image classification task. This example will present a framework for how
    you can adapt a pre-trained image recognition model to a new marketing-relevant
    challenge of your choice, such as identifying desired product features.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the transfer learning process involves finding a pre-trained
    model that has been extensively trained on a large and diverse dataset. Here,
    we will use the VGG16 model, known for its robust performance in image recognition.
    VGG16 is a convolutional neural network that achieves this high performance through
    training on the ImageNet dataset using over a million images in order to classify
    them into a thousand image categories, as detailed by Simonyan and Zisserman in
    their 2014 paper, *Very deep convolutional networks for large-scale image recognition*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll summarize the full architecture of this model using `model.summary`
    to give us a better understanding of its layers and components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B30999_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Model architecture summary'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown here, the VGG16 architecture is a deep neural network consisting of
    multiple layers designed to process and transform the image into a form that is
    increasingly abstract and useful for classification tasks. Here’s a simple breakdown
    based on the preceding summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This accepts images of size 224 x 224 pixels, which is a standard
    dimension for many image processing tasks. Each image has three color channels
    (Red, Green, and Blue), which explains the `3` in the shape given in *Figure 10.8*.
    The `None` given in each layer at the start is effectively a placeholder for the
    batch size, allowing for any number of images to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional layers (Conv2D)**: These perform the core of the work in the
    network. VGG16 has multiple convolutional layers stacked on top of each other,
    each with a set number of filters that detect different features in the image
    at various levels of granularity. For example, the first convolutional layer might
    detect edges, while deeper layers might identify more complex patterns like textures
    or specific objects. The progression in the number of filters from 64 to 512 reflects
    an increase in the complexity of features being detected in the image as it moves
    through the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max pooling layers (MaxPooling2D)**: These are interspersed with the convolutional
    layers and serve to reduce the spatial dimensions (width and height) of the input
    for the next convolutional layer. For instance, a pooling layer following a 224
    x 224 convolutional layer output reduces it to 112 x 112, thus reducing the computation
    required and helping in the detection of dominant features that are invariant
    to small shifts and rotations in the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully connected layers (Dense)**: Near the end of the network, the flattened
    output from the convolutional layers is fed into these densely connected layers.
    These are used to combine all features from the prior convolutional layers to
    eventually classify the image into one of the 1,000 categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer (predictions)**: The final layer outputs the probabilities of
    the image belonging to each of the 1,000 classes trained in the ImageNet dataset.
    The class with the highest probability is taken as the prediction of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a visualization of this summary taken from a medical research
    article by Yang et al. ([https://www.nature.com/articles/s41598-021-99015-3](https://www.nature.com/articles/s41598-021-99015-3)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Visualization of the architecture of the VGG16 deep neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now adapt this model for transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the model into memory again, but this time with the option `include_top=False`
    in order to truncate the model after the last convolution layer. This will move
    the top layers responsible for classification into predefined categories and allow
    the model to serve as a feature extractor, which means it outputs a complex feature
    map that represents the input image’s key characteristics but doesn’t output a
    specific category prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then adapt this feature-rich model to our new task of product classification
    by adding our own classification layers that will be trained on top of the feature
    extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the model can be fine-tuned for transfer learning using new, task-specific
    data. This involves setting the pre-trained base model layers to non-trainable,
    thereby freezing the weights of the pre-trained layers, which is generally advised
    to avoid having the initial phase of training forget the majority of the base
    model’s initial feature extraction weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Self-guided exercise: Fine-tuning VGG16 on your data**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To apply transfer learning using the VGG16 model to classify images of product
    features, follow these steps:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Collect images**: Gather images of products with features your company aims
    to classify. Sources can include online catalogs, social media, or direct captures
    of physical inventory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prepare the data**: Resize images to 224 x 224 pixels to match the VGG16
    input size and normalize pixel values to range `[0, 1]`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Train the model**: Use the compiled model and fit your images (`new_data`)
    and their binary labels `[0, 1]`, for instance using:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**Evaluate and fine-tune**: Based on performance, consider adjusting your learning
    rate or gradually unfreezing layers in the base VGG16 model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning using API services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transfer learning through API services offers a practical solution for marketing
    professionals who wish to utilize cutting-edge AI without the complexities of
    a local implementation. Some of the practical benefits of using an available API
    service for transfer learning are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access to advanced models**: Continuous updates and optimizations of API-accessible
    models ensure the use of cutting-edge technology that is more advanced than what
    may be publicly available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational efficiency**: Offloading the computational demands to cloud-based
    services removes the need for sophisticated hardware that may be inaccessible
    for individuals or small businesses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified parameter management**: This refers to the automatic handling
    of learning rates, gradient adjustments, and other complex training parameters
    that can affect model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To apply transfer learning effectively, it’s crucial that the fine-tuning dataset
    mirrors the unique characteristics of your brand. This could include customer
    feedback or vetted marketing copy and product descriptions. If a proprietary dataset
    is unavailable, publicly accessible data such as reviews or social media posts
    about similar products or services could be used. The key is to compile this data
    into a single file, typically in CSV format, in a location that is accessible
    to the API. Each entry then needs to be clearly labeled according to its expected
    label or output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example input in CSV format that gives the model a deeper
    understanding of what sustainability means in the context of our specific brand,
    setting the stage for what will be discussed in the hands-on example in the next
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can initiate the model fine-tuning using OpenAI’s API with the GPT-4
    model, replacing `openai.api_key` with your actual key and `path/to/your/dataset.csv`
    with a path to the dataset’s location. Before executing this code, remember to
    consult the latest OpenAI API documentation ([https://beta.openai.com/docs/](https://beta.openai.com/docs/))
    for any updates or changes in the API usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Overcoming challenges in transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While transfer learning can be a powerful approach for fine-tuning pre-trained
    models to better perform better on new tasks, it is not without its challenges
    and limitations. The following are a couple of key challenges to consider when
    applying transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model drift**: In the context of transfer learning, model drift refers to
    the gradual decline in a model’s accuracy as the data it was fine-tuned on becomes
    less representative of the current environment or trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, imagine using a model pre-trained on marketing data from 2019 to
    predict consumer preferences for 2024\. Initially, the model may perform well
    after being fine-tuned via transfer learning with more recent 2020 data. However,
    as consumer behavior shifts due to reasons such as new social media platforms
    like TikTok gaining in popularity or significant changes in economic conditions,
    the model may start recommending outdated messaging strategies.
  prefs: []
  type: TYPE_NORMAL
- en: To address model drift, one needs to continuously update the model with recent
    data from the target domain and monitor its performance closely. This can be achieved
    by continuous learning techniques that allow the model to adjust dynamically as
    new data comes in. Further resources on continuous learning are given in the info
    box in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain mismatch**: Domain mismatch in transfer learning occurs when the source
    domain (where the model was initially trained) and the target domain (where the
    model is fine-tuned) differ significantly. In transfer learning, this means the
    pre-trained model’s knowledge may not generalize well to the new domain, leading
    to poor performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, consider a model pre-trained on English-language customer reviews
    from an e-commerce platform, which is then fine-tuned to analyze reviews in Japanese
    for a local market. Even after fine-tuning, the model may struggle to capture
    cultural nuances and linguistic differences, leading to incorrect sentiment analysis.
    This may be because the pre-trained model’s reliance on patterns learned from
    English data makes it less effective when applied to Japanese reviews where the
    expressions and cultural references are different.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, a more appropriate approach would be to fine-tune the model
    with a carefully curated dataset of Japanese customer reviews that specifically
    captures the specific linguistic and cultural nuances of the local market.
  prefs: []
  type: TYPE_NORMAL
- en: '**Staying ahead of model drift**'
  prefs: []
  type: TYPE_NORMAL
- en: To maintain the accuracy of your transfer learning models, especially as consumer
    behavior evolves, continuous learning is key.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous learning is often implemented within the practices of MLOps. Learn
    more about MLOps principles and best practices at [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/ai-ml).
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced the fundamentals and techniques behind how FSL and transfer
    learning can adapt pre-trained models to better address new marketing tasks, we
    will now demonstrate the importance of FSL through a hands-on marketing example.
  prefs: []
  type: TYPE_NORMAL
- en: Applying FSL to improve brand consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous exploration of ZSL in *Chapter 9*, we demonstrated how a pre-trained
    model like GPT-4 could generate marketing copy for an e-commerce brand launching
    eco-friendly products. This ZSL approach, while powerful, primarily relied on
    the model’s ability to infer context and content from generalized pre-training
    using prompts emphasizing terms like *sustainable* and *eco-friendly*.
  prefs: []
  type: TYPE_NORMAL
- en: As the examples in this section will show, while ZSL provides a solid foundation
    for generating brand-relevant content, it often lacks the precision required for
    capturing the deeper, more nuanced aspects of a brand’s ethos. This is particularly
    true for brands whose identity is heavily tied to specific practices or principles
    that may not be well represented in the generalized training of the **large language
    model** (**LLM**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps for effective FSL in marketing campaigns**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt refinement and execution**: Begin with a basic prompt like one that
    would be used for ZSL, but now add a few examples of desired outputs or contexts
    that reflect the brand’s unique aspects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output analysis**: Evaluate the content to ensure it maintains the necessary
    brand consistency. Note deviations from these expectations as they provide insights
    into areas needing further examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative refinement**: Based on the feedback and performance of the initial
    outputs, refine the examples and prompts iteratively, continuing until you achieve
    your desired marketing KPIs (see *Chapter 2*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continuing our scenario from the section on *Zero-shot learning for marketing
    copy* in *Chapter 9*, we revisit our journey with the sustainability-focused e-commerce
    brand that we introduced earlier. Now, in response to evolving market trends and
    corporate directives, the company known for its eco-friendly kitchenware is undergoing
    a rebranding to align with the growing consumer demand for social responsibility.
    Recent insights reveal that consumers are increasingly interested in supporting
    brands that not only offer sustainable products but also demonstrate a tangible
    commitment to fair labor practices and positive community impact. This shift is
    particularly pronounced among affluent younger consumers who value ethical production
    and are willing to pay a premium for products that make a real difference in the
    lives of workers and communities. Furthermore, this demographic wants their products
    not just to have eco-friendly packaging but for them to be deemed “zero waste.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate how GPT-4 can effectively capture our newly defined sustainability
    campaign goals by enhancing output content through prompt engineering. This includes
    the subtle clarifications around sustainability mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: A commitment to fair labor practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero waste packaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Active community engagement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will exemplify this through a case study aimed at boosting an email marketing
    campaign. This involves iterative refinement of prompts, closely tied to monitoring
    key performance indicators previously discussed in *Chapter 2*, to ensure alignment
    with our rebranded marketing objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking with ZSL and FSL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before integrating FSL into our email marketing campaign, it’s important to
    benchmark the effectiveness of ZSL at capturing the refined brand strategy for
    FSL is introduced. Here, we will compare a baseline response generated using ZSL
    that’s devoid of specific examples that align with our rebranded sustainability
    focus. We’ll define a function to execute the API completion and then illustrate
    how ZSL performs without the nuanced context of our updated campaign goals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, while the response is clear, it is too generic and lacks the
    specificity and context needed to meet the specific goals of our rebranding initiative.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the value FSL can provide, here, we revisit the exact same base prompt
    as earlier, but also include context specific to our updated campaign goals via
    FSL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding text, the aspects of the LLM-generated content that organically
    relate to our specific goals—environmental sustainability, fair labor practices,
    and community engagement—are highlighted for clarity. This comparison with ZSL
    underscores the capability of FSL to produce content that is more tailored and
    relevant to the strategic nuances of our rebrand by leveraging the initial contextual
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an email marketing campaign
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s explore the application of FSL in enhancing the personalization
    and relevance of email marketing campaigns, significantly boosting both customer
    engagement and conversion rates. The following is a concise guide on how to utilize
    FSL to optimize email campaigns effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial email creation**: Generate the initial batch of emails using FSL,
    specifically focusing on your core marketing goals—in this case, sustainability
    and ethical practices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collect initial metrics and responses**: Analyze the initial reactions from
    customers to understand what aspects of the content resonate the most. This analysis
    is crucial in identifying successful elements and areas that may require additional
    refinement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative refinement**: Refine the content of the emails based on the engagement
    metrics and feedback received from customers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continued feedback integration**: Continuously integrate new insights to
    keep the email marketing campaign dynamically aligned with evolving customer preferences
    and market trends.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let’s examine what these steps entail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Initial email creation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in employing FSL in an email marketing campaign is to clearly
    define the campaign’s objectives. For our eco-friendly e-commerce brand, the objective
    might be to launch a new line of eco-friendly kitchenware that aligns with the
    rebrand goals. Let’s consider our objective for this scenario to be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: With the campaign’s objective clearly defined, the next step involves creating
    prompts that instruct the AI on the content that’s appropriate for the email message.
  prefs: []
  type: TYPE_NORMAL
- en: '**Structuring the initial email for your marketing campaign**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following general structure is helpful in creating an effective initial
    message for your email marketing campaign that increases engagement and conversion
    with your audience:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**: Greet the customer and introduce the new product line enthusiastically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Body**: Provide detailed information about the core aspects of the product
    – in this case, the sustainability and ethical manufacturing aspects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call to action**: Include a call to action that encourages the recipient
    to take specific steps such as making a purchase or visiting a website.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Closing**: Conclude with a warm closing that reinforces the brand’s commitment
    to the core aspects of the product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll define a starting prompt for the model, which will be used to generate
    the initial batch of emails. Similar to the earlier product description example,
    each element of the `fsl_prompts` is designed to progressively build a narrative
    around the brand’s commitment to sustainability, ethical practices, and community
    involvement, as required by the corporate rebrand, in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: The first pair of prompts ensures the content appropriately emphasizes the eco-friendly
    and zero-waste aspect of the kitchenware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second set of prompts addresses the context expected when discussing the
    ethical labor practices involved in the production process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final two prompts tie the product to community engagement efforts, linking
    purchases to initiatives that are important to socially responsible consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To ensure that the generated content aligns with our campaign goals, we’ll
    also adjust our base prompt to be more specific to an email campaign and increase
    the `max_tokens` to reflect the longer content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following email content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Collect and analyze initial metrics and responses'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the deployment of our email campaign with the newly created content,
    it is crucial to assess its effectiveness using KPIs. For this evaluation, we
    assume we already have representative data from the first seven days post-launch
    that will allow us to examine the metrics of open rate, click-through rate, conversion
    rate, and unsubscribe rate. This analysis helps us identify areas for improvement
    by visualizing these KPIs over time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following graph, we can observe the trends in open rates, click-through
    rates, conversion rates, and unsubscribe rates over the first week following the
    campaign launch. This allows us to quickly identify patterns, which can be critical
    for diagnosing issues with the campaign’s content or delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Email campaign KPIs during the first week after launch'
  prefs: []
  type: TYPE_NORMAL
- en: '**Benchmarking your email campaign performance**'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of your marketing campaign metrics can be tricky,
    especially without industry benchmarks as a reference. To quickly assess whether
    your KPIs are on target, start by comparing them to established benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a recent comparison of email marketing benchmarks: [https://www.webfx.com/blog/marketing/email-marketing-benchmarks/](https://www.webfx.com/blog/marketing/email-marketing-benchmarks/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate and review the average performance across these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on these KPIs, the following are some observations and possible explanations
    that we may consider addressing in our next iteration of the email campaign:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low open rates**: This may suggest that the email subject lines are not sufficiently
    engaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Click-through rates**: Some recipients view the emails but do not click through
    at expected rates, possibly due to less compelling content or offers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modest conversion rates**: This indicates a need for stronger calls to action
    or improvements in the effectiveness of landing pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rising unsubscribe rate**: This could point to issues with the frequency
    or relevance of the content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to these KPIs, assume we’ve collected feedback from email recipients
    who responded directly to this email with their questions and concerns. We can
    also gather information, for example, by employing sentiment analysis techniques
    discussed in *Chapter 5*, allowing us to determine the core topics and sentiments
    from product reviews and social media posts from customers who read our emails.
    Here’s an overview of the positive feedback that we will assume to have received:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appreciation for **sustainable practices**: “I love that your kitchenware is
    made from recycled materials.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Approval of **ethical manufacturing**: “It’s reassuring to see a brand commit
    to fair wages and safe working conditions.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enthusiasm for the **product range**: “The stainless steel cookware looks fantastic!”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the highlights of the negative feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Desire for more **product variety**: “I wish that you advertised more color
    options for the product lines.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Questions about **product care**: “I’m concerned about how I can care for these
    products to ensure they last longer.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concerns over **price points**: “The products are great, but they’re a bit
    pricey compared to non-eco-friendly alternatives.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the positive comments validate our campaign’s core messages, the criticisms
    and questions provide actionable insights for our future email messaging. Using
    this feedback, in the next section, we will hone in on how the negative critiques
    can be exploited to refine our email marketing strategies and improve our KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Iterative refinement'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After reviewing the initial metrics and customer feedback from the previous
    sections, it’s clear that while some aspects of the campaign resonate well, there
    are key areas needing improvement. The next step is to refine our FSL prompts
    based on the initial metrics, address the specific criticisms from customer feedback,
    better meet customer expectations, and enhance the overall effectiveness of the
    campaign. The following are the new elements of the prompt, with commentary included
    after each element of the prompt, highlighting their value to our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This prompt sets the tone for the email, capturing the key elements from our
    previous FSL attempt but also emphasizing the importance of an engaging subject
    line in direct response to the low open rate observed in the initial KPI metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This response caters to feedback desiring more variety. It emphasizes the range
    of colors available, aiming to attract customers looking to personalize their
    purchases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Addressing customer concerns about maintenance, this prompt reassures customers
    that the products are not only sustainable but also easy to care for, enhancing
    their appeal by ensuring longevity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This prompt directly tackles the issue of price sensitivity noted in the feedback.
    It reframes the cost as an investment in quality and sustainability, arguing for
    long-term value and environmental impact.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the API recall, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After deploying these updates emails on April 7, assume we collect the next
    seven days of KPI data and observe the following updated trends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot, where we observe significant changes in the
    KPIs after FSL refinements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Email campaign KPIs after campaign changes and improvements were
    made in response to customer feedback'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this, we have the following average KPIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'By comparing these metrics to the previous ones, we see significant improvements
    across all areas. While attributing the exact origins of these improvements is
    not possible without techniques such as A/B testing (*Chapter 6*) or causal inference
    (*Chapter 3*), one can still speculate what may be behind these improved KPIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open rate** improved significantly, which may reflect the more engaging subject
    lines, including “Revolutionize your Kitchen…”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Click-through rate** saw an increase, which may be attributed to the inclusion
    of requested features like product variety and detailed care instructions, which
    increased customer engagement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversion rate** increased, indicating that the email content was not only
    more engaging but also more effective at convincing customers of the value and
    relevance of the products, especially after addressing price concerns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsubscribe rate** decreased, reflecting higher content satisfaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: Continued feedback integration'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in utilizing FSL for email marketing campaigns is to establish
    a robust system for continuous feedback integration. This approach ensures that
    the campaign remains dynamic and responsive to the evolving needs and preferences
    of customers, as well as broader market trends. By integrating continuous feedback,
    the campaign not only maintains relevance but also strengthens customer engagement
    and promotes brand loyalty over time.
  prefs: []
  type: TYPE_NORMAL
- en: Effective feedback integration requires mechanisms that allow customers to share
    their thoughts and experiences easily. This includes embedding quick survey links
    directly in emails, enabling direct email responses, and engaging with customers
    through social media interactions related to the campaign. These channels facilitate
    the flow of information from customers back to marketers, providing valuable insights
    that can be used to refine and improve the campaign continuously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example feedback collection strategies**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are additional ways to collect direct customer feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Live chat feedback**: Implement live chat on your website to allow real-time
    interactions and immediate feedback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive content**: Use quizzes and polls in your emails or on digital
    platforms to make feedback collection engaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback QR codes**: QR codes on products or ads can link to feedback forms
    so customers can easily respond on their devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To effectively analyze and utilize ongoing feedback, setting up a real-time
    feedback dashboard can be immensely beneficial. This dashboard can serve as a
    central hub for monitoring and analyzing feedback alongside standard marketing
    KPIs, providing a comprehensive view of campaign performance. To build such a
    dashboard, consider using software solutions like Tableau, Microsoft Power BI,
    or Google Data Studio, which offer powerful tools and intuitive interfaces for
    creating interactive and real-time data visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps for creating your own feedback dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select a dashboard tool**: Choose a platform that best fits your technical
    capabilities and budget. Tableau, Microsoft Power BI, and Google Data Studio are
    popular choices due to their robust features and scalability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Integrate data sources**: Connect the dashboard tool to the data sources
    that gather your marketing KPIs and feedback. This may include email campaign
    management tools, social media analytics, and customer feedback systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Design the dashboard**: Create visualizations that clearly display the key
    metrics you need to track. Customize the dashboard to highlight trends, spikes,
    and dips in campaign performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Schedule regular reviews**: Set up regular intervals, bi-weekly or monthly,
    for example, to review the dashboard insights. Use these sessions to assess the
    effectiveness of recent changes and to plan further strategic adjustments based
    on the data insights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Keeping your emails out of the spam folder**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To maximize the effectiveness of your email campaigns, it’s essential to prevent
    your messages from landing in the spam folder. Here are three key strategies for
    achieving this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Set up email authentication**: Implement standards like SPF, DKIM, and DMARC
    to authenticate your emails. This helps establish trust with email providers to
    reduce the risk of your emails being marked as spam.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintain list hygiene**: Regularly clean your email list by removing inactive
    subscribers and ensuring that all recipients have opted in to receive your emails.
    This not only boosts engagement but also helps protect your sender reputation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize email content**: Other optimization strategies include personalizing
    your emails using FSL, limiting the number of links, and maintaining a balanced
    image-to-text ratio.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the potential of FSL and its promise for refining AI-driven
    marketing strategies to enhance brand presence. Building on the principles introduced
    through ZSL, we explored how FSL leverages a limited dataset to enable rapid adaptation
    of AI models to new tasks. This is crucial in the fast-paced marketing domain,
    where aligning quickly with evolving consumer preferences and market trends can
    have a significant impact on a brand’s relevance and engagement.
  prefs: []
  type: TYPE_NORMAL
- en: While FSL focuses on quick adaptability using minimal examples, transfer learning
    complements this by applying pre-trained models fine-tuned for specific tasks,
    thereby minimizing the need for extensive retraining. The chapter emphasized practical
    strategies combining these methodologies to optimize your marketing efforts. Through
    approaches like the MAML approach, we demonstrated how you can use meta-learning
    frameworks for marketing.
  prefs: []
  type: TYPE_NORMAL
- en: As we proceed, the next chapter will introduce the concept of **retrieval-augmented
    generation** (**RAG**). We will explore how RAG can dynamically produce content
    that reflects the latest available data by integrating generative models with
    advanced information retrieval techniques. This approach not only increases the
    relevance of the content generated but also enhances precision in consumer targeting,
    making your marketing efforts significantly more effective. The upcoming discussion
    will cover the technical setup of a knowledge retrieval system and practical applications
    of RAG in marketing, through which we aim to provide you with robust tools for
    writing precisely targeted marketing content that resonates with your current
    and potential customers.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  prefs: []
  type: TYPE_IMG
