- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lazy Learning – Classification Using Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A curious type of dining experience has appeared in cities around the world.
    Patrons are served in a completely darkened restaurant by waiters who move via
    memorized routes, using only their senses of touch and sound. The allure of these
    establishments is the belief that depriving oneself of sight will enhance the
    senses of taste and smell, and foods will be experienced in new ways. Each bite
    provides a sense of wonder while discovering the flavors the chef has prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Can you imagine how a diner experiences the unseen food? Upon first bite, the
    senses are overwhelmed. What are the dominant flavors? Does the food taste savory
    or sweet? Does it taste like something they’ve eaten previously? Personally, I
    imagine this process of discovery in terms of a slightly modified adage—if it
    smells like a duck and tastes like a duck, then you are probably eating duck.
  prefs: []
  type: TYPE_NORMAL
- en: 'This illustrates an idea that can be used for machine learning—as does another
    maxim involving poultry—birds of a feather flock together. Stated differently,
    things that are alike tend to have properties that are alike. Machine learning
    uses this principle to classify data by placing it in the same category as similar
    or “nearest” neighbors. This chapter is devoted to classifiers that use this approach.
    You will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: The key concepts that define nearest neighbor classifiers and why they are considered
    “lazy” learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods to measure the similarity of two examples using distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply a popular nearest neighbor classifier called k-NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all of this talk about food is making you hungry, our first task will be
    to understand the k-NN approach by putting it to use while we settle a long-running
    culinary debate.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding nearest neighbor classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a single sentence, **nearest neighbor** classifiers are defined by their
    characteristic of classifying unlabeled examples by assigning them the class of
    similar labeled examples. This is analogous to the dining experience described
    in the chapter introduction, in which a person identifies new foods through comparison
    to those previously encountered. With nearest neighbor classification, computers
    apply a human-like ability to recall past experiences to make conclusions about
    current circumstances. Despite the simplicity of this idea, nearest neighbor methods
    are extremely powerful. They have been used successfully for:'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision applications, including optical character recognition and facial
    recognition in still images and video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation systems that predict whether a person will enjoy a movie or song
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns in genetic data to detect specific proteins or diseases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, nearest neighbor classifiers are well suited for classification
    tasks where relationships among the features and the target classes are numerous,
    complicated, or otherwise extremely difficult to understand, yet the items of
    similar class types tend to be fairly homogeneous. Another way of putting it would
    be to say that if a concept is difficult to define, but you know it when you see
    it, then nearest neighbors might be appropriate. On the other hand, if the data
    is noisy and thus no clear distinction exists among the groups, nearest neighbor
    algorithms may struggle to identify the class boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: The k-NN algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The nearest neighbors approach to classification is exemplified by the **k-nearest
    neighbors** algorithm (**k-NN**). Although this is perhaps one of the simplest
    machine learning algorithms, it is still used widely. The strengths and weaknesses
    of this algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Simple and effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes no assumptions about the underlying data distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast training phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Does not produce a model, limiting the ability to understand how the features
    are related to the class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires selection of an appropriate *k*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow classification phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nominal features and missing data require additional processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The k-NN algorithm gets its name from the fact that it uses information about
    an example’s *k* nearest neighbors to classify unlabeled examples. The letter
    *k* is a variable implying that any number of nearest neighbors could be used.
    After choosing *k*, the algorithm requires a training dataset made up of examples
    that have been classified into several categories, as labeled by a nominal variable.
    Then, for each unlabeled record in the test dataset, k-NN identifies the *k* records
    in the training data that are the “nearest” in similarity. The unlabeled test
    instance is assigned the class representing the majority of the *k* nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this process, let’s revisit the blind tasting experience described
    in the introduction. Suppose that prior to eating the mystery meal, we had created
    a dataset in which we recorded our impressions of a set of previously tasted ingredients.
    To keep things simple, we rated only two features of each ingredient. The first
    is a measure from 1 to 10 of how crunchy the ingredient is, and the second is
    a score from 1 to 10 measuring how sweet the ingredient tastes. We then labeled
    each ingredient as one of three types of food: fruits, vegetables, or proteins,
    ignoring other foods such as grains and fats.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first few rows of such a dataset might be structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ingredient** | **Sweetness** | **Crunchiness** | **Food type** |'
  prefs: []
  type: TYPE_TB
- en: '| Apple | 10 | 9 | Fruit |'
  prefs: []
  type: TYPE_TB
- en: '| Bacon | 1 | 4 | Protein |'
  prefs: []
  type: TYPE_TB
- en: '| Banana | 10 | 1 | Fruit |'
  prefs: []
  type: TYPE_TB
- en: '| Carrot | 7 | 10 | Vegetable |'
  prefs: []
  type: TYPE_TB
- en: '| Celery | 3 | 10 | Vegetable |'
  prefs: []
  type: TYPE_TB
- en: 'The k-NN algorithm treats the features as coordinates in a multidimensional
    **feature space**, which is a space comprising all possible combinations of feature
    values. Because the ingredient dataset includes only two features, its feature
    space is two-dimensional. We can plot two-dimensional data on a scatterplot, with
    the *x* dimension indicating the ingredient’s sweetness and the *y* dimension
    indicating the crunchiness. After adding a few more ingredients to the taste dataset,
    the scatterplot might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B17290_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: A scatterplot of selected foods’ crunchiness versus sweetness'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you notice a pattern? Similar types of food tend to be grouped closely together.
    As illustrated in *Figure 3.2*, vegetables tend to be crunchy but not sweet; fruits
    tend to be sweet and either crunchy or not crunchy; and proteins tend to be neither
    crunchy nor sweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Foods that are similarly classified tend to have similar attributes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that after constructing this dataset, we decide to use it to settle
    the age-old question: is a tomato a fruit or a vegetable? We can use the nearest
    neighbor approach to determine which class is a better fit, as shown in *Figure
    3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The tomato’s nearest neighbors provide insight into whether it
    is a fruit or vegetable'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring similarity with distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Locating the tomato’s nearest neighbors requires a **distance function**, which
    is a formula that measures the similarity between two instances.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to calculate distance. The choice of distance function may
    impact the model’s performance substantially, although it is difficult to know
    which to use except by comparing them directly on the desired learning task. Traditionally,
    the k-NN algorithm uses **Euclidean distance**, which is the distance one would
    measure if it were possible to use a ruler to connect two points. Euclidean distance
    is measured “as the crow flies,” which implies the shortest direct route. This
    is illustrated in the previous figure by the dotted lines connecting the tomato
    to its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Another common distance measure is **Manhattan distance**, which is based on
    the paths a pedestrian would take by walking city blocks. If you are interested
    in learning more about other distance measures, you can read the documentation
    for R’s distance function using the `?dist` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Euclidean distance is specified by the following formula, where *p* and *q*
    are the examples to be compared, each having *n* features. The term *p*¹ refers
    to the value of the first feature of example *p*, while *q*¹ refers to the value
    of the first feature of example *q*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The distance formula involves comparing the values of each example’s features.
    For example, to calculate the distance between the tomato (sweetness = 6, crunchiness
    = 4), and the green bean (sweetness = 3, crunchiness = 7), we can use the formula
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar vein, we can calculate the distance between the tomato and several
    of its closest neighbors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ingredient** | **Sweetness** | **Crunchiness** | **Food type** | **Distance
    to the tomato** |'
  prefs: []
  type: TYPE_TB
- en: '| Grape | 8 | 5 | Fruit | sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Green bean | 3 | 7 | Vegetable | sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Nuts | 3 | 6 | Protein | sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Orange | 7 | 3 | Fruit | sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 |'
  prefs: []
  type: TYPE_TB
- en: To classify the tomato as a vegetable, protein, or fruit, we’ll begin by assigning
    the tomato the food type of its single nearest neighbor. This is called 1-NN classification
    because *k = 1*. The orange is the single nearest neighbor to the tomato, with
    a distance of 1.4\. Because an orange is a fruit, the 1-NN algorithm would classify
    a tomato as a fruit.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the k-NN algorithm with *k = 3* instead, it performs a vote among
    the three nearest neighbors: orange, grape, and nuts. Now, because the majority
    class among these neighbors is fruit (with two of the three votes), the tomato
    again is classified as a fruit.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an appropriate k
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision of how many neighbors to use for k-NN determines how well the model
    will generalize to future data. The balance between overfitting and underfitting
    the training data is a problem known as the **bias-variance tradeoff**. Choosing
    a large *k* reduces the impact of variance caused by noisy data but can bias the
    learner such that it runs the risk of ignoring small but important patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we took the extreme stance of setting a very large *k*, as large as
    the total number of observations in the training data. With every training instance
    represented in the final vote, the most common class always has a majority of
    the voters. The model would consequently always predict the majority class, regardless
    of the nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: On the opposite extreme, using a single nearest neighbor allows noisy data and
    outliers to unduly influence the classification of examples. For example, suppose
    some of the training examples were accidentally mislabeled. Any unlabeled example
    that happens to be nearest to the incorrectly labeled neighbor will be predicted
    to have the incorrect class, even if nine other nearby neighbors would have voted
    differently.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the best *k* value is somewhere between these two extremes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.4* illustrates, more generally, how the decision boundary (depicted
    by a dashed line) is affected by larger or smaller *k* values. Smaller values
    allow more complex decision boundaries that more carefully fit the training data.
    The problem is that we do not know whether the straight boundary or the curved
    boundary better represents the true underlying concept to be learned.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: A larger k has higher bias and lower variance than a smaller k'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the choice of *k* depends on the difficulty of the concept to be
    learned and the number of records in the training data. One common approach is
    to begin with *k* equal to the square root of the number of training examples.
    In the food classifier developed previously, we might set *k = 4* because there
    were 15 example ingredients in the training data and the square root of 15 is
    3.87.
  prefs: []
  type: TYPE_NORMAL
- en: However, such rules may not always result in the single best *k*. An alternative
    approach is to test several *k* values on a variety of test datasets and choose
    the one that delivers the best classification performance. That said, unless the
    data is very noisy, a large training dataset can make the choice of *k* less important.
    This is because even subtle concepts will have a sufficiently large pool of examples
    to vote as nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: A less common, but still interesting, solution to this problem is to choose
    a larger *k* and use a weighted voting process in which the vote of closer neighbors
    is considered more authoritative than the vote of neighbors that are far away.
    Some k-NN implementations offer this option.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for use with k-NN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Features are typically transformed to a standard range prior to applying the
    k-NN algorithm. The rationale for this step is that the distance formula is highly
    dependent on how features are measured. In particular, if certain features have
    a much larger range of values than others, the distance measurements will be strongly
    dominated by the features with larger ranges. This wasn’t a problem for the food
    tasting example, as both sweetness and crunchiness were measured on a scale from
    1 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: However, suppose that we added an additional feature to the dataset to represent
    a food’s spiciness, which was measured using the Scoville scale. If you are unfamiliar
    with this metric, it is a standardized measure of spice heat, ranging from zero
    (not at all spicy) to over a million (for the hottest chili peppers). Since the
    difference between spicy and non-spicy foods can be over a million while the difference
    between sweet and non-sweet or crunchy and non-crunchy foods is at most 10, the
    difference in scale allows the spice level to impact the distance function much
    more than the other two factors. Without adjusting our data, we might find that
    our distance measures only differentiate foods by their spiciness; the impact
    of crunchiness and sweetness would be dwarfed by the contribution of spiciness.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to rescale the features by shrinking or expanding their range
    such that each one contributes relatively equally to the distance formula. For
    example, if sweetness and crunchiness are both measured on a scale from 1 to 10,
    we would also like spiciness to be measured on a scale from 1 to 10\. There are
    several common ways to accomplish such scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The traditional method of rescaling features for k-NN is **min-max normalization**.
    This process transforms a feature such that all values fall in a range between
    0 and 1\. The formula for normalizing a feature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: To transform each value of feature *X*, the formula subtracts the minimum *X*
    value and divides it by the range of *X*. The resulting normalized feature values
    can be interpreted as indicating how far, from 0 percent to 100 percent, the original
    value fell along the range between the original minimum and maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common transformation is called **z-score standardization**. The following
    formula subtracts the mean value of feature *X*, and divides the result by the
    standard deviation of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula, which is based on properties of the normal distribution covered
    in *Chapter 2*, *Managing and Understanding Data*, rescales each of a feature’s
    values in terms of how many standard deviations they fall above or below the mean.
    The resulting value is called a **z-score**. The z-scores fall in an unbounded
    range of negative and positive numbers. Unlike the normalized values, they have
    no predefined minimum and maximum.
  prefs: []
  type: TYPE_NORMAL
- en: The same rescaling method used on the k-NN training dataset must also be applied
    to the test examples that the algorithm will later classify. This can lead to
    a tricky situation for min-max normalization, as the minimum or maximum of future
    cases might be outside the range of values observed in the training data. If you
    know the theoretical minimum or maximum value ahead of time, you can use these
    constants rather than the observed minimum and maximum values. Alternatively,
    you can use z-score standardization under the assumption that the future examples
    are taken from a distribution with the same mean and standard deviation as the
    training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Euclidean distance formula is undefined for nominal data. Therefore, to
    calculate the distance between nominal features, we need to convert them into
    a numeric format. A typical solution utilizes **dummy coding**, where a value
    of 1 indicates one category, and 0 indicates the other. For instance, dummy coding
    for a male or non-male sex variable could be constructed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how dummy coding of the two-category (binary) sex variable results in
    a single new feature named male. There is no need to construct a separate feature
    for non-male. Since both are mutually exclusive, knowing one or the other is enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is true more generally as well. An *n*-category nominal feature can be
    dummy coded by creating binary indicator variables for *n - 1* levels of the feature.
    For example, dummy coding for a three-category temperature variable (for example,
    hot, medium, or cold) could be set up as *(3 - 1) = 2* features, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowing that hot and medium are both 0 provides enough information to know that
    the temperature is cold, and thus, a third binary feature for the cold category
    is unnecessary. However, a widely used close sibling of dummy coding known as
    **one-hot encoding** creates binary features for all *n* levels of the feature,
    rather than *n - 1* as with dummy coding. It is known as “one-hot” because only
    one attribute is coded as 1 and the others are set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, there is virtually no difference between these two methods, and
    the results of machine learning will be unaffected by the choice of coding. This
    being said, one-hot encoding can cause problems with linear models, such as those
    described in *Chapter 6*, *Forecasting Numeric Data – Regression Methods*, and
    thus one-hot encoding is often avoided among statisticians or in fields like economics
    that rely heavily on such models. On the other hand, one-hot encoding has become
    prevalent in the field of machine learning and is often treated synonymously with
    dummy coding for the simple reason that the choice makes virtually no difference
    in the model fit; yet, in one-hot encoding, the model itself may be easier to
    understand since all levels of the categorical features are specified explicitly.
    This book uses only dummy coding since it can be used universally, but you may
    encounter one-hot encoding elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: A convenient aspect of both dummy and one-hot coding is that the distance between
    dummy-coded features is always one or zero, and thus, the values fall on the same
    scale as min-max normalized numeric data. No additional transformation is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: If a nominal feature is ordinal (one could make such an argument for temperature),
    an alternative to dummy coding is to number the categories and apply normalization.
    For instance, cold, warm, and hot could be numbered as 1, 2, and 3, which normalizes
    to 0, 0.5, and 1\. A caveat to this approach is that it should only be used if
    the steps between categories are equivalent. For instance, although income categories
    for poor, middle class, and wealthy are ordered, the difference between poor and
    middle class may be different than the difference between middle class and wealthy.
    Since the steps between groups are not equal, dummy coding is a safer approach.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the k-NN algorithm lazy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification algorithms based on nearest neighbor methods are considered **lazy
    learning** algorithms because, technically speaking, no abstraction occurs. The
    abstraction and generalization processes are skipped altogether, which undermines
    the definition of learning proposed in *Chapter 1*, *Introducing Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Under the strict definition of learning, a lazy learner is not really learning
    anything. Instead, it merely stores the training data verbatim. This allows the
    training phase, which is not actually training anything, to occur very rapidly.
    Of course, the downside is that the process of making predictions tends to be
    relatively slow by comparison. Due to the heavy reliance on the training instances
    rather than an abstracted model, lazy learning is also known as **instance-based
    learning** or **rote learning**.
  prefs: []
  type: TYPE_NORMAL
- en: As instance-based learners do not build a model, the method is said to be in
    a class of **non-parametric** learning methods—no parameters are learned about
    the data. Without generating theories about the underlying data, non-parametric
    methods limit our ability to understand how the classifier is using the data,
    yet it can still make useful predictions. Non-parametric learning allows the learner
    to find natural patterns rather than trying to fit the data into a preconceived
    and potentially biased functional form.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Machine learning algorithms have different biases and may come
    to different conclusions!'
  prefs: []
  type: TYPE_NORMAL
- en: Although k-NN classifiers may be considered lazy, they are still quite powerful.
    As you will soon see, the simple principles of nearest neighbor learning can be
    used to automate the process of screening for cancer.
  prefs: []
  type: TYPE_NORMAL
- en: Example – diagnosing breast cancer with the k-NN algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Routine breast cancer screening allows the disease to be diagnosed and treated
    prior to it causing noticeable symptoms. The process of early detection involves
    examining the breast tissue for abnormal lumps or masses. If a lump is found,
    a fine-needle aspiration biopsy is performed, which uses a hollow needle to extract
    a small sample of cells from the mass. A clinician then examines the cells under
    a microscope to determine whether the mass is likely to be malignant or benign.
  prefs: []
  type: TYPE_NORMAL
- en: If machine learning could automate the identification of cancerous cells, it
    would provide considerable benefit to the health system. Automated processes are
    likely to improve the efficiency of the detection process, allowing physicians
    to spend less time diagnosing and more time treating the disease. An automated
    screening system might also provide greater detection accuracy by removing the
    inherently subjective human component from the process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s investigate the utility of machine learning for detecting cancer by applying
    the k-NN algorithm to measurements of biopsied cells from women with abnormal
    breast masses.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will utilize the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI
    Machine Learning Repository at `http://archive.ics.uci.edu/ml`. This data was
    donated by researchers at the University of Wisconsin and includes measurements
    from digitized images of fine-needle aspirations of a breast mass. The values
    represent characteristics of the cell nuclei present in the digital image.
  prefs: []
  type: TYPE_NORMAL
- en: To read more about this dataset, refer to *Breast Cancer Diagnosis and Prognosis
    via Linear Programming, Mangasarian OL, Street WN, Wolberg WH, Operations Research,
    1995, Vol. 43, pp. 570-577*.
  prefs: []
  type: TYPE_NORMAL
- en: The breast cancer data includes 569 examples of cancer biopsies, each with 32
    features. One feature is an identification number, another is the cancer diagnosis,
    and 30 are numeric-valued laboratory measurements. The diagnosis is coded as “M”
    to indicate malignant or “B” to indicate benign.
  prefs: []
  type: TYPE_NORMAL
- en: The 30 numeric measurements comprise the mean, standard error, and worst (that
    is, largest) value for 10 different characteristics of the digitized cell nuclei,
    such as radius, texture, area, smoothness, and compactness. Based on the feature
    names, the dataset seems to measure the shape and size of the cell nuclei, but
    unless you are an oncologist, you are unlikely to know how each of these relates
    to benign or malignant masses. No such expertise is necessary, as the computer
    will discover the important patterns during the machine learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By exploring the data, we may be able to shine some light on the relationships
    between the features and the cancer status. In doing so, we will prepare the data
    for use with the k-NN learning method.
  prefs: []
  type: TYPE_NORMAL
- en: If you plan on following along, download the code and `wisc_bc_data.csv` files
    from the GitHub repository and save them to your R working directory. For this
    book, the dataset was modified very slightly from its original form. In particular,
    a header line was added, and the rows of data were randomly ordered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by importing the CSV data file as we have done in previous chapters,
    saving the Wisconsin breast cancer data to the `wbcd` data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the command `str(wbcd)`, we can confirm that the data is structured with
    569 examples and 32 features, as we expected. The first several lines of output
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first feature is an integer variable named `id`. As this is simply a unique
    identifier (ID) for each patient in the data, it does not provide useful information,
    and we will need to exclude it from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the machine learning method, ID variables should always be excluded.
    Neglecting to do so can lead to erroneous findings because the ID can be used
    to correctly predict each example. Therefore, a model that includes an ID column
    will almost definitely suffer from overfitting and generalize poorly to future
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s drop the `id` feature from our data frame. As it is in the first column,
    we can exclude it by making a copy of the `wbcd` data frame without column 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next feature, `diagnosis`, is of particular interest as it is the target
    outcome we hope to predict. This feature indicates whether the example is from
    a benign or malignant mass. The `table()` output indicates that 357 masses are
    benign, while 212 are malignant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Many R machine learning classifiers require the target feature to be coded
    as a factor, so we will need to recode the `diagnosis` column. We will also take
    this opportunity to give the `"B"` and `"M"` values more informative labels using
    the `labels` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at the `prop.table()` output, we now see that the values have
    been labeled `Benign` and `Malignant`, with 62.7 percent and 37.3 percent of the
    masses, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining 30 features are all numeric and, as expected, consist of three
    different measurements of 10 characteristics. For illustrative purposes, we will
    only take a closer look at three of these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the three side by side, do you notice anything problematic about
    the values? Recall that the distance calculation for k-NN is heavily dependent
    upon the measurement scale of the input features. Since smoothness ranges from
    0.05 to 0.16, while area ranges from 143.5 to 2501.0, the impact of area is going
    to be much greater than smoothness in the distance calculation. This could potentially
    cause problems for our classifier, so let’s apply normalization to rescale the
    features to a standard range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation – normalizing numeric data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To normalize these features, we need to create a `normalize()` function in
    R. This function takes a vector `x` of numeric values, and for each value in `x`,
    subtracts the minimum `x` value and divides it by the range of `x` values. Lastly,
    the resulting vector is returned. The code for the function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the previous code, the `normalize()` function is available
    for use in R. Let’s test the function on a couple of vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The function appears to be working correctly. Even though the values in the
    second vector are 10 times larger than the first vector, after normalization,
    they are identical.
  prefs: []
  type: TYPE_NORMAL
- en: We can now apply the `normalize()` function to the numeric features in our data
    frame. Rather than normalizing each of the 30 numeric variables individually,
    we will use one of R’s functions to automate the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lapply()` function takes a list and applies a specified function to each
    list element. As a data frame is a list of equal-length vectors, we can use `lapply()`
    to apply `normalize()` to each feature in the data frame. The final step is to
    convert the list returned by `lapply()` to a data frame using the `as.data.frame()`
    function. The full process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In plain English, this command applies the `normalize()` function to columns
    2 to 31 in the `wbcd` data frame, converts the resulting list to a data frame,
    and assigns it the name `wbcd_n`. The `_n` suffix is used here as a reminder that
    the values in `wbcd` have been normalized.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that the transformation was applied correctly, let’s look at one
    variable’s summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `area_mean` variable, which originally ranged from 143.5 to
    2501.0, now ranges from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify data preparation for this example, min-max normalization was applied
    to the entire dataset—including the rows that will later become the test set.
    In a way, this violates our simulation of unseen future data since, in practice,
    one will generally not know the true minimum and maximum values at the time of
    model training and future values might fall outside the previously observed range.
    A better approach might be to normalize the test set using only the minimum and
    maximum values observed in the training data, and potentially even capping any
    future values at the prior minimum or maximum levels. This being said, whether
    normalization is applied to training and test sets together or separately is unlikely
    to notably impact the model’s performance and does not do so here.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating training and test datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although all 569 biopsies are labeled with a benign or malignant status, it
    is not very interesting to predict what we already know. Additionally, any performance
    measures we obtain during training may be misleading, as we do not know the extent
    to which the data has been overfitted or how well the learner will generalize
    to new cases. For these reasons, a more interesting question is how well our learner
    performs on a dataset of unseen data. If we had access to a laboratory, we could
    apply our learner to measurements taken from the next 100 masses of unknown cancer
    status and see how well the machine learner’s predictions compare to diagnoses
    obtained using conventional methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the absence of such data, we can simulate this scenario by dividing our
    data into two portions: a training dataset that will be used to build the k-NN
    model and a test dataset that will be used to estimate the predictive accuracy
    of the model. We will use the first 469 records for the training dataset and the
    remaining 100 to simulate new patients.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data extraction methods presented in *Chapter 2*, *Managing and Understanding
    Data*, we will split the `wbcd_n` data frame into `wbcd_train` and `wbcd_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If the previous commands are confusing, remember that data is extracted from
    data frames using the `[row, column]` syntax. A blank value for the row or column
    value indicates that all rows or columns should be included. Hence, the first
    line of code requests rows 1 to 469 and all columns, and the second line requests
    100 rows from 470 to 569 and all columns.
  prefs: []
  type: TYPE_NORMAL
- en: When constructing training and test datasets, it is important that each dataset
    is a representative subset of the full set of data. The `wbcd` records were already
    randomly ordered, so we could simply extract 100 consecutive records to create
    a representative test dataset. This would not be appropriate if the data was ordered
    chronologically or in groups of similar values. In these cases, random sampling
    methods would be needed. Random sampling will be discussed in *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we constructed our normalized training and test datasets, we excluded
    the target variable, `diagnosis`. For training the k-NN model, we will need to
    store these class labels in factor vectors, split between the training and test
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code takes the `diagnosis` factor in the first column of the `wbcd` data
    frame and creates the vectors `wbcd_train_labels` and `wbcd_test_labels`. We will
    use these in the next steps of training and evaluating our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Equipped with our training data and vector of labels, we are now ready to classify
    our test records. For the k-NN algorithm, the training phase involves no model
    building; the process of training a so-called “lazy” learner like k-NN simply
    involves storing the input data in a structured format.
  prefs: []
  type: TYPE_NORMAL
- en: 'To classify our test instances, we will use a k-NN implementation from the
    `class` package, which provides a set of basic R functions for classification.
    If this package is not already installed on your system, you can install it by
    typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To load the package during any session in which you wish to use the functions,
    simply enter the `library(class)` command.
  prefs: []
  type: TYPE_NORMAL
- en: The `knn()` function in the `class` package provides a standard, traditional
    implementation of the k-NN algorithm. For each instance in the test data, the
    function will identify the *k* nearest neighbors, using Euclidean distance, where
    *k* is a user-specified number. The test instance is classified by taking a “vote”
    among the *k* nearest neighbors—specifically, this involves assigning the class
    of the majority of neighbors. A tie vote is broken at random.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several other k-NN functions in other R packages that provide more
    sophisticated or more efficient implementations. If you run into limitations with
    `knn()`, search for k-NN on the CRAN website: [https://cran.r-project.org](https://cran.r-project.org).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training and classification using the `knn()` function is performed in a single
    command that requires four parameters, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B17290_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: kNN classification syntax'
  prefs: []
  type: TYPE_NORMAL
- en: We now have nearly everything we need to apply the k-NN algorithm to this data.
    We’ve split our data into training and test datasets, each with the same numeric
    features. The labels for the training data are stored in a separate factor vector.
    The only remaining parameter is `k`, which specifies the number of neighbors to
    include in the vote.
  prefs: []
  type: TYPE_NORMAL
- en: As our training data includes 469 instances, we might try `k = 21`, an odd number
    roughly equal to the square root of 469\. With a two-category outcome, using an
    odd number eliminates the possibility of ending with a tie vote.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the `knn()` function to classify the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `knn()` function returns a factor vector of predicted labels for each of
    the examples in the `wbcd_test` dataset. We have assigned these predictions to
    `wbcd_test_pred`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step of the process is to evaluate how well the predicted classes in
    the `wbcd_test_pred` vector match the actual values in the `wbcd_test_labels`
    vector. To do this, we can use the `CrossTable()` function in the `gmodels` package,
    which was introduced in *Chapter 2*, *Managing and Understanding Data*. If you
    haven’t done so already, please install this package using the `install.packages("gmodels")`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the package with the `library(gmodels)` command, we can create
    a cross tabulation indicating the agreement between the predicted and actual label
    vectors. Specifying `prop.chisq = FALSE` excludes the unnecessary chi-square values
    from the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting table looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The cell percentages in the table indicate the proportion of values that fall
    into four categories. The top-left cell indicates the **true negative** results.
    These 61 of 100 values are cases where the mass was benign and the k-NN algorithm
    correctly identified it as such. The bottom-right cell indicates the **true positive**
    results, where the classifier and the clinically determined label agree that the
    mass is malignant. A total of 37 of 100 predictions were true positives.
  prefs: []
  type: TYPE_NORMAL
- en: The cells falling on the other diagonal contain counts of examples where the
    k-NN prediction disagreed with the true label. The two examples in the lower-left
    cell are **false negative** results; in this case, the predicted value was benign,
    but the tumor was actually malignant. Errors in this direction could be extremely
    costly, as they might lead a patient to believe that they are cancer-free, but
    in reality, the disease may continue to spread.
  prefs: []
  type: TYPE_NORMAL
- en: The top-right cell would contain the **false positive** results, if there were
    any. These values occur when the model has classified a mass as malignant when
    it actually was benign. Although such errors are less dangerous than a false negative
    result, they should also be avoided, as they could lead to additional financial
    burden on the health care system or stress for the patient, as unnecessary tests
    or treatment may be provided.
  prefs: []
  type: TYPE_NORMAL
- en: If we desired, we could eliminate all false negatives by classifying every mass
    as malignant. Obviously, this is not a realistic strategy. Still, it illustrates
    the fact that prediction involves striking a balance between the false positive
    rate and the false negative rate. In *Chapter 10*, *Evaluating Model Performance*,
    you will learn methods for evaluating predictive accuracy that can be used to
    optimize performance to the costs of each type of error.
  prefs: []
  type: TYPE_NORMAL
- en: A total of 2 out of 100, or 2 percent of masses were incorrectly classified
    by the k-NN approach. While 98 percent accuracy seems impressive for a few lines
    of R code, we might try another iteration of the model to see if we can improve
    the performance and reduce the number of values that have been incorrectly classified,
    especially because the errors were dangerous false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will attempt two simple variations on our previous classifier. First, we
    will employ an alternative method for rescaling our numeric features. Second,
    we will try several different *k* values.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation – z-score standardization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although normalization is commonly used for k-NN classification, z-score standardization
    may be a more appropriate way to rescale the features in a cancer dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Since z-score standardized values have no predefined minimum and maximum, extreme
    values are not compressed towards the center. Even without medical training, one
    might suspect that a malignant tumor might lead to extreme outliers as tumors
    grow uncontrollably. With this in mind, it might be reasonable to allow the outliers
    to be weighted more heavily in the distance calculation. Let’s see whether z-score
    standardization improves our predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To standardize a vector, we can use R’s built-in `scale()` function, which
    by default rescales values using the z-score standardization. The `scale()` function
    can be applied directly to a data frame, so there is no need to use the `lapply()`
    function. To create a z-score standardized version of the `wbcd` data, we can
    use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This rescales all features with the exception of `diagnosis` in the first column
    and stores the result as the `wbcd_z` data frame. The `_z` suffix is a reminder
    that the values were z-score transformed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that the transformation was applied correctly, we can look at the
    summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The mean of a z-score standardized variable should always be zero, and the range
    should be fairly compact. A z-score less than -3 or greater than 3 indicates an
    extremely rare value. Examining the summary statistics with these criteria in
    mind, the transformation seems to have worked.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have done before, we need to divide the z-score-transformed data into
    training and test sets, and classify the test instances using the `knn()` function.
    We’ll then compare the predicted labels to the actual labels using `CrossTable()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, in the following table, the results of our new transformation
    show a slight decline in accuracy. Using the same instances in which we had previously
    classified 98 percent of examples correctly, we now classified only 95 percent
    correctly. Making matters worse, we did no better at classifying the dangerous
    false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Testing alternative values of k
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We may be able to optimize the performance of the k-NN model by examining its
    performance across various *k* values. Using the normalized training and test
    datasets, the same 100 records need to be classified using several different choices
    of *k*. Given that we are testing only six *k* values, these iterations can be
    performed most simply by using copy-and-paste of our previous `knn()` and `CrossTable()`
    functions. However, it is also possible to write a `for` loop that runs these
    two functions for each of the values in a vector named `k_values`, as demonstrated
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `for` loop can almost be read as a simple sentence: for each value named
    `k_val` in the `k_values` vector, run the `knn()` function while setting the parameter
    `k` to the current `k_val`, and then produce the `CrossTable()` for the resulting
    predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated approach to looping using one of R’s `apply()` functions
    is described in *Chapter 7*, *Black-Box Methods – Neural Networks and Support
    Vector Machines*, to test various values of a cost parameter and plot the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The false negatives, false positives, and overall error rate are shown for
    each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **k value** | **False negatives** | **False positives** | **Error rate**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 3 | 4 percent |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2 | 0 | 2 percent |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 3 | 0 | 3 percent |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 3 | 0 | 3 percent |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 2 | 0 | 2 percent |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 4 | 0 | 4 percent |'
  prefs: []
  type: TYPE_TB
- en: Although the classifier was never perfect, the 1-NN approach was able to avoid
    some of the false negatives at the expense of adding false positives. It is important
    to keep in mind, however, that it would be unwise to tailor our approach too closely
    to our test data; after all, a different set of 100 patient records is likely
    to be somewhat different from those used to measure our performance.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to be certain that a learner will generalize to future data, you
    might create several sets of 100 patients at random and repeatedly retest the
    result. Such methods to carefully evaluate the performance of machine learning
    models will be discussed further in *Chapter 10*, *Evaluating Model Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about classification using k-NN. Unlike many classification
    algorithms, k-nearest neighbors does not do any learning—at least not according
    to the formal definition of machine learning. Instead, it simply stores the training
    data verbatim. Unlabeled test examples are then matched to the most similar records
    in the training set using a distance function, and the unlabeled example is assigned
    the label of its nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Although k-NN is a very simple algorithm, it can tackle extremely complex tasks,
    such as the identification of cancerous masses. In a few simple lines of R code,
    we were able to correctly identify whether a mass was malignant or benign 98 percent
    of the time in an example using real-world data. Although this teaching dataset
    was designed to streamline the process of building a model, the exercise demonstrated
    the ability of learning algorithms to make accurate predictions much like a human
    can.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine a classification method that uses probability
    to estimate the likelihood that an observation falls into certain categories.
    It will be interesting to compare how this approach differs from k-NN. Later,
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, we will learn
    about a close relative to k-NN, which uses distance measures for a completely
    different learning task.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
