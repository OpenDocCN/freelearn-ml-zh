- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Lazy Learning – Classification Using Nearest Neighbors
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 懒惰学习 – 使用最近邻进行分类
- en: A curious type of dining experience has appeared in cities around the world.
    Patrons are served in a completely darkened restaurant by waiters who move via
    memorized routes, using only their senses of touch and sound. The allure of these
    establishments is the belief that depriving oneself of sight will enhance the
    senses of taste and smell, and foods will be experienced in new ways. Each bite
    provides a sense of wonder while discovering the flavors the chef has prepared.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一种好奇的餐饮体验在全球各地城市中出现。顾客在完全黑暗的餐厅中被服务员服务，服务员通过记忆中的路线移动，仅使用他们的触觉和听觉。这些场所的吸引力在于相信剥夺自己的视觉将增强味觉和嗅觉，食物将以新的方式被体验。每一口都提供了一种惊奇感，同时发现了厨师准备的风味。
- en: Can you imagine how a diner experiences the unseen food? Upon first bite, the
    senses are overwhelmed. What are the dominant flavors? Does the food taste savory
    or sweet? Does it taste like something they’ve eaten previously? Personally, I
    imagine this process of discovery in terms of a slightly modified adage—if it
    smells like a duck and tastes like a duck, then you are probably eating duck.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想象一位食客是如何体验未见过的食物的吗？第一口咬下去，感官会感到震惊。主要的味道是什么？食物是咸的还是甜的？它尝起来像他们以前吃过的东西吗？我个人认为这个过程可以用一个稍微修改过的谚语来描述——如果它闻起来像鸭子，尝起来也像鸭子，那么你很可能在吃鸭子。
- en: 'This illustrates an idea that can be used for machine learning—as does another
    maxim involving poultry—birds of a feather flock together. Stated differently,
    things that are alike tend to have properties that are alike. Machine learning
    uses this principle to classify data by placing it in the same category as similar
    or “nearest” neighbors. This chapter is devoted to classifiers that use this approach.
    You will learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了可以用于机器学习的一个想法——就像另一个涉及家禽的格言——物以类聚。换句话说，相似的事物往往具有相似的性质。机器学习使用这个原则通过将其放置在与相似或“最近”邻居相同的类别中来对数据进行分类。本章致力于使用这种方法进行分类的分类器。你将学习：
- en: The key concepts that define nearest neighbor classifiers and why they are considered
    “lazy” learners
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义最近邻分类器的关键概念以及为什么它们被认为是“懒惰”的学习者
- en: Methods to measure the similarity of two examples using distance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量两个例子之间相似度的方法
- en: How to apply a popular nearest neighbor classifier called k-NN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用一个流行的最近邻分类器，称为k-NN
- en: If all of this talk about food is making you hungry, our first task will be
    to understand the k-NN approach by putting it to use while we settle a long-running
    culinary debate.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有关于食物的谈话让你感到饥饿，我们的第一个任务将是通过将其应用于解决一个长期存在的烹饪辩论，来理解k-NN方法。
- en: Understanding nearest neighbor classification
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解最近邻分类
- en: 'In a single sentence, **nearest neighbor** classifiers are defined by their
    characteristic of classifying unlabeled examples by assigning them the class of
    similar labeled examples. This is analogous to the dining experience described
    in the chapter introduction, in which a person identifies new foods through comparison
    to those previously encountered. With nearest neighbor classification, computers
    apply a human-like ability to recall past experiences to make conclusions about
    current circumstances. Despite the simplicity of this idea, nearest neighbor methods
    are extremely powerful. They have been used successfully for:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用一句话来说，**最近邻**分类器通过其将未标记的例子分类为相似标记例子类别的特性来定义。这与章节引言中描述的餐饮体验类似，其中一个人通过比较之前遇到的食物来识别新食物。在最近邻分类中，计算机应用了一种类似人类的能力，回忆过去的经验，对当前情况做出结论。尽管这个想法很简单，但最近邻方法非常强大。它们已被成功用于：
- en: Computer vision applications, including optical character recognition and facial
    recognition in still images and video
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉应用，包括静态图像和视频中的光学字符识别和面部识别
- en: Recommendation systems that predict whether a person will enjoy a movie or song
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统预测一个人是否会喜欢一部电影或一首歌
- en: Identifying patterns in genetic data to detect specific proteins or diseases
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别遗传数据中的模式以检测特定的蛋白质或疾病
- en: In general, nearest neighbor classifiers are well suited for classification
    tasks where relationships among the features and the target classes are numerous,
    complicated, or otherwise extremely difficult to understand, yet the items of
    similar class types tend to be fairly homogeneous. Another way of putting it would
    be to say that if a concept is difficult to define, but you know it when you see
    it, then nearest neighbors might be appropriate. On the other hand, if the data
    is noisy and thus no clear distinction exists among the groups, nearest neighbor
    algorithms may struggle to identify the class boundaries.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最近邻分类器非常适合于特征与目标类别之间存在众多、复杂或难以理解的关系的分类任务，而相似类别类型的项往往相当同质。另一种说法可能是，如果一个概念难以定义，但你一看到就知道，那么最近邻可能适用。另一方面，如果数据有噪声，因此组间没有明显的区分，最近邻算法可能难以识别类别边界。
- en: The k-NN algorithm
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-NN算法
- en: 'The nearest neighbors approach to classification is exemplified by the **k-nearest
    neighbors** algorithm (**k-NN**). Although this is perhaps one of the simplest
    machine learning algorithms, it is still used widely. The strengths and weaknesses
    of this algorithm are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分类中的最近邻方法以**k-最近邻算法**（**k-NN**）为例。尽管这可能是最简单的机器学习算法之一，但它仍然被广泛使用。该算法的优点和缺点如下：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Simple and effective
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单而有效
- en: Makes no assumptions about the underlying data distribution
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对底层数据分布不做假设
- en: Fast training phase
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段快速
- en: '|'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Does not produce a model, limiting the ability to understand how the features
    are related to the class
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不产生模型，限制了理解特征与类别之间关系的能力
- en: Requires selection of an appropriate *k*
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要选择合适的**k**
- en: Slow classification phase
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类阶段较慢
- en: Nominal features and missing data require additional processing
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名义特征和缺失数据需要额外处理
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The k-NN algorithm gets its name from the fact that it uses information about
    an example’s *k* nearest neighbors to classify unlabeled examples. The letter
    *k* is a variable implying that any number of nearest neighbors could be used.
    After choosing *k*, the algorithm requires a training dataset made up of examples
    that have been classified into several categories, as labeled by a nominal variable.
    Then, for each unlabeled record in the test dataset, k-NN identifies the *k* records
    in the training data that are the “nearest” in similarity. The unlabeled test
    instance is assigned the class representing the majority of the *k* nearest neighbors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法的名字来源于它使用关于示例的**k**个最近邻的信息来对未标记的示例进行分类。字母**k**是一个变量，意味着可以使用任意数量的最近邻。选择**k**之后，算法需要一个由已分类的示例组成的训练数据集，这些示例被一个名义变量标记。然后，对于测试数据集中的每个未标记记录，k-NN识别出训练数据中与相似度“最近”的**k**个记录。未标记的测试实例被分配给代表**k**个最近邻中大多数的类别。
- en: 'To illustrate this process, let’s revisit the blind tasting experience described
    in the introduction. Suppose that prior to eating the mystery meal, we had created
    a dataset in which we recorded our impressions of a set of previously tasted ingredients.
    To keep things simple, we rated only two features of each ingredient. The first
    is a measure from 1 to 10 of how crunchy the ingredient is, and the second is
    a score from 1 to 10 measuring how sweet the ingredient tastes. We then labeled
    each ingredient as one of three types of food: fruits, vegetables, or proteins,
    ignoring other foods such as grains and fats.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个过程，让我们回顾一下引言中描述的盲品体验。假设在品尝神秘餐点之前，我们创建了一个数据集，记录了我们之前品尝的一组成分的印象。为了简化问题，我们只对每个成分的两个特征进行了评分。第一个是从1到10的脆度度量，第二个是从1到10的甜度评分。然后，我们将每个成分标记为三种食物类型之一：水果、蔬菜或蛋白质，忽略其他食物如谷物和脂肪。
- en: 'The first few rows of such a dataset might be structured as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个数据集的前几行可能结构如下：
- en: '| **Ingredient** | **Sweetness** | **Crunchiness** | **Food type** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **成分** | **甜度** | **脆度** | **食物类型** |'
- en: '| Apple | 10 | 9 | Fruit |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 苹果 | 10 | 9 | 水果 |'
- en: '| Bacon | 1 | 4 | Protein |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 火腿 | 1 | 4 | 蛋白质 |'
- en: '| Banana | 10 | 1 | Fruit |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 香蕉 | 10 | 1 | 水果 |'
- en: '| Carrot | 7 | 10 | Vegetable |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 胡萝卜 | 7 | 10 | 蔬菜 |'
- en: '| Celery | 3 | 10 | Vegetable |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 芹菜 | 3 | 10 | 蔬菜 |'
- en: 'The k-NN algorithm treats the features as coordinates in a multidimensional
    **feature space**, which is a space comprising all possible combinations of feature
    values. Because the ingredient dataset includes only two features, its feature
    space is two-dimensional. We can plot two-dimensional data on a scatterplot, with
    the *x* dimension indicating the ingredient’s sweetness and the *y* dimension
    indicating the crunchiness. After adding a few more ingredients to the taste dataset,
    the scatterplot might look like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN算法将特征视为多维**特征空间**中的坐标，这是一个包含所有可能的特征值组合的空间。因为成分数据集只包括两个特征，所以其特征空间是二维的。我们可以在散点图上绘制二维数据，其中*x*轴表示成分的甜度，而*y*轴表示脆度。在味道数据集中添加更多成分后，散点图可能看起来像这样：
- en: '![Graphical user interface  Description automatically generated](img/B17290_03_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成](img/B17290_03_01.png)'
- en: 'Figure 3.1: A scatterplot of selected foods’ crunchiness versus sweetness'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：所选食品的脆度与甜度散点图
- en: 'Do you notice a pattern? Similar types of food tend to be grouped closely together.
    As illustrated in *Figure 3.2*, vegetables tend to be crunchy but not sweet; fruits
    tend to be sweet and either crunchy or not crunchy; and proteins tend to be neither
    crunchy nor sweet:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您注意到一个模式吗？相似类型的食品往往紧密地聚集在一起。如图*3.2*所示，蔬菜往往脆而不甜；水果往往甜且要么脆要么不脆；蛋白质往往既不脆也不甜：
- en: '![Diagram  Description automatically generated](img/B17290_03_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B17290_03_02.png)'
- en: 'Figure 3.2: Foods that are similarly classified tend to have similar attributes'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：被相似分类的食品往往具有相似的特征
- en: 'Suppose that after constructing this dataset, we decide to use it to settle
    the age-old question: is a tomato a fruit or a vegetable? We can use the nearest
    neighbor approach to determine which class is a better fit, as shown in *Figure
    3.3*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在构建了这个数据集之后，决定用它来解决一个古老的疑问：番茄是水果还是蔬菜？我们可以使用最近邻方法来确定哪个类别更合适，如图*3.3*所示：
- en: '![Diagram  Description automatically generated](img/B17290_03_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B17290_03_03.png)'
- en: 'Figure 3.3: The tomato’s nearest neighbors provide insight into whether it
    is a fruit or vegetable'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：番茄的最近邻提供了关于它是水果还是蔬菜的见解
- en: Measuring similarity with distance
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用距离测量相似性
- en: Locating the tomato’s nearest neighbors requires a **distance function**, which
    is a formula that measures the similarity between two instances.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 定位番茄的最近邻需要一个**距离函数**，这是一个衡量两个实例之间相似性的公式。
- en: There are many ways to calculate distance. The choice of distance function may
    impact the model’s performance substantially, although it is difficult to know
    which to use except by comparing them directly on the desired learning task. Traditionally,
    the k-NN algorithm uses **Euclidean distance**, which is the distance one would
    measure if it were possible to use a ruler to connect two points. Euclidean distance
    is measured “as the crow flies,” which implies the shortest direct route. This
    is illustrated in the previous figure by the dotted lines connecting the tomato
    to its neighbors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种计算距离的方法。距离函数的选择可能会对模型的性能产生重大影响，尽管除了直接在期望的学习任务上比较它们之外，很难知道应该使用哪种。传统上，k-NN算法使用**欧几里得距离**，这是如果可以使用尺子连接两个点时测量的距离。欧几里得距离是“如鸟飞”测量的，这暗示了最短的直接路线。这在上一个图中通过连接番茄及其邻居的虚线说明了。
- en: Another common distance measure is **Manhattan distance**, which is based on
    the paths a pedestrian would take by walking city blocks. If you are interested
    in learning more about other distance measures, you can read the documentation
    for R’s distance function using the `?dist` command.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的距离度量是**曼哈顿距离**，它基于行人通过走城市街区所走的路径。如果您想了解更多关于其他距离度量的信息，可以使用R的`?dist`命令查看距离函数的文档。
- en: 'Euclidean distance is specified by the following formula, where *p* and *q*
    are the examples to be compared, each having *n* features. The term *p*¹ refers
    to the value of the first feature of example *p*, while *q*¹ refers to the value
    of the first feature of example *q*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离由以下公式指定，其中*p*和*q*是要比较的例子，每个例子都有*n*个特征。术语*p*¹指的是例子*p*的第一个特征值，而*q*¹指的是例子*q*的第一个特征值：
- en: '![](img/B17290_03_001.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_03_001.png)'
- en: 'The distance formula involves comparing the values of each example’s features.
    For example, to calculate the distance between the tomato (sweetness = 6, crunchiness
    = 4), and the green bean (sweetness = 3, crunchiness = 7), we can use the formula
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 距离公式涉及比较每个示例特征的值。例如，为了计算番茄（甜度=6，脆度=4）和绿豆（甜度=3，脆度=7）之间的距离，我们可以使用以下公式：
- en: '![](img/B17290_03_002.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_03_002.png)'
- en: 'In a similar vein, we can calculate the distance between the tomato and several
    of its closest neighbors as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以计算番茄与其几个最近邻居之间的距离，如下所示：
- en: '| **Ingredient** | **Sweetness** | **Crunchiness** | **Food type** | **Distance
    to the tomato** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **成分** | **甜度** | **脆度** | **食物类型** | **到番茄的距离** |'
- en: '| Grape | 8 | 5 | Fruit | sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 葡萄 | 8 | 5 | 水果 | sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 |'
- en: '| Green bean | 3 | 7 | Vegetable | sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 绿豆 | 3 | 7 | 蔬菜 | sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 |'
- en: '| Nuts | 3 | 6 | Protein | sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 坚果 | 3 | 6 | 蛋白质 | sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 |'
- en: '| Orange | 7 | 3 | Fruit | sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 橙子 | 7 | 3 | 水果 | sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 |'
- en: To classify the tomato as a vegetable, protein, or fruit, we’ll begin by assigning
    the tomato the food type of its single nearest neighbor. This is called 1-NN classification
    because *k = 1*. The orange is the single nearest neighbor to the tomato, with
    a distance of 1.4\. Because an orange is a fruit, the 1-NN algorithm would classify
    a tomato as a fruit.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将番茄分类为蔬菜、蛋白质或水果，我们首先将番茄分配给其单个最近邻居的食物类型。这被称为1-NN分类，因为*k = 1*。橙子是番茄的单个最近邻居，距离为1.4。因为橙子是水果，所以1-NN算法会将番茄分类为水果。
- en: 'If we use the k-NN algorithm with *k = 3* instead, it performs a vote among
    the three nearest neighbors: orange, grape, and nuts. Now, because the majority
    class among these neighbors is fruit (with two of the three votes), the tomato
    again is classified as a fruit.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用*k = 3*的k-NN算法，它将在三个最近邻居中进行投票：橙子、葡萄和坚果。现在，因为在这三个邻居中，多数类别是水果（有三个投票中的两个），所以番茄再次被分类为水果。
- en: Choosing an appropriate k
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的k值
- en: The decision of how many neighbors to use for k-NN determines how well the model
    will generalize to future data. The balance between overfitting and underfitting
    the training data is a problem known as the **bias-variance tradeoff**. Choosing
    a large *k* reduces the impact of variance caused by noisy data but can bias the
    learner such that it runs the risk of ignoring small but important patterns.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如何决定k-NN算法中使用的邻居数量决定了模型将如何泛化到未来的数据。在过拟合和欠拟合训练数据之间的平衡是一个被称为**偏差-方差权衡**的问题。选择较大的*k*值可以减少由噪声数据引起的方差的影响，但可能会使学习器产生偏差，从而有忽略小但重要模式的风险。
- en: Suppose we took the extreme stance of setting a very large *k*, as large as
    the total number of observations in the training data. With every training instance
    represented in the final vote, the most common class always has a majority of
    the voters. The model would consequently always predict the majority class, regardless
    of the nearest neighbors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们采取极端立场，将一个非常大的*k*值设定为与训练数据中观察到的总数一样大。由于每个训练实例都代表最终投票，最常见的类别总是拥有大多数投票者。因此，模型将始终预测多数类别，无论最近的邻居是什么。
- en: On the opposite extreme, using a single nearest neighbor allows noisy data and
    outliers to unduly influence the classification of examples. For example, suppose
    some of the training examples were accidentally mislabeled. Any unlabeled example
    that happens to be nearest to the incorrectly labeled neighbor will be predicted
    to have the incorrect class, even if nine other nearby neighbors would have voted
    differently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在相反的极端情况下，使用单个最近邻居允许噪声数据和异常值过度影响示例的分类。例如，假设一些训练示例被意外地错误标记。任何恰好与错误标记的邻居最近的未标记示例将被预测为具有错误的类别，即使其他九个附近的邻居会投票不同。
- en: Obviously, the best *k* value is somewhere between these two extremes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，最佳的*k*值介于这两个极端之间。
- en: '*Figure 3.4* illustrates, more generally, how the decision boundary (depicted
    by a dashed line) is affected by larger or smaller *k* values. Smaller values
    allow more complex decision boundaries that more carefully fit the training data.
    The problem is that we do not know whether the straight boundary or the curved
    boundary better represents the true underlying concept to be learned.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.4* 展示了更一般的情况，说明了决策边界（由虚线表示）如何受到较大或较小的 *k* 值的影响。较小的值允许更复杂的决策边界，更仔细地拟合训练数据。问题是，我们不知道直线边界还是曲线边界更好地代表了要学习的真实概念。'
- en: '![](img/B17290_03_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_03_04.png)'
- en: 'Figure 3.4: A larger k has higher bias and lower variance than a smaller k'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：较大的 *k* 相比较小的 *k* 具有更高的偏差和更低的方差
- en: In practice, the choice of *k* depends on the difficulty of the concept to be
    learned and the number of records in the training data. One common approach is
    to begin with *k* equal to the square root of the number of training examples.
    In the food classifier developed previously, we might set *k = 4* because there
    were 15 example ingredients in the training data and the square root of 15 is
    3.87.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，*k* 的选择取决于要学习概念的程度和训练数据中的记录数量。一种常见的方法是从 *k* 等于训练示例数量的平方根开始。在之前开发的食品分类器中，我们可能会将
    *k = 4*，因为训练数据中有15种示例成分，15的平方根是3.87。
- en: However, such rules may not always result in the single best *k*. An alternative
    approach is to test several *k* values on a variety of test datasets and choose
    the one that delivers the best classification performance. That said, unless the
    data is very noisy, a large training dataset can make the choice of *k* less important.
    This is because even subtle concepts will have a sufficiently large pool of examples
    to vote as nearest neighbors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样的规则并不总是导致最佳的 *k*。一种替代方法是测试多种 *k* 值在各种测试数据集上的表现，并选择提供最佳分类性能的那个。尽管如此，除非数据非常嘈杂，否则大量训练数据集可以使
    *k* 的选择不那么重要。这是因为即使是微妙的概念也会有一个足够大的示例池来作为最近邻进行投票。
- en: A less common, but still interesting, solution to this problem is to choose
    a larger *k* and use a weighted voting process in which the vote of closer neighbors
    is considered more authoritative than the vote of neighbors that are far away.
    Some k-NN implementations offer this option.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的一个不太常见但仍然有趣的解决方案是选择一个较大的 *k*，并使用加权投票过程，其中较近的邻居的投票被认为比较远的邻居的投票更有权威性。一些
    k-NN 实现提供了这个选项。
- en: Preparing data for use with k-NN
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据以用于 k-NN
- en: Features are typically transformed to a standard range prior to applying the
    k-NN algorithm. The rationale for this step is that the distance formula is highly
    dependent on how features are measured. In particular, if certain features have
    a much larger range of values than others, the distance measurements will be strongly
    dominated by the features with larger ranges. This wasn’t a problem for the food
    tasting example, as both sweetness and crunchiness were measured on a scale from
    1 to 10.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 k-NN 算法之前，特征通常会被转换到标准范围内。这一步骤的合理性在于距离公式高度依赖于特征的测量方式。特别是，如果某些特征的范围值比其他特征大得多，距离测量将强烈地受较大范围的特征所支配。在食品品尝的例子中，这不是问题，因为甜味和脆性都是在1到10的尺度上测量的。
- en: However, suppose that we added an additional feature to the dataset to represent
    a food’s spiciness, which was measured using the Scoville scale. If you are unfamiliar
    with this metric, it is a standardized measure of spice heat, ranging from zero
    (not at all spicy) to over a million (for the hottest chili peppers). Since the
    difference between spicy and non-spicy foods can be over a million while the difference
    between sweet and non-sweet or crunchy and non-crunchy foods is at most 10, the
    difference in scale allows the spice level to impact the distance function much
    more than the other two factors. Without adjusting our data, we might find that
    our distance measures only differentiate foods by their spiciness; the impact
    of crunchiness and sweetness would be dwarfed by the contribution of spiciness.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们向数据集添加了一个额外的特征来表示食物的辣度，这是使用斯科维尔尺度测量的。如果你不熟悉这个指标，它是一种标准化的辣度衡量，范围从零（一点也不辣）到超过一百万（对于最热的辣椒）。由于辣味和非辣味食物之间的差异可能超过一百万，而甜味和非甜味或脆性和非脆性食物之间的差异最多为10，因此这种尺度差异使得辣度对距离函数的影响远大于其他两个因素。如果不调整我们的数据，我们可能会发现我们的距离度量只能区分食物的辣度；脆性和甜味的影响会被辣度的贡献所淹没。
- en: The solution is to rescale the features by shrinking or expanding their range
    such that each one contributes relatively equally to the distance formula. For
    example, if sweetness and crunchiness are both measured on a scale from 1 to 10,
    we would also like spiciness to be measured on a scale from 1 to 10\. There are
    several common ways to accomplish such scaling.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是通过缩小或扩大特征的范围来重新缩放特征，使得每个特征对距离公式的贡献相对相等。例如，如果甜度和脆度都是用1到10的尺度来衡量的，我们也希望辣度也是用1到10的尺度来衡量。有几种常见的方法可以实现这种缩放。
- en: 'The traditional method of rescaling features for k-NN is **min-max normalization**.
    This process transforms a feature such that all values fall in a range between
    0 and 1\. The formula for normalizing a feature is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN特征缩放的传统方法是**最小-最大归一化**。这个过程将特征转换为一个值，使其落在0到1之间的范围内。归一化特征的公式如下：
- en: '![](img/B17290_03_003.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_03_003.png)'
- en: To transform each value of feature *X*, the formula subtracts the minimum *X*
    value and divides it by the range of *X*. The resulting normalized feature values
    can be interpreted as indicating how far, from 0 percent to 100 percent, the original
    value fell along the range between the original minimum and maximum.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要转换特征*X*的每个值，公式从最小*X*值中减去，然后除以*X*的范围。得到的归一化特征值可以解释为表示原始值在原始最小值和最大值之间的范围内，从0%到100%的距离。
- en: 'Another common transformation is called **z-score standardization**. The following
    formula subtracts the mean value of feature *X*, and divides the result by the
    standard deviation of *X*:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的转换方法被称为**z分数标准化**。以下公式从特征*X*的均值中减去，然后将结果除以*X*的标准差：
- en: '![](img/B17290_03_004.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_03_004.png)'
- en: This formula, which is based on properties of the normal distribution covered
    in *Chapter 2*, *Managing and Understanding Data*, rescales each of a feature’s
    values in terms of how many standard deviations they fall above or below the mean.
    The resulting value is called a **z-score**. The z-scores fall in an unbounded
    range of negative and positive numbers. Unlike the normalized values, they have
    no predefined minimum and maximum.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式基于*第2章*中介绍的**正态分布**的性质，根据特征值相对于均值的多少个标准差进行重新缩放。得到的值称为**z分数**。z分数落在负数和正数的无界范围内。与归一化值不同，它们没有预定义的最小值和最大值。
- en: The same rescaling method used on the k-NN training dataset must also be applied
    to the test examples that the algorithm will later classify. This can lead to
    a tricky situation for min-max normalization, as the minimum or maximum of future
    cases might be outside the range of values observed in the training data. If you
    know the theoretical minimum or maximum value ahead of time, you can use these
    constants rather than the observed minimum and maximum values. Alternatively,
    you can use z-score standardization under the assumption that the future examples
    are taken from a distribution with the same mean and standard deviation as the
    training examples.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在k-NN训练数据集上使用的相同缩放方法也必须应用于算法随后将进行分类的测试示例。这可能导致最小-最大归一化出现棘手的情况，因为未来的案例的最小值或最大值可能超出训练数据中观察到的值范围。如果你事先知道理论上的最小值或最大值，你可以使用这些常数而不是观察到的最小值和最大值。或者，你可以假设未来的示例来自与训练示例具有相同均值和标准差的分布，使用z分数标准化。 '
- en: 'The Euclidean distance formula is undefined for nominal data. Therefore, to
    calculate the distance between nominal features, we need to convert them into
    a numeric format. A typical solution utilizes **dummy coding**, where a value
    of 1 indicates one category, and 0 indicates the other. For instance, dummy coding
    for a male or non-male sex variable could be constructed as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离公式对于名义数据是未定义的。因此，为了计算名义特征之间的距离，我们需要将它们转换为数值格式。一个典型的解决方案是使用**虚拟编码**，其中1表示一个类别，0表示另一个。例如，男性或非男性性别变量的虚拟编码可以构建如下：
- en: '![Diagram  Description automatically generated](img/B17290_03_05.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图解 描述自动生成](img/B17290_03_05.png)'
- en: Notice how dummy coding of the two-category (binary) sex variable results in
    a single new feature named male. There is no need to construct a separate feature
    for non-male. Since both are mutually exclusive, knowing one or the other is enough.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，二元（二进制）性别变量的虚拟编码会产生一个名为male的单个新特征。不需要为非男性构建单独的特征。由于两者互斥，知道其中一个就足够了。
- en: 'This is true more generally as well. An *n*-category nominal feature can be
    dummy coded by creating binary indicator variables for *n - 1* levels of the feature.
    For example, dummy coding for a three-category temperature variable (for example,
    hot, medium, or cold) could be set up as *(3 - 1) = 2* features, as shown here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这在更广泛的意义上也是正确的。一个*n*-类别的名义特征可以通过为特征的*n - 1*个级别创建二进制指示变量来进行虚拟编码。例如，对于一个三分类的温度变量（例如，热、中等或冷）的虚拟编码可以设置为*(3
    - 1) = 2*个特征，如下所示：
- en: '![Text  Description automatically generated](img/B17290_03_06.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17290_03_06.png)'
- en: Knowing that hot and medium are both 0 provides enough information to know that
    the temperature is cold, and thus, a third binary feature for the cold category
    is unnecessary. However, a widely used close sibling of dummy coding known as
    **one-hot encoding** creates binary features for all *n* levels of the feature,
    rather than *n - 1* as with dummy coding. It is known as “one-hot” because only
    one attribute is coded as 1 and the others are set to 0.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 知道热和中等都是0，就足以知道温度是冷的，因此，对于冷类别不需要第三个二进制特征。然而，一个广泛使用的虚拟编码的近亲，称为**独热编码**，为特征的*n*个级别创建二进制特征，而不是像虚拟编码那样*n
    - 1*。它被称为“独热”，因为只有一个属性被编码为1，其他都被设置为0。
- en: In practice, there is virtually no difference between these two methods, and
    the results of machine learning will be unaffected by the choice of coding. This
    being said, one-hot encoding can cause problems with linear models, such as those
    described in *Chapter 6*, *Forecasting Numeric Data – Regression Methods*, and
    thus one-hot encoding is often avoided among statisticians or in fields like economics
    that rely heavily on such models. On the other hand, one-hot encoding has become
    prevalent in the field of machine learning and is often treated synonymously with
    dummy coding for the simple reason that the choice makes virtually no difference
    in the model fit; yet, in one-hot encoding, the model itself may be easier to
    understand since all levels of the categorical features are specified explicitly.
    This book uses only dummy coding since it can be used universally, but you may
    encounter one-hot encoding elsewhere.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这两种方法几乎没有任何区别，机器学习的成果也不会受到编码选择的影响。尽管如此，独热编码可能会给线性模型带来问题，例如在第6章中描述的*预测数值数据
    - 回归方法*，因此独热编码在统计学家或像经济学这样高度依赖此类模型的领域中通常被避免。另一方面，独热编码在机器学习领域已经变得普遍，并且通常与虚拟编码同义，仅仅是因为这种选择对模型拟合几乎没有影响；然而，在独热编码中，模型本身可能更容易理解，因为所有分类特征的级别都被明确指定。这本书只使用虚拟编码，因为它可以通用，但你可能在其他地方遇到独热编码。
- en: A convenient aspect of both dummy and one-hot coding is that the distance between
    dummy-coded features is always one or zero, and thus, the values fall on the same
    scale as min-max normalized numeric data. No additional transformation is necessary.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟编码和独热编码的一个方便之处在于，虚拟编码的特征之间的距离总是1或0，因此，这些值与min-max归一化的数值数据处于相同的尺度上。不需要额外的转换。
- en: If a nominal feature is ordinal (one could make such an argument for temperature),
    an alternative to dummy coding is to number the categories and apply normalization.
    For instance, cold, warm, and hot could be numbered as 1, 2, and 3, which normalizes
    to 0, 0.5, and 1\. A caveat to this approach is that it should only be used if
    the steps between categories are equivalent. For instance, although income categories
    for poor, middle class, and wealthy are ordered, the difference between poor and
    middle class may be different than the difference between middle class and wealthy.
    Since the steps between groups are not equal, dummy coding is a safer approach.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个名义特征是序数的（有人可以为温度提出这样的论点），虚拟编码的一个替代方案是对类别进行编号并应用归一化。例如，冷、暖和热可以编号为1、2和3，这归一化到0、0.5和1。这种方法的注意事项是，它应该只在类别之间的步骤相等时使用。例如，尽管贫困、中产阶级和富裕的收入类别是有序的，但贫困和中产阶级之间的差异可能不同于中产阶级和富裕之间的差异。由于组之间的步骤不相等，虚拟编码是一个更安全的方法。
- en: Why is the k-NN algorithm lazy?
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么k-NN算法是懒惰的？
- en: Classification algorithms based on nearest neighbor methods are considered **lazy
    learning** algorithms because, technically speaking, no abstraction occurs. The
    abstraction and generalization processes are skipped altogether, which undermines
    the definition of learning proposed in *Chapter 1*, *Introducing Machine Learning*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最近邻方法的分类算法被认为是**懒惰学习**算法，因为从技术角度来说，没有发生抽象化。抽象化和泛化过程完全被跳过，这违反了在*第一章*，“介绍机器学习”中提出的学习的定义。
- en: Under the strict definition of learning, a lazy learner is not really learning
    anything. Instead, it merely stores the training data verbatim. This allows the
    training phase, which is not actually training anything, to occur very rapidly.
    Of course, the downside is that the process of making predictions tends to be
    relatively slow by comparison. Due to the heavy reliance on the training instances
    rather than an abstracted model, lazy learning is also known as **instance-based
    learning** or **rote learning**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在严格的学习定义下，懒惰学习器实际上并没有学习任何东西。相反，它只是逐字逐句地存储训练数据。这使得训练阶段，实际上并没有进行任何训练，可以非常快速地进行。当然，缺点是预测过程通常相对较慢。由于高度依赖于训练实例而不是抽象化的模型，懒惰学习也被称为**基于实例的学习**或**死记硬背学习**。
- en: As instance-based learners do not build a model, the method is said to be in
    a class of **non-parametric** learning methods—no parameters are learned about
    the data. Without generating theories about the underlying data, non-parametric
    methods limit our ability to understand how the classifier is using the data,
    yet it can still make useful predictions. Non-parametric learning allows the learner
    to find natural patterns rather than trying to fit the data into a preconceived
    and potentially biased functional form.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于实例的学习者不构建模型，这种方法被称为**非参数**学习方法的类别——关于数据没有学习任何参数。由于没有生成关于潜在数据的理论，非参数方法限制了我们对分类器如何使用数据的理解，尽管它仍然可以做出有用的预测。非参数学习允许学习者找到自然模式，而不是试图将数据拟合到预先设定的和可能存在偏差的函数形式。
- en: '![Diagram  Description automatically generated](img/B17290_03_07.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图解  自动生成的描述](img/B17290_03_07.png)'
- en: 'Figure 3.5: Machine learning algorithms have different biases and may come
    to different conclusions!'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：机器学习算法有不同的偏差，可能会得出不同的结论！
- en: Although k-NN classifiers may be considered lazy, they are still quite powerful.
    As you will soon see, the simple principles of nearest neighbor learning can be
    used to automate the process of screening for cancer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然k-NN分类器可能被认为是懒惰的，但它们仍然非常强大。正如你很快就会看到的，最近邻学习的简单原理可以用来自动化癌症筛查的过程。
- en: Example – diagnosing breast cancer with the k-NN algorithm
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 使用k-NN算法诊断乳腺癌
- en: Routine breast cancer screening allows the disease to be diagnosed and treated
    prior to it causing noticeable symptoms. The process of early detection involves
    examining the breast tissue for abnormal lumps or masses. If a lump is found,
    a fine-needle aspiration biopsy is performed, which uses a hollow needle to extract
    a small sample of cells from the mass. A clinician then examines the cells under
    a microscope to determine whether the mass is likely to be malignant or benign.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定期乳腺癌筛查可以在疾病引起明显症状之前对其进行诊断和治疗。早期检测的过程涉及检查乳腺组织中的异常肿块或团块。如果发现肿块，则进行细针穿刺活检，使用空心针从肿块中提取一小部分细胞。然后，临床医生在显微镜下检查这些细胞，以确定肿块是否可能是恶性的或良性的。
- en: If machine learning could automate the identification of cancerous cells, it
    would provide considerable benefit to the health system. Automated processes are
    likely to improve the efficiency of the detection process, allowing physicians
    to spend less time diagnosing and more time treating the disease. An automated
    screening system might also provide greater detection accuracy by removing the
    inherently subjective human component from the process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习能够自动化识别癌细胞，将对医疗系统带来相当大的好处。自动化的流程可能会提高检测过程的效率，让医生有更多时间用于治疗疾病而不是诊断。自动筛查系统也可能通过消除过程中固有的主观性人类因素，提供更高的检测准确性。
- en: Let’s investigate the utility of machine learning for detecting cancer by applying
    the k-NN algorithm to measurements of biopsied cells from women with abnormal
    breast masses.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将k-NN算法应用于来自有异常乳腺肿块女性的活检细胞测量值，来调查机器学习在检测癌症方面的效用。
- en: Step 1 – collecting data
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 - 收集数据
- en: We will utilize the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI
    Machine Learning Repository at `http://archive.ics.uci.edu/ml`. This data was
    donated by researchers at the University of Wisconsin and includes measurements
    from digitized images of fine-needle aspirations of a breast mass. The values
    represent characteristics of the cell nuclei present in the digital image.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用来自UCI机器学习仓库的威斯康星乳腺癌（诊断）数据集，网址为`http://archive.ics.uci.edu/ml`。这些数据由威斯康星大学的研究人员捐赠，包括来自乳腺肿块细针吸取的数字化图像的测量值。这些值代表数字图像中存在的细胞核的特征。
- en: To read more about this dataset, refer to *Breast Cancer Diagnosis and Prognosis
    via Linear Programming, Mangasarian OL, Street WN, Wolberg WH, Operations Research,
    1995, Vol. 43, pp. 570-577*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于这个数据集的信息，请参阅 *《通过线性规划进行乳腺癌诊断和预后，Mangasarian OL，Street WN，Wolberg WH，运筹学，1995，第43卷，第570-577页》*。
- en: The breast cancer data includes 569 examples of cancer biopsies, each with 32
    features. One feature is an identification number, another is the cancer diagnosis,
    and 30 are numeric-valued laboratory measurements. The diagnosis is coded as “M”
    to indicate malignant or “B” to indicate benign.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据包括569个癌症活检示例，每个示例有32个特征。其中一个特征是识别号，另一个是癌症诊断，其余30个是数值型实验室测量值。诊断用“M”表示恶性，用“B”表示良性。
- en: The 30 numeric measurements comprise the mean, standard error, and worst (that
    is, largest) value for 10 different characteristics of the digitized cell nuclei,
    such as radius, texture, area, smoothness, and compactness. Based on the feature
    names, the dataset seems to measure the shape and size of the cell nuclei, but
    unless you are an oncologist, you are unlikely to know how each of these relates
    to benign or malignant masses. No such expertise is necessary, as the computer
    will discover the important patterns during the machine learning process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这30个数值测量值包括10个不同特征的平均值、标准误差和最差（即最大）值，例如数字化细胞核的半径、纹理、面积、平滑度和紧密度。根据特征名称，该数据集似乎测量细胞核的形状和大小，但除非你是肿瘤学家，否则你不太可能知道这些特征中的每一个如何与良性或恶性肿块相关。不需要这样的专业知识，因为计算机将在机器学习过程中发现重要的模式。
- en: Step 2 – exploring and preparing the data
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: By exploring the data, we may be able to shine some light on the relationships
    between the features and the cancer status. In doing so, we will prepare the data
    for use with the k-NN learning method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索数据，我们可能能够揭示特征与癌症状态之间的关系。在这样做的时候，我们将为使用k-NN学习方法准备数据。
- en: If you plan on following along, download the code and `wisc_bc_data.csv` files
    from the GitHub repository and save them to your R working directory. For this
    book, the dataset was modified very slightly from its original form. In particular,
    a header line was added, and the rows of data were randomly ordered.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算跟随操作，请从GitHub仓库下载代码和`wisc_bc_data.csv`文件，并将它们保存到你的R工作目录中。对于这本书，数据集与其原始形式略有不同。特别是，添加了一个标题行，并且数据行的顺序是随机排列的。
- en: 'We’ll begin by importing the CSV data file as we have done in previous chapters,
    saving the Wisconsin breast cancer data to the `wbcd` data frame:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像前几章所做的那样，首先导入CSV数据文件，将威斯康星乳腺癌数据保存到`wbcd`数据框中：
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the command `str(wbcd)`, we can confirm that the data is structured with
    569 examples and 32 features, as we expected. The first several lines of output
    are as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令`str(wbcd)`，我们可以确认数据结构为569个示例和32个特征，正如我们所预期的。输出的一些前几行如下：
- en: '[PRE1]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first feature is an integer variable named `id`. As this is simply a unique
    identifier (ID) for each patient in the data, it does not provide useful information,
    and we will need to exclude it from the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个特征是一个名为`id`的整数变量。由于这只是一个为数据中的每个患者提供的唯一标识符（ID），它不提供有用的信息，因此我们需要将其排除在模型之外。
- en: Regardless of the machine learning method, ID variables should always be excluded.
    Neglecting to do so can lead to erroneous findings because the ID can be used
    to correctly predict each example. Therefore, a model that includes an ID column
    will almost definitely suffer from overfitting and generalize poorly to future
    data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用哪种机器学习方法，ID变量都应该始终排除。忽略这一点可能导致错误的结果，因为ID可以用来正确预测每个示例。因此，包含ID列的模型几乎肯定会过度拟合，并且对未来数据的泛化能力较差。
- en: 'Let’s drop the `id` feature from our data frame. As it is in the first column,
    we can exclude it by making a copy of the `wbcd` data frame without column 1:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的数据框中删除`id`特征。由于它位于第一列，我们可以通过复制不带第1列的`wbcd`数据框来排除它：
- en: '[PRE3]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next feature, `diagnosis`, is of particular interest as it is the target
    outcome we hope to predict. This feature indicates whether the example is from
    a benign or malignant mass. The `table()` output indicates that 357 masses are
    benign, while 212 are malignant:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个特征`diagnosis`特别有趣，因为它是我们希望预测的目标结果。这个特征表示示例是否来自良性或恶性的肿块。`table()`输出表明有357个肿块是良性的，而212个是恶性的：
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Many R machine learning classifiers require the target feature to be coded
    as a factor, so we will need to recode the `diagnosis` column. We will also take
    this opportunity to give the `"B"` and `"M"` values more informative labels using
    the `labels` parameter:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 许多R机器学习分类器需要将目标特征编码为因子，因此我们需要重新编码`diagnosis`列。我们也将利用这个机会，使用`labels`参数给`"B"`和`"M"`值赋予更具有信息量的标签：
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we look at the `prop.table()` output, we now see that the values have
    been labeled `Benign` and `Malignant`, with 62.7 percent and 37.3 percent of the
    masses, respectively:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看`prop.table()`的输出时，现在我们看到值已经被标记为`良性`和`恶性`，分别占总体质量的62.7%和37.3%：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The remaining 30 features are all numeric and, as expected, consist of three
    different measurements of 10 characteristics. For illustrative purposes, we will
    only take a closer look at three of these features:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的30个特征都是数值型，并且如预期的那样，由10个特性的三种不同测量组成。为了说明目的，我们只将更仔细地查看这三个特征：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Looking at the three side by side, do you notice anything problematic about
    the values? Recall that the distance calculation for k-NN is heavily dependent
    upon the measurement scale of the input features. Since smoothness ranges from
    0.05 to 0.16, while area ranges from 143.5 to 2501.0, the impact of area is going
    to be much greater than smoothness in the distance calculation. This could potentially
    cause problems for our classifier, so let’s apply normalization to rescale the
    features to a standard range of values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这三个并排，你注意到值有什么问题吗？回想一下，k-NN的距离计算高度依赖于输入特征的测量尺度。由于平滑度范围从0.05到0.16，而面积范围从143.5到2501.0，面积在距离计算中的影响将远大于平滑度。这可能会给我们的分类器带来潜在问题，所以让我们应用归一化来重新缩放特征到标准值范围内。
- en: Transformation – normalizing numeric data
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换 – 归一化数值数据
- en: 'To normalize these features, we need to create a `normalize()` function in
    R. This function takes a vector `x` of numeric values, and for each value in `x`,
    subtracts the minimum `x` value and divides it by the range of `x` values. Lastly,
    the resulting vector is returned. The code for the function is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了归一化这些特征，我们需要在R中创建一个`normalize()`函数。这个函数接受一个数值值向量`x`，并对`x`中的每个值，减去`x`的最小值，然后除以`x`值的范围。最后，返回结果向量。该函数的代码如下：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After executing the previous code, the `normalize()` function is available
    for use in R. Let’s test the function on a couple of vectors:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，`normalize()`函数在R中可供使用。让我们在几个向量上测试这个函数：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The function appears to be working correctly. Even though the values in the
    second vector are 10 times larger than the first vector, after normalization,
    they are identical.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 函数看起来工作正常。尽管第二个向量中的值是第一个向量的10倍，但在归一化后，它们是相同的。
- en: We can now apply the `normalize()` function to the numeric features in our data
    frame. Rather than normalizing each of the 30 numeric variables individually,
    we will use one of R’s functions to automate the process.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将`normalize()`函数应用于我们的数据框中的数值特征。我们不会单独归一化30个数值变量中的每一个，而是将使用R的一个函数来自动化这个过程。
- en: 'The `lapply()` function takes a list and applies a specified function to each
    list element. As a data frame is a list of equal-length vectors, we can use `lapply()`
    to apply `normalize()` to each feature in the data frame. The final step is to
    convert the list returned by `lapply()` to a data frame using the `as.data.frame()`
    function. The full process looks like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`lapply()`函数接受一个列表，并将指定的函数应用于每个列表元素。由于数据框是等长向量的列表，我们可以使用`lapply()`将`normalize()`应用于数据框中的每个特征。最后一步是使用`as.data.frame()`函数将`lapply()`返回的列表转换为数据框。整个过程如下所示：'
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In plain English, this command applies the `normalize()` function to columns
    2 to 31 in the `wbcd` data frame, converts the resulting list to a data frame,
    and assigns it the name `wbcd_n`. The `_n` suffix is used here as a reminder that
    the values in `wbcd` have been normalized.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说，这个命令将`normalize()`函数应用于`wbcd`数据框的第2至31列，将结果列表转换为数据框，并将其命名为`wbcd_n`。这里使用`_n`后缀作为提醒，说明`wbcd`中的值已经被归一化。
- en: 'To confirm that the transformation was applied correctly, let’s look at one
    variable’s summary statistics:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认转换是否正确应用，让我们看一下一个变量的摘要统计信息：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As expected, the `area_mean` variable, which originally ranged from 143.5 to
    2501.0, now ranges from 0 to 1.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`area_mean` 变量，最初的范围是 143.5 到 2501.0，现在范围是 0 到 1。
- en: To simplify data preparation for this example, min-max normalization was applied
    to the entire dataset—including the rows that will later become the test set.
    In a way, this violates our simulation of unseen future data since, in practice,
    one will generally not know the true minimum and maximum values at the time of
    model training and future values might fall outside the previously observed range.
    A better approach might be to normalize the test set using only the minimum and
    maximum values observed in the training data, and potentially even capping any
    future values at the prior minimum or maximum levels. This being said, whether
    normalization is applied to training and test sets together or separately is unlikely
    to notably impact the model’s performance and does not do so here.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本例的数据准备，对整个数据集（包括后来将成为测试集的行）应用了最小-最大归一化。从某种意义上说，这违反了我们模拟未知未来数据的假设，因为在实践中，在模型训练时通常不知道真正的最小值和最大值，未来的值可能会超出之前观察到的范围。可能更好的方法是在训练数据中仅使用观察到的最小值和最大值来归一化测试集，甚至可能将任何未来的值限制在先前的最小值或最大值水平。尽管如此，无论是将归一化应用于训练集和测试集一起还是分开，都不太可能对模型的性能产生显著影响，并且在这里也没有这样做。
- en: Data preparation – creating training and test datasets
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 创建训练集和测试集
- en: Although all 569 biopsies are labeled with a benign or malignant status, it
    is not very interesting to predict what we already know. Additionally, any performance
    measures we obtain during training may be misleading, as we do not know the extent
    to which the data has been overfitted or how well the learner will generalize
    to new cases. For these reasons, a more interesting question is how well our learner
    performs on a dataset of unseen data. If we had access to a laboratory, we could
    apply our learner to measurements taken from the next 100 masses of unknown cancer
    status and see how well the machine learner’s predictions compare to diagnoses
    obtained using conventional methods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有569个活检都被标记为良性或恶性，但预测我们已经知道的事情并不很有趣。此外，我们在训练过程中获得的任何性能指标可能具有误导性，因为我们不知道数据过度拟合的程度以及学习者对新案例的泛化能力有多好。因此，一个更有趣的问题是我们的学习者在未见数据集上的表现如何。如果我们能够访问一个实验室，我们可以将我们的学习者应用于从下一个100个未知癌症状态的肿瘤中获得的测量值，并查看机器学习者的预测与使用传统方法获得的诊断相比有多好。
- en: 'In the absence of such data, we can simulate this scenario by dividing our
    data into two portions: a training dataset that will be used to build the k-NN
    model and a test dataset that will be used to estimate the predictive accuracy
    of the model. We will use the first 469 records for the training dataset and the
    remaining 100 to simulate new patients.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有此类数据的情况下，我们可以通过将我们的数据分成两部分来模拟这种场景：一个用于构建 k-NN 模型的训练集和一个用于估计模型预测准确性的测试集。我们将使用前469条记录作为训练集，剩余的100条记录来模拟新患者。
- en: 'Using the data extraction methods presented in *Chapter 2*, *Managing and Understanding
    Data*, we will split the `wbcd_n` data frame into `wbcd_train` and `wbcd_test`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第2章中介绍的 *数据管理和理解* 数据提取方法，我们将 `wbcd_n` 数据框拆分为 `wbcd_train` 和 `wbcd_test`：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If the previous commands are confusing, remember that data is extracted from
    data frames using the `[row, column]` syntax. A blank value for the row or column
    value indicates that all rows or columns should be included. Hence, the first
    line of code requests rows 1 to 469 and all columns, and the second line requests
    100 rows from 470 to 569 and all columns.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果之前的命令令人困惑，请记住数据是通过使用 `[行, 列]` 语法从数据框中提取的。行或列值为空表示应包含所有行或列。因此，第一行代码请求第1行到第469行的所有列，第二行请求第470行到第569行的100行和所有列。
- en: When constructing training and test datasets, it is important that each dataset
    is a representative subset of the full set of data. The `wbcd` records were already
    randomly ordered, so we could simply extract 100 consecutive records to create
    a representative test dataset. This would not be appropriate if the data was ordered
    chronologically or in groups of similar values. In these cases, random sampling
    methods would be needed. Random sampling will be discussed in *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建训练集和测试集时，每个数据集必须是完整数据集的代表性子集。`wbcd`记录已经被随机排序，因此我们可以简单地提取100个连续记录来创建一个代表性的测试集。如果数据是按时间顺序或按相似值分组排序的，这就不合适了。在这些情况下，需要使用随机抽样方法。随机抽样将在*第5章*，*分而治之
    – 使用决策树和规则进行分类*中讨论。
- en: 'When we constructed our normalized training and test datasets, we excluded
    the target variable, `diagnosis`. For training the k-NN model, we will need to
    store these class labels in factor vectors, split between the training and test
    datasets:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建归一化的训练集和测试集时，我们排除了目标变量`diagnosis`。为了训练k-NN模型，我们需要将这些类别标签存储在因子向量中，并在训练集和测试集之间分割：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code takes the `diagnosis` factor in the first column of the `wbcd` data
    frame and creates the vectors `wbcd_train_labels` and `wbcd_test_labels`. We will
    use these in the next steps of training and evaluating our classifier.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码从`wbcd`数据框的第一列中的`diagnosis`因子创建向量`wbcd_train_labels`和`wbcd_test_labels`。我们将在训练和评估分类器的下一步中使用这些向量。
- en: Step 3 – training a model on the data
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: Equipped with our training data and vector of labels, we are now ready to classify
    our test records. For the k-NN algorithm, the training phase involves no model
    building; the process of training a so-called “lazy” learner like k-NN simply
    involves storing the input data in a structured format.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了我们的训练数据和标签向量，我们现在可以准备好对测试记录进行分类。对于k-NN算法，训练阶段不涉及模型构建；训练所谓的“懒惰”学习器（如k-NN）的过程只是将输入数据以结构化格式存储。
- en: 'To classify our test instances, we will use a k-NN implementation from the
    `class` package, which provides a set of basic R functions for classification.
    If this package is not already installed on your system, you can install it by
    typing:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对测试实例进行分类，我们将使用`class`包中的k-NN实现，该包提供了一组基本的R分类函数。如果此包尚未安装到您的系统上，您可以通过键入以下命令进行安装：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To load the package during any session in which you wish to use the functions,
    simply enter the `library(class)` command.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要在任何会话中加载包以使用函数，只需输入`library(class)`命令。
- en: The `knn()` function in the `class` package provides a standard, traditional
    implementation of the k-NN algorithm. For each instance in the test data, the
    function will identify the *k* nearest neighbors, using Euclidean distance, where
    *k* is a user-specified number. The test instance is classified by taking a “vote”
    among the *k* nearest neighbors—specifically, this involves assigning the class
    of the majority of neighbors. A tie vote is broken at random.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`class`包中的`knn()`函数提供了kNN算法的标准、传统实现。对于测试数据中的每个实例，该函数将使用欧几里得距离识别最近的*k*个邻居，其中*k*是一个用户指定的数字。通过在最近的*k*个邻居中进行“投票”，对测试实例进行分类——具体来说，这涉及到将大多数邻居的类别分配给测试实例。平票将通过随机方式打破。'
- en: 'There are several other k-NN functions in other R packages that provide more
    sophisticated or more efficient implementations. If you run into limitations with
    `knn()`, search for k-NN on the CRAN website: [https://cran.r-project.org](https://cran.r-project.org).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其他R包中还有几个其他k-NN函数，它们提供了更复杂或更高效的实现。如果您在使用`knn()`时遇到限制，请在CRAN网站上搜索k-NN：[https://cran.r-project.org](https://cran.r-project.org)。
- en: 'Training and classification using the `knn()` function is performed in a single
    command that requires four parameters, as shown in the following table:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`knn()`函数进行训练和分类是通过单个命令完成的，该命令需要四个参数，如下表所示：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B17290_03_08.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  自动生成的描述](img/B17290_03_08.png)'
- en: 'Figure 3.6: kNN classification syntax'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：kNN分类语法
- en: We now have nearly everything we need to apply the k-NN algorithm to this data.
    We’ve split our data into training and test datasets, each with the same numeric
    features. The labels for the training data are stored in a separate factor vector.
    The only remaining parameter is `k`, which specifies the number of neighbors to
    include in the vote.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎已经拥有了应用k-NN算法到这些数据所需的一切。我们已经将数据分为训练集和测试集，每个集都有相同的数值特征。训练数据的标签存储在一个单独的因子向量中。唯一剩下的参数是`k`，它指定了投票中要包含的邻居数量。
- en: As our training data includes 469 instances, we might try `k = 21`, an odd number
    roughly equal to the square root of 469\. With a two-category outcome, using an
    odd number eliminates the possibility of ending with a tie vote.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练数据包括469个实例，我们可能会尝试`k = 21`，这是一个大约等于469平方根的奇数。在双类别结果中，使用奇数消除了最终出现平局投票的可能性。
- en: 'Now we can use the `knn()` function to classify the test data:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`knn()`函数对测试数据进行分类：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `knn()` function returns a factor vector of predicted labels for each of
    the examples in the `wbcd_test` dataset. We have assigned these predictions to
    `wbcd_test_pred`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`knn()`函数返回一个因子向量，其中包含`wbcd_test`数据集中每个示例的预测标签。我们已经将这些预测分配给了`wbcd_test_pred`。'
- en: Step 4 – evaluating model performance
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: The next step of the process is to evaluate how well the predicted classes in
    the `wbcd_test_pred` vector match the actual values in the `wbcd_test_labels`
    vector. To do this, we can use the `CrossTable()` function in the `gmodels` package,
    which was introduced in *Chapter 2*, *Managing and Understanding Data*. If you
    haven’t done so already, please install this package using the `install.packages("gmodels")`
    command.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 流程的下一步是评估`wbcd_test_pred`向量中的预测类别与`wbcd_test_labels`向量中的实际值匹配得有多好。为此，我们可以使用`gmodels`包中的`CrossTable()`函数，该函数在*第2章*，*管理和理解数据*中介绍。如果您还没有这样做，请使用`install.packages("gmodels")`命令安装此包。
- en: 'After loading the package with the `library(gmodels)` command, we can create
    a cross tabulation indicating the agreement between the predicted and actual label
    vectors. Specifying `prop.chisq = FALSE` excludes the unnecessary chi-square values
    from the output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`library(gmodels)`命令加载包后，我们可以创建一个交叉表，表示预测标签和实际标签向量之间的协议。指定`prop.chisq = FALSE`将排除输出中的不必要卡方值：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The resulting table looks like this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的表格看起来像这样：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The cell percentages in the table indicate the proportion of values that fall
    into four categories. The top-left cell indicates the **true negative** results.
    These 61 of 100 values are cases where the mass was benign and the k-NN algorithm
    correctly identified it as such. The bottom-right cell indicates the **true positive**
    results, where the classifier and the clinically determined label agree that the
    mass is malignant. A total of 37 of 100 predictions were true positives.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的单元格百分比表示落入四个类别的值的比例。左上角的单元格表示**真阴性**结果。这100个值中的61个是肿块良性且k-NN算法正确识别为良性病例。右下角的单元格表示**真阳性**结果，其中分类器和临床确定的标签都认为肿块是恶性的。总共100个预测中有37个是真正的阳性。
- en: The cells falling on the other diagonal contain counts of examples where the
    k-NN prediction disagreed with the true label. The two examples in the lower-left
    cell are **false negative** results; in this case, the predicted value was benign,
    but the tumor was actually malignant. Errors in this direction could be extremely
    costly, as they might lead a patient to believe that they are cancer-free, but
    in reality, the disease may continue to spread.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一对角线上的单元格包含着k-NN预测与真实标签不一致的示例计数。左下角的两个示例是**假阴性**结果；在这种情况下，预测值是良性的，但实际上肿瘤是恶性的。这种方向的错误可能极其昂贵，因为它们可能导致患者相信他们是癌症-free，但实际上疾病可能仍在扩散。
- en: The top-right cell would contain the **false positive** results, if there were
    any. These values occur when the model has classified a mass as malignant when
    it actually was benign. Although such errors are less dangerous than a false negative
    result, they should also be avoided, as they could lead to additional financial
    burden on the health care system or stress for the patient, as unnecessary tests
    or treatment may be provided.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有的话，右上角的单元格将包含**假阳性**结果。这些值发生在模型将一个良性肿块分类为恶性的情况下。尽管这种错误比假阴性结果危险更小，但它们也应该被避免，因为它们可能导致医疗保健系统或患者的额外财务负担或压力，因为可能提供不必要的测试或治疗。
- en: If we desired, we could eliminate all false negatives by classifying every mass
    as malignant. Obviously, this is not a realistic strategy. Still, it illustrates
    the fact that prediction involves striking a balance between the false positive
    rate and the false negative rate. In *Chapter 10*, *Evaluating Model Performance*,
    you will learn methods for evaluating predictive accuracy that can be used to
    optimize performance to the costs of each type of error.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，我们可以通过将每个样本分类为恶性来消除所有假阴性。显然，这不是一个现实的策略。然而，这说明了预测涉及在假阳性率和假阴性率之间取得平衡的事实。在*第10章*，*评估模型性能*中，你将学习到评估预测准确性的方法，这些方法可以用来优化性能，并考虑到每种类型错误的成本。
- en: A total of 2 out of 100, or 2 percent of masses were incorrectly classified
    by the k-NN approach. While 98 percent accuracy seems impressive for a few lines
    of R code, we might try another iteration of the model to see if we can improve
    the performance and reduce the number of values that have been incorrectly classified,
    especially because the errors were dangerous false negatives.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在100个样本中，共有2个样本被k-NN方法错误分类，即2%。虽然98%的准确率对于几行R代码来说似乎很令人印象深刻，但我们可能尝试对模型进行另一轮迭代，看看是否可以提高性能并减少错误分类的样本数量，尤其是因为这些错误是危险的假阴性。
- en: Step 5 – improving model performance
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 – 提高模型性能
- en: We will attempt two simple variations on our previous classifier. First, we
    will employ an alternative method for rescaling our numeric features. Second,
    we will try several different *k* values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试对之前的分类器进行两种简单的变化。首先，我们将采用一种替代方法来缩放我们的数值特征。其次，我们将尝试几个不同的*k*值。
- en: Transformation – z-score standardization
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换 – z分数标准化
- en: Although normalization is commonly used for k-NN classification, z-score standardization
    may be a more appropriate way to rescale the features in a cancer dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然归一化通常用于k-NN分类，但z分数标准化可能是在癌症数据集中缩放特征的更合适方式。
- en: Since z-score standardized values have no predefined minimum and maximum, extreme
    values are not compressed towards the center. Even without medical training, one
    might suspect that a malignant tumor might lead to extreme outliers as tumors
    grow uncontrollably. With this in mind, it might be reasonable to allow the outliers
    to be weighted more heavily in the distance calculation. Let’s see whether z-score
    standardization improves our predictive accuracy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于z分数标准化的值没有预定义的最小值和最大值，极端值不会被压缩到中心。即使没有医学培训，一个人也可能怀疑恶性肿瘤可能导致极端的异常值，因为肿瘤不受控制地生长。考虑到这一点，允许在距离计算中更重视异常值可能是合理的。让我们看看z分数标准化是否可以提高我们的预测准确性。
- en: 'To standardize a vector, we can use R’s built-in `scale()` function, which
    by default rescales values using the z-score standardization. The `scale()` function
    can be applied directly to a data frame, so there is no need to use the `lapply()`
    function. To create a z-score standardized version of the `wbcd` data, we can
    use the following command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要标准化一个向量，我们可以使用R的内置`scale()`函数，该函数默认使用z分数标准化来缩放值。`scale()`函数可以直接应用于数据框，因此不需要使用`lapply()`函数。要创建`wbcd`数据的z分数标准化版本，我们可以使用以下命令：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This rescales all features with the exception of `diagnosis` in the first column
    and stores the result as the `wbcd_z` data frame. The `_z` suffix is a reminder
    that the values were z-score transformed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这将重新缩放所有特征，除了第一列的`diagnosis`，并将结果存储为`wbcd_z`数据框。`_z`后缀是一个提醒，表示这些值已经进行了z分数转换。
- en: 'To confirm that the transformation was applied correctly, we can look at the
    summary statistics:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认转换是否正确应用，我们可以查看摘要统计信息：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The mean of a z-score standardized variable should always be zero, and the range
    should be fairly compact. A z-score less than -3 or greater than 3 indicates an
    extremely rare value. Examining the summary statistics with these criteria in
    mind, the transformation seems to have worked.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: z分数标准化的变量的平均值应该始终为零，范围应该相当紧凑。z分数小于-3或大于3表示一个极其罕见的价值。考虑到这些标准，检查摘要统计信息，转换似乎已经生效。
- en: 'As we have done before, we need to divide the z-score-transformed data into
    training and test sets, and classify the test instances using the `knn()` function.
    We’ll then compare the predicted labels to the actual labels using `CrossTable()`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，我们需要将z分数转换后的数据分为训练集和测试集，并使用`knn()`函数对测试实例进行分类。然后我们将使用`CrossTable()`比较预测标签和实际标签：
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Unfortunately, in the following table, the results of our new transformation
    show a slight decline in accuracy. Using the same instances in which we had previously
    classified 98 percent of examples correctly, we now classified only 95 percent
    correctly. Making matters worse, we did no better at classifying the dangerous
    false negatives.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在下面的表格中，我们新转换的结果显示准确性略有下降。使用我们之前正确分类98个百分比的相同实例，我们现在只正确分类了95个百分比。更糟糕的是，我们在分类危险的反例方面也没有做得更好。
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Testing alternative values of k
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试k的不同值
- en: 'We may be able to optimize the performance of the k-NN model by examining its
    performance across various *k* values. Using the normalized training and test
    datasets, the same 100 records need to be classified using several different choices
    of *k*. Given that we are testing only six *k* values, these iterations can be
    performed most simply by using copy-and-paste of our previous `knn()` and `CrossTable()`
    functions. However, it is also possible to write a `for` loop that runs these
    two functions for each of the values in a vector named `k_values`, as demonstrated
    in the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能可以通过检查其在各种*k*值上的性能来优化k-NN模型的性能。使用归一化的训练和测试数据集，需要使用几种不同的*k*值对相同的100条记录进行分类。鉴于我们只测试了六个*k*值，这些迭代可以通过复制粘贴我们之前的`knn()`和`CrossTable()`函数来最简单地执行。然而，也可以编写一个`for`循环，为名为`k_values`的向量中的每个值运行这两个函数，如下面的代码所示：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `for` loop can almost be read as a simple sentence: for each value named
    `k_val` in the `k_values` vector, run the `knn()` function while setting the parameter
    `k` to the current `k_val`, and then produce the `CrossTable()` for the resulting
    predictions.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`for`循环几乎可以读作一个简单的句子：对于`k_values`向量中命名的每个`k_val`值，运行`knn()`函数，同时将参数`k`设置为当前的`k_val`，然后为结果预测生成`CrossTable()`。'
- en: A more sophisticated approach to looping using one of R’s `apply()` functions
    is described in *Chapter 7*, *Black-Box Methods – Neural Networks and Support
    Vector Machines*, to test various values of a cost parameter and plot the result.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章*黑盒方法 - 神经网络和支持向量机*中描述了一种更复杂的方法，使用R的`apply()`函数之一进行循环，以测试成本参数的各种值并绘制结果。
- en: 'The false negatives, false positives, and overall error rate are shown for
    each iteration:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个迭代，显示了假阴性、假阳性和总体错误率：
- en: '| **k value** | **False negatives** | **False positives** | **Error rate**
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **k值** | **假阴性** | **假阳性** | **错误率** |'
- en: '| 1 | 1 | 3 | 4 percent |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 3 | 4 百分比 |'
- en: '| 5 | 2 | 0 | 2 percent |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2 | 0 | 2 百分比 |'
- en: '| 11 | 3 | 0 | 3 percent |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3 | 0 | 3 百分比 |'
- en: '| 15 | 3 | 0 | 3 percent |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 3 | 0 | 3 百分比 |'
- en: '| 21 | 2 | 0 | 2 percent |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 2 | 0 | 2 百分比 |'
- en: '| 27 | 4 | 0 | 4 percent |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 4 | 0 | 4 百分比 |'
- en: Although the classifier was never perfect, the 1-NN approach was able to avoid
    some of the false negatives at the expense of adding false positives. It is important
    to keep in mind, however, that it would be unwise to tailor our approach too closely
    to our test data; after all, a different set of 100 patient records is likely
    to be somewhat different from those used to measure our performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分类器从未完美，但1-NN方法能够通过增加假阳性来避免一些假阴性。然而，重要的是要记住，将我们的方法过于紧密地定制到测试数据上是不明智的；毕竟，一组不同的100份患者记录可能与我们用来衡量我们性能的记录有所不同。
- en: If you need to be certain that a learner will generalize to future data, you
    might create several sets of 100 patients at random and repeatedly retest the
    result. Such methods to carefully evaluate the performance of machine learning
    models will be discussed further in *Chapter 10*, *Evaluating Model Performance*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要确保学习者能够推广到未来的数据，你可能会随机创建几组100名患者的集合，并反复重新测试结果。这些方法将在第10章*评估模型性能*中进一步讨论，以仔细评估机器学习模型的性能。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about classification using k-NN. Unlike many classification
    algorithms, k-nearest neighbors does not do any learning—at least not according
    to the formal definition of machine learning. Instead, it simply stores the training
    data verbatim. Unlabeled test examples are then matched to the most similar records
    in the training set using a distance function, and the unlabeled example is assigned
    the label of its nearest neighbors.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了使用k-NN进行分类。与许多分类算法不同，k-最近邻算法不做任何学习——至少不是根据机器学习的正式定义。相反，它只是逐字存储训练数据。然后使用距离函数将未标记的测试示例与训练集中最相似的记录相匹配，并将未标记的示例分配给其最近的邻居的标签。
- en: Although k-NN is a very simple algorithm, it can tackle extremely complex tasks,
    such as the identification of cancerous masses. In a few simple lines of R code,
    we were able to correctly identify whether a mass was malignant or benign 98 percent
    of the time in an example using real-world data. Although this teaching dataset
    was designed to streamline the process of building a model, the exercise demonstrated
    the ability of learning algorithms to make accurate predictions much like a human
    can.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 k-NN 是一个非常简单的算法，但它可以处理极其复杂的任务，例如癌变组织的识别。在几行简单的 R 代码中，我们能够在使用真实世界数据的例子中，98%
    的时间正确地识别出组织是恶性还是良性。尽管这个教学数据集是为了简化建模过程而设计的，但这个练习展示了学习算法能够像人类一样做出准确预测的能力。
- en: In the next chapter, we will examine a classification method that uses probability
    to estimate the likelihood that an observation falls into certain categories.
    It will be interesting to compare how this approach differs from k-NN. Later,
    in *Chapter 9*, *Finding Groups of Data – Clustering with k-means*, we will learn
    about a close relative to k-NN, which uses distance measures for a completely
    different learning task.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一种使用概率来估计观察值落入某些类别可能性的分类方法。比较这种方法与 k-NN 的不同之处将非常有趣。稍后，在第 9 章 *寻找数据组
    - 使用 k-means 进行聚类* 中，我们将了解 k-NN 的一个近亲，它使用距离度量来完成一个完全不同的学习任务。
- en: Join our book’s Discord space
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下链接加入我们的 Discord 社区，与志同道合的人交流，并与其他 4000 多人一起学习：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/r.jpg)'
