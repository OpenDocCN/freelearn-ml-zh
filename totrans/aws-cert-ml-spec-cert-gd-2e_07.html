<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer105" class="Content">
			<h1 class="chapter-number"><a id="_idTextAnchor969"/>7 </h1>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor970"/>Evaluating and Optimizing Models</h1>
			<p>It is now time to learn how to evaluate and optimize machine learning models. During the process of modeling, or even after model completion, you might want to understand how your model is performing. Each type of model has its own set of metrics that can be used to evaluate performance, and that is what you are going to study in <span class="No-Break">this chapter.</span></p>
			<p>Apart from model evaluation, as a data scientist, you might also need to improve your model’s performance by tuning the hyperparameters of your algorithm. You will take a look at some nuances of this <span class="No-Break">modeling task.</span></p>
			<p>In this chapter, the following topics will <span class="No-Break">be covered:</span></p>
			<ul>
				<li>Introducing <span class="No-Break">model evaluation</span></li>
				<li>Evaluating <span class="No-Break">classification models</span></li>
				<li>Evaluating <span class="No-Break">regression models</span></li>
				<li><span class="No-Break">Model optimization</span></li>
			</ul>
			<p>Alright, time to <span class="No-Break">rock it!</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor971"/><a id="_idTextAnchor972"/>Introducing model evaluation</h1>
			<p>There are several different scenarios in <a id="_idTextAnchor973"/>which you might want to evaluate model performance. Some of them are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>You are creating a model and testing different approaches and/or algorithms. Therefore, you need to compare these models to select the <span class="No-Break">best one.</span></li>
				<li>You have just completed your model and you need to document your work, which includes specifying the model’s performance metrics that you got from the <span class="No-Break">modeling phase.</span></li>
				<li>Your model is running in a production environment, and you need to track its performance. If you encounter model drift, then you might want to retrain <span class="No-Break">the model.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">The term model drift is used to <a id="_idTextAnchor974"/>refer to the problem of model deterioration. When you are building a machine learning model, you must use data to train the algorithm. This set of data is known as training data, and it reflects the business rules at a particular point in time. If these business rules change over time, your model will probably fail to adapt to those changes. This is because it was trained on top of another dataset, which was reflecting another business scenario. To solve this problem, you must retrain the model so that it can consider the rules of the new <span class="No-Break">business scenario.</span></p>
			<p>Model evaluations are commonly inserted in the context of testing. You have learned about holdout validation and cross-validation before. However, both testing approaches share the same requirement: they need a metric in order to <span class="No-Break">evaluate performance.</span></p>
			<p>These metrics are specific to the problem domain. For example, there are specific metrics for regression models, classification models, clustering, natural language processing, and more. Therefore, during the design of your testing approach, you have to consider what type of model you are building in order to define the <span class="No-Break">evaluation metrics.</span></p>
			<p>In the following sections, you will take a look at the most important metrics and concepts that you should know to evaluate <span class="No-Break">your models.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor975"/><a id="_idTextAnchor976"/>Evaluating classification models</h1>
			<p>Classification models are one of <a id="_idTextAnchor977"/>the most traditional classes of problems that you <a id="_idTextAnchor978"/>might face, either during the exam or during your journey as a data scientist. A very important artifact that you might want to generate during the classification model evaluation is known as a <span class="No-Break">confusion matrix.</span></p>
			<p>A confusion matrix compares your model predictions against the real values of each class under evaluation. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows what a <a id="_idTextAnchor979"/>confusion matrix looks like in a binary <span class="No-Break">classification problem:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B21197_07_01.jpg" alt="Figure 7.1 – A confusion matrix" width="1650" height="527"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor980"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – A confusion matrix</p>
			<p>There are the following c<a id="_idTextAnchor981"/>omponents in a <span class="No-Break">confusion matrix:</span></p>
			<ul>
				<li>TP: This is the number of true positive cases. Here, you have to count the number of cases that have been predicted as true and are, indeed, true. For example, in a fraud detection system, this would be the number of fraudulent transactions that were correctly predicted <span class="No-Break">as fraud.</span></li>
				<li>TN: This is the number of true negative cases. Here, you have to count the number of cases that have been predicted as false and are, indeed, false. For example, in a fraud detection system, this would be the number of non-fraudulent transactions that were correctly predicted as <span class="No-Break">not fraud.</span></li>
				<li>FN: This is the number of false negative cases. Here, you have to count the number of cases that have been predicted as false but are, instead, true. For example, in a fraud detection system, this would be the number of fraudulent transactions that were wrongly predicted as <span class="No-Break">not fraud.</span></li>
				<li>FP: This is the number of false positive cases. Here, you have to count the number of cases that have been p<a id="_idTextAnchor982"/>redicted as true but are, instead, false. For example, in a fraud detection system, this would be the number of non-fraudulent transactions that were wrongly predicted <span class="No-Break">as fraud.</span></li>
			</ul>
			<p>In a perfect scenario, your confusion matrix will have only true positive and true negative cases, which means that your model has an accuracy of 100%. In practical terms, if that type of scenario o<a id="_idTextAnchor983"/>ccurs, you should be skeptical instead of happy, since it is expected that your model will contain some level of errors. If your model does not contain errors, you are likely to be suffering from overfitting issues, so <span class="No-Break">be careful.</span></p>
			<p>Once false negatives and false positives are expected, the best you can do is prioritize one of them. For example, you can reduce the number of false negatives by increasing the number of false positives and vice versa. This is known as the precision versus recall trade-off. Let’s take a look at these <span class="No-Break">metrics next.</span></p>
			<h2 id="_idParaDest-174">E<a id="_idTextAnchor984"/><a id="_idTextAnchor985"/>xtracting metrics from a confusion matrix</h2>
			<p>The simplest metric that can be e<a id="_idTextAnchor986"/>xtracted from a c<a id="_idTextAnchor987"/>onfusion matrix is known as <strong class="bold">accuracy</strong>. Accuracy is given by the following equation, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B21197_07_02.jpg" alt="Figure 7.2 – Formula for accuracy" width="1331" height="74"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Formula for accuracy</p>
			<p>For the sake of demonstration, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> shows a confusion matrix <span class="No-Break">with data.</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B21197_07_03.jpg" alt="Figure 7.3 – A confusion matrix filled with some examples" width="1498" height="611"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor988"/>igure 7.3 – A confusion matrix filled with some examples</p>
			<p>According to <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, the accuracy would be (100 + 90) / 210, which is equal to 0.90. There is a common issue that occurs when utilizing an accuracy metric, which is related to the balance of each class. Problems with highly imbalanced classes, such as 99% positive cases and 1% negative cases, will imp<a id="_idTextAnchor989"/>act the accuracy score and make <span class="No-Break">it useless.</span></p>
			<p>For example, if your tra<a id="_idTextAnchor990"/>ining data has 99% positive cases (the majority class), your model is likely to correctly classify most of the positive cases but work badly in the classification of negative cases (the minority class). The accuracy will be very high (due to the correctness of the classification of the positive cases), regardless of the bad results in the minority <span class="No-Break">class classification.</span></p>
			<p>The point is that on highly imbalanced problems, you usually have more interest in correctly classifying the minority class, not the majority class. That is the case in most fraud detection systems, for example, where the minority class corresponds to fraudulent cases. For imbalanced problems, you should look for other types of metrics, which you will learn <span class="No-Break">about next.</span></p>
			<p>Another important metric that you<a id="_idTextAnchor991"/> can extract from a con<a id="_idTextAnchor992"/>fusion matrix is known as recall, which is given by the following equation, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B21197_07_04.jpg" alt="Figure 7.4 – Formula for recall" width="1433" height="83"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Formula for recall</p>
			<p>In other words, recall is the number of true positives over the total number of posi<a id="_idTextAnchor993"/>tive cases. Recall is also known <span class="No-Break">as sensitivity</span><span class="No-Break">.</span></p>
			<p>With the values in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, recall is given by 100 / 112, which is equa<a id="_idTextAnchor994"/>l to 0.89. Precision, on the other hand, is given by the following formula, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B21197_07_05.jpg" alt="Figure 7.5 – Formula for precision" width="1610" height="82"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Formula for precision</p>
			<p>In other words, precision is the number of true positives over the total number of predicted positive cases. Precision is al<a id="_idTextAnchor995"/>so known as positive <span class="No-Break">predictive power.</span></p>
			<p>With the values in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, precision is given by 100 / 108, which is equal to 0.93. In general, you can increase preci<a id="_idTextAnchor996"/>sion at the cost of decreasing recall and vice versa. There is another model evaluation artifact in which you can play around with this precision versus recall trade-off. It is known<a id="_idTextAnchor997"/> as a <span class="No-Break">precision-recall curve.</span></p>
			<p>Precision-recall curves summarize the precision versus recall trade-off by using different probability thresholds. For example, the default threshold is 0.5, where any prediction above 0.5 will be considered true; otherwise, it is false. You can change the default threshold according to your need so that you can prioritize recall or precision. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> shows an example of a <span class="No-Break">precision-recall curve:</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B21197_07_06.jpg" alt="Figure 7.6 – A precision-recall curve" width="966" height="478"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – A precision-recall curve</p>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em>, increasing the precision will reduce the amount of recall and vice versa. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> shows the precision/recall for each threshold for a <strong class="bold">gradient boosting model</strong> (as shown by the orange line) compared to a <strong class="bold">no-skill model</strong> (as shown by the blue dashed line). A perfect model will approx<a id="_idTextAnchor998"/>imate the curve to the point (1,1), forming a squared corner on the top right-hand side of <span class="No-Break">the chart.</span></p>
			<p>Another visual analysis you can use on top of confus<a id="_idTextAnchor999"/>ion matrixes is known <a id="_idTextAnchor1000"/>as a <strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>) curve. ROC curves summarize the tr<a id="_idTextAnchor1001"/>ade-off between the <strong class="bold">true positive rate</strong> and the <strong class="bold">false positive rate</strong> according to differ<a id="_idTextAnchor1002"/>ent thresholds, as in the <span class="No-Break">precision-recall curve.</span></p>
			<p>You already know about the true positive rate, or sensitivity, which is the same as what you have just learned about with the precision-recall curve. The other dimension of an ROC curve is the <strong class="bold">false positive rate</strong>, which is the number of false positives over the number of false positives plus <span class="No-Break">true negatives.</span></p>
			<p>In literature, you might find the false positive rate referr<a id="_idTextAnchor1003"/>ed to as <strong class="bold">inverted specificity</strong>, represented by <em class="italic">1 – specificity</em>. Specificity is given as the number of true negatives over the number of true negatives plus false positives. Furthermore, false-positive rates or inverted specificity are the same. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.7</em> shows what an ROC curve <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B21197_07_07.jpg" alt="Figure 7.7 – ROC curve" width="1003" height="501"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – ROC curve</p>
			<p>A perfect model will approximate the curve t<a id="_idTextAnchor1004"/>o the point (0,1), forming a squared corner on the top left-hand side of the chart. The orange line represents the trade-o<a id="_idTextAnchor1005"/>ff between the true positive rate and the false positive rate of a gradient-boosting classifier. The dashed blue line represents a no-skill model, which cannot predict the <span class="No-Break">classes properly.</span></p>
			<p>To summarize, you can use ROC curves for fairly <a id="_idTextAnchor1006"/>balanced datasets and precision-recall curves for moderate to <span class="No-Break">imbalanced datasets.</span></p>
			<h2 id="_idParaDest-175">Summari<a id="_idTextAnchor1007"/><a id="_idTextAnchor1008"/>zing precision and recall</h2>
			<p>Sometimes, you might want to use a m<a id="_idTextAnchor1009"/>etric that summarizes precision and recall,<a id="_idTextAnchor1010"/> instead of prioritizing one over the other. Two very popular metrics can be used to summarize precision and recall: <strong class="bold">F1 score</strong> and <strong class="bold">Area Under </strong><span class="No-Break"><strong class="bold">Curve</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AUC</strong></span><span class="No-Break">).</span></p>
			<p>The F1 score, also known a<a id="_idTextAnchor1011"/>s the <strong class="bold">F-measure</strong>, computes the harmoni<a id="_idTextAnchor1012"/>c mean of precision and recall. AUC summarizes the approxi<a id="_idTextAnchor1013"/>mation of the area under the <span class="No-Break">precision-recall curve.</span></p>
			<p>That brings us to the end of this section on classification metrics. Let’s now take a look at the evaluation metrics for <span class="No-Break">regression models.</span></p>
			<h1 id="_idParaDest-176">Evaluat<a id="_idTextAnchor1014"/><a id="_idTextAnchor1015"/>ing regression models</h1>
			<p>Regression models are quite differe<a id="_idTextAnchor1016"/>nt from classification models since the outcome of the model is a continuous number. Therefore, the metrics around regression models aim to monitor the difference between real and <span class="No-Break">predicted values.</span></p>
			<p>The simplest way to check the difference between a predicted value (<em class="italic">yhat</em>) and its actual value (<em class="italic">y</em>) is by performing a simple subtraction operation, where the error will be equal to the absolute value of <em class="italic">yhat – y</em>. This metric is known a<a id="_idTextAnchor1017"/>s the <strong class="bold">Mean Absolute </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MAE</strong></span><span class="No-Break">).</span></p>
			<p>Since you usually have to evaluate the error of each prediction, <em class="italic">i</em>, you have to take the mean value of the errors. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.8  </em>depicts formula that shows how this error can be <span class="No-Break">formally defined:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B21197_07_08.jpg" alt="Figure 7.8 – Formula for error of each prediction" width="887" height="82"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure<a id="_idTextAnchor1018"/> 7.8 – Formula for error of each prediction</p>
			<p>Sometimes, you might want to penalize bigger errors over smaller errors. To achieve this, you can use another metric, known as<a id="_idTextAnchor1019"/> the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>). The MSE will square each error and return the <span class="No-Break">mean value.</span></p>
			<p>By squaring errors, the MSE will penalize bigger ones. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.9</em> depicts formula that shows how the MSE can be <span class="No-Break">formally defined:</span></p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B21197_07_09.jpg" alt="Figure 7.9 – Formula for MSE" width="1068" height="222"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure <a id="_idTextAnchor1020"/>7.9 – Formula for MSE</p>
			<p>There is a potential interpret<a id="_idTextAnchor1021"/>ation problem with the MSE. Since it has to compute the squared error, it might be difficult to interpret the final results from a business perspective. The <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) works around this interpret<a id="_idTextAnchor1022"/>ation issue, by taking the square root of the MSE. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.10</em> depicts the <span class="No-Break">RMSE equation:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B21197_07_10.jpg" alt="Figure 7.10 – Formula for RMSE" width="1044" height="116"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7<a id="_idTextAnchor1023"/>.10 – Formula for RMSE</p>
			<p>The RMSE is one of the most used metrics for regression models, since it can penalize larger errors and remains easy <span class="No-Break">to interpret.</span></p>
			<h2 id="_idParaDest-177">Exploring<a id="_idTextAnchor1024"/><a id="_idTextAnchor1025"/> other regression metrics</h2>
			<p>There are many more metrics that are s<a id="_idTextAnchor1026"/>uitable for regression problems, in addition to the ones that you have just learned. You will not learn about most of them here, but you will be introduced to a few more metrics that might be important for you <span class="No-Break">to know.</span></p>
			<p>One of these metrics is known as t<a id="_idTextAnchor1027"/>he <strong class="bold">Mean Absolute Percentage Error</strong> (<strong class="bold">MAPE</strong>). As the name suggests, the MAPE will compute the absolute percentage error of each prediction and then take the average value. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em> depicts formula that shows how this metric <span class="No-Break">is computed:</span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B21197_07_11.jpg" alt="Figure 7.11 – Formula for MAPE" width="1402" height="345"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.<a id="_idTextAnchor1028"/>11 – Formula for MAPE</p>
			<p>The MAPE is broadly used in forecasting models since it is very simple<a id="_idTextAnchor1029"/> to interpret, and it provides a very good sense of how far (or close) the predictions are from the actual values (in terms of <span class="No-Break">a percentage).</span></p>
			<p>You have now completed this section on regression metrics. Next, you will learn about <span class="No-Break">model optimization.</span></p>
			<h1 id="_idParaDest-178">Model optim<a id="_idTextAnchor1030"/><a id="_idTextAnchor1031"/>ization</h1>
			<p>As you know, understanding evaluation metrics is very important in order to measure your model’s performance and document your work. In the same way, when you want to optimize your current models, evaluating metrics also plays a very important role in defining the baseline performance that you want <span class="No-Break">to challenge.</span></p>
			<p>The process of model optimization consists of<a id="_idTextAnchor1032"/> finding the best configuration (also known as hyperparameters) of the machine learning algorithm for a particular data distribution. You do not want to find hyperparameters that overfit the training data, in the same way that you do not want to find hyperparameters that underfit the <span class="No-Break">training data.</span></p>
			<p>You learned about overfitting and underfitting in <a href="B21197_01.xhtml#_idTextAnchor018"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><em class="italic">, Machine Learning Fundamentals</em>. In the same chapter, you also learned how to avoid these two types of <span class="No-Break">modeling issues.</span></p>
			<p>In this section, you will learn about some techniques that you can use to find the best configuration for a particular algorithm and dataset. You can combine these techniques of model optimization with other methods, such as cross-validation, to find the best set of hyperparameters for your model and avoid <span class="No-Break">fitting issues.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Always remember that you do not want to optimize your algorithm to the underlying training data but to the data distribution behind the training data, so that your model will work on the training data as well as the production data (the data that has never been exposed to your model during the training process). A machine learning model that works only on the training data is useless. That is why combining model-tuning techniques (such as the ones you will learn about next) with sampling techniques (such as cross-validation) makes all the difference when it comes to creating a <span class="No-Break">good model.</span></p>
			<h2 id="_idParaDest-179">Grid search<a id="_idTextAnchor1033"/><a id="_idTextAnchor1034"/></h2>
			<p><strong class="bold">Grid search</strong> is probably the most popular met<a id="_idTextAnchor1035"/>hod for model optim<a id="_idTextAnchor1036"/>ization. It consists of testing different combinations of the algorithm and selecting the best one. Here, there are two important points that you need to pay <span class="No-Break">attention to:</span></p>
			<ul>
				<li>How to define the best configuration of <span class="No-Break">the model</span></li>
				<li>How many configurations should <span class="No-Break">be tested</span></li>
			</ul>
			<p>The best model is defined based on an evaluation metric. In other words, you have to first define which metric you are going to use to evaluate the model’s performance. Secondly, you have to define how you are going to evaluate the model. Usually, cross-validation is used to evaluate the model on multiple datasets that have never been used <span class="No-Break">for training.</span></p>
			<p>In terms of the number of combinations/configurations, this is the most challenging part when playing with grid search. Each hyperparameter of an algorithm may have multiple or, sometimes, infinite possibilities of values. If you consider that an algorithm will usually have multiple hyperparameters, this becomes a function with quadratic cost, where the number of unique combinations to test is given as <em class="italic">the number of values of hyperparameter a * the number of values of hyperparameter b * the number of values of hyperparameter i</em>. <em class="italic">Table 7.1</em> shows how you could potentially set a grid search configuration for a decision <span class="No-Break">tree model:</span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor1037"/></strong><span class="No-Break"><strong class="bold">Criterion</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Max depth</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Min </strong><span class="No-Break"><strong class="bold">samples leaf</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Gini, Entropy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2, 5 =, <span class="No-Break">10</span></p>
						</td>
						<td class="No-Table-Style">
							<p>10, <span class="No-Break">20, 30</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.1 – Grid search configuration</p>
			<p>In <em class="italic">Table 7.1</em>, there are three hyperparameters: <strong class="bold">Criterion</strong>, <strong class="bold">Max depth</strong>, and <strong class="bold">Min samples leaf</strong>. Each of these hyperparameters has a list of values for testing. That means by the end of the grid search process, you will have tested 18 models (2 * 3 * 3), where only the best one will <span class="No-Break">be selected.</span></p>
			<p>As you might have noticed, all the <a id="_idTextAnchor1038"/>different combinations of those three hyperparameters will be tested. For example, consider <span class="No-Break">the following:</span></p>
			<ul>
				<li>Criterion = Gini, Max depth = 2, Min samples leaf = <span class="No-Break">10</span></li>
				<li>Criterion = Gini, Max depth = 5, Min samples leaf = <span class="No-Break">10</span></li>
				<li>Criterion = Gini, Max depth = 10, Min samples leaf = <span class="No-Break">10</span></li>
			</ul>
			<p>Some other questions that you might have could be <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Considering that a particular algorithm might have several hyperparameters, which ones should <span class="No-Break">I tune?</span></li>
				<li>Considering that a particular hyperparameter might accept infinite values, which values should <span class="No-Break">I test?</span></li>
			</ul>
			<p>These are good questions, and grid search <a id="_idTextAnchor1039"/>will not give you a straight answer for them. Instead, this is closer to an empirical process, where you have to test as much as you need to achieve your <span class="No-Break">target performance.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Of course, grid search cannot guarantee that you will come up with your target performance. That depends on the algorithm and the <span class="No-Break">training data.</span></p>
			<p>A common practice, though, is to define the values for testing by <a id="_idTextAnchor1040"/>using a <strong class="bold">linear space</strong> or <strong class="bold">log space</strong>, where you can <a id="_idTextAnchor1041"/>manually set the limits of the hyperparameter you want to test and the number of values for testing. Then, the intermediate values will be drawn by a linear or <span class="No-Break">log function.</span></p>
			<p>As you might imagine, grid search can <a id="_idTextAnchor1042"/>take a long time to run. A number of alternative methods have been proposed to work around this time issue. <strong class="bold">Random search</strong> is one of them, where <a id="_idTextAnchor1043"/>the list of values for testing is randomly selected from the <span class="No-Break">search space.</span></p>
			<p>Another method that has gained rapid adoption across <a id="_idTextAnchor1044"/>the industry is known as <strong class="bold">Bayesian optimization</strong>. Algorithm optimizations, such as <strong class="bold">gradient descent</strong>, try to find what is <a id="_idTextAnchor1045"/>called the <strong class="bold">global minima</strong>, by calculating <a id="_idTextAnchor1046"/>derivatives of the cost function. The global minima are the points where you find the algorithm configuration with the least <span class="No-Break">associated cost.</span></p>
			<p>Bayesian optimization is useful when <a id="_idTextAnchor1047"/>calculating derivatives is not an option. So you <a id="_idTextAnchor1048"/>can use the <strong class="bold">Bayes theorem</strong>, a probabilistic approach, to find the global minima using the smallest number <span class="No-Break">of steps.</span></p>
			<p>In practical terms, Bayesian optimization will start testing the entire search space to find the most promising set of optimal hyperparameters. Then, it will perform more tests specifically in the place where the global minima are likely <span class="No-Break">to be.</span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor1049"/><a id="_idTextAnchor1050"/>Summary</h1>
			<p>In this chapter, you learned about the main metrics for model evaluation. You started with the metrics for classification problems and then you moved on to the metrics for <span class="No-Break">regression problems.</span></p>
			<p>In terms of classification metrics, you have been introduced to the well-known confusion matrix, which is probably the most important artifact for performing a model evaluation on <span class="No-Break">classification models.</span></p>
			<p>You learned about true positives, true negatives, false positives, and false negatives. Then, you learned how to combine these components to extract other metrics, such as accuracy, precision, recall, the F1 score, <span class="No-Break">and AUC.</span></p>
			<p>You then went even deeper and learned about ROC curves, as well as precision-recall curves. You learned that you can use ROC curves to evaluate fairly balanced datasets and precision-recall curves for moderate to <span class="No-Break">imbalanced datasets.</span></p>
			<p>By the way, when you are dealing with imbalanced datasets, remember that using accuracy might not be a <span class="No-Break">good idea.</span></p>
			<p>In terms of regression metrics, you learned that the most popular ones, and the ones most likely to be present in the <em class="italic">AWS Machine Learning Specialty</em> exam, are the MAE, MSE, RMSE, and MAPE. Make sure you know the basics of each of them before taking <span class="No-Break">the exam.</span></p>
			<p>Finally, you learned about methods for hyperparameter optimization, such as grid search and Bayesian optimization. In the next chapter, you will have a look at AWS application services for AI/ML. But first, take a moment to practice these questions about model evaluation and <span class="No-Break">model optimization.</span></p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor1051"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH07"><span class="No-Break">https://packt.link/MLSC01E2_CH07</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B21197_07_12.jpg" alt="Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B21197_07_13.jpg" alt="Figure 7.13 – Chapter Review Questions for Chapter 7" width="1404" height="945"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Chapter Review Questions for Chapter 7</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor1052"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor1053"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor1054"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor1055"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor1056"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table002-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.2 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>