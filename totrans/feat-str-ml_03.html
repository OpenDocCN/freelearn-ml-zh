<html><head></head><body>
		<div id="_idContainer027">
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Chapter 2: What Problems Do Feature Stores Solve?</h1>
			<p>In the last chapter, we discussed the different stages in the <strong class="bold">machine learning </strong>(<strong class="bold">ML</strong>) life cycle, the difficult and time-consuming stages of ML, and how far we are from an ideal world. In this chapter, we'll explore one area of ML, which is ML feature management. ML feature management is the process of creating features, storing them in persistent storage, and serving them at scale for model training and inference. It is one of the most important stages of ML, although it is often overlooked. In data science/engineering teams in the early stages of ML, the absence of feature management has been a major hindrance to getting their ML models to production. </p>
			<p>As a data scientist/ML engineer, you may have found innovative ways to store and retrieve features for your ML model. But mostly, the solutions we build are not reusable, and every solution has limitations. For example, some of us might be using S3 buckets to store features, whereas other data scientists in the team might be using transactional databases. One may be more comfortable using CSV files and the other might prefer using Avro or Parquet files. Due to personal preference and a lack of standardization, each model will probably have a different way of managing features. Good feature management, on the other hand, should do the following:</p>
			<ul>
				<li>Make features discoverable</li>
				<li>Lead to easy reproducibility of models</li>
				<li>Accelerate model development and productionization</li>
				<li>Fuel reuse of features within and across teams</li>
				<li>Make feature monitoring easy</li>
			</ul>
			<p>The aim of this chapter is to explain how data scientists and engineers strive to achieve better feature management and yet fall short of expectations. We will review different approaches adopted by teams to bring features into production, common problems with these approaches, and how we can do better with a feature store. By the end of this chapter, you will understand how a feature store meets the objectives mentioned previously and provides standardization across teams. </p>
			<p>In this chapter, we will cover the following topics: </p>
			<ul>
				<li>Importance of features in production</li>
				<li>Ways to bring features to production</li>
				<li>Common problems with the approaches used for bringing features to production</li>
				<li>Feature store to the rescue</li>
				<li>Philosophy behind feature stores</li>
			</ul>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Importance of features in production</h1>
			<p>Before <a id="_idIndexMarker048"/>discussing how to bring features to production, let's understand why <a id="_idIndexMarker049"/>features are needed in production. Let's go through an example.</p>
			<p>We often use taxi and food delivery services. One of the good things about these services is that they tell us how long it will take for our taxi or food to arrive. Also, most of the time, it is approximately correct. How does it predict this accurately? It uses ML, of course. The ML model predicts how long it will take for the taxi or food to arrive. For a model like that to be successful, not only does it need a good feature engineering and ML algorithm, but also the most recent features. Though we don't know the exact feature set that the model uses, let's look at a couple of features that change dynamically and are very important.</p>
			<p>With food delivery services, the major components that affect the delivery time are restaurants, drivers, traffic, and customers. The model probably uses a set of slow-changing features that are updated regularly, maybe daily or weekly, and a set of dynamic features that change every few minutes. The slow-changing features might include the average number of orders a restaurant receives at different times of the day from the app and in person, the average time it takes for an order to be ready, and so on. It might seem like these features are not slow-changing, but if you think about it, the average number of orders might differ based on restaurant location, seasonality, time of the day, day of the week, and more. Dynamic features include how long the last five orders took, the number of cancelations in the past 30 minutes, and the current number of orders for the restaurant. Similarly, driver features might include average order delivery time with respect to distance, how often the driver cancels orders, and whether the driver is picking up multiple orders. Apart from these features, there will be traffic features, which change much more dynamically.</p>
			<p>With many<a id="_idIndexMarker050"/> dynamic features in play, even if one of them is an <a id="_idIndexMarker051"/>hour old, the model's predictions will go off the charts. For example, if there is a crash on the delivery route and traffic features don't capture it and use it for inference, the model will predict that food will arrive more quickly than it actually will. Similarly, if the model cannot get the current number of orders at the restaurant, it will use the old value and predict a value that may be far from the truth. Hence the more up-to-date features a model gets, the better the predictions will be. Also, another thing to keep in mind is that the app will not give the features; the app can only give information such as the restaurant ID and the customer ID. The model will have to fetch the features and facts from a different location, ideally a feature store. Wherever the features are being fetched from, the infrastructure serving it must scale in and scale out based on traffic to efficiently use resources and also to accommodate requests at low latency with a very low percentage of errors, if any.</p>
			<p>Just like the food delivery service, the model we built in the first chapter needs the features during inference and the more up to date the features are, the better the customer's <strong class="bold">lifetime value </strong>(<strong class="bold">LTV</strong>) prediction<a id="_idIndexMarker052"/> will be. Good predictions will lead to better actions, resulting in excellent customer experience, hence better customer affinity, and better business.</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Ways to bring features to production</h1>
			<p>Now that we understand the need for features in production, let's look at some traditional ways of bringing features to production. Let's consider two types of pipelines: batch model pipelines and online/transactional model pipelines:</p>
			<ul>
				<li><strong class="bold">Batch models</strong>: These <a id="_idIndexMarker053"/>are models that are run on a schedule, such as hourly, daily, weekly, and so on. Two of the common batch models are forecasting and customer segmentation. Batch inference is easier and less complex than its counterpart since it doesn't have any latency requirements; inference can run for minutes or hours. Batch models can use distributed computational frameworks such as Spark. Also, they can be run with simple infrastructure. Most ML models start as batch models and, over time, depending on the available infrastructure and requirements, they go on to become online/transactional models.</li>
			</ul>
			<p>Though batch models' infrastructure is<a id="_idIndexMarker054"/> simple to build and manage, these models have drawbacks, such as the predictions not always being up to date. Since there is a time lag on predictions, it might cost business. For example, let's say a manufacturing plant uses order forecasting models to acquire raw materials. Depending on the time lag of the batch forecast models, the business might need to bear the cost of the shortage in raw materials versus overstocking raw materials in the warehouse.</p>
			<ul>
				<li><strong class="bold">Online/transactional models</strong>: Online models follow the pull paradigm; the prediction will be generated on demand. Online models<a id="_idIndexMarker055"/> take advantage of the current reality and use that for predictions. Online models are transactional, need low-latency serving, and should scale based on incoming traffic. A typical online model is a recommendation model, which could be product recommendation, design recommendation, and so on.</li>
			</ul>
			<p>Though real-time <a id="_idIndexMarker056"/>prediction sounds fancy, online models face a different set of challenges. It is easier to build applications whose latency is 8 hours than it is to build an application with a latency of 100 milliseconds. The latency of online models is usually in milliseconds. That means the model has a few milliseconds to figure out what the most up-to-date value is (which means generating or getting the latest features for the model) and predict the outcomes. For this to happen, the model needs a supporting infrastructure to serve the data required for prediction. Online models are usually hosted as REST API endpoints, which again need scaling, monitoring, and more.</p>
			<p>Now that we understand the difference between batch and online models, let's look at how batch model pipelines work.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Batch model pipeline</h2>
			<p>As discussed, batch model pipeline<a id="_idIndexMarker057"/> latency requirements can range from minutes to hours. Batch models usually run on a schedule, hence they will be orchestrated using tools such as Airflow or AWS Step Functions. Let's look at a typical batch model pipeline and how features are brought to production.</p>
			<p><em class="italic">Figure 2.1</em> depicts typical <a id="_idIndexMarker058"/>batch model pipelines:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18024_02_01.jpg" alt="Figure 2.1 – Batch model pipelines&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Batch model pipelines</p>
			<p>As discussed in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, An Overview of the Machine Learning Life Cycle</em>, once the model development is complete and it's ready for productionization, the notebook will be refactored to remove unwanted code. Some data engineers also break down a single notebook into multiple logical steps, such as feature engineering, model training, and model prediction. The refactored notebooks or refactored Python scripts generated from the notebook are scheduled using an orchestration framework such as Airflow. In a typical pipeline, the first stage will read the raw data from different data sources, perform data cleaning, and carry out feature engineering, which will be used by subsequent stages in the pipeline. Once the model prediction stage is complete, the prediction output will be written to a persistent store, maybe a database or an S3 bucket. The results will be accessed from the persistent storage as and when needed. If a stage in the pipeline fails for any reason (such as a data accessibility issue or errors in the code), the pipeline will be set up to trigger an alarm and stop further execution.</p>
			<p>If you haven't already noticed, in the batch model pipeline, the features are generated when the pipeline runs. In some cases, it also retrains a new model with the latest data, and in others, it uses a previously trained model and predicts using the features generated from the data available at the time when the pipeline is run. As you can see in <em class="italic">Figure 2.1</em>, every new model that is built starts at the raw data source, repeats the same steps, and joins the <a id="_idIndexMarker059"/>production pipeline list. We will discuss the problems in this approach in the later section. Let's look at the different ways to bring features to production in online models next.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Online model pipeline</h2>
			<p>Online models have the <a id="_idIndexMarker060"/>special requirement of serving features in near real time, as these models are customer-facing or need to make business decisions in real time. There are different ways to bring features to production in an online model. Let's discuss them one by one in this section. One thing to keep in mind is that these approaches are not exactly how everybody does it; they are merely a representation of group approaches. Different teams use different versions of these approaches.</p>
			<h3>Packaging features along with models</h3>
			<p>To deploy<a id="_idIndexMarker061"/> an online model, it will have to be packaged <a id="_idIndexMarker062"/>first. Again, there are different standards that teams follow depending on the tools they use. Some might use packing libraries such as MLflow, joblib, or ONNX. Others might package the model directly as the REST API Docker image. As data scientists and data engineers have a different set of skills, as mentioned in <em class="italic">Figure 1.1</em> of <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, An Overview of the Machine Learning Life Cycle</em>, the ideal approach is to provide data scientists with tools to package models using libraries such as MLflow, joblib, and ONNX, and save the model to a model registry. The data engineers can then use the registered model to build REST APIs and deploy it. There is also out-of-the-box support to deploy MLflow-packaged models as AWS SageMaker endpoints with a simple <strong class="bold">command-line interface </strong>(<strong class="bold">CLI</strong>) command. It also supports building REST API Docker images with a CLI command, which then can be deployed in any container environment.</p>
			<p>While libraries such as MLflow and joblib provide a way to package Python objects, they also support adding additional dependencies if required. For example, MLflow provides a set of built-in flavors to support the packaging of models using ML libraries such as scikit-learn, PyTorch, Keras, and TensorFlow. It adds all the required dependencies for the ML library. Packaging models with built-in flavors is as simple as using the following code:</p>
			<pre class="source-code">mlflow.&lt;MLlib&gt;.save_model(model_object)</pre>
			<pre class="source-code">## example scikit-learn</pre>
			<pre class="source-code">mlflow.sklearn.save_model(model_object)</pre>
			<p>Along with the required dependencies, you can package the <strong class="source-inline">features.csv</strong> file and load it in the <strong class="source-inline">predict</strong> method of the model. Though this might sound like an easy deployment option, the result of this is not far away from a batch model. Since features are packaged along with the model, they are static. Any change in the raw dataset will not affect the model unless a new version of the model is built with a new set of features generated from the latest data and packaged along with the model. However, this might be a good<a id="_idIndexMarker063"/> first step from batch to online models. The <a id="_idIndexMarker064"/>reason I say this is that instead of running it as a batch model, you have now made it a pull-based inference. Also, you have defined a REST endpoint input and output format for the consumer of the model. The only pending step is to get the latest features to the model instead of static features, which are packaged. Once that is implemented, the consumers of the model won't have to make any changes and consumers will be served with predictions using the latest available data. </p>
			<h3>Push-based inference</h3>
			<p>Unlike pull-based inference, where the <a id="_idIndexMarker065"/>model is scored when needed, in the push-based inference pattern, predictions are run proactively and kept ready in a transactional database or key-value store so that they can be served at low latency when the request comes. Let's look at a typical architecture of online models using push-based inference:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18024_02_02.jpg" alt="Figure 2.2 – Push-based inference&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Push-based inference</p>
			<p><em class="italic">Figure 2.2</em> shows the <a id="_idIndexMarker066"/>architecture of push-based inference. The idea here is similar to batch model inference, but the difference is that the pipeline also considers the real-time dataset, which is changing dynamically. The operation of a push-based model works as follows:</p>
			<ul>
				<li>The real-time data (in this example, user interactions with the website) will be captured and pushed to a queue, such as Kafka, Kinesis, or Event Hubs.</li>
				<li>The feature engineering pipelines subscribe to a specific set of topics or a specific set of queues depending on what data is needed to generate features of the model. This also depends on the tools and the architecture. There may be a single queue or multiple queues depending on how huge/diverse the application is.</li>
				<li>Whenever an event of interest appears in the queue, the feature engineering pipeline will pick this event and regenerate the features of the model using other datasets.<p class="callout-heading">Note</p><p class="callout">Not all features are dynamic. Some features may not change very much or very often. For example, a customer's geographic location might not change often.</p></li>
				<li>The newly<a id="_idIndexMarker067"/> generated features are used to run the prediction of the data point. </li>
				<li>The results are stored in a transactional database or a key-value store.</li>
				<li>Whenever required, the website or application will query the database to get new predictions for the specific ID (such as <strong class="source-inline">CustomerId</strong> when serving recommendations for a customer on a website).</li>
				<li>This process repeats every time a new event of interest appears on the queue.</li>
				<li>A new pipeline will be added for every new ML model.</li>
			</ul>
			<p>This approach might seem easy and straightforward, as the only additional requirement here is real-time streaming data. However, this has limitations; the whole pipeline will have to run within milliseconds so that the recommendations are available before the application makes the next query for prediction. This is achievable but might involve a higher cost of operationalization because this is not just one pipeline: every pipeline for real-time models will have to have a similar latency requirement. Also, this will not be a copy-and-paste infrastructure because each model will have a different set of requirements when it comes to incoming traffic. For example, a model working on order features might require fewer processing instances, whereas a model working with clickstream data will <a id="_idIndexMarker068"/>require more data processing instances. Another thing to keep in mind is that although it might look like they are writing data to the same database, most of the time, it involves different databases and different technologies.</p>
			<p>Let's look at a better solution next.</p>
			<h3>Pull-based inference</h3>
			<p>In <a id="_idIndexMarker069"/>contrast to push-based inference, in pull-based inference, predictions are run at the time of the request. Instead of storing the predictions, feature sets of a specific model are stored in a transactional database or key-value store. During prediction, the feature set can be accessed with low latency. Let's look at the typical architecture of a pull-based inference model and the components involved:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18024_02_03.jpg" alt="Figure 2.3 – Using transactional/key-value store for features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Using transactional/key-value store for features</p>
			<p><em class="italic">Figure 2.3</em> shows another way of bringing features to production: the pull-based mechanism. Half of the pipeline works in a similar way to the push-based inference that we just discussed. The difference here is that after feature engineering, the features are written to a transactional database or key-value store. These features will be kept up to date by the pipelines. Once the<a id="_idIndexMarker070"/> features are available, the model works as follows:</p>
			<ol>
				<li>The model's <strong class="source-inline">predict</strong> API would have a contract similar to the one mentioned here:<p class="source-code">def predict(entity_id: str) -&gt; dict</p></li>
				<li>When the application needs to query the model, it will hit the REST endpoint with <strong class="source-inline">entity_id</strong>.</li>
				<li>The model will use <strong class="source-inline">entity_id</strong> to query the key-value store to get the features required to score the model.</li>
				<li>The features are used to score the model and return the predictions.</li>
			</ol>
			<p>This approach is <a id="_idIndexMarker071"/>ideal if you don't have the feature store infrastructure. Also, we will discuss this extensively in the next chapter. Again, there are a few concerns involved with this approach, which are the repetition of the work, deploying and scaling the feature engineering pipeline, and managing multiple key-value store infrastructures, among others. </p>
			<h3>Calculating features on demand</h3>
			<p>Let's discuss one <a id="_idIndexMarker072"/>last approach before we move on and discuss the problems with these approaches. In the approaches discussed so far, the data pipelines calculate the features proactively as and when the data arrives, or when the pipeline runs. However, it is possible to calculate the features on demand when there is a request for the inference. This means when the application queries the model for a prediction, the model will request another system for the features. The system uses raw data from different sources and calculates the features on demand. This may be the most difficult architecture to implement, but I heard on the <em class="italic">TWIML AI podcast with Sam Charrington</em>,<em class="italic"> episode 326: Metaflow, A Human-Centric Framework for Data Science with Ville Tuulos</em>, that Netflix has a system that can generate features on demand with a latency of seconds.</p>
			<p>The <strong class="source-inline">predict</strong> API might look similar to the one in the last approach: </p>
			<p>def predict(entity_id: str) -&gt; dict </p>
			<p>It then invokes the system to get features for the given entity, runs predictions using the features, and returns the results. As you can imagine, all of this will have to happen within a few seconds. The on-demand feature engineering to be executed in real time might require a huge infrastructure with multiple caches between different storage systems. Keeping these systems in sync is not an easy architecture design. It's just a dream infrastructure for most of us. I haven't seen one so far. Hopefully, we will get there soon.</p>
			<p>In this section, we discussed multiple ways to bring features into production for inference. There may be many other ways of achieving this, but most solutions are variations that revolve <a id="_idIndexMarker073"/>around one of these approaches. Now that we understand why and how we bring features to production, let's look at the common issues that these approaches have and what can be done to overcome them.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Common problems with the approaches used for bringing features to production</h1>
			<p>The<a id="_idIndexMarker074"/> approaches discussed in the previous section seem like good solutions. However, not only does every approach have its own technical difficulties, such as infrastructure sizing, keeping up with a service-level agreement (SLA), and interaction with different systems, but they have a few common problems as well. This is expected in a growing technical domain until it reaches a level of saturation. I want to dedicate this section to the common problems that exist in these approaches.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Re-inventing the wheel</h2>
			<p>One of the<a id="_idIndexMarker075"/> common problems in engineering is building something that already exists. The reasons for that could be many; for example, a person developing a solution may not know that it already exists, or the existing solution is inefficient, or there is a need for additional functionality. We have the same problem here. </p>
			<p>In many organizations, data scientists work in a specific domain and with a team supporting them, which usually includes ML engineers, data engineers, and data analysts. Their goal is to get their model to production. Though the other teams working in parallel also have the goal of getting their model to production, they hardly ever collaborate with each other due to their schedules and delivery timelines. As discussed in the first chapter, every persona in the team has different skill sets, different levels of experience with the available tools, and different preferences. Also, data engineers on two different teams rarely have the same preference. This leads to each team finding a solution to productionizing their model, which involves building feature engineering pipelines, feature management, model management, and monitoring. </p>
			<p>After coming up with a successful solution, even if the team (let's call it team-A) shares their knowledge and success with other teams, the response you would get would be <em class="italic">good to know</em>, <em class="italic">interesting</em>, <em class="italic">could be useful for us</em>. But it will never materialize into the other teams' solutions. The reason for that is not that other teams are indifferent to what team-A achieved. Apart from the knowledge, nothing that was built by team-A in many cases is reusable. The options that the other teams are left with are to copy the code and adapt it to their needs and hope it works or implement a similar-looking pipeline. Hence, most teams end up building their own solution for the model. The interesting thing <a id="_idIndexMarker076"/>is that even team-A will re-build the same pipeline for the next model they work on in most cases.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Feature re-calculation</h2>
			<p>Let's start with a<a id="_idIndexMarker077"/> question. Ask yourself this: <em class="italic">how much memory does your phone have?</em> Most probably you know the answer by heart. If you are not sure, you might check the memory in the settings and answer. Either way, if I or somebody else asks you the same question in an hour, I'm pretty sure you will not go back into the phone's settings and check again before answering, unless you have changed your phone. So why are we doing this in all our ML pipelines for features?</p>
			<p>When you go back and look at the approaches discussed previously, all of them have this common problem. Let's say team-A just completed a customer LTV model successfully and took it to production. Now team-A has been assigned another project, which is to predict the next purchase day of the customer. There is a high chance that the features that were effective in the customer LTV model can be effective here as well. Though these features are being calculated periodically to support the production model, team-A will start again with the raw data, calculate these features from scratch, and use them for the model development. Not only that, but they replicate the whole pipeline, though there are overlaps.</p>
			<p>As a result of this re-calculation, depending on the setup and the tools that team-A uses, they will be wasting compute, storage, and man-hours, whereas with better feature management, team-A could have gotten a head start on the new project, which is also a cost-effective solution.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Feature discoverability and sharing</h2>
			<p>As discussed, one<a id="_idIndexMarker078"/> of the problems is<a id="_idIndexMarker079"/> recalculation within the same team. The other part of this problem is even bigger. That is re-calculation across the teams and domains. Just like in the <em class="italic">Re-inventing the wheel</em> section, where teams were trying to figure out how to bring ML features to production, data scientists are re-discovering the data and features themselves here.</p>
			<p>One of the major drivers of this is a lack of trust and discoverability. Let's talk about discoverability first. Whenever data scientists work on a model and do a great job of data mining, exploration, and feature engineering, there are very limited ways of sharing it, as we discussed in the first chapter. The data scientist can use emails and presentations to do that. However, there is no way for anybody to discover what's available and selectively build what is not and use both in the model. Even if it's possible to discover other data scientists work, it is not possible to use it without figuring out data access and re-calculating features. </p>
			<p>The other driver for re-inventing the wheel in data discovery and feature engineering is trust. Though evidence is clear that there is a production model that uses the generated features, data scientists often find it difficult to trust programs developed by others that generate the features. Since raw data is trustworthy as it will have SLAs and schema validations, data scientists often end up re-discovering and generating the features.</p>
			<p>Hence the <a id="_idIndexMarker080"/>solution required here is<a id="_idIndexMarker081"/> an application that can make the features generated by others discoverable, sharable, and, most importantly, trustworthy, that is, a person/team who owns and manages the features they produce.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Training vs Serving skew </h2>
			<p>One other common problem in ML is training and serving skew. This happens when the feature engineering code used to generate the features for model training is different from the code used to generate the features for model prediction/serving. This could happen for many reasons; for instance, during model training, the data scientist may have used PySpark for generating the features, where as while productionizing the pipeline, the ML/Data engineer who took over, used a different technologies that is required by the production infrastructure. There are few problems with this. One is, there are two versions of feature engineering code, and the other problem is this could cause the training versus serving skew since the data generated by two versions of the pipeline may not be same for the same raw data input.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Model reproducibility</h2>
			<p>Model reproducibility <a id="_idIndexMarker082"/>is one of the common issues to tackle in ML. I have heard the story of how after a data scientist quit his job, the model he was working on was lost and his team couldn't reproduce the model many times. One of the main reasons for this is again the lack of feature management tools. You might ask what the problem is in reproducing the same model when you have the history of raw data. Let's take a look.</p>
			<p>Let's say there was a data scientist, Ram, working on an ML model to recommend products to customers. Ram spent a month working on it and came up with a brilliant model. With the help of data engineers on the team, the model was deployed to production. But Ram was not challenged enough in this job, so he quit and moved on to a different firm. Unfortunately, the production system went down, Ram didn't follow the MLOps standards of saving the model to a registry, so the model was lost and could not be recovered.</p>
			<p>Now, the responsibility for rebuilding the model is given to Dee, a new data scientist on the team, who is smart and uses the same dataset that was used by Ram, and performs the same data cleansing and feature engineering as if Dee is a reincarnation of Ram. Unfortunately, Dee's model cannot get the same results as Ram's. No matter how many times Dee tries, she cannot reproduce the model.</p>
			<p>One of the reasons for this is that the data has changed over time, which in turn affects the feature<a id="_idIndexMarker083"/> values and hence the model. There is no way to go back in time to produce the same features that were used the first time. As model reproducibility/repeatability is one of the crucial aspects of ML, we need to time travel. This means that data scientists should be able to go back in time and fetch the feature from a specific time in the past, just like in <em class="italic">Avengers: Endgame</em>, so that the models can be reproduced consistently.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Low latency</h2>
			<p>One other problem that<a id="_idIndexMarker084"/> all the approaches are trying to solve is low-latency feature serving. The ability to provide features at low latency decides whether a model can be hosted as an online model or a batch model. This has problems such as building and managing infrastructure and keeping features up to date. As it doesn't make sense to set up all models to be transactional, at the same time there is a high chance that a feature used in one batch model could be of great use in a different online model. So, the ability to switch the low-latency serving on and off would be a great benefit to data scientists.</p>
			<p>So far in this section, we have been through some of the common problems with the approaches discussed in the previous section. The question that still remains unanswered is what can be done to make this better? Is there a single tool or set of tools that exists today that can help us solve these common problems? As it turns out, the answer is <em class="italic">yes</em>, there is one tool that can solve all the problems we have talked about so far. It is called a <em class="italic">feature store</em>. In the next section, let's see what feature stores are, how they solve <a id="_idIndexMarker085"/>these problems, and the philosophy behind them. </p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Feature stores to the rescue</h1>
			<p>Let's begin this section with the definition of feature stores. A <strong class="bold">feature store</strong> is an operational data<a id="_idIndexMarker086"/> system for managing and serving ML features to models in production. It can serve feature data to models from a low-latency online store (for real-time prediction) or from an offline store (for scale-out batch scoring or model training). As the definition points out, it's a whole package that helps you create and manage ML features, and accelerate the operationalization of models. Before we dive deeper into feature stores, let's look at how the architecture of ML pipelines<a id="_idIndexMarker087"/> changes with the introduction of a feature store:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18024_02_04.jpg" alt="Figure 2.4 – ML pipelines with a feature store&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – ML pipelines with a feature store</p>
			<p><em class="italic">Figure 2.4</em> depicts the architecture of ML pipelines when you include a feature store. You may think that <em class="italic">Figure 2.4</em> looks the same as <em class="italic">Figure 2.3 </em>and I just replaced a bunch of small data stores with a bigger one and called it a feature store. Yes, it might seem that way, but there is more to it. Unlike a traditional data store, a feature store has a special set of capabilities; it is not a data store, but rather a data system (as its definition states), and it can do much more than just storage and retrieval.</p>
			<p>With the feature <a id="_idIndexMarker088"/>store being part of the ML pipeline, this is how the entire pipeline works:</p>
			<ol>
				<li value="1">Once the data scientist has a problem statement, the starting point will not be raw data anymore. It will be the feature store.</li>
				<li>The data scientist will connect to the feature store, browse through the repository, and use the features of interest.</li>
				<li>If this is the first model, the feature store might be empty. From here, the data scientist will go into the discovery phase, figure out the dataset, build a feature engineering pipeline, and ingest the features into the feature store. The feature store decouples feature engineering pipelines from the rest of the stages in ML.</li>
				<li>If the feature store is not empty but there are not enough features available in the feature store, the data scientist will discover the data of interest and another feature engineering pipeline will be added to sink a new set of features into the feature store. This approach makes the features available for the model that the data scientist is working on and for other data scientists who find these features useful in their model.</li>
				<li>Once the data scientist is happy with the feature set, the model will be trained, validated, and tested. If the model performance is not good, the data scientist will go back to discover new data and features.</li>
				<li>When the model is ready to be deployed, the <strong class="source-inline">predict</strong> method of the model will include code to fetch the required features for generating model predictions.</li>
				<li>The ready<a id="_idIndexMarker089"/> model will be deployed as a REST endpoint if it's an online model; otherwise, it will be used to perform batch predictions.</li>
			</ol>
			<p>Now that we understand how the pipeline works, let's go through the problems we discussed in the previous section and learn how the feature store solves them.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Standardizing ML with a feature store</h2>
			<p>Once the <a id="_idIndexMarker090"/>feature store is standardized at the team level, though<a id="_idIndexMarker091"/> there may be different ways of reading data and building feature engineering pipelines, but beyond feature engineering, the rest of the pipeline becomes a standard implementation. The ML engineers and data scientists don't have to come up with new ways to bring features to production. After feature engineering, the data scientists and ML engineers will ingest the features into the feature store. The feature store, by definition, can serve features at low latency. All ML engineers will have to do after that is update their <strong class="source-inline">predict</strong> method to fetch the required features from the feature store and return the predictions. This not only makes the life <a id="_idIndexMarker092"/>of the ML engineers easy, it sometimes also offloads managing <a id="_idIndexMarker093"/>feature management infrastructure.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Feature store avoids reprocessing data</h2>
			<p>As <a id="_idIndexMarker094"/>mentioned in the definition, a feature store has an offline store, and data from the offline store can be retrieved for model training or batch inference. Model training here doesn't mean training the same model. The features ingested into the feature store can be used for the training of another model.</p>
			<p>Let's take the same example we used while discussing the problem: team-A just completed the production deployment of the customer LTV model. The next model team-A starts working on is predicting the next purchase date. When the data scientists start working on this model, they don't have to go back to raw data and re-calculate the features that were used to build the customer LTV model. The data scientist can connect to the feature store, which is being updated with the latest features of the previous model, and get the features required to train the new model. However, the data scientist will have to build a data cleaning and feature engineering pipeline for the additional features that they find useful from the raw data. Again, the newly added features can be reused in the next model. This makes model development efficient and cost-effective.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Features are discoverable and sharable with the feature store </h2>
			<p>In the previous <a id="_idIndexMarker095"/>paragraph, we discussed the reuse of features within <a id="_idIndexMarker096"/>a team. Feature stores help data scientists achieve that. The other major issue was re-calculating and re-discovering useful data and features across teams due to the lack of feature discoverability. Guess what? Feature stores solve that too. Data scientists can connect to a feature store and browse through the existing feature tables and schemas. If they find any of the existing features useful, data scientists can use them in the model without re-discovering or re-calculating them.</p>
			<p>Another problem involved in sharing was trust. Although feature stores don't solve this completely, they address it to some extent. Since the feature tables are created and managed by a team, the data scientists can always reach out to the owners to get access and also discuss other aspects, such as SLAs and monitoring. If you haven't noticed yet, feature stores facilitate collaboration between teams. This can be beneficial for both teams, with data scientists and ML engineers from different teams working together and sharing each other's expertise.</p>
			<p>No more training versus serving skew</p>
			<p>With Feature Store, training versus serving skew will never occur. Once the feature engineering is complete, the features will be ingested into the feature store and feature store is the source for model training. Hence, the data scientists will use the features in feature store to train the ML model. Once the model is trained and moved to production, the production model will fetch the online store or historical store again for model prediction. As these features are used by both - training and prediction, serving is generated by the same pipeline/code and we will never have this issue with feature store.</p>
			<p>Model reproducibility with feature stores</p>
			<p>The other <a id="_idIndexMarker097"/>major issue with the previously discussed<a id="_idIndexMarker098"/> architecture was model reproducibility. This is an issue: the data is changing frequently, which in turn leads to features changing and hence the model changing, though the same set of features is being used to build the model. The only way to solve this problem was to go back in time and fetch the same state data that produced the old model. That may be a very complex problem to solve since it will involve multiple data stores. However, it is possible to store generated features in such a way that it allows data scientists to time travel.</p>
			<p>Yes, that is exactly what a feature store does. A feature store has an offline store, which stores historical data and allows users to go back in time and get the value of a feature at a specific point in time. With a feature store, a data scientist can get features from a specific time in history, so reproducing the model consistently is possible. Model reproducibility is no longer an issue with feature stores.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Serving features at low latency with feature stores</h2>
			<p>Though all the <a id="_idIndexMarker099"/>solutions were able to achieve the low-latency serving in some way, the solutions were not uniform. ML engineers had to come up with a solution to solve this issue and also build and manage the infrastructure. However, having a feature store in the ML pipeline makes this simple and also offloads the infrastructure management to the other team in cases where the platform team manages the feature store. Even without that, having the ability to run a few commands and have the low-latency serving up and running is a handy tool for ML engineers.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Philosophy behind feature stores</h1>
			<p>In this chapter, we have<a id="_idIndexMarker100"/> discussed different issues with ML pipelines and how feature stores help data scientists solve them and accelerate ML development. In this section, let's try to understand the philosophy behind feature stores and try to make sense of why having a feature store in our ML pipeline may be the ideal way to accelerate ML. Let's start with a real-world example as we are trying to build real-world experience with ML. You will be given the names of two phones; your job is to figure out which one is better. The names are iPhone 13 Pro and Google Pixel 6 Pro. You have an infinite amount of time to find the answer; continue reading once you have the answer.</p>
			<p>As Ralph Waldo Emerson said, <em class="italic">It's not the destination, it is the journey</em>. Whatever your answer may be, however long you took to arrive at it, let's look at how you might have arrived at it. Some of you might have got an answer right away, but if you haven't used either of these phones, you probably would have googled <strong class="source-inline">iPhone 13 Pro vs Google Pixel 6 Pro</strong>. You would have browsed through a few links, which would give you a comparison of the phones:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18024_02_05.jpg" alt="Figure 2.5 – iPhone 13 Pro versus Google Pixel 6 Pro &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – iPhone 13 Pro versus Google Pixel 6 Pro </p>
			<p>This was a great way of comparing two phones. Some of you might have done a lot more to arrive at the answer, but I'm sure none of us went and bought both phones, read through the specs provided by Apple and Google, used each of them for a month, and became experts with each of them before answering the question.</p>
			<p>In this task, we were smart enough to use the expertise and work done by others. Although there are a lot of comparisons on the internet, we chose the one that works for us. Not only in this task, but in most tasks, from buying a phone to buying a home, we try to use expert opinion to make a decision. If you look at it in a certain way, these are features in our decision-making. Along with the expert's opinion, we also include our own constraints and features, such as budget, memory if it's a phone, the number of seats if it's a car, and the number of rooms if it's a house. We use a combination of these to decide and take action. In most cases, this approach works, and in some cases, we might do a lot more research and become experts too.</p>
			<p>The use of feature <a id="_idIndexMarker101"/>stores in ML is an attempt to achieve something similar; it is like Google for data scientists. Instead of a generic search like Google, data scientists are looking for something specific, and are also sharing their expertise with other data scientists. If what is available in the feature store doesn't work for a data scientist, they will go to raw data, explore, understand, become experts in it, and come up with distinguishing features about a particular entity, which could be products, customers, and so on. This workflow of ML with feature stores will not only help data scientists use each other's expertise but also standardize and accelerate ML development.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Summary</h1>
			<p>In this chapter, we discussed common problems in ML feature management, different architectures of productionizing ML models, and ways to bring features to production. We also explored the issues involved with these approaches and how feature stores solve these issues by standardizing practices and providing additional features that a traditional data store does not.</p>
			<p>Now that we understand what feature stores have to offer, in the next chapter, we'll get our hands dirty with feature stores and explore the terminology, features, typical architecture of a feature store, and much more.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Further reading</h1>
			<ul>
				<li><em class="italic">Feast documentation</em>: <a href="https://docs.feast.dev/">https://docs.feast.dev/</a></li>
			</ul>
		</div>
	</body></html>