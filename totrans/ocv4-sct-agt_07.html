<html><head></head><body>
        

                            
                    <h1 class="header-title">Equipping Your Car with a Rearview Camera and Hazard Detection</h1>
                
            
            
                
<p>"Comes the morning and the headlights fade away."<br/>
                                                  – The Living Daylights (1987)</p>
<p>James Bond is a car thief. The movies show that he has stolen many automobiles, often from innocent bystanders. We do not know whether these unfortunate people ever recovered their property but, even if they did, the damages from collisions, submersions, bullets, and rockets would have had a lasting impact on their insurance premiums. Bond has also stolen a propeller plane, a tank, and a moon buggy.</p>
<p>The man has been driving since the 1950s, and perhaps it is time that he stopped.</p>
<p>Be that as it may, we can break away from the old Cold War days of indifference to collateral damage. With modern technology, we can provide a driver with timely information about others who are sharing the road. This information may make it easier to avoid collisions and to properly aim the vehicle's rocket launchers so that a chase scene can be conducted in an orderly manner, without flattening whole city blocks. Secret agents will not lose so many cars and, thus, will not feel compelled to steal so many.</p>
<p>Since driver assistance is a broad topic, let's focus on one scenario. Twilight and nighttime are difficult times for drivers, including secret agents. We might be blinded by the lack of natural light or the glare of headlights. However, we can make a computer vision system that sees headlights (or rear lights) clearly and can estimate their distance from them. This system can also distinguish between lights of different colors, a feature which is relevant to identifying signals and types of vehicles.</p>
<p>We will choose computationally inexpensive techniques, suitable for a low-powered computer—namely, Raspberry Pi—which we can plug into a car's cigarette lighter through an adapter. An LCD panel can display the relevant information, along with a live, rear-view video feed that is less glaring than real headlights.</p>
<p>This project presents us with several new topics and challenges, such as the following:</p>
<ul>
<li>How to detect blobs of light and classify their color</li>
<li>How to estimate the distance from the camera to a detected object whose real-world size is known</li>
<li>How to set up a low-budget lab where we can experiment with lights of many colors</li>
<li>How to set up a Raspberry Pi and peripherals in a car</li>
</ul>
<p>Realistically, our quick, homemade project is not sufficiently robust to be relied upon as an automotive safety tool, so take it with a grain of salt. However, it is a fun introduction to analyzing signal lights and wiring up a custom in-car computer. The choice of Raspberry Pi as a platform challenges us to think about the car as an environment for rapid prototyping. We can plug in any standard peripherals, including a webcam, keyboard, mouse, and even a monitor, giving us a complete desktop Linux system with Python—on wheels! (Snakes in a car!) For more exotic projects, the Pi is compatible with many electronics kits, too! A smartphone or tablet is also a good alternative for use in a car, and is easier to power than a Pi with a monitor, but the Pi excels as a well-rounded prototyping tool.</p>
<p>All we need now is a name for our project. So, let the app be known as <kbd>The Living Headlights</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li><strong>A Python environment with the following modules</strong>: OpenCV, NumPy, SciPy, wxPython</li>
</ul>
<p>Setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>. Refer to the setup instructions for any version requirements. Basic instructions for running Python code are covered in <a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml">Appendix C</a>, <em>Running with Snakes (or, First Steps with Python)</em>.</p>
<p class="mce-root">The completed project for this chapter can be found in this book's GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a>, in the <kbd>Chapter005</kbd> folder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning The Living Headlights app</h1>
                
            
            
                
<p>For this app, we need to return to the cross-platform wxPython framework. Optionally, we can also develop and test our wxPython application on a Windows, Mac, or Linux desktop or laptop before deploying it to our Raspberry Pi computer. With the Raspbian operating system, the Pi can run wxPython, just as any Linux desktop could.</p>
<p>The GUI for <kbd>The Living Headlights</kbd> includes a live video feed, a set of controls where the user can enter their true distance from headlights, and a label that initially displays a set of instructions, as seen in the following screenshot:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/1ac2b3e7-2f88-40ba-96a3-6aa0bce4c04c.png" style="width:40.92em;height:35.83em;"/></p>
<p class="mce-root"/>
<p>When a pair of headlights is detected, the user must perform a one-time calibration step. This step consists of entering the true distance between the camera and headlights (specifically, the midpoint between the headlights) and then clicking on the Calibrate button. Thereafter, the app continuously updates and displays an estimate of the headlights' distance and color, as seen in the label at the bottom of the following screenshot:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/b5f11f4d-a46c-4e46-b15d-6f825222e86c.png" style="width:40.42em;height:35.33em;"/></p>
<p class="mce-root"/>
<p>The calibration and the selected unit (Meters or Feet) are stored in a configuration file when the app closes. They are reloaded from this file when the app reopens. The calibration remains valid as long as the same camera and lens are used, the lens does not zoom, and the spacing between two headlights in a pair remains approximately constant for all pairs of headlights.</p>
<p>Atop the video feed, colored circles are drawn to mark detected lights, and lines are drawn between pairs of detected lights whose colors match. Such a pair is considered to be a set of headlights.</p>
<p>Next, let's consider techniques for detecting lights and classifying their colors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Detecting lights as blobs</h1>
                
            
            
                
<p>To the human eye, light can appear both very bright and very colorful. Imagine a sunny landscape or a storefront lit by a neon sign; they are bright and colorful! However, a camera captures a range of contrast that is much narrower and not as intelligently selected, so that the sunny landscape or neon-lit storefront can look washed out. This problem of poorly controlled contrast is especially bad in cheap cameras or cameras that have small sensors, such as webcams. As a result, bright light sources tend to be imaged as big white blobs with thin rims of color. These blobs also tend to mimic a lens's iris—typically, a polygon approximating a circle.</p>
<p>The thought of all lights becoming white and circular makes the world seem like a poorer place, if you ask me. Nonetheless, in computer vision, we can take advantage of such a predictable pattern. We can look for white blobs that are nearly circular and we can infer their human-perceptible color from a sample that includes extra pixels around the rim.</p>
<p><strong>Blob detection</strong> is actually a major branch of computer vision. Unlike the face detectors (or other object detectors) that we discussed in previous chapters, a blob detector is not trained. There is no concept of a reference image, so meaningful classifications such as <em>This blob is a light</em> or <em>This blob is skin </em>are more complicated to produce. Classification goes beyond the ken of the blob detector itself. We explicitly define thresholds between non-lights and lights, and between different human-perceptible colors of lights, based on <em>a priori</em> knowledge about typical shapes and colors of light sources, as imaged by a webcam.</p>
<p>Other terms for a blob include a <em>connected component</em> and a <em>region</em>. However, in this book, we just say <em>blob</em>.</p>
<p>At its simplest, blob detection consists of the five following steps:</p>
<ol>
<li>Partition the image into two or more colors. For example, this can be accomplished by <em>binary thresholding</em> (also called <strong>binarization</strong>), whereby all grayscale values above a threshold are converted into white and all grayscale values below the threshold are converted into black.</li>
<li>Find the <em>contour</em> of each contiguously colored region, that is, each blob. The contour is a set of points describing the region's outline.</li>
<li>Merge blobs that are deemed to be neighbors.</li>
<li>Optionally, determine each blob's <em>features</em>. These are<strong> </strong>higher-level measurements such as the center point, radius, and circularity. The usefulness of these features lies in their simplicity. For further blob-related computations and logic, it may be best to avoid complex representation, such as a contour's many points.</li>
<li>Reject blobs that fail to meet certain measurable criteria.</li>
</ol>
<p>OpenCV implements a simple blob detector in a class called <kbd>cv2.SimpleBlobDetector</kbd> (appropriately enough). This class's constructor takes an instance of a helper class called <kbd>cv2.SimpleBlobDetector_Params</kbd>, which describes the criteria for accepting or rejecting a candidate blob. <kbd>SimpleBlobDetector_Params</kbd> has the following member variables:</p>
<ul>
<li><kbd>thresholdStep</kbd>, <kbd>minThreshold</kbd>, and <kbd>maxThreshold</kbd>: The search for blobs is based on a series of binarized images (analogous to the series of scaled images that are searched by a Haar cascade detector, as described in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a>, <em>Training a Smart Alarm to Recognize the Villain and His Cat</em>). The thresholds for binarization are based on the range and step size given by these variables. We use <kbd>8</kbd>, <kbd>191</kbd>, and <kbd>255</kbd>.</li>
<li><kbd>minRepeatability</kbd>: This variable minus one is the minimum number of neighbors that a blob must have. We use <kbd>2</kbd>, meaning that a blob must have at least one neighbor. If we did not require at least one neighbor, the detector would tend to report a large number of blobs, with a lot of overlap between blobs.</li>
<li><kbd>minDistBetweenBlobs</kbd>: Blobs must be at least this many pixels apart. Blobs that are closer than the minimum distance from each other are counted as neighbors. We use a minimum distance calculated as two percent of the image's larger dimension (typically width).</li>
<li><kbd>filterByColor</kbd> (<kbd>True</kbd> or <kbd>False</kbd>) and <kbd>blobColor</kbd>: If <kbd>filterByColor</kbd> is <kbd>True</kbd>, a blob's central pixel must exactly match <kbd>blobColor</kbd>. We use <kbd>True</kbd> and <kbd>255</kbd> (white), based on our assumption that light sources are white blobs.</li>
<li><kbd>filterByArea</kbd> (<kbd>True</kbd> or <kbd>False</kbd>), <kbd>minArea</kbd>, and <kbd>maxArea</kbd>: If <kbd>filterByArea</kbd> is <kbd>True</kbd>, a blob's area in pixels must fall within the given range. We use <kbd>True</kbd> and a range calculated as 0.5 percent to 10 percent of the image's larger dimension (typically width).</li>
<li><kbd>filterByCircularity</kbd> (<kbd>True</kbd> or <kbd>False</kbd>), <kbd>minCircularity</kbd>, and <kbd>maxCircularity</kbd>: If <kbd>filterByCircularity</kbd> is <kbd>True</kbd>, a blob's circularity must fall within the given range, where circularity is defined as <kbd>4 * PI * area / (perimeter ^ 2)</kbd>. A circle's circularity is 1.0 and a line's circularity is 0.0. For our approximately circular light sources, we use <kbd>True</kbd> and the range 0.7 to 1.0.</li>
<li><kbd>filterByInertia</kbd> (<kbd>True</kbd> or <kbd>False</kbd>), <kbd>minInertiaRatio</kbd>, and <kbd>maxInertiaRatio</kbd>: If <kbd>filterByInertia</kbd> is <kbd>True</kbd>, a blob's inertia ratio must fall within the given range. A relatively high inertia ratio implies that the blob is relatively elongated (and would thus require more torque to rotate along its longest axis). A circle's inertia ratio is 1.0 and a line's inertia ratio is 0.0. We use <kbd>filterByInertia=False</kbd> (no filtering by inertia) because the circularity test already gives sufficient control over the shape for our purposes.</li>
<li><kbd>filterByConvexity</kbd> (<kbd>True</kbd> or <kbd>False</kbd>), <kbd>minConvexity</kbd>, and <kbd>maxConvexity</kbd>: If <kbd>filterByConvexity</kbd> is <kbd>True</kbd>, a blob's convexity must fall within the given range, where convexity is defined as <kbd>area/hullArea</kbd>. Here, <kbd>hullArea</kbd> refers to the area of the convex hull—the convex polygon surrounding all the points of a contour with the minimum area. Convexity is always more than 0.0 and less than 1.0. A relatively high convexity implies that the contour is relatively smooth. We use <kbd>filterByConvexity=False</kbd> (no filtering by convexity) because the circularity test already gives sufficient control over the shape for our purposes.</li>
</ul>
<p>Although these parameters cover many useful criteria, they are designed for grayscale images and do not provide a practical means of filtering or classifying blobs based on separate criteria for hue, saturation, and luminosity. The suggested values in the preceding list are tuned to extract bright blobs of light. However, we may want to classify such blobs by subtle variations in color, especially around the blob's edge.</p>
<p><strong>Hue</strong> refers to a color's angle on the color wheel, where <em>0</em> degrees is red, <em>120</em> is green, and <em>240</em> is blue. The hue in degrees can be calculated from RGB values with the following formula:</p>
<p><img class="fm-editor-equation" src="img/85334e33-6266-4d3c-b482-05437d6097d8.png" style="width:27.25em;height:1.42em;"/></p>
<p><strong>Saturation</strong> refers to a color's distance from grayscale. There are several alternative formulations of an RGB color's saturation. We use the following formulation, which some authors call <strong>chroma</strong> instead of saturation:</p>
<p><img class="fm-editor-equation" src="img/e863ddc6-697d-461c-8dd0-600f505f599f.png" style="width:21.92em;height:1.58em;"/></p>
<p>We can classify a light source's human-perceptible color based on the average hue and saturation of the blob and some surrounding pixels. The combination of a low saturation and a blue or yellow hue tends to suggest that the light will appear white to human vision. Other light sources may appear (in order of ascending hue) as red, orange/amber/yellow, green (a wide range from spring green to emerald), blue/purple (another wide range), or pink, to give just a few examples. Threshold values can be chosen based on trial and error.</p>
<p>Using the techniques we've mentioned, we can detect the location, pixel radius, and perceptual color of light sources. However, we need additional techniques to get an estimate of the real distance between the camera and a pair of headlights. Let's turn our attention to this problem now.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Estimating distances (a cheap approach)</h1>
                
            
            
                
<p>Suppose we have an object sitting in front of a pinhole camera. Regardless of the distance between the camera and the object, the following equation holds true:</p>
<p><img class="fm-editor-equation" src="img/90fd8367-ddea-474b-a401-522814d03c63.png" style="width:33.17em;height:1.42em;"/></p>
<p>We may use any unit (such as pixels) in the equation's left-hand side and any unit (such as meters) in its right-hand side (on each side of the equation, the division cancels the unit). Moreover, we may define the object's size based on anything linear that we can detect in the image, such as the diameter of a detected blob or the width of a detected face rectangle.</p>
<p>Let's rearrange the equation to illustrate that the distance to the object is inversely proportional to the object's size in the image:</p>
<p><img class="fm-editor-equation" src="img/034b8d7f-f388-49a7-88af-e6cedf04b718.png" style="width:33.58em;height:1.42em;"/></p>
<p>Let's assume that the object's real size and the camera's focal length are constant (a constant focal length means that the lens does not zoom and we do not swap the lens for a different lens). Consider the following arrangement, which isolates this pair of constants on the right-hand side of the equation:</p>
<p><img class="fm-editor-equation" src="img/7627267c-3d3f-464d-9942-bb924375dd37.png" style="width:34.58em;height:1.25em;"/></p>
<p>As the right-hand side of the equation is constant, so is the left. We may conclude that the following relationship holds true over time:</p>
<p><img class="fm-editor-equation" src="img/d50c2596-5ff5-4127-8000-20e550eece2e.png" style="width:33.58em;height:1.17em;"/></p>
<p>Let's solve the following equation for the new distance:</p>
<p><img class="fm-editor-equation" src="img/9898b8b0-4da5-4ae8-aa7b-649f9632fbeb.png" style="width:32.83em;height:1.33em;"/></p>
<p>Now, let's think about applying this equation in software. To provide a ground truth, the user must take a single, true measurement of the distance to use as the <em>old</em> distance in all future calculations. As well as this, we must know the object's old pixel size and its subsequent new size so that we can compute the new distance any time there is a detection result. Let's review the following assumptions:</p>
<ul>
<li>There is no lens distortion; the pinhole camera model applies</li>
<li>Focal length is constant; no zoom is applied and the lens is not swapped for a different lens</li>
<li>The object is rigid; its real-world measurements do not change</li>
<li>The camera is always viewing the same side of the object; the relative rotation of the camera and object does not change</li>
</ul>
<p>You might wonder whether the first assumption is problematic, as webcams often have cheap wide angle lenses with significant distortion. Despite lens distortion, does the object's size in the image remain inversely proportional to the real distance between the camera and object? The following paper reports experimental results for a lens that appears to distort badly and an object that is located off-center (in an image region where distortion is likely to be especially bad)—M. N. A. Wahab, N. Sivadev, and K. Sundaraj. <em>Target distance estimation using monocular vision system for mobile robot</em>. <strong>IEEE Conference on Open Systems</strong> (<strong>ICOS</strong>) 2011 Proceedings, vol. 11, no. 15, p. 25-28. September 2011.</p>
<p>Using exponential regression, the authors show that the following model is a good fit for experimental data (<em>R^2=0.995</em>):</p>
<pre>distanceInCentimeters = 4042 * (objectSizeInPixels ^ -1.2)  </pre>
<p>Note that the exponent is close to <em>-1</em>, and thus the statistical model is not far from the ideal inverse relationship. (Even the poor-quality lens and off-center subject did not disprove our assumptions!)</p>
<p>We can also ensure that the second assumption (no zooming and no swapping of the lens) holds true.</p>
<p>Let's consider the third and fourth assumptions (rigidity and constant rotation) in the case of a camera and object—one in each car on a highway. Except in a crash, most of a car's exterior parts are rigid. Except when passing or pulling over, one car travels directly behind the other on a surface that is mostly flat and mostly straight. However, on a road that is hilly or has many turns, these assumptions start to fall apart. It becomes more difficult to predict which side of the object is currently being viewed; thus, it is more difficult to say whether our reference measurements apply to a particular side.</p>
<p>Of course, we need to define a generic car part to be our <em>object</em>. The headlights (and the space between them) are a decent choice, since we have a method for detecting them and the distance between headlights is consistent across many cars—although not all.</p>
<p>All distance estimation techniques in computer vision rely on some assumptions or calibration steps that relate to the camera, the object, the relationship between camera and object, or lighting. For comparison, let's consider some of the following common distance estimation techniques:</p>
<ul>
<li>A <strong>time-of-flight</strong> (<strong>ToF</strong>) camera shines a light on objects and measures the intensity of any reflected light. This intensity is used to estimate the distance at each pixel based on the known fall-off characteristics of the light source. Some ToF cameras, such as Microsoft Kinect, use an infrared light source. Other, more expensive ToF cameras scan a scene with a laser or even use a grid of lasers. ToF cameras may suffer from interference if other bright lights are being imaged, so they are poorly suited to our application.</li>
<li>A <strong>stereo camera</strong> consists of two parallel cameras with a known, fixed distance between them. In each frame, a pair of images is captured, features are identified, and a <em>disparity</em> or pixel distance is calculated for each pair of corresponding features. We can convert disparity into real distance based on the cameras' known field of view and the distance between them. For our application, stereo techniques would be feasible, but they are also computationally expensive and use a lot of input bus bandwidth. Optimizing these techniques for Raspberry Pi would be a big challenge.</li>
<li><strong>Structure from Motion</strong> (<strong>SfM</strong>) techniques only need a single, regular camera, but rely on moving the camera by known distances over time. For each pair of images taken from neighboring locations, disparities are calculated, as with a stereo camera. In this scenario, as well as knowing the camera's movements, we must know the object's movements or lack thereof. Due to these limitations, SfM techniques are poorly suited to our application, as our camera and object are mounted on two freely moving vehicles.</li>
<li>Various <strong>3D feature tracking</strong> techniques entail estimating the rotation of an object, as well as its distance and other coordinates. Edges and texture details are also considered. The differences between models of cars make it difficult to define one set of features that are suitable for 3D tracking, and so 3D feature tracking is not well-suited to our application. Moreover, 3D tracking is computationally expensive, especially by the standards of a low-powered computer such as Raspberry Pi.</li>
</ul>
<div><p>For more information on these techniques, refer to the following books, available from Packt Publishing:</p>
<ul>
<li class="BulletWithinInformationBoxPACKT0">Kinect and other ToF cameras are covered in the first edition of my book, <em>OpenCV Computer Vision with Python</em>, specifically <em>Chapter 5, Detecting Foreground/Background Regions and Depth</em>.</li>
<li class="BulletWithinInformationBoxPACKT0">3D feature tracking and SfM are covered in <em>Mastering OpenCV with Practical Computer Vision Projects</em>, specifically <em>Chapter 3, Markerless Augmented Reality,</em> and <em>Chapter 4, Exploring Structure from Motion Using OpenCV</em>.</li>
<li>Stereo vision and 3D feature tracking are covered in Robert Laganière's <em>OpenCV 3 Computer Vision Application Programming Cookbook</em>, specifically <em>Chapter 10, Estimating Projective Relations in Images</em>.</li>
<li>Stereo vision and 3D pose estimation are also covered in Alexey Spizhevoy and Aleksandr Rybnikov's <em>OpenCV 3 Computer Vision with Python Cookbook</em>, specifically <em>Chapter 9, Multiple View Geometry</em>.</li>
</ul>
</div>
<p>On balance, the simplistic approach—based on pixel distances being inversely proportional to real distances—is a justifiable choice given our application and our intent to support the Pi.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing The Living Headlights app</h1>
                
            
            
                
<p><kbd>The Living Headlights</kbd> app will use the following files:</p>
<ul>
<li><kbd>LivingHeadlights.py</kbd>: This is a new file that contains our application class and its <kbd>main</kbd> function.</li>
<li><kbd>ColorUtils.py</kbd>: This is a new file that contains the utility functions required to convert colors into different representations.</li>
<li><kbd>GeomUtils.py</kbd>: This contains utility functions for geometric calculations. Copy or link to the version that we used in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>.</li>
<li><kbd>PyInstallerUtils.py</kbd>: This contains utility functions for accessing resources in a PyInstaller application bundle. Copy or link to the version that we used in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>.</li>
<li><kbd>ResizeUtils.py</kbd>: This contains utility functions for resizing images, including camera capture dimensions. Copy or link to the version that we used in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>.</li>
<li><kbd>WxUtils.py</kbd>: This contains utility functions for using OpenCV images in wxPython apps. Copy or link to the version that we used in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>.</li>
</ul>
<p>Let's get started with the creation of <kbd>ColorUtils.py</kbd>. Here, we need functions to calculate a color's hue and saturation according to the formulae mentioned in the <em>Detecting lights as blobs</em> section. The module's implementation is shown in the following code:</p>
<pre>import math<br/><br/><br/>def hueFromBGR(color):<br/>    b, g, r = color<br/>    # Note: sqrt(3) = 1.7320508075688772<br/>    hue = math.degrees(math.atan2(<br/>        1.7320508075688772 * (g - b), 2 * r - g - b))<br/>    if hue &lt; 0.0:<br/>        hue += 360.0<br/>    return hue<br/><br/>def saturationFromBGR(color):<br/>    return max(color) - min(color)</pre>
<p class="BulletWithinInformationBoxPACKT0">If we want to convert an entire image (that is, every pixel) to hue, saturation, and either luminosity or value, we can use the following OpenCV method, <kbd>cvtColor</kbd>:</p>
<pre class="CodeWithinInformationBoxPACKT0">hslImage = cv2.cvtColor(bgrImage, cv2.COLOR_BGR2HLS)<br/><br/>hsvImage = cv2.cvtColor(bgrImage, cv2.COLOR_BGR2HSV)</pre>
<p>See the following Wikipedia article for definitions of saturation, luminosity, and value in HSV and HSL color models at <a href="https://en.wikipedia.org/wiki/HSL_and_HSV">https://en.wikipedia.org/wiki/HSL_and_HSV</a>. Our definition of saturation is called <strong>chroma</strong> in the Wikipedia article, which differs from HSL saturation, and in turn differs again from HSV saturation. Moreover, OpenCV represents hue in units of two degrees (a range of <kbd>0</kbd> to <kbd>180</kbd>) so that the hue channel fits inside a byte.<br/>
<br/>
For some types of image segmentation problems, it is useful to convert the entire image into HSV, HSL, or another color model. For example, see Rebecca Stone's blog post about segmenting images of clown fish at <a href="https://realpython.com/python-opencv-color-spaces/">https://realpython.com/python-opencv-color-spaces/</a>, or Vikas Gupta's blog post about segmenting images of Rubik's cubes at <a href="https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/">https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/</a>.<br/></p>
<p>We have written our own conversion functions because, for our purposes, converting an entire image is unnecessary; we just need to convert a sample from each blob. We also prefer a more accurate floating-point representation instead of the byte-sized integer representation that OpenCV imposes.</p>
<p>We also need to modify <kbd>GeomUtils.py</kbd> by adding a function to calculate the Euclidean distance between two 2D points, such as the pixel coordinates of two headlights in an image. At the top of the file, let's add an import statement and implement the function, as shown in the following code:</p>
<pre>import math<br/><br/><br/>def dist2D(p0, p1):<br/>    deltaX = p1[0] - p0[0]<br/>    deltaY = p1[1] - p0[1]<br/>    return math.sqrt(deltaX * deltaX +<br/>                     deltaY * deltaY)</pre>
<p>Distances (and other magnitudes) can also be calculated using NumPy's <kbd>linalg.norm</kbd> function, as seen in the following code:</p>
<pre>dist = numpy.linalg.norm(a1 - a0)</pre>
<p>Here, <kbd>a0</kbd> and <kbd>a1</kbd> can be any size and shape. However, for a low-dimensional space such as 2D or 3D coordinate vectors, the overhead of using NumPy arrays is probably not worthwhile, so a utility function such as ours is a reasonable alternative.</p>
<p>The preceding code contains all the new utility functions. Now, let's create a file, <kbd>LivingHeadlights.py</kbd>, for the app's <kbd>main</kbd> class, <kbd>LivingHeadlights</kbd>. Like <kbd>InteractiveRecognizer</kbd> in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a><em>, Training a Smart Alarm to Recognize the Villain and His Cat</em>, <kbd>LivingHeadlights</kbd> is a class for a wxPython app that captures and processes images on a background thread (to avoid blocking the GUI on the main thread), allows a user to enter reference data, serializes its reference data when exiting, and deserializes its reference data when starting up again. This time, serialization and deserialization is accomplished using Python's <kbd>cPickle</kbd> module or, if <kbd>cPickle</kbd> is unavailable for any reason, the less-optimized <kbd>pickle</kbd> module. Let's add the following import statements to the start of <kbd>LivingHeadlights.py</kbd>:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import numpy<br/>import cv2<br/>import os<br/>import threading<br/>import wx<br/><br/>try:<br/>   import cPickle as pickle<br/>except:<br/>   import pickle<br/><br/>import ColorUtils<br/>import GeomUtils<br/>import PyInstallerUtils<br/>import ResizeUtils<br/>import WxUtils</pre>
<p>Let's also define some BGR color values and names at the start of the module. We will classify each blob as one of the following colors, depending on its hue and saturation:</p>
<pre>COLOR_Red =         ((  0,   0, 255), 'red')<br/>COLOR_YellowWhite = ((223, 247, 255), 'yellowish white')<br/>COLOR_AmberYellow = ((  0, 191, 255), 'amber or yellow')<br/>COLOR_Green =       ((128, 255, 128), 'green')<br/>COLOR_BlueWhite =   ((255, 231, 223), 'bluish white')<br/>COLOR_BluePurple =  ((255,  64,   0), 'blue or purple')<br/>COLOR_Pink =        ((240, 128, 255), 'pink')</pre>
<p>Now, let's begin implementing the class. The initializer takes several arguments relating to the configuration of the blob detector and the camera. Refer back to the <em>Detecting lights as blobs</em> section for explanations of the blob detection parameters supported by OpenCV's <kbd>SimpleBlobDetector</kbd> and <kbd>SimpleBlobDetector_Params</kbd> classes. The class declaration and initializer declaration is as follows:</p>
<pre>class LivingHeadlights(wx.Frame):<br/><br/>    def __init__(self, configPath, thresholdStep=8.0,<br/>                 minThreshold=191.0, maxThreshold=255.0,<br/>                 minRepeatability=2,<br/>                 minDistBetweenBlobsProportional=0.02,<br/>                 minBlobAreaProportional=0.005,<br/>                 maxBlobAreaProportional=0.1,<br/>                 minBlobCircularity=0.7, cameraDeviceID=0,<br/>                 imageSize=(640, 480),<br/>                 title='The Living Headlights'):</pre>
<p>We start the initializer's implementation by setting a public Boolean variable that indicates to the app to display a mirrored image and a protected Boolean variable that ensures the app is running, as follows:</p>
<pre>        self.mirrored = True<br/><br/>        self._running = True</pre>
<p>If there is any configuration file saved from a previous run of the app, we deserialize the reference measurements (the pixel distance between lights and the real distance in meters between lights and the camera), as well as the user's preferred unit of measurement (<kbd>meters</kbd> or <kbd>feet</kbd>), as follows:</p>
<pre>        self._configPath = configPath<br/>        self._pixelDistBetweenLights = None<br/>        if os.path.isfile(configPath):<br/>            with open(self._configPath, 'rb') as file:<br/>                self._referencePixelDistBetweenLights = \<br/>                        pickle.load(file)<br/>                self._referenceMetersToCamera = \<br/>                        pickle.load(file)<br/>                self._convertMetersToFeet = pickle.load(file)<br/>        else:<br/>            self._referencePixelDistBetweenLights = None<br/>            self._referenceMetersToCamera = None<br/>            self._convertMetersToFeet = False</pre>
<p>Now, we initialize a <kbd>VideoCapture</kbd> object and try to configure the size of the captured images. If the requested size is unsupported, we fall back to the default size, as shown in the following code:</p>
<pre>        self._capture = cv2.VideoCapture(cameraDeviceID)<br/>        size = ResizeUtils.cvResizeCapture(<br/>                self._capture, imageSize)<br/>        w, h = size<br/>        self._imageWidth, self._imageHeight = w, h</pre>
<p>We also need to declare variables for the images we will capture, process, and display. Initially, these are <kbd>None</kbd>. We also need to create a lock to manage thread-safe access to an image that will be captured and processed on one thread, and then drawn to the screen on another thread. The relevant declarations are as follows:</p>
<pre>        self._image = None<br/>        self._grayImage = None<br/><br/>        self._imageFrontBuffer = None<br/>        self._imageFrontBufferLock = threading.Lock()</pre>
<p>Now, we create a <kbd>SimpleBlobDetector_Params</kbd> object and a <kbd>SimpleBlobDetector</kbd> object based on the arguments passed to the app's initializer, as follows:</p>
<pre>        minDistBetweenBlobs = \<br/>                min(w, h) * \<br/>                minDistBetweenBlobsProportional<br/><br/>        area = w * h<br/>        minBlobArea = area * minBlobAreaProportional<br/>        maxBlobArea = area * maxBlobAreaProportional<br/><br/>        detectorParams = cv2.SimpleBlobDetector_Params()<br/><br/>        detectorParams.minDistBetweenBlobs = \<br/>                minDistBetweenBlobs<br/><br/>        detectorParams.thresholdStep = thresholdStep<br/>        detectorParams.minThreshold = minThreshold<br/>        detectorParams.maxThreshold = maxThreshold<br/><br/>        detectorParams.minRepeatability = minRepeatability<br/><br/>        detectorParams.filterByArea = True<br/>        detectorParams.minArea = minBlobArea<br/>        detectorParams.maxArea = maxBlobArea<br/><br/>        detectorParams.filterByColor = True<br/>        detectorParams.blobColor = 255<br/><br/>        detectorParams.filterByCircularity = True<br/>        detectorParams.minCircularity = minBlobCircularity<br/><br/>        detectorParams.filterByInertia = False<br/><br/>        detectorParams.filterByConvexity = False<br/><br/>        self._detector = cv2.SimpleBlobDetector_create(<br/>                detectorParams)</pre>
<p>Here, we specify the style of the app's window and we initialize the following base class, <kbd>wx.Frame</kbd>:</p>
<pre>        style = wx.CLOSE_BOX | wx.MINIMIZE_BOX | \<br/>                wx.CAPTION | wx.SYSTEM_MENU | \<br/>                wx.CLIP_CHILDREN<br/>        wx.Frame.__init__(self, None, title=title,<br/>                          style=style, size=size)<br/>        self.SetBackgroundColour(wx.Colour(232, 232, 232))</pre>
<p>We now need to bind the <em>Esc</em> key to a callback that closes the app, as follows:</p>
<pre>        self.Bind(wx.EVT_CLOSE, self._onCloseWindow)<br/><br/>        quitCommandID = wx.NewId()<br/>        self.Bind(wx.EVT_MENU, self._onQuitCommand,<br/>                  id=quitCommandID)<br/>        acceleratorTable = wx.AcceleratorTable([<br/>            (wx.ACCEL_NORMAL, wx.WXK_ESCAPE,<br/>             quitCommandID)<br/>        ])<br/>        self.SetAcceleratorTable(acceleratorTable)</pre>
<p>Now, let's create the GUI elements, including the bitmap, the text field for the reference distance, radio buttons for the unit (<kbd>meters</kbd> or <kbd>feet</kbd>), and the Calibrate button. We also need to bind callbacks for various input events, as shown in the following code:</p>
<pre>        self._videoPanel = wx.Panel(self, size=size)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_ERASE_BACKGROUND,<br/>                self._onVideoPanelEraseBackground)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_PAINT, self._onVideoPanelPaint)<br/><br/>        self._videoBitmap = None<br/><br/>        self._calibrationTextCtrl = wx.TextCtrl(<br/>                self, style=wx.TE_PROCESS_ENTER)<br/>        self._calibrationTextCtrl.Bind(<br/>                wx.EVT_KEY_UP,<br/>                self._onCalibrationTextCtrlKeyUp)<br/><br/>        self._distanceStaticText = wx.StaticText(self)<br/>        if self._referencePixelDistBetweenLights is None:<br/>            self._showInstructions()<br/>        else:<br/>            self._clearMessage()<br/><br/>        self._calibrationButton = wx.Button(<br/>                self, label='Calibrate')<br/>        self._calibrationButton.Bind(<br/>                wx.EVT_BUTTON, self._calibrate)<br/>        self._calibrationButton.Disable()<br/><br/>        border = 12<br/><br/>        metersButton = wx.RadioButton(self,<br/>                                      label='Meters')<br/>        metersButton.Bind(wx.EVT_RADIOBUTTON,<br/>                          self._onSelectMeters)<br/><br/>        feetButton = wx.RadioButton(self, label='Feet')<br/>        feetButton.Bind(wx.EVT_RADIOBUTTON,<br/>                        self._onSelectFeet)</pre>
<p>We need to ensure that the proper radio buttons start in the selected state, depending on the configuration data that we deserialized earlier, as follows:</p>
<pre>        if self._convertMetersToFeet:<br/>            feetButton.SetValue(True)<br/>        else:<br/>            metersButton.SetValue(True)</pre>
<p>Next, we stack the radio buttons vertically using a <kbd>BoxSizer</kbd>, as follows:</p>
<pre>        unitButtonsSizer = wx.BoxSizer(wx.VERTICAL)<br/>        unitButtonsSizer.Add(metersButton)<br/>        unitButtonsSizer.Add(feetButton)</pre>
<p>We then line up all of our controls horizontally, again using a <kbd>BoxSizer</kbd>, as follows:</p>
<pre>        controlsSizer = wx.BoxSizer(wx.HORIZONTAL)<br/>        style = wx.ALIGN_CENTER_VERTICAL | wx.RIGHT<br/>        controlsSizer.Add(self._calibrationTextCtrl, 0,<br/>                          style, border)<br/>        controlsSizer.Add(unitButtonsSizer, 0, style,<br/>                          border)<br/>        controlsSizer.Add(self._calibrationButton, 0,<br/>                          style, border)<br/>        controlsSizer.Add(self._distanceStaticText, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL)</pre>
<p>To finish our layout, we place the controls below the image, as shown in the following code:</p>
<pre>        rootSizer = wx.BoxSizer(wx.VERTICAL)<br/>        rootSizer.Add(self._videoPanel)<br/>        rootSizer.Add(controlsSizer, 0,<br/>                      wx.EXPAND | wx.ALL, border)<br/>        self.SetSizerAndFit(rootSizer)</pre>
<p>The last thing we do in the initializer is start a background thread to capture and process images from the camera using the following code:</p>
<pre>        self._captureThread = threading.Thread(<br/>                target=self._runCaptureLoop)<br/>        self._captureThread.start()</pre>
<p>When closing the app, we first ensure that the capture thread terminates, just as we did for the <kbd>InteractiveRecognizer</kbd> in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a>, <em>Training a Smart Alarm to Recognize the Villain and His Cat</em>. We also use <kbd>pickle</kbd> or <kbd>cPickle</kbd> to serialize the reference measurements and preferred unit (<kbd>meters</kbd> or <kbd>feet</kbd>) to a file. The implementation of the relevant callback is as follows:</p>
<pre>    def _onCloseWindow(self, event):<br/>        self._running = False<br/>        self._captureThread.join()<br/>        configDir = os.path.dirname(self._configPath)<br/>        if not os.path.isdir(configDir):<br/>            os.makedirs(configDir)<br/>        with open(self._configPath, 'wb') as file:<br/>            pickle.dump(self._referencePixelDistBetweenLights,<br/>                        file)<br/>            pickle.dump(self._referenceMetersToCamera, file)<br/>            pickle.dump(self._convertMetersToFeet, file)<br/>        self.Destroy()</pre>
<p>The callback associated with the <em>Esc</em> button just closes the app, as follows:</p>
<pre>    def _onQuitCommand(self, event):<br/>        self.Close()</pre>
<p>The video panel's erase and paint events are bound to callbacks, <kbd>_onVideoPanelEraseBackground</kbd> and<kbd>_onVideoPanelPaint</kbd>, which have the same implementations as <kbd>InteractiveRecognizer</kbd> in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a>, <em>Training a Smart Alarm to Recognize the Villain and His Cat</em>, as follows:</p>
<pre>    def _onVideoPanelEraseBackground(self, event):<br/>        pass<br/><br/>    def _onVideoPanelPaint(self, event):<br/><br/>        self._imageFrontBufferLock.acquire()<br/><br/>        if self._imageFrontBuffer is None:<br/>            self._imageFrontBufferLock.release()<br/>            return<br/><br/>        # Convert the image to bitmap format.<br/>        self._videoBitmap = \<br/>                WxUtils.wxBitmapFromCvImage(self._imageFrontBuffer)<br/><br/>        self._imageFrontBufferLock.release()<br/><br/>        # Show the bitmap.<br/>        dc = wx.BufferedPaintDC(self._videoPanel)<br/>        dc.DrawBitmap(self._videoBitmap, 0, 0)</pre>
<p>When either of the radio buttons are selected, we need to record the newly selected unit of measurement, as seen in the following two callback methods:</p>
<pre>    def _onSelectMeters(self, event):<br/>        self._convertMetersToFeet = False<br/><br/>    def _onSelectFeet(self, event):<br/>        self._convertMetersToFeet = True</pre>
<p>Whenever a new character is entered in the text field, we need to call a helper method to validate the text as potential input, as follows:</p>
<pre>    def _onCalibrationTextCtrlKeyUp(self, event):<br/>        self._enableOrDisableCalibrationButton()</pre>
<p>When the Calibrate button is clicked, we parse the measurement from the text field, clear the text field, convert the measurement into <kbd>meters</kbd> if necessary, and store it. The button's callback is implemented as follows:</p>
<pre>    def _calibrate(self, event):<br/>        self._referencePixelDistBetweenLights = \<br/>                self._pixelDistBetweenLights<br/>        s = self._calibrationTextCtrl.GetValue()<br/>        self._calibrationTextCtrl.SetValue('')<br/>        self._referenceMetersToCamera = float(s)<br/>        if self._convertMetersToFeet:<br/>            self._referenceMetersToCamera *= 0.3048</pre>
<p>As in <a href="49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml">Chapter 3</a>, <em>Training a Smart Alarm to Recognize the Villain and His Cat</em>, the background thread runs a loop, which includes capturing an image, calling a helper method to process the image, and then handing the image to another thread for display. Optionally, the image may be mirrored (flipped horizontally) before being displayed. The loop's implementation is as follows:</p>
<pre>    def _runCaptureLoop(self):<br/>        while self._running:<br/>            success, self._image = self._capture.read(<br/>                    self._image)<br/>            if self._image is not None:<br/>                self._detectAndEstimateDistance()<br/>                if (self.mirrored):<br/>                    self._image[:] = numpy.fliplr(self._image)<br/><br/>                # Perform a thread-safe swap of the front and<br/>                # back image buffers.<br/>                self._imageFrontBufferLock.acquire()<br/>                self._imageFrontBuffer, self._image = \<br/>                        self._image, self._imageFrontBuffer<br/>                self._imageFrontBufferLock.release()<br/><br/>                # Send a refresh event to the video panel so<br/>                # that it will draw the image from the front<br/>                # buffer.<br/>                self._videoPanel.Refresh()</pre>
<p>The helper method for processing the image is quite long, so let's look at it in several chunks. First, we detect blobs in a gray version of the image and then initialize a dictionary to sort the blobs by color, as follows:</p>
<pre>    def _detectAndEstimateDistance(self):<br/><br/>        self._grayImage = cv2.cvtColor(<br/>                self._image, cv2.COLOR_BGR2GRAY,<br/>                self._grayImage)<br/>        blobs = self._detector.detect(self._grayImage)<br/>        blobsForColors = {}</pre>
<p>For each blob, we crop out a square region that is likely to include a white circle of light, plus some more saturated pixels around the edge, as shown in the following code:</p>
<pre>        for blob in blobs:<br/><br/>            centerXAsInt, centerYAsInt = \<br/>                    (int(n) for n in blob.pt)<br/>            radiusAsInt = int(blob.size)<br/><br/>            minX = max(0, centerXAsInt - radiusAsInt)<br/>            maxX = min(self._imageWidth,<br/>                       centerXAsInt + radiusAsInt)<br/>            minY = max(0, centerYAsInt - radiusAsInt)<br/>            maxY = min(self._imageHeight,<br/>                       centerYAsInt + radiusAsInt)<br/><br/>            region = self._image[minY:maxY, minX:maxX]</pre>
<p>Next, we find the average hue and saturation of the region and, using those values, we classify the blob as one of the colors we defined at the top of this module, as follows:</p>
<pre>            # Get the region's dimensions, which may<br/>            # differ from the blob's diameter if the blob<br/>            # extends past the edge of the image.<br/>            h, w = region.shape[:2]<br/><br/>            meanColor = region.reshape(w * h, 3).mean(0)<br/>            meanHue = ColorUtils.hueFromBGR(meanColor)<br/>            meanSaturation = ColorUtils.saturationFromBGR(<br/>                    meanColor)<br/><br/>            if meanHue &lt; 22.5 or meanHue &gt; 337.5:<br/>                color = COLOR_Red<br/>            elif meanHue &lt; 67.5:<br/>                if meanSaturation &lt; 25.0:<br/>                    color = COLOR_YellowWhite<br/>                else:<br/>                    color = COLOR_AmberYellow<br/>            elif meanHue &lt; 172.5:<br/>                color = COLOR_Green<br/>            elif meanHue &lt; 277.5:<br/>                if meanSaturation &lt; 25.0:<br/>                    color = COLOR_BlueWhite<br/>                else:<br/>                    color = COLOR_BluePurple<br/>            else:<br/>                color = COLOR_Pink<br/><br/>            if color in blobsForColors:<br/>                blobsForColors[color] += [blob]<br/>            else:<br/>                blobsForColors[color] = [blob]</pre>
<div><br/>
<p>Depending on your camera's color rendition, you may need to tweak some of the hue and saturation thresholds.</p>
<p>Note that our color-matching logic is based on perceptual (subjective) similarity and not on the geometric distance in any color model, such as RGB, HSV, or HSL. Perceptually, a <em>green</em> light could be emerald green (geometrically close to cyan), neon green, or even spring green (geometrically close to yellow), but most people would never mistake a spring green light for an <em>amber</em> light, nor a yellowish-orange light for a <em>red</em> light. Within the reddish and yellowish ranges, most people perceive more abrupt distinctions between colors.</p>
</div>
<p>Finally, after classifying all blobs, we call a helper method that handles the classification results and a helper method that may enable or disable the Calibrate button, as follows:</p>
<pre>        self._processBlobsForColors(blobsForColors)<br/>        self._enableOrDisableCalibrationButton()</pre>
<p>Based on the color classification results, we want to highlight the blobs in certain colors, draw lines that connect pairs of like-colored blobs (if any), and display a message about the estimated distance to the first such pair of blobs. We use the BGR color values and human-readable color names that we defined at the top of this module. The relevant code is as follows:</p>
<pre>    def _processBlobsForColors(self, blobsForColors):<br/><br/>        self._pixelDistBetweenLights = None<br/><br/>        for color in blobsForColors:<br/><br/>            prevBlob = None<br/><br/>            for blob in blobsForColors[color]:<br/><br/>                colorBGR, colorName = color<br/><br/>                centerAsInts = \<br/>                        tuple(int(n) for n in blob.pt)<br/>                radiusAsInt = int(blob.size)<br/><br/>                # Fill the circle with the selected color.<br/>                cv2.circle(self._image, centerAsInts,<br/>                           radiusAsInt, colorBGR,<br/>                           cv2.FILLED, cv2.LINE_AA)<br/>                # Outline the circle in black.<br/>                cv2.circle(self._image, centerAsInts,<br/>                           radiusAsInt, (0, 0, 0), 1,<br/>                           cv2.LINE_AA)<br/><br/>                if prevBlob is not None:<br/><br/>                    if self._pixelDistBetweenLights is \<br/>                            None:<br/>                        self._pixelDistBetweenLights = \<br/>                                GeomUtils.dist2D(blob.pt,<br/>                                             prevBlob.pt)<br/>                        wx.CallAfter(self._showDistance,<br/>                                     colorName)<br/><br/>                    prevCenterAsInts = \<br/>                        tuple(int(n) for n in prevBlob.pt)<br/><br/>                    # Connect the current and previous<br/>                    # circle with a black line.<br/>                    cv2.line(self._image, prevCenterAsInts,<br/>                             centerAsInts, (0, 0, 0), 1,<br/>                             cv2.LINE_AA)<br/><br/>                prevBlob = blob</pre>
<p>Next, let's look at the helper method that enables or disables the Calibrate button. The button should be enabled only when a pixel distance between two lights is being measured and a number (the real distance between the lights and camera) is in the text field. The following code illustrates the tests for these conditions:</p>
<pre>    def _enableOrDisableCalibrationButton(self):<br/>        s = self._calibrationTextCtrl.GetValue()<br/>        if len(s) &lt; 1 or \<br/>                self._pixelDistBetweenLights is None:<br/>            self._calibrationButton.Disable()<br/>        else:<br/>            # Validate that the input is a number.<br/>            try:<br/>                float(s)<br/>                self._calibrationButton.Enable()<br/>            except:<br/>                self._calibrationButton.Disable()</pre>
<p>The helper method that shows the instructional message is as follows:</p>
<pre>    def _showInstructions(self):<br/>        self._showMessage(<br/>                'When a pair of lights is highlighted, '<br/>                'enter the\ndistance and click '<br/>                '"Calibrate".')</pre>
<p>The helper method that shows the estimated distance in either <kbd>meters</kbd> or <kbd>feet</kbd> is as follows:</p>
<pre>    def _showDistance(self, colorName):<br/>        if self._referenceMetersToCamera is None:<br/>            return<br/>        value = self._referenceMetersToCamera * \<br/>                self._referencePixelDistBetweenLights / \<br/>                self._pixelDistBetweenLights<br/>        if self._convertMetersToFeet:<br/>            value /= 0.3048<br/>            unit = 'feet'<br/>        else:<br/>            unit = 'meters'<br/>        self._showMessage(<br/>                'A pair of %s lights was spotted\nat '<br/>                '%.2f %s.' % \<br/>                (colorName, value, unit))</pre>
<p>Once the message is cleared, we need to leave an endline character so that the label still has the same height as when it is populated, as follows:</p>
<pre>    def _clearMessage(self):<br/>        # Insert an endline for consistent spacing.<br/>        self._showMessage('\n')</pre>
<p>Showing a message simply entails changing the text of the <kbd>StaticText</kbd> object, as seen in the following helper method:</p>
<pre>    def _showMessage(self, message):<br/>        self._distanceStaticText.SetLabel(message)</pre>
<p>The class is complete. Now, we just need the following <kbd>main</kbd> function (similar to our <kbd>main</kbd> functions for previous wxPython apps) to specify a file path for serialization and deserialization and to launch the app, as shown in the following code:</p>
<pre>def main():<br/>    app = wx.App()<br/>    configPath = PyInstallerUtils.resourcePath(<br/>            'config.dat')<br/>    livingHeadlights = LivingHeadlights(configPath)<br/>    livingHeadlights.Show()<br/>    app.MainLoop()<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>There we have it! That's the whole implementation of <kbd>The Living Headlights</kbd> app! This project's code is short, but it does include some unusual requirements for setup and testing. Let's turn to these tasks now.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing The Living Headlights app at home</h1>
                
            
            
                
<p>Do not run out onto the highway at night to point your laptop's webcam into the headlights! We can devise more convenient and safer ways to test <kbd>The Living Headlights</kbd>, even if you don't own a car or don't drive.</p>
<p>A pair of LED flashlights is a good proxy for a pair of headlights. A flashlight with many LEDs (for example, 19) is preferable because it creates a denser circle of light that is more likely to be detected as exactly one blob. To ensure that the distance between the two flashlights remains constant, we can attach them to a rigid object, such as a board, using brackets, clamps, or tape. My father Bob Howse is great at constructing such things. Take a look at my flashlight holder in the following image:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/70d2121d-7996-49c6-8aad-302ceee5bcd8.png" style="width:27.33em;height:18.17em;"/></p>
<p>The following image shows a frontal view of the flashlight holder, including a decorative grill:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/6b2646bc-1b73-4a68-8467-a514a82bf34d.png" style="width:23.75em;height:15.75em;"/></p>
<p>Set up the lights in front of the webcam (parallel to the webcam's lens), run the app, and make sure that the lights are being detected. Then, using a tape measure, find the distance between the webcam and the center point between the front of the lights, as seen in the following image:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/fecc212d-5b48-4ead-8f7d-06d1cd67abaa.png" style="width:18.00em;height:24.00em;"/></p>
<p>Type the distance into the text field and click Calibrate. Then, move the lights either closer to or further away from the camera, ensuring they are parallel to the camera's lens. Check that the app is updating the distance estimate appropriately.</p>
<p>To simulate colored car lights, place a thick piece of colored glass in front of the flashlights, as close to the light source as possible. Stained glass (the kind used in church windows) works well, and you may find it in craft supply stores. Colored lens filters for photography or videography should also work. They are widely available, new or used, from camera stores. Colored acetate or other thin materials do not work as well, as the LED lights are very intense. The following image shows an existing light setup using an orange or amber-colored stained glass filter:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/9d2840c7-1398-4453-adfb-2fcd5ec5c20f.png" style="width:22.00em;height:14.58em;"/></p>
<p>The following screenshot shows the app's analysis of the lighting setup:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/416095d9-f470-4c71-b227-184acd6ed462.png" style="width:29.17em;height:25.58em;"/></p>
<p>Check that the app is reporting the appropriate color for the detected lights. Depending on your particular camera's color rendition, you may find that you'll need to adjust some of the hue and saturation thresholds in the <kbd>detectAndEstimateDistance</kbd> method. You might also want to experiment with adjusting the attributes of the <kbd>SimpleBlobDetector_Params</kbd> object in the initializer to see their effects on the detection of lights and other blobs.</p>
<p>Once we are satisfied that the app is working well with our homemade apparatus, we can step up to a more realistic level of testing!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing The Living Headlights app in a car</h1>
                
            
            
                
<p>When choosing the hardware for a car-based setup, it's important to consider the following questions:</p>
<ul>
<li>Can the car's outlets power the hardware?</li>
<li>Can the hardware fit conveniently in the car?</li>
</ul>
<p>A Raspberry Pi draws power from a 5V supply through its micro USB port. We can satisfy this power requirement by plugging a USB adapter into the car's cigarette lighter and then connecting it to the Pi through a USB to micro USB cable. Make sure that your adapter's voltage is exactly 5V and that its amperage is equal to or greater than the recommended amperage for your Pi model. For example, the official documentation at <a href="https://www.raspberrypi.org/documentation/faqs/">https://www.raspberrypi.org/documentation/faqs/</a> recommends a 5V, 2.5A power supply for Raspberry Pi 3 Model B. The following image shows a setup using a first-generation Raspberry Pi Model A:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/e7383f4d-d5de-431f-90d5-b31e8fb30f74.png" style="width:27.75em;height:18.42em;"/></p>
<p>Normally, the cigarette lighter is a 12V power source, so it can power a variety of devices through an adapter. You could even power a chain of devices, and the Pi need not be the first device in the chain. Later in this section, we will discuss the example of a Pi drawing power from a USB port on a SunFounder LCD display, which in turn draws power from a cigarette lighter receptacle through an adapter.</p>
<p>Standard USB peripherals, such as a webcam, mouse, and keyboard, can draw enough power from Pi's USB ports. Although the Pi only has two USB ports, we can use a USB splitter to power to a webcam, mouse, and keyboard simultaneously. Alternatively, some keyboards have a built-in touchpad that can be used as a mouse. Another option is to simply make do with only using two peripherals at a time and swapping one of them for the third peripheral as needed. In any case, once our app has been started and calibrated (and once we are driving!), we no longer need the keyboard or mouse input.</p>
<p>The webcam should sit against the inside of the car's rear window. The webcam's lens should be as close to the window as possible to reduce the visibility of grime, moisture, and reflections (for example, the reflection of the webcam's <em>on</em> light). If the Raspberry Pi lies just behind the car's front seats, the webcam cable should be able to reach the back window, while the power cable should still reach the USB adapter in the cigarette lighter receptacle. If not, use a longer USB to micro USB cable for the power and, if necessary, position the Pi farther back in the car. Alternatively, use a webcam with a longer cable. The following image shows the suggested positioning of the Pi:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/00e2cfac-3746-40bc-9453-ddc9e96bc302.png" style="width:26.67em;height:17.67em;"/></p>
<p>Similarly, the following image shows the suggested positioning of the camera:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/3639c08b-2644-4899-baf9-d0e412b24505.png" style="width:26.67em;height:17.75em;"/></p>
<p>Now, it's time for the hard part—the display. For video output, the Pi supports HDMI (as found in new TVs and many new monitors). Some older Pi models also support composite RCA (as found in old TVs). For other common connectors, we can use an adapter, such as HDMI to DVI or HDMI to VGA. The Pi also has limited support (through third-party kernel extensions) for video output through DSI or SPI (as found in cellphone displays and prototyping kits).</p>
<p>Do not use a CRT television or monitor in a vehicle or in any environment where it is liable to be bumped. A CRT may implode if the glass is damaged. Instead, use an LCD television or monitor.</p>
<p>A small display is desirable because it can be more conveniently mounted on the dashboard and it consumes less power. For example, the SunFounder Raspberry Pi 10.1 HDMI IPS LCD Monitor requires a 12V, 1A power source. This display includes a USB port that can deliver 5V, 2A of power, which satisfies the recommended power specs for most Pi versions, including Raspberry Pi 2 Model B, but not quite Raspberry Pi 3 Model B. For more information, see the product's page on the SunFounder website, <a href="https://www.sunfounder.com/10-1-inch-hdmi-lcd.html">https://www.sunfounder.com/10-1-inch-hdmi-lcd.html</a>.</p>
<p>Typically, though, a display needs a much higher voltage and wattage than the cigarette lighter can supply. Conveniently, some cars have an electrical outlet that resembles a wall socket, with the standard voltage for the type of socket but a lower maximum wattage. My car has a 110V, 150W, outlet for two-pronged North American plugs (NEMA 1-15P). As seen in the following image, I used an extension cord to convert the two-pronged connection into a three-pronged connection (NEMA 5-15P) that my monitor cables use:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/6f690c8b-30ce-4df9-9e8a-0b4ce99aa708.png" style="width:27.83em;height:18.50em;"/></p>
<p>I tried plugging in three different monitors (one at a time, of course), with the following results:</p>
<ul>
<li><strong>HP Pavilion 25xi (25", <em>1920 x 1080</em>)</strong>: Does not turn on. Presumably requires a higher wattage.</li>
<li><strong>HP w2207 (22", <em>1680 x 1050</em>, 19.8 lbs)</strong>: Does not turn on, but its weight and sturdy hinge make it useful as a flail to beat off hijackers—just in case the rocket launchers fail.</li>
<li><strong>Xplio XP22WD (22", <em>1440 x 900</em>)</strong>: Turns on and works!</li>
</ul>
<p>If you are unable to power a monitor from any of your car's outlets, an alternative is to use a battery block to power the monitor. Another alternative is to use a laptop or netbook as a substitute for the entire Pi-based system.</p>
<p>The XP22WD's ports are seen in the following image. To connect the Pi, I am using an HDMI to DVI cable because the monitor does not have an HDMI port:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/906fccc5-3bee-47a8-a032-4aa8cabd7b9f.png" style="width:30.83em;height:20.50em;"/></p>
<p>Unfortunately, my monitors are too big to mount on a dashboard! However, for the purpose of testing the system on my driveway, placing the monitor in the passenger seat is fine, as follows:</p>
<p class="Normal1 CDPAlignCenter CDPAlign"><img src="img/1b727ae1-5f32-4b70-af26-2bc582d4608c.png" style="width:30.83em;height:20.42em;"/></p>
<p>Voilà! We've proved that a car can power a Pi, peripherals, and a desktop monitor! As soon as the car is turned on, our system boots and runs in exactly the same way as a Linux desktop. We can now launch <kbd>The Living Headlights</kbd> app from the command line, or from an IDE such as Geany. Our app's behavior on Pi should be identical to its behavior on a conventional desktop system, except that on Pi, we will experience a lower frame rate (less <em>frequent</em> frame updates) and greater lag (less <em>timely</em> frame updates). Raspberry Pi has relatively limited processing power; therefore, it will need more time to process each frame, and a greater number of camera frames will be dropped while the software processes an old frame.</p>
<p>Once you get your app running in a car, remember to recalibrate it so that it estimates distances based on the size of real headlights and not the size of a flashlight rig! The most practical way to perform this recalibration would be with two parked cars. One parked car should have its headlights on, and it should be behind the car that contains the Pi. Measure the distance between the parked cars, and use this as the calibration value.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter gave us the opportunity to scale down the complexity of our algorithms to support low-powered hardware. We also played with colorful lights, a homemade toy car, a puzzle of adapters, and a real car!</p>
<p>There is plenty of scope for extending the functionality of <kbd>The Living Headlights</kbd>. For example, we could take an average of multiple reference measurements or store different reference measurements for different colors of lights. We could analyze patterns of flashing, colored lights across multiple frames to judge whether the vehicle behind us is a police car or a road maintenance truck, or is even signaling to turn. We could try to detect the flash of rocket launchers, though testing might be problematic.</p>
<p>The next chapter's project is not something a driver should use, though! In the next chapter, we are going to take a pen-and-paper sketch in one hand and a smartphone in the other as we turn a geometric drawing into a physics simulation!</p>


            

            
        
    </body></html>