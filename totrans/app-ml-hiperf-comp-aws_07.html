<html><head></head><body>
		<div id="_idContainer120">
			<h1 id="_idParaDest-129" class="chapter-number"><a id="_idTextAnchor128"/>7</h1>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/>Deploying Machine Learning Models at Scale</h1>
			<p>In previous chapters, we learned about how to store data, carry out data processing, and perform model training for machine learning applications. After training a machine learning model and validating it using a test dataset, the next task is generally to perform inference on new and unseen data. It is important for any machine learning application that the trained model should generalize well for unseen data to avoid overfitting. In addition, for real-time applications, the model should be able to carry out inference with minimal latency while accessing all the relevant data (both new and stored) needed for the model to do inference. Also, the compute resources associated with the model should be able to scale up or down depending on the number of inference requests, in order to optimize cost while not sacrificing performance and inference requirements for real-time machine <span class="No-Break">learning applications.</span></p>
			<p>For use cases that do not require real-time inference, the trained model should be able to carry out inference on very large datasets with thousands of variables in a reasonable amount of time as well. In addition, in several scenarios, we may not want to go through the effort of managing servers and software packages needed for inference, and instead, focus our effort on developing and improving our machine <span class="No-Break">learning models.</span></p>
			<p>Keeping all these aforementioned factors in mind, AWS provides multiple options for deploying machine learning models to carry out inference on new and unseen data. These options consist of real-time inference, batch inference, and asynchronous inference. In this chapter, we are going to discuss the managed deployment options of machine learning models using Amazon SageMaker, along with various features such as high availability of models, auto-scaling, and <span class="No-Break">blue/green deployments.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Managed deployment <span class="No-Break">on AWS</span></li>
				<li>Choosing the right <span class="No-Break">deployment option</span></li>
				<li><span class="No-Break">Batch inference</span></li>
				<li><span class="No-Break">Real-time inference</span></li>
				<li><span class="No-Break">Asynchronous inference</span></li>
				<li>High availability of <span class="No-Break">model endpoints</span></li>
				<li><span class="No-Break">Blue/green deployments</span></li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Managed deployment on AWS</h1>
			<p>Data scientists and machine learning practitioners working on developing machine learning models to solve business problems are often very focused on model development. Problem formulation and developing an elegant solution, choosing the right algorithm, and training the model so that it provides reliable and accurate results are the main components of the machine learning problem solving cycle that we want our data scientists and data engineers to <span class="No-Break">focus on.</span></p>
			<p>However, once we have a good model, we want to run real-time or batch inference on new data. Deploying the model and then managing it are tasks that often require dedicated engineers and computation resources. This is because, we first need to make sure that we have all the right packages and libraries for the model to work correctly. Then, we also need to decide on the type and amount of compute resources needed for the model to run. In real-time applications, we often end up designing for peak performance requirements, just like provisioning servers for <span class="No-Break">IT projects.</span></p>
			<p>After the model is deployed and is running, we also need to make sure that everything stays working in the manner that we expect it to. Furthermore, in real world scenarios, data scientists often have to manually carry out analysis periodically to detect model or data drift. In the event that either of these drifts are detected, the data scientists go through the entire cycle of exploratory analysis, feature engineering, model development, model training, hyperparameter optimization, model evaluation, and model deployment again. All these tasks consume a lot of effort and resources and due to this reason, many organizations have moved to automating these processes using <strong class="bold">machine learning operations</strong> (<strong class="bold">MLOps</strong>) workflows and managed model deployment options that scale well with <span class="No-Break">varying workloads.</span></p>
			<p>Amazon SageMaker offers multiple fully managed model deployment options. In this section, we give an overview of these managed deployment options along with their benefits and then discuss a few of these deployment options in detail in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/>Amazon SageMaker managed model deployment options</h2>
			<p>Amazon SageMaker offers the following managed deployment <span class="No-Break">model options:</span></p>
			<ul>
				<li><strong class="bold">Batch Transform</strong>: SageMaker Batch Transform is used to carry out inference on large datasets. There is no persistent endpoint in this case. This method is commonly used to carry out inference in a non-real-time machine learning use case requiring offline inference on <span class="No-Break">larger datasets.</span></li>
				<li><strong class="bold">Real-time endpoint</strong>: A SageMaker real-time endpoint is for use cases where a persistent machine learning model endpoint is needed, which carries out inference on a few data samples in <span class="No-Break">real time.</span></li>
				<li><strong class="bold">Asynchronous inference</strong>: Amazon SageMaker Asynchronous Inference deploys an asynchronous endpoint for carrying out inference on large payloads (up to 1 GB) with large processing times and <span class="No-Break">low latency.</span></li>
				<li><strong class="bold">Serverless Inference</strong>: In all the previous methods, the user is required to select the compute instance types for inference. Amazon SageMaker Serverless Inference automatically chooses the server type and scales up and down based on the load on the endpoint. It is often useful for applications that have unpredictable <span class="No-Break">traffic patterns.</span></li>
			</ul>
			<p>Letâ€™s explore the variety of available compute <span class="No-Break">resources next.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>The variety of compute resources available</h2>
			<p>To carry out inference, there are a wide variety of computation instances available. At the time of writing, approximately 70+ instances are available for carrying out machine learning inference. These instances have varying levels of computation power and memory available to serve different use cases. There is also the option of using <strong class="bold">graphical processing units</strong> (<strong class="bold">GPUs</strong>) for inference. In addition, SageMaker also supports Inf1 instances for high-performance and low-cost inference. These options make SageMaker model deployment and inference highly versatile and suitable for a variety of machine learning <span class="No-Break">use cases.</span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Cost-effective model deployment</h2>
			<p>Amazon SageMaker has various options for optimizing model deployment cost. There are multi-model endpoints, where multiple models can share a container. This helps with reducing hosting costs since endpoint utilization is increased due to multiple models sharing the same endpoint. In addition, it also enables time sharing of memory resources across different models. SageMaker also has the option of building and deploying multi-container endpoints. Furthermore, we can attach scaling policies to our endpoints to allocate more compute resources when traffic increases, and shut down instances when traffic decreases in order to save <span class="No-Break">on costs.</span></p>
			<p>Another cost-effective option for model deployment is SageMaker Serverless Inference. Serverless Inference utilizes AWS Lambda to scale up and down compute resources as traffic increases or decreases. It is especially useful for scenarios with unpredictable <span class="No-Break">traffic patterns.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>Blue/green deployments</h2>
			<p>SageMaker automatically uses blue/green deployment whenever we update a SageMaker model endpoint. In blue/green deployment, SageMaker uses a new fleet of instances to deploy the updated endpoints and then shifts the traffic to the updated endpoint from the old fleet to the new one. Amazon SageMaker offers the following traffic-shifting strategies for <span class="No-Break">blue/green deployments:</span></p>
			<ul>
				<li>All at once <span class="No-Break">traffic shifting</span></li>
				<li>Canary <span class="No-Break">traffic shifting</span></li>
				<li>Linear <span class="No-Break">traffic shifting</span></li>
			</ul>
			<p>We will discuss these traffic patterns in more detail later in <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor135"/>Inference recommender</h2>
			<p>With Amazon SageMaker Inference Recommender, we can automatically get recommendations on the type of compute instance to use for deploying our model endpoint. It gives us instance recommendations by load testing various instances and outputs inference costs, along with throughput and latency, for the tested instance types. This helps us decide on the type of instance to use for deploying our <span class="No-Break">model endpoint.</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>MLOps integration</h2>
			<p>Using Amazon SageMaker, we can easily build machine learning workflows that integrate with <strong class="bold">continuous integration and continuous delivery</strong> (<strong class="bold">CI/CD</strong>) pipelines. These workflows can be used to automate the entire machine learning life cycle, including data labeling, data processing and feature engineering, model training and registry, post-processing, endpoint deployment, and model monitoring for data and model drift monitoring. For model deployment, these workflows can be used to automate the process of doing batch inference, as well as pushing model endpoints from development to staging to <span class="No-Break">production environments.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/>Model registry</h2>
			<p>Amazon SageMaker provides the capability to register and catalog machine learning models with the SageMaker model registry. Using the model registry, we can include different versions of a trained model in a model package group. This way, whenever we train and register a model, it is added as a new version to the model package group. In addition, using the model registry, we can also associate metadata and training metrics to a machine learning model, approve or reject a model, and if approved, move the models to production. These features of the model registry facilitate the building of CI/CD pipelines needed for automating machine <span class="No-Break">learning workflows.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Elastic inference</h2>
			<p>For machine learning use cases that require very high throughput and low latency, we often end up using GPU machines, thereby increasing the cost of inference significantly. Using Amazon SageMaker, we can add elastic inference to our endpoints. Elastic inference provides inference acceleration to our endpoint, by attaching just the right amount of GPU-powered inference acceleration to any SageMaker instance type. This helps significantly with latency and throughput, while also achieving it at a much lower cost compared to using GPU instances <span class="No-Break">for inference.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/>Deployment on edge devices</h2>
			<p>Many machine learning use cases require models to run on edge devices such as mobile devices, cameras, and specialized devices. These devices often have low compute resources, memory, and storage. Furthermore, deploying, managing, and monitoring machine learning models on a fleet of devices is a difficult task because of the variability in device hardware and operating systems. With Amazon SageMaker Edge Manager, machine learning models can be deployed, monitored, and managed on a fleet of devices with different hardware and software configurations. SageMaker Edge Manager uses SageMaker Neo to compile machine learning models and packages these compiled models to be deployed on edge devices. In addition, we can also sample the data used by the model on the edge devices and send them to the cloud to carry out analysis to determine quality issues such as data and <span class="No-Break">model drift.</span></p>
			<p>Now, letâ€™s discuss the various model deployment options on AWS in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Choosing the right deployment option</h1>
			<p>As mentioned in the previous section, AWS has multiple model deployment and inference options. It can get confusing and overwhelming sometimes to decide on the right option for model deployment. The decision to select the right model deployment option really depends on the use case parameters and requirements. A few important factors to consider while deciding on deployment options are listed <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Do we have an application that needs a real-time, persistent endpoint to carry out on-demand inference on new data in real time and very quickly with low latency and <span class="No-Break">high availability?</span></li>
				<li>Can our application wait for a minute or two for the compute resources to come online before getting the <span class="No-Break">inference results?</span></li>
				<li>Do we have a use case where we do not need results in near real time? Can we do inference on a batch of data once a day/week or on an <span class="No-Break">as-needed basis?</span></li>
				<li>Do we have an unpredictable and non-uniform traffic pattern requiring inference? Do we need to scale up and down our compute resources based on <span class="No-Break">the traffic?</span></li>
				<li>How big is the data (number of data points/rows) that we are trying to do <span class="No-Break">inference on?</span></li>
				<li>Do we need dedicated resources all the time to carry out inference or can we follow a <span class="No-Break">serverless approach?</span></li>
				<li>Can we pack in multiple models in a single endpoint to save <span class="No-Break">on cost?</span></li>
				<li>Do we need to have an inference pipeline consisting of multiple models, pre-processing steps, and <span class="No-Break">post-processing steps?</span></li>
			</ul>
			<p>In the following subsections, we will discuss when to pick the different types of model deployment and inference options provided by Amazon SageMaker, while addressing the previously mentioned questions. We will also provide examples of typical example use cases for each of the model <span class="No-Break">deployment options.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/>Using batch inference</h2>
			<p><strong class="bold">Amazon SageMaker Batch Transform</strong> is used when there is no need for a persistent, real-time endpoint and inference can be done on large batches of data. The following examples illustrate the use of SageMaker <span class="No-Break">Batch Transform:</span></p>
			<ul>
				<li><strong class="bold">Predictive maintenance</strong>: In a manufacturing plant, sensor and machine data for various components could be collected the entire day. For such a use case, there is no need for real-time or asynchronous endpoints. At night, machine learning models can be used for predicting whether a component is about to fail, or whether a part of the machinery needs maintenance. These models would run on large batches of data and carry out inference using SageMaker Batch Transform. The results from these models could then be used to make and execute a <span class="No-Break">maintenance schedule.</span></li>
				<li><strong class="bold">Home prices prediction</strong>: Real estate companies collect data for a few days (and sometimes weeks) before coming out with new home prices and market direction predictions. These models do not need real-time or asynchronous endpoints as they need to be run only after a few days or weeks and on large amounts of data. For such use cases, SageMaker Batch Transform is the ideal option for inference. SageMaker Batch Transform jobs could run on a fixed interval in a machine learning pipeline on new and historical data, carrying out inference to predict home price adjustments and market direction by localities. These results can, in turn, then be used to determine if the machine learning models need to <span class="No-Break">be retrained.</span></li>
			</ul>
			<p>We will cover batch, real-time, and asynchronous inference options in Amazon SageMaker in detail in the later sections of <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Using real-time endpoints</h2>
			<p>Amazon SageMaker real-time endpoints should be the choice for model deployment when there is a need for a real-time persistent model endpoint, doing predictions with low latency as new data arrives. Real-time endpoints are fully managed by Amazon SageMaker and can be deployed as multi-model and multi-container endpoints. The following are some example use cases for <span class="No-Break">real-time endpoints:</span></p>
			<ul>
				<li><strong class="bold">Fraudulent transaction</strong>: A customer uses a credit card to purchase an item online or physically in a retail store. This financial transaction can be carried out by the actual owner of the credit card or it can be a stolen credit card as well. The financial institution needs to make the decision in real time whether to approve the transaction or not. In such a scenario, a machine learning model can be deployed as a real-time endpoint. This model could use customerâ€™s demographic data and history of purchases from historical data tables, while also using some data from the current transaction, such as IP address and web browser parameters (if it is an online transaction), or store location and image and/or video from a camera in real time (if it is a physical transaction), to classify whether the transaction is fraudulent or not. This decision can then be used by the financial institution to either approve or reject the transaction, or contact the customer for notification, manual authentication, <span class="No-Break">and approval.</span></li>
				<li><strong class="bold">Real-time sentiment analysis</strong>: A customer is having a chat or phone conversation with a customer care agent. A machine learning model is analyzing the chat or transcribed text from voice conversation in real time to decide on the sentiment that the customer is showing. Based on the sentiment, if the customer is unhappy, the agent can offer various promotions or escalate the case to a supervisor before things get out of hand. This machine learning model should be deployed using a real-time endpoint so that the sentiment can be correctly determined without any lag <span class="No-Break">or delay.</span></li>
				<li><strong class="bold">Quality assurance</strong>: In a manufacturing plant, products are being assembled on an assembly line and there needs to be strict quality control to remove defective products as soon as possible. This is again an application where real-time inference from a machine learning model classifying the objects as defective or normal using live images or video feed will <span class="No-Break">be useful.</span></li>
			</ul>
			<p>Similar to real-time endpoints, we also have the option of using asynchronous endpoints, which we will learn about in the <span class="No-Break">following section.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Using asynchronous inference</h2>
			<p>Amazon SageMaker Asynchronous Inference endpoints are very similar to real-time endpoints. Asynchronous endpoints can queue inference requests and are the deployment option of choice when near real-time latency is needed, while also processing large workloads. The following example illustrates a potential asynchronous endpoints <span class="No-Break">use case.</span></p>
			<p><strong class="bold">Train track inspection</strong>: Several trains are running on their everyday routes with attached cameras that take images and videos of train tracks and switches for defect detection. These trains do not have a high bandwidth available to transmit this data in real time for inference. When these trains dock at a station, a large number of images and videos could be sent to a SageMaker Asynchronous Inference endpoint for inference to find out whether everything is normal, or if there are any defects present anywhere on the track or switches. The compute instance associated with the asynchronous endpoint will start as soon as it receives data from any of the trains, carrying out inference on the data, then shutting down once all the data has been processed. This will help with the reduction in costs compared to a <span class="No-Break">real-time endpoint.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor144"/>Batch inference</h1>
			<p>For carrying out batch inference on datasets, we can use SageMaker Batch Transform. It should be used for inference when there is no need for a real-time persistent deployed machine learning model. Batch Transform is also useful when the dataset for inference is large or if we need to carry out heavy preprocessing on the dataset. For example, removing bias or noise from the data, converting speech data to text, and filtering and normalization of images and <span class="No-Break">video data.</span></p>
			<p>We can pass input data to SageMaker Batch Transform in either one file or using multiple files. For tabular data in one file, each row in the file is interpreted as one data record. If we have selected more than one instance for carrying out the batch transform job, SageMaker distributes the input files to different instances for batch transform jobs. Individual data files can also be split into multiple mini-batches and batch transform on can be carried out these mini-batches in parallel on separate instances. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows a simplified typical architecture example of SageMaker <span class="No-Break">Batch Transform:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B18493_07_001.jpg" alt="Figure 7.1 â€“ Example architecture for Amazon SageMaker Batch Transform"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 â€“ Example architecture for Amazon SageMaker Batch Transform</p>
			<p>As shown in the figure, data is read from <strong class="bold">Amazon S3</strong>. Preprocessing and feature engineering is carried out on this data using <strong class="bold">Amazon SageMaker Processing</strong> in order to transform the data in the right format that the machine learning model expects. A trained machine learning model is then used by <strong class="bold">Amazon SageMaker Batch Transform</strong> to carry out batch inference. The results are then written back to <strong class="bold">Amazon S3</strong>. The machine learning model used by Batch Transform could either be trained using Amazon SageMaker or can be a model trained outside of Amazon SageMaker. The two main steps in Batch Transform are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Creating a <span class="No-Break">transformer object.</span></li>
				<li>Creating a batch transform job for carrying <span class="No-Break">out inference.</span></li>
			</ol>
			<p>These steps are described in the <span class="No-Break">following subsections.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Creating a transformer object</h2>
			<p>We first need to create an object of the <strong class="source-inline">Transformer</strong> class in order to run a SageMaker batch transform job. While creating the <strong class="source-inline">Transformer</strong> object, we can specify the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">model_name</strong>: This is the name of the machine learning model that we are going to use for inference. This can also be a built-in SageMaker model, for which we can directly use the <strong class="source-inline">transformer</strong> method of the <span class="No-Break">built-in estimator.</span></li>
				<li><strong class="source-inline">instance_count</strong>: The number of EC2 instances that we are going to use to run our batch <span class="No-Break">transform job.</span></li>
				<li><strong class="source-inline">instance_type</strong>: The type of EC2 instances that we can use. A large variety of instances are available to be used for batch transform jobs. The choice of instance depends on our use caseâ€™s compute and memory requirements, as well as the data type <span class="No-Break">and size.</span></li>
			</ul>
			<p>In addition, we can also specify several other parameters such as batch strategy, and output path. The complete list of parameters can be found on the documentation page in the <em class="italic">References</em> section. In the following example, we used SageMakerâ€™s built-in XGBoost container. For running batch transformers for your own containers/models or frameworks, such as PyTorch and TensorFlow, the container images <span class="No-Break">may vary.</span></p>
			<p><em class="italic">Figures 7.2</em> â€“ <em class="italic">7.8</em> show an example of SageMakerâ€™s XGBoost model being fit on our training data and then a transformer object being created for this training model. We specified that the transform job should run on one instance of type <strong class="source-inline">ml.m4.xlarge</strong> and should expect <span class="No-Break"><strong class="source-inline">text/csv</strong></span><span class="No-Break"> data:</span></p>
			<ol>
				<li value="1">As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, we can specify the various packages and SageMaker parameters needed to run the code in <span class="No-Break">the example.</span></li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B18493_07_002.jpg" alt="Figure 7.2 â€“ Setting up the packages and bucket in SageMaker and downloading the dataset to be used for model training and inference"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 â€“ Setting up the packages and bucket in SageMaker and downloading the dataset to be used for model training and inference</p>
			<ol>
				<li value="2">As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, we can read the data and carry out one hot encoding on the <span class="No-Break">categorical variables.</span></li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B18493_07_003.jpg" alt="Figure 7.3 â€“ Doing one hot encoding on categorical variables and displaying the results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 â€“ Doing one hot encoding on categorical variables and displaying the results</p>
			<ol>
				<li value="3"><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> shows the data being split into training, validation, and testing partitions to be used during the model <span class="No-Break">training process.</span></li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B18493_07_004.jpg" alt="Figure 7.4 â€“ Splitting the data into train, validation, and test sets for model training and testing"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 â€“ Splitting the data into train, validation, and test sets for model training and testing</p>
			<ol>
				<li value="4">As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em>, we can rearrange the columns in the data table in the order that the machine learning model (XGBoost) expects it to be. Furthermore, we can also upload the training and validation data files to an S3 bucket for the model <span class="No-Break">training step.</span></li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B18493_07_005.jpg" alt="Figure 7.5 â€“ Reorganizing the data and uploading to S3 bucket for training"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 â€“ Reorganizing the data and uploading to S3 bucket for training</p>
			<ol>
				<li value="5"><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> shows that we are using SageMakerâ€™s XGBoost container for training our model. It also specifies the data channels for training and validating <span class="No-Break">the model.</span></li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18493_07_006.jpg" alt="Figure 7.6 â€“ Specifying the container for model training along with &#10;training and validation data path in S3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 â€“ Specifying the container for model training along with training and validation data path in S3</p>
			<p>As shown in <em class="italic">Figure 7.7</em>, we need to define the estimator, the instance type, and instance count, and set various hyperparameters needed by the XGBoost model. We will <a id="_idIndexMarker589"/>also start the training job by calling the <strong class="source-inline">fit</strong> method.</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B18493_07_007.jpg" alt="Figure 7.7 â€“ Defining the SageMaker estimator for training and setting up the hyperparameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 â€“ Defining the SageMaker estimator for training and setting up the hyperparameters</p>
			<p>Next, letâ€™s create a batch <span class="No-Break">transform job.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/>Creating a batch transform job for carrying out inference</h2>
			<p>After creating <a id="_idIndexMarker590"/>the transformer object, we <a id="_idIndexMarker591"/>need to create a batch transform job. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.8</em> shows an example of starting a batch transform job using the <strong class="source-inline">transform</strong> method call of the batch transformer object. In this transform call, we will specify the location of data in Amazon S3 on which we want to carry out batch inference. In addition, we will also specify the content type of the data (<strong class="source-inline">text/csv</strong>, in this case), and how the records are split in the data file (each line containing one record, in <span class="No-Break">this case).</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B18493_07_008.jpg" alt="Figure 7.8 â€“ Creating a transformer object, preparing data for batch inference, and starting the batch transform job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 â€“ Creating a transformer object, preparing data for batch inference, and starting the batch transform job</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.9</em> shows an <a id="_idIndexMarker592"/>example of reading <a id="_idIndexMarker593"/>the results from S3 and then plotting the results (actual <span class="No-Break">versus predictions):</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B18493_07_009.jpg" alt="Figure 7.9 â€“ Creating a helper function for reading the results of batch transform, and plotting the results (actual versus predictions)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 â€“ Creating a helper function for reading the results of batch transform, and plotting the results (actual versus predictions)</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.10</em> shows the <span class="No-Break">resulting plot:</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B18493_07_010.jpg" alt="Figure 7.10 â€“ Plot of actual versus prediction ï»¿Ringsï»¿ values"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 â€“ Plot of actual versus prediction Rings values</p>
			<p>The code <a id="_idIndexMarker594"/>and steps discussed in this section <a id="_idIndexMarker595"/>outline the process of training a machine learning model and then carrying out batch inference on the data using <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/>Optimizing a batch transform job</h2>
			<p>SageMaker <a id="_idIndexMarker596"/>Batch Transform also gives us the option <a id="_idIndexMarker597"/>of optimizing the transform job using a few hyperparameters, as <span class="No-Break">described here:</span></p>
			<ul>
				<li><strong class="source-inline">max_concurrent_transforms</strong>: The maximum number of HTTP requests that <a id="_idIndexMarker598"/>can be made to each batch transform container at any given time. To get the best performance, this value should be equal to the number of compute workers that we are using to run our batch <span class="No-Break">transform job.</span></li>
				<li><strong class="source-inline">max_payload</strong>: This value specifies the maximum size (in MB) of the payload in <a id="_idIndexMarker599"/>a single HTTP request sent to the batch transform <span class="No-Break">for inference.</span></li>
				<li><strong class="source-inline">strategy</strong>: This <a id="_idIndexMarker600"/>specifies the strategy whether we want to have just one record or multiple records in <span class="No-Break">a batch.</span></li>
			</ul>
			<p>SageMaker <a id="_idIndexMarker601"/>Batch Transform is a very useful option to <a id="_idIndexMarker602"/>carry out inference on large datasets for use cases that do not require real-time latency and high throughput. The execution of SageMaker Batch Transform can be carried out by using an MLOps workflow, which can be triggered whenever there is new data on which inference needs to be carried out. Automated reports can then be generated using the batch transform <span class="No-Break">job results.</span></p>
			<p>Next, we will learn about deploying a real-time endpoint for making predictions on data using Amazon SageMakerâ€™s real-time <span class="No-Break">inference option.</span></p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>Real-time inference</h1>
			<p>As discussed earlier in this chapter, the need for real-time inference arises when we need results <a id="_idIndexMarker603"/>with very low latency. Several day-to-day use cases are examples of using real-time inference from machine learning models, such as face detection, fraud detection, defect and anomaly detection, and sentiment analysis in live chats. Real-time inference in Amazon SageMaker can be carried out by deploying our model to the SageMaker hosting services as a real-time endpoint. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em> shows a typical SageMaker machine learning workflow of using a <span class="No-Break">real-time endpoint.</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B18493_07_011.jpg" alt="Figure 7.11 â€“ Example architecture of a SageMaker real-time endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 â€“ Example architecture of a SageMaker real-time endpoint</p>
			<p>In this figure, we first read our data from an Amazon S3 bucket. Data preprocessing and feature engineering are carried out on this data using Amazon SageMaker Processing. A machine <a id="_idIndexMarker604"/>learning model is then trained on the processed data, followed by results evaluation and post-processing (if any). After that, the model is deployed as a real-time endpoint for carrying out inference on new data in real time with low latency. Also shown in the figure is SageMaker Model Monitor, which is attached to the endpoint in order to detect data and concept drift on new data that is sent to the endpoint for inference. SageMaker also provides the option of registering our machine learning model with the SageMaker Model Registry, for various purposes, such as cataloging, versioning, and <span class="No-Break">automating deployment.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Hosting a machine learning model as a real-time endpoint</h2>
			<p>Amazon SageMaker provides us with many options to host a model or multiple models as <a id="_idIndexMarker605"/>real-time <a id="_idIndexMarker606"/>endpoints. We can either use SageMaker Python SDK, the AWS SDK for Python (Boto3), the SageMaker console, or <a id="_idIndexMarker607"/>the AWS <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) to host our models. Furthermore, these endpoints can also host a single model, multiple models in one container in a single endpoint, and multiple models using different containers in a single endpoint. In addition, a single endpoint can also host models containing preprocessing logic as a serial inference pipeline. We will go through these multiple options in the <span class="No-Break">following subsections.</span></p>
			<h3>Single model</h3>
			<p>As mentioned <a id="_idIndexMarker608"/>in the preceding section, we can host a model endpoint using multiple options. Here, we will show you how to host an endpoint containing a single model using the Amazon SageMaker SDK. There are two steps involved in creating a single model endpoint using the Amazon SageMaker SDK, as <span class="No-Break">described here:</span></p>
			<ol>
				<li value="1"><strong class="bold">Creating a model object</strong>: We need a model object to deploy as an endpoint. The <a id="_idIndexMarker609"/>first step to create a real-time endpoint is to use the <strong class="source-inline">Model</strong> class to create a model object that can be deployed as a HTTPS endpoint. We can also use the model trained in SageMaker. For example, the XGBoost model trained in the example shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Creating the endpoint</strong>: The next step is to use the model objectâ€™s <strong class="source-inline">deploy()</strong> method <a id="_idIndexMarker610"/>to create an HTTPS endpoint. The <strong class="source-inline">deploy</strong> method requires the instance type as well as an initial instance count to deploy the model with. This is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B18493_07_012.jpg" alt="Figure 7.12 â€“ Calling the deploy method of the XGBoost estimator we trained earlier to deploy our model on a single instance of type ml.m4.xlarge"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 â€“ Calling the deploy method of the XGBoost estimator we trained earlier to deploy our model on a single instance of type ml.m4.xlarge</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.13</em> show a custom inference function to serialize the test data, and send it to the real-time endpoint <span class="No-Break">for inference:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B18493_07_013.jpg" alt=" Figure 7.13 â€“ Serializing the data to be sent to the real-time endpoint. Also, creating a helper function to carry out inference using the endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 7.13 â€“ Serializing the data to be sent to the real-time endpoint. Also, creating a helper function to carry out inference using the endpoint</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.14</em> displays the <a id="_idIndexMarker611"/>inference results for a few records along with the actual values of our <span class="No-Break">target variableâ€”</span><span class="No-Break"><strong class="source-inline">Rings</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B18493_07_014.jpg" alt="Figure 7.14 â€“ Showing a few prediction results versus actual values (Rings)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 â€“ Showing a few prediction results versus actual values (Rings)</p>
			<p>The endpoint will continue incurring costs as long as it is not deleted. Therefore, we should use real-time endpoints only when we have a use case in which we need inference <a id="_idIndexMarker612"/>results in real time. For use cases where we can do inference in batches, we should use SageMaker <span class="No-Break">Batch Transform.</span></p>
			<h3>Multiple models</h3>
			<p>For hosting multiple models in a single endpoint, we can use SageMaker multi-model endpoints. These endpoints can be used to also host multiple variants of the same model. Multi-model endpoints are a very cost-effective method of saving our inference cost <a id="_idIndexMarker613"/>for real-time endpoints since the endpoint utilization is generally more when we use multi-model endpoints. We can use business logic to decide on the model to use <span class="No-Break">for inference.</span></p>
			<p>With multi-model endpoints, memory resources across models are also shared. This is very useful when our models are comparable in size and latency. If there are models that have significantly different latency requirements or transactions per second, then single model endpoints for the models are recommended. We can create multi-model endpoints using either the SageMaker console or the AWS SDK for Python (Boto3). We can follow similar steps as those for the creation of a single-model endpoint to create a multi-model endpoint, with a few differences. The steps for Boto3 are <span class="No-Break">as follows:</span></p>
			<ol>
				<li value="1">First, we need a container supporting multi-model <span class="No-Break">endpoints deployment.</span></li>
				<li>Then, we need to create a model that uses this container using Boto3 <span class="No-Break">SageMaker client.</span></li>
				<li>For multi-model endpoints, we also need to create an endpoint configuration, specifying instance types and <span class="No-Break">initial counts.</span></li>
				<li>Finally, we need to create the endpoint using the <strong class="source-inline">create_endpoint()</strong>API call of the Boto3 <span class="No-Break">SageMaker client.</span></li>
			</ol>
			<p>While invoking a multi-model endpoint, we also need to pass a target model parameter to specify the model that we want to use for inference with the data in <span class="No-Break">the request.</span></p>
			<h3>Multiple containers</h3>
			<p>We can also deploy models that use different containers (such as different frameworks) as multi-container endpoints. These containers can be run individually or can also be run in a <a id="_idIndexMarker614"/>sequence as an inference pipeline. Multi-container endpoints also help improve endpoint utilization efficiency, hence cutting down on the cost associated with real-time endpoints. Multi-container endpoints can be created using Boto3. The process to create a multi-container endpoint is very similar to creating multi-model and single-model endpoints. First, we need to create a model with multiple containers as a parameter, followed by creating an endpoint configuration, and finally creating <span class="No-Break">the endpoint.</span></p>
			<h3>Inference pipelines</h3>
			<p>We can also host real-time endpoints consisting of two to five containers as an inference pipeline <a id="_idIndexMarker615"/>behind a single endpoint. Each of these containers can be a pretrained SageMaker built-in algorithm, our custom algorithm, preprocessing code, predictions, or postprocessing code. All the containers in the inference pipeline function in a sequential manner. The first container processes the initial HTTP request. The response from the first container is sent as a request to the second container, and so on. The response from the final container is sent by SageMaker to the client. An inference pipeline can be considered as a single model that can be hosted behind a single endpoint or can also be used to run batch transform jobs. Since all the containers in an inference pipeline are running on the same EC2 instance, there is very low latency in communication between <span class="No-Break">the containers.</span></p>
			<h3>Monitoring deployed models</h3>
			<p>After deploying a model into production, data scientists and machine learning engineers have <a id="_idIndexMarker616"/>to continuously check on the modelâ€™s quality. This is because with time, the modelâ€™s quality may drift and it may start to predict incorrectly. This may occur because of several reasons, such as a change in one or more variablesâ€™ distribution in the dataset, the introduction of bias in the dataset with time, or some other unknown process or parameter <span class="No-Break">being changed.</span></p>
			<p>Traditionally, data scientists often run their analysis every few weeks to check if there has been any change in the data or model quality, and if there is, they go through the entire process of feature engineering, model training, and deployment again. With SageMaker Model Monitor, these steps can be automated. We can set up alarms to detect if there is any drift in data quality, model quality, bias drift in the model and feature distribution drift, and then take corrective actions such as fixing quality issues and <span class="No-Break">retraining models.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.15</em> shows an example of using SageMaker Model Monitor with an endpoint (real-time <span class="No-Break">or asynchronous):</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18493_07_015.jpg" alt="Figure 7.15 â€“ SageMaker Model Monitor workflow for a deployed endpoint (real-time or asynchronous)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 â€“ SageMaker Model Monitor workflow for a deployed endpoint (real-time or asynchronous)</p>
			<p>First, we <a id="_idIndexMarker617"/>need to enable Model Monitor on our endpoint, either at the time of creation or later. In addition, we also need to run a baseline processing job. This processing job analyzes the data and creates statistics and constraints for the baseline data (generally the same dataset with which the model has been trained or validated on). We also need to enable data capture on our SageMaker endpoint to be able to capture the new data along with inference results. Another processing job is then run on fixed intervals to create new statistics and constraints, compares them with the baseline statistics and constraints, and then configures alarms using Amazon CloudWatch metrics in case there is drift in any of the metrics we <span class="No-Break">are analyzing.</span></p>
			<p>In case of a violation in any of the metrics, alarms are generated and these can be sent to a data scientist or machine learning engineer for further analysis and model retraining if needed. Alternatively, we can also use these alarms to trigger a workflow (MLOps) to retrain our machine learning model. Model Monitor is a very valuable tool for <a id="_idIndexMarker618"/>data scientists. It can cut down on tedious manual processes for detecting bias and drift in data and model as time progresses after deploying <span class="No-Break">a model.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Asynchronous inference</h1>
			<p>SageMaker real-time endpoints are suitable for machine learning use cases that have very low <a id="_idIndexMarker619"/>latency inference requirements (up to 60 seconds), along with the data size for inference not being large (maximum 6 MB). On the other hand, batch transforms are suitable for offline inference on very large datasets. Asynchronous inference is another relatively new inference option in SageMaker that can process data up to 1 GB and can take up to 15 minutes in processing inference requests. Hence, they are useful for use cases that do not have very low latency <span class="No-Break">inference requirements.</span></p>
			<p>Asynchronous endpoints have several similarities to real-time endpoints. To create asynchronous endpoints, like with real-time endpoints, we need to carry out the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Create <span class="No-Break">a model.</span></li>
				<li>Create an endpoint configuration for the asynchronous endpoint. There are some additional parameters for <span class="No-Break">asynchronous endpoints.</span></li>
				<li>Create the <span class="No-Break">asynchronous endpoint.</span></li>
			</ol>
			<p>Asynchronous endpoints also have differences when compared to real-time endpoints, as <span class="No-Break">outlined here:</span></p>
			<ul>
				<li>One main difference from real-time endpoints is that we can scale endpoint instances down to zero when there are no inference requests. This can cut down on the costs associated with having an endpoint <span class="No-Break">for inference.</span></li>
				<li>Another key difference compared to a real-time endpoint is that instead of passing the payload in line with the request for inference, we upload the data to an Amazon S3 location, and pass on the S3 URI along with the request. Internally, SageMaker keeps a queue of these requests and processes them in the order that they were received. Just like real-time endpoints, we can also do monitoring on the asynchronous endpoint in order to detect model and data drift, as well as <span class="No-Break">new bias.</span></li>
			</ul>
			<p>The code to <a id="_idIndexMarker620"/>show a SageMaker asynchronous endpoint is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>, using the same model that we created in the batch <span class="No-Break">transform example.</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18493_07_016.jpg" alt="Figure 7.16 â€“ Creating an asynchronous endpoint configuration, followed by the creation of the asynchronous endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 â€“ Creating an asynchronous endpoint configuration, followed by the creation of the asynchronous endpoint</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17</em> shows a sample of results for carrying out asynchronous inference on Abalone data used for the batch transform and real-time endpoints examples in <span class="No-Break">previous sections.</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B18493_07_017.jpg" alt="Figure 7.17 â€“ Serializing the inference request and calling the asynchronous endpoint to carry out inference on the data."/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 â€“ Serializing the inference request and calling the asynchronous endpoint to carry out inference on the data.</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.18</em> shows the <a id="_idIndexMarker621"/>actual and predicted results for a few <span class="No-Break">data points.</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B18493_07_018.jpg" alt="Figure 7.18 â€“ Showing a few predicted results versus the actual values (ï»¿Ringsï»¿) using the asynchronous endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 â€“ Showing a few predicted results versus the actual values (Rings) using the asynchronous endpoint</p>
			<p>In the following section, we will look into the high availability and fault tolerance capabilities of <span class="No-Break">SageMaker endpoints.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>The high availability of model endpoints</h1>
			<p>Amazon <a id="_idIndexMarker622"/>SageMaker provides fault tolerance and high availability of the deployed endpoints. In this section, we will discuss various features and options of AWS cloud infrastructure and Amazon SageMaker, that we can use to ensure that our endpoints are fault-tolerant, resilient, and <span class="No-Break">highly available.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor152"/>Deployment on multiple instances</h2>
			<p>SageMaker gives us the option of deploying our endpoints on multiple instances. This protects <a id="_idIndexMarker623"/>from instance failures. If one instance goes down, then other instances can still serve the inference requests. In addition, if our endpoints are deployed on multiple instances and an availability zone outage occurs or an instance fails, SageMaker automatically tries to distribute our instances across different availability zones, thereby improving the resiliency of our endpoints. It is also a good practice to deploy our endpoints using small instance types spread across different <span class="No-Break">availability zones.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>Endpoints autoscaling</h2>
			<p>Oftentimes, we design our online applications for peak load and traffic. This is also true for machine <a id="_idIndexMarker624"/>learning-based applications, where we need hosted endpoints to carry out inference in real time or near real time. In such a scenario, we generally deploy models using the maximum number of instances in order to serve the peak workload <span class="No-Break">and traffic.</span></p>
			<p>If we donâ€™t do this, then our application may start timing out when there are more inference requests than the instances can handle in a combined fashion. This approach results in either the wastage of compute resources or interruptions in the service of our machine <span class="No-Break">learning applications.</span></p>
			<p>To avoid this kind of scenario, Amazon SageMaker lets us configure our endpoints with an autoscaling policy. Using the autoscaling policy, the number of instances on which our endpoint is deployed increases as traffic increases, and decreases as traffic decreases. This helps not only with the high availability of our inference endpoints but also helps in reducing the cost significantly. We can enable autoscaling for a model using either the SageMaker console, the AWS CLI, or the Application Auto <span class="No-Break">Scaling API.</span></p>
			<p>In order to apply autoscaling, we need an autoscaling policy. The autoscaling policy uses Amazon CloudWatch metrics and target values assigned by us to decide when to scale the instances up or down. We also need to define the minimum and maximum number of instances that the endpoint can be deployed on. Other components of the autoscaling policy include the required permissions to carry out autoscaling, a service-linked IAM role, and a cool-down period to wait after a scaling activity before starting the next <span class="No-Break">scaling activity.</span></p>
			<p><em class="italic">Figures 7.19</em> â€“ <em class="italic">7.22</em> show autoscaling being configured on an asynchronous endpoint using <a id="_idIndexMarker625"/>the Amazon <span class="No-Break">SageMaker console:</span></p>
			<ol>
				<li value="1"><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.19</em> shows that initially the endpoint is just deployed on a <span class="No-Break">single instance:</span></li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B18493_07_019.jpg" alt="Figure 7.19 â€“ The endpoint run time settings showing the endpoint being deployed on an instance and autoscaling not being used"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 â€“ The endpoint run time settings showing the endpoint being deployed on an instance and autoscaling not being used</p>
			<p>After clicking on the <strong class="bold">Configure auto scaling</strong> button, your screen should look like those shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.20</em> and <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.21</em></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">As seen in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.20</em>, we need to update the minimum and maximum instance counts to <strong class="source-inline">2</strong> and <span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">, respectively.</span></li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B18493_07_020.jpg" alt="Figure 7.20 â€“ Configuring autoscaling on the endpoint to use 2 â€“ 10 instances, depending on traffic (using SageMaker console)."/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 â€“ Configuring autoscaling on the endpoint to use 2 â€“ 10 instances, depending on traffic (using SageMaker console).</p>
			<ol>
				<li value="3">As seen <a id="_idIndexMarker626"/>in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.21</em>, we need to update the value for the <strong class="bold">SageMakerVariantInvocationsPerInstance</strong> built-in metric <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">200</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B18493_07_021.jpg" alt="Figure 7.21 â€“ Setting the scale in and out time along with the target metric threshold to trigger autoscaling"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 â€“ Setting the scale in and out time along with the target metric threshold to trigger autoscaling</p>
			<p>Once this <a id="_idIndexMarker627"/>target value is hit and we are out of the cool-down period, SageMaker will automatically start a new instance or shut down the instance regardless of whether we are above or below the <span class="No-Break">target value.</span></p>
			<ol>
				<li value="4"><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.22</em> shows the results of the endpoint being updated with our <span class="No-Break">autoscaling policy.</span></li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B18493_07_022.jpg" alt="Figure 7.22 â€“ The endpoint running with the autoscaling policy applied"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.22 â€“ The endpoint running with the autoscaling policy applied</p>
			<p>The autoscaling policy can also be updated or deleted after <span class="No-Break">being applied.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>Endpoint modification without disruption</h2>
			<p>We can also modify deployed endpoints without affecting the availability of the models deployed <a id="_idIndexMarker628"/>in production. In addition to applying an autoscaling policy as discussed in the previous section, we can also update compute instance configurations of the endpoints. We can also add new model variants and change the traffic patterns between different model variants. All these tasks can be achieved without negatively affecting the endpoints deployed <span class="No-Break">in production.</span></p>
			<p>Now that weâ€™ve discussed how we can ensure that our endpoints are highly available, letâ€™s discuss Blue/green <span class="No-Break">deployments next.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/>Blue/green deployments</h1>
			<p>In a production environment where our models are running to make inferences in real time or near real time, it is very important that when we need to update our endpoints that it can <a id="_idIndexMarker629"/>happen without any disruption or problems. Amazon SageMaker automatically uses blue/green deployment methodology whenever we update our endpoints. In this kind of scenario, a new fleet, called the green fleet, is provisioned with our updated endpoints. The workload is then shifted from the old fleet, called the blue fleet, to the green fleet. After an evaluation period to make sure that everything is running without any issues, the blue fleet is terminated. SageMaker also provides the following three different traffic-shifted modes for blue/green deployment, allowing us to have more control over the <span class="No-Break">traffic-shifting patterns.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor156"/>All at once</h2>
			<p>In this <a id="_idIndexMarker630"/>traffic-shifting mode, all of the traffic is shifted at once from the blue fleet to the green fleet. The blue (old) fleet is kept in service for a period of time (called the baking period) to make sure everything is working fine, and performance and functionality are as expected. After the baking period, the blue fleet is terminated all at once. This type of blue/green deployment minimizes the overall update duration, while also minimizing the cost. One disadvantage of the all-at-once method is that regressive updates affect all of the traffic since the entire traffic is shifted to the green fleet <span class="No-Break">at once.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Canary</h2>
			<p>In canary blue/green deployment, a small portion of the traffic is first shifted from the blue fleet <a id="_idIndexMarker631"/>to the green fleet. This portion of the green fleet that starts serving a portion of the traffic is called the canary, and it should be less than 50% of the new fleetâ€™s capacity. If everything works fine during the baking period and no CloudWatch alarms are triggered, the rest of the traffic is also shifted to the green fleet, after which the blue fleet is terminated. If any alarms go off during the baking period, SageMaker rolls back all the traffic to the <span class="No-Break">blue fleet.</span></p>
			<p>An advantage of using canary blue/green deployment is that it confines the blast radius of regressive updates only to the canary fleet and not to the whole fleet, unlike all-at-once blue/green deployment. A disadvantage of using canary deployment is that both the blue and the green fleets are operational for the entire deployment period, thus adding to <span class="No-Break">the cost.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>Linear</h2>
			<p>In linear <a id="_idIndexMarker632"/>blue/green deployment, traffic is shifted from the blue fleet to the green fleet in a fixed number of pre-specified steps. In the beginning, the first portion of traffic is shifted to the green fleet, and the same portion of the blue fleet is deactivated. If no alarms go off during the baking period, SageMaker initiates the shifting of the second portion, and so on. If at any point an alarm goes off, SageMaker rolls back all the traffic to the blue fleet. Since traffic is shifted over to the green fleet in several steps, linear blue/green deployment reduces the risk of regressive updates significantly. The cost of the linear deployment method is proportional to the number of steps configured to shift the traffic from the blue fleet to the <span class="No-Break">green fleet.</span></p>
			<p>As discussed in this section, SageMaker has these blue/green deployment methods to make sure that there are safety guardrails when we are updating our endpoints. These blue/green deployment methods ensure that we are able to update our inference endpoints with no or minimal disruption to our deployed machine learning models in a <span class="No-Break">production environment.</span></p>
			<p>Letâ€™s now summarize what weâ€™ve covered in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor159"/>Summary</h1>
			<p>In this chapter, we discussed the various managed deployment methods available when using Amazon SageMaker. We talked about the suitability of the different deployment/inference methods for different use case types. We showed examples of how we can do batch inference and deploy real-time and asynchronous endpoints. We also discussed how SageMaker can be configured to automatically scale both up and down, and how SageMaker ensures that in case of an outage, our endpoints are deployed to multiple availability zones. We also touched upon the various blue/green deployment methodologies available with Amazon SageMaker, in order to update our endpoints <span class="No-Break">in production.</span></p>
			<p>In a lot of real-world scenarios, we do not have high-performance clusters of instances available for carrying out inference on new and unseen data in real time. For such applications, we need to use edge computing devices. These devices often have limitations on compute power, memory, connectivity, and bandwidth, and need the models to be optimized to be able to use on these <span class="No-Break">edge devices.</span></p>
			<p>In the next chapter, we will extend this discussion to learn about using machine learning models on <span class="No-Break">edge devices.</span></p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor160"/>References</h1>
			<p>You can refer to the following resources for <span class="No-Break">more information:</span></p>
			<ul>
				<li>Hosting multiple models on a single <span class="No-Break">endpoint: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</span></a></li>
				<li>Amazon EC2 Inf1 <span class="No-Break">instances: </span><a href="https://aws.amazon.com/ec2/instance-types/inf1/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/inf1/</span></a></li>
				<li>Using your own inference code with SageMaker Batch <span class="No-Break">Transform: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html</span></a></li>
				<li>Multiple models with different containers behind a single <span class="No-Break">endpoint: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html</span></a></li>
				<li>Serverless inference on Amazon <span class="No-Break">SageMaker: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html</span></a></li>
				<li>Blue/green deployments using Amazon <span class="No-Break">SageMaker: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html</span></a></li>
				<li>Canary traffic <span class="No-Break">shifting: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html</span></a></li>
				<li>SageMakerâ€™s Transformer <span class="No-Break">class: </span><a href="https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html?highlight=transformer</span></a></li>
				<li>Amazon SageMaker real-time inference:  <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html</span></a></li>
				<li>Best practices for hosting models using Amazon <span class="No-Break">SageMaker: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-best-practices.html</span></a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer121">
			</div>
		</div>
		<div>
			<div id="_idContainer122" class="IMG---Figure">
			</div>
		</div>
	</body></html>