- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What Is MLOps, and Why Is It So Important for Every ML Team?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning operations** (**MLOps**) is a pivotal practice for modern
    ML teams, encompassing the blend of technological and operational best practices.
    At its heart, MLOps seeks to address the challenges of productionizing ML models
    and fostering better collaboration between data scientists and IT teams. With
    the rapid advancements in technology and increasing reliance on ML solutions,
    MLOps is becoming the backbone of a sustainable and scalable ML strategy. This
    chapter will delve deep into the essence of MLOps, detailing its significance,
    its various maturity levels, and the role of Google’s Vertex AI in facilitating
    MLOps. By the end of this chapter, you will be equipped with a robust understanding
    of MLOps principles and what tools in Vertex AI can be used to implement those
    principles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is MLOps important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps maturity levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can Vertex AI help with implementing ML Ops?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s embark on this enlightening journey to master MLOps on Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Why is MLOps important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the development and integration of ML models become more and more common
    in today’s world, the need for a robust operational framework has become more
    critical than ever. MLOps aims to address this requirement by streamlining the
    entire process of developing, deploying, and monitoring ML models. In this section,
    we will discuss the importance of MLOps due to the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardizing and automating** **ML workflows**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps aims to standardize and automate various stages of the ML life cycle,
    from data ingestion and preprocessing to model training, evaluation, and deployment.
    By doing so, it minimizes the likelihood of human errors, facilitates reproducibility,
    and improves overall efficiency. Google’s Vertex AI offers managed services for
    each stage of the ML workflow, which helps organizations achieve consistency,
    automate processes, and reduce operational overhead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Monitoring and managing** **model performance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the key aspects of MLOps is continuously monitoring and managing the
    performance of deployed models. This is crucial, as the effectiveness of ML models
    may degrade over time due to changes in data distribution, unforeseen edge cases,
    or evolving user behaviors. Google’s Vertex AI supports MLOps by providing tools
    for monitoring model performance and generating alerts when performance thresholds
    are breached. Furthermore, it enables seamless integration with other monitoring
    and logging services in the Google Cloud ecosystem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Ensuring scalability** **and flexibility**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps facilitates the scaling of ML solutions by providing a framework that
    can easily accommodate increased data volumes, more complex models, and additional
    infrastructure requirements. Google’s Vertex AI is built to handle these demands,
    offering a range of services and tools that scale automatically, support distributed
    training and prediction, and allow users to choose the optimal hardware configurations
    for their specific use cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Security** **and compliance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps emphasizes the importance of security and compliance in ML workflows,
    ensuring that data privacy and regulatory requirements are met. Google’s Vertex
    AI supports these objectives by providing a secure environment for model training,
    storage, and deployment. Its integration with Google Cloud’s **Identity and Access
    Management** (**IAM**) enables fine-grained control over access to resources,
    while encryption options protect data both at rest and in transit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While developing ML models, there can be many shortcuts taken or complexities
    overlooked that accumulate “debt” over time, leading to system maintenance difficulties
    or failures in the future. As an ML solution deployed in production evolves, developers
    might add more features to improve accuracy and add overall value for users. Each
    feature often has its own specific way of preprocessing, normalizing, and configuring
    parameters. Over time, managing these configurations and related changes can become
    increasingly complex and the solutions might accumulate significant technical
    debt. MLOps provides the necessary practices and tooling to manage and mitigate
    these debts, ensuring sustainable and scalable ML deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, MLOps plays a vital role in streamlining ML workflows, enhancing
    collaboration, and ensuring that models are secure, scalable, and well maintained.
    Google’s Vertex AI, with its comprehensive suite of tools and services, empowers
    organizations to embrace MLOps and unlock the full potential of their ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the different levels of maturity or complexity of typical
    MLOps implementations seen in the industry. Keep in mind that the architectures
    we will discuss are just representative samples of different levels of complexity.
    In the real world, you will see many different variations of these implementations,
    and each implementation is as unique as the organization using it.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing different MLOps maturity levels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most new ML teams and organizations go through a phased MLOps journey as they
    build and refine their MLOps strategy. They usually start with a fully manual
    step-by-step process where data science/data engineering teams take an extremely
    manual, ad hoc approach to building and deploying models. Once a few models have
    been deployed and stabilized in production, it slowly becomes apparent that this
    manual process is not very scalable and that the team needs to put some processes
    and automation in place.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, as issues arise in production, it also becomes apparent that
    this ad hoc approach is not easily auditable or reproducible. As the usage of
    the ML solution grows, it graduates from being just an experiment to something
    the organization becomes increasingly dependent on. Compliance teams and leadership
    also start making requests to make the model deployment process more well organized
    and auditable to ensure compliance with the company’s IT policies. At this stage,
    the team leadership will need to put together a high-level MLOps strategy and
    implementation roadmap based on the resources they have at their disposal and
    the solution roadmap of projects they are working on. Some organizations decide
    to spend their time building the entire MLOps stack upfront, while others might
    decide to step through the different maturity levels one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the different maturity levels most ML organizations typically
    progress through.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps maturity level 0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the phase where an organization has just started experimenting with
    ML solutions and has not prepared a well-baked MLOps strategy in terms of what
    the standard process and tooling would look like as they scale up their ML adoptions.
    At this stage, the landscape looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The organization has just 1-2 models deployed in production for each business
    unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI/ML development is handled by a small centralized team of data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus is on deployment speed instead of consistent processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of ML tools is unclear, and the leadership wants to figure out what
    works and what doesn’t before committing to a particular ML platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most, if not all, steps are manual
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows different key components of an MLOps solution at
    maturity level 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 2.1 – MLOps solution: maturity level 0](img/B17792_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1 – MLOps solution: maturity level 0'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, at MLOps maturity level 0, most of
    the handovers from one process to the next are manual. This ensures a short path
    to production deployment (when needed) by not requiring a build and test of extensive
    automation processes. The downside is obviously the significant time the team
    needs to spend every time the pipeline needs to be run. So, the ML engineering
    teams need to have a roadmap/strategy in place to add automation over time.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps maturity level 1 – automating basic ML steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key characteristic of maturity level 1 in MLOps is fully automated data and
    ML model training pipelines. At this stage, full data acquisition and model retraining
    can be triggered with a single click of a button or an API call.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most significant bottlenecks that are most apparent to the team closest
    to the solution development is the data acquisition and model training process.
    In the *MLOps maturity level 0* section, most of the steps around data ingestion
    and model builds were done manually and are prone to errors even when repeating
    similar steps with slightly modified parameters. So, the data and ML model training
    pipelines end up being the very first components to get automated. It also helps
    that most of the data and ML pipelining work is being done within the same teams;
    that is, data pipeline work is done by the data team and the ML model design and
    building is done by the data science/ML team. This reduces the cross-team dependency
    on automation. There is obviously still a dependence on the IT team to ensure
    the availability of the right orchestration and automation tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – MLOps maturity level 1](img/B17792_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – MLOps maturity level 1
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, most components in the MLOps pipeline are
    automated at this maturity level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The components of the ML pipeline that are automated are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data imports**: The process of importing the latest data is almost entirely
    automated and started through external triggers that indicate the availability
    of newer data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature/data engineering**: Automatically triggered as soon as preceding
    data transfer steps are completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: At this stage, data validation is primarily handled through
    the orchestrated scripts triggered at the end of data/feature engineering steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**: Once the data validation is successfully completed, the
    orchestration layer triggers the model training step, to rebuild the model with
    newly available data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model validation**: Automated benchmarking of the model’s predictive performance,
    done through orchestrated scripts to ensure the model meets the business and technical
    requirements before being approved for production deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Registration of new model in the centralized model registry**: Once the model
    has been validated, it gets pushed to a centralized model registry that keeps
    a catalog of all approved models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other new components that get introduced at this stage to support the automation
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Repository**: To ensure consistency in the data and pipeline executions,
    creating a centralized repository to host the pipeline code/logic is important.
    For this, enterprises use their in-house Git implementations, tools such as GitHub
    or GitLab, or cloud-based code repository tools such as **Google Cloud Platform**
    (**GCP**) Cloud Source Repositories. (It’s beyond the scope of this book to cover
    these tools in detail.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Store**: Centralized catalog/repository of features that can make
    experiment automation/reproducibility easier and more consistent. This can be
    done using standalone specialized tools, such as **FEAST** or Vertex AI Feature
    Store, or a custom implementation using standard data warehouse tools. Vertex
    AI Feature Store is discussed in detail in [*Chapter 11*](B17792_11.xhtml#_idTextAnchor153),
    *MLOps Governance with* *Vertex AI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these fundamental MLOps components in place, your solution would be a lot
    more efficient and a lot less prone to human errors.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps maturity level 2 – automated model deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Key characteristics of MLOps maturity level 2 are continuous deployment capabilities,
    which can automatically deploy any new ML models in the production environment,
    and triggering the creation of new ML models based on the triggers from monitoring
    services when shifts are detected in model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram depicts a representative MLOps implementation with maturity
    level 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – MLOps maturity level 2](img/B17792_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – MLOps maturity level 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data acquisition and ML modeling portion have been automated in the
    maturity level 2 architecture, the next bottleneck that becomes apparent is the
    integration of the model build side with the model deployment side. This has two
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: The process of taking a newly trained model that has passed the validation step
    and deploying it in production. For example, a new model that passes the validation
    tests should automatically get deployed in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking the data collected by the monitoring service, turning it into actionable
    insights, and triggering updates to models. For example, a drift in incoming feature
    values in production can trigger the retraining of the ML model with newer data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other new component that gets introduced at this stage to support the automation
    is Metadata Store. This is the repository of all metadata that gets generated
    during every step of data acquisition, feature generation, model development,
    and deployment. This becomes important during automated deployments and retraining
    as the source of the model development history. For example, to monitor the model
    in production for data drift or training skew, the monitoring process needs access
    to the model’s history to get the dataset on which it was trained. The other use
    case not related to automation is that Metadata Store can enable model auditing.
    If during the model development process all key parameters around the data and
    model are being logged in to Metadata Store, then it can provide end-to-end visibility
    around the model to the model auditors. An example of this is shown in the following
    figure, where you can see the journey of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Vertex AI Metadata Store](img/B17792_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Vertex AI Metadata Store
  prefs: []
  type: TYPE_NORMAL
- en: Now let us see how you can use Vertex AI to implement end-to-end MLOps solutions.
  prefs: []
  type: TYPE_NORMAL
- en: How can Vertex AI help with implementing MLOps?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI is a platform that provides tools and resources for the
    end-to-end implementation of the ML development life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI can help with MLOps by providing features such as automated model
    building and deployment, model versioning and tracking, and monitoring and managing
    models in production. Additionally, it provides tools for collaboration and shared
    access to resources, allowing teams to work together on large and distributed
    ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertex AI can also help with other aspects of MLOps, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data management**: Vertex AI can help with data preparation, labeling, and
    management, which are crucial for building accurate ML models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimentation**: Vertex AI can help track and manage experiments, including
    comparing and selecting the best models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model governance**: Vertex AI can help manage model access and permissions
    and monitor models for drift and compliance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous integration and continuous delivery** (**CI/CD**): Vertex AI can
    help with automating the process of building, testing, and deploying models, which
    is important for keeping models updated and making sure they are always running
    smoothly in production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Vertex AI can help with scaling ML models to handle large
    amounts of data and traffic, which is essential for maintaining the performance
    of models in production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: Vertex AI can help with monitoring and measuring the performance
    of ML models in production, which is vital for understanding how well models are
    working and identifying areas for improvement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, Vertex AI can help with MLOps by providing a comprehensive platform
    for managing the entire ML life cycle on GCP, from development to deployment and
    maintenance, making it easier to build, deploy, and manage ML models in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table shows which Vertex AI tools or features can help you implement which
    components of a typical MLOps pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **MLOps Components** | **Vertex** **AI Tool** | **Other** **GCP Tools** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Management | Vertex AI Feature Store | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Data Management | Vertex AI Datasets | BigQueryGoogle Cloud Storage |'
  prefs: []
  type: TYPE_TB
- en: '| Data Exploration & Analysis | Vertex AI Workbench | Data Fusion |'
  prefs: []
  type: TYPE_TB
- en: '| Metadata Store | Vertex AI Metadata Store | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Workflow Orchestration | Vertex AI Pipelines | Kubeflow on **Google Kubernetes**
    **Engine** (**GKE**)Composer (Airflow) |'
  prefs: []
  type: TYPE_TB
- en: '| Model Registry | Vertex AI Model Registry | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Model Development | Vertex AI TrainingVertex AI Experiments | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Model Serving/Prediction Service | Vertex AI Batch PredictionsVertex AI Endpoints
    | Custom deployments on GCE or GKE |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring | Vertex AI Monitoring | N/A |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – MLOps to GCP product mapping
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at these Vertex AI tools in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Workbench** is a fully managed Jupyter Notebook-based development
    environment that supports the entire data science workflow. The key features include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration with most Google Cloud data sources, such as BigQuery and Google
    Cloud Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to create and use highly scalable Jupyter **virtual machine** (**VM**)
    instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to trigger and utilize most GCP services through a **software development**
    **kit** (**SDK**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to kick off cluster and job creation in Dataproc (managed Spark
    service)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notebook scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Vertex AI Workbench dashboard with a list
    of all deployed Jupyter Notebook instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Vertex AI Workbench dashboard](img/B17792_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Vertex AI Workbench dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Data Management** is, as the name suggests, Vertex AI’s native
    dataset management service. It allows users to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Import data into Vertex AI and manage the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage labels and multiple annotation sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When used in conjunction with Metadata Store, it helps track data lineage to
    models for troubleshooting and audits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate data statistics and visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Vertex AI Datasets dashboard with a list
    of all datasets created with the tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Vertex AI Datasets dashboard](img/B17792_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Vertex AI Datasets dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Feature Store** is a managed feature catalog service, part of Vertex
    AI. Key features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to import and organize the feature values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to serve the feature values in batch or real-time modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to monitor the change over time in feature values and generate alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following screenshot shows the Vertex AI Feature Store dashboard with a
    list of all the features and entities present in the tool.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Vertex AI Feature Store dashboard](img/B17792_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Vertex AI Feature Store dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Pipelines** is a managed Kubeflow service in the Vertex AI platform,
    which helps you orchestrate complex data and ML workflows. Key features include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for the Kubeflow Pipelines SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for **TensorFlow** **Extended** (**TFX**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to deploy vertically scalable components as containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native integration with all Vertex AI tools and most Google Cloud services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following screenshot shows the Vertex AI Pipelines dashboard with a list
    of all recently executed pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Vertex AI Pipelines dashboard](img/B17792_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Vertex AI Pipelines dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex ML Metadata Store** can store key parameters/artifacts from the ML
    pipelines to enable the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lineage tracking and auditing for ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking downstream usage of ML artifacts for better governance and compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following screenshot shows a Vertex AI Metadata Store dashboard. The following
    figure shows a sample Metadata Store artifact depicting the lineage of an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Vertex AI Metadata Store](img/B17792_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Vertex AI Metadata Store
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Model Registry** is a centralized registry for all ML models regardless
    of whether they were custom models or models generated by using AutoML capabilities.
    Key features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully managed model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamless model deployments for a batch or real-time prediction service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to compare models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure shows the Vertex AI Model Registry dashboard with a list
    of registered models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Vertex AI Model Registry](img/B17792_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Vertex AI Model Registry
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Training** is the core managed training service in Vertex AI that
    enables users to run complex ML model training jobs without managing the underlying
    infrastructure. Key capabilities include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Native support for TensorFlow, XGBoost, and scikit-learn custom models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to deploy training jobs using custom containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate ML for a variety of use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to deploy models on extremely scalable on-demand clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managed TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertex AI Experiments** is a managed experiment orchestration feature that
    enables the data science team to kick off a number of similar training jobs with
    slightly differing hyperparameters as part of model design.'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the Vertex AI Experiments dashboard with a list of
    executed training experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Vertex AI Experiments dashboard](img/B17792_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Vertex AI Experiments dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Batch Predictions** provides fully managed batch predictions for
    models, uploaded to the model registry.'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the Vertex AI Batch Predictions dashboard with a
    list of batch prediction jobs run on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Vertex AI Batch Predictions dashboard](img/B17792_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Vertex AI Batch Predictions dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Endpoints** is a managed model serving capability for real-time
    prediction use cases. Key features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Configurable serving infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to detect the number of performance issues relating to increased
    prediction latency, capacity bottlenecks, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure shows the dashboard for Vertex AI Endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Vertex AI monitoring dashboard](img/B17792_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Vertex AI monitoring dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex AI Monitoring** helps automate the monitoring of deployed models in
    production to identify performance issues proactively. Key features include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Drift detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training-serving skew detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, Google Cloud Vertex AI offers a comprehensive and robust set
    of tools that can help you streamline the entire ML development life cycle, from
    data management to model deployment and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the core concepts of MLOps and explored the
    various levels of maturity that MLOps solutions exhibit in real-world scenarios.
    We also discussed the various tools and resources that are available as part of
    the Vertex AI platform, which are designed to help streamline and automate the
    process of building, deploying, and maintaining ML models.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, we hope you are able to better articulate the fundamentals
    of MLOps practices and develop a high-level MLOps strategy for your organization,
    taking into consideration your organization’s unique requirements and goals, and
    understand how Vertex AI, as a part of GCP, can play a crucial role in your organization’s
    MLOps strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we cover the options available in Google Cloud to store
    data in order to support your ML solution’s training needs. We also discuss the
    tools available to you to help you transform large-scale datasets as part of feature
    engineering activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Machine Learning Tools for Custom Models on Google Cloud'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will get an overview of the key ML tools that Google Cloud
    offers. You will learn about the different options in Google Vertex AI for storing
    and transforming data, training ML models, model optimization, model explainability,
    and so on. In addition to this, you will learn about the deployment tools, automation
    tools, and governance tools that Vertex AI offers for ML orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B17792_03.xhtml#_idTextAnchor041), *It’s All about Data – Options
    to Store and Transform ML Datasets*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B17792_04.xhtml#_idTextAnchor056), *Vertex AI Workbench – a One-Stop
    Tool for* *AI/ML* *Development Needs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17792_05.xhtml#_idTextAnchor066), *No-Code Options for Building
    ML Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17792_06.xhtml#_idTextAnchor079), *Low-Code Options for Building
    ML Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17792_07.xhtml#_idTextAnchor093), *Training Fully Custom ML
    Models with Vertex AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17792_08.xhtml#_idTextAnchor102), *ML Model Explainability*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17792_09.xhtml#_idTextAnchor121), *Model Optimizations – Hyperparameter
    Tuning and NAS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17792_10.xhtml#_idTextAnchor136), *Vertex AI Deployment and
    Automation Tools – Orchestration through Managed Kubeflow Pipelines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17792_11.xhtml#_idTextAnchor153), *MLOps Governance with Vertex
    AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
