- en: Chapter 10. Face Recognition and Face Emotion Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at how to detect objects such as a car,
    chair, cat, and dog, using Convolutional Neural Networks and the YOLO (You Only
    Look Once) algorithm. In this chapter, we will be detecting human faces. Apart
    from that, we will be looking at expressions of the human face, such as a human
    face seeming happy, neutral, sad, and so on. So, this chapter will be interesting,
    because we are going to focus on some of the latest techniques of face detection
    and face emotion recognition. We are dividing this chapter into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face emotion recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we will cover how face detection works, and after that, we will move
    on to the face emotion recognition part. In general, we will cover the following
    topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concepts of face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches for implementing face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset for face emotion recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concepts of face emotion recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the face emotion recognition model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the process for optimization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to develop two applications. One application will recognize human faces,
    and the other will recognize the emotion of the human faces. We will discuss both
    of them in this section. We will look at what exactly we want to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Face recognition application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This application should basically identify human faces from an image or a real-time
    video stream. Refer to the following photo; it will help you understand what I
    mean by identifying faces from an image or a real-time video stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face recognition application](img/B08394_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Demo output for understanding the face recognition application'
  prefs: []
  type: TYPE_NORMAL
- en: 'Images source: https://unsplash.com/photos/Q13lggdvtVY'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure (Figure 10.1), when we provide any image
    as the input, in the first step, the machine can recognize the number of human
    faces present in the image. As the output, we can get cropped images of the faces.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond this, I also want the application to identify the name of the person
    based on the face. I think you are familiar with this kind of application. Let
    me remind you. When you upload an image on Facebook, the face recognition mechanism
    of Facebook immediately recognizes names of people who are part of that image,
    and suggests that you tag them in your image. We will develop similar functionality
    here in terms of the face recognition application. Now let's move on to another
    part of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Face emotion recognition application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part of the application, we want to build an application that can detect
    the type of emotion on a human face. We will try to recognize the following seven
    emotions:'
  prefs: []
  type: TYPE_NORMAL
- en: Anger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disgust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Happiness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surprise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we will categorize facial emotions into these seven types. This kind of
    application will be helpful to know what kind of feeling the person is experiencing,
    and this insight will help in performing sentiment analysis, body language analysis,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will first build the face recognition application, and after that,
    we will move on to the face emotion recognition application.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will set up the coding environment for the face recognition
    application. We will look at how to install dependencies. We will be installing
    the following two libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: dlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: face_recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the installation process.
  prefs: []
  type: TYPE_NORMAL
- en: Installing dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to install the dlib library, we need to perform the following steps.
    We can install this library either on a Linux operating system (OS), or on macOS.
    Let''s follow the stepwise instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the source code of dlib by executing this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now jump to the `dlib` directory by executing this command: `cd dlib`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we need to build the main `dlib` library, so we need to execute the following
    commands stepwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo mkdir build`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cd build`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cmake .. -DDLIB_USE_CUDA=0 -DUSE_AVX_INSTRUCTIONS=1`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cmake --build`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the project has been built successfully, you can move to the next installation
    steps. You also need to install **OpenCV**. The installation steps for OpenCV
    have already been given in [Chapter 10,](ch10.xhtml "Chapter 10. Face Recognition
    and Face Emotion Recognition") *Real-Time Object Detection*.
  prefs: []
  type: TYPE_NORMAL
- en: Installing face_recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to install the `face_recognition` library, we need to execute the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands install the `face_recognition` library only if we have
    a perfectly installed `dlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the preceding two libraries have been installed, we can move on to the
    next section, in which we will be discussing the key concepts of face recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts of face recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the major concepts of face recognition. These
    concepts will include the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the face recognition dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm for face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the face recognition dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may wonder why I haven't discussed anything related to the dataset until
    now. This is because I don't want to confuse you by providing all the details
    about the datasets of two different applications. The dataset that we will cover
    here is going to be used for **face recognition**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build a face recognition engine from scratch, then you can use
    following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: CAS-PEAL Face Dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeled Faces in the Wild
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss them in further detail.
  prefs: []
  type: TYPE_NORMAL
- en: CAS-PEAL Face Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a huge dataset for face recognition tasks. It has various types of face
    images. It contains face images with different sources of variations, especially
    Pose, Emotion, Accessories, and Lighting (PEAL) for face recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains 99,594 images of 1,040 individuals, of which 595 are
    male individuals and 445 are female individuals. The captured images of the individuals
    are with varying poses, emotions, accessories, and lighting. Refer to the following
    photo to see this. You can also refer to the following link if you want to see
    the sample dataset: [http://www.jdl.ac.cn/peal/index.html](http://www.jdl.ac.cn/peal/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '![CAS-PEAL Face Dataset](img/B08394_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: CAS-PEAL Face Dataset sample image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: http://www.jdl.ac.cn/peal/Image/Pose_normal/NormalCombination-9-Cameras.jpg'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download this dataset from the following link: [http://www.jdl.ac.cn/peal/download.htm](http://www.jdl.ac.cn/peal/download.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled Faces in the Wild
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This dataset is also referred to as the LFW dataset. It is used in the `face_recognition`
    library. We will be using this library to build our face recognition application.
    This dataset contains more than 13,000 images of faces collected from the web.
    Each face is labeled with the name of the person pictured. So, the dataset is
    a labeled dataset. There are 1,680 people pictured with two or more distinct face
    images in the dataset. You can refer to the sample dataset using the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Labeled Faces in the Wild](img/B08394_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Sample images from the LFW dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: http://vis-www.cs.umass.edu/lfw/person/AJ_Cook.html'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find out more about this dataset by clicking on [http://vis-www.cs.umass.edu/lfw/index.html](http://vis-www.cs.umass.edu/lfw/index.html).
    You can also download the dataset by using the same link. You can refer to the
    Caltech 10,000 web faces dataset as well by clicking on [http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/](http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/).
    You should also refer to the INRIA Person Dataset, which will be quite useful.
    The link for the INRIA Person Dataset is [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/).
  prefs: []
  type: TYPE_NORMAL
- en: In order to build the face recognition application, we will be using the `face_recognition`
    library. We are using the pre-trained model provided by this library via its API.
    We will certainly explore the algorithm and the concept behind this pre-trained
    model and library. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for face recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at the core algorithm that is used for face recognition.
    The name of the algorithm is **Histogram of Oriented Gradients** (**HOG**). We
    will see how HOG is used in face recognition tasks. A face recognition (FR) task
    is basically a classification task, as we are detecting the face from the image
    as well as trying to identify the person's name with the help of the person's
    face. HOG is a good option to try out.
  prefs: []
  type: TYPE_NORMAL
- en: The other approach is to use the Convolutional Neural Network (CNN). In this
    section, we will also cover CNN for the FR task. So, let's start out with HOG!
  prefs: []
  type: TYPE_NORMAL
- en: Histogram of Oriented Gradients (HOG)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The HOG algorithm is one of the best approaches for state-of-the-art results
    for face recognition. The HOG method was introduced by Dalal and Triggs in their
    seminal 2005 paper, available at [http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).
    The HOG image descriptor and a linear Support Vector Machine can be used to train
    highly accurate classifiers that can classify human detectors. So, HOG can be
    applied to an FR task as well. First, we will cover the basic intuition behind
    the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'HOG is a type of feature descriptor. A feature descriptor is a representation
    of an image that simplifies the image by extracting useful information and ignoring
    the information. Here, our focus will be on the faces only. So, we will be ignoring
    other objects, if there are any. The LWF dataset has less noise, so the task of
    generating an accurate feature descriptor is comparatively easy. The step-by-step
    process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: In order to find the faces in image, we will start by converting
    our color image into black and white, because we don''t need color data to recognize
    the face. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Converting a color image to a black and white image'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: In this step, we will look at every single pixel in our image at
    a time. For every single pixel, we want to look at pixels that directly surround
    it. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Process of scanning every single pixel of the image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Here, our goal is to find out how dark the current pixel is with respect
    to the pixels directly surrounding it. We need to draw an arrow that indicates
    the direction in which the pixels of the image are getting darker. In order to
    achieve this, we scan the entire image. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: An arrow direction from a light pixel to a dark pixel'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, we have considered a pixel and the
    other pixels surrounding it. By looking at the pixels, we can easily figure out
    that the arrow head is pointing toward the darker pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: If we repeat this process for every single pixel in the image, then
    we will end up with every pixel being replaced with arrows. These arrows are called
    *gradients*. These *gradients* show the flow from light to dark pixels across
    the entire image. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Gradient arrows for the entire image'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can see the kind of output we get after generating
    a gradient for the input image. The scanning of the entire image might seem like
    a random thing to do, but there is a reason for replacing the pixels with gradients.
    If we analyze the original pixel values directly, then really dark images and
    really light images of the same person will have totally different pixel values,
    which make things more complicated when we try to recognize the face of the person.
    Here, we are considering the direction in which the brightness of the pixel changes.
    We find that both really dark images and really light images of the same person
    will end up with the exact same representation for the face. This kind of representation
    will be easy for us to deal with in terms of the face recognition task. This is
    the main reason for generating the gradient for the entire image. There is one
    challenge, though, which we will discuss in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Saving the gradient for every single pixel gives us too much information,
    and there is the chance that we may use this amount of information inefficiently.
    So, we need to obtain the bare minimum information that we will be using for the
    FR task. We will achieve this by just considering the basic flow of lightness
    or darkness at a higher level, so we can see the basic pattern of the image. The
    process for achieving this is given in step 6.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: We will break up this image into small squares of 16 x16 pixels each.
    In each square, we will count the number of gradient points in each major direction,
    which means we will count how many arrows point up, point down, point right, point
    left, and so on. After counting this, we will replace that square in the image
    with the arrow directions that were the strongest. The end result is that we convert
    the original image into a simple representation that captures the basic structure
    of a face. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Simple face representation in the HOG version'
  prefs: []
  type: TYPE_NORMAL
- en: This kind of representation is easy to process for the FR task; it's called
    the HOG version of the image. It represents the features that we will consider
    in the FR task, and that is why this representation is referred to as an HOG features
    descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: In order to find out the faces in this HOG image, we have to find out
    the part of our image that looks the most similar to a known HOG pattern that
    was extracted from a bunch of other training faces. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Process of recognizing a face using the HOG version of our image'
  prefs: []
  type: TYPE_NORMAL
- en: Using this technique, we can easily recognize the faces in any image.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN) for FR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will look at how a CNN can be used to recognize the faces
    from the images. This section is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple CNN architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how a CNN works for FR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple CNN architecture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'I don''t want to get too deep into how a CNN works, as I have already provided
    most of the necessary details in [Chapter 9](ch09.xhtml "Chapter 9. Building a
    Real-Time Object Recognition App"), *Building Real-Time Object Detection*; however,
    I want to remind you about some necessary stuff regarding CNN. First, refer to
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple CNN architecture](img/B08394_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: The CNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, there is a convolutional layer, a pooling
    layer, a fully connected layer, and an output layer. There are different activation
    functions, penalties, and SoftMax functions involved. This is high-level information.
    For this FR task, we can use three convolutional and pooling layers with ReLU
    as activation functions. You can add more layers, but it will become more computationally
    expensive to train.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how CNN works for FR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Intuitively, the CNN model performs the following steps in order to build a
    good FR application. The basic process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Look at a picture. Crop the images that contain only faces.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Now, in this step, we focus on a face and try to understand that even
    if a face is turned in a weird direction, or an image is taken in bad lighting,
    we need to identify the proper placement of the face in this kind of image. Step
    3 will give us a solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: In order to identify the face from any image, whether that image is
    taken in bad lighting conditions, or the orientation of the face seems totally
    weird, we need to identify the face. To achieve that, we pick out unique features
    of the face that can be used to tell us something unique about the person''s face.
    With the help of these unique features, we can identify the face of the same person,
    as well as the face of the different persons. These features can include how big
    the eyes are, how long the face is, and so on. There are 68 specific points that
    should be considered; and they are called landmarks. These points are defined
    based on the face landmark estimation. Refer to the following paper to get more
    details about this: [http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf](http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf).
    Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding how CNN works for FR](img/B08394_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: 68 points for face landmark estimation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: We need to identify the person''s face with their name, so in order
    to achieve this, we will compare the unique features of that face with all the
    people we already know in order to determine the person''s name. Suppose you have
    added the images for Bill Gates, Barack Obama, and so on. You have generated the
    unique features for their faces, and now we will compare their unique facial features
    with these already generated facial features, and if the features are similar,
    then we get to know the name of the person, which is Barack Obama or Bill Gates
    in the given image. The identification of the person based on their facial features
    is a classification problem, which can easily be solved by CNN. We are generating
    a face embedding vector of size 128 measurements. As an input, we should provide
    this face embedding vector. Once we complete the training, our application will
    be ready to identify the person''s name.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: The trained model looks at all the faces we have measured in the past,
    and looks at the person who has the closest measurements to our faces'' measurements.
    That is our match.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding approach is for our CNN-based FR and real-time face recognition
    task. We have covered the basic concepts and the idea behind the algorithms that
    are used in the FR task. Now let's start the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for implementing face recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing the FR application. We are using the
    `face_recognition` library. We have already configured the environment for that.
    We will be implementing the following approaches here:'
  prefs: []
  type: TYPE_NORMAL
- en: The HOG-based approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CNN-based approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's start coding!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the HOG-based approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this approach, we are using the HOG algorithm to find out two things: the
    total number of faces in the image, and the paces. We are using the API of the
    `face_recgnition` library. You can find the code by clicking on the following
    GitHub link: [https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py).
    The code snippet is provided in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the HOG-based approach](img/B08394_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Code snippet for the HOG-based approach for FR'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we have given an image as input, and with the help
    of the API of the `face_recognition` library, we can find the pixel location of
    the face in an image. Here, we will also count how many faces there are in an
    image, and with the help of the `Image` library, we can crop the faces from the
    given image. You can find the output of this script in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the HOG-based approach](img/B08394_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: Output of the HOG-based approach for FR'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the cropped face output in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the HOG-based approach](img/B08394_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Cropped face output'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the last output diagram with the help of a simple API, we
    can build a simple face recognition application. This approach is kind of a baseline
    approach for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the CNN-based approach. The HOG-based approach is less
    accurate compared to the CNN-based approach. If we use GPU for the CNN-based approach,
    then we can train the model in a less amount of time. Now let's look at the code
    for the CNN-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the CNN-based approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this approach, we will be using the `face_recognition` library, where we
    specified the name of the model. Our model''s name is `cnn`. This particular approach
    will load the pre-trained model via the `face_recognition` API, and we can generate
    a more accurate output. You can find the code by clicking on the following GitHub
    link: [https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py).
    Refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the CNN-based approach](img/B08394_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Code snippet for the CNN-based approach for FR'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the implementation code is almost the same as earlier, but the difference
    is that we have provided the model name as `cnn` during the API call. You can
    see the output of this implementation in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the CNN-based approach](img/B08394_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Output of the CNN-based approach for FR'
  prefs: []
  type: TYPE_NORMAL
- en: The output of this implementation is the same as the last one. This version
    of the implementation is fast, and has better accuracy. Now let's try to implement
    the FR task for a real-time video stream.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing real-time face recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will implement the FR task for a real-time video stream.
    We will try to identify the name of the person who has appeared in the video.
    Doesn''t that sound interesting? Let''s begin. You can find the code by clicking
    on the following GitHub link: [https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py](https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we are using the API of the `face_recognition` library. We are using
    `OpenCV` as well. First of all, we need to feed the sample image of the person
    with the person''s name, so that the machine can learn the name of the person
    and identify it during the testing. In this implementation, I have fed the image
    of Barack Obama and Joe Biden. You can add your image as well. If the face features
    are familiar and match with the already-fed images, then the script returns the
    name of the person, and if the face features are not familiar to the given image,
    then that person''s face is tagged as *Unknown.* Refer to the implementation in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing real-time face recognition](img/B08394_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Implementation of real-time FR'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code, I have provided sample images of Barack
    Obama and Joe Biden. I have also provided the names of the people whose images
    I''m feeding to my script. I have used the same face recognition API for detecting
    and recognizing the face in the video stream. When you run the script, your webcam
    streams your real-time video and this script detects the face, and if you provide
    the image of the person that the machine knows, then it identifies it correctly
    this time as well. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing real-time face recognition](img/B08394_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.18: Output of the real-time FR'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, I have not provided my image to the machine, so it identifies
    me as **Unknown**, whereas it can identify Barack Obama's image. You can also
    find the animated image by clicking on [https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif](https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif).
  prefs: []
  type: TYPE_NORMAL
- en: We are finished with the first part of the chapter, which entails developing
    an application that can recognize human faces, as well as identify the name of
    the person based on their face. We implemented three different variations of FR.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will look at how to develop a face emotion recognition
    (FER) application. We need different kinds of datasets to build this application,
    so we will start by understanding a dataset for FER.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset for face emotion recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To develop an FER application, we are considering the `FER2013` dataset. You
    can download this dataset from [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data).
    We need to know the basic details about this dataset. The dataset credit goes
    to Pierre-Luc Carrier and Aaron Courville as part of an ongoing research project.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of 48x48 pixel grayscale images of faces. The task is
    to categorize each of the faces based on the emotion that has been shown in the
    image in the form of facial expressions. The seven categories are as follows,
    and for each of them there is a numeric label that expresses the category of the
    emotion:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0` = Anger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` = Disgust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2` = Fear'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3` = Happiness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4` = Sadness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5` = Surprise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`6` = Neutral'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This dataset has the `fer2013.csv` file. This csv file will be used as our
    training dataset. Now let''s look at the attributes of the file. There are three
    columns in the file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Emotion**: This column contains the numeric label of the facial expression.
    For fear, this column contains the value `2`; for sadness, this column contains
    the value `4`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pixels**: This column contains the pixel values of the individual images.
    It represents the matrix of pixel values of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: This column contains the general tag about whether the particular
    data record will be used for training purposes or for testing purposes. There
    are three labels that are part of this column, and those are *Training*, *PublicTest*,
    and *PrivateTest*. For training purposes, there are 28,709 data samples. The public
    test set consist of 3,589 data samples, and the private test set consists of another
    3,589 data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's cover the concepts that will help us develop the FER application.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts of face emotion recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using Convolutional Neural Network (CNN) to develop the FER application.
    Earlier, we looked at the basic architecture of CNN. In order to develop FER applications,
    we will be using the following CNN architecture and optimizer. We are building
    CNN that is two layers deep. We will be using two fully connected layers and the
    SoftMax function to categorize the facial emotions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using several layers made of the convolutional layer, followed by
    the ReLU (Rectified Linear Unit) layer, followed by the max pooling layer. Refer
    to the following diagram, which will help you conceptualize the arrangement of
    the CNN layers. Let''s look at the working of CNN. We will cover the following
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SoftMax layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the convolutional layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this layer, we will feed our image in the form of pixel values. We are using
    a sliding window of 3 x 3 dimension, which slides through the entire image. The
    area that is chosen by the sliding window is called the *receptive field*. It
    is the patch of the image. A sliding window is just the matrix of a 3 x 3 dimension,
    and it can scan the entire image. By using the sliding window, we scan nine pixel
    values of the image using the matrix of a 3 x 3 dimension. This receptive field
    or a piece of the image is the input of the convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the convolutional layer](img/B08394_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.19: Sliding window and receptive field'
  prefs: []
  type: TYPE_NORMAL
- en: 'This receptive field carries the values in the form of pixel values of the
    input image. These pixel values are called feature maps, features, filter, weight
    matrix, or kernel. We already had a matrix of 3 x 3, which is referred to as the
    feature map. The size of the feature map is one of the hyperparameters. We can
    take the n x n matrix, where n >=1\. Here, we have considered a 3 x 3 matrix to
    understand the operation. Now it''s time to perform a simple math operation, the
    steps for which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Get the feature map. Here, the feature map means the image patch that
    is generated in the form of a receptive field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: We need to perform the dot product between the feature map and the
    entire image. Again, we scan the entire image using the sliding window, and generate
    the value of dot products.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: We need to sum up all the values that we get from obtaining the dot
    product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: We need to divide the value of the summation for the dot product by
    the total number of pixels in the feature. In this explanation, we have a total
    of nine pixels, so we will divide the sum by 9\. As an output, we get the image
    that is referred to as a feature image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the convolutional layer](img/B08394_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.20: Math operation for the convolutional layer'
  prefs: []
  type: TYPE_NORMAL
- en: We will repeat this operation for almost all the possible positions of the image,
    and we will try out all the possible combinations, which is the reason why this
    operation is referred to as convolutional. Now let's look at the ReLU layer.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ReLU layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This layer basically introduces nonlinearity to the convolutional network.
    Here, we need to use the `activation` function. For this application, we have
    chosen the Rectified Linear Unit as the activation function. This layer performs
    some sort of normalization to our feature map. Let''s see what it does to our
    feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: This layer takes the feature map as the input that is generated by
    the convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: This layer just converts the negative values into zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the ReLU layer](img/B08394_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.21: Operations performed by the ReLU layer'
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time to look at the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the pooling layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using this layer, we shrink the image. We will be using the max pooling operation
    here. We need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We need to feed the feature map as the input, and this time, the output
    of the ReLU layer is given as the input to this layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: We need to pick up the window size. Generally, we pick up the size
    of 2 x 2 pixels or 3 x 3 pixels. We will be taking 2 x 2 as our window size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: We scan the entire image based on this window size, and we will take
    the maximum value from four pixel values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can understand the operation by referring to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the pooling layer](img/B08394_10_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.22: Operations performed by the max pooling layer'
  prefs: []
  type: TYPE_NORMAL
- en: We can do deep stacking of these layers as deep as we need. You can repeat the
    convolutional, ReLU, and pooling layers an n number of times in order to make
    CNN deep.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fully connected layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The output of all the layers is passed on to the fully connected layer. This
    layer has a voting mechanism. All image patches are considered for votes. The
    image patches of 2 x 2 matrices are arranged in a horizontal way. The vote depends
    on how strongly a value predicts the face expression. If certain values of this
    layer are high, it means they are close to 1, and if certain values of this layer
    are low, it means they are close to 0\. For each category, certain cell values
    are close to 1 and others are 0, and this way, our network will predict the category.
    Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the fully connected layer](img/B08394_10_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.23: Intuitive understanding of the fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: There is only one math operation that's been performed here. We are taking average
    values. As you can see in preceding diagram, the first, fourth, fifth, tenth,
    and eleventh cells of the fully connected layer are predicting one category, so
    we need to sum up all the values present in those cells and find their average.
    This average value tells us how confident our network is when it predicts the
    class. We can stack up as many fully connected layers as we want. Here, the number
    of neurons is the hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the SoftMax layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use the SoftMax layer, which converts the feature''s values into
    the probability value. The equation for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the SoftMax layer](img/B08394_10_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.24: The SoftMax equation'
  prefs: []
  type: TYPE_NORMAL
- en: 'This layer takes the feature values, and using the preceding equation, it generates
    the probability value. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the SoftMax layer](img/B08394_10_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.25: Process of calculating the SoftMax function'
  prefs: []
  type: TYPE_NORMAL
- en: Updating the weight based on backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The weight of the CNN network has been updated based on the backpropagation
    technique. We will measure the difference between our predicted answer and the
    actual answer. Based on this error measurement, we calculate the gradient for
    the loss function, which tells us whether we should increase the weight or decrease
    it. If the predicted answer and the actual answers are the same then there will
    be no change in weights.
  prefs: []
  type: TYPE_NORMAL
- en: We have understood most of the core concepts of CNN that we will be using for
    developing face emotion recognition model.
  prefs: []
  type: TYPE_NORMAL
- en: Building the face emotion recognition model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement the application of FER using CNN. For coding
    purposes, we will be using the `TensorFlow`, `TFLearn`, `OpenCV`, and `Numpy`
    libraries. You can find the code by using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow.](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow)
    These are the steps that we need to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be preparing the dataset that can be used in our application.
    As you know, our dataset is in grayscale. We have two options. One is that we
    need to use only black and white images, and if we are using black and white images,
    then there will be two channels. The second option is that we can convert the
    grayscale pixel values into RGB (red, green, and blue) images and build the CNN
    with three channels. For our development purposes, we are using two channels as
    our images are in grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we are loading the dataset and converting it into the `numpy`
    array. After the conversion, we will save it as a `.npy` format so that we can
    load that dataset as and when needed. We are saving actual data records in one
    file and the labels of their data records in another file. Our input datafile
    name is `fer2013.csv`. Our output file that contains the data is `data_set_fer2013.npy`,
    and labels are present in the `data_labels_fer2013.npy` file. The script name
    that performs this task is `csv_to_numpy.py`. You can refer to its code using
    this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet for loading the dataset in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the data](img/B08394_10_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.26: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the `helper` function is given in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the data](img/B08394_10_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.27: Code snippet for the helper function'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how we can load the data that we have saved in the `.npy`
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at how we are going to use the dataset we have
    prepared so that we can use it for training. Here, we create a separate script
    to help us load the data. In this, we define a test dataset that we will be using
    during testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple and straightforward code. You can find the code snippet in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the data](img/B08394_10_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.28: Code snippet for the data loader script'
  prefs: []
  type: TYPE_NORMAL
- en: 'This class and its methods will be used when we write the script for training.
    You can see the code of this script using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at how to train the model, so that it can recognize
    the facial emotion. These are the steps that we will be performing. You can find
    the code for this training step by referring to this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data using the dataset_loader script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we are loading the dataset with the help of the script that we have written
    and understood in the last section. You can find the code snippet in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the data using the dataset_loader script](img/B08394_10_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.29: Loading the data during the training of the model'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build the CNN that is actually used for training.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Convolutional Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we will be building the CNN that will be used for training purposes.
    Here, we have three layers of the convolutional network, the ReLU layer, and the
    pooling layer. The first two layers have 64 neurons, and the last one has 128
    neurons. We have added the dropout layer. For some neurons, the dropout layer
    sets the value to zero. This layer selects the neurons that have not changed their
    weight for a long time, or have not been activated for a long time. This will
    make our training more effective. We have two fully connected layers, and one
    fully connected layer using the SoftMax function to derive the probability for
    the facial emotion class. We are using the `momentum` function for performing
    the gradient descent. Here, our loss function is categorical cross-entropy. Refer
    to the code snippet in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the Convolutional Neural Network](img/B08394_10_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.30: Code snippet for building CNN'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how to perform training.
  prefs: []
  type: TYPE_NORMAL
- en: Training for the FER application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we need to start training so that our model can learn to predict
    facial emotions. In this step, we will be defining some hyperparameters for training.
    Refer to the code snippet in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training for the FER application](img/B08394_10_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.31: Code snippet for performing training'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, we have set the epoch to `100`. The
    training batch size is `50`. We can see the `shuffle` parameter, which acts as
    the flag. The value of this parameter is `true`, which indicates that we are shuffling
    our dataset during training.
  prefs: []
  type: TYPE_NORMAL
- en: The command to start training is `$ python emotion_recognition.py train`.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and saving the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we are defining the `predict` method. This method helps us generate
    a prediction. We have also defined the method that can help us save the trained
    model. We need to save the model, because we can load it as and when needed for
    testing. You can find the code snippet in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting and saving the trained model](img/B08394_10_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.32: Code snippet for predicting the class'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting and saving the trained model](img/B08394_10_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.33: Code snippet for saving the trained model'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to look at the testing matrix. After that, we need to test our
    trained model. So, before testing our model, we should understand the testing
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the testing matrix for the facial emotion
    application. The concept of testing is really simple. We need to start observing
    the training steps. We are tracking the values for loss and accuracy. Based on
    that, we can decide the accuracy of our model. Doesn''t this sound simple? We
    have trained the model for 30 epochs. This amount of training requires more than
    three hours. We have achieved 63.88% training accuracy. Refer to the code snippet
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the testing matrix](img/B08394_10_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.34: Training progress for getting an idea of training accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: This is the training accuracy. If we want to check the accuracy on the validation
    dataset, then that is given in the training step as well. We have defined the
    validation set. With the help of this validation dataset, the trained model generates
    its prediction. We compare the predicted class and actual class labels. After
    that, we generate the validation accuracy that you can see in the preceding diagram.
    Here, `val_acc` is 66.37%, which is great. To date, this application has been
    able to achieve up to 65 to 70% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to load the trained model and test it. Here, we will be using the
    video stream. The FER application will detect the emotion based on my facial expression.
    You can refer to the code using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet for this in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.35: Code snippet for loading the trained model and performing testing'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to start testing, we need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ python emotion_recognition.py poc`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This testing will use your webcam. I have some demo files that I want to share
    here. Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.36: Code snippet for FER application identifying the emotion of disgust'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also refer to the the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.37: tThe FER application identifying the happy emotion'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.38: Code snippet for the FER application identifying the neutral
    emotion'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.39: Code snippet for the FER application identifying the angry emotion'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how we can improve this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will list all the points that create problems. We should
    try to improve them. The following are things that I feel we can improve upon:'
  prefs: []
  type: TYPE_NORMAL
- en: If you find out that class sampling is not proper in your case, then you can
    adopt the sampling methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can add more layers to our neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can try different gradient descent techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, training takes a lot of time that means training is computationally
    expensive. When we trained the model, we used GPUs even though GPU training takes
    a long time. We can use multiple GPUs, but that is expensive, and a cloud instance
    with multiple GPUs is not affordable. So, if we can use transfer learning in this
    application, or use the pre-trained model, then we will achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen in the previous section, because of the lack of computation
    hardware, we have achieved a 66% accuracy rate. In order to improve the accuracy
    further, we can use the pre-trained model, which will be more convenient.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process for optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few problems that I have described in the previous sections. We
    can add more layers to our CNN, but that will become more computationally expensive,
    so we are not going to do that. We have sampled our dataset well, so we do not
    need to worry about that.
  prefs: []
  type: TYPE_NORMAL
- en: As part of the optimization process, we will be using the pre-trained model
    that is trained by using the `keras` library. This model uses many layers of CNNs.
    It will be trained on multiple GPUs. So, we will be using this pre-trained model,
    and checking how this will turn out.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will be implementing the code that can use the pre-trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have achieved approximately a 66% accuracy rate; for an FER application,
    the best accuracy will be approximately 69%. We will achieve this by using the
    pre-trained model. So, let's look at the implementation, and how we can use it
    to achieve the best possible outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing the best possible approach for the
    FER application. This pre-trained model has been built by using dense and deep
    convolutional layers. Because of the six-layer deep CNN, and with the help of
    the stochastic gradient descent (SGD) technique, we can build the pre-trained
    model. The number of neurons for each layer were 32, 32, 64, 64, 128,128, 1,024,
    and 512, respectively. All layers are using ReLU as an activation function. The
    3 x 3 matrix will be used to generate the initial feature map, and the 2 x 2 matrix
    will be used to generate the max pooling. You can download the model from this
    GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras](https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look at the code by referring to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10\. 40: Code snippet for using the pre-trained FER model'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we loaded the pre-trained `keras` model. We are providing
    two provisions. We can use this script for detecting the facial expression from
    the image as well as by providing the video stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test the facial expression present in any image, then we need
    to execute the `$ python image_test.py tes.jpg` command. I have applied it to
    this model on the `tes.jpg` image. You can see the output image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.41: Output of the FER application for the image'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test the model for the video stream, then you need to execute
    this command: `$python realtime_facial_expression.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the output in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.42: Output of the FER application for the video stream'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the output file in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.43: Output of the FER application for the video stream'
  prefs: []
  type: TYPE_NORMAL
- en: This application provides us with approximately 67% accuracy, which is great.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to develop the face detection application
    using the `face_recognition` library, which uses the HOG-based model to identify
    the faces in the images. We have also used the pre-trained convolutional neural
    network, which identifies the faces from a given image. We developed real-time
    face recognition to detect the names of people. For face recognition, we used
    a pre-trained model and already available libraries. In the second part of the
    chapter, we developed the face emotion recognition application, which can detect
    seven major emotions a human face can carry. We used `TensorFlow`, `OpenCV`, `TFLearn`,
    and `Keras` in order to build the face emotion recognition model. This model has
    fairly good accuracy for predicting the face emotion. We achieved the best possible
    accuracy of 67%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the computer vision domain is moving quickly in terms of research.
    You can explore many fresh and cool concepts, such as `deepfakes` and 3D human
    pose estimation (machine vision) by the Facebook AI Research group. You can refer
    to the `deepfakes` GitHub repository by clicking here: [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
    You can refer to the paper on 3D human pose estimation by clicking on this link:
    [https://arxiv.org/pdf/1705.03098.pdf](https://arxiv.org/pdf/1705.03098.pdf).
    Both the concepts are new and fresh, so you can refer to them and make some fun
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be the last chapter of this book. In the course of the
    chapter, we will try to make a gaming bot. This chapter will heavily use reinforcement
    learning techniques. We will be developing a bot that can play Atari games on
    its own. So keep reading!
  prefs: []
  type: TYPE_NORMAL
