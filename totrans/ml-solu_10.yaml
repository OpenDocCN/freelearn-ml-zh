- en: Chapter 10. Face Recognition and Face Emotion Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 面部识别与面部情感识别
- en: 'In the previous chapter, we looked at how to detect objects such as a car,
    chair, cat, and dog, using Convolutional Neural Networks and the YOLO (You Only
    Look Once) algorithm. In this chapter, we will be detecting human faces. Apart
    from that, we will be looking at expressions of the human face, such as a human
    face seeming happy, neutral, sad, and so on. So, this chapter will be interesting,
    because we are going to focus on some of the latest techniques of face detection
    and face emotion recognition. We are dividing this chapter into two parts:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何使用卷积神经网络和YOLO（You Only Look Once）算法检测诸如汽车、椅子、猫和狗等物体。在本章中，我们将检测人脸。除此之外，我们还将研究人脸的表情，例如人脸看起来快乐、中性、悲伤等。因此，本章将很有趣，因为我们将关注一些最新的面部检测和面部情感识别技术。我们将把本章分为两部分：
- en: Face detection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部检测
- en: Face emotion recognition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部情感识别
- en: 'First, we will cover how face detection works, and after that, we will move
    on to the face emotion recognition part. In general, we will cover the following
    topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍面部检测的工作原理，然后我们将继续到面部情感识别部分。总的来说，我们将在本章中涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Setting up the coding environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: Understanding the concepts of face recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解面部识别的概念
- en: Approaches for implementing face recognition
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部识别实现的途径
- en: Understanding the dataset for face emotion recognition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解面部情感识别的数据集
- en: Understanding the concepts of face emotion recognition
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解面部情感识别的概念
- en: Building the face emotion recognition model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建面部情感识别模型
- en: Understanding the testing matrix
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: Problems with the existing approach
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有方法的局限性
- en: How to optimize the existing approach
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: Understanding the process for optimization
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解优化过程
- en: The best approach
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Implementing the best approach
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Summary
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Introducing the problem statement
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: We want to develop two applications. One application will recognize human faces,
    and the other will recognize the emotion of the human faces. We will discuss both
    of them in this section. We will look at what exactly we want to develop.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望开发两个应用。一个应用将识别人脸，另一个将识别人脸的情感。我们将在本节中讨论这两个应用。我们将探讨我们到底想开发什么。
- en: Face recognition application
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别应用
- en: 'This application should basically identify human faces from an image or a real-time
    video stream. Refer to the following photo; it will help you understand what I
    mean by identifying faces from an image or a real-time video stream:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用应基本能够从图像或实时视频流中识别出人脸。参考以下照片，它将帮助你理解我所说的从图像或实时视频流中识别人脸的含义：
- en: '![Face recognition application](img/B08394_10_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![面部识别应用](img/B08394_10_01.jpg)'
- en: 'Figure 10.1: Demo output for understanding the face recognition application'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：理解面部识别应用演示输出
- en: 'Images source: https://unsplash.com/photos/Q13lggdvtVY'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://unsplash.com/photos/Q13lggdvtVY
- en: As you can see in the preceding figure (Figure 10.1), when we provide any image
    as the input, in the first step, the machine can recognize the number of human
    faces present in the image. As the output, we can get cropped images of the faces.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图（图10.1）所示，当我们提供任何图像作为输入时，在第一步，机器可以识别图像中存在多少人脸。作为输出，我们可以得到人脸的裁剪图像。
- en: Beyond this, I also want the application to identify the name of the person
    based on the face. I think you are familiar with this kind of application. Let
    me remind you. When you upload an image on Facebook, the face recognition mechanism
    of Facebook immediately recognizes names of people who are part of that image,
    and suggests that you tag them in your image. We will develop similar functionality
    here in terms of the face recognition application. Now let's move on to another
    part of the application.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我还希望应用能够根据面部识别出人的名字。我想你熟悉这类应用。让我提醒你。当你将图片上传到Facebook时，Facebook的人脸识别机制会立即识别出图片中的人名，并建议你在图片中标记他们。在这里，我们将开发类似的功能，即面部识别应用。现在让我们继续到应用的另一部分。
- en: Face emotion recognition application
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部情感识别应用
- en: 'In this part of the application, we want to build an application that can detect
    the type of emotion on a human face. We will try to recognize the following seven
    emotions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用的部分，我们想要构建一个能够检测人脸情绪类型的应用程序。我们将尝试识别以下七种情绪：
- en: Anger
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 愤怒
- en: Disgust
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 厌恶
- en: Fear
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恐惧
- en: Happiness
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幸福
- en: Sadness
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 悲伤
- en: Surprise
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惊讶
- en: Neutral
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中性
- en: So, we will categorize facial emotions into these seven types. This kind of
    application will be helpful to know what kind of feeling the person is experiencing,
    and this insight will help in performing sentiment analysis, body language analysis,
    and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将面部情绪分为这七种类型。这种应用程序将有助于了解人们正在经历什么样的感受，这种洞察力将有助于进行情感分析、肢体语言分析等。
- en: Here, we will first build the face recognition application, and after that,
    we will move on to the face emotion recognition application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先构建人脸识别应用程序，然后继续构建人脸情绪识别应用程序。
- en: Setting up the coding environment
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置编码环境
- en: 'In this section, we will set up the coding environment for the face recognition
    application. We will look at how to install dependencies. We will be installing
    the following two libraries:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置人脸识别应用程序的编码环境。我们将查看如何安装依赖项。我们将安装以下两个库：
- en: dlib
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dlib
- en: face_recognition
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: face_recognition
- en: Let's begin the installation process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始安装过程。
- en: Installing dlib
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 dlib
- en: 'In order to install the dlib library, we need to perform the following steps.
    We can install this library either on a Linux operating system (OS), or on macOS.
    Let''s follow the stepwise instructions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装 dlib 库，我们需要执行以下步骤。我们可以在 Linux 操作系统（OS）或 macOS 上安装此库。让我们按照逐步说明进行：
- en: 'Download the source code of dlib by executing this command:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行此命令下载 dlib 的源代码：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now jump to the `dlib` directory by executing this command: `cd dlib`.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过执行此命令跳转到 `dlib` 目录：`cd dlib`。
- en: 'Now we need to build the main `dlib` library, so we need to execute the following
    commands stepwise:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要构建主要的`dlib`库，因此我们需要逐步执行以下命令：
- en: '`sudo mkdir build`.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sudo mkdir build`。'
- en: '`cd build`.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cd build`。'
- en: '`cmake .. -DDLIB_USE_CUDA=0 -DUSE_AVX_INSTRUCTIONS=1`.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cmake .. -DDLIB_USE_CUDA=0 -DUSE_AVX_INSTRUCTIONS=1`。'
- en: '`cmake --build`.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cmake --build`。'
- en: Once the project has been built successfully, you can move to the next installation
    steps. You also need to install **OpenCV**. The installation steps for OpenCV
    have already been given in [Chapter 10,](ch10.xhtml "Chapter 10. Face Recognition
    and Face Emotion Recognition") *Real-Time Object Detection*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦项目成功构建，你可以进入下一个安装步骤。你还需要安装 **OpenCV**。OpenCV 的安装步骤已在[第 10 章](ch10.xhtml "第
    10 章。实时目标检测")中给出。
- en: Installing face_recognition
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 face_recognition
- en: 'In order to install the `face_recognition` library, we need to execute the
    following commands:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装 `face_recognition` 库，我们需要执行以下命令：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding commands install the `face_recognition` library only if we have
    a perfectly installed `dlib`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令只有在`dlib`库完美安装的情况下才会安装`face_recognition`库。
- en: Once the preceding two libraries have been installed, we can move on to the
    next section, in which we will be discussing the key concepts of face recognition.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了前两个库，我们就可以进入下一部分，我们将讨论人脸识别的关键概念。
- en: Understanding the concepts of face recognition
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解人脸识别的概念
- en: 'In this section, we will look at the major concepts of face recognition. These
    concepts will include the following topics:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨人脸识别的主要概念。这些概念将包括以下主题：
- en: Understanding the face recognition dataset
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人脸识别数据集
- en: The algorithm for face recognition
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸识别算法
- en: Understanding the face recognition dataset
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解人脸识别数据集
- en: You may wonder why I haven't discussed anything related to the dataset until
    now. This is because I don't want to confuse you by providing all the details
    about the datasets of two different applications. The dataset that we will cover
    here is going to be used for **face recognition**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我直到现在还没有讨论与数据集相关的内容。这是因为我不想通过提供两个不同应用程序的数据集的所有细节来混淆你。我们将在这里覆盖的数据集将用于**人脸识别**。
- en: 'If you want to build a face recognition engine from scratch, then you can use
    following datasets:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要从头开始构建人脸识别引擎，则可以使用以下数据集：
- en: CAS-PEAL Face Dataset
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CAS-PEAL 人脸数据集
- en: Labeled Faces in the Wild
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 野生人脸标签
- en: Let's discuss them in further detail.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步详细讨论它们。
- en: CAS-PEAL Face Dataset
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CAS-PEAL 人脸数据集
- en: This is a huge dataset for face recognition tasks. It has various types of face
    images. It contains face images with different sources of variations, especially
    Pose, Emotion, Accessories, and Lighting (PEAL) for face recognition tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于面部识别任务的大型数据集。它包含各种类型的人脸图像。它包含具有不同变化来源的人脸图像，特别是用于面部识别任务的姿态、情绪、配饰和光照（PEAL）。
- en: 'This dataset contains 99,594 images of 1,040 individuals, of which 595 are
    male individuals and 445 are female individuals. The captured images of the individuals
    are with varying poses, emotions, accessories, and lighting. Refer to the following
    photo to see this. You can also refer to the following link if you want to see
    the sample dataset: [http://www.jdl.ac.cn/peal/index.html](http://www.jdl.ac.cn/peal/index.html).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含1,040个人的99,594张图片，其中595人是男性，445人是女性。捕捉到的个人图像具有不同的姿态、情绪、配饰和光照。请参考以下照片查看这一点。如果您想查看样本数据集，也可以参考以下链接：[http://www.jdl.ac.cn/peal/index.html](http://www.jdl.ac.cn/peal/index.html)。
- en: '![CAS-PEAL Face Dataset](img/B08394_10_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CAS-PEAL人脸数据集](img/B08394_10_02.jpg)'
- en: 'Figure 10.2: CAS-PEAL Face Dataset sample image'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：CAS-PEAL人脸数据集样本图像
- en: 'Image source: http://www.jdl.ac.cn/peal/Image/Pose_normal/NormalCombination-9-Cameras.jpg'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：http://www.jdl.ac.cn/peal/Image/Pose_normal/NormalCombination-9-Cameras.jpg
- en: 'You can download this dataset from the following link: [http://www.jdl.ac.cn/peal/download.htm](http://www.jdl.ac.cn/peal/download.htm)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接下载这个数据集：[http://www.jdl.ac.cn/peal/download.htm](http://www.jdl.ac.cn/peal/download.htm)
- en: Labeled Faces in the Wild
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LFW人脸数据集
- en: 'This dataset is also referred to as the LFW dataset. It is used in the `face_recognition`
    library. We will be using this library to build our face recognition application.
    This dataset contains more than 13,000 images of faces collected from the web.
    Each face is labeled with the name of the person pictured. So, the dataset is
    a labeled dataset. There are 1,680 people pictured with two or more distinct face
    images in the dataset. You can refer to the sample dataset using the following
    diagram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集也被称为LFW数据集。它在`face_recognition`库中被使用。我们将使用这个库来构建我们的面部识别应用。这个数据集包含了从网络上收集的超过13,000张人脸图片。每张人脸都标有图片中人物的姓名。因此，这个数据集是一个标记数据集。数据集中有1,680个人的两张或更多不同的人脸图像。您可以通过以下图表来参考样本数据集：
- en: '![Labeled Faces in the Wild](img/B08394_10_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![LFW人脸数据集](img/B08394_10_03.jpg)'
- en: 'Figure 10.3: Sample images from the LFW dataset'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：LFW数据集的样本图像
- en: 'Image source: http://vis-www.cs.umass.edu/lfw/person/AJ_Cook.html'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：http://vis-www.cs.umass.edu/lfw/person/AJ_Cook.html
- en: Note
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can find out more about this dataset by clicking on [http://vis-www.cs.umass.edu/lfw/index.html](http://vis-www.cs.umass.edu/lfw/index.html).
    You can also download the dataset by using the same link. You can refer to the
    Caltech 10,000 web faces dataset as well by clicking on [http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/](http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/).
    You should also refer to the INRIA Person Dataset, which will be quite useful.
    The link for the INRIA Person Dataset is [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击[http://vis-www.cs.umass.edu/lfw/index.html](http://vis-www.cs.umass.edu/lfw/index.html)了解更多关于这个数据集的信息。您也可以通过相同的链接下载数据集。您还可以通过点击[http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/](http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/)来参考Caltech
    10,000网络人脸数据集。您还应该参考INRIA人脸数据集，这将非常有用。INRIA人脸数据集的链接是[http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/).
- en: In order to build the face recognition application, we will be using the `face_recognition`
    library. We are using the pre-trained model provided by this library via its API.
    We will certainly explore the algorithm and the concept behind this pre-trained
    model and library. So let's begin!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建面部识别应用，我们将使用`face_recognition`库。我们正在使用这个库通过其API提供的预训练模型。我们肯定会探索这个预训练模型和库背后的算法和概念。所以，让我们开始吧！
- en: Algorithms for face recognition
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部识别算法
- en: In this section, we will look at the core algorithm that is used for face recognition.
    The name of the algorithm is **Histogram of Oriented Gradients** (**HOG**). We
    will see how HOG is used in face recognition tasks. A face recognition (FR) task
    is basically a classification task, as we are detecting the face from the image
    as well as trying to identify the person's name with the help of the person's
    face. HOG is a good option to try out.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看用于人脸识别的核心算法。该算法的名称是**方向梯度直方图**（**HOG**）。我们将了解HOG在人脸识别任务中的应用。人脸识别（FR）任务基本上是一个分类任务，因为我们不仅从图像中检测人脸，还试图通过人脸识别出人的名字。HOG是一个很好的尝试选项。
- en: The other approach is to use the Convolutional Neural Network (CNN). In this
    section, we will also cover CNN for the FR task. So, let's start out with HOG!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用卷积神经网络（CNN）。在本节中，我们还将介绍CNN在人脸识别任务中的应用。因此，让我们从HOG开始吧！
- en: Histogram of Oriented Gradients (HOG)
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方向梯度直方图（HOG）
- en: The HOG algorithm is one of the best approaches for state-of-the-art results
    for face recognition. The HOG method was introduced by Dalal and Triggs in their
    seminal 2005 paper, available at [http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).
    The HOG image descriptor and a linear Support Vector Machine can be used to train
    highly accurate classifiers that can classify human detectors. So, HOG can be
    applied to an FR task as well. First, we will cover the basic intuition behind
    the algorithm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HOG算法是用于人脸识别的最佳方法之一。HOG方法由Dalal和Triggs在他们的开创性2005年论文中提出，该论文可在[http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf)找到。HOG图像描述符和线性支持向量机可以用来训练高度准确的分类器，这些分类器可以分类人脸检测器。因此，HOG也可以应用于人脸识别任务。首先，我们将介绍算法背后的基本直觉。
- en: 'HOG is a type of feature descriptor. A feature descriptor is a representation
    of an image that simplifies the image by extracting useful information and ignoring
    the information. Here, our focus will be on the faces only. So, we will be ignoring
    other objects, if there are any. The LWF dataset has less noise, so the task of
    generating an accurate feature descriptor is comparatively easy. The step-by-step
    process is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: HOG是一种特征描述符。特征描述符是一种通过提取有用信息并忽略其他信息来简化图像的图像表示。在这里，我们的重点将放在人脸上。因此，如果有其他物体，我们将忽略它们。LWF数据集噪声较少，因此生成准确特征描述符的任务相对容易。逐步过程如下：
- en: '**Step 1**: In order to find the faces in image, we will start by converting
    our color image into black and white, because we don''t need color data to recognize
    the face. Refer to the following diagram:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步**：为了在图像中找到人脸，我们首先将彩色图像转换为黑白图像，因为我们不需要颜色数据来识别人脸。参考以下图表：'
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_04.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_04.jpg)'
- en: 'Figure 10.4: Converting a color image to a black and white image'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：将彩色图像转换为黑白图像
- en: '**Step 2**: In this step, we will look at every single pixel in our image at
    a time. For every single pixel, we want to look at pixels that directly surround
    it. Refer to the following diagram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步**：在这个步骤中，我们将一次查看图像中的每个单独的像素。对于每个单独的像素，我们希望查看直接围绕它的像素。参考以下图表：'
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_05.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_05.jpg)'
- en: 'Figure 10.5: Process of scanning every single pixel of the image'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：扫描图像中每个单独像素的过程
- en: 'Step 3: Here, our goal is to find out how dark the current pixel is with respect
    to the pixels directly surrounding it. We need to draw an arrow that indicates
    the direction in which the pixels of the image are getting darker. In order to
    achieve this, we scan the entire image. Refer to the following diagram:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：在这里，我们的目标是找出当前像素相对于其直接周围的像素有多暗。我们需要画一个箭头，指示图像像素变暗的方向。为了实现这一点，我们需要扫描整个图像。参考以下图表：
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_06.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_06.jpg)'
- en: 'Figure 10.6: An arrow direction from a light pixel to a dark pixel'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：从亮像素到暗像素的箭头方向
- en: As you can see in the preceding diagram, we have considered a pixel and the
    other pixels surrounding it. By looking at the pixels, we can easily figure out
    that the arrow head is pointing toward the darker pixel.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们考虑了一个像素及其周围的像素。通过观察像素，我们可以轻松地判断箭头头指向较暗的像素。
- en: 'Step 4: If we repeat this process for every single pixel in the image, then
    we will end up with every pixel being replaced with arrows. These arrows are called
    *gradients*. These *gradients* show the flow from light to dark pixels across
    the entire image. Refer to the following diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：如果我们对图像中的每个像素重复此过程，那么最终每个像素都将被箭头替换。这些箭头被称为*梯度*。这些*梯度*显示了整个图像中从亮到暗像素的流动。参考以下图示：
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_07.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_07.jpg)'
- en: 'Figure 10.7: Gradient arrows for the entire image'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：整个图像的梯度箭头
- en: In the preceding diagram, you can see the kind of output we get after generating
    a gradient for the input image. The scanning of the entire image might seem like
    a random thing to do, but there is a reason for replacing the pixels with gradients.
    If we analyze the original pixel values directly, then really dark images and
    really light images of the same person will have totally different pixel values,
    which make things more complicated when we try to recognize the face of the person.
    Here, we are considering the direction in which the brightness of the pixel changes.
    We find that both really dark images and really light images of the same person
    will end up with the exact same representation for the face. This kind of representation
    will be easy for us to deal with in terms of the face recognition task. This is
    the main reason for generating the gradient for the entire image. There is one
    challenge, though, which we will discuss in the next step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您可以看到在为输入图像生成梯度后的输出类型。扫描整个图像可能看起来像是一件随机的事情，但用梯度替换像素是有原因的。如果我们直接分析原始像素值，那么同一个人的非常暗和非常亮的图像将具有完全不同的像素值，这在我们尝试识别该人的面部时会使事情变得更加复杂。在这里，我们考虑像素亮度变化的方向。我们发现，同一个人的非常暗和非常亮的图像最终都会得到面部完全相同的表示。这种表示对我们处理面部识别任务来说将很容易处理。这就是生成整个图像梯度的主要原因。然而，尽管如此，我们将在下一步讨论一个挑战。
- en: 'Step 5: Saving the gradient for every single pixel gives us too much information,
    and there is the chance that we may use this amount of information inefficiently.
    So, we need to obtain the bare minimum information that we will be using for the
    FR task. We will achieve this by just considering the basic flow of lightness
    or darkness at a higher level, so we can see the basic pattern of the image. The
    process for achieving this is given in step 6.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：为每个像素保存梯度给我们提供了太多的信息，而且我们可能会以低效的方式使用这些信息。因此，我们需要获得我们将用于FR任务的最基本信息。我们将通过只考虑更高层次上的亮度和暗度的基本流动来实现这一点，这样我们就可以看到图像的基本模式。实现这一点的过程在第6步中给出。
- en: 'Step 6: We will break up this image into small squares of 16 x16 pixels each.
    In each square, we will count the number of gradient points in each major direction,
    which means we will count how many arrows point up, point down, point right, point
    left, and so on. After counting this, we will replace that square in the image
    with the arrow directions that were the strongest. The end result is that we convert
    the original image into a simple representation that captures the basic structure
    of a face. Refer to the following diagram:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：我们将此图像分解成每个16 x 16像素的小方块。在每个方块中，我们将计算每个主要方向上的梯度点数，这意味着我们将计算有多少箭头向上、向下、向右、向左等。在计数之后，我们将用最强的箭头方向替换图像中的那个方块。最终结果是，我们将原始图像转换成一个简单的表示，该表示捕捉到了面部的基本结构。参考以下图示：
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_08.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_08.jpg)'
- en: 'Figure 10.8: Simple face representation in the HOG version'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：HOG版本中的简单面部表示
- en: This kind of representation is easy to process for the FR task; it's called
    the HOG version of the image. It represents the features that we will consider
    in the FR task, and that is why this representation is referred to as an HOG features
    descriptor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示对于FR任务来说很容易处理；它被称为图像的HOG版本。它代表了我们在FR任务中要考虑的特征，这就是为什么这种表示被称为HOG特征描述符。
- en: 'Step 7: In order to find out the faces in this HOG image, we have to find out
    the part of our image that looks the most similar to a known HOG pattern that
    was extracted from a bunch of other training faces. Refer to the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：为了找出这个HOG图像中的面部，我们必须找出我们的图像中看起来最像从其他训练面部中提取的已知HOG模式的部分。参考以下图示：
- en: '![Histogram of Oriented Gradients (HOG)](img/B08394_10_09.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![方向梯度直方图（HOG）](img/B08394_10_09.jpg)'
- en: 'Figure 10.9: Process of recognizing a face using the HOG version of our image'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：使用我们图像的HOG版本识别人脸的过程
- en: Using this technique, we can easily recognize the faces in any image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这项技术，我们可以轻松地识别任何图像中的人脸。
- en: Convolutional Neural Network (CNN) for FR
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于FR的卷积神经网络（CNN）
- en: 'In this section, we will look at how a CNN can be used to recognize the faces
    from the images. This section is divided into two parts:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何使用CNN从图像中识别人脸。本节分为两部分：
- en: Simple CNN architecture
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的CNN架构
- en: Understanding how a CNN works for FR
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CNN在FR中的应用
- en: Simple CNN architecture
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单的CNN架构
- en: 'I don''t want to get too deep into how a CNN works, as I have already provided
    most of the necessary details in [Chapter 9](ch09.xhtml "Chapter 9. Building a
    Real-Time Object Recognition App"), *Building Real-Time Object Detection*; however,
    I want to remind you about some necessary stuff regarding CNN. First, refer to
    the following diagram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想深入探讨CNN是如何工作的，因为我已经在[第9章](ch09.xhtml "第9章. 构建实时物体识别应用")*构建实时物体检测*中提供了大部分必要的细节；然而，我想提醒你一些关于CNN的必要信息。首先，参考以下图表：
- en: '![Simple CNN architecture](img/B08394_10_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![简单的CNN架构](img/B08394_10_10.jpg)'
- en: 'Figure 10.10: The CNN architecture'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：CNN架构
- en: As you can see in the preceding diagram, there is a convolutional layer, a pooling
    layer, a fully connected layer, and an output layer. There are different activation
    functions, penalties, and SoftMax functions involved. This is high-level information.
    For this FR task, we can use three convolutional and pooling layers with ReLU
    as activation functions. You can add more layers, but it will become more computationally
    expensive to train.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的图表中所见，有一个卷积层、一个池化层、一个全连接层和一个输出层。涉及不同的激活函数、惩罚和SoftMax函数。这是高级信息。对于这个FR任务，我们可以使用三个卷积层和池化层，使用ReLU作为激活函数。您可以添加更多层，但这会使训练的计算成本更高。
- en: Understanding how CNN works for FR
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解CNN在FR中的应用
- en: 'Intuitively, the CNN model performs the following steps in order to build a
    good FR application. The basic process is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，CNN模型按照以下步骤构建一个良好的FR应用。基本过程如下：
- en: 'Step 1: Look at a picture. Crop the images that contain only faces.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：观察一张图片。裁剪只包含人脸的图像。
- en: 'Step 2: Now, in this step, we focus on a face and try to understand that even
    if a face is turned in a weird direction, or an image is taken in bad lighting,
    we need to identify the proper placement of the face in this kind of image. Step
    3 will give us a solution.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：现在，在这个步骤中，我们专注于人脸，并试图理解即使人脸朝向奇怪的方向，或者图像是在不良光照下拍摄的，我们仍需要识别这类图像中人脸的正确位置。第3步将为我们提供解决方案。
- en: 'Step 3: In order to identify the face from any image, whether that image is
    taken in bad lighting conditions, or the orientation of the face seems totally
    weird, we need to identify the face. To achieve that, we pick out unique features
    of the face that can be used to tell us something unique about the person''s face.
    With the help of these unique features, we can identify the face of the same person,
    as well as the face of the different persons. These features can include how big
    the eyes are, how long the face is, and so on. There are 68 specific points that
    should be considered; and they are called landmarks. These points are defined
    based on the face landmark estimation. Refer to the following paper to get more
    details about this: [http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf](http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf).
    Take a look at the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：为了从任何图像中识别人脸，无论图像是在不良光照条件下拍摄的，还是人脸的朝向看起来非常奇怪，我们需要识别人脸。为了实现这一点，我们挑选出人脸的独特特征，这些特征可以告诉我们关于人脸的独特信息。借助这些独特特征，我们可以识别同一个人的脸，以及不同人的脸。这些特征可以包括眼睛的大小、脸的长度等等。有68个特定的点需要考虑；它们被称为关键点。这些点是基于人脸关键点估计定义的。请参考以下论文以获取更多详细信息：[http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf](http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf)。请查看以下图表：
- en: '![Understanding how CNN works for FR](img/B08394_10_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![理解CNN在FR中的应用](img/B08394_10_11.jpg)'
- en: 'Figure 10.11: 68 points for face landmark estimation'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：用于人脸关键点估计的68个点
- en: 'Step 4: We need to identify the person''s face with their name, so in order
    to achieve this, we will compare the unique features of that face with all the
    people we already know in order to determine the person''s name. Suppose you have
    added the images for Bill Gates, Barack Obama, and so on. You have generated the
    unique features for their faces, and now we will compare their unique facial features
    with these already generated facial features, and if the features are similar,
    then we get to know the name of the person, which is Barack Obama or Bill Gates
    in the given image. The identification of the person based on their facial features
    is a classification problem, which can easily be solved by CNN. We are generating
    a face embedding vector of size 128 measurements. As an input, we should provide
    this face embedding vector. Once we complete the training, our application will
    be ready to identify the person''s name.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：我们需要通过名字识别某人的面孔，因此为了实现这一点，我们将比较该面孔的独特特征与我们已经知道的所有人，以确定该人的名字。假设你已经添加了比尔·盖茨、巴拉克·奥巴马等人的图片。你已经为他们生成了独特的面部特征，现在我们将比较这些已经生成的面部特征，如果特征相似，那么我们就能知道图像中的人的名字，即巴拉克·奥巴马或比尔·盖茨。基于面部特征的识别是一个分类问题，可以通过CNN轻松解决。我们正在生成一个128维度的面孔嵌入向量。作为输入，我们应该提供这个面孔嵌入向量。一旦我们完成训练，我们的应用程序将准备好识别人的名字。
- en: 'Step 5: The trained model looks at all the faces we have measured in the past,
    and looks at the person who has the closest measurements to our faces'' measurements.
    That is our match.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：训练好的模型会查看我们过去测量过的所有面孔，并找到与我们的面孔测量值最接近的人。那就是我们的匹配对象。
- en: The preceding approach is for our CNN-based FR and real-time face recognition
    task. We have covered the basic concepts and the idea behind the algorithms that
    are used in the FR task. Now let's start the implementation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法是为我们的基于CNN的FR和实时人脸识别任务。我们已经涵盖了FR任务中使用的算法的基本概念和背后的思想。现在让我们开始实现。
- en: Approaches for implementing face recognition
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现人脸识别的方法
- en: 'In this section, we will be implementing the FR application. We are using the
    `face_recognition` library. We have already configured the environment for that.
    We will be implementing the following approaches here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现FR应用程序。我们正在使用`face_recognition`库。我们已经为它配置了环境。我们将在这里实现以下方法：
- en: The HOG-based approach
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于HOG的方法
- en: The CNN-based approach
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于CNN的方法
- en: Real-time face recognition
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时人脸识别
- en: Now let's start coding!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始编码！
- en: Implementing the HOG-based approach
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基于HOG的方法
- en: 'In this approach, we are using the HOG algorithm to find out two things: the
    total number of faces in the image, and the paces. We are using the API of the
    `face_recgnition` library. You can find the code by clicking on the following
    GitHub link: [https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py).
    The code snippet is provided in the following diagram:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们使用HOG算法找出两件事：图像中的面孔总数和步态。我们使用`face_recognition`库的API。您可以通过点击以下GitHub链接找到代码：[https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_example.py)。代码片段如下所示：
- en: '![Implementing the HOG-based approach](img/B08394_10_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![基于HOG的方法实现](img/B08394_10_12.jpg)'
- en: 'Figure 10.12: Code snippet for the HOG-based approach for FR'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：基于HOG的FR方法代码片段
- en: 'In the preceding diagram, we have given an image as input, and with the help
    of the API of the `face_recognition` library, we can find the pixel location of
    the face in an image. Here, we will also count how many faces there are in an
    image, and with the help of the `Image` library, we can crop the faces from the
    given image. You can find the output of this script in the following figure:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们提供了一个图像作为输入，借助`face_recognition`库的API，我们可以找到图像中面孔的像素位置。在这里，我们还将计算图像中有多少个面孔，借助`Image`库，我们可以从提供的图像中裁剪面孔。您可以在以下图中找到此脚本的输出：
- en: '![Implementing the HOG-based approach](img/B08394_10_13.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![基于HOG的方法实现](img/B08394_10_13.jpg)'
- en: 'Figure 10.13: Output of the HOG-based approach for FR'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：基于HOG的FR方法输出
- en: 'Refer to the cropped face output in the following screenshot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下截图中的裁剪面孔输出：
- en: '![Implementing the HOG-based approach](img/B08394_10_14.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![实现基于HOG的方法](img/B08394_10_14.jpg)'
- en: 'Figure 10.14: Cropped face output'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：裁剪后的面孔输出
- en: As you can see in the last output diagram with the help of a simple API, we
    can build a simple face recognition application. This approach is kind of a baseline
    approach for us.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在最后一个输出图中所见，通过简单的API，我们可以构建一个简单的人脸识别应用程序。这种方法对我们来说是一种基线方法。
- en: Now let's move on to the CNN-based approach. The HOG-based approach is less
    accurate compared to the CNN-based approach. If we use GPU for the CNN-based approach,
    then we can train the model in a less amount of time. Now let's look at the code
    for the CNN-based approach.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向基于CNN的方法。与基于HOG的方法相比，基于CNN的方法更准确。如果我们为基于CNN的方法使用GPU，那么我们可以以更短的时间训练模型。现在让我们看看基于CNN方法的代码。
- en: Implementing the CNN-based approach
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基于CNN的方法
- en: 'In this approach, we will be using the `face_recognition` library, where we
    specified the name of the model. Our model''s name is `cnn`. This particular approach
    will load the pre-trained model via the `face_recognition` API, and we can generate
    a more accurate output. You can find the code by clicking on the following GitHub
    link: [https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py).
    Refer to the code snippet given in the following figure:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们将使用`face_recognition`库，其中我们指定了模型的名称。我们模型的名称是`cnn`。这个特定的方法将通过`face_recognition`
    API加载预训练模型，我们可以生成更准确的结果。您可以通过点击以下GitHub链接找到代码：[https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py](https://github.com/jalajthanaki/Face_recognition/blob/master/face_detection_GPU_example.py)。请参考以下图中的代码片段：
- en: '![Implementing the CNN-based approach](img/B08394_10_15.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![实现基于CNN的方法](img/B08394_10_15.jpg)'
- en: 'Figure 10.15: Code snippet for the CNN-based approach for FR'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：基于CNN方法的FR代码片段
- en: 'Here, the implementation code is almost the same as earlier, but the difference
    is that we have provided the model name as `cnn` during the API call. You can
    see the output of this implementation in the following diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，实现代码几乎与之前相同，但不同之处在于我们在API调用期间提供了模型名称`cnn`。您可以在以下图中看到这个实现的输出：
- en: '![Implementing the CNN-based approach](img/B08394_10_16.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![实现基于CNN的方法](img/B08394_10_16.jpg)'
- en: 'Figure 10.16: Output of the CNN-based approach for FR'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：基于CNN方法的FR输出
- en: The output of this implementation is the same as the last one. This version
    of the implementation is fast, and has better accuracy. Now let's try to implement
    the FR task for a real-time video stream.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现的输出与上一个相同。这个版本的实现速度快，并且准确性更高。现在让我们尝试为实时视频流实现FR任务。
- en: Implementing real-time face recognition
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现实时人脸识别
- en: 'In this section, we will implement the FR task for a real-time video stream.
    We will try to identify the name of the person who has appeared in the video.
    Doesn''t that sound interesting? Let''s begin. You can find the code by clicking
    on the following GitHub link: [https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py](https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为实时视频流实现FR任务。我们将尝试识别视频中出现的个人的名字。这难道不很有趣吗？让我们开始吧。您可以通过点击以下GitHub链接找到代码：[https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py](https://github.com/jalajthanaki/Face_recognition/blob/master/Real_time_face_detection.py)。
- en: 'Again, we are using the API of the `face_recognition` library. We are using
    `OpenCV` as well. First of all, we need to feed the sample image of the person
    with the person''s name, so that the machine can learn the name of the person
    and identify it during the testing. In this implementation, I have fed the image
    of Barack Obama and Joe Biden. You can add your image as well. If the face features
    are familiar and match with the already-fed images, then the script returns the
    name of the person, and if the face features are not familiar to the given image,
    then that person''s face is tagged as *Unknown.* Refer to the implementation in
    the following diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们正在使用`face_recognition`库的API。我们也在使用`OpenCV`。首先，我们需要提供带有个人名字的个人的样本图像，这样机器就可以在学习过程中识别出这个人的名字，并在测试时识别它。在这个实现中，我提供了巴拉克·奥巴马和乔·拜登的图像。您也可以添加您的图像。如果人脸特征熟悉并且与已提供的图像匹配，则脚本将返回该人的名字；如果人脸特征与给定图像不熟悉，则该人的脸被标记为*未知*。请参考以下图中的实现：
- en: '![Implementing real-time face recognition](img/B08394_10_17.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![实现实时人脸识别](img/B08394_10_17.jpg)'
- en: 'Figure 10.17: Implementation of real-time FR'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17：实时FR的实现
- en: 'As you can see in the preceding code, I have provided sample images of Barack
    Obama and Joe Biden. I have also provided the names of the people whose images
    I''m feeding to my script. I have used the same face recognition API for detecting
    and recognizing the face in the video stream. When you run the script, your webcam
    streams your real-time video and this script detects the face, and if you provide
    the image of the person that the machine knows, then it identifies it correctly
    this time as well. Refer to the following diagram:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我提供了巴拉克·奥巴马和乔·拜登的样本图像。我还提供了我将图像输入脚本的人的名字。我使用了相同的面部识别API来检测和识别视频流中的面部。当你运行脚本时，你的网络摄像头会实时传输视频，此脚本会检测面部，如果你提供机器已知的该人的图像，那么这次它也能正确识别。
- en: '![Implementing real-time face recognition](img/B08394_10_18.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![实现实时面部识别](img/B08394_10_18.jpg)'
- en: 'Figure 10.18: Output of the real-time FR'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：实时FR的输出
- en: As you can see, I have not provided my image to the machine, so it identifies
    me as **Unknown**, whereas it can identify Barack Obama's image. You can also
    find the animated image by clicking on [https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif](https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我没有向机器提供我的图像，因此它将我识别为**未知**，而它可以识别巴拉克·奥巴马的图像。您也可以通过点击[https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif](https://github.com/jalajthanaki/Face_recognition/blob/master/img/Demo.gif)找到动画图像。
- en: We are finished with the first part of the chapter, which entails developing
    an application that can recognize human faces, as well as identify the name of
    the person based on their face. We implemented three different variations of FR.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了本章的第一部分，即开发一个可以识别人类面部，并根据面部识别出人的名字的应用程序。我们实现了三种不同的面部识别（FR）变体。
- en: In the upcoming section, we will look at how to develop a face emotion recognition
    (FER) application. We need different kinds of datasets to build this application,
    so we will start by understanding a dataset for FER.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何开发一个面部表情识别（FER）应用。为了构建此应用，我们需要不同类型的数据集，因此我们将从理解一个用于FER的数据集开始。
- en: Understanding the dataset for face emotion recognition
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解面部表情识别数据集
- en: To develop an FER application, we are considering the `FER2013` dataset. You
    can download this dataset from [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data).
    We need to know the basic details about this dataset. The dataset credit goes
    to Pierre-Luc Carrier and Aaron Courville as part of an ongoing research project.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个面部表情识别（FER）应用，我们正在考虑使用`FER2013`数据集。您可以从[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)下载此数据集。我们需要了解关于此数据集的基本细节。数据集归功于Pierre-Luc
    Carrier和Aaron Courville，作为持续研究项目的一部分。
- en: 'This dataset consists of 48x48 pixel grayscale images of faces. The task is
    to categorize each of the faces based on the emotion that has been shown in the
    image in the form of facial expressions. The seven categories are as follows,
    and for each of them there is a numeric label that expresses the category of the
    emotion:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含48x48像素灰度人脸图像。任务是根据图像中显示的表情将每个面部分类。以下为七个类别，每个类别都有一个表示情绪类别的数字标签：
- en: '`0` = Anger'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` = 生气'
- en: '`1` = Disgust'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` = 厌恶'
- en: '`2` = Fear'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2` = 恐惧'
- en: '`3` = Happiness'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3` = 快乐'
- en: '`4` = Sadness'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4` = 悲伤'
- en: '`5` = Surprise'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`5` = 惊讶'
- en: '`6` = Neutral'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6` = 中立'
- en: 'This dataset has the `fer2013.csv` file. This csv file will be used as our
    training dataset. Now let''s look at the attributes of the file. There are three
    columns in the file, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含`fer2013.csv`文件。此csv文件将用作我们的训练数据集。现在让我们看看文件的属性。文件中有三列，如下所示：
- en: '**Emotion**: This column contains the numeric label of the facial expression.
    For fear, this column contains the value `2`; for sadness, this column contains
    the value `4`, and so on.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情绪**: 这列包含面部表情的数字标签。对于恐惧，此列包含值为`2`；对于悲伤，此列包含值为`4`，依此类推。'
- en: '**Pixels**: This column contains the pixel values of the individual images.
    It represents the matrix of pixel values of the image.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**像素**: 这列包含单个图像的像素值。它代表图像的像素值矩阵。'
- en: '**Usage**: This column contains the general tag about whether the particular
    data record will be used for training purposes or for testing purposes. There
    are three labels that are part of this column, and those are *Training*, *PublicTest*,
    and *PrivateTest*. For training purposes, there are 28,709 data samples. The public
    test set consist of 3,589 data samples, and the private test set consists of another
    3,589 data samples.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用法**：此列包含有关特定数据记录是否用于训练目的或测试目的的一般标签。此列有三个标签，分别是*Training*（训练）、*PublicTest*（公共测试）和*PrivateTest*（私有测试）。用于训练目的的数据样本有28,709个。公共测试集包含3,589个数据样本，私有测试集也包含3,589个数据样本。'
- en: Now let's cover the concepts that will help us develop the FER application.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解有助于我们开发FER应用的概念。
- en: Understanding the concepts of face emotion recognition
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解面部情感识别的概念
- en: We are using Convolutional Neural Network (CNN) to develop the FER application.
    Earlier, we looked at the basic architecture of CNN. In order to develop FER applications,
    we will be using the following CNN architecture and optimizer. We are building
    CNN that is two layers deep. We will be using two fully connected layers and the
    SoftMax function to categorize the facial emotions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用卷积神经网络（CNN）来开发FER应用。之前，我们看到了CNN的基本架构。为了开发FER应用，我们将使用以下CNN架构和优化器。我们正在构建一个两层深的CNN。我们将使用两个全连接层和SoftMax函数来分类面部情感。
- en: 'We will be using several layers made of the convolutional layer, followed by
    the ReLU (Rectified Linear Unit) layer, followed by the max pooling layer. Refer
    to the following diagram, which will help you conceptualize the arrangement of
    the CNN layers. Let''s look at the working of CNN. We will cover the following
    layers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由卷积层构成的多层结构，随后是ReLU（修正线性单元）层，再之后是最大池化层。参考以下图表，它将帮助您理解CNN层的排列。让我们看看CNN的工作原理。我们将涵盖以下层：
- en: The convolutional layer
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: The ReLU layer
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU层
- en: The pooling layer
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: The fully connected layer
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: The SoftMax layer
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SoftMax层
- en: Understanding the convolutional layer
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积层
- en: In this layer, we will feed our image in the form of pixel values. We are using
    a sliding window of 3 x 3 dimension, which slides through the entire image. The
    area that is chosen by the sliding window is called the *receptive field*. It
    is the patch of the image. A sliding window is just the matrix of a 3 x 3 dimension,
    and it can scan the entire image. By using the sliding window, we scan nine pixel
    values of the image using the matrix of a 3 x 3 dimension. This receptive field
    or a piece of the image is the input of the convolutional network.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个层中，我们将以像素值的形式输入我们的图像。我们使用一个3 x 3维度的滑动窗口，它在整个图像上滑动。滑动窗口选中的区域被称为*感受野*。它是图像的一部分。滑动窗口只是一个3
    x 3维度的矩阵，它可以扫描整个图像。通过使用滑动窗口，我们使用3 x 3维度的矩阵扫描图像的九个像素值。这个感受野或图像的一部分是卷积网络的输入。
- en: 'Refer to the following diagram:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表：
- en: '![Understanding the convolutional layer](img/B08394_10_19.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![理解卷积层](img/B08394_10_19.jpg)'
- en: 'Figure 10.19: Sliding window and receptive field'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19：滑动窗口和感受野
- en: 'This receptive field carries the values in the form of pixel values of the
    input image. These pixel values are called feature maps, features, filter, weight
    matrix, or kernel. We already had a matrix of 3 x 3, which is referred to as the
    feature map. The size of the feature map is one of the hyperparameters. We can
    take the n x n matrix, where n >=1\. Here, we have considered a 3 x 3 matrix to
    understand the operation. Now it''s time to perform a simple math operation, the
    steps for which are as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个感受野以输入图像的像素值形式携带值。这些像素值被称为特征图、特征、滤波器、权重矩阵或核。我们已经有了一个3 x 3的矩阵，被称为特征图。特征图的大小是一个超参数。我们可以取n
    x n矩阵，其中n >= 1。在这里，我们考虑了一个3 x 3的矩阵来理解操作。现在进行简单的数学运算，步骤如下：
- en: 'Step 1: Get the feature map. Here, the feature map means the image patch that
    is generated in the form of a receptive field.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：获取特征图。在这里，特征图指的是以感受野形式生成的图像块。
- en: 'Step 2: We need to perform the dot product between the feature map and the
    entire image. Again, we scan the entire image using the sliding window, and generate
    the value of dot products.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：我们需要在特征图和整个图像之间执行点积。再次，我们使用滑动窗口扫描整个图像，并生成点积的值。
- en: 'Step 3: We need to sum up all the values that we get from obtaining the dot
    product.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：我们需要将我们从点积中获得的所有值加起来。
- en: 'Step 4: We need to divide the value of the summation for the dot product by
    the total number of pixels in the feature. In this explanation, we have a total
    of nine pixels, so we will divide the sum by 9\. As an output, we get the image
    that is referred to as a feature image.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：我们需要将点积求和的值除以特征中的总像素数。在这个解释中，我们总共有九个像素，所以我们将总和除以9。作为输出，我们得到被称为特征图像的图像。
- en: 'Refer to the following diagram:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表：
- en: '![Understanding the convolutional layer](img/B08394_10_20.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![理解卷积层](img/B08394_10_20.jpg)'
- en: 'Figure 10.20: Math operation for the convolutional layer'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20：卷积层的数学运算
- en: We will repeat this operation for almost all the possible positions of the image,
    and we will try out all the possible combinations, which is the reason why this
    operation is referred to as convolutional. Now let's look at the ReLU layer.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复这个操作，几乎针对图像的所有可能位置，并尝试所有可能的组合，这就是为什么这个操作被称为卷积操作。现在让我们看看ReLU层。
- en: Understanding the ReLU layer
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解ReLU层
- en: 'This layer basically introduces nonlinearity to the convolutional network.
    Here, we need to use the `activation` function. For this application, we have
    chosen the Rectified Linear Unit as the activation function. This layer performs
    some sort of normalization to our feature map. Let''s see what it does to our
    feature map:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层基本上为卷积网络引入了非线性。在这里，我们需要使用`激活`函数。对于这个应用，我们选择了修正线性单元作为激活函数。这个层对我们的特征图进行某种归一化。让我们看看它对特征图做了什么：
- en: 'Step 1: This layer takes the feature map as the input that is generated by
    the convolutional layer.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：这个层以卷积层生成的特征图作为输入。
- en: 'Step 2: This layer just converts the negative values into zero.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：这个层只是将负值转换为零。
- en: 'Refer the following diagram:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表：
- en: '![Understanding the ReLU layer](img/B08394_10_21.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![理解ReLU层](img/B08394_10_21.jpg)'
- en: 'Figure 10.21: Operations performed by the ReLU layer'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21：ReLU层的操作
- en: Now it is time to look at the pooling layer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看池化层了。
- en: Understanding the pooling layer
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解池化层
- en: 'Using this layer, we shrink the image. We will be using the max pooling operation
    here. We need to perform the following steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个层，我们缩小图像。我们将在这里使用最大池化操作。我们需要执行以下步骤：
- en: 'Step 1: We need to feed the feature map as the input, and this time, the output
    of the ReLU layer is given as the input to this layer.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：我们需要将特征图作为输入，这次，ReLU层的输出被作为这个层的输入。
- en: 'Step 2: We need to pick up the window size. Generally, we pick up the size
    of 2 x 2 pixels or 3 x 3 pixels. We will be taking 2 x 2 as our window size.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：我们需要选择窗口大小。通常，我们选择2 x 2像素或3 x 3像素的大小。我们将2 x 2作为我们的窗口大小。
- en: 'Step 3: We scan the entire image based on this window size, and we will take
    the maximum value from four pixel values.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：我们将根据这个窗口大小扫描整个图像，并从四个像素值中取最大值。
- en: 'You can understand the operation by referring to the following diagram:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过参考以下图表来理解这个操作：
- en: '![Understanding the pooling layer](img/B08394_10_22.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![理解池化层](img/B08394_10_22.jpg)'
- en: 'Figure 10.22: Operations performed by the max pooling layer'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.22：最大池化层的操作
- en: We can do deep stacking of these layers as deep as we need. You can repeat the
    convolutional, ReLU, and pooling layers an n number of times in order to make
    CNN deep.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些层堆叠得尽可能深。你可以重复卷积、ReLU和池化层n次，以使CNN变深。
- en: Understanding the fully connected layer
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解全连接层
- en: 'The output of all the layers is passed on to the fully connected layer. This
    layer has a voting mechanism. All image patches are considered for votes. The
    image patches of 2 x 2 matrices are arranged in a horizontal way. The vote depends
    on how strongly a value predicts the face expression. If certain values of this
    layer are high, it means they are close to 1, and if certain values of this layer
    are low, it means they are close to 0\. For each category, certain cell values
    are close to 1 and others are 0, and this way, our network will predict the category.
    Refer to the following diagram:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 所有层的输出都传递到全连接层。这一层有一个投票机制。所有图像块都被考虑在内。2 x 2矩阵的图像块以水平方式排列。投票取决于一个值预测面部表情的强度。如果这一层的某些值很高，这意味着它们接近1，如果某些值很低，这意味着它们接近0。对于每个类别，某些单元格的值接近1，而其他值是0，这样我们的网络就会预测类别。请参考以下图表：
- en: '![Understanding the fully connected layer](img/B08394_10_23.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![理解全连接层](img/B08394_10_23.jpg)'
- en: 'Figure 10.23: Intuitive understanding of the fully connected layer'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.23：全连接层的直观理解
- en: There is only one math operation that's been performed here. We are taking average
    values. As you can see in preceding diagram, the first, fourth, fifth, tenth,
    and eleventh cells of the fully connected layer are predicting one category, so
    we need to sum up all the values present in those cells and find their average.
    This average value tells us how confident our network is when it predicts the
    class. We can stack up as many fully connected layers as we want. Here, the number
    of neurons is the hyperparameter.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里只执行了一个数学运算。我们正在取平均值。正如您在前面的图表中看到的，第一、第四、第五、第十和第十一行的全连接层正在预测一个类别，因此我们需要将这些单元格中所有现有值的总和，并找到它们的平均值。这个平均值告诉我们我们的网络在预测类别时的信心程度。我们可以堆叠尽可能多的全连接层。在这里，神经元的数量是超参数。
- en: Understanding the SoftMax layer
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解SoftMax层
- en: 'We can also use the SoftMax layer, which converts the feature''s values into
    the probability value. The equation for this is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用SoftMax层，它将特征值转换为概率值。这个方程如下：
- en: '![Understanding the SoftMax layer](img/B08394_10_24.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![理解SoftMax层](img/B08394_10_24.jpg)'
- en: 'Figure 10.24: The SoftMax equation'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24：SoftMax方程
- en: 'This layer takes the feature values, and using the preceding equation, it generates
    the probability value. Refer to the following diagram:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层获取特征值，并使用前面的方程生成概率值。请参考以下图表：
- en: '![Understanding the SoftMax layer](img/B08394_10_25.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![理解SoftMax层](img/B08394_10_25.jpg)'
- en: 'Figure 10.25: Process of calculating the SoftMax function'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25：计算SoftMax函数的过程
- en: Updating the weight based on backpropagation
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于反向传播更新权重
- en: The weight of the CNN network has been updated based on the backpropagation
    technique. We will measure the difference between our predicted answer and the
    actual answer. Based on this error measurement, we calculate the gradient for
    the loss function, which tells us whether we should increase the weight or decrease
    it. If the predicted answer and the actual answers are the same then there will
    be no change in weights.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 基于反向传播技术，CNN网络的权重已经更新。我们将测量我们的预测答案与实际答案之间的差异。基于这个误差测量，我们计算损失函数的梯度，这告诉我们是否应该增加权重或减少它。如果预测答案和实际答案相同，则权重将不会发生变化。
- en: We have understood most of the core concepts of CNN that we will be using for
    developing face emotion recognition model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经理解了我们将用于开发面部情感识别模型的CNN的大部分核心概念。
- en: Building the face emotion recognition model
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建面部情感识别模型
- en: 'In this section, we will implement the application of FER using CNN. For coding
    purposes, we will be using the `TensorFlow`, `TFLearn`, `OpenCV`, and `Numpy`
    libraries. You can find the code by using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow.](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow)
    These are the steps that we need to follow:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现使用CNN的FER应用。为了编码目的，我们将使用`TensorFlow`、`TFLearn`、`OpenCV`和`Numpy`库。您可以通过使用此GitHub链接找到代码：[https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow)。这是我们需要遵循的步骤：
- en: Preparing the data
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Loading the data
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据
- en: Training the model
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Preparing the data
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: In this section, we will be preparing the dataset that can be used in our application.
    As you know, our dataset is in grayscale. We have two options. One is that we
    need to use only black and white images, and if we are using black and white images,
    then there will be two channels. The second option is that we can convert the
    grayscale pixel values into RGB (red, green, and blue) images and build the CNN
    with three channels. For our development purposes, we are using two channels as
    our images are in grayscale.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we are loading the dataset and converting it into the `numpy`
    array. After the conversion, we will save it as a `.npy` format so that we can
    load that dataset as and when needed. We are saving actual data records in one
    file and the labels of their data records in another file. Our input datafile
    name is `fer2013.csv`. Our output file that contains the data is `data_set_fer2013.npy`,
    and labels are present in the `data_labels_fer2013.npy` file. The script name
    that performs this task is `csv_to_numpy.py`. You can refer to its code using
    this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/tree/master/data)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet for loading the dataset in the following diagram:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the data](img/B08394_10_26.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.26: Code snippet for loading the dataset'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the `helper` function is given in the following diagram:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Preparing the data](img/B08394_10_27.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.27: Code snippet for the helper function'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how we can load the data that we have saved in the `.npy`
    format.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at how we are going to use the dataset we have
    prepared so that we can use it for training. Here, we create a separate script
    to help us load the data. In this, we define a test dataset that we will be using
    during testing.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple and straightforward code. You can find the code snippet in
    the following figure:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the data](img/B08394_10_28.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.28: Code snippet for the data loader script'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'This class and its methods will be used when we write the script for training.
    You can see the code of this script using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/dataset_loader.py).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at how to train the model, so that it can recognize
    the facial emotion. These are the steps that we will be performing. You can find
    the code for this training step by referring to this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data using the dataset_loader script
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用dataset_loader脚本加载数据
- en: 'Here, we are loading the dataset with the help of the script that we have written
    and understood in the last section. You can find the code snippet in the following
    figure:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用我们在上一节中编写的并理解的脚本加载数据集。您可以在下面的图中找到代码片段：
- en: '![Loading the data using the dataset_loader script](img/B08394_10_29.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![使用dataset_loader脚本加载数据](img/B08394_10_29.jpg)'
- en: 'Figure 10.29: Loading the data during the training of the model'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.29：在模型训练过程中加载数据
- en: Now let's build the CNN that is actually used for training.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建实际用于训练的CNN。
- en: Building the Convolutional Neural Network
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建卷积神经网络
- en: 'In this step, we will be building the CNN that will be used for training purposes.
    Here, we have three layers of the convolutional network, the ReLU layer, and the
    pooling layer. The first two layers have 64 neurons, and the last one has 128
    neurons. We have added the dropout layer. For some neurons, the dropout layer
    sets the value to zero. This layer selects the neurons that have not changed their
    weight for a long time, or have not been activated for a long time. This will
    make our training more effective. We have two fully connected layers, and one
    fully connected layer using the SoftMax function to derive the probability for
    the facial emotion class. We are using the `momentum` function for performing
    the gradient descent. Here, our loss function is categorical cross-entropy. Refer
    to the code snippet in the following diagram:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将构建用于训练目的的CNN。在这里，我们有三层卷积网络、ReLU层和池化层。前两层有64个神经元，最后一层有128个神经元。我们添加了dropout层。对于一些神经元，dropout层将值设置为0。这个层选择那些长时间没有改变权重或长时间未激活的神经元。这将使我们的训练更加有效。我们有两个全连接层，以及一个使用SoftMax函数来推导面部情感类概率的全连接层。我们使用`momentum`函数来进行梯度下降。在这里，我们的损失函数是分类交叉熵。参考下面的图中代码片段：
- en: '![Building the Convolutional Neural Network](img/B08394_10_30.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![构建卷积神经网络](img/B08394_10_30.jpg)'
- en: 'Figure 10.30: Code snippet for building CNN'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.30：构建CNN的代码片段
- en: Now let's see how to perform training.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何进行训练。
- en: Training for the FER application
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为FER应用进行训练
- en: 'In this step, we need to start training so that our model can learn to predict
    facial emotions. In this step, we will be defining some hyperparameters for training.
    Refer to the code snippet in the following diagram:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们需要开始训练，以便我们的模型可以学会预测面部情感。在这一步，我们将定义一些训练的超参数。参考下面的图中代码片段：
- en: '![Training for the FER application](img/B08394_10_31.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![为FER应用进行训练](img/B08394_10_31.jpg)'
- en: 'Figure 10.31: Code snippet for performing training'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.31：执行训练的代码片段
- en: As you can see in the preceding diagram, we have set the epoch to `100`. The
    training batch size is `50`. We can see the `shuffle` parameter, which acts as
    the flag. The value of this parameter is `true`, which indicates that we are shuffling
    our dataset during training.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们将epoch设置为`100`。训练批次大小为`50`。我们可以看到`shuffle`参数，它作为标志。此参数的值为`true`，表示我们在训练过程中正在打乱我们的数据集。
- en: The command to start training is `$ python emotion_recognition.py train`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练的命令是`$ python emotion_recognition.py train`。
- en: Predicting and saving the trained model
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测和保存训练模型
- en: 'In this step, we are defining the `predict` method. This method helps us generate
    a prediction. We have also defined the method that can help us save the trained
    model. We need to save the model, because we can load it as and when needed for
    testing. You can find the code snippet in the following diagram:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们正在定义`predict`方法。这个方法帮助我们生成预测。我们还定义了一个可以帮助我们保存训练模型的方法。我们需要保存模型，因为我们可以在需要时加载它进行测试。您可以在下面的图中找到代码片段：
- en: '![Predicting and saving the trained model](img/B08394_10_32.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![预测和保存训练模型](img/B08394_10_32.jpg)'
- en: 'Figure 10.32: Code snippet for predicting the class'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.32：预测类别的代码片段
- en: 'Refer to the code snippet in the following diagram:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 参考下面的图中代码片段：
- en: '![Predicting and saving the trained model](img/B08394_10_33.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![预测和保存训练模型](img/B08394_10_33.jpg)'
- en: 'Figure 10.33: Code snippet for saving the trained model'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.33：保存训练模型的代码片段
- en: Now it's time to look at the testing matrix. After that, we need to test our
    trained model. So, before testing our model, we should understand the testing
    matrix.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是查看测试矩阵的时候了。之后，我们需要测试我们的训练模型。所以，在我们测试模型之前，我们应该理解测试矩阵。
- en: Understanding the testing matrix
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: 'In this section, we will look at the testing matrix for the facial emotion
    application. The concept of testing is really simple. We need to start observing
    the training steps. We are tracking the values for loss and accuracy. Based on
    that, we can decide the accuracy of our model. Doesn''t this sound simple? We
    have trained the model for 30 epochs. This amount of training requires more than
    three hours. We have achieved 63.88% training accuracy. Refer to the code snippet
    in the following diagram:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the testing matrix](img/B08394_10_34.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.34: Training progress for getting an idea of training accuracy'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: This is the training accuracy. If we want to check the accuracy on the validation
    dataset, then that is given in the training step as well. We have defined the
    validation set. With the help of this validation dataset, the trained model generates
    its prediction. We compare the predicted class and actual class labels. After
    that, we generate the validation accuracy that you can see in the preceding diagram.
    Here, `val_acc` is 66.37%, which is great. To date, this application has been
    able to achieve up to 65 to 70% accuracy.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to load the trained model and test it. Here, we will be using the
    video stream. The FER application will detect the emotion based on my facial expression.
    You can refer to the code using this GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py](https://github.com/jalajthanaki/Facial_emotion_recognition_using_TensorFlow/blob/master/emotion_recognition.py).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet for this in the following figure:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_35.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.35: Code snippet for loading the trained model and performing testing'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to start testing, we need to execute the following command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '`$ python emotion_recognition.py poc`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'This testing will use your webcam. I have some demo files that I want to share
    here. Refer to the following figure:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_36.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.36: Code snippet for FER application identifying the emotion of disgust'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Also refer to the the following figure:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_37.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.37: tThe FER application identifying the happy emotion'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in the following figure:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_38.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.38: Code snippet for the FER application identifying the neutral
    emotion'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the code snippet given in the following figure:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the model](img/B08394_10_39.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.39: Code snippet for the FER application identifying the angry emotion'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how we can improve this approach.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the existing approach
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will list all the points that create problems. We should
    try to improve them. The following are things that I feel we can improve upon:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: If you find out that class sampling is not proper in your case, then you can
    adopt the sampling methods
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can add more layers to our neural network
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can try different gradient descent techniques.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, training takes a lot of time that means training is computationally
    expensive. When we trained the model, we used GPUs even though GPU training takes
    a long time. We can use multiple GPUs, but that is expensive, and a cloud instance
    with multiple GPUs is not affordable. So, if we can use transfer learning in this
    application, or use the pre-trained model, then we will achieve better results.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen in the previous section, because of the lack of computation
    hardware, we have achieved a 66% accuracy rate. In order to improve the accuracy
    further, we can use the pre-trained model, which will be more convenient.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process for optimization
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few problems that I have described in the previous sections. We
    can add more layers to our CNN, but that will become more computationally expensive,
    so we are not going to do that. We have sampled our dataset well, so we do not
    need to worry about that.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: As part of the optimization process, we will be using the pre-trained model
    that is trained by using the `keras` library. This model uses many layers of CNNs.
    It will be trained on multiple GPUs. So, we will be using this pre-trained model,
    and checking how this will turn out.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will be implementing the code that can use the pre-trained
    model.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have achieved approximately a 66% accuracy rate; for an FER application,
    the best accuracy will be approximately 69%. We will achieve this by using the
    pre-trained model. So, let's look at the implementation, and how we can use it
    to achieve the best possible outcome.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing the best possible approach for the
    FER application. This pre-trained model has been built by using dense and deep
    convolutional layers. Because of the six-layer deep CNN, and with the help of
    the stochastic gradient descent (SGD) technique, we can build the pre-trained
    model. The number of neurons for each layer were 32, 32, 64, 64, 128,128, 1,024,
    and 512, respectively. All layers are using ReLU as an activation function. The
    3 x 3 matrix will be used to generate the initial feature map, and the 2 x 2 matrix
    will be used to generate the max pooling. You can download the model from this
    GitHub link: [https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras](https://github.com/jalajthanaki/Facial_emotion_recognition_using_Keras)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look at the code by referring to the following diagram:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_40.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10\. 40: Code snippet for using the pre-trained FER model'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we loaded the pre-trained `keras` model. We are providing
    two provisions. We can use this script for detecting the facial expression from
    the image as well as by providing the video stream.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test the facial expression present in any image, then we need
    to execute the `$ python image_test.py tes.jpg` command. I have applied it to
    this model on the `tes.jpg` image. You can see the output image as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_41.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.41: Output of the FER application for the image'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test the model for the video stream, then you need to execute
    this command: `$python realtime_facial_expression.py`.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the output in the following figure:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_42.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.42: Output of the FER application for the video stream'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the output file in the following figure:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_10_43.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.43: Output of the FER application for the video stream'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: This application provides us with approximately 67% accuracy, which is great.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to develop the face detection application
    using the `face_recognition` library, which uses the HOG-based model to identify
    the faces in the images. We have also used the pre-trained convolutional neural
    network, which identifies the faces from a given image. We developed real-time
    face recognition to detect the names of people. For face recognition, we used
    a pre-trained model and already available libraries. In the second part of the
    chapter, we developed the face emotion recognition application, which can detect
    seven major emotions a human face can carry. We used `TensorFlow`, `OpenCV`, `TFLearn`,
    and `Keras` in order to build the face emotion recognition model. This model has
    fairly good accuracy for predicting the face emotion. We achieved the best possible
    accuracy of 67%.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the computer vision domain is moving quickly in terms of research.
    You can explore many fresh and cool concepts, such as `deepfakes` and 3D human
    pose estimation (machine vision) by the Facebook AI Research group. You can refer
    to the `deepfakes` GitHub repository by clicking here: [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
    You can refer to the paper on 3D human pose estimation by clicking on this link:
    [https://arxiv.org/pdf/1705.03098.pdf](https://arxiv.org/pdf/1705.03098.pdf).
    Both the concepts are new and fresh, so you can refer to them and make some fun
    applications.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be the last chapter of this book. In the course of the
    chapter, we will try to make a gaming bot. This chapter will heavily use reinforcement
    learning techniques. We will be developing a bot that can play Atari games on
    its own. So keep reading!
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
