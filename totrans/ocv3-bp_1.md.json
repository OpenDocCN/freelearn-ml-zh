["```py\n#include <stdio.h>\n#include <time.h>\n\n#include <opencv2/core.hpp>\n#include <opencv2/videoio.hpp>\n\nint main(int argc, char *argv[]) {\n\n  const int cameraIndex = 0;\n  const bool isColor = true;\n  const int w = 320;\n  const int h = 240;\n  const double captureFPS = 187.0;\n  const double writerFPS = 60.0;\n  // With MJPG encoding, OpenCV requires the AVI extension.\n  const char filename[] = \"SlowMo.avi\";\n  const int fourcc = cv::VideoWriter::fourcc('M','J','P','G');\n  const unsigned int numFrames = 3750;\n\n  cv::Mat mat;\n\n  // Initialize and configure the video capture.\n  cv::VideoCapture capture(cameraIndex);\n  if (!isColor) {\n    capture.set(cv::CAP_PROP_MODE, cv::CAP_MODE_GRAY);\n  }\n  capture.set(cv::CAP_PROP_FRAME_WIDTH, w);\n  capture.set(cv::CAP_PROP_FRAME_HEIGHT, h);\n  capture.set(cv::CAP_PROP_FPS, captureFPS);\n\n  // Initialize the video writer.\n  cv::VideoWriter writer(\n      filename, fourcc, writerFPS, cv::Size(w, h), isColor);\n\n  // Get the start time.\n  clock_t startTicks = clock();\n\n  // Capture frames and write them to the video file.\n  for (unsigned int i = 0; i < numFrames;) {\n    if (capture.read(mat)) {\n      writer.write(mat);\n      i++;\n    }\n  }\n\n  // Get the end time.\n  clock_t endTicks = clock();\n\n  // Calculate and print the actual frame rate.\n  double actualFPS = numFrames * CLOCKS_PER_SEC /\n      (double)(endTicks - startTicks);\n  printf(\"FPS: %.1f\\n\", actualFPS);\n}\n```", "```py\n$ g++ UnblinkingEye.cpp -o UnblinkingEye -lopencv_core -lopencv_videoio\n\n```", "```py\nconst bool isColor = false;\n```", "```py\n  cv::VideoCapture capture(cameraIndex);\n  if (!isColor) {\n capture.set(cv::CAP_PROP_MODE, cv::CAP_MODE_GRAY);\n }\n  capture.set(cv::CAP_PROP_FRAME_WIDTH, w);\n  capture.set(cv::CAP_PROP_FRAME_HEIGHT, h);\n  capture.set(cv::CAP_PROP_FPS, captureFPS);\n\n  cv::VideoWriter writer(\n      filename, fourcc, writerFPS, cv::Size(w, h), isColor);\n```", "```py\nint width = 320, height = 240;\nint matType = CV_8UC3; // 8 bpp per channel, 3 channels\nvoid *pData;\n\n// Use the camera SDK to capture image data.\nsomeCaptureFunction(&pData);\n\n// Create the matrix. No data are copied; the pointer is copied.\ncv::Mat mat(height, width, matType, pData);\n```", "```py\nvoid *pData = mat.data;\n```", "```py\n$ git clone â€“b develop https://github.com/occipital/OpenNI2.git\n\n```", "```py\nexport OPENNI2_INCLUDE=\"<openni2_path>/Include\"\nexport OPENNI2_REDIST=\"<openni2_path>/Bin/x64-Release\"\n```", "```py\n    > cd <xtion_firmware_unzip_path>\\UsbUpdate\n    > !Update-RD108x!\n\n    ```", "```py\n#include <stdio.h>\n#include <stdlib.h>\n```", "```py\n#include <opencv2/core.hpp>\n#include <opencv2/highgui.hpp>\n#include <opencv2/imgproc.hpp>\n#include <OpenNI.h>\n```", "```py\nint main(int argc, char *argv[]) {\n\n  const openni::SensorType sensorType = openni::SENSOR_IR;\n//  const openni::SensorType sensorType = openni::SENSOR_COLOR;\n//  const openni::SensorType sensorType = openni::SENSOR_DEPTH;\n  const char windowName[] = \"Infravision\";\n```", "```py\n  int srcMatType;\n  if (sensorType == openni::SENSOR_COLOR) {\n    srcMatType = CV_8UC3;\n  } else {\n    srcMatType = CV_16U;\n  }\n```", "```py\n  openni::Status status;\n\n  status = openni::OpenNI::initialize();\n  if (status != openni::STATUS_OK) {\n    printf(\n        \"Failed to initialize OpenNI:\\n%s\\n\",\n        openni::OpenNI::getExtendedError());\n    return EXIT_FAILURE;\n  }\n```", "```py\n  openni::Device device;\n  status = device.open(openni::ANY_DEVICE);\n  if (status != openni::STATUS_OK) {\n    printf(\n        \"Failed to open device:\\n%s\\n\",\n        openni::OpenNI::getExtendedError());\n    openni::OpenNI::shutdown();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  const openni::SensorInfo *sensorInfo =\n      device.getSensorInfo(sensorType);\n  if (sensorInfo == NULL) {\n    printf(\"Failed to find sensor of appropriate type\\n\");\n    device.close();\n    openni::OpenNI::shutdown();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  openni::VideoStream stream;\n  status = stream.create(device, sensorType);\n  if (status != openni::STATUS_OK) {\n    printf(\n        \"Failed to create stream:\\n%s\\n\",\n        openni::OpenNI::getExtendedError());\n    device.close();\n    openni::OpenNI::shutdown();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  // Select the video mode with the highest resolution.\n  {\n    const openni::Array<openni::VideoMode> *videoModes =\n        &sensorInfo->getSupportedVideoModes();\n    int maxResolutionX = -1;\n    int maxResolutionIndex = 0;\n    for (int i = 0; i < videoModes->getSize(); i++) {\n      int resolutionX = (*videoModes)[i].getResolutionX();\n      if (resolutionX > maxResolutionX) {\n        maxResolutionX = resolutionX;\n        maxResolutionIndex = i;\n      }\n    }\n    stream.setVideoMode((*videoModes)[maxResolutionIndex]);\n  }\n```", "```py\n  status = stream.start();\n  if (status != openni::STATUS_OK) {\n    printf(\n        \"Failed to start stream:\\n%s\\n\",\n        openni::OpenNI::getExtendedError());\n    stream.destroy();\n    device.close();\n    openni::OpenNI::shutdown();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  openni::VideoFrameRef frame;\n  cv::Mat dstMat;\n  cv::namedWindow(windowName);\n```", "```py\n  // Capture and display frames until any key is pressed.\n  while (cv::waitKey(1) == -1) {\n    status = stream.readFrame(&frame);\n    if (frame.isValid()) {\n      cv::Mat srcMat(\n          frame.getHeight(), frame.getWidth(), srcMatType,\n          (void *)frame.getData(), frame.getStrideInBytes());\n      if (sensorType == openni::SENSOR_COLOR) {\n        cv::cvtColor(srcMat, dstMat, cv::COLOR_RGB2BGR);\n      } else {\n        srcMat.convertTo(dstMat, CV_8U);\n      }\n      cv::imshow(windowName, dstMat);\n    }\n  }\n```", "```py\n  cv::destroyWindow(windowName);\n\n  stream.stop();\n  stream.destroy();\n  device.close();\n  openni::OpenNI::shutdown();\n}\n```", "```py\n$ g++ Infravision.cpp -o Infravision \\\n -I include -I $OPENNI2_INCLUDE -L $OPENNI2_REDIST \\\n -Wl,-R$OPENNI2_REDIST -Wl,-R$OPENNI2_REDIST/OPENNI2 \\\n -lopencv_core -lopencv_highgui -lopencv_imgproc -lOpenNI2\n\n```", "```py\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <time.h>\n```", "```py\n#include <flycapture/C/FlyCapture2_C.h>\n#include <opencv2/core.hpp>\n#include <opencv2/imgproc.hpp>\n#include <opencv2/objdetect.hpp>\n#include <SDL2/SDL.h>\n```", "```py\n#ifdef _WIN32\n#define snprintf sprintf_s\n#endif\n```", "```py\nvoid showFC2Error(fc2Error error) {\n  if (error != FC2_ERROR_OK) {\n    SDL_ShowSimpleMessage(SDL_MESSAGEBOX_ERROR,\n            \"FlyCapture2 Error\",\n            fc2ErrorToDescription(error), NULL);\n  }\n}\n\nvoid showSDLError() {\n  SDL_ShowSimpleMessageBox(\n      SDL_MESSAGEBOX_ERROR, \"SDL2 Error\", SDL_GetError(), NULL);\n}\n```", "```py\nint main(int argc, char *argv[]) {\n\n  const unsigned int cameraIndex = 0u;\n  const unsigned int numImagesPerFPSMeasurement = 240u;\n  const int windowWidth = 1440;\n  const int windowHeight = 900;\n  const char cascadeFilename[] = \"haarcascade_frontalface_alt.xml\";\n  const double detectionScaleFactor = 1.25;\n  const int detectionMinNeighbours = 4;\n  const int detectionFlags = CV_HAAR_SCALE_IMAGE;\n  const cv::Size detectionMinSize(120, 120);\n  const cv::Size detectionMaxSize;\n  const cv::Scalar detectionDrawColor(255.0, 0.0, 255.0);\n  char strBuffer[256u];\n  const size_t strBufferSize = 256u;\n```", "```py\n  int matType;\n  cv::Mat equalizedGrayMat;\n```", "```py\n#ifdef _WIN32\n  snprintf(strBuffer, strBufferSize, \"%s/../%s\", argv[0], cascadeFilename);\n  cv::CascadeClassifier detector(strBuffer);\n#else\n  cv::CascadeClassifier detector(cascadeFilename);\n#endif\n  if (detector.empty()) {\n    snprintf(strBuffer, strBufferSize, \"%s could not be loaded.\",\n              cascadeFilename);\n    SDL_ShowSimpleMessageBox(\n      SDL_MESSAGEBOX_ERROR, \"Failed to Load Cascade File\", strBuffer,NULL);\n    return EXIT_FAILURE;\n  }\n  std::vector<cv::Rect> detectionRects;\n```", "```py\n  fc2Error error;\n\n  fc2Image image;\n  error = fc2CreateImage(&image);\n  if (error != FC2_ERROR_OK) {\n    showFC2Error(error);\n    return EXIT_FAILURE;\n  }\n```", "```py\n  fc2Context context;\n  error = fc2CreateContext(&context);\n  if (error != FC2_ERROR_OK) {\n    showFC2Error(error);\n    return EXIT_FAILURE;\n  }\n```", "```py\n  fc2PGRGuid cameraGUID;\n  error = fc2GetCameraFromIndex(context, cameraIndex, &cameraGUID);\n  if (error != FC2_ERROR_OK) {\n    showFC2Error(error);\n    return EXIT_FAILURE;\n  }\n```", "```py\n  error = fc2Connect(context, &cameraGUID);\n  if (error != FC2_ERROR_OK) {\n    showFC2Error(error);\n    return EXIT_FAILURE;\n  }\n```", "```py\n  error = fc2StartCapture(context);\n  if (error != FC2_ERROR_OK) {\n    fc2Disconnect(context);\n    showFC2Error(error);\n    return EXIT_FAILURE;\n  }\n```", "```py\n  if (SDL_Init(SDL_INIT_VIDEO) < 0) {\n    fc2StopCapture(context);\n    fc2Disconnect(context);\n    showSDLError();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  SDL_Window *window = SDL_CreateWindow(\n      \"LookSpry\", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED,\n      windowWidth, windowHeight, 0u);\n  if (window == NULL) {\n    fc2StopCapture(context);\n    fc2Disconnect(context);\n    showSDLError();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  SDL_Renderer *renderer = SDL_CreateRenderer(window, -1, 0u);\n  if (renderer == NULL) {\n    fc2StopCapture(context);\n    fc2Disconnect(context);\n    SDL_DestroyWindow(window);\n    showSDLError();\n    return EXIT_FAILURE;\n  }\n```", "```py\n  SDL_RendererInfo rendererInfo;\n  SDL_GetRendererInfo(renderer, &rendererInfo);\n\n  if (strcmp(rendererInfo.name, \"direct3d\") == 0) {\n    SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"best\");\n  } else if (strcmp(rendererInfo.name, \"opengl\") == 0) {\n    SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, \"linear\");\n  }\n```", "```py\n  snprintf(strBuffer, strBufferSize, \"LookSpry | %s\",\n      rendererInfo.name);\n  SDL_SetWindowTitle(window, strBuffer);\n```", "```py\n  SDL_Texture *videoTex = NULL;\n  void *videoTexPixels;\n  int pitch;\n```", "```py\n  clock_t startTicks = clock();\n  clock_t endTicks;\n  unsigned int numImagesCaptured = 0u;\n```", "```py\n  bool running = true;\n  bool detecting = true;\n  bool mirroring = true;\n```", "```py\n  SDL_Event event;\n  while (running) {\n    while (SDL_PollEvent(&event)) {\n      if (event.type == SDL_QUIT) {\n        running = false;\n        break;\n      } else if (event.type == SDL_KEYUP) {\n        switch(event.key.keysym.sym) {\n        // When 'd' is pressed, start or stop [d]etection.\n        case SDLK_d:\n          detecting = !detecting;\n          break;\n        // When 'm' is pressed, [m]irror or un-mirror the video.\n        case SDLK_m:\n          mirroring = !mirroring;\n          break;\n        default:\n          break;\n        }\n      }\n    }\n```", "```py\n    error = fc2RetrieveBuffer(context, &image);\n    if (error != FC2_ERROR_OK) {\n       fc2Disconnect(context);\n       SDL_DestroyTexture(videoTex);\n       SDL_DestroyRenderer(renderer);\n       SDL_DestroyWindow(window);\n       showFC2Error(error);\n       return EXIT_FAILURE;\n    }\n```", "```py\n    if (videoTex == NULL) {\n      equalizedGrayMat.create(image.rows, image.cols, CV_8UC1);\n      SDL_RenderSetLogicalSize(renderer, image.cols, image.rows);\n```", "```py\n      Uint32 videoTexPixelFormat;\n      switch (image.format) {\n        // For monochrome capture modes, plan to render captured data\n        // to the Y plane of a planar YUV texture.\n        case FC2_PIXEL_FORMAT_RAW8:\n        case FC2_PIXEL_FORMAT_MONO8:\n          videoTexPixelFormat = SDL_PIXELFORMAT_YV12;\n          matType = CV_8UC1;\n          break;\n```", "```py\n        // For color capture modes, plan to render captured data\n        // to the entire space of a texture in a matching color\n        // format.\n        case FC2_PIXEL_FORMAT_422YUV8:\n          videoTexPixelFormat = SDL_PIXELFORMAT_UYVY;\n          matType = CV_8UC2;\n          break;\n        case FC2_PIXEL_FORMAT_RGB:\n          videoTexPixelFormat = SDL_PIXELFORMAT_RGB24;\n          matType = CV_8UC3;\n          break;\n        case FC2_PIXEL_FORMAT_BGR:\n          videoTexPixelFormat = SDL_PIXELFORMAT_BGR24;\n          matType = CV_8UC3;\n          break;\n```", "```py\n        default:\n          fc2StopCapture(context);\n          fc2Disconnect(context);\n          SDL_DestroyTexture(videoTex);\n          SDL_DestroyRenderer(renderer);\n          SDL_DestroyWindow(window);\n                SDL_ShowSimpleMessageBox(\n          SDL_MESSAGEBOX_ERROR,\n          \"Unsupported FlyCapture2 Pixel Format\",\n          \"LookSpry supports RAW8, MONO8, 422YUV8, RGB, and BGR.\",\n          NULL);\n          return EXIT_FAILURE;\n      }\n```", "```py\n      videoTex = SDL_CreateTexture(\n          renderer, videoTexPixelFormat, SDL_TEXTUREACCESS_STREAMING,\n          image.cols, image.rows);\n      if (videoTex == NULL) {\n        fc2StopCapture(context);\n        fc2Disconnect(context);\n        SDL_DestroyRenderer(renderer);\n        SDL_DestroyWindow(window);\n        showSDLError();\n        return EXIT_FAILURE;\n      }\n```", "```py\n      snprintf(\n          strBuffer, strBufferSize, \"LookSpry | %s | %dx%d --> %dx%d\",\n          rendererInfo.name, image.cols, image.rows, windowWidth,\n          windowHeight);\n      SDL_SetWindowTitle(window, strBuffer);\n    }\n```", "```py\n    cv::Mat srcMat(image.rows, image.cols, matType, image.pData,\n            image.stride);\n    if (detecting) {\n      switch (image.format) {\n        // For monochrome capture modes, just equalize.\n        case FC2_PIXEL_FORMAT_RAW8:\n        case FC2_PIXEL_FORMAT_MONO8:\n          cv::equalizeHist(srcMat, equalizedGrayMat);\n          break;\n        // For color capture modes, convert to gray and equalize.\n        cv::cvtColor(srcMat, equalizedGrayMat,\n               cv::COLOR_YUV2GRAY_UYVY);\n          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);\n          break;\n        case FC2_PIXEL_FORMAT_RGB:\n          cv::cvtColor(srcMat, equalizedGrayMat, cv::COLOR_RGB2GRAY);\n          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);\n          break;\n        case FC2_PIXEL_FORMAT_BGR:\n          cv::cvtColor(srcMat, equalizedGrayMat, cv::COLOR_BGR2GRAY);\n          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);\n          break;\n        default:\n          break;\n      }\n```", "```py\n      // Run the detector on the equalized image.\n      detector.detectMultiScale(\n          equalizedGrayMat, detectionRects, detectionScaleFactor,\n          detectionMinNeighbours, detectionFlags, detectionMinSize,\n          detectionMaxSize);\n      // Draw the resulting detection rectangles on the original image.\n      for (cv::Rect detectionRect : detectionRects) {\n        cv::rectangle(srcMat, detectionRect, detectionDrawColor);\n      }\n    }\n```", "```py\n    SDL_LockTexture(videoTex, NULL, &videoTexPixels, &pitch);\n```", "```py\n    switch (image.format) {\n    case FC2_PIXEL_FORMAT_RAW8:\n    case FC2_PIXEL_FORMAT_MONO8:\n      // Make the planar YUV video gray by setting all bytes in its U\n      // and V planes to 128 (the middle of the range).\n      memset(((unsigned char *)videoTexPixels + image.dataSize), 128,\n             image.dataSize / 2u);\n      break;\n    default:\n      break;\n    }\n```", "```py\n    if (mirroring) {\n      // Flip the image data while copying it to the texture.\n      cv::Mat dstMat(image.rows, image.cols, matType, videoTexPixels,\n                     image.stride);\n      cv::flip(srcMat, dstMat, 1);\n    } else {\n      // Copy the image data, as-is, to the texture.\n      // Note that the PointGrey image and srcMat have pointers to the\n      // same data, so the following code does reference the data that\n      // we modified earlier via srcMat.\n      memcpy(videoTexPixels, image.pData, image.dataSize);\n    }\n```", "```py\n    SDL_UnlockTexture(videoTex);\n    SDL_RenderCopy(renderer, videoTex, NULL, NULL);\n    SDL_RenderPresent(renderer);\n```", "```py\n    numImagesCaptured++;\n    if (numImagesCaptured >= numImagesPerFPSMeasurement) {\n      endTicks = clock();\n      snprintf(\n        strBuffer, strBufferSize,\n        \"LookSpry | %s | %dx%d --> %dx%d | %ld FPS\",\n        rendererInfo.name, image.cols, image.rows, windowWidth,\n        windowHeight,\n        numImagesCaptured * CLOCKS_PER_SEC /\n         (endTicks - startTicks));\n      SDL_SetWindowTitle(window, strBuffer);\n      startTicks = endTicks;\n      numImagesCaptured = 0u;\n    }\n  }\n```", "```py\n  fc2StopCapture(context);\n  fc2Disconnect(context);\n  SDL_DestroyTexture(videoTex);\n  SDL_DestroyRenderer(renderer);\n  SDL_DestroyWindow(window);\n  return EXIT_SUCCESS;\n}\n```", "```py\n$ g++ LookSpry.cpp -o LookSpry `sdl2-config --cflags --libs` \\\n -lflycapture-c -lopencv_core -lopencv_imgproc -lopencv_objdetect\n\n```", "```py\ndiagonalFOVDegrees =\n  2 * atan(0.5 * sensorDiagonal / focalLength) * 180/pi\n```", "```py\nmagnificationRatio = totalExtension / focalLength\n```"]