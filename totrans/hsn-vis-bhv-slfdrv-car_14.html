<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer234">
			<h1 id="_idParaDest-231"><a id="_idTextAnchor250"/><em class="italic">Chapter 11</em>: Mapping Our Environments</h1>
			<p>There are a few fundamental things that your self-driving car needs to navigate the world.</p>
			<p>First of all, you need to have a map of your environment. This map is very similar to the map you use on your phone for getting to your favorite restaurant.</p>
			<p>Secondly, you need a way to localize your position on that map in the real world. On your phone, this is the blue dot localized by the GPS.</p>
			<p>In this chapter, you will learn about various ways for your self-driving car to map and localize through its environment, so it knows where it is in the world. You can imagine why this is important since the entire reason for making a self-driving car is to go places!</p>
			<p>You will learn the following topics to help you build a self-driving car worthy of being called Magellan:</p>
			<ul>
				<li>Why you need maps and localization</li>
				<li>Types of mapping and localization</li>
				<li>Open source mapping tools</li>
				<li>SLAM with an Ouster lidar and Google Cartographer</li>
			</ul>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor251"/>Technical requirements</h1>
			<p>The chapter requires the following software:</p>
			<ul>
				<li>Linux</li>
				<li>ROS Melodic: <a href="http://wiki.ros.org/melodic/Installation/Ubuntu">http://wiki.ros.org/melodic/Installation/Ubuntu</a></li>
				<li>Python 3.7: <a href="https://www.python.org/downloads/release/python-370/">https://www.python.org/downloads/release/python-370/</a></li>
				<li>C++</li>
				<li>Google Cartographer ROS: <a href="https://github.com/cartographer-project/cartographer_ros">https://github.com/cartographer-project/cartographer_ros</a></li>
				<li><strong class="source-inline">ouster_example_cartographer</strong>: <a href="https://github.com/Krishtof-Korda/ouster_example_cartographer">https://github.com/Krishtof-Korda/ouster_example_cartographer</a></li>
			</ul>
			<p>The code for the chapter can be found at the following link:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars</a></p>
			<p>The code in Action videos for the chapter can be found here:</p>
			<p><a href="https://bit.ly/2IVkJVZ">https://bit.ly/2IVkJVZ</a></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor252"/>Why you need maps and localization</h1>
			<p>In this chapter, you will <a id="_idIndexMarker691"/>learn the importance of maps and localization, and the combination <a id="_idIndexMarker692"/>of them. Maps and localization are things we often take for granted in the modern world, but as you will see, they are very important, especially for self-driving cars, where the amazing human brain is not utilized.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor253"/>Maps</h2>
			<p>Take a moment <a id="_idIndexMarker693"/>and imagine a world without cell phones, without MapQuest (yup, I'm an elder millennial), without paper maps, and without Anaximander of Greece! </p>
			<p>How well do you think you could navigate from your home to a city you have never been to, let alone the new Trader Joe's that just opened a few cities away? I am sure you could do it, but you would probably stop every few kilometers and ask a local for the next few directions to get you closer to that bold and earthy Two Buck Chuck. But you can see why maps really make our lives easier and open up possibilities to venture to new places with little fear of getting lost and ending up at Walley World.</p>
			<p>Now, you are very fortunate that companies such as Google and Apple have painstakingly mapped every street, alleyway, and side street you can think of. That is a huge task and we benefit from it every day. Hooray maps!</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor254"/>Localization</h2>
			<p>Okay, now <a id="_idIndexMarker694"/>imagine that you are teleported here:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="Images/Figure_11.1_B16322.jpg" alt="Figure 11.1 – Monkey Face in Russia. Image source: http://bit.ly/6547672-17351141" width="1536" height="817"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Monkey Face in Russia. Image source: <a href="http://bit.ly/6547672-17351141">http://bit.ly/6547672-17351141</a></p>
			<p>You have been given a map of the area and need to find your way to the nearest body of water. The first thing you need to do – after you stop shaking from being teleported – is figure out where in the world you are on the map you have. You would likely look around you to pick out landmarks nearby, and then try to find those landmarks on your map. <em class="italic">Monkey Face, I'm right in the middle of the Monkey Face!</em> Congratulations, you just localized yourself in the map and can use it to find the elixir of life!</p>
			<p>Now you see why both a map and localization are necessary for navigating the world and environments.</p>
			<p>Now, you say, <em class="italic">But wait, what if something has changed in the world since the map was generated?!</em></p>
			<p>Surely you have been driving along, following the sweet voice of navigation from your phone when <em class="italic">wham!</em> You just <a id="_idIndexMarker695"/>rolled up to some road construction that has closed the road and is forcing you to take a 30-minute detour. You blurt out to your phone, <em class="italic">Curse you, nav voice from oblivion! How did you not know that there was construction?</em></p>
			<p>The truth is that your dear navigator voice, no matter how up to date, will always miss real-time information about the world. Just imagine some ducks crossing the road; the voice will never warn you about that. In the next section, you will learn the many ways to save the ducks with various types of mapping and localization.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor255"/>Types of mapping and localization</h1>
			<p>The field of <a id="_idIndexMarker696"/>localization and mapping is absolutely full of amazing research and is continually growing. The advancement of GPUs and computer processing speeds has led to the development <a id="_idIndexMarker697"/>of some very exciting algorithms.</p>
			<p>Quickly, let's get back to saving our ducks! Recall in the previous section that our dear sat-nav voice did not see the ducks crossing the road in front of us. A map will never be completely accurate since the world is ever-changing and morphing. Therefore, we must have a way to not only localize using a pre-built map but also build a map in real time so that we can see when new obstacles appear in our map and navigate around them. Introducing SLAM for the ducks (not dunks).</p>
			<p>Although <a id="_idIndexMarker698"/>there are independent methods for mapping and localization, in this chapter, we will focus on <strong class="bold">Simultaneous Localization and Mapping</strong> (<strong class="bold">SLAM</strong>). If you are curious, though, the following is a quick breakdown of the most commonly used algorithms for independent localization and mapping:</p>
			<ul>
				<li>Particle filters</li>
				<li>Markov localization</li>
				<li>Grid localization</li>
				<li>Extended Kalman filters for range-bearing localization</li>
				<li>Kalman filters for dead reckoning (odometry)<p class="callout-heading">Note</p><p class="callout">You can <a id="_idIndexMarker699"/>read more about localization here:</p><p class="callout"><a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf">https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf</a></p><p class="callout"><a href="http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf">http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf</a></p><p class="callout"><a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1999_3/fox_dieter_1999_3.pdf">https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1999_3/fox_dieter_1999_3.pdf</a></p></li>
			</ul>
			<p>Some example types of mapping are as follows:</p>
			<ul>
				<li>Occupancy grid</li>
				<li>Feature-based (landmark)</li>
				<li>Topological (graph-based)</li>
				<li>Visual teach and repeat<p class="callout-heading">Note</p><p class="callout">To read <a id="_idIndexMarker700"/>more about mapping, refer to the following links:</p><p class="callout"><a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf">https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf</a></p><p class="callout"><a href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1996_8/thrun_sebastian_1996_8.pdf">https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1996_8/thrun_sebastian_1996_8.pdf</a></p></li>
			</ul>
			<p>There is a <a id="_idIndexMarker701"/>lot of great information on these algorithms and implementations, but for this book, we will <a id="_idIndexMarker702"/>focus on the most widely used form of localization and mapping, the simultaneous kind: SLAM.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor256"/>Simultaneous localization and mapping (SLAM)</h2>
			<p>Let's jump <a id="_idIndexMarker703"/>back into our imagination for a moment.</p>
			<p>Imagine you suddenly woke up one night and there was absolutely no light, no moon, no glow worms – just pitch black! Fear not, you will use the magic of SLAM to navigate from your bed to get that tasty midnight snack!</p>
			<p>You fumble your left hand around until you feel the edge of your bed. Boom, you just localized yourself on the bed and have mapped the left edge of the bed in your mind. You make the assumption that you didn't flip vertically in bed while sleeping, so this really is the left side of your bed.</p>
			<p>Next, you swing your legs over the edge of the bed, slowly lowering yourself until you feel the floor. Blam, you just mapped a portion of your floor. Now, you carefully stand up and put your arms out in front of you. You sway your arms in a Lissajous curve in front of you, like Helen Keller searching for a spider's web. Simultaneously, you carefully sweep your feet across the floor, like a modern interpretive dancer looking for any steps, transitions, edges, and pitfalls, so you don't trip.</p>
			<p>Each time <a id="_idIndexMarker704"/>you move forward, you carefully keep track in your mind which direction you are facing and how far you have stepped (<strong class="bold">odometry</strong>). All the time, you are building a mental map of the <a id="_idIndexMarker705"/>room and using your hands and feet as range sensors, giving you a sense of where you are in the room (<strong class="bold">localizing</strong>). Each time you find an obstacle, you store that in your mental map and navigate gingerly around it. You are SLAMing!</p>
			<p>SLAM typically uses some kind of range-finding sensor, such as a lidar sensor:</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="Images/Figure_11.2_B16322.jpg" alt="Figure 11.2 – OS1-128 digital lidar sensor, courtesy of Ouster, Inc." width="1446" height="464"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – OS1-128 digital lidar sensor, courtesy of Ouster, Inc.</p>
			<p>When you were navigating your room, your arms and legs were acting as your range finders. Lidar sensors use laser light, which illuminates the environment and bounces off objects. The time of flight between the light <a id="_idIndexMarker706"/>leaving and returning is used to estimate the range to the object using the speed of light. Lidar sensors, such as the OS1-128, produce rich and dense point clouds with highly accurate distance information:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="Images/Figure_11.3_B16322.jpg" alt="Figure 11.3 – Lidar point cloud in an urban setting, courtesy of Ouster, Inc." width="1650" height="779"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Lidar point cloud in an urban setting, courtesy of Ouster, Inc.</p>
			<p>This distance information is what SLAM algorithms use to localize and map the world.</p>
			<p>An <strong class="bold">Inertial Measurement Unit</strong> (<strong class="bold">IMU</strong>) is also needed to help estimate the pose of the vehicle and estimate <a id="_idIndexMarker707"/>the distance traveled between successive measurements. One reason that Ouster lidar sensors are popular for map creation is that they come with an IMU built in, which lets you start mapping with a single device. Later in the chapter, you will learn how to map with an Ouster lidar sensor and Google Cartographer.</p>
			<p>SLAM is the concept of building a map on the fly with no <em class="italic">a priori</em> information and simultaneously localizing in the map as it is being built. You can imagine that this is very difficult and is a bit of a <em class="italic">chicken or egg</em> problem. To localize, you need a map (the egg) to localize from, but at the same time, in order to build your map on the fly, you need to localize (the chicken) and know where you are on the <a id="_idIndexMarker708"/>map you are trying to build. This is like a problem from a time travel movie: surviving to live long enough to go back in time to save yourself in the first place. Does your head hurt yet?</p>
			<p>The good news is that this field has been studied for over 30 years and has borne beautiful fruit in the form of algorithms for robotics and self-driving cars. Let's see what lies ahead!</p>
			<h3>Types of SLAM</h3>
			<p>The following <a id="_idIndexMarker709"/>is a brief list of some state-of-the-art algorithms used throughout robotics, drone mapping, and self-driving industries. Each of these algorithms has different applications. RGB-D SLAM, for example, is used for camera-based SLAM, while LIO SAM is specific to lidar sensors. Kinetic fusion is another interesting form of SLAM used <a id="_idIndexMarker710"/>to map complex objects indoors. A more complete list can be found on the KITTI website at <a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php">http://www.cvlibs.net/datasets/kitti/eval_odometry.php</a>:</p>
			<ul>
				<li><strong class="bold">LIO SAM</strong>: <a href="https://arxiv.org/pdf/2007.00258.pdf">https://arxiv.org/pdf/2007.00258.pdf</a></li>
				<li><strong class="bold">LOAM</strong>: <a href="https://ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf">https://ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf</a></li>
				<li><strong class="bold">RGB-D SLAM</strong>: <a href="https://felixendres.github.io/rgbdslam_v2/">https://felixendres.github.io/rgbdslam_v2/</a></li>
				<li><strong class="bold">Kinetic fusion</strong>: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf</a></li>
			</ul>
			<p>Next, you will learn about a very important method of reducing error in a SLAM algorithm.</p>
			<h3>Loop closure in SLAM</h3>
			<p>One thing <a id="_idIndexMarker711"/>to consider with mapping and localization is that nothing is perfect. You will never find a sensor that is perfectly accurate. All sensors are probabilistic with some distribution containing a mean and variance of a measurement. These are determined empirically at the factory during the calibration process and then provided in the datasheet. You may ask, <em class="italic">Why do I care?</em></p>
			<p>Good question! The fact that the sensors always have some error means the longer you navigate using these sensors, the more your map and the estimation of your position within that map will drift from reality.</p>
			<p>SLAM algorithms <a id="_idIndexMarker712"/>almost universally have a trick up their sleeve to combat this drift: <strong class="bold">loop closure</strong>! Loop closure works like this. Let's say you pass by the Aldar building on your trip to Abu Dhabi:</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="Images/Figure_11.4_B16322.jpg" alt="Figure 11.4 – The Aldar headquarters building, Abu Dhabi, UAE" width="951" height="533"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The Aldar headquarters building, Abu Dhabi, UAE</p>
			<p>You register this magnificent circular building into your map and continue on your way. Then, sometime later, perhaps after you grabbed lunch at Li Beirut, you drive back, passing the Aldar building a <a id="_idIndexMarker713"/>second time. Now when you pass it, you measure your distance from it and compare that to where you think you are relative to when you first registered it on your map. You realize that you are not where you expect to be relative to it. Snap! The algorithm takes this information and iteratively corrects the entire map to represent where you really are in the world.</p>
			<p>SLAM is constantly doing this with every feature it maps and returns to later. You will see this in action when you play with the open source SLAM in the next few sections. Before that, let's quickly show you some of the available open source mapping tools available to you for your mapping pleasure.</p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor257"/>Open source mapping tools</h1>
			<p>SLAM is quite <a id="_idIndexMarker714"/>complicated to implement and understand, but fortunately, there are plenty of open source solutions that you can use in your self-driving car. The website <em class="italic">Awesome Open Source</em> (<a href="https://awesomeopensource.com/projects/slam">https://awesomeopensource.com/projects/slam</a>) has a treasure <a id="_idIndexMarker715"/>trove of SLAM algorithms that you can use. </p>
			<p>Here is a curated selection to whet your appetite:</p>
			<ul>
				<li><strong class="bold">Cartographer by Google</strong> (<a href="https://github.com/cartographer-project/cartographer">https://github.com/cartographer-project/cartographer</a>)</li>
				<li><strong class="bold">LIO-SAM by TixiaoShan</strong> (<a href="https://github.com/TixiaoShan/LIO-SAM">https://github.com/TixiaoShan/LIO-SAM</a>)</li>
				<li><strong class="bold">LeGO-LOAM by RobustFieldAutonomy</strong> (<a href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM">https://github.com/RobustFieldAutonomyLab/LeGO-LOAM</a>)</li>
			</ul>
			<p>Since Cartographer is by far the most popular and supported, you will get to play with and experience all it has to offer in the next section.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor258"/>SLAM with an Ouster lidar and Google Cartographer</h1>
			<p>This is the <a id="_idIndexMarker716"/>moment you have <a id="_idIndexMarker717"/>been waiting for: building maps with hands-on experience using Cartographer and an Ouster lidar sensor!</p>
			<p>An Ouster lidar <a id="_idIndexMarker718"/>was chosen for this hands-on example because it <a id="_idIndexMarker719"/>has a built-in <strong class="bold">IMU</strong>, which is needed to perform SLAM. This means that you don't need to purchase another sensor to provide the inertial data.</p>
			<p>The example <a id="_idIndexMarker720"/>you will see is the offline processing of data collected from an Ouster sensor and is adapted from the work of Wil Selby. Please visit Wil Selby's website <a id="_idIndexMarker721"/>home page for more cool projects and ideas: <a href="https://www.wilselby.com/">https://www.wilselby.com/</a>.</p>
			<p>Selby also has a related project that performs the SLAM online (in real time) for a DIY driverless car in ROS: <a href="https://github.com/wilselby/diy_driverless_car_ROS">https://github.com/wilselby/diy_driverless_car_ROS</a>.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor259"/>Ouster sensor</h2>
			<p>You can <a id="_idIndexMarker722"/>learn more about the Ouster data format and usage of the sensor from the OS1 user guide:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/blob/master/Chapter11/OS1-User-Guide-v1.14.0-beta.12.pdf">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/blob/master/Chapter11/OS1-User-Guide-v1.14.0-beta.12.pdf</a></p>
			<p>Don't worry, you don't need to buy a sensor to get your hands dirty in this chapter. We have provided you with some sample data collected from an OS1-128 for you to use. You will see later how to download the data.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor260"/>The repo</h2>
			<p>You will <a id="_idIndexMarker723"/>find the code for this chapter in the <strong class="source-inline">ouster_example_cartographer</strong> submodule at the following link: </p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter11">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter11</a></p>
			<p>To ensure that you have the latest code in the submodule, you can run the following command from within the <strong class="source-inline">Chapter11</strong> folder:</p>
			<p class="source-code">$ git submodule update --remote ouster_example_cartographer</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor261"/>Getting started with cartographer_ros</h2>
			<p>Before we <a id="_idIndexMarker724"/>dive into the code, you are encouraged to learn the basics of Cartographer by reading the algorithm walkthrough:</p>
			<p><a href="https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html">https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html</a></p>
			<p>Let's begin with a quick overview of the Cartographer configuration files needed to make it work using your sensor.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor262"/>Cartographer_ros configuration</h2>
			<p>Cartographer <a id="_idIndexMarker725"/>needs the following configuration files to understand about your sensor, robot, transforms, and so on. The files can be found in the <strong class="source-inline">ouster_example_cartographer/cartographer_ros/</strong> folder:</p>
			<ul>
				<li><strong class="source-inline">configuration_files/demo_3d.rviz</strong></li>
				<li><strong class="source-inline">configuration_files/cart_3d.lua</strong></li>
				<li><strong class="source-inline">urdf/os_sensor.urdf</strong></li>
				<li><strong class="source-inline">launch/offline_cart_3d.launch</strong></li>
				<li><strong class="source-inline">configuration_files/assets_writer_cart_3d.lua</strong></li>
				<li><strong class="source-inline">configuration_files/transform.lua</strong></li>
			</ul>
			<p>The files referenced here are for performing offline SLAM on a bag collected from an Ouster sensor.</p>
			<p>Now, let's step through each file and explain how it contributes to making SLAM work inside ROS.</p>
			<h3>demo_3d.rviz</h3>
			<p>This file <a id="_idIndexMarker726"/>sets the configuration of the <strong class="source-inline">rviz</strong> GUI window. It's based on the example file provided in the <strong class="source-inline">cartographer_ros</strong> source files: </p>
			<p><a href="https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/demo_3d.rviz">https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/demo_3d.rviz</a></p>
			<p>It specifies the frames of reference. The details of the various reference frames are available at the following link:</p>
			<p><a href="https://www.ros.org/reps/rep-0105.html">https://www.ros.org/reps/rep-0105.html</a></p>
			<p>The following code snippet is where you will add your frame names based on the sensor you are using for your project:</p>
			<p class="source-code">      Frames:</p>
			<p class="source-code">        All Enabled: true</p>
			<p class="source-code">        base_link:</p>
			<p class="source-code">          Value: true</p>
			<p class="source-code">        map:</p>
			<p class="source-code">          Value: true</p>
			<p class="source-code">        odom:</p>
			<p class="source-code">          Value: true</p>
			<p class="source-code">        os:</p>
			<p class="source-code">          Value: true</p>
			<p class="source-code">        os_imu:</p>
			<p class="source-code">          Value: true</p>
			<p>The following are the definitions of each frame from the preceding code:</p>
			<ul>
				<li><strong class="source-inline">base_link</strong> is the coordinate frame of your robot.</li>
				<li><strong class="source-inline">map</strong> is the fixed coordinate frame of the world.</li>
				<li><strong class="source-inline">odom</strong> is a world-fixed frame that is computed based on odometry from the IMU, wheel encoders, visual odometry, and so on. This can drift over time, but can be useful in maintaining continuous smooth position information without discrete jumps. Cartographer uses this frame to publish non-loop-closing local SLAM results.</li>
				<li><strong class="source-inline">os</strong> is the coordinate frame of the Ouster sensor or any other lidar sensor you have chosen for your project. This is used to transform lidar range readings to the <strong class="source-inline">base_link</strong> frame.</li>
				<li><strong class="source-inline">os_imu</strong> is the coordinate frame of the IMU in the Ouster sensor or any other IMU you have chosen for your project. This is the frame that Cartographer will track during SLAM. It will also be transformed back to the <strong class="source-inline">base_link</strong> frame.</li>
			</ul>
			<p>Next, the <a id="_idIndexMarker727"/>hierarchy <strong class="source-inline">tf</strong> transform tree of frames is defined so that you can transform between any of the frames:</p>
			<p class="source-code">      Tree:</p>
			<p class="source-code">        map:</p>
			<p class="source-code">          odom:</p>
			<p class="source-code">            base_link:</p>
			<p class="source-code">              os:</p>
			<p class="source-code">                {}</p>
			<p class="source-code">              os_imu:</p>
			<p class="source-code">                {}</p>
			<p>You can see that the <strong class="source-inline">os</strong> and <strong class="source-inline">os_imu</strong> frames are both related to <strong class="source-inline">base_link</strong> (the vehicle frame). This means you cannot directly transform from <strong class="source-inline">os</strong> (the lidar frame) to <strong class="source-inline">os_imu</strong> (the IMU frame). Instead, you would transform both into the <strong class="source-inline">base_link</strong> frame. From there, you can transform up the <strong class="source-inline">tf</strong> tree all the way to the map frame. This is what Cartographer will do when building the map using the lidar range measurements and IMU pose measurements.</p>
			<p>Next, <strong class="source-inline">RobotModel</strong> is configured to display the links (meaning sensors, arms, or anything that has a coordinate frame on the robot that you want to track) in their correct pose according to the <strong class="source-inline">tf</strong> transform tree previously defined.</p>
			<p>The following code snippet shows you where to put your link names previously defined in the <strong class="source-inline">Frames</strong> section:</p>
			<p class="source-code">Class: rviz/RobotModel</p>
			<p class="source-code">      Collision Enabled: false</p>
			<p class="source-code">      Enabled: true</p>
			<p class="source-code">      Links:</p>
			<p class="source-code">        All Links Enabled: true</p>
			<p class="source-code">        Expand Joint Details: false</p>
			<p class="source-code">        Expand Link Details: false</p>
			<p class="source-code">        Expand Tree: false</p>
			<p class="source-code">        Link Tree Style: Links in Alphabetic Order</p>
			<p class="source-code">        base_link:</p>
			<p class="source-code">          Alpha: 1</p>
			<p class="source-code">          Show Axes: false</p>
			<p class="source-code">          Show Trail: false</p>
			<p class="source-code">        os:</p>
			<p class="source-code">          Alpha: 1</p>
			<p class="source-code">          Show Axes: false</p>
			<p class="source-code">          Show Trail: false</p>
			<p class="source-code">          Value: true</p>
			<p class="source-code">        os_imu:</p>
			<p class="source-code">          Alpha: 1</p>
			<p class="source-code">          Show Axes: false</p>
			<p class="source-code">          Show Trail: false</p>
			<p class="source-code">          Value: true</p>
			<p>You can see <strong class="source-inline">base_link</strong>, the <strong class="source-inline">os</strong> lidar, and <strong class="source-inline">os_imu</strong> links are added here. </p>
			<p>Next, <strong class="source-inline">rviz/PointCloud2</strong> is mapped to the topic for the <strong class="source-inline">PointCloud2</strong> lidar points data, which for an Ouster lidar <a id="_idIndexMarker728"/>sensor bag file is stored in the <strong class="source-inline">/os_cloud_node/points</strong> topic. If you are using any other lidar sensor, you would place that lidar's topic name in the <strong class="source-inline">Topic:</strong> field:</p>
			<p class="source-code">      Name: PointCloud2</p>
			<p class="source-code">      Position Transformer: XYZ</p>
			<p class="source-code">      Queue Size: 200</p>
			<p class="source-code">      Selectable: true</p>
			<p class="source-code">      Size (Pixels): 3</p>
			<p class="source-code">      Size (m): 0.029999999329447746</p>
			<p class="source-code">      Style: Flat Squares</p>
			<p class="source-code">      Topic: /os_cloud_node/points</p>
			<p>You can see that the topic from the lidar is mapped as a <strong class="source-inline">PointCloud2</strong> type.</p>
			<p>That wraps up the specific configurations for the lidar and IMU sensors in <strong class="source-inline">rviz</strong>. Next, you will see how the <strong class="source-inline">cart_3d.lua</strong> file is modified to match your robot-specific layout.</p>
			<h3>cart_3d.lua</h3>
			<p>This file <a id="_idIndexMarker729"/>sets the configuration of the robot SLAM tuning parameters. A <strong class="source-inline">.lua</strong> file should be robot-specific, rather than bag-specific. It is based on the example file provided in the <strong class="source-inline">cartographer_ros</strong> source files:</p>
			<p><a href="https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/backpack_3d.lua">https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/backpack_3d.lua</a></p>
			<p>You are encouraged to tune the parameters in the <strong class="source-inline">.lua</strong> file based on your specific application. A guide for tuning is available at the following link: </p>
			<p><a href="https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html">https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html</a></p>
			<p>Here, we will touch quickly on some options that you can configure for your self-driving car:</p>
			<p class="source-code">options = {</p>
			<p class="source-code">  map_builder = MAP_BUILDER,</p>
			<p class="source-code">  trajectory_builder = TRAJECTORY_BUILDER,</p>
			<p class="source-code">  map_frame = "<strong class="bold">map</strong>",</p>
			<p class="source-code">  tracking_frame = "<strong class="bold">os_imu</strong>",</p>
			<p class="source-code">  published_frame = "<strong class="bold">base_link</strong>",</p>
			<p class="source-code">  <strong class="bold">odom_frame = "base_link",</strong></p>
			<p class="source-code">  provide_odom_frame = false,</p>
			<p class="source-code">  publish_frame_projected_to_2d = false,</p>
			<p class="source-code">  use_odometry = false,</p>
			<p class="source-code">  use_nav_sat = false,</p>
			<p class="source-code">  use_landmarks = false,</p>
			<p class="source-code">  <strong class="bold">num_laser_scans = 0,</strong></p>
			<p class="source-code">  num_multi_echo_laser_scans = 0,</p>
			<p class="source-code">  num_subdivisions_per_laser_scan = 1,</p>
			<p class="source-code">  <strong class="bold">num_point_clouds = 1</strong>,</p>
			<p class="source-code">  lookup_transform_timeout_sec = 0.2,</p>
			<p class="source-code">  submap_publish_period_sec = 0.3,</p>
			<p class="source-code">  pose_publish_period_sec = 5e-3,</p>
			<p class="source-code">  trajectory_publish_period_sec = 30e-3,</p>
			<p class="source-code">  rangefinder_sampling_ratio = 1.,</p>
			<p class="source-code">  odometry_sampling_ratio = 1.,</p>
			<p class="source-code">  fixed_frame_pose_sampling_ratio = 1.,</p>
			<p class="source-code">  imu_sampling_ratio = 1.,</p>
			<p class="source-code">  landmarks_sampling_ratio = 1.,</p>
			<p class="source-code">}</p>
			<p>The preceding options are configured for offline SLAM from the bag file provided on the Ouster website at the following link:</p>
			<p><a href="https://data.ouster.io/downloads/os1_townhomes_cartographer.zip">https://data.ouster.io/downloads/os1_townhomes_cartographer.zip</a></p>
			<p><a href="https://data.ouster.io/downloads/os1_townhomes_cartographer.zip">https://data.ouster.io/downloads/os1_townhomes_cartographer.zip</a></p>
			<p>You will <a id="_idIndexMarker730"/>need to modify the highlighted ones if you are doing online (real-time) SLAM on your self-driving car:</p>
			<ul>
				<li><strong class="source-inline">odom_frame = "base_link"</strong>: This should be set to <strong class="source-inline">odom</strong> so that Cartographer publishes the non-loop-closing continuous pose as <strong class="source-inline">odom_frame</strong>.</li>
				<li><strong class="source-inline">provide_odom_frame = false</strong>: This should be set to <strong class="source-inline">true</strong> so that Cartographer knows that the <strong class="source-inline">odom_frame</strong> is published.</li>
				<li><strong class="source-inline">num_laser_scans = 0</strong>: This should be set to <strong class="source-inline">1</strong> so that the lidar sensor's scan data is used straight from the sensor, rather than from point clouds from a bag file.</li>
				<li><strong class="source-inline">num_point_clouds = 1</strong>: This should be set to <strong class="source-inline">0</strong> if not using a bag file and you are instead using a live lidar scan.</li>
			</ul>
			<p>Next, you will see how the sensor <strong class="source-inline">urd</strong> file is configured. </p>
			<h3>os_sensor.urdf</h3>
			<p>This file is <a id="_idIndexMarker731"/>used to configure the physical transforms of your self-driving car. Each sensor you mount on the vehicle will be a link. Think of links as rigid bodies, like links in a chain. Each link is rigid in a chain, but the links can move relative to each other and each have their own coordinate frames.</p>
			<p>In this file, you will see that we have set up the Ouster sensor as the robot, <strong class="source-inline">&lt;robot name="os_sensor"&gt;</strong>.</p>
			<p>We added links that represent the lidar coordinate frame, <strong class="source-inline">&lt;link name="os_lidar"&gt;</strong>, and the IMU coordinate frame, <strong class="source-inline">&lt;link name="os_imu"&gt;</strong>, of the sensor.</p>
			<p>The following code shows how we provide the transforms from each frame back to the <strong class="source-inline">base_link</strong> frame:</p>
			<p class="source-code">  &lt;joint name="sensor_link_joint" type="fixed"&gt;</p>
			<p class="source-code">    &lt;parent link="<strong class="bold">base_link</strong>" /&gt;</p>
			<p class="source-code">    &lt;child link="<strong class="bold">os_sensor</strong>" /&gt;</p>
			<p class="source-code">    &lt;origin <strong class="bold">xyz="0 0 0" rpy="0 0 0"</strong> /&gt;</p>
			<p class="source-code">  &lt;/joint&gt;</p>
			<p class="source-code">  &lt;joint name="imu_link_joint" type="fixed"&gt;</p>
			<p class="source-code">    &lt;parent link="<strong class="bold">os_sensor</strong>" /&gt;</p>
			<p class="source-code">    &lt;child link="<strong class="bold">os_imu</strong>" /&gt;</p>
			<p class="source-code">    &lt;origin <strong class="bold">xyz="0.006253 -0.011775 0.007645" rpy="0 0 0"</strong> /&gt;</p>
			<p class="source-code">  &lt;/joint&gt;</p>
			<p class="source-code">  &lt;joint name="os1_link_joint" type="fixed"&gt;</p>
			<p class="source-code">    &lt;parent link="<strong class="bold">os_sensor</strong>" /&gt;</p>
			<p class="source-code">    &lt;child link="<strong class="bold">os_lidar</strong>" /&gt;</p>
			<p class="source-code">    &lt;origin <strong class="bold">xyz="0.0 0.0 0.03618" rpy="0 0 3.14159"</strong> /&gt;</p>
			<p class="source-code">  &lt;/joint&gt;</p>
			<p>You can see that <strong class="source-inline">os_sensor</strong> is placed at the center of the <strong class="source-inline">base_link</strong> coordinate frame, while <strong class="source-inline">os_imu</strong> and <strong class="source-inline">os_lidar</strong> are given their respective translations and rotations relative to <strong class="source-inline">os_sensor</strong>. These translations <a id="_idIndexMarker732"/>and rotations are provided in the Ouster sensor user guide under <em class="italic">Section 8</em>:</p>
			<p><a href="https://github.com/Krishtof-Korda/ouster_example_cartographer/blob/master/OS1-User-Guide-v1.14.0-beta.12.pdf">https://github.com/Krishtof-Korda/ouster_example_cartographer/blob/master/OS1-User-Guide-v1.14.0-beta.12.pdf</a></p>
			<p>Next, you will learn how the launch file is configured to call all the previous configuration files and launch the SLAM process.</p>
			<h3>offline_cart_3d.launch</h3>
			<p>This file <a id="_idIndexMarker733"/>is used to call all the configuration files previously discussed.</p>
			<p>It also remaps the <strong class="source-inline">points2</strong> and <strong class="source-inline">imu</strong> topics to the bag file Ouster <strong class="source-inline">os_cloud_node</strong> topics. If you are using another type of lidar sensor, simply use the topic name of that sensor in place:</p>
			<p class="source-code">    &lt;remap from="points2" to="<strong class="bold">/os_cloud_node/points" </strong>/&gt;</p>
			<p class="source-code">    &lt;remap from="imu" to="<strong class="bold">/os_cloud_node/imu" </strong>/&gt;</p>
			<p>Next, you will learn how the <strong class="source-inline">assets_writer_cart_3d.lua</strong> file is used to save the map data.</p>
			<h3>assets_writer_cart_3d.lua</h3>
			<p>This file <a id="_idIndexMarker734"/>is used to configure the options for generating the fully aggregated point cloud that will be output in <strong class="source-inline">.ply</strong> format.</p>
			<p>You can set the <strong class="source-inline">VOXEL_SIZE</strong> value that should be used to downsample the points and only take the centroid. This is important since without down sampling, you would need tremendous processing cycles.</p>
			<h3>VOXEL_SIZE = 5e-2</h3>
			<p>You also <a id="_idIndexMarker735"/>set <strong class="source-inline">min_max_range_filter</strong>, which only keeps points that are within a specified range from the lidar sensor. This is usually based on the specs in the datasheet of the lidar sensor. The Ouster OS1 datasheet can be found on the Ouster (<a href="https://outser.com/">https://outser.com/</a>) website.</p>
			<p>The following <a id="_idIndexMarker736"/>code snippet shows where you can configure the range filter options:</p>
			<p class="source-code">  tracking_frame = "<strong class="bold">os_imu</strong>",</p>
			<p class="source-code">  pipeline = {</p>
			<p class="source-code">    {</p>
			<p class="source-code">      action = "min_max_range_filter",</p>
			<p class="source-code">      min_range = 1.,</p>
			<p class="source-code">      max_range = 60.,</p>
			<p class="source-code">    },</p>
			<p>Finally, you <a id="_idIndexMarker737"/>will learn how the <strong class="source-inline">transform.lua</strong> file is used to do 2D projections.</p>
			<h3>The transform.lua file</h3>
			<p>This file is <a id="_idIndexMarker738"/>a generic file for performing transforms and is used in the previous file to create the 2D map x-ray and probability grid images.</p>
			<p>Fantastic, now that you understand what each configuration file does, it's time for you to see it in action! The next section will guide you through running SLAM using a prebuilt Docker image. This will hopefully get you SLAMing quicker than you can say <em class="italic">The cars of the future will drive us!</em></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor263"/>Docker image</h2>
			<p>A Docker image <a id="_idIndexMarker739"/>has been created for you to download. This will help to ensure that all the required packages are installed and minimizes the time you need to get everything working.</p>
			<p>If you are running on a Linux operating system, you can simply run <a href="http://install-docker.sh">install-docker.sh</a>, located in the <strong class="source-inline">ouster_example_cartographer</strong> submodule, with the following command:</p>
			<p class="source-code">$ ./install-docker.sh </p>
			<p>If you are on another operating system (Windows 10 or macOS), you can download and install Docker <a id="_idIndexMarker740"/>directly from their website:</p>
			<p><a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a></p>
			<p>You can verify that Docker was installed correctly with the following command:</p>
			<p class="source-code">$ docker –version</p>
			<p>Great! Hopefully, things have gone smoothly, and you are ready to run the Docker image in a container. It is highly recommended to use a Linux machine with an Nvidia graphics card in order to make the code and Docker image work. The <strong class="source-inline">run-docker.sh</strong> script is provided to help start Docker with the correct options <a id="_idIndexMarker741"/>for your graphics processor. It is highly recommended to use a Nvidia GPU to process the SLAM efficiently. You can use other GPUs but the support for them is low.</p>
			<p>The following section will provide you some troubleshooting steps for connecting Docker with your Nvidia GPU.</p>
			<h3>Docker Nvidia troubleshooting</h3>
			<p>Depending <a id="_idIndexMarker742"/>on the Nvidia setup on your Linux machine, you may need to perform the following commands before connecting to your Docker container:</p>
			<p class="source-code"># Stop docker before running 'sudo dockerd --add-runtime=nvidia=/usr/bin/nvidia-container-runtime'</p>
			<p class="source-code">$ sudo systemctl stop docker</p>
			<p class="source-code"># Change mode of docker.sock if you have a permission issue</p>
			<p class="source-code">$ sudo chmod 666 /var/run/docker.sock</p>
			<p class="source-code"># Add the nvidia runtime to allow docker to use nvidia GPU</p>
			<p class="source-code"># This needs to be run in a separate shell from run-docker.sh</p>
			<p class="source-code">$ sudo dockerd --add-runtime=nvidia=/usr/bin/nvidia-container-runtime</p>
			<p>Now, you can run Docker and connect it to your GPU with the following command:</p>
			<p class="source-code">$ ./run-docker.sh</p>
			<p>This script will pull the latest Docker image from Docker Hub and run the image either with the Nvidia runtime, if available, or simply on the CPU.</p>
			<p>This file <a id="_idIndexMarker743"/>also has many useful commands in the comments for running Cartographer in 2D or 3D mode. You will learn about 3D mode here.</p>
			<p>The next few sections will walk you through the steps of performing SLAM on the data you will download from Ouster.</p>
			<h3>Getting the sample data</h3>
			<p>The sample <a id="_idIndexMarker744"/>data that you will be SLAMing is available from the Ouster website.</p>
			<p>Download it with the following commands:</p>
			<p class="source-code">$ mkdir /root/bags</p>
			<p class="source-code">$ cd /root/bags</p>
			<p class="source-code">$ curl -O https://data.ouster.io/downloads/os1_townhomes_cartographer.zip</p>
			<p class="source-code">$ unzip /root/bags/os1_townhomes_cartographer.zip -d /root/bags/</p>
			<h3>Sourcing the workspace</h3>
			<p>You will <a id="_idIndexMarker745"/>need to source the <strong class="source-inline">catkin</strong> workspace to ensure it is set up with ROS:</p>
			<p class="source-code">$ source /root/catkin_ws/devel/setup.bash</p>
			<h3>Validating rosbag</h3>
			<p>It is a good <a id="_idIndexMarker746"/>idea to validate <strong class="source-inline">rosbag</strong> using the built-in cartographer bag validation tool. This will ensure that the bag has continuous data and will produce results:</p>
			<p class="source-code">$ rosrun cartographer_ros cartographer_rosbag_validate -bag_filename /root/bags/os1_townhomes_cartographer.bag</p>
			<h3>Preparing to launch</h3>
			<p>To run your <a id="_idIndexMarker747"/>offline SLAM on the bag, you first need to get to the launchpad:</p>
			<p class="source-code">$ cd /root/catkin_ws/src/ouster_example_cartographer/cartographer_ros/launch</p>
			<h3>Launching offline on the bag</h3>
			<p>Now, you <a id="_idIndexMarker748"/>are ready to launch the offline SLAM. This will create a <strong class="source-inline">.pbstream</strong> file that will be used later to write your assets, such as the following:</p>
			<ul>
				<li><strong class="source-inline">.ply</strong>, the point cloud file</li>
				<li>A 2D x-ray image of the mapped space</li>
				<li>A 2D probability grid image of an open versus an occupied area</li>
			</ul>
			<p>The following command will launch the offline SLAM process on your bag file:</p>
			<p class="source-code">$ roslaunch offline_cart_3d.launch bag_filenames:=/root/bags/os1_townhomes_cartographer.bag</p>
			<p>You should see an <strong class="source-inline">rviz</strong> window open that looks something like that in the following figure:</p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="Images/Figure_11.5_B16322.jpg" alt="Figure 11.5 – The rviz window Cartographer launch" width="1127" height="636"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – The rviz window Cartographer launch</p>
			<p>Now, you can sit back and watch in wonder as Cartographer meticulously performs SLAM.</p>
			<p>First, it will <a id="_idIndexMarker749"/>make smaller local submaps. Then, it will scan match the submap to the global map. You will notice that it snaps the point cloud every few seconds when it has collected enough data to match the global map.</p>
			<p>When the process is complete, you will have a file in the <strong class="source-inline">/root/bags</strong> folder named <strong class="source-inline">os1_townhomes_cartographer.bag.pbstream</strong>. You will use this file to write your assets.</p>
			<h3>Writing your sweet, sweet assets</h3>
			<p>I hope you <a id="_idIndexMarker750"/>are ready because you are about to get the final product from SLAM – a map of some random street you have never seen before. Just what you dreamed of, right? </p>
			<p>Run the following command to collect your prize! </p>
			<p class="source-code">$ roslaunch assets_writer_cart_3d.launch bag_filenames:=/root/bags/os1_townhomes_cartographer.bag  pose_graph_filename:=/root/bags/os1_townhomes_cartographer.bag.pbstream</p>
			<p>This will <a id="_idIndexMarker751"/>take a while; go grab a bite of your favorite comfort food. We will see you back here in an hour.</p>
			<p>Welcome back! Feast your eyes on your prizes!</p>
			<h3>Opening your first prize</h3>
			<p>Voila! Your <a id="_idIndexMarker752"/>very own x-ray 2D map!</p>
			<p class="source-code">$ xdg-open os1_townhomes_cartographer.bag_xray_xy_all.png</p>
			<p>This is how the output appears:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="Images/Figure_11.6_B16322.jpg" alt="Figure 11.6 – 2D x-ray map of townhomes" width="1383" height="1057"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – 2D x-ray map of townhomes</p>
			<h3>Opening your second prize</h3>
			<p>Shazam! Your <a id="_idIndexMarker753"/>very own probability grid 2D map!</p>
			<p class="source-code">$ xdg-open os1_townhomes_cartographer.bag_probability_grid.png</p>
			<p>This is how the output appears:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="Images/Figure_11.7_B16322.jpg" alt="Figure 11.7 – 2D probability grid map of townhomes" width="1179" height="818"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – 2D probability grid map of townhomes</p>
			<h3>Your final prize</h3>
			<p>You will find a file in the <strong class="source-inline">/root/bags</strong> folder that is named <strong class="source-inline">os1_townhomes_cartographer.bag_points.ply</strong>. This prize will take a little more effort to truly appreciate.</p>
			<p>You can <a id="_idIndexMarker754"/>use any tool that is capable of opening a <strong class="source-inline">.ply</strong> file. CloudCompare is a <strong class="bold">FOSS</strong> (that is, a <strong class="bold">free open source software</strong>) tool for <a id="_idIndexMarker755"/>this and can be downloaded from the following link:</p>
			<p><a href="https://www.danielgm.net/cc/">https://www.danielgm.net/cc/</a></p>
			<p>You can also use CloudCompare to save your <strong class="source-inline">.ply</strong> file into other formats, such as XYZ, XYZRGB, CGO, ASC, CATIA ASC, PLY, LAS, PTS, or PCD.</p>
			<p><strong class="source-inline">unitycoder</strong> has good instructions for making the conversion available at the following link:</p>
			<p><a href="https://github.com/unitycoder/UnityPointCloudViewer/wiki/Converting-Points-Clouds-with-CloudCompare">https://github.com/unitycoder/UnityPointCloudViewer/wiki/Converting-Points-Clouds-with-CloudCompare</a></p>
			<p>This is how the output appears:</p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="Images/Figure_11.8_B16322.jpg" alt="Figure 11.8 – Point cloud 3D map viewed in CloudCompare" width="1133" height="621"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Point cloud 3D map viewed in CloudCompare</p>
			<p>Have a <a id="_idIndexMarker756"/>look at <em class="italic">Figure 11.8</em> and <em class="italic">Figure 11.9</em>, which show what the 3D map of points looks like using the CloudCompare viewer:</p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="Images/Figure_11.9_B16322.jpg" alt="Figure 11.9 – Point cloud 3D map viewed in CloudCompare, top-view" width="1136" height="621"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Point cloud 3D map viewed in CloudCompare, top-view</p>
			<p>Congratulations <a id="_idIndexMarker757"/>on making your first of what we hope are many maps! This is just the beginning of your journey and we can't wait to see what you make with your newfound skills! Next, we will summarize everything you learned.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor264"/>Summary</h1>
			<p>Wow, you have come a long way in this chapter and book. You began with nothing but a mobile phone and a blue GPS dot. You traveled across the globe to Russia and found the life-juice at the Monkey Face. You grabbed some snacks by SLAMing your way through your Cimmerian dark home. You learned the difference between maps and localization, and the various types of each. You picked up some open source tools and lashed them to your adventure belt for future use.</p>
			<p>You also learned how to apply the open source Cartographer on Ouster OS1-128 lidar sensor data, coupled with the built-in IMU to generate dense and tangible maps of some really nice townhomes that you manipulated using CloudCompare. Now you know how to create maps and can go out and map your own spaces and localize within them! The world is your Ouster (pardon me, oyster)! We can't wait to see what you build next with your creativity and knowhow!</p>
			<p>We really hope that you enjoyed learning with us; we certainly enjoyed sharing this knowledge with you and hope you are inspired to build the future!</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor265"/>Questions</h1>
			<p>You should be able to answer the following questions now:</p>
			<ol>
				<li value="1">What is the difference between mapping and localization?</li>
				<li>What frame does Cartographer typically use as the tracking frame?</li>
				<li>Why is SLAM needed?</li>
				<li>In which file do you set <strong class="source-inline">min_max_range_filter</strong>?</li>
			</ol>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor266"/>Further reading</h1>
			<ul>
				<li>W. Hess, D. Kohler, H. Rapp, and D. Andor, <em class="italic">Real-Time Loop Closure in 2D LIDAR SLAM</em>: <a href="https://opensource.googleblog.com/2016/10/introducing-cartographer.html">https://opensource.googleblog.com/2016/10/introducing-cartographer.html</a> (<a href="https://research.google/pubs/pub45466/">https://research.google/pubs/pub45466/</a>), in <em class="italic">Robotics and Automation (ICRA)</em>, 2016 IEEE International Conference on. IEEE, 2016. pp. 1271–1278.</li>
				<li>Cartographer: <a href="https://github.com/cartographer-project/cartographer_ros">https://github.com/cartographer-project/cartographer_ros</a></li>
				<li>More on Cartographer: <a href="https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html">https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html</a></li>
				<li>Localization types: <a href="https://www.cpp.edu/~ftang/courses/CS521/notes/Localization.pdf">https://www.cpp.edu/~ftang/courses/CS521/notes/Localization.pdf</a></li>
				<li>RGB-D SLAM: <a href="https://felixendres.github.io/rgbdslam_v2/">https://felixendres.github.io/rgbdslam_v2/</a></li>
				<li>Probabilistic algorithms in robotics: <a href="http://robots.stanford.edu/papers/thrun.probrob.pdf">http://robots.stanford.edu/papers/thrun.probrob.pdf</a></li>
			</ul>
		</div>
	</div>



  </body></html>