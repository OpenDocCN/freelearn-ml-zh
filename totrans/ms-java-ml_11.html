<html><head></head><body><div class="appendix" title="Appendix&#xA0;B.&#xA0;Probability"><div class="titlepage"><div><div><h1 class="title"><a id="appB"/>Appendix B. Probability</h1></div></div></div><p>Essential concepts in probability are presented here in brief. </p><div class="section" title="Axioms of probability"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec83"/>Axioms of probability</h1></div></div></div><p>Kolmogorov's <a id="id1931" class="indexterm"/>axioms of probability can be stated in terms of the sample space <span class="emphasis"><em>S</em></span> of possible events, <span class="emphasis"><em>E</em></span>1, <span class="emphasis"><em>E</em></span>2, <span class="emphasis"><em>E</em></span>3, …<span class="emphasis"><em>E</em></span>n and the real-valued probability <span class="emphasis"><em>P(E)</em></span> of an event <span class="emphasis"><em>E</em></span>. The axioms are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>P(E) ≥ 0 for all E </em></span><span class="emphasis"><em>ϵ</em></span><span class="emphasis"><em> S</em></span></li><li class="listitem"><span class="emphasis"><em>P(S) = 1</em></span></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_10_image059.jpg" alt="Axioms of probability"/></span></li></ol></div><p>Together, these axioms say that probabilities cannot be negative numbers—impossible events have zero probability—no events outside the sample space are possible as it is the universe of possibilities under consideration, and that the probability of either of two mutually exclusive events occurring is equal to the sum of their individual probabilities. </p></div></div>
<div class="section" title="Bayes' theorem"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec84"/>Bayes' theorem</h1></div></div></div><p>The probability <a id="id1932" class="indexterm"/>of an event <span class="strong"><strong>E</strong></span> conditioned on evidence <span class="strong"><strong>X</strong></span> is proportional to the prior probability of the event and the likelihood of the evidence given that the event has occurred. This is Bayes' Theorem:</p><div class="mediaobject"><img src="graphics/B05137_10_image062.jpg" alt="Bayes' theorem"/></div><p>
<span class="emphasis"><em>P(X)</em></span> is the normalizing constant, which is also called the marginal probability of <span class="emphasis"><em>X</em></span>. <span class="emphasis"><em>P(E)</em></span> is the prior, and <span class="emphasis"><em>P(X|E)</em></span> is the likelihood. <span class="emphasis"><em>P(E|X)</em></span> is also called the posterior probability. </p><p>Bayes' <a id="id1933" class="indexterm"/>Theorem expressed in terms of the posterior and prior odds is known as Bayes' Rule.</p><div class="section" title="Density estimation"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec160"/>Density estimation</h2></div></div></div><p>Estimating <a id="id1934" class="indexterm"/>the hidden probability density function of a random <a id="id1935" class="indexterm"/>variable from sample data randomly drawn from the population is known as density estimation. Gaussian mixtures and kernel density estimates are examples used in feature engineering, data modeling, and clustering.</p><p>Given a probability density function <span class="emphasis"><em>f(X)</em></span> for a random variable <span class="emphasis"><em>X</em></span>, the probabilities associated with the values of <span class="emphasis"><em>X</em></span> can be found as follows:</p><div class="mediaobject"><img src="graphics/B05137_10_image069.jpg" alt="Density estimation"/></div><p>Density estimation can be parametric, where it is assumed that the data is drawn from a known family of distributions and <span class="emphasis"><em>f(x)</em></span> is estimated by estimating the parameters of the distribution, for example, µ and σ<sup>2</sup> in the case of a normal distribution. The other approach is non-parametric, where no assumption is made about the distribution of the observed data and the data is allowed to determine the form of the distribution. </p></div><div class="section" title="Mean"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec161"/>Mean</h2></div></div></div><p>The long-run <a id="id1936" class="indexterm"/>average value of a random variable is known as the <a id="id1937" class="indexterm"/>expectation or mean. The sample mean is the corresponding average over the observed data. </p><p>In the case of a discrete random variable, the mean is given by:</p><div class="mediaobject"><img src="graphics/B05137_10_image073.jpg" alt="Mean"/></div><p>For example, the mean number of pips turning up on rolling a single fair die is 3.5.</p><p>For a <a id="id1938" class="indexterm"/>continuous random variable with probability density function <span class="emphasis"><em>f(x)</em></span>, the <a id="id1939" class="indexterm"/>mean is:</p><div class="mediaobject"><img src="graphics/B05137_10_image075.jpg" alt="Mean"/></div></div><div class="section" title="Variance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec162"/>Variance</h2></div></div></div><p>Variance <a id="id1940" class="indexterm"/>is the expectation of the square of the difference <a id="id1941" class="indexterm"/>between the random variable and its mean.</p><p>In the discrete case, with the mean defined as previously discussed, and with the probability mass function <span class="emphasis"><em>p(x)</em></span>, the variance is:</p><div class="mediaobject"><img src="graphics/B05137_10_image077.jpg" alt="Variance"/></div><p>In the continuous case, it is as follows:</p><div class="mediaobject"><img src="graphics/B05137_10_image078.jpg" alt="Variance"/></div><p>Some continuous distributions do not have a mean or variance. </p></div><div class="section" title="Standard deviation"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec163"/>Standard deviation</h2></div></div></div><p>Standard <a id="id1942" class="indexterm"/>deviation is a measure of how spread out the <a id="id1943" class="indexterm"/>data is in relation to its mean value. It is the square root of variance, and unlike variance, it is expressed in the same units as the data. The standard <a id="id1944" class="indexterm"/>deviation in the case of discrete and continuous random <a id="id1945" class="indexterm"/>variables are given here:,</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Discrete case:<div class="mediaobject"><img src="graphics/B05137_10_image079.jpg" alt="Standard deviation"/></div></li><li class="listitem" style="list-style-type: disc">Continuous case:<div class="mediaobject"><img src="graphics/B05137_10_image080.jpg" alt="Standard deviation"/></div></li></ul></div></div><div class="section" title="Gaussian standard deviation"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec164"/>Gaussian standard deviation</h2></div></div></div><p>The <a id="id1946" class="indexterm"/>standard deviation of a sample drawn <a id="id1947" class="indexterm"/>randomly from a larger population is a biased estimate of the population standard deviation. Based on the particular distribution, the correction to this biased estimate can differ. For a Gaussian or normal distribution, the variance is adjusted by a value of <span class="inlinemediaobject"><img src="graphics/B05137_10_image081.jpg" alt="Gaussian standard deviation"/></span> .</p><p>Per the definition given earlier, the biased estimate <span class="emphasis"><em>s</em></span> is given by:</p><div class="mediaobject"><img src="graphics/B05137_10_image083.jpg" alt="Gaussian standard deviation"/></div><p>In the preceding equation, <span class="inlinemediaobject"><img src="graphics/B05137_10_image084.jpg" alt="Gaussian standard deviation"/></span> is the sample mean.</p><p>The <a id="id1948" class="indexterm"/>unbiased estimate, which <a id="id1949" class="indexterm"/>uses Bessel's correction, is:</p><div class="mediaobject"><img src="graphics/B05137_10_image085.jpg" alt="Gaussian standard deviation"/></div></div><div class="section" title="Covariance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec165"/>Covariance</h2></div></div></div><p>In a<a id="id1950" class="indexterm"/> joint distribution of two random variables, the expectation <a id="id1951" class="indexterm"/>of the product of the deviations of the random variables from their respective means is called the covariance. Thus, for two random variables <span class="strong"><strong>X</strong></span> and <span class="strong"><strong>Y</strong></span>, the equation is as follows:</p><div class="mediaobject"><img src="graphics/B05137_10_image087.jpg" alt="Covariance"/></div><p>= <span class="emphasis"><em>E[XY] – μ</em></span>x<span class="emphasis"><em> μ</em></span>y</p><p>If the two random variables are independent, then their covariance is zero.</p></div><div class="section" title="Correlation coefficient"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec166"/>Correlation coefficient</h2></div></div></div><p>When <a id="id1952" class="indexterm"/>the covariance is normalized by the product <a id="id1953" class="indexterm"/>of the standard deviations of the two random variables, we get the correlation coefficient <span class="emphasis"><em>ρ</em></span><sub>X,Y</sub>, also known as the Pearson product-moment correlation coefficient: </p><div class="mediaobject"><img src="graphics/B05137_10_image091.jpg" alt="Correlation coefficient"/></div><p>The correlation coefficient can take values between -1 and 1 only. A coefficient of +1 means a perfect increasing linear relationship between the random variables. -1 means a perfect decreasing linear relationship. If the two variables are <a id="id1954" class="indexterm"/>independent of each other, the Pearson's coefficient is 0.</p></div><div class="section" title="Binomial distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec167"/>Binomial distribution</h2></div></div></div><p>Discrete <a id="id1955" class="indexterm"/>probability distribution with parameters <span class="strong"><strong>n</strong></span> and <span class="strong"><strong>p</strong></span>. A random <a id="id1956" class="indexterm"/>variable is a binary variable, with the probability of outcome given by <span class="strong"><strong>p</strong></span> and <span class="strong"><strong>1 – p</strong></span> in a single trial. The probability mass function gives the probability of <span class="strong"><strong>k</strong></span> successes out of <span class="strong"><strong>n</strong></span> independent trials. </p><p>Parameters: <span class="emphasis"><em>n, k</em></span>
</p><p>PMF:</p><div class="mediaobject"><img src="graphics/B05137_10_image097.jpg" alt="Binomial distribution"/></div><p>Where: </p><div class="mediaobject"><img src="graphics/B05137_10_image098.jpg" alt="Binomial distribution"/></div><p>This is the Binomial coefficient.</p><p>Mean: E[<span class="emphasis"><em>X</em></span>] = <span class="emphasis"><em>np</em></span>
</p><p>Variance: <span class="emphasis"><em>Var(X)</em></span> = <span class="emphasis"><em>np</em></span>(1 – <span class="emphasis"><em>p</em></span>) </p></div><div class="section" title="Poisson distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec168"/>Poisson distribution</h2></div></div></div><p>The <a id="id1957" class="indexterm"/>Poisson distribution gives the probability <a id="id1958" class="indexterm"/>of the number of occurrences of an event in a given time period or in a given region of space.</p><p>Parameter λ, is the average number of occurrences in a given interval. The probability mass function of observing <span class="emphasis"><em>k</em></span> events in that interval is</p><p>PMF: </p><div class="mediaobject"><img src="graphics/B05137_10_image102.jpg" alt="Poisson distribution"/></div><p>Mean: E[<span class="emphasis"><em>X</em></span>] = λ</p><p>Variance: <span class="emphasis"><em>Var(X)</em></span> = λ</p></div><div class="section" title="Gaussian distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec169"/>Gaussian distribution</h2></div></div></div><p>The <a id="id1959" class="indexterm"/>Gaussian distribution, also known as the <a id="id1960" class="indexterm"/>normal distribution, is a continuous probability distribution. Its probability density function is expressed in terms of the mean and variance as follows:</p><div class="mediaobject"><img src="graphics/B05137_10_image105.jpg" alt="Gaussian distribution"/></div><p>Mean: µ</p><p>Standard deviation: σ</p><p>Variance: σ2</p><p>The standard normal distribution is the case when the mean is 0 and the standard deviation is 1. The PDF of the standard normal distribution is given as follows:</p><div class="mediaobject"><img src="graphics/B05137_10_image109.jpg" alt="Gaussian distribution"/></div></div><div class="section" title="Central limit theorem"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec170"/>Central limit theorem</h2></div></div></div><p>The <a id="id1961" class="indexterm"/>central limit theorem says that when you have <a id="id1962" class="indexterm"/>several independent and identically distributed random variables with a distribution that has a well-defined mean and variance, the average value (or sum) over a large number of these observations is approximately normally distributed, irrespective of the parent distribution. Furthermore, the limiting normal distribution has the same mean as the parent distribution and a variance equal to the underlying variance divided by the sample size.</p><p>Given a random sample <span class="emphasis"><em>X</em></span>1, <span class="emphasis"><em>X</em></span>2, <span class="emphasis"><em>X</em></span>3 … <span class="emphasis"><em>X</em></span>n with <span class="emphasis"><em>µ</em></span> = <span class="emphasis"><em>E</em></span>[<span class="emphasis"><em>X</em></span>i] and <span class="emphasis"><em>σ</em></span>2<span class="emphasis"><em> = Var(X</em></span>i<span class="emphasis"><em>)</em></span>, the sample mean:<span class="inlinemediaobject"><img src="graphics/B05137_10_image113.jpg" alt="Central limit theorem"/></span></p><p> is approximately normal <span class="inlinemediaobject"><img src="graphics/B05137_10_image114.jpg" alt="Central limit theorem"/></span>
</p><p>There <a id="id1963" class="indexterm"/>are several variants of the central limit theorem <a id="id1964" class="indexterm"/>where independence or the constraint of being identically distributed are relaxed, yet convergence to the normal distribution still follows.</p></div><div class="section" title="Error propagation"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec171"/>Error propagation</h2></div></div></div><p>Suppose <a id="id1965" class="indexterm"/>there is a random variable <span class="emphasis"><em>X</em></span>, which is a <a id="id1966" class="indexterm"/>function of multiple observations each with their own distributions. What can be said about the mean and variance of <span class="emphasis"><em>X</em></span> given the corresponding values for measured quantities that make up <span class="emphasis"><em>X</em></span>? This is the problem of error propagation.</p><p>Say <span class="emphasis"><em>x</em></span> is the quantity to be determined via observations of variables <span class="emphasis"><em>u</em></span>, <span class="emphasis"><em>v</em></span>, and so on:</p><p>
<span class="emphasis"><em>x = f(u, v, )</em></span>
</p><p>Let us assume that:</p><div class="mediaobject"><img src="graphics/B05137_10_image118.jpg" alt="Error propagation"/></div><p>The uncertainty in <span class="emphasis"><em>x</em></span> in terms of the variances of <span class="emphasis"><em>u</em></span>, <span class="emphasis"><em>v</em></span>, and so on, can be expressed by the variance of <span class="emphasis"><em>x</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_10_image119.jpg" alt="Error propagation"/></div><p>From the Taylor expansion of the variance of <span class="emphasis"><em>x</em></span>, we get the following:</p><div class="mediaobject"><img src="graphics/B05137_10_image120.jpg" alt="Error propagation"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B05137_10_image500.jpg" alt="Error propagation"/></span> is the covariance.</p><p>Similarly, we can determine the propagation error of the mean. Given <span class="emphasis"><em>N</em></span> measurements with <span class="emphasis"><em>x</em></span><sub>i</sub> with <a id="id1967" class="indexterm"/>uncertainties characterized by <span class="emphasis"><em>s</em></span>i, the following can be written:</p><div class="mediaobject"><img src="graphics/B05137_10_image125.jpg" alt="Error propagation"/></div><p>With:</p><div class="mediaobject"><img src="graphics/B05137_10_image126.jpg" alt="Error propagation"/></div><p>These equations assume that the <a id="id1968" class="indexterm"/>covariance is 0. </p><p>Suppose <span class="emphasis"><em>s</em></span>i<span class="emphasis"><em> = s</em></span> – that is, all observations have the same error. </p><p>Then, <span class="inlinemediaobject"><img src="graphics/B05137_10_image128.jpg" alt="Error propagation"/></span>.</p><p>Since <span class="inlinemediaobject"><img src="graphics/B05137_10_image129.jpg" alt="Error propagation"/></span>
</p><p>Therefore, <span class="inlinemediaobject"><img src="graphics/B05137_10_image134.jpg" alt="Error propagation"/></span>.</p></div></div></body></html>