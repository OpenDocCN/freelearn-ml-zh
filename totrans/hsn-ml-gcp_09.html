<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Networks with TensorFlow and Keras</h1>
                </header>
            
            <article>
                
<p>Neural network is a supervised learning algorithm that is loosely inspired by the way the brain functions. Similarly to the way neurons are connected to each other in the brain, a neural network takes an input and passes it through a function, based on which certain subsequent neurons get excited, and the output is produced.</p>
<p>In this chapter, we will focus on the practical implementation of neural networks with TensorFlow and Keras. TensorFlow provides a low-level framework to create neural network models. Keras is a high-level neural network API that significantly simplifies the task of defining neural network models. We'll show how to use Keras on top of TensorFlow to define and train models on GCP. We'll present the Keras API in Python and work with a simple feedforward network applied on the classic MNIST dataset. Also, we will go through the different components of a neural network:</p>
<ul>
<li>Initialization</li>
<li>Metrics and loss functions</li>
<li>Activation functions</li>
<li>Depth of the network</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of a neural network</h1>
                </header>
            
            <article>
                
<p>The origin of neural networks comes from the fact that every function cannot be approximated by a linear/logistic regression—there can be potentially complex shapes within data that can only be approximated by complex functions.</p>
<p>The more complex the function (with some way to take care of overfitting), the better the <span>prediction </span>accuracy.</p>
<p>The following image explains the way in which neural networks work towards fitting data into a model.</p>
<p>The typical structure of a neural network is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-839 image-border" src="assets/865559ee-6429-4f87-814d-e3cf61f67be0.png" style=""/></div>
<p>The input level/layer in this diagram is typically made up of the independent variables that are used to predict the output (dependent variable) level or layer.</p>
<p>The hidden level/layer is used to transform the input variables into a higher-order function. The way in which a hidden layer transforms the output is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a4a500a3-be3d-4cb7-ae44-32b5749d9bce.png" style=""/></div>
<p>In the preceding diagram, <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, ..., <em>x<sub>n</sub></em> are the independent variables and <em>x<sub>0</sub></em> is the bias term (similar to the way we have a bias in linear/logistic regression).</p>
<p><em>w<sub>1</sub></em>, <em>w<sub>2</sub></em>, ..., <em>w<sub>n</sub></em> are the weights given to each of the input variables. If <em>a</em> is one of the neurons in the hidden layer, it would be equal to:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/64ad64ea-3c02-4406-8d03-e44531e9d5d5.png" style="width:10.50em;height:4.83em;"/></div>
<p>The function that we see in this equation is the activation function that we are applying on top of the summation so that we attain nonlinearity. We need nonlinearity so that our model can learn complex patterns.</p>
<p>Moreover, having more than one hidden layer helps in achieving a high amount of nonlinearity.</p>
<p>A detail of the various parameters that can be tweaked in a neural network will be provided in the subsequent sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up Google Cloud Datalab</h1>
                </header>
            
            <article>
                
<p>In order to set up Google Cloud Datalab, we click on the Cloud Shell icon:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/736905a6-f3b1-47a2-aaeb-53b4119139a8.png"/></div>
<p>Within the Cloud Shell, set the project that needs to be worked on, as follows:</p>
<pre><strong>gcloud config set core/project gcp-test-196204</strong></pre>
<p>Once the project is set, configure the zone as follows:</p>
<pre><strong>gcloud config set compute/zone us-west1-b</strong></pre>
<p>Finally, create a Datalab instance by specifying:</p>
<ul>
<li>For a CPU version:</li>
</ul>
<pre style="padding-left: 60px"><strong>datalab create --no-create-repository mlgcp</strong></pre>
<ul>
<li>For a GPU version:</li>
</ul>
<p style="padding-left: 60px">First, you need to request a GPU version through the quotas page, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe6460f0-3652-461f-9321-19d69eaddf09.png"/></div>
<p>Submit the quota request and you should receive the permission to use GPU in the given region soon.</p>
<p>Note that a GPU version is better while building neural network models as the multiple processors in the GPU can then work on updating multiple weights of a neural network in parallel.</p>
<p>Change the port to <kbd>8081</kbd> to open Datalab and thereby the notebooks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing and importing the required packages</h1>
                </header>
            
            <article>
                
<p>TensorFlow, as a package, is built to perform neural network computations. It works with the lazy evaluation concept, where the various elements of a neural network connection are to be specified, before executing the code.</p>
<p>Another API named Keras makes building neural networks a lot easier. In this chapter, we will be first leveraging the Keras package with TensorFlow running in its backend, and then we'll show how to build a neural network using the premade estimator and a custom estimator in TensorFlow.</p>
<p>In the previous chapters, we understood how to set up Datalab notebooks. In this chapter, we will see how to install and import the required packages into Datalab notebooks.</p>
<p>By default, Datalab comes with a preinstalled TensorFlow package. However, it does not contain Keras by default. Let's look at installing the <kbd>keras</kbd> package:</p>
<pre><strong>!pip install keras</strong></pre>
<p>Once Keras is installed, let's import both the required packages:</p>
<pre><strong>import keras as K</strong><br/><strong>import tensorflow as tf</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working details of a simple neural network</h1>
                </header>
            
            <article>
                
<p>In order to understand how neural networks work, we will build a very simple network. The input and the expected output are as follows:</p>
<pre><strong>import numpy as np</strong><br/><strong>x=np.array([[1,2],[3,4]])</strong><br/><strong>y=np.array([0,1])</strong></pre>
<p>Note that <kbd>x</kbd> is the input dataset with two variables for each of the two rows. <kbd>y</kbd> is the expected output for the two inputs.</p>
<p>Essentially, we have the input and output layers in place.</p>
<p>As an example, for one of the preceding data points, the input and the output values of the network will look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e1de07bf-73a8-44e7-8719-8c01f8e8122b.png"/></div>
<p>In traditional machine learning, you would find the relation directly between the input and output values. However, the neural network architecture works with the following intuition:</p>
<p><em>"The input values can be represented in a richer (higher) dimensional space. The more the dimensions in which the input values are represented, the more is the complexity in the input dataset captured."</em></p>
<p>With the preceding intuition, let's build a hidden layer <span>with three units</span><span> in a neural network:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fc46a2d8-8de2-4b5b-9bc2-dbee9603b1a0.png"/></div>
<p>Now that the layer is built, let's make connections between each unit, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3984e44-7444-4192-870e-4d9b77380dd1.png" style=""/></div>
<p>Now that a connection between each unit is made, there will be a certain amount of weightage that is associated with each connection. In the following diagram, we will initialize the weight that each connection represents:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9917cca2-b2e9-47f1-8765-7f6423fec1c2.png" style=""/></div>
<div class="packt_infobox">Note that the weights <strong>W</strong> represent the strength of connection.</div>
<p>Now we have built a simple neural network. Let's randomly initialize the weight values between the input and hidden layers to understand how the hidden layer values are computed:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f25dab5f-fd2b-4b22-a21f-b9744f0c2437.png" style=""/></div>
<p>Hidden layer values are computed as the multiplications of the input values and weights associated with them, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>h1 = 1*1 + 2*(2) = 5</em></p>
<p class="CDPAlignCenter CDPAlign"><em>h2 = 1*0.5 + 2*(-1) = -1.5</em></p>
<p class="CDPAlignCenter CDPAlign"><em>h3 = 1*(-0.2) + 2*0.1 = 0</em></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed023d95-dd5e-460a-9cc8-e39ffa0ae792.png"/></div>
<p>Now that the hidden values are calculated, we pass them through an activation function. The intuition for an activation function is as follows:</p>
<p><em>"The neural network in the state that we presented previously (without an activation function) is a big linear combination of input variables. Nonlinearity can only be obtained by performing an activation on top of the hidden layer values."</em></p>
<p>For simplicity, as of now, we will assume that the nonlinearity that we are going to apply is the sigmoid function.</p>
<p>A sigmoid function works as follows:</p>
<ul>
<li>It takes an input value, <em>x</em>, and transforms into a new value, <em>1/(1+exp(-x))</em></li>
</ul>
<p>The nonlinearity of a sigmoid curve looks like this for various values of <em>x</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-836 image-border" src="assets/6bd9cc7f-a00b-4fee-9499-db4cd4263c5c.png" style=""/></div>
<p>Thus, the hidden layer values, which were 5, -1.5, and 0, are transformed to <strong>0.99</strong>, <strong>0.18</strong>, and <strong>0.5</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/17f2320a-f801-4a04-bad1-839c73ddf6f7.png"/></div>
<p>Now that the hidden layer values are computed, let's initialize the weights connecting the hidden layer to the output layer.</p>
<p>Note that again the weights are initialized randomly:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b165d73c-fe58-45c8-bddb-ab9ec7da4d28.png"/></div>
<p>Now that the weights are initialized, let's calculate the value associated with the output layer:</p>
<p class="CDPAlignCenter CDPAlign"><em>0.99*1 + 0.18*(-1) + 0.5*0.2 = 0.91</em></p>
<p>The expected value at the output layer is <em>0.91</em>, while the actual value is 0.</p>
<p>Thus, the loss associated in this scenario is <em>(0.91 - 0)^2 = 0.83</em>.</p>
<p>The process until now, where we calculate the loss corresponding to the weight values, is called the <strong>feedforward process</strong>.</p>
<p>So far, in this section, we have understood:</p>
<ul>
<li>Weights</li>
<li>Activation function</li>
<li>Loss calculation</li>
</ul>
<p>In the preceding scenario, while the loss function remains constant for a given objective that we try to solve, the weight initialization and activation functions can vary for different network architectures.</p>
<p>The objective for the problem<span> laid out just now </span>would be to minimize the loss corresponding to a network architecture by iteratively varying the weights.</p>
<p>For example, in the preceding architecture, the loss can be reduced by changing the final weight from the hidden layer to the output layer connection from <em>0.2</em> to <em>0.1</em>. Once the weight is changed, the loss reduces from <em>0.83</em> to <em>0.74</em>.</p>
<p>The process by which weights are changed iteratively to minimize the loss value is called <strong>backpropagation</strong>.</p>
<p>The number of times a weight change happens per given dataset is called the <strong>epoch</strong>. Essentially, an epoch constitutes feedforward and backpropagation.</p>
<p>One of the techniques to intelligently arrive at the optimal weight values is called <strong>gradient descent</strong>—more on various weight optimizers in a later section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p>In the previous section, we have seen the intuition of how weights are updated in backpropagation. In this section, we will see the details of how the weight update process works:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1388d54b-4868-4ad2-a2d0-084e37258c47.png" style=""/></div>
<p>In the backpropagation process, we start with the weights at the end of the neural network and work backwards.</p>
<p>In the preceding diagram (1), we iteratively change the values of weights by a small amount (0.01) for each of the weights connecting the hidden layer to the output layer:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Original weight</strong></p>
</td>
<td>
<p><strong>Changed weight</strong></p>
</td>
<td>
<p><strong>Error</strong></p>
</td>
<td>
<p><strong>Reduction in error</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1.01</p>
</td>
<td>
<p>0.84261</p>
</td>
<td>
<p>-1.811</p>
</td>
</tr>
<tr>
<td>
<p>-1</p>
</td>
<td>
<p>-0.99</p>
</td>
<td>
<p>0.849</p>
</td>
<td>
<p>-0.32</p>
</td>
</tr>
<tr>
<td>
<p>0.2</p>
</td>
<td>
<p>0.21</p>
</td>
<td>
<p>0.837</p>
</td>
<td>
<p>-0.91</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>From the preceding table, we notice that instead of increasing the weight values, one should reduce them to improve the error:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Original weight</strong></p>
</td>
<td>
<p><strong>Changed weight</strong></p>
</td>
<td>
<p><strong>Error</strong></p>
</td>
<td>
<p><strong>Reduction in error</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.99</p>
</td>
<td>
<p>0.8108</p>
</td>
<td>
<p>1.792</p>
</td>
</tr>
<tr>
<td>
<p>-1</p>
</td>
<td>
<p>-1.01</p>
</td>
<td>
<p>0.8248</p>
</td>
<td>
<p>0.327</p>
</td>
</tr>
<tr>
<td>
<p>0.2</p>
</td>
<td>
<p>0.19</p>
</td>
<td>
<p>0.819</p>
</td>
<td>
<p>0.9075</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now we note that, for some weight updates, the improvement in error is high, while for some other weight updates, the improvement in error is low.</p>
<p>This indicates that, for some weights for which error is improving by a lot, the weight update could be faster; while for some weights for which the error improvement is relatively low, the weight update could be slower.</p>
<p>The changed weight for the weight with a value of 1 could then be:</p>
<p class="CDPAlignCenter CDPAlign"><em>Changed weight = original weight + learning rate X reduction in error</em></p>
<p>For now, let's assume that the learning rate is <em>0.05</em>; then:</p>
<p class="CDPAlignCenter CDPAlign"><em>Changed weight = 1 + 0.05*(1.792) = 1.089</em></p>
<p>The other weights will be changed using the same formula.</p>
<p>Intuitively, the learning rate helps us in building trust in the algorithm. For example, when deciding on the magnitude of a weight update, we would potentially not change everything in one go but take a more careful approach in updating the weights more slowly.</p>
<p>Once all the weights are updated using the process laid out, the backpropagation process is done and we proceed with forward propagation again.</p>
<p>A feedforward and a backpropagation step are together called an <strong>epoch</strong>.</p>
<p>Note that we calculated the error value in predicting a data point at a time, thus forming a batch size of 1. In practice, we calculate the error values for a group of data points and then keep updating the weights using a batch of data rather than a single data point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a simple neural network in Keras</h1>
                </header>
            
            <article>
                
<p>From the preceding discussion, we have seen that the key components in a neural network are:</p>
<ul>
<li>Hidden layers</li>
<li>Activation in a hidden layer</li>
<li>Loss function</li>
</ul>
<p>Along with these, there are a few other key components in a neural network. However, we will learn about them in a later section.</p>
<p>For now, we will build a neural network model with the given toy dataset in Keras, with the knowledge we've gained in the sections so far:</p>
<p>Import the relevant functions:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/51b9d334-61a2-4fb3-812a-231a893e4a0a.png"/></div>
<p>The sequential model is a linear stack of layers (input, hidden, and output).</p>
<p>Within each layer, <kbd>dense</kbd> helps in implementing the operations specified in the network.</p>
<p>Let us go ahead and build the network as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-835 image-border" src="assets/9edb3c7f-67a4-441c-af99-5eef74270293.png" style=""/></div>
<p>In our data, we take the input dataset, which is two-dimensional, and convert it into a three-dimensional hidden layer unit in the first step.</p>
<p>Once the hidden layer values are calculated, we pass them through a sigmoid activation in the second step.</p>
<p>The preceding two steps are captured in the second line of the model specification.</p>
<p>From the hidden layer, we connect it to an output layer that is one-dimensional, and hence the third line of code has <kbd>Dense(1)</kbd>.</p>
<p>Let's look at the summary of the model that we specified:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e7356041-3d62-45a9-86da-c8dae010d184.png"/></div>
<p>Let's understand the output shape column for the preceding summary: <kbd>(None, 3)</kbd>.</p>
<p><kbd>None</kbd> means that the output is agnostic of the number of inputs (not to be confused with the dimension of inputs). <kbd>3</kbd> represents the number of units in the hidden layer.</p>
<p>Similarly <kbd>(None,1)</kbd> in the second layer represents the dimension of the output layer (which has only one unit in the output layer).</p>
<p><kbd>Param #</kbd> represents the number of parameters associated with the network.</p>
<p>Note that the connections between the input and hidden layers have a total of nine parameters, as there are six weight values (as shown in the diagram in the previous section) and three bias terms associated with each unit in the hidden layer.</p>
<p>Similarly, there are four parameters in the connection between the hidden and output layers, as there are three weight values between the hidden and output layers and one bias term associated with the output layer.</p>
<p>Now that the network architecture is specified, let's compile the model, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b05fbe6d-bd08-49cc-b2ed-a6c07c011ce3.png" style=""/></div>
<p>In the preceding line of code, we are specifying that the loss is calculated based on mean squared error, which is the average of the squared difference between actual and predicted values across all data points in the input.</p>
<p>Similarly, we specify that the optimization technique is based on stochastic gradient descent.</p>
<p>Now that the model structure, the loss function that we are computing, and optimization technique that we are using are specified, let's fit the model on the input and output values.</p>
<p>The additional metrics that we need to specify while fitting the model are:</p>
<ul>
<li>Input and output values</li>
<li>Number of epochs to be run on the model:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/26661b76-cd2a-4e69-93c2-2e950eaf36d3.png" style=""/></div>
<p>Note that the input and output variables that we specified are <kbd>x</kbd>, <kbd>y</kbd>.</p>
<p>Also, you should notice that the loss values decrease over different epochs, as the weight values are adjusted to minimize the loss as much as possible over the 10 epochs.</p>
<p>Now that the model is built, let's look at obtaining the weight values at each layer:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c51afcf7-0d82-486b-98e5-0d9b59acd7dc.png" style=""/></div>
<p>The values corresponding to a new input value can now be calculated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c1ebc879-11d9-4654-9f39-a8859a799e9e.png" style=""/></div>
<p>In the preceding code snippet, we have initialized a new input and predicted the output corresponding to this new input using the optimal weights that were obtained by running the model.</p>
<p>Let's understand how the output is obtained.</p>
<p>Obtain the values corresponding to the three units in the hidden layer:</p>
<p class="CDPAlignCenter CDPAlign"><em>h1 = 2*(-0.985) + 5*(-0.3587) + 0.00195 = -3.76</em></p>
<p class="CDPAlignCenter CDPAlign"><em>h2 = 2*0.537 + 5*(-0.8225) + 0.0011 = -3.025</em></p>
<p class="CDPAlignCenter CDPAlign"><em>h3 = 2*(-0.24) + 5*0.98 - 0.0027 = 4.421</em></p>
<p>Once the hidden layer values are calculated, we pass them through the sigmoid activation function, as specified in the model architecture:</p>
<p class="CDPAlignCenter CDPAlign"><em>final h1 = sigmoid(h1) = 0.0226</em></p>
<p class="CDPAlignCenter CDPAlign"><em>final h2 = sigmoid(h2) = 0.0462</em></p>
<p class="CDPAlignCenter CDPAlign"><em>final h3 = sigmoid(h3) = 0.988</em></p>
<p>Once the final hidden layer unit values are obtained, we multiply them with the weights connecting hidden layer to output layer, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Output = 0.0226 * 0.834+ 0.0462*0.6618 + (-0.401)*0.988 + 0.14615 = -0.20051</em></p>
<p>Note that the value we obtained is the same value that was obtained in the <kbd>model.predict</kbd> function. This proves the architecture functionality that we have learnt so far.</p>
<p>Now that we have built the model, let's re-execute our code and see whether the results remain the same:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e49c79d-154a-451b-8f57-58517b12a09a.png" style=""/></div>
<p>Note that the loss values are different from what we obtained in the previous iteration. This is because weights are randomly initialized in the first epoch of a neural network run. One way to fix this is by setting a seed. A seed helps in initializing the same set of random values every time a neural network runs.</p>
<p>Note that the seed should be run every time a model is rebuilt. The code snippet for setting a seed looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a0076a96-ceac-44f4-b3e4-f824fc296f81.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the various loss functions</h1>
                </header>
            
            <article>
                
<p>As discussed in the previous chapter, there are two types of dependent variables—continuous and categorical variables. In the case of continuous variable prediction, the loss (error) function can be calculated by using the sum of squared error values across all predictions.</p>
<p>In cases where the dependent variable is a categorical variable with only two distinct values associated with it, loss is calculated as the binary cross-entropy error using this formula:</p>
<p class="CDPAlignCenter CDPAlign"><em>y*logp + (1-y)*log(1-p)</em></p>
<p>In cases where the dependent variable is a categorical variable with multiple distinct values, the loss is calculated using the categorical cross-entropy error as:</p>
<p class="CDPAlignCenter CDPAlign"><em>∑ y*logp</em></p>
<p>Where <em>p</em> is the probability of the event being a 1.</p>
<p>Categorical variables are typically one-hot encoded in practice as follows:</p>
<p>Let's say the output across three different rows is <kbd>[1,2,3]</kbd>; then the output values are represented as <kbd>[[1,0,0], [0,1,0], [0,0,1]]</kbd>. where each index value represents whether a distinct value is present or not. In the above example, the zeroth index corresponds to 1 and hence only the first row has a value of 1 for zeroth while the rest have a value of 0.</p>
<p>The other loss functions that are available in Keras are:</p>
<ul>
<li>Mean absolute error</li>
<li>Mean absolute percentage error</li>
<li>Mean squared logarithmic error</li>
<li>Squared hinge</li>
<li>Hinge</li>
<li>Categorical hinge</li>
<li>Logcosh</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Softmax activation</h1>
                </header>
            
            <article>
                
<p>From the preceding section, we should notice that in the case of categorical variable prediction, the number of units in the output layer would be the same as the number of distinct values in the dependent variable.</p>
<p>Also, note that the predicted value cannot be greater than 1 or less than 0 for any of the units in the output layer. At the same time, the sum of the values across all nodes in the output should be equal to 1.</p>
<p>For example, let's say the output across two nodes of output is -1 and 5. Given that the expected value of outputs should be between 0 and 1 (the probability of an event happening), we pass the output values through softmax activation, as follows:</p>
<ul>
<li>Pass the values through an exponential function:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>exp(-1) = 0.367</em></p>
<p class="CDPAlignCenter CDPAlign"><em>exp(5) = 148</em></p>
<ul>
<li>Normalize the output values to obtain a probability between 0 to 1 and also to ensure that the sum of probabilities between the two output nodes is 1:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>0.367/(0.367+148) =0.001</em></p>
<p class="CDPAlignCenter CDPAlign"><em>148/(0.367+148) = 0.999</em></p>
<p>Thus, the softmax activation helps us in converting the output values into probability numbers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a more complex network in Keras</h1>
                </header>
            
            <article>
                
<p>So far, we have built a neural network that is fairly simple. A traditional neural network would have a few more parameters that can be varied to achieve a better predictive power.</p>
<p>Let's understand them by using the classic MNIST dataset. MNIST is a handwritten digit dataset that contains images of size 28 x 28 pixels that are represented as NumPy arrays of 28 x 28 dimensions.</p>
<p>Each image is of a digit and the challenge in hand is to predict the digit the image corresponds to.</p>
<p>Let's download and explore some of the images present in the MNIST dataset, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f34a52e8-4f0d-44fa-9495-131e838eedb9.png" style=""/></div>
<p>In the preceding code snippet, we are importing the MNIST object and downloading the MNIST dataset using the <kbd>load_data</kbd> function.</p>
<p>Also note that the <kbd>load_data</kbd> function helps in automatically splitting the MNIST dataset into train and test datasets.</p>
<p>Let's visualize one of the images within the train dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0241eec9-501f-4751-b3d3-92f08e45db49.png" style=""/></div>
<p>Note that the preceding digit is <span class="packt_screen">5</span> and the grid that we are seeing is 28 x 28 in size.</p>
<p>Let's look at the shapes of input and output to further understand the datasets:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3f4fa73-b959-48af-b3f1-914a0f973d38.png" style=""/></div>
<p>Given that each input image is 28 x 28 in size, let's flatten it to get the scores of the <kbd>784</kbd> pixel values:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-831 image-border" src="assets/04f96bfc-b9bc-4773-8ce4-878cf33925e4.png" style=""/></div>
<p>The output layer needs to predict whether the image corresponds to one of the <span>digits from </span>0 to 9. Thus, the output layer consists of 10 units corresponding to each of the 10 different digits:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/13a72a8e-6d12-4524-a411-bab582b3cd45.png" style=""/></div>
<p>In the preceding code, <kbd>to_categorical</kbd> provides a one hot-encoded version of the label.</p>
<p>Now that we have the train and test datasets in place, let's go ahead and build the architecture of neural network in the following section:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c02cda27-9b0a-4fb8-bcda-67e5a506bb3e.png" style=""/></div>
<p>Note that <kbd>batch_size</kbd> in the preceding screenshot refers to the number of data points that are considered to update weights. The intuition for batch size is:</p>
<p><em>"If, in a dataset of 1,000 data points, the batch size is 100, then there are 10 weight updates while sweeping through the whole data"</em>.</p>
<p>Note that the accuracy in predicting the labels on the test dataset is ∼91%.</p>
<p>This accuracy increases to 94.9% once the number of epochs reaches 300. Note that for an accuracy of 94.9% on the test dataset, the accuracy on the train dataset is ∼99%.</p>
<p>This is a classic case of overfitting, and the ways to deal with it will be discussed in subsequent chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p>So far, we have considered only the sigmoid activation function in a hidden layer. However, there are quite a few other activation functions that are useful in building a neural network. This chart gives the details of various activation functions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-830 image-border" src="assets/8fa34ec3-ebff-4b57-9437-f0da6dc3ccc6.png" style=""/></div>
<p>The more commonly used activation functions are ReLU, TanH, and logistic or sigmoid activations.</p>
<p>Let's explore the accuracy on the test dataset for various activation functions:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe94c2b5-ed6b-439e-b860-635d1068f7c4.png" style=""/></div>
<p>Note that the accuracy on the test dataset is a mere 29.75% when using ReLU activation.</p>
<p>However, while performing ReLU activation, it is always a good idea to scale the data before fitting the model. Scaling is a way of reducing the magnitude of all the values in the input dataset.</p>
<p>Let's scale the inputs first, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/be4ffadd-9399-4638-b695-f58eba88976a.png" style=""/></div>
<p>Now, let's rerun the model and see the accuracy on the test dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1e95b47e-5803-4f46-9e0b-dce2c9c006b3.png" style=""/></div>
<p>Note that after running 10 iterations, the accuracy on the test dataset is 88.1%. Now, let's run the model for 300 epochs so that we can compare the outputs of sigmoid activation and ReLU activation.</p>
<p>You will notice that the accuracy on the test dataset is 95.76%, which is slightly better than the sigmoid activation accuracy. However, the accuracy on the train dataset is 96%, which indicates that it is not likely to overfit on the dataset; hence, more epochs might further increase the accuracy on the test dataset.</p>
<p>Let's rerun the model using TanH activation without scaling first and with scaling later.</p>
<p>When the model is run on unscaled data, the accuracy after 10 epochs is 92.89%, and after 300 epochs, it is 94.6%.</p>
<p>The accuracy on the test data once we scale the input dataset is 88% after 10 epochs and 93% after 300 epochs.</p>
<p>Note that the issue of overfitting does not arise (train dataset accuracy is much higher than test dataset accuracy) when the datasets are scaled, irrespective of the activation function used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizers</h1>
                </header>
            
            <article>
                
<p>In the previous section, we explored various activation functions and noticed that the ReLU activation function gives a better result when run over a high number of epochs.</p>
<p>In this section, we will look at the impact of varying the optimizer while the activation function remains ReLU on the scaled dataset.</p>
<p>The various loss functions and their corresponding accuracies on the test dataset when run for 10 epochs are as follows:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Optimizer</strong></p>
</td>
<td>
<p><strong>Test dataset accuracy</strong></p>
</td>
</tr>
<tr>
<td>
<p>SGD</p>
</td>
<td>
<p>88%</p>
</td>
</tr>
<tr>
<td>
<p>RMSprop</p>
</td>
<td>
<p>98.44%</p>
</td>
</tr>
<tr>
<td>
<p>Adam</p>
</td>
<td>
<p>98.4%</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now we have seen that RMSprop and Adam optimizers perform better than the stochastic gradient descent optimizer; let's look at the other parameter within an optimizer that can be modified to improve the accuracy of the model—learning rate.</p>
<p>The learning rate of an optimizer can be varied by specifying it as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-837 image-border" src="assets/bce8d1f4-5631-4b31-9079-3de0738f108a.png" style=""/></div>
<p>In the preceding code snippet, <kbd>lr</kbd> represents learning rate. The typical values of learning rate vary between 0.001 and 0.1.</p>
<p>On the MNIST dataset, <span>the accuracy </span>did not improve <span>further </span>when we changed the learning rate; however, typically for a lower learning rate, more epochs are required to reach the same amount of accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Increasing the depth of network</h1>
                </header>
            
            <article>
                
<p>An increase in the depth of a hidden layer is the same as increasing the number of hidden layers in a neural network.</p>
<p>Typically, for a higher number of hidden units in a hidden layer and/or higher number of hidden layers, <span>the predictions are </span>more accurate.</p>
<p>Given that the Adam optimizer or RMSprop has a saturated accuracy after certain number of epochs, let's switch back to stochastic gradient descent to understand the accuracy when the model is run for 300 epochs; but we are using more number of units in the hidden layer this time:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed5b6e76-c5a8-4d5e-b74a-64472e449166.png"/></div>
<p>Note that by using 2,000 units in the hidden layer, our accuracy increases to 95.76% by the end of 300 epochs. This is potentially because the input can now be expressed in a higher dimensional space, and hence a better representation can be learned when compared to the 1,000-dimensional space scenario.</p>
<p>Now, we will increase the number of hidden layers to understand the impact of building deep neural networks on accuracy:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4230117b-a90e-4e2c-b2c8-ba6c95dc5557.png"/></div>
<p>Note that when the network is deep, with two hidden layers instead of one, the accuracy after 300 epochs is 97.24%, which is a clear improvement when compared to the single hidden layer network.</p>
<p>Similar, to the way in which the network learned more complex representations of data when the number of hidden units in a layer increased, the network also learned complex representations of data when the number of hidden layers increased.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Impact on change in batch size</h1>
                </header>
            
            <article>
                
<p>As discussed earlier, the lesser the batch size, the more often the weights get updated in a given neural network. This results in a lesser number of epochs required to achieve a certain accuracy on the network. At the same time, if the batch size is too low, the network structure might result in instability in the model.</p>
<p>Let's compare the previously built network with a lower batch size in one scenario and a bigger batch size in the next scenario:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-838 image-border" src="assets/f3e45aaa-8509-4236-a5dc-119658fddd62.png" style=""/></div>
<p>Note that in the preceding scenario, where the batch size is very high, the <span>test dataset </span>accuracy at the end of 300 epochs is only 89.91%.</p>
<p>The reason for this is that the network with batch size 1,024 would have learned the weights much faster than the network with batch size 30,000, as the number of weight updates is much higher when the batch size is lower.</p>
<p>In the next scenario, we will reduce the batch size to a very small number to see the impact on network accuracy:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a1f1e527-2398-405b-8b81-f4fb18872520.png"/></div>
<p>Note that while accuracy improves considerably very quickly to 97.77% within 10 epochs itself, it takes significant time to produce results, as the number of weight updates is high per epoch. This results in more calculations and thus more time to execute.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing neural networks in TensorFlow</h1>
                </header>
            
            <article>
                
<p>In previous sections, we have understood how a neural network works and also how to build a neural network model in Keras. In this section, we will be working toward building a neural network model in TensorFlow. There are two ways in which we can build models in TensorFlow:</p>
<ul>
<li>Using premade estimators</li>
<li>Defining custom estimators</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using premade estimators</h1>
                </header>
            
            <article>
                
<p>Premade estimators are similar to the methods available in packages such as scikit-learn, where the input features and output labels are specified, along with the various hyperparameters. A method can then optimize for solving a loss function that is predefined to a default value but can be varied by passing a different function in a parameter.</p>
<p>Let's explore building the training and test datasets in the code:</p>
<ol>
<li>Import the relevant packages:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b7a409f4-1dd3-4f2f-9290-0dad3ff2b104.png" style=""/></div>
<ol start="2">
<li>Import the dataset. We will work on the <kbd>MNIST</kbd> dataset for this exercise:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b940c4c3-e1c2-445f-8467-90e4b6790406.png" style=""/></div>
<p style="padding-left: 60px">The shapes of images and labels are as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/809adaf0-0328-4a47-a4cf-3114e34e6635.png"/></div>
<p style="padding-left: 60px">The premade function works on label value instead of the one-hot encoded version. Let's convert the one-hot encoded label into a value, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5baad61b-cf52-426e-a337-d9630167f0d8.png"/></div>
<p style="padding-left: 60px">Let's understand how the data points look:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/99112603-b35c-4f9c-9b17-bdb3a1749bbc.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6a89b41a-b076-4a78-9155-e777a6988c16.png"/></div>
<ol start="3">
<li>Input the dataset into a function that consumes the independent (<kbd>x</kbd>) and dependent (<kbd>y</kbd>) variables:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3f55e04a-fbe8-4980-b108-93febe1843be.png"/></div>
<p>Note that we named the independent variables as <kbd>x2</kbd> and dependent variable as <kbd>y</kbd>.</p>
<p>Also, note that we have passed the arrays that form the independent and dependent variable values.</p>
<p> <kbd>batch_size</kbd> indicates the number of training examples that are consumed to calculate the loss function, and <kbd>num_epochs = None</kbd> indicates that the number of epochs to be run will be provided later.</p>
<p> <kbd>train_input_fn</kbd> returns features and labels, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bad45146-8465-4949-aab3-ef433ae3013a.png"/></div>
<p>Similarly, we pass the test dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d0dcc8b8-d5b3-4b53-ae3a-4854fb1982f8.png"/></div>
<p>Note that, in the case of the test dataset, <kbd>num_epochs = 1</kbd> as we pass it through only the feedforward for the test dataset once the model weights are derived from training.</p>
<p>A dataset could potentially contain multiple columns, so let's specify the feature column and its type, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8f6f4e63-208b-43ff-a80b-8fe617bd421b.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/62dca6f6-d5ac-4671-ab7d-c764a436580d.png"/></div>
<p>If there are multiple columns, we would specify all the columns in a list, as follows:</p>
<p><kbd>feature_columns = [feature_x1, feature_x2]</kbd></p>
<p>Where <kbd>feature_x1</kbd> is one feature and <kbd>feature_x2</kbd> is another feature.</p>
<p>Now, we shall specify the number of hidden layers and also the hidden units in each layer:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0d9de532-1d77-4152-9fff-da1e0a314d25.png"/></div>
<p>Note that by specifying the number of hidden units in the preceding way, we have specified that there are three hidden layers, where the first hidden layer has <strong>512</strong> units, the second hidden layer has <strong>256</strong> units, and the final hidden layer has <strong>128</strong> units.</p>
<p>Now that we have specified the features and hidden layers, let's specify the architecture of neural network, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-829 image-border" src="assets/ab29fc30-66ea-45c6-b222-70d96bd8e8c4.png" style=""/></div>
<p>Now that we have specified the model architecture, we can go ahead and train the model. If you would like to further change the hyperparameters that are available in the function, you can check out the hyperparameter levers that are available by using the <kbd>help</kbd> function, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-832 image-border" src="assets/49440387-51ad-428c-a503-4944dba2af7e.png" style=""/></div>
<p>The following code runs the neural network model for 2,000 epochs in total:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/742bca8a-62a9-4168-acbf-1320b8a2860d.png" style=""/></div>
<p>Now that our model is run, let's evaluate the accuracy on the test dataset, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dab4748b-8130-4ece-93a3-805dbb7ed300.png" style=""/></div>
<p>We can see that the accuracy of the model on the test dataset is 97.2%.</p>
<p>So far, we have been implementing a model using premade estimators; in the next sections, we will look into defining the model without premade estimators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating custom estimators</h1>
                </header>
            
            <article>
                
<p>A premade estimator limits the full potential to which TensorFlow can be used; for example, we would not be able to have different dropout values after different layers. In this regard, let's go ahead and create a function of our own, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/138f6adc-4399-44c0-9025-4b9af8a89d6e.png"/></div>
<p>Let's explore each part of the preceding snippet of code in detail:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9a300918-5bfc-4b29-947b-b3e4d3fa44ac.png"/></div>
<p>The function takes features (independent variables) and labels (dependent variable) as input. <kbd>mode</kbd> indicates whether we want to train, predict, or evaluate the given data.</p>
<p><kbd>params</kbd> provides us with the functionality to supply information about parameters; for example, learning rate:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d56831e5-22b5-45f0-94f3-0a6092653fe8.png"/></div>
<p>The preceding snippet of code is similar to the way in which we defined model architecture in Keras, where we specified the inputs, the hidden layer activation, and the number of units in the hidden layer:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6829427b-481e-4d0f-bf9e-d79ee3d174eb.png"/></div>
<p>If our mode is to predict the class, we would not have to train the model, but just pass the predicted class, thus estimator spec in such scenario would just need to calculate the <kbd>y_pred_cls</kbd> values, thus the following code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/66dcb13b-ddfe-43af-ba70-6b1870d8331d.png"/></div>
<p>If the mode is to train or test the model, we would have to calculate the loss and hence the following code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/97320ae6-c050-4b42-b029-5c70ea927109.png"/></div>
<p>In the preceding code, the first line is used to define the cross-entropy calculation. The second line takes the average of cross entropy across all the rows.</p>
<p><kbd>optimizer</kbd> specifies the optimizer we are interested in and the learning rate. <kbd>train_op</kbd> specifies that we are interested in minimizing loss, and the <kbd>global_step</kbd> <span>parameter </span>keeps a count of the step (epoch) that the model is currently in. <kbd>metrics</kbd> specifies the metrics that we are interested in calculating, and the final <kbd>spec</kbd> that would be calculated would be a combination of all the preceding parameters that we have defined.</p>
<p>Once the model architecture and the estimator spec that needs to be returned are defined, we define the parameters and mode as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1e13a8c9-38ce-44c2-b097-44a682bf8753.png"/></div>
<p>From the preceding code, the function learns the parameter that needs to be changed and also the model architecture that needs to be worked on (<kbd>model_fn</kbd>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f622206b-09cd-4071-8c50-26772df523dc.png"/></div>
<p>We run the model by specifying the mode (in this case, <kbd>train</kbd>) by a certain number of epochs (<kbd>2000</kbd> in this case).</p>
<p>After running the model, we evaluate the model's accuracy on the test dataset, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3d42e5a-24e4-4ba3-b678-6c10cb6c08f4.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned to set up Datalab to execute neural networks on Google Cloud. We also learned the structure of a neural network and how various parameters, such as depth, number of hidden units, activation function, optimizer, batch size, and number of epochs, impact the accuracy of the model. We also saw how to implement a neural network in both Keras and TensorFlow. Topics such as using premade estimators and creating custom estimators in TensorFlow were covered.</p>


            </article>

            
        </section>
    </body></html>