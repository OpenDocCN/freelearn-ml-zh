- en: Chapter 3. An Introduction to Hadoop's Architecture and Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this chapter onwards, we start with the implementation aspects of Machine
    learning. Let's start learning the platform of choice—a platform that can scale
    to Advanced Enterprise Data needs (big data needs of Machine learning in specific)—Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we cover Hadoop platform and its capabilities in addressing
    large-scale loading, storage, and processing challenges for Machine learning.
    In addition to an overview of Hadoop Architecture, its core frameworks, and the
    other supporting ecosystem components, also included here is a detailed installation
    process with an example deployment approach. Though there are many commercial
    distributions of Hadoop, our focus in this chapter is to cover the open source,
    Apache distribution of Hadoop (latest version 2.x).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics are covered in-depth:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Apache Hadoop, its evolution history, the core concepts,
    and the ecosystem frameworks that comprise Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop distributions and specific offerings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation and set up of the Hadoop environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.0—HDFS and MapReduce (also **YARN** (**Yet Another Resource Negotiator**))
    architectures with example implementation scenarios, using different components
    of the architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the purpose of the core ecosystem components, setting up and learning
    to build and run the programs using examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Machine learning specific ecosystem extensions such as Mahout and
    R Connectors ([Chapter 4](ch04.html "Chapter 4. Machine Learning Tools, Libraries,
    and Frameworks"), *Machine Learning Tools, Libraries, and Frameworks*, covers
    the implementation details)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Hadoop is an open source, Java-based project from the Apache Software
    Foundation. The core purpose of this software has been to provide a platform that
    is scalable, extensible, and fault tolerant for the distributed storage and processing
    of big data. Please refer to [Chapter 2](ch02.html "Chapter 2. Machine learning
    and Large-scale datasets"), *Machine learning and Large-scale Datasets* for more
    information on what data qualifies as big data. The following image is the standard
    logo of Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Apache Hadoop](img/B03980_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At the heart of it, it leverages clusters of nodes that can be commodity servers
    and facilitates parallel processing. The name Hadoop was given by its creator
    Doug Cutting, naming it after his child's yellow stuffed toy elephant. Till date,
    Yahoo! has been the largest contributor and an extensive user of Hadoop. More
    details of Hadoop, its architecture, and download links are accessible at [http://hadoop.apache.org/](http://hadoop.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop is an industry standard platform for big data, and it comes with extensive
    support for all the popular Machine learning tools in the market. This platform
    is now used by several big firms such as Microsoft, Google, Yahoo!, and IBM. It
    is also used to address specific Machine learning requirements like sentiment
    analysis, search engines, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections cover some key characteristics of the Hadoop platform
    that make it ideal for facilitating efficiency in large-scale data storage and
    processing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of Hadoop (the platform of choice)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following figure (Source Cloudera Inc.) explains the evolution of the Hadoop
    platform. With Doug Cutting and Mike Cafarella starting it all in 2002 to build
    a greatly scalable search engine that is open source and hence extensible and
    running over a bunch of machines. Some important milestones in this evolution
    phase have been by Google that released the **Google File System** (**GFS**) in
    October 2003, followed by the MapReduce framework in December 2004 that evolved
    to form the core frameworks HDFS and MapReduce/YARN respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other significant milestone had been Yahoo''s contribution and adoption
    around February 2008 when Yahoo implemented a production version that had indexing
    of searches implemented over 10,000 Hadoop cluster nodes. The following table
    depicts the evolution of Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of Hadoop (the platform of choice)](img/B03980_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop and its core elements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following concept map depicts the core elements and aspects of the Hadoop
    platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop and its core elements](img/B03980_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning solution architecture for big data (employing Hadoop)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let us look at the essential architecture components for implementing
    a Machine learning solution considering big data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed solution architecture should support the consumption of a variety
    of data sources in an efficient and cost-effective way. The following figure summarizes
    the core architecture components that should potentially be a part of the Machine
    learning solution technology stack. The choice of frameworks can either be open
    source or packaged license options. In the context of this book, we consider the
    latest version of open source (Apache) distribution of Hadoop and its ecosystem
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vendor specific frameworks and extensions are out of scope for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning solution architecture for big data (employing Hadoop)](img/B03980_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the next sections, we'll discuss in detail each of these Reference Architecture
    layers and the required frameworks in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: The Data Source layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Data Source layer forms a critical part of the Machine learning Reference
    Architecture. There are many internal and external data feeds that form an input
    to solving a Machine learning problem. These feeds can be structured, unstructured,
    or semi-structured in nature. Moreover, in real-time, batch, or near real time
    mode, they need to be seamlessly integrated and consolidated for analytics engines
    and visualization tools.
  prefs: []
  type: TYPE_NORMAL
- en: Before ingesting this data into the system for further processing, it is important
    to remove the irrelevance or the noise in the data. Some unique techniques can
    be applied to clean and filter the data.
  prefs: []
  type: TYPE_NORMAL
- en: These consolidated datasets are also called data lakes in big data and the data
    aggregation context. Hadoop is one of the storage options of choice for Data Lakes.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows the variety of data sources that form a primary
    source of input.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Data Source layer](img/B03980_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data architectures have always been designed to support some of the protocols
    such as JMS, HTTP, XML, and so on. However, now, the recent advancements in the
    field of big data have brought about significant changes. So now, the new age
    data sources include data streams from social networking sites, GPS data, machine-generated
    data such as user access logs, and other proprietary data formats.
  prefs: []
  type: TYPE_NORMAL
- en: The Ingestion layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Data Ingestion layer is responsible for bringing data in from multiple data
    sources into the system, with a primary responsibility to ensure data quality.
    This layer has the capability to filter, transform, integrate, and validate data.
    It is important that the choice of technology to implement this layer should be
    able to support high volumes and other characteristics of data. The following
    meta model shows the composition and flow of functions of the Ingestion Layer.
    An ingestion layer could potentially be an **ETL** short for (**Extract, Transform,
    and Load**) capability in the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listed below are a set of basic requirements for an ingestion layer:'
  prefs: []
  type: TYPE_NORMAL
- en: High-speed transformation of data from any source system in any manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing large volumes of records in minimal time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Producing output in a semantically rich format so that any target system can
    query for **smart data**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The architecture framework for ingestion layer needs to provide the following
    capabilities; the upcoming model depicts various layers and compositions:'
  prefs: []
  type: TYPE_NORMAL
- en: An Adapter Framework—any product group or application should be able to use
    the Adapter Framework to quickly, reliably, and programmatically develop connectors
    to different data sources (Files, CSV format, and DB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high speed, parallel transformation execution engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A job execution framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantified output generator framework![The Ingestion layer](img/B03980_03_06.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ingestion layer loads the relevant data into the storage layer, which in
    our current context is the Hadoop Storage Layer that is primarily a file-based
    storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept map below lists ingestion core patterns (these patterns address
    performance and scalability needs of a Machine learning architecture):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Ingestion layer](img/B03980_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Parallel Processing and Partitioning Patterns**: The fundamental architecture
    for handling large volume ingestion requirements is to parallelize the execution.
    Running transformations on different input data in parallel and partitioning a
    single, large volume input into smaller batches for processing in parallel helps
    achieve parallelization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline design patterns**: When designing the workflows for ingestion jobs,
    there are specific issues that need to be addressed, such as avoiding large sequential
    pipelines that enable parallel processing. Similarly, from the data reliability
    point of view, creating appropriate audit and execution logs is important to manage
    the entire ingestion execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation patterns**: There are different categories of transformation.
    One of the main aspects of the transformation designs is to handle dependency.
    The patterns mentioned in the first category (parallelization) also handle dependency
    requirements. Other issues relate to the dependency on the past and historical
    data, which is especially significant when processing additional loads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage Design**: When loading data into the target data store, there are
    issues such as recovering from failed transformations or reloading data for specific
    feeds (for example, when there should be a fixed transformation rule).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Load patterns**: One of the biggest performance bottlenecks in data
    ingestion is the speed of loading data into the target data mart. Especially when
    the target is an RDBMS, parallelization strategies lead to concurrency issues
    while loading the data, limiting the throughput of the ingestion that is possible.
    The patterns present certain techniques of how to realize the data load and address
    performance and concurrency issues while loading data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hadoop Storage layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning architecture has a distributed storage layer that supports
    parallel processing for running analytics or heavy computations over big data.
    The usage of distributed storage and processing large volumes in parallel is a
    fundamental change in the way an enterprise handles big data.
  prefs: []
  type: TYPE_NORMAL
- en: A typical distributed storage facilitates high performance by parallel processing
    the algorithms that run over petabyte scale data with fault-tolerance, reliability,
    and parallel processing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the current context of Hadoop architecture, **Hadoop Distributed File System**
    (**HDFS**) is the core storage mechanism. In this section, let us have a brief
    look at HDFS and NoSQL (Not-only-SQL) storage options. The following sections
    cover HDFS and its architecture in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS is one of the core components and acts as a database for Hadoop. It is
    a distributed file system that stores large-scale data across a cluster of nodes.
    It comes with a framework to ensure data reliability and fault tolerance. Applications
    can store files in parts or whole depending on the size, and it facilitates the
    write once read many times.
  prefs: []
  type: TYPE_NORMAL
- en: Since HDFS is a file system, access to data for consumption or manipulation
    is not simple and requires some complex file operation programs. Another way of
    bringing in easier data management is by using non-relational stores called NoSQL
    stores.
  prefs: []
  type: TYPE_NORMAL
- en: The following model represents various NoSQL data store categories that are
    available with examples for each of the categories. Every data store category
    caters to a particular business requirement, and it is important to understand
    the purpose of each of these categories of NoSQL store to make the right choice
    for a given requirement. The CAP theorem (that stands for consistency, availability,
    and partition tolerance) attributes are satisfied to a varying degree for each
    of the NoSQL stores, resulting in support for optimized storage systems that are
    expected to work for combinations of these attributes. In reality, these NoSQL
    stores may have to coexist with relational stores as they would need a system
    of record to sync up on a need basis, or a better case is where we would need
    to use a combination of relational and non-relational data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts types of NoSQL databases and some of the products
    in the market:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Hadoop Storage layer](img/B03980_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop was originally meant for \batch processing where the data into HDFS is
    loaded in batch or a scheduled manner. Usually, the storage layer has data loaded
    in batch. Some of the core and ecosystem components that facilitate data loading
    or ingestion into HDFS are Sqoop, **HIHO** (**Hadoop-in Hadoop-out**) MapReduce
    function, and ETL functions among others.
  prefs: []
  type: TYPE_NORMAL
- en: The Hadoop (Physical) Infrastructure layer – supporting appliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The difference between the traditional architectures and big data (for Machine
    learning) architecture is the importance that the underlying infrastructure grabs.
    Performance, Scalability, Reliability, Fault Tolerance, High Availability, and
    Disaster Recovery are some of the important quality attributes that this architecture
    is required to support. The underlying infrastructure of the platform handles
    these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The Hadoop Infrastructure is a distributed architecture or model where data
    is not stored in one place, but is distributed across multiple or a cluster of
    nodes. The data distribution strategy can be intelligent (as in the case of Greenplum)
    or can be simply mathematical (as in the case of Hadoop). The distributed file
    system nodes are linked over a network. This is referred to as **Shared Nothing
    Architecture** (**SNA**), and the big data solutions work on this reference architecture.
    Along with the data being distributed across multiple nodes, the processes run
    locally to the data nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This is first cited in Michael Stonebraker's paper that can be accessed at [http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf](http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes that have data stored are called data nodes and those where processing
    happens are called compute nodes. The data and compute nodes can be collocated
    or decoupled. The following figure represents an SNA context that has data and
    compute nodes collocated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Hadoop (Physical) Infrastructure layer – supporting appliance](img/B03980_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shared nothing data architecture supports parallel processing. Redundancy is
    a default expectation as it deals with a variety of data from diverse sources.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and HDFS, over a grid infrastructure connected, over a fast gigabit network,
    or a virtual cloud infrastructure, forms the infrastructure layer that supports
    large-scale Machine learning architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates big data infrastructure setup using commodity
    servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Hadoop (Physical) Infrastructure layer – supporting appliance](img/B03980_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop platform / Processing layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The platform or processing layer for Hadoop is the core data processing layer
    of the Machine learning architecture tools. This layer facilitates querying or
    accessing data stored in Hadoop's storage layer (NoSQL databases that use the
    HDFS storage file system typically), sitting at the top of the Hadoop infrastructure
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: As learned in [Chapter 2](ch02.html "Chapter 2. Machine learning and Large-scale
    datasets"), *Machine learning, and Large-scale datasets*, technological advancements
    in the field of computing now facilitate handling large volumes of distributed
    computing and parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce framework of Hadoop helps to store and analyze large volumes of
    data efficiently and in an inexpensive way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of the Hadoop platform or processing layer are listed next;
    these components are a part of the ecosystem and are discussed in detail in the
    sections that follow in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MapReduce**: MapReduce is a programming paradigm that is used to efficiently
    execute a function over a larger volume of data, typically in a batch mode. The
    *map* function is responsible for distributing the tasks across multiple systems,
    distributing the load equally and managing the processing in parallel. Post processing;
    the *reduce* function assimilates and combines the elements to provide a result.
    A step-by-step implementation on Hadoop''s native MapReduce architecture, MapReduce
    v2, and YARN is covered in the *Hadoop ecosystem components* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hive**: Hive is a data warehouse framework for Hadoop and is responsible
    for aggregating high volumes of data with SQL-like functions. Hive facilitates
    an efficient way of storing data that uses resources optimally. The configuration
    and implementation aspects of Hive are covered in the *Hadoop ecosystem components*
    section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pig**: Pig is a simple scripting language that facilitates querying and manipulating
    data held on HDFS. It internally runs the functions in a MapReduce paradigm and
    is often perceived to simplify building MapReduce functions. A detailed step-by-step
    guide to configuring, learning the syntax, and building essential functions is
    covered in the *Hadoop ecosystem components* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sqoop**: Sqoop is a data import tool for Hadoop that has inbuilt functions
    to import data from specific tables, columns, or complete database onto the file
    system. Post processing, Sqoop supports extracting data from several Relational
    databases and NoSQL data stores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HBase**: HBase is a Hadoop compliant NoSQL data store (a columnar NoSQL data
    store) that uses HDFS as the underlying file system. It supports distributed storage
    and automatic linear scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZooKeeper**: ZooKeeper is a monitoring and coordinating service that helps
    keep a check on the Hadoop instances and nodes. It is responsible for keeping
    the infrastructure synchronized and protects the distributed system from partial
    failures and ensures data consistency. The ZooKeeper framework can work standalone
    or outside Hadoop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More of these ecosystem components are discussed in depth in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: The Analytics layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More often, enterprises have some real **Business Intelligence** (**BI**) tools
    that are responsible for running some analytical queries and producing some MIS
    reports or dashboards. There is a need for modern Machine learning or analytics
    tools and frameworks to coexist with them. There is now a need for the analytics
    to run either in a traditional way on the data warehouses or big data stores as
    well that can handle structured, semi-structured, and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can expect the data flow between the traditional data stores
    and the big data stores using tools such as Sqoop.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL stores are known for low latency; they facilitate real-time analytics.
    Many open source analytics frameworks have simplified building models, and run
    complex statistical and mathematical algorithms using simple out-of-box functions.
    All that is required now is to understand the relevance of each of the algorithms
    and the ability to choose a suitable algorithm or approach, given a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look into the below listed open source Analytics and Machine learning
    frameworks in the chapters to follow.
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python (scikit-learn distribution)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to one of the upcoming Spring projects called **Spring XD**
    is covered, as it looked like a comprehensive Machine learning solution that can
    run on Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The Consumption layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The insights generated from the analytics layer or the results of data processing
    are consumed in many ways by the end clients. Some of the ways this data can be
    made available for consumption are:'
  prefs: []
  type: TYPE_NORMAL
- en: Service APIs (for example, Web Service Interfaces (SOAP based or REST))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reporting engines and data marts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashboard and Visualization tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the options, **Visualization** is core and not only an important way
    of distributing or communicating the results of Machine learning but also a good
    way of representing data in a way that helps in decision making. Very evidently
    data visualization is gaining traction in the field of big data and analytics.
    A visualization that best represents the data and the underlying patterns and
    the relationships is what is the key to decision making.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consumption layer](img/B03980_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are two types of visualizations; one is those that explain the data, and
    second is those that explore data and the underlying patterns. Visualization is
    now being looked at as a new language to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining and exploring data with Visualizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visualizations for explaining and exploring data are unique and are used for
    different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The Visualizations for explaining are the typical ones we see in marketing and
    sales presentations. This is the case where data on the hand is clean to the maximum
    extent. The meaning of the data is clear, and communication is done by the final
    decision makers.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Visualizations for exploring help to correct data and link
    the related and useful attributes of data in the quest for understanding the data
    as such. Visualization for exploring sometimes can be inaccurate. The exploration
    usually happens iteratively, and there might be several rounds of refining the
    Visualizations before some sense is made out of the data on hand. There is a need
    to get rid of some irrelevant attributes in data or even the data itself (the
    one identified to be *noise*). This step of data exploration using Visualization
    sometimes replaces running complex algorithms and often requires statistical acumen.
  prefs: []
  type: TYPE_NORMAL
- en: Some popular visualization tools in the market (both open source and commercial)
    are Highcharts JS, D3, Tableau, and others. Although we use some of these frameworks
    to demonstrate how to depict and communicate insights, we are not explicitly covering
    any of the visualization options in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect is that these visualization tools usually need to
    leverage, traditional data warehousing tools and big data analysis tools. The
    following figure depicts how the proposed Machine learning architecture can support
    having existing data warehouses or BI tools coexist with big data analysis tools.
    As explained in [Chapter 1](ch01.html "Chapter 1. Introduction to Machine learning"),
    *Introduction to Machine learning*, the aggregated data and the data lakes become
    the core input to any big data analysis tools that run the Machine learning tools.
    The new age data storage mantra is semantic data structures. More on semantic
    data architectures are covered as a part of emerging data architectures in [Chapter
    14](ch14.html "Chapter 14. New generation data architectures for Machine learning"),
    *New generation data architectures for Machine learning*. The following figure
    depicts a high-level view of visualization in the context of data lakes and data
    warehouses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Explaining and exploring data with Visualizations](img/B03980_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Security and Monitoring layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When large volumes of data are being processed and consolidated across a variety
    of sources, security becomes of utmost importance and, in the case of sensitive
    data, the need for protecting data privacy is critical and sometimes the key compliance
    requirement too. The required authentication and authorization checks need to
    be implemented as a part of executing Machine learning algorithms. This is more
    of a prerequisite and cannot be an afterthought in the Machine learning architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Data Ingestion and the processing function are the main areas that require strict
    security implementation, given the criticality of controlling data access.
  prefs: []
  type: TYPE_NORMAL
- en: By the virtue of distributed architecture, big data applications are inherently
    prone to security vulnerabilities; it is necessary that security implementation
    is taken care of, and it does not impact performance, scalability, or functionality
    with the ease of execution and maintenance of these applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Machine learning architecture as such should support the following as a
    basic necessity for security:'
  prefs: []
  type: TYPE_NORMAL
- en: Authentication for each node in the cluster with the support for standard protocols
    like Kerberos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since it is a file system, there needs to be a minimum support for encryption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication with the nodes should always use **SSL** (**Secure Socket Layer**),
    TLS, or others that include NameNode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure keys and tokens and usage of standard key management systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation of distributed logging for tracking to trace any issues across
    layers easily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next significant requirement is monitoring. The distributed data architecture
    comes with robust monitoring and support tools that can handle large clusters
    of nodes that are connected in a federated model.
  prefs: []
  type: TYPE_NORMAL
- en: There are always SLAs for the downtime of an application, and it is important
    that the recovery mechanism adheres to these SLAs while ensuring the availability
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that these nodes and clusters communicate with the monitoring
    system in a machine independent way, and the usage of XML-like formats is key.
    The data storage needs for the monitoring systems should not impact the overall
    performance of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, every big data stack comes with an in-built monitoring framework or
    tool. Also, there are open source tools such as Ganglia and Nagios that can be
    integrated and used for monitoring the big data applications.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop core components framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apache Hadoop has two core components:'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Distributed File System also called HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce (in the version 2.x of Hadoop, this is called YARN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the Hadoop components are represented in the Machine learning solution
    architecture. Using Hadoop we work around these two core components and form the
    eco-system components for Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this chapter is Apache Hadoop 2.x distribution. There have been
    few architectural changes to HDFS and MapReduce in this version. We first cover
    the core architecture, and then the changes that have come in as a part of the
    2.x architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Distributed File System (HDFS)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**HDFS** is inspired and built from **GFS** (**Google File System**). It is
    a distributed file system that is elastically scalable, with support load balancing
    and fault tolerance to ensure high availability. It has data redundancy built
    in to demonstrate reliability and consistency in data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop Distributed File System (HDFS)](img/B03980_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS implements the Master-slave architecture. Here, the master node is called
    NameNode, and the slave nodes are called DataNodes. NameNode is the entry point
    for all client applications, and the distribution of data across the DataNodes
    happens via the NameNode. The actual data is not passed through NameNode server
    to ensure that NameNode does not become a bottleneck for any data distribution.
    Only the metadata is communicated to the client, and the actual data movement
    happens directly between the clients and DataNodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both NameNode and DataNode are referred to as daemons in the Hadoop architecture.
    NameNode requires a high-end machine and is expected to run only the NameNode
    daemon. The following points justify the need for a high-end machine for NameNode:'
  prefs: []
  type: TYPE_NORMAL
- en: The entire cluster's metadata is held in the memory for quicker access, and
    there is a need for more memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NameNode is both the single point of entry and failure for the Hadoop cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NameNode coordinates with several hundreds or thousands of DataNodes and
    manages batch jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDFS is built on the traditional hierarchical file system where the creation
    of new directories, adding new files, deletion directories or subdirectories,
    removal of files, renaming, and moving or updating a file are common tasks. Details
    of the directories, files, data nodes, and blocks created and stored in each of
    the DataNodes are stored as metadata in the NameNode.
  prefs: []
  type: TYPE_NORMAL
- en: There is another node in this architecture that NameNode communicates with,
    called secondary Namenode. The secondary Namenode is not a backup for NameNode
    and hence, does not failover to the secondary Namenode. Instead, it is used to
    store a copy of the metadata and log files from NameNode. NameNode holds the metadata
    for the data blocks and related distribution details in a file called `fsimage`.
    This image file is not updated for every data operation in the file system and
    is tracked periodically by logging them in separate log files. This ensures faster
    I/O and thus the efficiency of the data import or export operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The secondary Namenode has a specific function with this regard. It periodically
    downloads the image and log files, and creates a new image by appending the current
    operations from the log file into the fsimage, then uploading the new image file
    back to NameNode. This eliminates any overhead on NameNode. Any restart on NameNode
    happens very quickly, and the efficiency of the system is ensured. The following
    figure depicts the communication workflow between the client application and HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop Distributed File System (HDFS)](img/B03980_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS is built for reading and writing large volumes of data between DataNodes.
    These large files are split into blocks of smaller files, usually of a fixed size
    such as 64 MB or 128 MB, and these blocks are distributed across DataNodes. For
    each of these blocks, overall three copies are stored to ensure redundancy and
    support fault tolerance. The number of copies can be changed, which is a configuration
    of the system. More information on the HDFS architecture and specific functions
    is covered in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary Namenode and Checkpoint process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'While defining the purpose and function of the secondary Namenode, we have
    learned one important function that takes care of updating or preparing the metadata
    for NameNode that is stored in a file called `fsimage`. This process of generating
    a new fsimage by merging the existing fsimage and the log file is called **Checkpoint**.
    The following figure depicts the checkpoint process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Secondary Namenode and Checkpoint process](img/B03980_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some configurations changes are to be done to the `cross-site.XML` file related
    to checkpoint process.
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.dir` | This is the directory path where the temporary
    fsimage files are held to run the merge process. |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.edits.dir` | This is the directory path where the
    temporary edits are held to run the merge process. The default value for this
    parameter is same as `dfs.namenode.checkpoint.dir` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.period` | The time gap between two checkpoint runs
    (in seconds). |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.txns` | Irrespective of the time gap configurations,
    this property defines after how many transactions a checkpoint process needs to
    be triggered. |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.check.period` | This property defines the frequency
    (in seconds) in which the NameNode is polled to check the un-checkpointed transactions.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.checkpoint.max-retries` | In the case of failure, the secondary
    Namenode retry is checkpointing. This property defines the number of times a secondary
    Namenode attempts a retry for checkpointing before it gives up. |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.num.checkpoints.retained` | This property represents the number
    of checkpoint files retained by both the NameNode and the secondary Namenode.
    |'
  prefs: []
  type: TYPE_TB
- en: The checkpoint process can be triggered by both NameNode and the secondary Namenode.
    Secondary Namenode is also responsible for taking backup of the `fsimage` files
    periodically, which will further help in recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting large data files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: HDFS stores smaller chunks of huge files across the data nodes distributed over
    the cluster. Before the files are stored, HDFS internally splits the entire file
    content into multiple data blocks each of a fixed size (default 64 MB). This size
    is configurable. There is no specific business logic followed to split the files
    and build the data blocks; it is purely driven by the file size. These data blocks
    are then stored on the DataNodes for the data read and write to happen in parallel.
    Each data block is again a file in itself in the local file system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts how a large file is split into smaller chunks
    or blocks of fixed size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting large data files](img/B03980_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The size of each block can be controlled by the following configuration parameter
    in `hdfs-site.xml`. The cluster-wide block size is controlled by the `dfs.blocksize
    configuration` property in `hdfs-site.XML` The default value in Hadoop 1.0 is
    64 MB and in Hadoop 2.x is 128 MB. The block size is determined by the effectiveness
    of the infrastructure and can get bigger with higher transfer speeds and the usage
    of the new age drives:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.blocksize` | The value is 134217728.The previous value in bytes represents
    128 MB, alternatively any value suffixed by a measure can be defined. For example,
    512m, 1g, 128k, and so on. |'
  prefs: []
  type: TYPE_TB
- en: Any update to the value in the block size will not be applied to the existing
    blocks; only new blocks are eligible.
  prefs: []
  type: TYPE_NORMAL
- en: Block loading to the cluster and replication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once the file is split, the data blocks are formed of a fixed block size and
    are configured for the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'By virtue of the distributed architecture, there is a strong need to store
    replicas of the data blocks to handle data reliability. By default, three copies
    of each data block are stored. The number of the copies configuration property
    is called replication factor. The following table lists all the configurations
    related to data loading and replication:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.replication` | The value is 3.This defines the number of replicas that
    need to be stored in each block. |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.replication.max` | Maximal block replication. |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.replication.min` | Minimal block replication. |'
  prefs: []
  type: TYPE_TB
- en: The NameNode is responsible for ensuring the block placement and replication
    as per the configuration is done. With these data blocks placed onto DataNodes,
    each DataNode in the cluster sends block status periodically to the NameNode.
    The fact that NameNode receives a signal from the DataNode implies that the DataNode
    is active and functioning properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'HDFS uses a **default block placement policy** that is targeted to achieve
    load balancing across the available nodes. Following is the scope of this policy:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the copy or replica is written to the DataNode that is creating the file;
    this facilitates a higher write performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the copy or replica is written to another DataNode from the same rack;
    this minimizes network traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, the replica is written to a DataNode in a different rack; this way even
    if a switch fails, there still is a copy of the data block available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A default block placement policy is applied that uses all the nodes on the rack
    without compromising on the performance, data reliability, and availability. The
    following image depicts how three blocks of data are placed across four nodes
    with a replication strategy of two extra copies. Some of these nodes are located
    in the racks for optimal fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Block loading to the cluster and replication](img/B03980_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Overall, the flow of loading data into HDFS is shown in the following flow
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Block loading to the cluster and replication](img/B03980_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Writing to and reading from HDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While writing a file to HDFS, the client first contacts NameNode and passes
    the details of the file that needs to be written to HDFS. NameNode provides details
    on the replication configurations and other metadata details that specify where
    to place the data blocks. The following figure explains this flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing to and reading from HDFS](img/B03980_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Handling failures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the Hadoop cluster starts up, the NameNode gets into a safe-mode state
    and receives a heartbeat signal from all the data nodes. The fact that the NameNode
    receives a block report from DataNodes indicates that the DataNodes are up and
    functioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now say that **Data Node 4** goes down; this would mean that **Name
    Node** does not receive any heartbeat signals from **Data Node 4**. **Name Node**
    registers the unavailability of **Name Node** and hence, whatever **Data Node
    4** does is load balanced to the other nodes that have the replicas. This data
    is then updated in the metadata register by **Name Node**. The following figure
    illustrates the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Handling failures](img/B03980_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HDFS has a command line interface called **FS Shell**. This facilitates the
    usage of shell commands to manage HDFS. The following screenshot shows the `Hadoop
    fs` command, and its usage/syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HDFS command line](img/B03980_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: RESTFul HDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To have external applications, especially web applications or similar applications,
    have easy access to the data in HDFS over HTTP. HDFS supports an additional protocol
    called WebHDFS that is based on the RESTful standards that facilitate giving access
    to HDFS data over HTTP, without any need for Java binding or the availability
    of a complete Hadoop environment. Clients can use common tools such as curl/wget
    to access the HDFS. While providing web services-based access to data stored in
    HDFS, WebHDFS the built-in security and parallel processing capabilities of the
    platform, are well retained.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable WebHDFS, make the following configuration changes in `hdfs-site.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: More details on WebHDFS REST API can be found at [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html).
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MapReduce is similar to HDFS. The Hadoop MapReduce framework is inspired and
    built on Google's MapReduce framework. It is a distributed computing framework
    that facilitates processing gigantic amounts of data in parallel across clusters
    and has built-in fault tolerance mechanisms. It works on operating and processing
    the local data paradigm, where the processing logic is moved to the data instead
    of data moved to the processing logic.
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce](img/B03980_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MapReduce architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MapReduce framework is also based on Master-slave architecture. The master
    job is called JobTracker, and the slave jobs are called TaskTrackers. Unlike NameNode
    and DataNodes, these are not physical nodes, but are daemon processors that are
    responsible for running the processing logic across the DataNodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**JobTracker**: JobTracker schedules the execution of a job that comprises
    of multiple tasks. It is responsible for running the tasks or jobs on the task
    trackers and in parallel, monitors the status of processing. In the case of any
    failures, it is responsible for rerunning the failed tasks on the task tracker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TaskTracker**: TaskTracker executes the tasks scheduled by the JobTracker
    and constantly communicates with JobTracker, working in cohesion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's draw the analogy between the Master-slave architecture on the HDFS
    and MapReduce. The NameNode runs the JobTracker and DataNodes run TaskTrackers.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical multi-node cluster, the NameNode and DataNodes are separate physical
    nodes, but in the case of a single node cluster, where the NameNode and DataNode
    are infrastructure wise the same, JobTracker and TaskTracker functions run on
    the same node. Single node clusters are used in the development environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce architecture](img/B03980_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are two functions in a MapReduce process—`Map` and `Reduce`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapper**: Mapper job splits the file into multiple chunks in parallel, and
    runs some basic functions such as sorting, filtering, and any other specific business
    or analytics functions as needed. The output of the Mapper function is input to
    the Reducer function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducer**: Reducer job is used to consolidate the results across Mappers,
    and is additionally used to perform any business or analytics function as needed.
    The intermediate output from the Mapper and Reducer jobs are stored on the file
    system as key-value pairs. Both the input and output of the map and reduce jobs
    are stored in HDFS. Overall, the MapReduce framework takes care of scheduling
    the tasks, monitoring the status, and handling failures (if any). The following
    diagram depicts how the `Map` and the `Reduce` functions work and operate on the
    data held in HDFS:![MapReduce architecture](img/B03980_03_24.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes MapReduce cater to the needs of large datasets?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the advantages of MapReduce programming framework are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel execution**: MapReduce programs are, by default, meant to be executed
    in parallel that can be executed on a cluster of nodes. Development teams need
    not focus on the internals of distributed computing and can just use the framework
    directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault Tolerance**: MapReduce framework works on Master-slave Architecture
    where, in case any node goes down, corrective actions are taken automatically
    by the framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: MapReduce framework, having the ability to work distributed
    and with the ability to scale-out (horizontal scalability), with growing volumes
    new nodes, can be added to the cluster whenever needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Locality**: One of the core premises that the MapReduce framework does
    is to take the program to the data as opposed to the traditional way of bringing
    data to the code. So to be precise, MapReduce always has local data to it, and
    this is one of the most important reasons for the performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce execution flow and components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will a take a deep dive into the execution flow of MapReduce
    and how each of the components function:'
  prefs: []
  type: TYPE_NORMAL
- en: A new job is submitted by the client to JobTracker (a MapReduce job) along with
    the input and the output file paths and required configurations. The job gets
    queued for execution and gets picked by the job scheduler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JobTracker gets the data at the place where the required data in context resides,
    and creates an execution plan that triggers TaskTrackers for the execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JobTracker submits the job to the identified TaskTrackers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TaskTrackers execute the task using the data that is local to them. If the data
    is not available on the local Data Node, it communicates with other DataNodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TaskTrackers reports the status back to JobTracker by the means of heartbeat
    signals. JobTracker is capable of handling any failure cases inherently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, JobTracker reports the output to the Job client on the completion of
    the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps just described are depicted in the following figure. There are two
    parts to the flow: the HDFS and the MapReduce with the Nodes and the Trackers
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce execution flow and components](img/B03980_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let us focus on some core components of the MapReduce program and learn how
    to code it up. The following flow diagram details how the flow starts from the
    input data to the output data, and how each component or function of MapReduce
    framework kicks in to execute. The blocks in dotted red boxes are the components,
    and the blue boxes represent data being transitioned through the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce execution flow and components](img/B03980_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Developing MapReduce components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MapReduce framework of Hadoop comprises a set of Java APIs that need to
    be extended or implemented to incorporate a specific function that is targeted
    to be executed in parallel over the Hadoop cluster. Following are some API implementations
    that need to be done:'
  prefs: []
  type: TYPE_NORMAL
- en: Input and output data format interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapper implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducer implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combiner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InputFormat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `InputFormat` class is responsible for reading data from a file and making
    it available as input to the `map` function. Two core functions are performed
    by this process; one is splitting the input data into logical fragments called
    InputSplits, and the second is the reading of these splits as key value pairs
    to feed into the `map` function. There are two distinct interfaces to perform
    these two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: InputSplit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RecordReader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting of the input file is not an essential function. In case we need to
    consider a complete file for processing, we will need to override the `isSplittable()`
    function and set the flag to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: OutputFormat
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `OutputFormat` API is responsible for validating that Hadoop has output
    data formats against output specification of the job. The RecordWriter implementation
    is responsible for writing the final output key value pairs to the file system.
    Every InputFormat API has a corresponding OutputFormat API. The following table
    lists some of the input and output format APIs of the MapReduce framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input Format API | Corresponding Output Format API |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `TextInputFormat` | `TextOutputFormat` |'
  prefs: []
  type: TYPE_TB
- en: '| `SequenceFileInputFormat` | `SequenceFileOutputFormat` |'
  prefs: []
  type: TYPE_TB
- en: '| `DBInputFormat` | `DBOutputFormat` |'
  prefs: []
  type: TYPE_TB
- en: Mapper implementation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the Mapper implementations need to extend the `Mapper<KeyIn, ValueIn, KeyOut,
    ValueOut>` base class and importantly override the `map()` method to implement
    the specific business function. The Mapper implementation class takes key-value
    pairs as input and returns a set of key-value pairs as output. Any other interim
    output subsequently is taken by the shuffle and sort function.
  prefs: []
  type: TYPE_NORMAL
- en: There is one Mapper instance for each InputSplit generated by the InputFormat
    for a given MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, there are four methods that the Mapper implementation class needs
    to extend from the base class. Following are the methods that are briefly described,
    along with the purpose of each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method name and Syntax | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `setup(Context)` | This is the first method that is called back when a mapper
    is initiated for execution. It is not mandatory to override this method unless
    any specific initializations need to be done or any specific configuration setup
    needs to be done. |'
  prefs: []
  type: TYPE_TB
- en: '| `map(Object, Object, Context)` | Overriding this method is the key to mapper
    implementation as this method would be invoked as a part of executing the mapper
    logic. It takes key-value pairs as input, and the response can be a collection
    of key-value pairs |'
  prefs: []
  type: TYPE_TB
- en: '| `clean (Context)` | This method is called at the end of the mapper function
    execution in the lifecycle and facilitates clearing any resources utilized by
    the mapper. |'
  prefs: []
  type: TYPE_TB
- en: '| `run (Context)` | Overriding this method provides additional capability to
    run multi-threaded mappers. |'
  prefs: []
  type: TYPE_TB
- en: Let's take an example from a given file; we want to find out how many times
    a word is repeated. In this case, `TextInputFormat` is used. In fact, this is
    the default InputFormat. The following diagram shows what the InputSplit function
    does. It splits every row and builds a key-value pair.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram shows how the text is stored in multiple blocks on DataNode. `TextInputFormat`
    then reads these blocks and multiple InputSplits (we can see that there are two
    InputSplits, and hence there are two mappers). Each mapper picks an InputSplit
    and generates a key value pair for each occurrence of the word followed by the
    number 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapper implementation](img/B03980_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output of the mapper function is written onto the disk at the end of the
    processing, and none of the intermediate results are written to the file system.
    They are held in the memory. This helps in optimizing performance. It's possible
    because the key space is partitioned and each mapper only gets a fragment of the
    total dataset. Now, in terms of how much memory should be assigned for this purpose,
    by default, 100 MB is allocated and for any changes to this value, the `io.sort.mb`
    property will have to be set. There is usually a threshold set to this limit and,
    in case it exceeds this, there is a background process that starts writing onto
    the disk. The following program snippet demonstrates how to implement a mapper
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hadoop 2.x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until Hadoop 2.x, all the distributions were focused on addressing the limitations
    in Hadoop 1.x but did not deviate from the core architecture. Hadoop 2.x really
    changed the underlying architecture assumptions and turned out to be a real breakthrough;
    most importantly, the introduction of YARN. YARN was a new framework for managing
    Hadoop cluster, which introduced the ability to handle real-time processing needs
    in addition to the batch. Some important issues that were addressed are listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Single NameNode issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dramatic increase in the number of nodes in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extension to the number of tasks that can be successfully addressed with Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts the difference between the Hadoop 1.x and 2.x
    architectures and how YARN wires MapReduce and HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop 2.x](img/B03980_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop ecosystem components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hadoop has spawned a bunch of auxiliary and supporting frameworks. The following
    figure depicts the gamut of supporting frameworks contributed by the open source
    developer groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop ecosystem components](img/B03980_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following table lists all the frameworks and purposes of each framework.
    These frameworks work with the Apache distribution of Hadoop. There are many frameworks
    built by Vendors, who are commercially positioned and are not in the scope of
    this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | URL | Purpose (in brief) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **HDFS** (**Hadoop Distributed File System**) | [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
    | Hadoop File storage system is a core component of Hadoop, which has a built-in
    fault tolerance (refer to HDFS section for more details on the architecture and
    implementation specifics). |'
  prefs: []
  type: TYPE_TB
- en: '| MapReduce | [http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
    | MapReduce is a programming model and framework for processing large volumes
    of data on a distributed platform such as Hadoop. The latest version of Apache
    MapReduce extends another framework Apache YARN.**YARN**: MapReduce has gone through
    a complete overhaul in Hadoop 2.0 and now it is called MapReduce 2\. but the MapReduce
    programming model has not changed. YARN provides a new resource management and
    job scheduling model, along with its implementation to execute MapReduce jobs.
    In most cases, your existing MapReduce jobs run without any changes. In some instances,
    minor updates and recompilation might be needed. |'
  prefs: []
  type: TYPE_TB
- en: '| Pig | [https://pig.apache.org/](https://pig.apache.org/) | Pig is a framework
    to execute data flows in parallel. It comes with a scripting language, Pig Latin,
    that helps in developing the data flows. Pig Latin comes with a bunch of internal
    operations for data such as join, split, sort, and so on. Pig runs on Hadoop and
    utilizes both HDFS and MapReduce. The compiled Pig Latin scripts run their functions
    in parallel and internally. |'
  prefs: []
  type: TYPE_TB
- en: '| Hive | [https://hive.apache.org/](https://hive.apache.org/) | Hive is a data
    warehouse framework for Hadoop. It supports querying and handling big datasets
    held in distributed stores. An SQL-like querying language called HiveQL can be
    used that allows plugging in the mapper and reducer programs. |'
  prefs: []
  type: TYPE_TB
- en: '| Flume | [http://flume.apache.org/](http://flume.apache.org/) | The Flume
    framework is more of an efficient transport framework that facilitates aggregating,
    analyzing, processing, and moving huge volumes of log data. It comes with an extensible
    data model and supports online analytics. |'
  prefs: []
  type: TYPE_TB
- en: '| Chukwa | [https://chukwa.apache.org/](https://chukwa.apache.org/) | The Chukwa
    framework comes with an API that helps in easily collecting, analyzing, and monitoring
    prominent collections of data. Chukwa runs at the top of the HDFS and MapReduce
    framework, thus inheriting Hadoop''s ability to scale. |'
  prefs: []
  type: TYPE_TB
- en: '| HBase | [http://hbase.apache.org/](http://hbase.apache.org/) | HBase is inspired
    from Google BigTable. It is a NoSQL, columnar data store built to complement the
    Hadoop platform, and supports real-time operations on the data. HBase is a Hadoop
    database that is responsible for backing MapReduce job outputs. |'
  prefs: []
  type: TYPE_TB
- en: '| HCatalog | [https://cwiki.apache.org/confluence/display/Hive/HCatalog](https://cwiki.apache.org/confluence/display/Hive/HCatalog)
    | HCatalog is like a relational view of the data in HDFS. It doesn''t matter where
    and how or what format the underlying data is stored. It is currently a part of
    Hive, and there are no separate distributions for the current distributions. |'
  prefs: []
  type: TYPE_TB
- en: '| Avro | [http://avro.apache.org/](http://avro.apache.org/) | The Apache Avro
    framework is more of an interface to data. It supports modeling, serializing,
    and making **Remote Procedure Calls** (**RPC**). Every schema representation in
    Avro, also called the metadata definition, resides close to the data and on the
    same file, thus making the file self-describing. |'
  prefs: []
  type: TYPE_TB
- en: '| HIHO | [https://github.com/sonalgoyal/hiho/wiki/About-HIHO](https://github.com/sonalgoyal/hiho/wiki/About-HIHO)
    | HIHO stands for Hadoop-in Hadoop-out. This framework helps connecting multiple
    data stores with the Hadoop system and facilitate interoperability. HIHO supports
    several RDBMS and file systems, providing internal functions to load and off-load
    data between RDBMS and HDFS in parallel. |'
  prefs: []
  type: TYPE_TB
- en: '| Sqoop | [http://sqoop.apache.org/](http://sqoop.apache.org/) | Sqoop is a
    widely adopted framework for data transfer between HDFS and RDBMS in bulk or batch.
    It is very similar to Flume but operates with RDBMS. Sqoop is one of the **ETL**
    (**Extract-Transform-Load**) tools for Hadoop. |'
  prefs: []
  type: TYPE_TB
- en: '| Tajo | [http://tajo.apache.org/](http://tajo.apache.org/) | Tajo is a distributed
    data warehouse system for Apache Hadoop that is relational in nature. Tajo supports
    ad-hoc querying and online integration, and extract-transform-load functions on
    large datasets stored in HDFS or other data stores. |'
  prefs: []
  type: TYPE_TB
- en: '| Oozie | [http://oozie.apache.org/](http://oozie.apache.org/) | Oozie is a
    framework that facilitates workflow management. It acts as a scheduler system
    for MapReduce jobs using **DAG** (**Direct Acyclical Graph**). Oozie can either
    be data aware or time aware while it schedules and executes jobs. |'
  prefs: []
  type: TYPE_TB
- en: '| ZooKeeper | [http://zookeeper.apache.org/](http://zookeeper.apache.org/)
    | Zookeeper, as the name says it all, is more like an orchestration and coordination
    service for Hadoop. It provides tools to build, manage, and provide high availability
    for distributed applications. |'
  prefs: []
  type: TYPE_TB
- en: '| Ambari | [http://ambari.apache.org/](http://ambari.apache.org/) | Ambari
    is an intuitive web UI for Hadoop management with RESTful APIs. Apache Ambari
    was a contribution from Hortonworks. It serves as an interface to many other Hadoop
    frameworks in the ecosystem. |'
  prefs: []
  type: TYPE_TB
- en: '| Mahout | [http://mahout.apache.org/](http://mahout.apache.org/) | Apache
    Mahout is an open source library for Machine learning algorithms. The design focus
    for Mahout is to provide a scalable library for huge data sets distributed across
    multiple systems. Apache Mahout is a tool to derive useful information from raw
    data. |'
  prefs: []
  type: TYPE_TB
- en: Hadoop installation and setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different ways to setup Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone operation**: In this operation, Hadoop runs in a non-distributed
    mode. All the daemons run within a single Java process and help in easy debugging.
    This setup is also called single node installation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudo-Distributed Operation**: In this operation, Hadoop is configured to
    run on a single node, but in a pseudo-distributed mode that can run different
    daemon processes on different JVMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully-Distributed Operation**: In this operation, Hadoop is configured to
    run on multiple nodes in a fully-distributed mode, and all Hadoop daemons such
    as NameNode, Secondary Namenode, and JobTracker in the Master node; and DataNode
    and TaskTracker in slave nodes (in short, run on a cluster of nodes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Ubuntu-based Hadoop Installation prerequisites are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Java v1.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating dedicated Hadoop user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring SSH access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable IPv6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Jdk 1.7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download Java using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Installing Jdk 1.7](img/B03980_03_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Unpack the binaries using this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a directory to install Java with the help of the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Copy the binaries into the newly created directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure the PATH parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or else, use this command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Include the following content at the end of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In Ubuntu, configure the path for Java:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for installation completion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Installing Jdk 1.7](img/B03980_03_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Creating a system user for Hadoop (dedicated)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create/add a new group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create/add a new user and attach it to the group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Creating a system user for Hadoop (dedicated)](img/B03980_03_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Create/configure the SSH key access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the SSH setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Disable IPv6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open `sysctl.conf` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following lines at the end of the file. Reboot the machine to update
    the configurations correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Steps for installing Hadoop 2.6.0
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download Hadoop 2.6.0 using this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Unpack compressed Hadoop file using this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move hadoop-2.6.0 directory (a new directory):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move Hadoop to a local folder (for convenience) with this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the owner of the folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, update the configuration files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are three site-specific configuration files and one environment setup
    configuration file to communicate with the Master node (NameNode) and slave nodes
    (DataNodes):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`core-site.xml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hdfs-site.xml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapred-site.xml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yarn-site.xml`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigate to the path that has the configuration files:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: yarn-site.xml
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Th `core-site.XML` file has the details of the Master node IP or the hostname,
    Hadoop temporary directory path, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: core-site.xml
  prefs: []
  type: TYPE_NORMAL
- en: 'The `hdfs-site.xml` file has the details of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Local file system path where NameNode stores namespace and transactions logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of local file system paths to store the blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of replications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: hdfs-site.xml
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mapred-site.xml` file has the details of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The host or IP and port where the JobTracker runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path on the HDFS where Map/Reduce stores the files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of paths on the local file system to store the intermediate MapReduce
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum limit of Map/Reduce tasks for every task tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of DataNodes that need to be included or excluded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of TaskTrackers that need to be included or excluded![Steps for installing
    Hadoop 2.6.0](img/B03980_03_38.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mapred-site.xml
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Edit the `.bashrc` file as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Starting Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start the NameNode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start the DataNode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start ResourceManager, use the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Starting Hadoop](img/B03980_03_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To start NodeManager:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check Hadoop Web interfaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NameNode: `http://localhost:50070`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Secondary Namenode: `http://localhost:50090`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To stop Hadoop, use this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Hadoop distributions and vendors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the Apache distribution for Hadoop being the open source and core version
    that the big data community is adopting, several vendors have their distributions
    of the open source adoption of Apache Hadoop. Some of them have purely added support
    while others have wrapped and extended the capabilities of Apache Hadoop and its
    ecosystem components. In many cases, they have their frameworks or libraries built
    over the core frameworks to add new functionality or features to the underlying
    core component.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, let us cover some of the distributions of Apache Hadoop and
    some differentiating data facts that help the development teams or organizations
    to take a decision about the distribution that works best for their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now consider the following vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloudera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hortonworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal / EMC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Category | Function/Framework | Cloudera | Hortonworks | MapR | Pivotal |
    IBM |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Performance and Scalability | Data Ingestion | Batch | Batch | Batch and
    Streaming | Batch and Streaming | Batch and Streaming |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Metadata architecture | Centralized | Centralized | Distributed | Centralized
    | Centralized |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | HBase performance | Spikes in latency | Spikes in latency | Low latency
    | Low latency | Spikes in latency |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | NoSQL Support | Mainly batch applications | Mainly batch applications
    | Batch and online systems | Batch and online systems | Batch and online systems
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Reliability | High Availability | Single failure recovery | Single failure
    recovery | Self-healing across multiple failures | Self-healing across multiple
    failures | Single failure recovery |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Disaster Recovery | File copy | N/A | Mirroring | Mirroring | File copy
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Replication | Data | Data | Data and metadata | Data and metadata | Data
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Snapshots | Consistent with closed files | Consistent with closed files
    | Point in time consistency | Consistent with closed files | Consistent with closed
    files |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Upgrading | Rolling upgrades | Planned | Rolling upgrades | Planned |
    Planned |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Manageability** | Volume Support | No | No | Yes | Yes | Yes |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Management Tools | Cloudera Manager | Ambari | MapR Control system |
    Proprietary console | Proprietary console |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Integration with REST API | Yes | Yes | Yes | Yes | Yes |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Job replacement control | No | No | Yes | Yes | No |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Data Access & Processing** | File System | HDFS, Read-only NFS | HDFS,
    read-only NFS | HDFS, read/write NFS and POSIX | HDFS, read/write NFS | HDFS,
    read-only NFS |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | File I/O | Append-only | Append-only | Read/write | Append-only | Append-only
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Security ACLs | Yes | Yes | Yes | Yes | Yes |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Authentication | Kerberos | Kerberos | Kerberos and Native | Kerberos
    and Native | Kerberos and Native |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered all about Hadoop, starting from core frameworks
    to ecosystem components. At the end of this chapter, readers should be able to
    set up Hadoop and run some MapReduce functions. Users should be able to run and
    manage a Hadoop environment and understand the command line usage using one or
    more ecosystem component.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, our focus is on the key Machine learning frameworks such
    as Mahout, Python, R, Spark, and Julia; these either have inherent support on
    the Hadoop platform, or need direct integration with the Hadoop platform for supporting
    large datasets.
  prefs: []
  type: TYPE_NORMAL
