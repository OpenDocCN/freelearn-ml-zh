- en: Chapter 3. An Introduction to Hadoop's Architecture and Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章。Hadoop 架构和生态系统的介绍
- en: From this chapter onwards, we start with the implementation aspects of Machine
    learning. Let's start learning the platform of choice—a platform that can scale
    to Advanced Enterprise Data needs (big data needs of Machine learning in specific)—Hadoop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将探讨机器学习的实现方面。让我们开始学习选择平台——一个可以扩展到高级企业数据需求（特别是机器学习的具体大数据需求）的平台——Hadoop。
- en: In this chapter, we cover Hadoop platform and its capabilities in addressing
    large-scale loading, storage, and processing challenges for Machine learning.
    In addition to an overview of Hadoop Architecture, its core frameworks, and the
    other supporting ecosystem components, also included here is a detailed installation
    process with an example deployment approach. Though there are many commercial
    distributions of Hadoop, our focus in this chapter is to cover the open source,
    Apache distribution of Hadoop (latest version 2.x).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Hadoop 平台及其在解决机器学习大规模加载、存储和处理挑战方面的能力。除了对 Hadoop 架构、其核心框架以及其他支持生态系统组件的概述外，还包括了一个详细的安装过程和示例部署方法。尽管有许多商业版的
    Hadoop，但本章的重点是介绍开源的 Apache Hadoop 发行版（最新版本 2.x）。
- en: 'In this chapter, the following topics are covered in-depth:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下主题将进行深入探讨：
- en: An introduction to Apache Hadoop, its evolution history, the core concepts,
    and the ecosystem frameworks that comprise Hadoop
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop 的介绍，其演变历史，核心概念以及构成 Hadoop 的生态系统框架
- en: Hadoop distributions and specific offerings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 发行版和特定产品
- en: Installation and set up of the Hadoop environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 环境的安装和设置
- en: Hadoop 2.0—HDFS and MapReduce (also **YARN** (**Yet Another Resource Negotiator**))
    architectures with example implementation scenarios, using different components
    of the architecture
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 2.0——HDFS 和 MapReduce（也称为 **YARN**（**另一个资源协调器**））架构，以及使用架构的不同组件的示例实现场景
- en: Understanding the purpose of the core ecosystem components, setting up and learning
    to build and run the programs using examples
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解核心生态系统组件的目的，设置并学习使用示例构建和运行程序
- en: Exploring Machine learning specific ecosystem extensions such as Mahout and
    R Connectors ([Chapter 4](ch04.html "Chapter 4. Machine Learning Tools, Libraries,
    and Frameworks"), *Machine Learning Tools, Libraries, and Frameworks*, covers
    the implementation details)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索针对机器学习特定生态系统扩展，如 Mahout 和 R Connectors ([第 4 章](ch04.html "第 4 章。机器学习工具、库和框架")，*机器学习工具、库和框架*，涵盖了实现细节）
- en: Introduction to Apache Hadoop
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop 简介
- en: 'Apache Hadoop is an open source, Java-based project from the Apache Software
    Foundation. The core purpose of this software has been to provide a platform that
    is scalable, extensible, and fault tolerant for the distributed storage and processing
    of big data. Please refer to [Chapter 2](ch02.html "Chapter 2. Machine learning
    and Large-scale datasets"), *Machine learning and Large-scale Datasets* for more
    information on what data qualifies as big data. The following image is the standard
    logo of Hadoop:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop 是 Apache 软件基金会的一个基于 Java 的开源项目。该软件的核心目的是提供一个可扩展、可扩展和容错的平台，用于分布式存储和处理大数据。请参阅
    [第 2 章](ch02.html "第 2 章。机器学习和大规模数据集")，*机器学习和大规模数据集*，了解更多关于哪些数据可以被视为大数据的信息。以下图像是
    Hadoop 的标准标志：
- en: '![Introduction to Apache Hadoop](img/B03980_03_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Hadoop 简介](img/B03980_03_01.jpg)'
- en: At the heart of it, it leverages clusters of nodes that can be commodity servers
    and facilitates parallel processing. The name Hadoop was given by its creator
    Doug Cutting, naming it after his child's yellow stuffed toy elephant. Till date,
    Yahoo! has been the largest contributor and an extensive user of Hadoop. More
    details of Hadoop, its architecture, and download links are accessible at [http://hadoop.apache.org/](http://hadoop.apache.org/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其核心是利用节点集群，这些节点可以是商用服务器，并促进并行处理。Hadoop 这个名字是由其创造者 Doug Cutting 取的，他以自己孩子的黄色填充玩具大象命名。到目前为止，Yahoo!
    是最大的贡献者和 Hadoop 的广泛使用者。有关 Hadoop 的更多详细信息、其架构和下载链接，请访问 [http://hadoop.apache.org/](http://hadoop.apache.org/)。
- en: Hadoop is an industry standard platform for big data, and it comes with extensive
    support for all the popular Machine learning tools in the market. This platform
    is now used by several big firms such as Microsoft, Google, Yahoo!, and IBM. It
    is also used to address specific Machine learning requirements like sentiment
    analysis, search engines, and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是大数据的行业标准平台，它为市场上所有流行的机器学习工具提供了广泛的支持。该平台现在被微软、谷歌、Yahoo！和IBM等几家大型公司使用。它还被用来解决特定的机器学习需求，如情感分析、搜索引擎等。
- en: The following sections cover some key characteristics of the Hadoop platform
    that make it ideal for facilitating efficiency in large-scale data storage and
    processing capabilities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节涵盖了Hadoop平台的一些关键特性，使其在大型数据存储和处理能力方面效率极高。
- en: Evolution of Hadoop (the platform of choice)
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop的发展（首选平台）
- en: The following figure (Source Cloudera Inc.) explains the evolution of the Hadoop
    platform. With Doug Cutting and Mike Cafarella starting it all in 2002 to build
    a greatly scalable search engine that is open source and hence extensible and
    running over a bunch of machines. Some important milestones in this evolution
    phase have been by Google that released the **Google File System** (**GFS**) in
    October 2003, followed by the MapReduce framework in December 2004 that evolved
    to form the core frameworks HDFS and MapReduce/YARN respectively.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（来源Cloudera Inc.）解释了Hadoop平台的发展历程。从2002年Doug Cutting和Mike Cafarella开始，旨在构建一个可高度扩展的、开源的、可扩展的、运行在多台机器上的搜索引擎。在这个演变阶段的一些重要里程碑包括Google在2003年10月发布的**Google文件系统**（**GFS**），随后在2004年12月发布的MapReduce框架，它演变成了核心框架HDFS和MapReduce/YARN。
- en: 'The other significant milestone had been Yahoo''s contribution and adoption
    around February 2008 when Yahoo implemented a production version that had indexing
    of searches implemented over 10,000 Hadoop cluster nodes. The following table
    depicts the evolution of Hadoop:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的里程碑发生在2008年2月左右，当时Yahoo贡献并采用了Hadoop的生产版本，该版本在超过10,000个Hadoop集群节点上实现了搜索索引。以下表格描述了Hadoop的发展历程：
- en: '![Evolution of Hadoop (the platform of choice)](img/B03980_03_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop的发展（首选平台）](img/B03980_03_02.jpg)'
- en: Hadoop and its core elements
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop及其核心元素
- en: 'The following concept map depicts the core elements and aspects of the Hadoop
    platform:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下概念图描绘了Hadoop平台的核心元素和方面：
- en: '![Hadoop and its core elements](img/B03980_03_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop及其核心元素](img/B03980_03_03.jpg)'
- en: Machine learning solution architecture for big data (employing Hadoop)
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据机器学习解决方案架构（采用Hadoop）
- en: In this section, let us look at the essential architecture components for implementing
    a Machine learning solution considering big data requirements.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们看看在考虑大数据需求时实施机器学习解决方案的基本架构组件。
- en: The proposed solution architecture should support the consumption of a variety
    of data sources in an efficient and cost-effective way. The following figure summarizes
    the core architecture components that should potentially be a part of the Machine
    learning solution technology stack. The choice of frameworks can either be open
    source or packaged license options. In the context of this book, we consider the
    latest version of open source (Apache) distribution of Hadoop and its ecosystem
    components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的解决方案架构应支持以高效且经济的方式消费各种数据源。以下图总结了可能成为机器学习解决方案技术栈一部分的核心架构组件。框架的选择可以是开源或打包许可选项。在本书的背景下，我们考虑了开源（Apache）Hadoop及其生态系统组件的最新版本。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Vendor specific frameworks and extensions are out of scope for this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不涉及特定供应商的框架和扩展。
- en: '![Machine learning solution architecture for big data (employing Hadoop)](img/B03980_03_04.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![大数据机器学习解决方案架构（采用Hadoop）](img/B03980_03_04.jpg)'
- en: In the next sections, we'll discuss in detail each of these Reference Architecture
    layers and the required frameworks in each layer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论这些参考架构层以及每一层所需的框架。
- en: The Data Source layer
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据源层
- en: The Data Source layer forms a critical part of the Machine learning Reference
    Architecture. There are many internal and external data feeds that form an input
    to solving a Machine learning problem. These feeds can be structured, unstructured,
    or semi-structured in nature. Moreover, in real-time, batch, or near real time
    mode, they need to be seamlessly integrated and consolidated for analytics engines
    and visualization tools.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源层是机器学习参考架构的关键部分。有许多内部和外部数据源形成了解决机器学习问题的输入。这些源可以是结构化、非结构化或半结构化的。此外，在实时、批量或近实时模式下，它们需要无缝集成和汇总，以便用于分析引擎和可视化工具。
- en: Before ingesting this data into the system for further processing, it is important
    to remove the irrelevance or the noise in the data. Some unique techniques can
    be applied to clean and filter the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据摄取到系统中进行进一步处理之前，去除数据中的无关或噪声是很重要的。可以应用一些独特的技术来清理和过滤数据。
- en: These consolidated datasets are also called data lakes in big data and the data
    aggregation context. Hadoop is one of the storage options of choice for Data Lakes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些综合数据集在大数据和数据聚合环境中也被称为数据湖。Hadoop是数据湖选择的存储选项之一。
- en: The following diagram shows the variety of data sources that form a primary
    source of input.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了构成主要输入源的各种数据源。
- en: '![The Data Source layer](img/B03980_03_05.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![数据源层](img/B03980_03_05.jpg)'
- en: Data architectures have always been designed to support some of the protocols
    such as JMS, HTTP, XML, and so on. However, now, the recent advancements in the
    field of big data have brought about significant changes. So now, the new age
    data sources include data streams from social networking sites, GPS data, machine-generated
    data such as user access logs, and other proprietary data formats.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据架构始终被设计来支持一些协议，例如JMS、HTTP、XML等。然而，现在，大数据领域的最新进展已经带来了显著的变化。因此，现在的新时代数据源包括来自社交网站的数据流、GPS数据、用户访问日志等机器生成数据以及其他专有数据格式。
- en: The Ingestion layer
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄取层
- en: The Data Ingestion layer is responsible for bringing data in from multiple data
    sources into the system, with a primary responsibility to ensure data quality.
    This layer has the capability to filter, transform, integrate, and validate data.
    It is important that the choice of technology to implement this layer should be
    able to support high volumes and other characteristics of data. The following
    meta model shows the composition and flow of functions of the Ingestion Layer.
    An ingestion layer could potentially be an **ETL** short for (**Extract, Transform,
    and Load**) capability in the architecture.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取层负责将来自多个数据源的数据引入系统，其主要责任是确保数据质量。这一层具有过滤、转换、集成和验证数据的能力。选择实施这一层的技术应能够支持高数据量和其他数据特性。以下元模型显示了摄取层组成和功能流程。摄取层可能是一个**ETL**（提取、转换和加载）能力的架构。
- en: 'Listed below are a set of basic requirements for an ingestion layer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了摄取层的一组基本要求：
- en: High-speed transformation of data from any source system in any manner
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任何源系统以任何方式高速转换数据
- en: Processing large volumes of records in minimal time
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以最短时间处理大量记录
- en: Producing output in a semantically rich format so that any target system can
    query for **smart data**
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以语义丰富的格式生成输出，以便任何目标系统都可以查询**智能数据**
- en: 'The architecture framework for ingestion layer needs to provide the following
    capabilities; the upcoming model depicts various layers and compositions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 摄取层的架构框架需要提供以下能力；以下模型描绘了各种层和组成：
- en: An Adapter Framework—any product group or application should be able to use
    the Adapter Framework to quickly, reliably, and programmatically develop connectors
    to different data sources (Files, CSV format, and DB)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适配器框架——任何产品组或应用都应该能够使用适配器框架快速、可靠和程序性地开发连接到不同数据源的连接器（文件、CSV格式和数据库）
- en: A high speed, parallel transformation execution engine
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个高速、并行变换执行引擎
- en: A job execution framework
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个作业执行框架
- en: Semantified output generator framework![The Ingestion layer](img/B03980_03_06.jpg)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义化输出生成框架![摄取层](img/B03980_03_06.jpg)
- en: The Ingestion layer loads the relevant data into the storage layer, which in
    our current context is the Hadoop Storage Layer that is primarily a file-based
    storage layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 摄取层将相关数据加载到存储层，在我们的当前环境中，这是以文件为基础的存储层Hadoop存储层。
- en: 'The concept map below lists ingestion core patterns (these patterns address
    performance and scalability needs of a Machine learning architecture):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的概念图列出了摄入核心模式（这些模式针对机器学习架构的性能和可扩展性需求）：
- en: '![The Ingestion layer](img/B03980_03_07.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![摄入层](img/B03980_03_07.jpg)'
- en: '**Parallel Processing and Partitioning Patterns**: The fundamental architecture
    for handling large volume ingestion requirements is to parallelize the execution.
    Running transformations on different input data in parallel and partitioning a
    single, large volume input into smaller batches for processing in parallel helps
    achieve parallelization.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行处理和分区模式**：处理大量摄入需求的基本架构是并行化执行。并行地对不同输入数据进行转换，并将单个大型输入数据分区成更小的批次以并行处理，有助于实现并行化。'
- en: '**Pipeline design patterns**: When designing the workflows for ingestion jobs,
    there are specific issues that need to be addressed, such as avoiding large sequential
    pipelines that enable parallel processing. Similarly, from the data reliability
    point of view, creating appropriate audit and execution logs is important to manage
    the entire ingestion execution.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道设计模式**：在设计摄入作业的工作流程时，需要解决一些特定问题，例如避免大型顺序管道以实现并行处理。同样，从数据可靠性角度来看，创建适当的审计和执行日志对于管理整个摄入执行过程非常重要。'
- en: '**Transformation patterns**: There are different categories of transformation.
    One of the main aspects of the transformation designs is to handle dependency.
    The patterns mentioned in the first category (parallelization) also handle dependency
    requirements. Other issues relate to the dependency on the past and historical
    data, which is especially significant when processing additional loads.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换模式**：存在不同类别的转换。转换设计的主要方面之一是处理依赖关系。在第一类（并行化）中提到的模式也处理依赖需求。其他问题与对过去和历史数据的依赖有关，这在处理额外负载时尤其重要。'
- en: '**Storage Design**: When loading data into the target data store, there are
    issues such as recovering from failed transformations or reloading data for specific
    feeds (for example, when there should be a fixed transformation rule).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储设计**：在将数据加载到目标数据存储时，存在诸如从失败的转换中恢复或为特定馈送重新加载数据等问题（例如，当应该有一个固定的转换规则时）。'
- en: '**Data Load patterns**: One of the biggest performance bottlenecks in data
    ingestion is the speed of loading data into the target data mart. Especially when
    the target is an RDBMS, parallelization strategies lead to concurrency issues
    while loading the data, limiting the throughput of the ingestion that is possible.
    The patterns present certain techniques of how to realize the data load and address
    performance and concurrency issues while loading data.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加载模式**：数据摄入过程中最大的性能瓶颈之一是将数据加载到目标数据仓库的速度。特别是当目标是关系型数据库管理系统（RDBMS）时，并行化策略在加载数据时会导致并发问题，从而限制了可能的摄入吞吐量。这些模式展示了实现数据加载并解决加载数据时的性能和并发问题的某些技术。'
- en: The Hadoop Storage layer
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop 存储层
- en: Machine learning architecture has a distributed storage layer that supports
    parallel processing for running analytics or heavy computations over big data.
    The usage of distributed storage and processing large volumes in parallel is a
    fundamental change in the way an enterprise handles big data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习架构具有支持在大型数据上运行分析或重计算的并行处理的分布式存储层。使用分布式存储和并行处理大量数据是企业在处理大数据方面的一种根本性变化。
- en: A typical distributed storage facilitates high performance by parallel processing
    the algorithms that run over petabyte scale data with fault-tolerance, reliability,
    and parallel processing capabilities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的分布式存储通过并行处理在PB级数据上运行的算法，提供容错、可靠性和并行处理能力，从而实现高性能。
- en: In the current context of Hadoop architecture, **Hadoop Distributed File System**
    (**HDFS**) is the core storage mechanism. In this section, let us have a brief
    look at HDFS and NoSQL (Not-only-SQL) storage options. The following sections
    cover HDFS and its architecture in more detail.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的 Hadoop 架构背景下，**Hadoop 分布式文件系统（HDFS**）是核心存储机制。在本节中，让我们简要了解一下 HDFS 和 NoSQL（非SQL）存储选项。以下几节将更详细地介绍
    HDFS 及其架构。
- en: HDFS is one of the core components and acts as a database for Hadoop. It is
    a distributed file system that stores large-scale data across a cluster of nodes.
    It comes with a framework to ensure data reliability and fault tolerance. Applications
    can store files in parts or whole depending on the size, and it facilitates the
    write once read many times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是核心组件之一，充当Hadoop的数据库。它是一个跨节点集群存储大规模数据的分布式文件系统。它附带一个框架来确保数据可靠性和容错性。应用程序可以根据大小将文件存储部分或全部，并且它促进了单次写入多次读取。
- en: Since HDFS is a file system, access to data for consumption or manipulation
    is not simple and requires some complex file operation programs. Another way of
    bringing in easier data management is by using non-relational stores called NoSQL
    stores.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HDFS是一个文件系统，对数据的访问或操作并不简单，需要一些复杂的文件操作程序。另一种使数据管理更简单的方法是使用非关系型存储，称为NoSQL存储。
- en: The following model represents various NoSQL data store categories that are
    available with examples for each of the categories. Every data store category
    caters to a particular business requirement, and it is important to understand
    the purpose of each of these categories of NoSQL store to make the right choice
    for a given requirement. The CAP theorem (that stands for consistency, availability,
    and partition tolerance) attributes are satisfied to a varying degree for each
    of the NoSQL stores, resulting in support for optimized storage systems that are
    expected to work for combinations of these attributes. In reality, these NoSQL
    stores may have to coexist with relational stores as they would need a system
    of record to sync up on a need basis, or a better case is where we would need
    to use a combination of relational and non-relational data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型表示各种NoSQL数据存储类别，以及每个类别的示例。每个数据存储类别都满足特定的业务需求，了解这些NoSQL存储类别的目的对于根据特定需求做出正确的选择非常重要。CAP定理（代表一致性、可用性和分区容错性）的属性在每种NoSQL存储中都有不同程度的满足，从而支持预期适用于这些属性组合的优化存储系统。实际上，这些NoSQL存储可能需要与关系型存储共存，因为它们需要一个记录系统来根据需要同步，或者更好的情况是我们需要使用关系型和非关系型数据的组合。
- en: 'The following figure depicts types of NoSQL databases and some of the products
    in the market:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了NoSQL数据库的类型以及市场上的一些产品：
- en: '![The Hadoop Storage layer](img/B03980_03_08.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop存储层](img/B03980_03_08.jpg)'
- en: Hadoop was originally meant for \batch processing where the data into HDFS is
    loaded in batch or a scheduled manner. Usually, the storage layer has data loaded
    in batch. Some of the core and ecosystem components that facilitate data loading
    or ingestion into HDFS are Sqoop, **HIHO** (**Hadoop-in Hadoop-out**) MapReduce
    function, and ETL functions among others.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop最初是为了批处理而设计的，其中数据以批量或计划的方式加载到HDFS中。通常，存储层的数据是以批量加载的。一些核心和生态系统组件，如Sqoop、**HIHO**（Hadoop-in
    Hadoop-out）MapReduce函数和ETL函数等，有助于数据加载或摄入HDFS。
- en: The Hadoop (Physical) Infrastructure layer – supporting appliance
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop（物理）基础设施层 - 支持设备
- en: The difference between the traditional architectures and big data (for Machine
    learning) architecture is the importance that the underlying infrastructure grabs.
    Performance, Scalability, Reliability, Fault Tolerance, High Availability, and
    Disaster Recovery are some of the important quality attributes that this architecture
    is required to support. The underlying infrastructure of the platform handles
    these requirements.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 传统架构与大数据（用于机器学习）架构之间的区别在于底层基础设施的重要性。性能、可扩展性、可靠性、容错性、高可用性和灾难恢复是该架构必须支持的一些重要质量属性。平台的基础设施处理这些需求。
- en: The Hadoop Infrastructure is a distributed architecture or model where data
    is not stored in one place, but is distributed across multiple or a cluster of
    nodes. The data distribution strategy can be intelligent (as in the case of Greenplum)
    or can be simply mathematical (as in the case of Hadoop). The distributed file
    system nodes are linked over a network. This is referred to as **Shared Nothing
    Architecture** (**SNA**), and the big data solutions work on this reference architecture.
    Along with the data being distributed across multiple nodes, the processes run
    locally to the data nodes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop基础设施是一个分布式架构或模型，其中数据不是存储在一个地方，而是分布到多个或一组节点上。数据分布策略可以是智能的（如Greenplum的情况），也可以是简单的数学方法（如Hadoop的情况）。分布式文件系统节点通过网络连接。这被称为**无共享架构**（**SNA**），大数据解决方案基于此参考架构。随着数据分布到多个节点，进程在数据节点本地运行。
- en: This is first cited in Michael Stonebraker's paper that can be accessed at [http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf](http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这首先在Michael Stonebraker的论文中被引用，该论文可通过[http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf](http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf)访问。
- en: 'Nodes that have data stored are called data nodes and those where processing
    happens are called compute nodes. The data and compute nodes can be collocated
    or decoupled. The following figure represents an SNA context that has data and
    compute nodes collocated:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据的节点称为数据节点，而处理发生的节点称为计算节点。数据节点和计算节点可以是本地化部署或解耦部署。以下图表示了一个具有本地化数据节点和计算节点的SNA（无共享架构）环境：
- en: '![The Hadoop (Physical) Infrastructure layer – supporting appliance](img/B03980_03_09.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop（物理）基础设施层 – 支持设备](img/B03980_03_09.jpg)'
- en: Shared nothing data architecture supports parallel processing. Redundancy is
    a default expectation as it deals with a variety of data from diverse sources.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无共享数据架构支持并行处理。冗余是默认期望，因为它处理来自不同来源的各种数据。
- en: Hadoop and HDFS, over a grid infrastructure connected, over a fast gigabit network,
    or a virtual cloud infrastructure, forms the infrastructure layer that supports
    large-scale Machine learning architecture.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop和HDFS通过网格基础设施连接，通过快速千兆网络或虚拟云基础设施连接，形成了支持大规模机器学习架构的基础设施层。
- en: 'The following figure illustrates big data infrastructure setup using commodity
    servers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了使用通用服务器设置的大数据基础设施：
- en: '![The Hadoop (Physical) Infrastructure layer – supporting appliance](img/B03980_03_10.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop（物理）基础设施层 – 支持设备](img/B03980_03_10.jpg)'
- en: Hadoop platform / Processing layer
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop平台/处理层
- en: The platform or processing layer for Hadoop is the core data processing layer
    of the Machine learning architecture tools. This layer facilitates querying or
    accessing data stored in Hadoop's storage layer (NoSQL databases that use the
    HDFS storage file system typically), sitting at the top of the Hadoop infrastructure
    layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的平台或处理层是机器学习架构工具的核心数据处理层。这一层便于查询或访问存储在Hadoop存储层（通常使用HDFS存储文件系统的NoSQL数据库）中的数据，位于Hadoop基础设施层的顶部。
- en: As learned in [Chapter 2](ch02.html "Chapter 2. Machine learning and Large-scale
    datasets"), *Machine learning, and Large-scale datasets*, technological advancements
    in the field of computing now facilitate handling large volumes of distributed
    computing and parallel processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第二章](ch02.html "第二章. 机器学习与大规模数据集")所述，*机器学习与大规模数据集*，计算领域的科技进步现在使得处理大量分布式计算和并行处理成为可能。
- en: The MapReduce framework of Hadoop helps to store and analyze large volumes of
    data efficiently and in an inexpensive way.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的MapReduce框架有助于高效且低成本地存储和分析大量数据。
- en: 'The key components of the Hadoop platform or processing layer are listed next;
    these components are a part of the ecosystem and are discussed in detail in the
    sections that follow in this chapter:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop平台或处理层的关键组件如下；这些组件是生态系统的一部分，将在本章后续部分详细讨论：
- en: '**MapReduce**: MapReduce is a programming paradigm that is used to efficiently
    execute a function over a larger volume of data, typically in a batch mode. The
    *map* function is responsible for distributing the tasks across multiple systems,
    distributing the load equally and managing the processing in parallel. Post processing;
    the *reduce* function assimilates and combines the elements to provide a result.
    A step-by-step implementation on Hadoop''s native MapReduce architecture, MapReduce
    v2, and YARN is covered in the *Hadoop ecosystem components* section.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce**：MapReduce是一种编程范式，用于高效地在大量数据上执行函数，通常以批量模式运行。*map*函数负责将任务分配到多个系统，均匀分配负载并并行管理处理。在后续处理中，*reduce*函数将元素同化并合并以提供结果。在*Hadoop生态系统组件*部分介绍了在Hadoop原生MapReduce架构、MapReduce
    v2和YARN上的逐步实现。'
- en: '**Hive**: Hive is a data warehouse framework for Hadoop and is responsible
    for aggregating high volumes of data with SQL-like functions. Hive facilitates
    an efficient way of storing data that uses resources optimally. The configuration
    and implementation aspects of Hive are covered in the *Hadoop ecosystem components*
    section.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hive**：Hive是Hadoop的数据仓库框架，负责使用类似SQL的函数聚合大量数据。Hive提供了一种高效的数据存储方式，能够最优地使用资源。Hive的配置和实现方面在*Hadoop生态系统组件*部分进行了介绍。'
- en: '**Pig**: Pig is a simple scripting language that facilitates querying and manipulating
    data held on HDFS. It internally runs the functions in a MapReduce paradigm and
    is often perceived to simplify building MapReduce functions. A detailed step-by-step
    guide to configuring, learning the syntax, and building essential functions is
    covered in the *Hadoop ecosystem components* section.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pig**：Pig是一种简单的脚本语言，它简化了在HDFS上查询和操作数据的操作。它内部以MapReduce范式运行函数，通常被认为简化了构建MapReduce函数的过程。在*Hadoop生态系统组件*部分提供了配置、学习语法和构建基本函数的详细逐步指南。'
- en: '**Sqoop**: Sqoop is a data import tool for Hadoop that has inbuilt functions
    to import data from specific tables, columns, or complete database onto the file
    system. Post processing, Sqoop supports extracting data from several Relational
    databases and NoSQL data stores.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sqoop**：Sqoop是Hadoop的数据导入工具，具有内置函数，可以从特定的表、列或整个数据库导入数据到文件系统。在后续处理中，Sqoop支持从多个关系型数据库和NoSQL数据存储中提取数据。'
- en: '**HBase**: HBase is a Hadoop compliant NoSQL data store (a columnar NoSQL data
    store) that uses HDFS as the underlying file system. It supports distributed storage
    and automatic linear scalability.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HBase**：HBase是一个兼容Hadoop的NoSQL数据存储（一个列式NoSQL数据存储），它使用HDFS作为底层文件系统。它支持分布式存储和自动线性可伸缩性。'
- en: '**ZooKeeper**: ZooKeeper is a monitoring and coordinating service that helps
    keep a check on the Hadoop instances and nodes. It is responsible for keeping
    the infrastructure synchronized and protects the distributed system from partial
    failures and ensures data consistency. The ZooKeeper framework can work standalone
    or outside Hadoop.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZooKeeper**：ZooKeeper是一个监控和协调服务，有助于监控Hadoop实例和节点。它负责保持基础设施同步，保护分布式系统免受部分故障的影响，并确保数据一致性。ZooKeeper框架可以独立于Hadoop工作。'
- en: More of these ecosystem components are discussed in depth in the following sections.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，将深入讨论这些生态系统组件。
- en: The Analytics layer
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析层
- en: More often, enterprises have some real **Business Intelligence** (**BI**) tools
    that are responsible for running some analytical queries and producing some MIS
    reports or dashboards. There is a need for modern Machine learning or analytics
    tools and frameworks to coexist with them. There is now a need for the analytics
    to run either in a traditional way on the data warehouses or big data stores as
    well that can handle structured, semi-structured, and unstructured data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的是，企业拥有一些真正的**商业智能（BI**）工具，这些工具负责运行一些分析查询并生成一些管理信息系统（MIS）报告或仪表板。现在需要现代机器学习或分析工具和框架与它们共存。现在需要分析可以在传统方式下在数据仓库或可以处理结构化、半结构化和非结构化数据的大数据存储上运行。
- en: In this case, we can expect the data flow between the traditional data stores
    and the big data stores using tools such as Sqoop.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以期待使用像Sqoop这样的工具在传统数据存储和大数据存储之间进行数据流。
- en: NoSQL stores are known for low latency; they facilitate real-time analytics.
    Many open source analytics frameworks have simplified building models, and run
    complex statistical and mathematical algorithms using simple out-of-box functions.
    All that is required now is to understand the relevance of each of the algorithms
    and the ability to choose a suitable algorithm or approach, given a specific problem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL存储以其低延迟而闻名；它们促进了实时分析。许多开源分析框架简化了模型的构建，并使用简单的开箱即用的函数运行复杂的统计和数学算法。现在所需的是理解每个算法的相关性，以及在给定特定问题时选择合适的算法或方法的能力。
- en: Let us look into the below listed open source Analytics and Machine learning
    frameworks in the chapters to follow.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看以下列出的开源分析和机器学习框架，在接下来的章节中会有详细介绍。
- en: R
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R
- en: Apache Mahout
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Mahout
- en: Python (scikit-learn distribution)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（scikit-learn发行版）
- en: Julia
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Julia
- en: Apache Spark
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: An introduction to one of the upcoming Spring projects called **Spring XD**
    is covered, as it looked like a comprehensive Machine learning solution that can
    run on Hadoop.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了即将推出的Spring项目之一，称为**Spring XD**，它看起来像是一个可以在Hadoop上运行的全面的机器学习解决方案。
- en: The Consumption layer
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消费层
- en: 'The insights generated from the analytics layer or the results of data processing
    are consumed in many ways by the end clients. Some of the ways this data can be
    made available for consumption are:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从分析层或数据处理中产生的洞察力以多种方式被最终客户消费。以下是一些使数据可供消费的方式：
- en: Service APIs (for example, Web Service Interfaces (SOAP based or REST))
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务API（例如，基于SOAP或REST的Web服务接口）
- en: Web applications
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络应用程序
- en: Reporting engines and data marts
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告引擎和数据集市
- en: Dashboard and Visualization tools
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪表板和可视化工具
- en: Of all the options, **Visualization** is core and not only an important way
    of distributing or communicating the results of Machine learning but also a good
    way of representing data in a way that helps in decision making. Very evidently
    data visualization is gaining traction in the field of big data and analytics.
    A visualization that best represents the data and the underlying patterns and
    the relationships is what is the key to decision making.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有选项中，**可视化**是核心，它不仅是一种重要的机器学习结果分发或沟通方式，而且是一种以有助于决策的方式表示数据的好方法。很明显，数据可视化正在大数据和数据分析领域获得关注。最能代表数据和其背后模式和关系的可视化是决策的关键。
- en: '![The Consumption layer](img/B03980_03_11.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![消费层](img/B03980_03_11.jpg)'
- en: There are two types of visualizations; one is those that explain the data, and
    second is those that explore data and the underlying patterns. Visualization is
    now being looked at as a new language to communicate.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的可视化；一种是为了解释数据，另一种是为了探索数据和其背后的模式。可视化现在被视为一种新的沟通语言。
- en: Explaining and exploring data with Visualizations
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用可视化解释和探索数据
- en: Visualizations for explaining and exploring data are unique and are used for
    different purposes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 用于解释和探索数据的可视化是独特的，用于不同的目的。
- en: The Visualizations for explaining are the typical ones we see in marketing and
    sales presentations. This is the case where data on the hand is clean to the maximum
    extent. The meaning of the data is clear, and communication is done by the final
    decision makers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 用于解释的可视化是我们通常在营销和销售演示中看到的典型可视化。在这种情况下，手头的数据尽可能干净。数据的意义是清晰的，沟通是通过最终决策者完成的。
- en: On the other hand, Visualizations for exploring help to correct data and link
    the related and useful attributes of data in the quest for understanding the data
    as such. Visualization for exploring sometimes can be inaccurate. The exploration
    usually happens iteratively, and there might be several rounds of refining the
    Visualizations before some sense is made out of the data on hand. There is a need
    to get rid of some irrelevant attributes in data or even the data itself (the
    one identified to be *noise*). This step of data exploration using Visualization
    sometimes replaces running complex algorithms and often requires statistical acumen.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，用于探索的可视化有助于纠正数据并连接数据的相关和有用属性，以理解数据本身。探索性的可视化有时可能不准确。探索通常以迭代方式进行，可能需要多次细化可视化，才能从现有数据中得出一些有意义的结论。有时需要去除数据中的某些无关属性，甚至整个数据（被识别为*噪声*）。使用可视化进行数据探索的这一步骤有时会取代运行复杂的算法，并且通常需要统计敏锐度。
- en: Some popular visualization tools in the market (both open source and commercial)
    are Highcharts JS, D3, Tableau, and others. Although we use some of these frameworks
    to demonstrate how to depict and communicate insights, we are not explicitly covering
    any of the visualization options in depth.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上一些流行的可视化工具（包括开源和商业）有Highcharts JS、D3、Tableau等。尽管我们使用其中一些框架来展示如何描绘和传达洞察力，但我们并没有深入探讨任何可视化选项。
- en: 'Another important aspect is that these visualization tools usually need to
    leverage, traditional data warehousing tools and big data analysis tools. The
    following figure depicts how the proposed Machine learning architecture can support
    having existing data warehouses or BI tools coexist with big data analysis tools.
    As explained in [Chapter 1](ch01.html "Chapter 1. Introduction to Machine learning"),
    *Introduction to Machine learning*, the aggregated data and the data lakes become
    the core input to any big data analysis tools that run the Machine learning tools.
    The new age data storage mantra is semantic data structures. More on semantic
    data architectures are covered as a part of emerging data architectures in [Chapter
    14](ch14.html "Chapter 14. New generation data architectures for Machine learning"),
    *New generation data architectures for Machine learning*. The following figure
    depicts a high-level view of visualization in the context of data lakes and data
    warehouses:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是，这些可视化工具通常需要利用传统的数据仓库工具和大数据分析工具。以下图展示了所提出的机器学习架构如何支持现有数据仓库或BI工具与大数据分析工具共存。正如在[第1章](ch01.html
    "第1章。机器学习简介")中所述，“机器学习简介”，聚合数据和数据湖成为运行机器学习工具的任何大数据分析工具的核心输入。新时代数据存储的箴言是语义数据结构。关于语义数据架构的更多内容将在[第14章](ch14.html
    "第14章。机器学习的新一代数据架构")中介绍，“机器学习的新一代数据架构”。以下图展示了数据湖和数据仓库背景下的可视化高级视图：
- en: '![Explaining and exploring data with Visualizations](img/B03980_03_12.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![使用可视化解释和探索数据](img/B03980_03_12.jpg)'
- en: Security and Monitoring layer
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全和监控层。
- en: When large volumes of data are being processed and consolidated across a variety
    of sources, security becomes of utmost importance and, in the case of sensitive
    data, the need for protecting data privacy is critical and sometimes the key compliance
    requirement too. The required authentication and authorization checks need to
    be implemented as a part of executing Machine learning algorithms. This is more
    of a prerequisite and cannot be an afterthought in the Machine learning architecture.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当在多个来源上处理和整合大量数据时，安全性变得极其重要，在敏感数据的情况下，保护数据隐私的需求是关键，有时也是关键合规要求。所需的身份验证和授权检查需要作为执行机器学习算法的一部分来实施。这更多的是一个先决条件，不能作为机器学习架构中的事后考虑。
- en: Data Ingestion and the processing function are the main areas that require strict
    security implementation, given the criticality of controlling data access.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取和处理功能是控制数据访问的关键区域，因此需要严格的安全实施。
- en: By the virtue of distributed architecture, big data applications are inherently
    prone to security vulnerabilities; it is necessary that security implementation
    is taken care of, and it does not impact performance, scalability, or functionality
    with the ease of execution and maintenance of these applications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分布式架构，大数据应用程序天生容易受到安全漏洞的影响；有必要确保安全实施得到妥善处理，并且不会影响这些应用程序的执行、可扩展性或功能。
- en: 'The Machine learning architecture as such should support the following as a
    basic necessity for security:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习架构本身应该支持以下作为安全性的基本需求：
- en: Authentication for each node in the cluster with the support for standard protocols
    like Kerberos
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对集群中的每个节点进行身份验证，支持如Kerberos等标准协议。
- en: Since it is a file system, there needs to be a minimum support for encryption
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它是一个文件系统，因此需要提供最低限度的加密支持。
- en: Communication with the nodes should always use **SSL** (**Secure Socket Layer**),
    TLS, or others that include NameNode
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与节点通信应始终使用**SSL**（**安全套接字层**）、TLS或其他包含NameNode的协议。
- en: Secure keys and tokens and usage of standard key management systems
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全密钥和令牌以及使用标准密钥管理系统。
- en: The implementation of distributed logging for tracking to trace any issues across
    layers easily
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现分布式日志记录以轻松跟踪跨层问题。
- en: The next significant requirement is monitoring. The distributed data architecture
    comes with robust monitoring and support tools that can handle large clusters
    of nodes that are connected in a federated model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要的需求是监控。分布式数据架构配备了强大的监控和支持工具，可以处理在联邦模型中连接的大量节点集群。
- en: There are always SLAs for the downtime of an application, and it is important
    that the recovery mechanism adheres to these SLAs while ensuring the availability
    of the application.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用程序的停机时间，总是有服务级别协议（SLA），并且恢复机制必须遵守这些SLA，同时确保应用程序的可用性。
- en: It is important that these nodes and clusters communicate with the monitoring
    system in a machine independent way, and the usage of XML-like formats is key.
    The data storage needs for the monitoring systems should not impact the overall
    performance of the application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些节点和集群以机器无关的方式与监控系统通信是很重要的，并且使用类似XML的格式是关键。监控系统的数据存储需求不应影响应用程序的整体性能。
- en: Usually, every big data stack comes with an in-built monitoring framework or
    tool. Also, there are open source tools such as Ganglia and Nagios that can be
    integrated and used for monitoring the big data applications.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个大数据栈都自带内置的监控框架或工具。此外，还有如Ganglia和Nagios等开源工具可以集成并用于监控大数据应用程序。
- en: Hadoop core components framework
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hadoop核心组件框架
- en: 'Apache Hadoop has two core components:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop有两个核心组件：
- en: Hadoop Distributed File System also called HDFS
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统也称为HDFS
- en: MapReduce (in the version 2.x of Hadoop, this is called YARN)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce（在Hadoop 2.x版本中称为YARN）
- en: The rest of the Hadoop components are represented in the Machine learning solution
    architecture. Using Hadoop we work around these two core components and form the
    eco-system components for Hadoop.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的其他组件在机器学习解决方案架构中得到了表示。使用Hadoop，我们围绕这两个核心组件工作，形成了Hadoop的生态系统组件。
- en: The focus of this chapter is Apache Hadoop 2.x distribution. There have been
    few architectural changes to HDFS and MapReduce in this version. We first cover
    the core architecture, and then the changes that have come in as a part of the
    2.x architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是Apache Hadoop 2.x版本。在这个版本中，HDFS和MapReduce（在Hadoop 2.x版本中称为YARN）的架构发生了一些变化。我们首先介绍核心架构，然后介绍2.x架构中引入的变化。
- en: Hadoop Distributed File System (HDFS)
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统 (HDFS)
- en: '**HDFS** is inspired and built from **GFS** (**Google File System**). It is
    a distributed file system that is elastically scalable, with support load balancing
    and fault tolerance to ensure high availability. It has data redundancy built
    in to demonstrate reliability and consistency in data.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**HDFS**（Hadoop分布式文件系统）是从**GFS**（Google文件系统）中启发并构建的。它是一个弹性可扩展的分布式文件系统，支持负载均衡和容错性，以确保高可用性。它内置了数据冗余，以展示数据的可靠性和一致性。'
- en: '![Hadoop Distributed File System (HDFS)](img/B03980_03_13.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop分布式文件系统 (HDFS)](img/B03980_03_13.jpg)'
- en: HDFS implements the Master-slave architecture. Here, the master node is called
    NameNode, and the slave nodes are called DataNodes. NameNode is the entry point
    for all client applications, and the distribution of data across the DataNodes
    happens via the NameNode. The actual data is not passed through NameNode server
    to ensure that NameNode does not become a bottleneck for any data distribution.
    Only the metadata is communicated to the client, and the actual data movement
    happens directly between the clients and DataNodes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS实现了主从架构。在这里，主节点被称为NameNode，从节点被称为DataNodes。NameNode是所有客户端应用程序的入口点，数据的分布是通过NameNode在DataNodes之间进行的。实际数据不会通过NameNode服务器传递，以确保NameNode不会成为任何数据分布的瓶颈。只有元数据会与客户端通信，实际的数据移动是直接在客户端和DataNodes之间进行的。
- en: 'Both NameNode and DataNode are referred to as daemons in the Hadoop architecture.
    NameNode requires a high-end machine and is expected to run only the NameNode
    daemon. The following points justify the need for a high-end machine for NameNode:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop架构中，NameNode和DataNode都被称作守护进程。NameNode需要高端机器，并且预期只运行NameNode守护进程。以下要点说明了为什么NameNode需要高端机器：
- en: The entire cluster's metadata is held in the memory for quicker access, and
    there is a need for more memory
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个集群的元数据都存储在内存中，以便快速访问，并且需要更多的内存
- en: NameNode is both the single point of entry and failure for the Hadoop cluster
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NameNode是Hadoop集群的单点入口和故障点
- en: The NameNode coordinates with several hundreds or thousands of DataNodes and
    manages batch jobs
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NameNode与数百或数千个DataNode协调，并管理批处理作业
- en: HDFS is built on the traditional hierarchical file system where the creation
    of new directories, adding new files, deletion directories or subdirectories,
    removal of files, renaming, and moving or updating a file are common tasks. Details
    of the directories, files, data nodes, and blocks created and stored in each of
    the DataNodes are stored as metadata in the NameNode.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS建立在传统的分层文件系统之上，其中创建新目录、添加新文件、删除目录或子目录、删除文件、重命名、移动或更新文件是常见任务。在各个DataNode上创建和存储的目录、文件、数据节点和块详情作为元数据存储在NameNode中。
- en: There is another node in this architecture that NameNode communicates with,
    called secondary Namenode. The secondary Namenode is not a backup for NameNode
    and hence, does not failover to the secondary Namenode. Instead, it is used to
    store a copy of the metadata and log files from NameNode. NameNode holds the metadata
    for the data blocks and related distribution details in a file called `fsimage`.
    This image file is not updated for every data operation in the file system and
    is tracked periodically by logging them in separate log files. This ensures faster
    I/O and thus the efficiency of the data import or export operations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，NameNode还会与另一个节点进行通信，这个节点被称为二级NameNode。二级NameNode不是NameNode的备份，因此不会切换到二级NameNode。相反，它用于存储从NameNode复制来的元数据和日志文件。NameNode将数据块的元数据和相关分布细节存储在一个名为`fsimage`的文件中。这个镜像文件不会在文件系统的每个数据操作后更新，而是通过在单独的日志文件中记录它们来定期跟踪。这确保了更快的I/O，从而提高了数据导入或导出操作的效率。
- en: 'The secondary Namenode has a specific function with this regard. It periodically
    downloads the image and log files, and creates a new image by appending the current
    operations from the log file into the fsimage, then uploading the new image file
    back to NameNode. This eliminates any overhead on NameNode. Any restart on NameNode
    happens very quickly, and the efficiency of the system is ensured. The following
    figure depicts the communication workflow between the client application and HDFS:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，二级NameNode具有特定的功能。它定期下载镜像和日志文件，通过将日志文件中的当前操作追加到fsimage中创建一个新的镜像，然后将新的镜像文件上传回NameNode。这消除了NameNode上的任何开销。NameNode的任何重启都非常快，从而确保了系统的效率。以下图展示了客户端应用程序和HDFS之间的通信工作流程：
- en: '![Hadoop Distributed File System (HDFS)](img/B03980_03_14.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Hadoop分布式文件系统（HDFS）](img/B03980_03_14.jpg)'
- en: HDFS is built for reading and writing large volumes of data between DataNodes.
    These large files are split into blocks of smaller files, usually of a fixed size
    such as 64 MB or 128 MB, and these blocks are distributed across DataNodes. For
    each of these blocks, overall three copies are stored to ensure redundancy and
    support fault tolerance. The number of copies can be changed, which is a configuration
    of the system. More information on the HDFS architecture and specific functions
    is covered in the following section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS是为了在DataNode之间读取和写入大量数据而构建的。这些大文件被分割成更小的文件块，通常大小固定，如64 MB或128 MB，这些块被分布到DataNode上。对于这些块中的每一个，总共存储三个副本以确保冗余和支持容错。副本的数量可以更改，这是系统的配置之一。关于HDFS架构和特定功能的更多信息将在下一节中介绍。
- en: Secondary Namenode and Checkpoint process
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二级NameNode和检查点过程
- en: 'While defining the purpose and function of the secondary Namenode, we have
    learned one important function that takes care of updating or preparing the metadata
    for NameNode that is stored in a file called `fsimage`. This process of generating
    a new fsimage by merging the existing fsimage and the log file is called **Checkpoint**.
    The following figure depicts the checkpoint process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义二级NameNode的目的和功能时，我们了解了一个重要的功能，它负责更新或准备存储在名为`fsimage`的文件中的NameNode元数据。通过合并现有的fsimage和日志文件来生成新的fsimage的过程称为**检查点**。以下图展示了检查点过程：
- en: '![Secondary Namenode and Checkpoint process](img/B03980_03_15.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![二级NameNode和检查点过程](img/B03980_03_15.jpg)'
- en: Some configurations changes are to be done to the `cross-site.XML` file related
    to checkpoint process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对与检查点过程相关的`cross-site.XML`文件进行一些配置更改。
- en: '| Property | Purpose |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 目的 |'
- en: '| --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `dfs.namenode.checkpoint.dir` | This is the directory path where the temporary
    fsimage files are held to run the merge process. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.dir` | 这是临时fsimage文件存放以运行合并过程的目录路径。|'
- en: '| `dfs.namenode.checkpoint.edits.dir` | This is the directory path where the
    temporary edits are held to run the merge process. The default value for this
    parameter is same as `dfs.namenode.checkpoint.dir` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.edits.dir` | 这是临时编辑运行合并过程所持有的目录路径。此参数的默认值与 `dfs.namenode.checkpoint.dir`
    相同 |'
- en: '| `dfs.namenode.checkpoint.period` | The time gap between two checkpoint runs
    (in seconds). |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.period` | 两次checkpoint运行之间的时间间隔（以秒为单位）。 |'
- en: '| `dfs.namenode.checkpoint.txns` | Irrespective of the time gap configurations,
    this property defines after how many transactions a checkpoint process needs to
    be triggered. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.txns` | 不论时间间隔配置如何，此属性定义了在触发checkpoint过程之前需要经过多少事务。
    |'
- en: '| `dfs.namenode.checkpoint.check.period` | This property defines the frequency
    (in seconds) in which the NameNode is polled to check the un-checkpointed transactions.
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.check.period` | 此属性定义了NameNode被轮询以检查未checkpoint的事务的频率（以秒为单位）。
    |'
- en: '| `dfs.namenode.checkpoint.max-retries` | In the case of failure, the secondary
    Namenode retry is checkpointing. This property defines the number of times a secondary
    Namenode attempts a retry for checkpointing before it gives up. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.checkpoint.max-retries` | 在失败的情况下，辅助NameNode会重试checkpoint。此属性定义了辅助NameNode在放弃之前尝试重试checkpoint的次数。
    |'
- en: '| `dfs.namenode.num.checkpoints.retained` | This property represents the number
    of checkpoint files retained by both the NameNode and the secondary Namenode.
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.num.checkpoints.retained` | 此属性表示NameNode和辅助NameNode保留的checkpoint文件数量。
    |'
- en: The checkpoint process can be triggered by both NameNode and the secondary Namenode.
    Secondary Namenode is also responsible for taking backup of the `fsimage` files
    periodically, which will further help in recovery.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: checkpoint过程可以由NameNode和辅助NameNode触发。辅助NameNode还负责定期备份`fsimage`文件，这有助于进一步恢复。
- en: Splitting large data files
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分割大数据文件
- en: HDFS stores smaller chunks of huge files across the data nodes distributed over
    the cluster. Before the files are stored, HDFS internally splits the entire file
    content into multiple data blocks each of a fixed size (default 64 MB). This size
    is configurable. There is no specific business logic followed to split the files
    and build the data blocks; it is purely driven by the file size. These data blocks
    are then stored on the DataNodes for the data read and write to happen in parallel.
    Each data block is again a file in itself in the local file system.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS将大量文件的小块存储在集群中分布的数据节点上。在文件存储之前，HDFS内部将整个文件内容分割成多个固定大小的数据块（默认为64 MB）。此大小是可以配置的。分割文件和构建数据块没有遵循特定的业务逻辑；它纯粹是由文件大小驱动的。然后，这些数据块存储在DataNodes上，以便数据读写可以并行进行。每个数据块在本地文件系统中本身也是一个文件。
- en: 'The following figure depicts how a large file is split into smaller chunks
    or blocks of fixed size:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何将大文件分割成更小的固定大小块：
- en: '![Splitting large data files](img/B03980_03_16.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![分割大数据文件](img/B03980_03_16.jpg)'
- en: 'The size of each block can be controlled by the following configuration parameter
    in `hdfs-site.xml`. The cluster-wide block size is controlled by the `dfs.blocksize
    configuration` property in `hdfs-site.XML` The default value in Hadoop 1.0 is
    64 MB and in Hadoop 2.x is 128 MB. The block size is determined by the effectiveness
    of the infrastructure and can get bigger with higher transfer speeds and the usage
    of the new age drives:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据块的大小可以通过以下配置参数在 `hdfs-site.xml` 中进行控制。集群范围内的数据块大小由 `hdfs-site.XML` 中的 `dfs.blocksize`
    配置属性控制。Hadoop 1.0中的默认值为64 MB，而在Hadoop 2.x中为128 MB。数据块的大小取决于基础设施的有效性，并且随着传输速度的提高和新一代驱动器的使用而增大：
- en: '| Property | Purpose |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 目的 |'
- en: '| --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `dfs.blocksize` | The value is 134217728.The previous value in bytes represents
    128 MB, alternatively any value suffixed by a measure can be defined. For example,
    512m, 1g, 128k, and so on. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.blocksize` | 该值为134217728。之前的字节数值代表128 MB，或者可以定义任何带有度量单位的值。例如，512m，1g，128k，等等。
    |'
- en: Any update to the value in the block size will not be applied to the existing
    blocks; only new blocks are eligible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对块大小值的任何更新都不会应用于现有块；只有新块才有资格。
- en: Block loading to the cluster and replication
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 块加载到集群和复制
- en: Once the file is split, the data blocks are formed of a fixed block size and
    are configured for the environment.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 文件分割后，数据块由固定大小的块组成，并针对环境进行配置。
- en: 'By virtue of the distributed architecture, there is a strong need to store
    replicas of the data blocks to handle data reliability. By default, three copies
    of each data block are stored. The number of the copies configuration property
    is called replication factor. The following table lists all the configurations
    related to data loading and replication:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分布式架构，存储数据块的副本以处理数据可靠性有很强的需求。默认情况下，每个数据块存储三个副本。副本数量的配置属性称为复制因子。以下表格列出了所有与数据加载和复制相关的配置：
- en: '| Property | Purpose |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 目的 |'
- en: '| --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `dfs.replication` | The value is 3.This defines the number of replicas that
    need to be stored in each block. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.replication` | 该值为3。这定义了每个块需要存储的副本数量。|'
- en: '| `dfs.replication.max` | Maximal block replication. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.replication.max` | 最大块复制。|'
- en: '| `dfs.namenode.replication.min` | Minimal block replication. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `dfs.namenode.replication.min` | 最小块复制。|'
- en: The NameNode is responsible for ensuring the block placement and replication
    as per the configuration is done. With these data blocks placed onto DataNodes,
    each DataNode in the cluster sends block status periodically to the NameNode.
    The fact that NameNode receives a signal from the DataNode implies that the DataNode
    is active and functioning properly.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: NameNode负责确保按照配置执行块放置和复制。将这些数据块放置到DataNodes后，集群中的每个DataNode定期向NameNode发送块状态。NameNode从数据节点接收信号的事实表明，数据节点是活跃的并且运行正常。
- en: 'HDFS uses a **default block placement policy** that is targeted to achieve
    load balancing across the available nodes. Following is the scope of this policy:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS使用一个**默认的块放置策略**，旨在在可用节点之间实现负载均衡。以下是该策略的范围：
- en: First, the copy or replica is written to the DataNode that is creating the file;
    this facilitates a higher write performance
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，副本或复制件被写入创建文件的DataNode；这有助于提高写入性能
- en: Second, the copy or replica is written to another DataNode from the same rack;
    this minimizes network traffic
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，副本或复制件被写入同一机架中的另一个DataNode；这样可以最小化网络流量
- en: Third, the replica is written to a DataNode in a different rack; this way even
    if a switch fails, there still is a copy of the data block available
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，副本被写入不同机架中的DataNode；这样即使交换机出现故障，仍然有数据块的副本可用
- en: A default block placement policy is applied that uses all the nodes on the rack
    without compromising on the performance, data reliability, and availability. The
    following image depicts how three blocks of data are placed across four nodes
    with a replication strategy of two extra copies. Some of these nodes are located
    in the racks for optimal fault tolerance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 应用一个默认的块放置策略，该策略使用机架上的所有节点，而不影响性能、数据可靠性和可用性。以下图像展示了如何使用两个额外副本的复制策略，将三个数据块放置在四个节点上。其中一些节点位于机架上，以实现最佳容错性。
- en: '![Block loading to the cluster and replication](img/B03980_03_17.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![块加载到集群和复制](img/B03980_03_17.jpg)'
- en: 'Overall, the flow of loading data into HDFS is shown in the following flow
    diagram:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，将数据加载到HDFS的流程如下所示：
- en: '![Block loading to the cluster and replication](img/B03980_03_18.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![块加载到集群和复制](img/B03980_03_18.jpg)'
- en: Writing to and reading from HDFS
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入和读取HDFS
- en: 'While writing a file to HDFS, the client first contacts NameNode and passes
    the details of the file that needs to be written to HDFS. NameNode provides details
    on the replication configurations and other metadata details that specify where
    to place the data blocks. The following figure explains this flow:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在将文件写入HDFS时，客户端首先联系NameNode，并传递需要写入HDFS的文件详情。NameNode提供有关复制配置和其他元数据详情，这些详情指定了放置数据块的位置。以下图解说明了这一流程：
- en: '![Writing to and reading from HDFS](img/B03980_03_19.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![写入和读取HDFS](img/B03980_03_19.jpg)'
- en: Handling failures
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理故障
- en: When the Hadoop cluster starts up, the NameNode gets into a safe-mode state
    and receives a heartbeat signal from all the data nodes. The fact that the NameNode
    receives a block report from DataNodes indicates that the DataNodes are up and
    functioning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当Hadoop集群启动时，NameNode进入安全模式状态，并从所有数据节点接收心跳信号。NameNode从数据节点接收块报告的事实表明，数据节点正在运行并正常工作。
- en: 'Let''s now say that **Data Node 4** goes down; this would mean that **Name
    Node** does not receive any heartbeat signals from **Data Node 4**. **Name Node**
    registers the unavailability of **Name Node** and hence, whatever **Data Node
    4** does is load balanced to the other nodes that have the replicas. This data
    is then updated in the metadata register by **Name Node**. The following figure
    illustrates the same:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设**数据节点4**出现故障；这意味着**名称节点**没有从**数据节点4**接收到任何心跳信号。**名称节点**记录了**名称节点**的不可用状态，因此，**数据节点4**所做的任何操作都会被负载均衡到其他拥有副本的节点。然后，**名称节点**会将这些数据更新到元数据注册表中。以下图示展示了相同的内容：
- en: '![Handling failures](img/B03980_03_20.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![处理故障](img/B03980_03_20.jpg)'
- en: HDFS command line
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HDFS命令行
- en: 'HDFS has a command line interface called **FS Shell**. This facilitates the
    usage of shell commands to manage HDFS. The following screenshot shows the `Hadoop
    fs` command, and its usage/syntax:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS有一个名为**FS Shell**的命令行界面。这便于使用shell命令来管理HDFS。以下截图显示了`Hadoop fs`命令及其用法/语法：
- en: '![HDFS command line](img/B03980_03_21.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![HDFS命令行](img/B03980_03_21.jpg)'
- en: RESTFul HDFS
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RESTful HDFS
- en: To have external applications, especially web applications or similar applications,
    have easy access to the data in HDFS over HTTP. HDFS supports an additional protocol
    called WebHDFS that is based on the RESTful standards that facilitate giving access
    to HDFS data over HTTP, without any need for Java binding or the availability
    of a complete Hadoop environment. Clients can use common tools such as curl/wget
    to access the HDFS. While providing web services-based access to data stored in
    HDFS, WebHDFS the built-in security and parallel processing capabilities of the
    platform, are well retained.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让外部应用程序，尤其是Web应用程序或类似的应用程序，能够通过HTTP轻松访问HDFS中的数据。HDFS支持一个名为WebHDFS的附加协议，该协议基于RESTful标准，便于通过HTTP访问HDFS数据，无需Java绑定或完整的Hadoop环境。客户端可以使用curl/wget等常用工具访问HDFS。在提供基于Web服务的HDFS数据访问时，WebHDFS保留了平台内置的安全性和并行处理能力。
- en: 'To enable WebHDFS, make the following configuration changes in `hdfs-site.xml`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用WebHDFS，请在`hdfs-site.xml`中进行以下配置更改：
- en: '[PRE0]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: More details on WebHDFS REST API can be found at [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于WebHDFS REST API的详细信息可以在[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html)找到。
- en: MapReduce
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce
- en: MapReduce is similar to HDFS. The Hadoop MapReduce framework is inspired and
    built on Google's MapReduce framework. It is a distributed computing framework
    that facilitates processing gigantic amounts of data in parallel across clusters
    and has built-in fault tolerance mechanisms. It works on operating and processing
    the local data paradigm, where the processing logic is moved to the data instead
    of data moved to the processing logic.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce与HDFS类似。Hadoop MapReduce框架受Google的MapReduce框架的启发和构建。它是一个分布式计算框架，便于在集群中并行处理大量数据，并具有内置的容错机制。它基于本地数据和数据处理范式，其中处理逻辑被移动到数据，而不是数据移动到处理逻辑。
- en: '![MapReduce](img/B03980_03_22.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce](img/B03980_03_22.jpg)'
- en: MapReduce architecture
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MapReduce架构
- en: 'The MapReduce framework is also based on Master-slave architecture. The master
    job is called JobTracker, and the slave jobs are called TaskTrackers. Unlike NameNode
    and DataNodes, these are not physical nodes, but are daemon processors that are
    responsible for running the processing logic across the DataNodes:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce框架也是基于主从架构的。主作业称为作业跟踪器（JobTracker），从作业称为任务跟踪器（TaskTrackers）。与名称节点和数据节点不同，这些不是物理节点，而是负责在数据节点上运行处理逻辑的守护进程处理器：
- en: '**JobTracker**: JobTracker schedules the execution of a job that comprises
    of multiple tasks. It is responsible for running the tasks or jobs on the task
    trackers and in parallel, monitors the status of processing. In the case of any
    failures, it is responsible for rerunning the failed tasks on the task tracker.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业跟踪器**：作业跟踪器安排多个任务的作业执行。它负责在任务跟踪器上运行任务或作业，并并行监控处理状态。在出现任何故障的情况下，它负责在任务跟踪器上重新运行失败的任务。'
- en: '**TaskTracker**: TaskTracker executes the tasks scheduled by the JobTracker
    and constantly communicates with JobTracker, working in cohesion.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务跟踪器**：任务跟踪器执行作业跟踪器安排的任务，并持续与作业跟踪器通信，协同工作。'
- en: Now, let's draw the analogy between the Master-slave architecture on the HDFS
    and MapReduce. The NameNode runs the JobTracker and DataNodes run TaskTrackers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将HDFS上的主从架构与MapReduce进行类比。NameNode运行JobTracker，DataNodes运行TaskTrackers。
- en: In a typical multi-node cluster, the NameNode and DataNodes are separate physical
    nodes, but in the case of a single node cluster, where the NameNode and DataNode
    are infrastructure wise the same, JobTracker and TaskTracker functions run on
    the same node. Single node clusters are used in the development environment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个典型的多节点集群中，NameNode和DataNodes是独立的物理节点，但在单节点集群的情况下，如果NameNode和DataNode在基础设施上是相同的，JobTracker和TaskTracker功能将在同一节点上运行。单节点集群用于开发环境。
- en: '![MapReduce architecture](img/B03980_03_23.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce架构](img/B03980_03_23.jpg)'
- en: There are two functions in a MapReduce process—`Map` and `Reduce`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce过程中有两个函数——`Map`和`Reduce`。
- en: '**Mapper**: Mapper job splits the file into multiple chunks in parallel, and
    runs some basic functions such as sorting, filtering, and any other specific business
    or analytics functions as needed. The output of the Mapper function is input to
    the Reducer function.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper**：Mapper作业将文件并行分割成多个块，并运行一些基本函数，如排序、过滤以及根据需要执行的其他特定业务或分析函数。Mapper函数的输出是Reducer函数的输入。'
- en: '**Reducer**: Reducer job is used to consolidate the results across Mappers,
    and is additionally used to perform any business or analytics function as needed.
    The intermediate output from the Mapper and Reducer jobs are stored on the file
    system as key-value pairs. Both the input and output of the map and reduce jobs
    are stored in HDFS. Overall, the MapReduce framework takes care of scheduling
    the tasks, monitoring the status, and handling failures (if any). The following
    diagram depicts how the `Map` and the `Reduce` functions work and operate on the
    data held in HDFS:![MapReduce architecture](img/B03980_03_24.jpg)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reducer**：Reducer作业用于合并Mapper的结果，并且可以根据需要执行任何业务或分析功能。Mapper和Reducer作业的中间输出存储在文件系统中作为键值对。Map和Reduce作业的输入和输出都存储在HDFS中。总的来说，MapReduce框架负责调度任务、监控状态和处理（如果有）故障。以下图表描述了`Map`和`Reduce`函数如何工作以及它们在HDFS中操作数据：![MapReduce架构](img/B03980_03_24.jpg)'
- en: What makes MapReduce cater to the needs of large datasets?
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是什么使得MapReduce能够满足大数据集的需求？
- en: 'Some of the advantages of MapReduce programming framework are listed as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce编程框架的一些优点如下列所示：
- en: '**Parallel execution**: MapReduce programs are, by default, meant to be executed
    in parallel that can be executed on a cluster of nodes. Development teams need
    not focus on the internals of distributed computing and can just use the framework
    directly.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行执行**：MapReduce程序默认是设计为并行执行的，可以在节点集群上执行。开发团队不需要关注分布式计算的内部细节，可以直接使用该框架。'
- en: '**Fault Tolerance**: MapReduce framework works on Master-slave Architecture
    where, in case any node goes down, corrective actions are taken automatically
    by the framework.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：MapReduce框架在主从架构上工作，如果在任何节点发生故障，框架会自动采取纠正措施。'
- en: '**Scalability**: MapReduce framework, having the ability to work distributed
    and with the ability to scale-out (horizontal scalability), with growing volumes
    new nodes, can be added to the cluster whenever needed.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：MapReduce框架具有分布式工作的能力，并且具有向外扩展（横向可伸缩性）的能力，随着数据量的增长，可以在需要时向集群添加新的节点。'
- en: '**Data Locality**: One of the core premises that the MapReduce framework does
    is to take the program to the data as opposed to the traditional way of bringing
    data to the code. So to be precise, MapReduce always has local data to it, and
    this is one of the most important reasons for the performance.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据局部性**：MapReduce框架的一个核心前提是将程序带到数据，而不是传统的将数据带到代码的方式。所以，更精确地说，MapReduce总是有本地数据，这是性能最重要的原因之一。'
- en: MapReduce execution flow and components
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MapReduce执行流程和组件
- en: 'In this section, we will a take a deep dive into the execution flow of MapReduce
    and how each of the components function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨MapReduce的执行流程以及每个组件如何工作：
- en: A new job is submitted by the client to JobTracker (a MapReduce job) along with
    the input and the output file paths and required configurations. The job gets
    queued for execution and gets picked by the job scheduler.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端通过JobTracker（一个MapReduce作业）提交一个新作业，包括输入和输出文件路径以及所需的配置。作业被排队等待执行，然后由作业调度器选取。
- en: JobTracker gets the data at the place where the required data in context resides,
    and creates an execution plan that triggers TaskTrackers for the execution.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JobTracker 在数据所在的位置获取数据，并创建一个执行计划，触发 TaskTrackers 进行执行。
- en: JobTracker submits the job to the identified TaskTrackers.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JobTracker 将作业提交给已识别的 TaskTrackers。
- en: TaskTrackers execute the task using the data that is local to them. If the data
    is not available on the local Data Node, it communicates with other DataNodes.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TaskTrackers 使用它们本地的数据执行任务。如果数据不在本地 Data Node 上，它会与其他 DataNodes 通信。
- en: TaskTrackers reports the status back to JobTracker by the means of heartbeat
    signals. JobTracker is capable of handling any failure cases inherently.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TaskTrackers 通过心跳信号将状态报告给 JobTracker。JobTracker 能够处理任何固有的故障情况。
- en: Finally, JobTracker reports the output to the Job client on the completion of
    the job.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，JobTracker 在作业完成后向 Job 客户端报告输出。
- en: 'The steps just described are depicted in the following figure. There are two
    parts to the flow: the HDFS and the MapReduce with the Nodes and the Trackers
    respectively.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的步骤在以下图中表示。流程分为两部分：HDFS 和 MapReduce，分别对应节点和追踪器。
- en: '![MapReduce execution flow and components](img/B03980_03_25.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce 执行流程和组件](img/B03980_03_25.jpg)'
- en: Let us focus on some core components of the MapReduce program and learn how
    to code it up. The following flow diagram details how the flow starts from the
    input data to the output data, and how each component or function of MapReduce
    framework kicks in to execute. The blocks in dotted red boxes are the components,
    and the blue boxes represent data being transitioned through the process.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注 MapReduce 程序的一些核心组件，并学习如何编写代码。以下流程图详细说明了流程从输入数据到输出数据的过程，以及 MapReduce 框架的每个组件或函数如何启动以执行。虚线红色方块是组件，蓝色方块代表通过过程传递的数据。
- en: '![MapReduce execution flow and components](img/B03980_03_26.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce 执行流程和组件](img/B03980_03_26.jpg)'
- en: Developing MapReduce components
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发 MapReduce 组件
- en: 'The MapReduce framework of Hadoop comprises a set of Java APIs that need to
    be extended or implemented to incorporate a specific function that is targeted
    to be executed in parallel over the Hadoop cluster. Following are some API implementations
    that need to be done:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 的 MapReduce 框架包含一组需要扩展或实现以包含特定功能的 Java API，这些功能旨在在 Hadoop 集群上并行执行。以下是需要完成的某些
    API 实现：
- en: Input and output data format interfaces
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出数据格式接口
- en: Mapper implementation
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mapper 实现
- en: Reducer implementation
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reducer 实现
- en: Partitioner
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Partitioner
- en: Combiner
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Combiner
- en: Driver
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driver
- en: Context
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Context
- en: InputFormat
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: InputFormat
- en: 'The `InputFormat` class is responsible for reading data from a file and making
    it available as input to the `map` function. Two core functions are performed
    by this process; one is splitting the input data into logical fragments called
    InputSplits, and the second is the reading of these splits as key value pairs
    to feed into the `map` function. There are two distinct interfaces to perform
    these two functions:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`InputFormat` 类负责从文件中读取数据并将其作为输入提供给 `map` 函数。这个过程执行了两个核心功能；一个是将输入数据分割成逻辑片段，称为
    InputSplits，另一个是将这些分割作为键值对读取，以供 `map` 函数使用。有两个不同的接口来执行这两个功能：'
- en: InputSplit
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InputSplit
- en: RecordReader
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RecordReader
- en: Splitting of the input file is not an essential function. In case we need to
    consider a complete file for processing, we will need to override the `isSplittable()`
    function and set the flag to `false`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文件的分割不是必需的功能。如果我们需要考虑整个文件进行处理，我们需要重写 `isSplittable()` 函数并将标志设置为 `false`。
- en: OutputFormat
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OutputFormat
- en: 'The `OutputFormat` API is responsible for validating that Hadoop has output
    data formats against output specification of the job. The RecordWriter implementation
    is responsible for writing the final output key value pairs to the file system.
    Every InputFormat API has a corresponding OutputFormat API. The following table
    lists some of the input and output format APIs of the MapReduce framework:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`OutputFormat` API 负责验证 Hadoop 是否有输出数据格式与作业的输出规范相匹配。RecordWriter 实现负责将最终的输出键值对写入文件系统。每个
    InputFormat API 都有一个对应的 OutputFormat API。以下表格列出了 MapReduce 框架的一些输入和输出格式 API：'
- en: '| Input Format API | Corresponding Output Format API |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 输入格式 API | 对应的输出格式 API |'
- en: '| --- | --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `TextInputFormat` | `TextOutputFormat` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `TextInputFormat` | `TextOutputFormat` |'
- en: '| `SequenceFileInputFormat` | `SequenceFileOutputFormat` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `SequenceFileInputFormat` | `SequenceFileOutputFormat` |'
- en: '| `DBInputFormat` | `DBOutputFormat` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `DBInputFormat` | `DBOutputFormat` |'
- en: Mapper implementation
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the Mapper implementations need to extend the `Mapper<KeyIn, ValueIn, KeyOut,
    ValueOut>` base class and importantly override the `map()` method to implement
    the specific business function. The Mapper implementation class takes key-value
    pairs as input and returns a set of key-value pairs as output. Any other interim
    output subsequently is taken by the shuffle and sort function.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: There is one Mapper instance for each InputSplit generated by the InputFormat
    for a given MapReduce job.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, there are four methods that the Mapper implementation class needs
    to extend from the base class. Following are the methods that are briefly described,
    along with the purpose of each method:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '| Method name and Syntax | Purpose |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| `setup(Context)` | This is the first method that is called back when a mapper
    is initiated for execution. It is not mandatory to override this method unless
    any specific initializations need to be done or any specific configuration setup
    needs to be done. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| `map(Object, Object, Context)` | Overriding this method is the key to mapper
    implementation as this method would be invoked as a part of executing the mapper
    logic. It takes key-value pairs as input, and the response can be a collection
    of key-value pairs |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| `clean (Context)` | This method is called at the end of the mapper function
    execution in the lifecycle and facilitates clearing any resources utilized by
    the mapper. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| `run (Context)` | Overriding this method provides additional capability to
    run multi-threaded mappers. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: Let's take an example from a given file; we want to find out how many times
    a word is repeated. In this case, `TextInputFormat` is used. In fact, this is
    the default InputFormat. The following diagram shows what the InputSplit function
    does. It splits every row and builds a key-value pair.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The diagram shows how the text is stored in multiple blocks on DataNode. `TextInputFormat`
    then reads these blocks and multiple InputSplits (we can see that there are two
    InputSplits, and hence there are two mappers). Each mapper picks an InputSplit
    and generates a key value pair for each occurrence of the word followed by the
    number 1.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapper implementation](img/B03980_03_27.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: The output of the mapper function is written onto the disk at the end of the
    processing, and none of the intermediate results are written to the file system.
    They are held in the memory. This helps in optimizing performance. It's possible
    because the key space is partitioned and each mapper only gets a fragment of the
    total dataset. Now, in terms of how much memory should be assigned for this purpose,
    by default, 100 MB is allocated and for any changes to this value, the `io.sort.mb`
    property will have to be set. There is usually a threshold set to this limit and,
    in case it exceeds this, there is a background process that starts writing onto
    the disk. The following program snippet demonstrates how to implement a mapper
    class.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Hadoop 2.x
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until Hadoop 2.x, all the distributions were focused on addressing the limitations
    in Hadoop 1.x but did not deviate from the core architecture. Hadoop 2.x really
    changed the underlying architecture assumptions and turned out to be a real breakthrough;
    most importantly, the introduction of YARN. YARN was a new framework for managing
    Hadoop cluster, which introduced the ability to handle real-time processing needs
    in addition to the batch. Some important issues that were addressed are listed
    as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Single NameNode issues
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dramatic increase in the number of nodes in the cluster
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extension to the number of tasks that can be successfully addressed with Hadoop
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts the difference between the Hadoop 1.x and 2.x
    architectures and how YARN wires MapReduce and HDFS:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop 2.x](img/B03980_03_28.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Hadoop ecosystem components
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hadoop has spawned a bunch of auxiliary and supporting frameworks. The following
    figure depicts the gamut of supporting frameworks contributed by the open source
    developer groups:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop ecosystem components](img/B03980_03_29.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: 'The following table lists all the frameworks and purposes of each framework.
    These frameworks work with the Apache distribution of Hadoop. There are many frameworks
    built by Vendors, who are commercially positioned and are not in the scope of
    this book:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | URL | Purpose (in brief) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| **HDFS** (**Hadoop Distributed File System**) | [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
    | Hadoop File storage system is a core component of Hadoop, which has a built-in
    fault tolerance (refer to HDFS section for more details on the architecture and
    implementation specifics). |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| MapReduce | [http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
    | MapReduce is a programming model and framework for processing large volumes
    of data on a distributed platform such as Hadoop. The latest version of Apache
    MapReduce extends another framework Apache YARN.**YARN**: MapReduce has gone through
    a complete overhaul in Hadoop 2.0 and now it is called MapReduce 2\. but the MapReduce
    programming model has not changed. YARN provides a new resource management and
    job scheduling model, along with its implementation to execute MapReduce jobs.
    In most cases, your existing MapReduce jobs run without any changes. In some instances,
    minor updates and recompilation might be needed. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Pig | [https://pig.apache.org/](https://pig.apache.org/) | Pig is a framework
    to execute data flows in parallel. It comes with a scripting language, Pig Latin,
    that helps in developing the data flows. Pig Latin comes with a bunch of internal
    operations for data such as join, split, sort, and so on. Pig runs on Hadoop and
    utilizes both HDFS and MapReduce. The compiled Pig Latin scripts run their functions
    in parallel and internally. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| Hive | [https://hive.apache.org/](https://hive.apache.org/) | Hive is a data
    warehouse framework for Hadoop. It supports querying and handling big datasets
    held in distributed stores. An SQL-like querying language called HiveQL can be
    used that allows plugging in the mapper and reducer programs. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| Flume | [http://flume.apache.org/](http://flume.apache.org/) | The Flume
    framework is more of an efficient transport framework that facilitates aggregating,
    analyzing, processing, and moving huge volumes of log data. It comes with an extensible
    data model and supports online analytics. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| Chukwa | [https://chukwa.apache.org/](https://chukwa.apache.org/) | The Chukwa
    framework comes with an API that helps in easily collecting, analyzing, and monitoring
    prominent collections of data. Chukwa runs at the top of the HDFS and MapReduce
    framework, thus inheriting Hadoop''s ability to scale. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| HBase | [http://hbase.apache.org/](http://hbase.apache.org/) | HBase is inspired
    from Google BigTable. It is a NoSQL, columnar data store built to complement the
    Hadoop platform, and supports real-time operations on the data. HBase is a Hadoop
    database that is responsible for backing MapReduce job outputs. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| HCatalog | [https://cwiki.apache.org/confluence/display/Hive/HCatalog](https://cwiki.apache.org/confluence/display/Hive/HCatalog)
    | HCatalog is like a relational view of the data in HDFS. It doesn''t matter where
    and how or what format the underlying data is stored. It is currently a part of
    Hive, and there are no separate distributions for the current distributions. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Avro | [http://avro.apache.org/](http://avro.apache.org/) | The Apache Avro
    framework is more of an interface to data. It supports modeling, serializing,
    and making **Remote Procedure Calls** (**RPC**). Every schema representation in
    Avro, also called the metadata definition, resides close to the data and on the
    same file, thus making the file self-describing. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| HIHO | [https://github.com/sonalgoyal/hiho/wiki/About-HIHO](https://github.com/sonalgoyal/hiho/wiki/About-HIHO)
    | HIHO stands for Hadoop-in Hadoop-out. This framework helps connecting multiple
    data stores with the Hadoop system and facilitate interoperability. HIHO supports
    several RDBMS and file systems, providing internal functions to load and off-load
    data between RDBMS and HDFS in parallel. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| Sqoop | [http://sqoop.apache.org/](http://sqoop.apache.org/) | Sqoop is a
    widely adopted framework for data transfer between HDFS and RDBMS in bulk or batch.
    It is very similar to Flume but operates with RDBMS. Sqoop is one of the **ETL**
    (**Extract-Transform-Load**) tools for Hadoop. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| Tajo | [http://tajo.apache.org/](http://tajo.apache.org/) | Tajo is a distributed
    data warehouse system for Apache Hadoop that is relational in nature. Tajo supports
    ad-hoc querying and online integration, and extract-transform-load functions on
    large datasets stored in HDFS or other data stores. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Oozie | [http://oozie.apache.org/](http://oozie.apache.org/) | Oozie is a
    framework that facilitates workflow management. It acts as a scheduler system
    for MapReduce jobs using **DAG** (**Direct Acyclical Graph**). Oozie can either
    be data aware or time aware while it schedules and executes jobs. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| ZooKeeper | [http://zookeeper.apache.org/](http://zookeeper.apache.org/)
    | Zookeeper, as the name says it all, is more like an orchestration and coordination
    service for Hadoop. It provides tools to build, manage, and provide high availability
    for distributed applications. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Ambari | [http://ambari.apache.org/](http://ambari.apache.org/) | Ambari
    is an intuitive web UI for Hadoop management with RESTful APIs. Apache Ambari
    was a contribution from Hortonworks. It serves as an interface to many other Hadoop
    frameworks in the ecosystem. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| Mahout | [http://mahout.apache.org/](http://mahout.apache.org/) | Apache
    Mahout is an open source library for Machine learning algorithms. The design focus
    for Mahout is to provide a scalable library for huge data sets distributed across
    multiple systems. Apache Mahout is a tool to derive useful information from raw
    data. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: Hadoop installation and setup
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different ways to setup Hadoop:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone operation**: In this operation, Hadoop runs in a non-distributed
    mode. All the daemons run within a single Java process and help in easy debugging.
    This setup is also called single node installation.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudo-Distributed Operation**: In this operation, Hadoop is configured to
    run on a single node, but in a pseudo-distributed mode that can run different
    daemon processes on different JVMs.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully-Distributed Operation**: In this operation, Hadoop is configured to
    run on multiple nodes in a fully-distributed mode, and all Hadoop daemons such
    as NameNode, Secondary Namenode, and JobTracker in the Master node; and DataNode
    and TaskTracker in slave nodes (in short, run on a cluster of nodes).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Ubuntu-based Hadoop Installation prerequisites are as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Java v1.7
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating dedicated Hadoop user
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring SSH access
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable IPv6
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Jdk 1.7
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download Java using this command:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Installing Jdk 1.7](img/B03980_03_30.jpg)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Unpack the binaries using this:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a directory to install Java with the help of the following command:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Copy the binaries into the newly created directory:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Configure the PATH parameters:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or else, use this command:'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Include the following content at the end of the file:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In Ubuntu, configure the path for Java:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Check for installation completion:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Installing Jdk 1.7](img/B03980_03_31.jpg)'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Creating a system user for Hadoop (dedicated)
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create/add a new group:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create/add a new user and attach it to the group:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Creating a system user for Hadoop (dedicated)](img/B03980_03_32.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Create/configure the SSH key access:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Verify the SSH setup:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Disable IPv6
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open `sysctl.conf` using the following command:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following lines at the end of the file. Reboot the machine to update
    the configurations correctly:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Steps for installing Hadoop 2.6.0
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download Hadoop 2.6.0 using this:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Unpack compressed Hadoop file using this:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Move hadoop-2.6.0 directory (a new directory):'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Move Hadoop to a local folder (for convenience) with this command:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Change the owner of the folder:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, update the configuration files.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are three site-specific configuration files and one environment setup
    configuration file to communicate with the Master node (NameNode) and slave nodes
    (DataNodes):'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`core-site.xml`'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hdfs-site.xml`'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapred-site.xml`'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yarn-site.xml`'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigate to the path that has the configuration files:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_33.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: yarn-site.xml
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Th `core-site.XML` file has the details of the Master node IP or the hostname,
    Hadoop temporary directory path, and so on.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_34.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: core-site.xml
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'The `hdfs-site.xml` file has the details of the following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Local file system path where NameNode stores namespace and transactions logs
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of local file system paths to store the blocks
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block size
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of replications
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_35.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: hdfs-site.xml
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mapred-site.xml` file has the details of the following:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: The host or IP and port where the JobTracker runs
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path on the HDFS where Map/Reduce stores the files
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of paths on the local file system to store the intermediate MapReduce
    data
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum limit of Map/Reduce tasks for every task tracker
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of DataNodes that need to be included or excluded
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of TaskTrackers that need to be included or excluded![Steps for installing
    Hadoop 2.6.0](img/B03980_03_38.jpg)
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mapred-site.xml
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Edit the `.bashrc` file as shown in the following screenshot:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![Steps for installing Hadoop 2.6.0](img/B03980_03_36.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: Starting Hadoop
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start the NameNode:'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To start the DataNode:'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To start ResourceManager, use the following command:'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Starting Hadoop](img/B03980_03_37.jpg)'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To start NodeManager:'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Check Hadoop Web interfaces:'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NameNode: `http://localhost:50070`'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Secondary Namenode: `http://localhost:50090`'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To stop Hadoop, use this:'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Hadoop distributions and vendors
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the Apache distribution for Hadoop being the open source and core version
    that the big data community is adopting, several vendors have their distributions
    of the open source adoption of Apache Hadoop. Some of them have purely added support
    while others have wrapped and extended the capabilities of Apache Hadoop and its
    ecosystem components. In many cases, they have their frameworks or libraries built
    over the core frameworks to add new functionality or features to the underlying
    core component.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: In this section, let us cover some of the distributions of Apache Hadoop and
    some differentiating data facts that help the development teams or organizations
    to take a decision about the distribution that works best for their requirements.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now consider the following vendors:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Cloudera
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hortonworks
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapR
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal / EMC
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Category | Function/Framework | Cloudera | Hortonworks | MapR | Pivotal |
    IBM |'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Performance and Scalability | Data Ingestion | Batch | Batch | Batch and
    Streaming | Batch and Streaming | Batch and Streaming |'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Metadata architecture | Centralized | Centralized | Distributed | Centralized
    | Centralized |'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | HBase performance | Spikes in latency | Spikes in latency | Low latency
    | Low latency | Spikes in latency |'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | NoSQL Support | Mainly batch applications | Mainly batch applications
    | Batch and online systems | Batch and online systems | Batch and online systems
    |'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Reliability | High Availability | Single failure recovery | Single failure
    recovery | Self-healing across multiple failures | Self-healing across multiple
    failures | Single failure recovery |'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Disaster Recovery | File copy | N/A | Mirroring | Mirroring | File copy
    |'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Replication | Data | Data | Data and metadata | Data and metadata | Data
    |'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Snapshots | Consistent with closed files | Consistent with closed files
    | Point in time consistency | Consistent with closed files | Consistent with closed
    files |'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Upgrading | Rolling upgrades | Planned | Rolling upgrades | Planned |
    Planned |'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Manageability** | Volume Support | No | No | Yes | Yes | Yes |'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Management Tools | Cloudera Manager | Ambari | MapR Control system |
    Proprietary console | Proprietary console |'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Integration with REST API | Yes | Yes | Yes | Yes | Yes |'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Job replacement control | No | No | Yes | Yes | No |'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Data Access & Processing** | File System | HDFS, Read-only NFS | HDFS,
    read-only NFS | HDFS, read/write NFS and POSIX | HDFS, read/write NFS | HDFS,
    read-only NFS |'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | File I/O | Append-only | Append-only | Read/write | Append-only | Append-only
    |'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Security ACLs | Yes | Yes | Yes | Yes | Yes |'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|   | Authentication | Kerberos | Kerberos | Kerberos and Native | Kerberos
    and Native | Kerberos and Native |'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered all about Hadoop, starting from core frameworks
    to ecosystem components. At the end of this chapter, readers should be able to
    set up Hadoop and run some MapReduce functions. Users should be able to run and
    manage a Hadoop environment and understand the command line usage using one or
    more ecosystem component.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, our focus is on the key Machine learning frameworks such
    as Mahout, Python, R, Spark, and Julia; these either have inherent support on
    the Hadoop platform, or need direct integration with the Hadoop platform for supporting
    large datasets.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
