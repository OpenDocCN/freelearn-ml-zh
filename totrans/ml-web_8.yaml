- en: Chapter 8. Sentiment Analyser Application for Movie Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we describe an application to determine the sentiment of movie
    reviews using algorithms and methods described throughout the book. In addition,
    the **Scrapy** library will be used to collect reviews from different websites
    through a search engine API (Bing search engine). The text and the title of the
    movie review is extracted using the newspaper library or following some pre-defined
    extraction rules of an HTML format page. The sentiment of each review is determined
    using a naive Bayes classifier on the most informative words (using the X *2*
    measure) in the same way as in [Chapter 4](text00032.html#ch04 "Chapter 4. Web
    Mining Techniques") , *Web Mining Techniques* . Also, the rank of each page related
    to each movie query is calculated for completeness using the PageRank algorithm
    discussed in [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* . This chapter will discuss the code used to build the
    application, including the Django models and views and the Scrapy scraper is used
    to collect data from the web pages of the movie reviews. We start by giving an
    example of what the web application will be and explaining the search engine API
    used and how we include it in the application. We then describe how we collect
    the movie reviews, integrating the Scrapy library into Django, the models to store
    the data, and the main commands to manage the application. All the code discussed
    in this chapter is available in the GitHub repository of the author inside the
    `chapter_8` folder at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Application usage overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The home web page is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application usage overview](img/Image00541.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The user can type in the movie name, if they want to know the review''s sentiments
    and relevance. For example, we look for *Batman vs Superman Dawn of Justice* in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application usage overview](img/Image00542.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The application collects and scrapes 18 reviews from the Bing search engine
    and, using the Scrapy library, it analyzes their sentiment (15 positive and 3
    negative). All data is stored in Django models, ready to be used to calculate
    the relevance of each page using the PageRank algorithm (the links at the bottom
    of the page as seen in the preceding screenshot). In this case, using the PageRank
    algorithm, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application usage overview](img/Image00543.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a list of the most relevant pages to our movie review search, setting
    a depth parameter 2 on the scraping crawler (refer the following section for further
    details). Note that to have a good result on page relevance, you have to crawl
    thousands of pages (the preceding screenshot shows results for around 50 crawled
    pages).
  prefs: []
  type: TYPE_NORMAL
- en: 'To write the application, we start the server as usual (see [Chapter 6](text00046.html#ch06
    "Chapter 6. Getting Started with Django") , *Getting Started with Django* , and
    [Chapter 7](text00050.html#page "Chapter 7. Movie Recommendation System Web Application")
    , *Movie Recommendation System Web Application* ) and the main app in Django.
    First, we create a folder to store all our codes, `movie_reviews_analyzer_app`
    , and then we initialize Django using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We set the settings in the `.py` file as we did in the *Settings* section of
    [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with Django") , *Getting
    Started with Django* , and the *Application Setup* section of [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* (of course, in this case the name is `webmining_server`
    instead of `server_movierecsys` ).
  prefs: []
  type: TYPE_NORMAL
- en: The sentiment analyzer application has the main views in the `.py` file in the
    main `webmining_server` folder instead of the `app` (pages) folder as we did previously
    (see [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with Django")
    , *Getting Started with Django* , and [Chapter 7](text00050.html#page "Chapter 7. Movie
    Recommendation System Web Application") , *Movie Recommendation System Web Application*
    ), because the functions now refer more to the general functioning of the server
    instead of the specific app (pages).
  prefs: []
  type: TYPE_NORMAL
- en: 'The last operation to make the web service operational is to create a `superuser`
    account and go live with the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that the structure of the application has been explained, we can discuss
    the different parts in more detail starting from the search engine API used to
    collect URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Search engine choice and the application code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since scraping directly from the most relevant search engines such as Google,
    Bing, Yahoo, and others is against their term of service, we need to take initial
    review pages from their REST API (using scraping services such as Crawlera, [http://crawlera.com/](http://www.crawlera.com/)
    , is also possible). We decided to use the Bing service, which allows 5,000 queries
    per month for free.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that, we register to the Microsoft Service to obtain the key
    needed to allow the search. Briefly, we followed these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Register online on [https://datamarket.azure.com](https://datamarket.azure.com)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **My Account** , take the **Primary Account Key** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Register a new application (under **DEVELOPERS** | **REGISTER** ; put **Redire**
    **ct URI** : `https://www.` `bing.com` )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, we can write a function that retrieves as many URLs relevant to
    our query as we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `API_KEY` parameter is taken from the Microsoft account, `query` is a string
    which specifies the movie name, and `num_reviews = 30` is the number of URLs returned
    in total from the Bing API. With the list of URLs that contain the reviews, we
    can now set up a scraper to extract from each web page the title and the review
    text using Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy setup and the application code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy is a Python library is used to extract content from web pages or to
    crawl pages linked to a given web page (see the *Web crawlers (or spiders)* section
    of [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques") , *Web
    Mining Techniques* , for more details). To install the library, type the following
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the executable in the `bin` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'From the `movie_reviews_analyzer_app` folder, we initialize our Scrapy project
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will create the following tree inside the `scrapy_spider` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `pipelines.py` and `items.py` files manage how the scraped data is stored
    and manipulated, and they will be discussed later in the *Spiders* and *Integrate
    Django with Scrapy* sections. The `settings.py` file sets the parameters each
    spider (or crawler) defined in the `spiders` folder uses to operate. In the following
    two sections, we describe the main parameters and spiders used in this application.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `settings.py` file collects all the parameters used by each spider in the
    Scrapy project to scrape web pages. The main parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DEPTH_LIMIT` : The number of subsequent pages crawled following an initial
    URL. The default is `0` and it means that no limit is set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LOG_ENABLED` : To allow/deny Scrapy to log on the terminal while executing
    default is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ITEM_PIPELINES = {''scrapy_spider.pipelines.ReviewPipeline'': 1000,}` : The
    path of the pipeline function to manipulate data extracted from each web page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONCURRENT_ITEMS = 200` : The number of concurrent items processed in the
    pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONCURRENT_REQUESTS = 5000` : The maximum number of simultaneous requests
    handled by Scrapy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONCURRENT_REQUESTS_PER_DOMAIN = 3000` : The maximum number of simultaneous
    requests handled by Scrapy for each specified domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The larger the depth, more the pages are scraped and, consequently, the time
    needed to scrape increases. To speed up the process, you can set high value on
    the last three parameters. In this application (the `spiders` folder), we set
    two spiders: a scraper to extract data from each movie review URL (`movie_link_results.py`
    ) and a crawler to generate a graph of webpages linked to the initial movie review
    URL (`recursive_link_results.py` ).'
  prefs: []
  type: TYPE_NORMAL
- en: Scraper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scraper on `movie_link_results.py` looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `Spider` class from `scrapy` is inherited by the `Search`
    class and the following standard methods have to be defined to override the standard
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` : The constructor of the spider needs to define the `start_urls`
    list that contains the URL to extract content from. In addition, we have custom
    variables such as `search_key` and `keywords` that store the information related
    to the query of the movie''s title used on the search engine API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_requests` : This function is triggered when `spider` is called and it
    declares what to do for each URL in the `start_urls` list; for each URL, the custom
    `parse_site` function will be called (instead of the default `parse` function).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse_site` : It is a custom function to parse data from each URL. To extract
    the title of the review and its text content, we used the newspaper library (`sudo
    pip install newspaper` ) or, if it fails, we parse the HTML file directly using
    some defined rules to avoid the noise due to undesired tags (each rule structure
    is defined with the `sel.xpath` command). To achieve this result, we select some
    popular domains (`rottentomatoes` , `cnn` , and so on) and ensure the parsing
    is able to extract the content from these websites (not all the extraction rules
    are displayed in the preceding code but they can be found as usual in the GitHub
    file). The data is then stored in a page `Django` model using the related Scrapy
    item and the `ReviewPipeline` function (see the following section).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CheckQueryinReview` : This is a custom function to check whether the movie
    title (from the query) is contained in the content or title of each web page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the spider, we need to type in the following command from the `scrapy_spider`
    (internal) folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pipelines define what to do when a new page is scraped by the spider. In
    the preceding case, the `parse_site` function returns a `PageItem` object, which
    triggers the following pipeline (`pipelines.py` ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This class simply saves each item (a new page in the spider notation).
  prefs: []
  type: TYPE_NORMAL
- en: Crawler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we showed in the overview (the preceding section), the relevance of the
    review is calculated using the PageRank algorithm after we have stored all the
    linked pages starting from the review''s URL. The crawler `recursive_link_results.py`
    performs this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CrawlSpider` class from `scrapy` is inherited by the `Search` class, and
    the following standard methods have to be defined to override the standard methods
    (as for the spider case):'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` : The is a constructor of the class. The `start_urls` parameter
    defines the starting URL from which the spider will start to crawl until the `DEPTH_LIMIT`
    value is reached. The `rules` parameter sets the type of URL allowed/denied to
    scrape (in this case, the same page but with different font sizes is disregarded)
    and it defines the function to call to manipulate each retrieved page (`parse_item`
    ). Also, a custom variable `search_id` is defined, which is needed to store the
    ID of the query within the other data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse_item` : This is a custom function called to store the important data
    from each retrieved page. A new Django item of the `Page` model (see the following
    section) from each page is created, which contains the title and content of the
    page (using the `xpath` HTML parser). To perform the PageRank algorithm, the connection
    from the page that links to each page and the page itself is saved as an object
    of the `Link` model using the related Scrapy item (see the following sections).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the crawler, we need to type the following from the (internal) `scrapy_spider`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Django models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data collected using the spiders needs to be stored in a database. In Django,
    the database tables are called models and defined in the `models.py` file (within
    the `pages` folder). The content of this file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each movie title typed on the home page of the application is stored in the
    `SearchTerm` model, while the data of each web page is collected in an object
    of the `Page` model. Apart from the content field (HTML, title, URL, content),
    the sentiment of the review and the depth in graph network are recorded (a Boolean
    also indicates if the web page is a movie review page or simply a linked page).
    The `Link` model stores all the graph links between pages, which are then used
    by the PageRank algorithm to calculate the relevance of the reviews web pages.
    Note that the `Page` model and the `Link` model are both linked to the related
    `SearchTerm` through a foreign key. As usual, to write these models as database
    tables, we type the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To populate these Django models, we need to make Scrapy interact with Django,
    and this is the subject of the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Django with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make paths easy to call, we remove the external `scrapy_spider` folder so
    that inside the `movie_reviews_analyzer_app` , the `webmining_server` folder is
    at the same level as the `scrapy_spider` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the Django path into the Scrapy `settings.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can install the library that will allow managing Django models from
    Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `items.py` file, we write the links between Django models and Scrapy
    items as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Each class inherits the `DjangoItem` class so that the original Django models
    declared with the `django_model` variable are automatically linked. The Scrapy
    project is now completed so we can continue our discussion explaining the Django
    codes that handle the data extracted by Scrapy and the Django commands needed
    to manage the applications.
  prefs: []
  type: TYPE_NORMAL
- en: Commands (sentiment analysis model and delete queries)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application needs to manage some operations that are not allowed to the
    final user of the service, such as defining a sentiment analysis model and deleting
    a query of a movie in order to redo it instead of retrieving the existing data
    from memory. The following sections will explain the commands to perform these
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis model loader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final goal of this application is to determine the sentiment (positive
    or negative) of the movie reviews. To achieve that, a sentiment classifier must
    be built using some external data, and then it should be stored in memory (cache)
    to be used by each query request. This is the purpose of the `load_sentimentclassifier.py`
    command displayed hereafter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'At the beginning of the file, the variable `method_selfeatures` sets the method
    of feature selection (in this case, the features are the words in the reviews;
    see [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques") , *Web
    Mining Techniques* , for further details) used to train the classifier `train_clf`
    . The maximum number of best words (features) is defined by the input parameter
    `num_bestwords` . The classifier and the best features (`bestwords` ) are then
    stored in the cache ready to be used by the application (using the `cache` module).
    The classifier and the methods to select the best words (features) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Three methods are written to select words in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stopword_filtered_words_features` : Eliminates the `stopwords` using the **Natural
    Language Toolkit** ( **NLTK** ) list of conjunctions and considers the rest as
    relevant words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`best_words_features` : Using the *X²* measure (`NLTK` library), the most informative
    words related to positive or negative reviews are selected (see [Chapter 4](text00032.html#ch04
    "Chapter 4. Web Mining Techniques") , *Web Mining Techniques* , for further details)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`best_bigrams_word_features` : Uses the *X²* measure (`NLTK` library) to find
    the 200 most informative bigrams from the set of words (see *Chapter 4* , *Web
    Mining Techniques* , for further details)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The chosen classifier is the Naive Bayes algorithm (see [Chapter 3](text00024.html#page
    "Chapter 3. Supervised Machine Learning") , *Supervised Machine Learning* ) and
    the labeled text (positive, negative sentiment) is taken from the `NLTK.corpus`
    of `movie_reviews` . To install it, open a terminal in Python and install `movie_reviews`
    from `corpus` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Deleting an already performed query
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we can specify different parameters (such as the feature selection method,
    the number of best words, and so on), we may want to perform and store again the
    sentiment of the reviews with different values. The `delete_query` command is
    needed for this purpose and it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the command without specifying the `searchid` (the ID of the query),
    the list of all the queries and related IDs will be shown. After that we can choose
    which query we want to delete by typing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can use the cached sentiment analysis model to show the user the online sentiment
    of the chosen movie, as we explain in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment reviews analyser – Django views and HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the code explained in this chapter (commands, Bing search engine, Scrapy,
    and Django models) is used in the function analyzer in `views.py` to power the
    home webpage shown in the *Application usage overview* section (after declaring
    the URL in the `urls.py` file as `url(r'^$','webmining_server.views.analyzer')`
    ).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The inserted movie title is stored in the `query` variable and sent to the
    `bing_api` function to collect review''s URL. The URL are then scraped calling
    Scrapy to find the review texts, which are processed using the `clf` classifier
    model and the selected most informative words (`bestwords` ) retrieved from the
    cache (or the same model is generated again in case the cache is empty). The counts
    of the predicted sentiments of the reviews (`positive_counts` , `negative_counts`
    , and `reviews_classified` ) are then sent back to the `home.html` (the `templates`
    folder) page, which uses the following Google pie chart code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The function `drawChart` calls the Google `PieChart` visualization function,
    which takes as input the data (the positive and negative counts) to create the
    pie chart. To have more details about how the HTML code interacts with the Django
    views, refer to [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with
    Django") , *Getting Started with Django* , in the *URL and views behind html web
    pages* section. From the result page with the sentiment counts (see the *Application
    usage overview* section), the PagerRank relevance of the scraped reviews can be
    calculated using one of the two links at the bottom of the page. The Django code
    behind this operation is discussed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'PageRank: Django view and the algorithm code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To rank the importance of the online reviews, we have implemented the PageRank
    algorithm (see [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* , in the *Ranking: PageRank algorithm* section) into
    the application. The `pgrank.py` file in the `pgrank` folder within the `webmining_server`
    folder implements the algorithm that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This code takes all the links stores associated with the given `SearchTerm`
    object and implements the PageRank score for each page *i* at time *t* , where
    *P(i)* is given by the recursive equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PageRank: Django view and the algorithm code](img/Image00544.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *N* is the total number of pages, and ![PageRank: Django view and the
    algorithm code](img/Image00545.jpg) ( *N[j] * is the number of out links of page
    *j* ) if page *j* points to *i* ; otherwise, *N* is `0` . The parameter *D* is
    the so-called **damping factor** (set to 0.85 in the preceding code), and it represents
    the probability to follow the transition given by the transition matrix *A* .
    The equation is iterated until the convergence parameter `eps` is satisfied or
    the maximum number of iterations, `num_iterations` , is reached. The algorithm
    is called by clicking either **scrape and calculate page rank (may take a long
    time)** or **calculate page rank** links at the bottom of the `home.html` page
    after the sentiment of the movie reviews has been displayed. The link is linked
    to the function `pgrank_view` in the `views.py` (through the declared URL in `urls.py:
    url(r''^pg-rank/(?P<pk>\d+)/'',''webmining_server.views.pgrank_view'', name=''pgrank_view'')`
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This code calls the crawler to collect all the linked pages to the reviews and
    calculate the PageRank scores using the code discussed earlier. Then the scores
    are displayed in the `pg-rank.html` page (in descending order by page rank score)
    as we showed in the *Application usage overview* section of this chapter. Since
    this function can take a long time to process (to crawl thousands of pages), the
    command `run_scrapelinks.py` has been written to run the Scrapy crawler (the reader
    is invited to read or modify the script as they like as an exercise).
  prefs: []
  type: TYPE_NORMAL
- en: Admin and API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the last part of the chapter, we describe briefly some possible admin management
    of the model and the implementation of an API endpoint to retrieve the data processed
    by the application. In the `pages` folder, we can set two admin interfaces in
    the `admin.py` file to check the data collected by the `SearchTerm` and `Page`
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that both `SearchTermAdmin` and `PageAdmin` display objects with decreasing
    ID (and `new_rank` in the case of `PageAdmin` ). The following screenshot is an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Admin and API](img/Image00546.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that although it is not necessary, the `Link` model has also been included
    in the admin interface (`admin.site.register(Link)` ). More interestingly, we
    can set up an API endpoint to retrieve the sentiment counts related to a movie''s
    title. In the `api.py` file inside the pages folder, we can have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PageCounts` class takes as input the ID of the search (the movie''s title)
    and it returns the sentiments, that is, positive and negative counts, for the
    movie''s reviews. To get the ID of `earchTerm` from a movie''s title, you can
    either look at the admin interface or use the other API endpoint `SearchTermsList`
    ; this simply returns the list of the movies'' titles together with the associated
    ID. The serializer is set on the `serializers.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To call these endpoints, we can again use the swagger interface (see [Chapter
    6](text00046.html#ch06 "Chapter 6. Getting Started with Django") , *Getting Started
    with Django* ) or use the `curl` command in the terminal to make these calls.
    For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described a movie review sentiment analyzer web application
    to make you familiar with some of the algorithms and libraries we discussed in
    [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning") , *Supervised
    Machine Learning* , [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* , and [Chapter 6](text00046.html#ch06 "Chapter 6. Getting
    Started with Django") , *Getting Started with Django* .
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the end of a journey: by reading this book and experimenting with the
    codes provided, you should have acquired significant practical knowledge about
    the most important machine learning algorithms used in the commercial environment
    nowadays.'
  prefs: []
  type: TYPE_NORMAL
- en: You should be now ready to develop your own web applications and ideas using
    Python and some machine learning algorithms, learned by reading this book. Many
    challenging data-related problems are present in the real world today, waiting
    to be solved by people who can grasp and apply the material treated in this book,
    and you, who have arrived at this point, are certainly one of those people.
  prefs: []
  type: TYPE_NORMAL
- en: 读累了记得休息一会哦~
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**公众号：古德猫宁李**'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书单分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书友学习交流
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 电子书打包资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 学习资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
