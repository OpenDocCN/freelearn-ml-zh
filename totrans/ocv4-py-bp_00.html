<html><head></head><body>
        

                            
                    <h1 class="header-title">Preface</h1>
                
            
            
                
<p class="mce-root">The goal of this book is to get you hands-on with a wide range of intermediate to advanced projects using the latest version of the <strong>OpenCV 4</strong> framework and the <strong>Python 3.8</strong> language instead of only covering the core concepts of computer vision in theoretical lessons.</p>
<p class="mce-root">This updated second edition has increased the depth of the concepts we tackle with OpenCV. It will guide you through working on independent hands-on projects that focus on essential computer vision concepts such as image processing, 3D scene reconstruction, object detection, and object tracking. It will also cover, with real-life examples, statistical learning and deep neural networks.</p>
<p class="mce-root">You will begin by understanding concepts such as image filters and feature matching, as well as using custom sensors such as the <strong>Kinect depth sensor</strong>. You will also learn how to reconstruct and visualize a scene in 3D, how to align images, and how to combine multiple images into a single one. As you advance through the book, you will learn how to recognize traffic signs and emotions on faces and detect and track objects in video streams using neural networks, even if they disappear for short periods of time.<br/></p>
<p class="mce-root">By the end of this OpenCV and Python book, you will have hands-on experience and be proficient at developing your own advanced computer vision applications according to specific business needs. Throughout the book, you will explore multiple machine learning and computer vision models such as <strong>Support Vector Machines</strong> (<strong>SVMs</strong>) and convolutional neural networks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Who this book is for</h1>
                
            
            
                
<p>This book is aimed at computer vision enthusiasts in pursuit of mastering their skills by developing advanced practical applications using OpenCV and other machine learning libraries.</p>
<p>Basic programming skills and Python programming knowledge is assumed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What this book covers</h1>
                
            
            
                
<p><a href="2e878463-75f1-40a5-b263-0c5aa9627328.xhtml">Chapter 1</a>, <em>Fun with Filters</em>, explores a number of interesting image filters (such as a black-and-white pencil sketch, warming/cooling filters, and a cartoonizer effect), and we'll apply them to the video stream of a webcam in real time.</p>
<p><a href="0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml">Chapter 2</a>, <em>Hand Gesture Recognition Using a Kinect Depth Sensor</em>, helps you develop an app to detect and track simple hand gestures in real time using the output of a depth sensor, such as Microsoft Kinect 3D Sensor or Asus Xtion.</p>
<p><a href="905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml">Chapter 3</a>, <em>Finding Objects via Feature Matching and Perspective Transforms</em>, helps you develop an app to detect an arbitrary object of interest in the video stream of a webcam, even if the object is viewed from different angles or distances, or under partial occlusion.</p>
<p><a href="efb28928-4399-4e3d-9ca3-6e773aaaa699.xhtml">Chapter 4</a>, <em>3D Scene Reconstruction Using Structure from Motion</em>, shows you how to reconstruct and visualize a scene in 3D by inferring its geometrical features from camera motion.</p>
<p><a href="bc92b221-21e9-4ade-9eea-308e0fb50e83.xhtml">Chapter 5</a>, <em>Using Computational Photography with OpenCV</em>, helps you develop command-line scripts that take images as input and produce panoramas or <strong>High Dynamic Range</strong> (<strong>HDR</strong>) images. The scripts will either align the images so that there is a pixel-to-pixel correspondence or stitch them creating a panorama, which is an interesting application of image alignment. In a panorama, the two images are not that of a plane but that of a 3D scene. In general, 3D alignment requires depth information. However, when the two images are taken by rotating the camera about its optical axis (as in the case of panoramas), we can align two images of a panorama.</p>
<p><a href="f4c74d0a-5d0a-4db9-911a-f2663f43ae90.xhtml">Chapter 6</a>, <em>Tracking Visu</em><em>ally Salient Objects</em>, helps you develop an app to track multiple visually salient objects in a video sequence (such as all the players on the field during a soccer match) at once.</p>
<p><a href="0e410c47-1679-4125-9614-54ec0adfa160.xhtml">Chapter 7</a>, <em>Learning to Recognize Traffic Signs</em>, shows you how to train a support vector machine to recognize traffic signs from the <strong>German Traffic Sign Recognition Benchmark</strong> (<strong>GTSRB</strong>) dataset.</p>
<p><a href="1bf0d987-dab1-465c-91de-809e69a818dd.xhtml">Chapter 8</a>, <em>Learning to Recognize Facial Emotions</em>, helps you develop an app that is able to both detect faces and recognize their emotional expressions in the video stream of a webcam in real time.</p>
<p><a href="8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml">Chapter 9</a>, <em>Learning to Recognize Facial Emotions</em>, walks you through developing an app for real-time object classification with deep convolutional neural networks. You will modify a classifier network to train on a custom dataset with custom classes. You will learn how to train a Keras model on a dataset and how to serialize and save your Keras model to a disk. You will then see how to classify new input images using your loaded Keras model. You will train a convolutional neural network using the image data you have to get a good classifier that will have very high accuracy. </p>
<p><a href="7eaac815-5888-4352-aa83-7a3b50d0d275.xhtml">Chapter 10</a>, <em>Learning to Detect and Track Objects</em>, guides you as you develop an app for real-time object detection with deep neural networks, connecting it to a tracker. You will learn how object detectors work and how they are trained. You will implement a Kalman filter-based tracker, which will use object position and velocity to predict where it is likely to be. After completing this chapter, you will be able to build your own real-time object detection and tracking applications.</p>
<p><a href="a4f1f102-9f62-4644-bcde-f478cd28621a.xhtml">Appendix A</a>, <em>Profiling and Accelerating Your Apps</em>, covers how to find bottlenecks in an app and achieve CPU- and CUDA-based GPU acceleration of existing code with Numba. </p>
<p><a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml">Appendix B</a>, <em>Setting Up a Docker Container</em>, walks you through replicating the environment that we have used to run the code in this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">To get the most out of this book</h1>
                
            
            
                
<p>All of our code use <strong>Python 3.8</strong>, which is available on a variety of operating systems, such as <strong>Windows</strong>, <strong>GNU Linux</strong>, <strong>macOS</strong>, and others. We have made an effort to use only libraries that are available on these three operating systems. We will go over the exact versions of each of the dependencies we have used, which can be installed using <kbd>pip</kbd> (Python's dependency management system). If you have trouble getting any of these working, we have Dockerfiles available with which we have tested all the code in this book, which we cover in <a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml"/><a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml">Appendix B</a>, <em>Setting Up a Docker Container</em>.</p>
<p>Here is a list of dependencies that we have used, with the chapters they were used in:</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
<tr>
<td style="width: 13%;">
<p><strong>Software required<br/></strong></p>
</td>
<td style="width: 5%;"><strong>Version</strong></td>
<td style="width: 10%;">
<p><strong>Chapter number</strong></p>
</td>
<td style="width: 26.2334%;">
<p><strong>Download links to the software</strong></p>
</td>
</tr>
<tr>
<td style="width: 13%;">
<p>Python</p>
</td>
<td style="width: 5%;">3.8</td>
<td style="width: 10%;">
<p>All</p>
</td>
<td style="width: 26.2334%;">
<p><a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a></p>
</td>
</tr>
<tr>
<td style="width: 13%;">
<p>OpenCV</p>
</td>
<td style="width: 5%;">4.2</td>
<td style="width: 10%;">
<p>All</p>
</td>
<td style="width: 26.2334%;"><a href="https://opencv.org/releases/">https://opencv.org/releases/</a></td>
</tr>
<tr>
<td style="width: 13%;">
<p>NumPy</p>
</td>
<td style="width: 5%;">1.18.1</td>
<td style="width: 10%;">
<p>All</p>
</td>
<td style="width: 26.2334%;">
<p><a href="http://www.scipy.org/scipylib/download.html">http://www.scipy.org/scipylib/download.html</a></p>
</td>
</tr>
<tr>
<td style="width: 13%;">wxPython
<p> </p>
</td>
<td style="width: 5%;">4.0
<p> </p>
</td>
<td style="width: 10%;">1, 4, 8<br/>
<p> </p>
</td>
<td style="width: 26.2334%;"><a href="http://www.wxpython.org/download.php">http://www.wxpython.org/download.php</a>
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13%;">matplotlib
<p> </p>
</td>
<td style="width: 5%;">3.1
<p> </p>
</td>
<td style="width: 10%;">4, 5, 6, 7<br/>
<p> </p>
</td>
<td style="width: 26.2334%;"><a href="http://matplotlib.org/downloads.html">http://matplotlib.org/downloads.html</a>
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13%;">SciPy
<p> </p>
</td>
<td style="width: 5%;">1.4
<p> </p>
</td>
<td style="width: 10%;">1, 10<br/>
<p> </p>
</td>
<td style="width: 26.2334%;"><a href="http://www.scipy.org/scipylib/download.html">http://www.scipy.org/scipylib/download.html</a>
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13%;">rawpy
<p> </p>
</td>
<td style="width: 5%;">0.14
<p> </p>
</td>
<td style="width: 10%;">5
<p> </p>
</td>
<td style="width: 26.2334%;"><a href="https://pypi.org/project/rawpy/">https://pypi.org/project/rawpy/</a>
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13%;">ExifRead</td>
<td style="width: 5%;">2.1.2</td>
<td style="width: 10%;">5</td>
<td style="width: 26.2334%;"><a href="https://pypi.org/project/ExifRead/">https://pypi.org/project/ExifRead/</a></td>
</tr>
<tr>
<td style="width: 13%;">TensorFlow</td>
<td style="width: 5%;">2.0</td>
<td style="width: 10%;">7, 9</td>
<td style="width: 26.2334%;"><a href="https://www.tensorflow.org/install">https://www.tensorflow.org/install</a></td>
</tr>
</tbody>
</table>
<p> </p>
<p>In order to run the codes, you will need a regular laptop or Personal Computer (PC). Some chapters require a webcam, which can be either an embedded laptop camera or an external one. <a href="0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml">Chapter 2</a>, <em>Hand Gesture Recognition Using a Kinect Depth Sensor</em> also requires a depth sensor that can be either a <strong>Microsoft 3D Kinect sensor</strong> or any other sensor, which is supported either by the <kbd>libfreenect</kbd> library or OpenCV, such as <strong>ASUS Xtion</strong>.</p>
<p>We have tested this using <strong>Python 3.8</strong> and <strong>Python 3.7</strong>, on <strong>Ubuntu 18.04</strong>.</p>
<p>If you already have Python on your computer, you can just get going with running the following on your terminal:</p>
<pre><strong>$ pip install -r requirements.txt</strong></pre>
<p>Here, <kbd>requirements.txt</kbd> is provided in the GitHub repository of the project, and has the following contents (which is the previously given table in a text file):</p>
<pre><strong>wxPython==4.0.5</strong><br/><strong>numpy==1.18.1</strong><br/><strong>scipy==1.4.1</strong><br/><strong>matplotlib==3.1.2</strong><br/><strong>requests==2.22.0</strong><br/><strong>opencv-contrib-python==4.2.0.32</strong><br/><strong>opencv-python==4.2.0.32</strong><br/><strong>rawpy==0.14.0</strong><br/><strong>ExifRead==2.1.2</strong><br/><strong>tensorflow==2.0.1</strong></pre>
<p>Alternatively, you can follow the instructions in <a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml">Appendix B</a>, <em>Setting Up a Docker Container</em>, to get everything working with a Docker container.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Download the example code files</h1>
                
            
            
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank" rel="noopener noreferrer">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="https://www.packtpub.com/support" target="_blank" rel="noopener noreferrer">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank" rel="noopener noreferrer">www.packt.com</a>.</li>
<li>Select the Support tab.</li>
<li>Click on Code Downloads.</li>
<li>Enter the name of the book in the Search box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p>The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
<p>We also have other code bundles from our rich catalog of books and videos available at <strong><a href="https://github.com/PacktPublishing/" target="_blank" rel="noopener noreferrer">https://github.com/PacktPublishing/</a></strong>. Check them out!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Code in Action</h1>
                
            
            
                
<p>Code in Action videos for this book can be viewed at <a href="http://bit.ly/2xcjKdS">http://bit.ly/2xcjKdS</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Download the color images</h1>
                
            
            
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://static.packt-cdn.com/downloads/9781789801811_ColorImages.pdf">http://static.packt-cdn.com/downloads/9781789801811_ColorImages.pdf</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conventions used</h1>
                
            
            
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "We will use <kbd>argparse</kbd> as we want our script to accept arguments."</p>
<p>A block of code is set as follows:</p>
<pre>import argparse<br/><br/>import cv2<br/>import numpy as np<br/><br/>from classes import CLASSES_90<br/>from sort import Sort</pre>
<p>Any command-line input or output is written as follows:</p>
<pre><strong>$ python chapter8.py collect</strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Select System info from the Administration panel."</p>
<p>Warnings or important notes appear like this.</p>
<p>Tips and tricks appear like this.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Get in touch</h1>
                
            
            
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <kbd>customercare@packtpub.com</kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="https://www.packtpub.com/support/errata" target="_blank" rel="noopener noreferrer">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank" rel="noopener noreferrer">authors.packtpub.com</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reviews</h1>
                
            
            
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank" rel="noopener noreferrer">packt.com</a>.<a href="https://www.packtpub.com/" target="_blank" rel="noopener noreferrer"/></p>


            

            
        
    </body></html>