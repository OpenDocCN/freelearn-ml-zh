- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI Risk Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As organizations increasingly rely on AI for critical decision-making and incorporate
    it into different areas of their businesses, effective AI risk management should
    be a top priority. Ensuring the safe and compliant deployment of ML systems is
    essential to establish trustworthiness in AI solutions. However, many organizations
    and individuals have very limited understanding of the risks associated with AI
    systems, often resulting in outcomes that may negatively impact organizations
    financially or legally. In this chapter, we will explore key AI risk scenarios,
    highlight the differences between AI risk management and traditional software
    risk management, and emphasize the importance of having a robust AI risk management
    practice. We will present a risk management framework that organizations can consider
    for managing AI risks. Finally, we will discuss how to manage risks at different
    stages of the ML lifecycle and design ML platforms that support risk management
    and AI governance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AI risk scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regulatory landscape around AI risk management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding AI risk management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying risk management across the AI lifecycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing ML platforms with governance and risk management considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding AI risk scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the organizations I have worked with have very limited knowledge about
    the risks presented in their AI systems. They often treat AI risks the same way
    they deal with risks associated with traditional software. In reality, AI systems
    present a new set of risks that we do not normally see in traditional software.
    With traditional software, the risk is mainly about software vulnerability, a
    legacy technology stack, malware, misconfiguration, and unauthorized access to
    data. AI systems are exposed to many of the same software risks; additionally,
    AI systems can present new kinds of risks such as bias and misinformation. These
    risks can have significant negative consequences for organizations and individuals
    that rely on AI systems for business operations and decision-making. AI risks
    can manifest in many different ways, such as displaying biased behavior or producing
    unexpected prediction results. Many of the AI risk scenarios are also silent risks
    that are difficult to detect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some scenarios where AI risks may arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias and discrimination**: One of the most well-known risks associated with
    AI is the potential for displaying bias and discrimination in AI systems. This
    can occur when ML algorithms are trained on biased data or when the algorithms
    themselves are susceptible to biased behaviors. In these cases, the algorithms
    may learn to discriminate against certain groups, leading to unfair or discriminatory
    outcomes. For example, a bank can have an ML model that’s trained using biased
    datasets, such as including gender and ethnic groups as inputs, resulting in discrimination
    against certain gender or ethnic groups and potential violation of laws and regulations,
    such as the Equal Credit Opportunity Act. Nowadays, many organizations use AI
    to screen resumes, and it has been found that some of these AI systems have displayed
    preferences and biases toward certain types of candidates. In 2018, researchers
    from MIT revealed that facial recognition systems from several major technology
    companies exhibited significant racial bias, misidentifying darker-skinned individuals
    at higher rates compared to lighter-skinned individuals. This incident raised
    concerns about the potential for AI systems to perpetuate and amplify societal
    biases, leading to discriminatory outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinformation and misinterpretation**: Another risk associated with AI is
    the potential for generating misinformation and misinterpretation of facts. This
    can occur when ML algorithms are used to process large amounts of data, but the
    data contains errors or inconsistencies that are not easily detected. As a result,
    the algorithms may generate inaccurate or misleading results, leading to potential
    wrong decision-making. With the fast rise of generative AI technologies, such
    as ChatGPT and Stable Diffusion models, it is also becoming more difficult for
    humans to distinguish reality from hallucination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the rapid advancement of deepfake technologies, which use AI to
    generate highly realistic synthetic audio, video, and images, has raised concerns
    about their potential misuse for spreading misinformation, impersonation, and
    manipulation. Incidents of deepfake videos being used to impersonate public figures
    and spread false narratives have already been reported.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Lack of interpretability**: Many ML algorithms, such as neural networks,
    can be complex and difficult to understand, even for trained experts. This lack
    of transparency can make it difficult to identify the causes of problems when
    they arise, making it harder to develop effective mitigation to address the problem.
    For example, when ChatGPT provides incorrect responses to user prompts, it is
    often impossible to understand why it made the mistake. For regulated industries,
    this presents a significant challenge when organizations want to adopt more advanced
    black-box algorithms such as neural networks, as these organizations often need
    to provide deterministic responses to specific inputs and questions, and how the
    decisions are made.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unintended consequences**: ML algorithms can sometimes produce unintended
    consequences or side effects that were not foreseen during the development process.
    The reason for this is that AI models are often optimized for a specific objective,
    such as increasing company profit, while ignoring other factors such as gender
    and race. For example, an AI-based target marketing system might target a subset
    of customers with incentives and benefits, while discriminating against minority
    or low-income customers, in its pursuit of profit maximization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial attacks**: ML algorithms can be vulnerable to adversarial attacks,
    which involve deliberately manipulating the input data to produce unexpected or
    undesirable outcomes, or planting backdoor access to ML models. For example, an
    attacker could use an adversarial attack to trick an AI-based fraud detection
    system into classifying fake financial transactions as legitimate transactions.
    ML models can also be compromised to reveal training data by using adversarial
    techniques such as membership inference attacks. There have been real-world examples
    of adversarial attacks. In 2017, researchers from the University of Michigan demonstrated
    a method to generate small, innocuous-looking patches that could be placed on
    physical objects, such as stop signs or pedestrians, to cause state-of-the-art
    object detection models to misclassify or fail to detect those objects. In another
    example, a chatbot implemented by a car dealership faced disruptions when mischievous
    users exploited a loophole, which, at times, prompted the bot to unintentionally
    propose extraordinary deals like acquiring brand-new cars for minimal costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy violation and sensitive data exposure**: Nowadays, many of the state-of-the-art
    models are trained using enormous amounts of data from many different sources,
    and, sometimes, personal or sensitive information is used in the development of
    these models. This inherently increases the risk of potential privacy violations
    and unintended disclosure of sensitive data. For example, to train a medical imaging
    model for cancer detection, you often need real patient data such as CT scans
    and other **protected health information** (**PHI**) or **personal identifiable
    information** (**PII**). If not handled correctly, this information can be exposed
    to people who are not authorized to access it. Also, as part of the model training,
    some sensitive data can be memorized by the trained model, and the model can potentially
    disclose this information when making predictions. In 2020, an investigative report
    by the New York Times revealed that people’s images were used in AI model training
    without their consent and knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third-party risks**: While third-party risks also exist with traditional
    software from third-party vendors, AI systems elevate these risks in areas that
    we have not seen before. With the advent of ML techniques such as transfer learning
    and fine-tuning from pre-trained models, more and more organizations are building
    custom models based on existing pre-trained models. However, given the black-box
    nature of these pre-trained models, it increases the uncertainty of the models’
    behavior and unknowns around the scientific validity of the models. Since the
    pre-trained models were outside of the security and process controls of the consuming
    organizations, the consuming organization may inherit risks that may already exist
    in the pre-trained models such as bias or backdoor attack vulnerabilities. For
    example, models with vulnerabilities such as arbitrary code execution and file
    writes have been detected in model hubs such as Hugging Face.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model testing risks**: Compared to traditional software, the testing standards
    and tools for AI-based software and models are underdeveloped. Traditional software
    testing mainly focuses on functional components such as user interface flow or
    business logic that’s well defined, and non-functional areas such as scalability
    and latency. With AI/ML testing, in addition to many of the traditional software
    testing requirements, there are new testing concepts such as error analysis of
    different failure modes, model sensitivity, model robustness, and adversarial
    testing, which are more difficult to perform than traditional software testing.
    The available testing tools in this domain are also very limited; often, data
    scientists and testers need to manually prepare different testing scenarios and
    testing data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the capabilities of AI systems extend far beyond traditional
    software, introducing many new potential risks and challenges. As these advanced
    technologies gain wide adoption and integration into critical domains, concerns
    over their responsible development and deployment have come to the forefront.
    Recognizing the unique complexities and far-reaching implications of AI, various
    regulatory bodies have taken proactive steps to establish guidelines and regulations
    aimed at mitigating these risks and ensuring the ethical and trustworthy use of
    AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: The regulatory landscape around AI risk management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the fast advancement of AI technologies and adoption in critical business
    decision-making, and the negative impacts that AI systems can potentially have
    on individuals, organizations, and societies, many countries and jurisdictions
    have established policies, guidance, and regulations to help manage the risks
    of AI adoption. It is also expected that more and more legislation will be proposed
    and passed by different countries and jurisdictions at a fast rate.
  prefs: []
  type: TYPE_NORMAL
- en: In the **United States** (**US**), the Federal Reserve and the **Office of the
    Comptroller of the Currency** (**OCC**) published the Supervisory Guidance on
    Model Risk Management (OCC 2011-2012/SR 11-7) as early as 2011\. SR 11-7 has become
    the key regulatory guidance for model risk management in the US. This guidance
    establishes the main principles for model risk management covering governance,
    policies and controls, model development, implementation and use, and model validation
    processes. In the governance and policy area, it provides guidance on model inventory
    management, risk rating, roles, and responsibilities. In the model development
    and implementation area, it covers topics such as the design process, data assessment,
    model testing, and documentation. In the validation area, it provides guidance
    on validation procedures, monitoring, and finding resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: In Europe, the **European Central Bank** (**ECB**) Banking Supervision launched
    the **Targeted Review of Internal Models** (**TRIM**) guideline in 2016 to provide
    guidance on the **model risk management** (**MRM**) framework. Specifically, the
    guideline states that an MRM framework needs to have a model inventory to allow
    a holistic view of the models and their applications, a guideline for identifying
    and mitigating known model deficiencies, definitions of roles and responsibilities,
    and definitions of policies, measurement procedures, and reporting.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, in 2021, the **European Union** (**EU**) introduced the EU AI
    Act to promote the benefits of AI, while also ensuring the safe and responsible
    use of AI in the EU. The Act takes a risk-based approach to regulating AI, with
    different requirements based on the level of risks associated with AI systems.
    For example, AI systems supporting critical infrastructure would be rated with
    the highest risk designation and require the strictest oversight and regulations.
    The regulation also proposes provisions for AI transparency and accountability
    in the AI decision-making process, such as requirements for explainability and
    the ability to challenge the decisions made by AI systems. It also has new rules
    for AI use in biometric identification and surveillance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AI risk management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address the various risks associated with AI and to comply with different
    compliance regulations, many organizations, especially in the regulated industry,
    have developed and implemented AI risk management programs. In short, AI risk
    management is the process of identifying, assessing, and mitigating the risk associated
    with the use of AI in automated decision-making. The ultimate goal of AI risk
    management is to establish trust in the AI/ML systems and ensure compliance with
    applicable rules and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Trusting an AI system requires rigorous assessment and consideration of the
    AI system across many different dimensions and criteria. Functionally, a trusted
    AI system needs to provide valid predictions/responses reliably for its intended
    use. This means that generated predictions/responses are consistently valid and
    can be trusted for reliable decision-making. Ethically, a trusted AI system needs
    to be safe to use, explainable, privacy-protected, and fair with bias properly
    managed and mitigated. From a cybersecurity perspective, a trusted AI system also
    needs to be secure and resilient against adversarial attacks. Lastly, a trustworthy
    AI system needs to provide transparency such as what and how data, algorithms,
    and models are used in the system. It is worth noting that it is often a balancing
    act and trade-off across these different dimensions when building and operating
    a trustworthy system, based on the needs and objectives of an organization. For
    example, to protect privacy, an organization might need to make sacrifices in
    model accuracy or prediction speed.
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand what it takes to have trust in AI systems, let’s explore and
    dive deep into the AI risk management framework and its various components. The
    following figure illustrates the key components of AI risk management, which mainly
    consists of applying MRM, enterprise risk management, and third-party risk management
    across the AI lifecycle, governed by a set of AI risk governance principles. In
    this chapter, our exclusive focus will be on MRM. Enterprise risk and third-party
    risk management extend beyond the realm of AI and are universal considerations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B20836_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: AI risk management components'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will delve into the details of governance oversight principles and
    the AI risk framework. We will focus mainly on the understanding of AI risk governance
    and MRM, with the understanding that traditional enterprise security and third-party
    risk management are also part of the overall AI risk management considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Governance oversight principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing AI risk management starts with establishing key governance principles.
    The principles clarify the ultimate goals for what needs to be accomplished with
    risk management programs. Depending on the business and the regulatory environment
    an organization is in, an organization can make a decision on whether it should
    be included in its risk management framework. The following are some of the key
    areas for consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparency**: AI systems should be designed in a way that allows stakeholders
    to understand how decisions are made and why. This may include the ability to
    explain the ML model predictions, as well as transparency on what and how the
    data and algorithms are used and implemented.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability**: Organizations should be accountable for the decisions made
    by AI systems, including any negative consequences that may arise from their use.
    This accountability will ensure the owning organizations are incentivized to institute
    relevant policies and processes to govern the ML lifecycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data governance**: Organizations should ensure that the data used to train
    AI systems is accurate, representative, ethical, and unbiased. Without proper
    data governance, it would be highly challenging to trust AI systems that are built
    with ungoverned data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human oversight**: While AI systems are meant to make decisions automatically
    in most cases without human intervention, organizations should have the ability
    to implement human oversight where required, meaning that humans should be involved
    in decision-making where it makes sense and should have the ability to override
    decisions when necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and security**: Appropriate policies and processes should be established
    to ensure AI systems are designed to protect the privacy and security of assets
    according to laws and regulations. Privacy and security breaches can have significant
    financial and non-financial implications for an organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness**: AI systems should not discriminate against certain individuals
    or groups based on attributes such as race and gender.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validity and reliability**: AI systems should be designed to produce reliable
    and valid results. Appropriate model validation and testing frameworks and processes
    need to be implemented to ensure highly reliable and predictable behaviors are
    displayed by AI systems in production. Mechanisms should be established to monitor
    system behaviors with processes for mitigation and rollback when abnormal behaviors
    are observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With governance oversight, organizations should also consider requirements for
    regulatory compliance, policies and guidelines around roles and responsibility,
    and standards and processes around AI systems and model inventory and risk classification,
    as well as how to deal with lifecycle and change management.
  prefs: []
  type: TYPE_NORMAL
- en: AI risk management framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the AI governance oversight principles defined, organizations can move
    forward to establish a formalized AI risk management framework and detailed mechanisms
    for risk identification, risk assessment, and risk mitigation across the end-to-end
    ML lifecycle. One of the common frameworks adopted by many organizations is the
    Three Lines of Defense MRM model that is commonly used in the financial services
    industry. This framework focuses on establishing policies, roles, responsibilities,
    and processes designed to identify, assess, mitigate, and audit potential model
    risks associated with business problem identification, data management, and model
    development, deployment, and uses.
  prefs: []
  type: TYPE_NORMAL
- en: The first line of defense is owned by business operations. This line of defense
    focuses on the development and use of ML models. Business operations are responsible
    for creating and retaining all data and model assumptions, model behavior, and
    model performance metrics in structured documents, based on model classification
    and risk exposure. Models are tested and registered, the associated artifacts
    are persisted, and results can be reproduced. Once models are deployed, system
    issues, model outputs, model bias, and data and model drifts are monitored and
    addressed according to the established procedures and guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: The second line of defense is owned by the risk management function, and it
    focuses on model validation. The risk management function is responsible for independently
    reviewing and validating the documents generated by the first line. This line
    of defense introduces standards on controls and documentation, making sure that
    documents are self-contained, results are reproducible, and the limitations of
    models are well-understood by stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: The internal audit owns the third line of defense. The third line of defense
    focuses more on control and processes and less on model artifacts and theories.
    Specifically, this line of defense is responsible for auditing the first and second
    lines of defense to ensure all established processes and guidelines are effectively
    followed and implemented. This line of defense provides independent validation
    of internal controls and reviews the documentation, timeliness, frequency, and
    completeness of the MRM activities.
  prefs: []
  type: TYPE_NORMAL
- en: The MRM alone mainly addresses the risks associated with model development and
    the development lifecycle. However, a comprehensive AI risk management framework
    also needs to cover other risks such as system scalability and reliability, unauthorized
    access of systems, denial of access, and third-party failure risks. MRM should
    also be combined with enterprise technology risk, cybersecurity management, and
    third-party risks to ensure comprehensive coverage of AI risks.
  prefs: []
  type: TYPE_NORMAL
- en: Applying risk management across the AI lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI risks can exist in any stage of the AI lifecycle, spanning from business
    problem identification to the uses of AI systems. In the following sections, we
    will explore the various risks that can arise at each stage of the AI lifecycle
    (as illustrated in *Figure 12.1*) and suggest effective strategies and considerations
    to mitigate them.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem identification and definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this initial stage of the AI lifecycle, organizations develop a comprehensive
    understanding of the business problems that AI can address. They also outline
    the overall solution approach and data prerequisites. It is critical during this
    phase to verify that the AI solution aligns with governance principles, standards,
    and requirements while achieving specific business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: One significant risk is the regulatory compliance risk, which arises when there
    is a lack of consideration for potential regulatory requirements. Organizations
    must understand applicable regulatory requirements related to AI projects, such
    as the EU AI Act, and take appropriate measures to address them in the problem
    identification phase. Failure to do so can result in non-compliance, impacting
    the entire project. Another critical consideration is the ethical risk. Ethics
    can play a vital role in an organization’s values and brand reputation. If not
    integrated into business problem identification, the final system may cause misalignment
    with core values and the brand, leading to reputational damage.
  prefs: []
  type: TYPE_NORMAL
- en: Unexpected consequences can arise if the system is not designed for the intended
    use. Misuse can result in unforeseen negative consequences, emphasizing the need
    for careful consideration of system design. Risk rating and classification help
    determine potential impacts and guide different levels of risk management. Without
    this, AI systems and their data may be mishandled, leading to unexpected consequences
    and potential adverse outcomes. Additionally, security and privacy requirement
    risks emphasize the need for implementing security and privacy measures. Without
    these requirements, organizations are at risk of privacy violations and adversarial
    manipulation of AI systems, compromising data integrity and security.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the potential risks identified at this stage, it is important to
    conduct an assessment to determine the severity and likelihood of the risk happening
    and the resulting impact. Determine whether any mitigation measures should be
    considered to reduce the risk, based on the risk tolerance level of each individual
    organization. Only move forward with the project if the key risks are understood,
    mitigated, or accepted.
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition and management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During this phase of the project lifecycle, it is crucial for organizations
    to identify appropriate data sources, establish a data acquisition strategy, and
    assess their technology capabilities for data processing and management. AI systems
    present a distinct set of data acquisition and processing risks, in addition to
    their exposure to many common data-related risks. These risks span from selecting
    the appropriate dataset to managing data end to end. These risks encompass every
    aspect, from the careful selection of datasets to the comprehensive management
    of data throughout its lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Risk considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One key risk is the data selection risk, where an incorrect or inadequately
    sampled dataset may introduce significant data relevancy or bias issues. This
    can lead to the development of a biased model or a model that fails to effectively
    address the problem at hand. For instance, in a credit scoring project, if certain
    demographic groups are underrepresented in the dataset during data collection,
    the resulting model may exhibit bias toward the overrepresented groups.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality and missing data pose significant challenges for data scientists,
    affecting the development of high-quality ML models. Ensuring data accuracy and
    addressing missing data issues is crucial for the successful development of robust
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling risks emerge as a concern due to the predominantly manual nature
    of data labeling processes. This not only becomes a bottleneck in model development
    but can also lead to poor model accuracy if mislabeling mistakes occur.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory and compliance data checks are vital for projects falling under specific
    regulatory compliance requirements. Enforcing regulatory and compliance data checks,
    such as data sovereignty rules, is essential to avoid potential fines and lawsuits
    resulting from violations, safeguarding the organization’s financial and reputational
    standing.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy becomes pertinent as AI capabilities improve with increased data,
    including personal information. The ethical use of personal data for analysis
    and model training requires careful consideration to prevent privacy infringements.
  prefs: []
  type: TYPE_NORMAL
- en: The adversarial attack risk introduces a new dimension of threats, wherein malicious
    actors can manipulate training data to cause the resulting model to behave incorrectly
    in targeted scenarios. For instance, manipulation of training data labels can
    lead to models learning incorrectly and producing inaccurate results. These data-related
    risks demand meticulous attention and mitigation strategies to ensure the successful
    and ethical deployment of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Risk mitigations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To effectively mitigate data-related risks, comprehensive strategies and mechanisms
    must be in place to address issues related to data quality, bias, human errors,
    and regulatory compliance requirements. These mechanisms encompass various initiatives,
    including the implementation of robust data validation methods, the establishment
    of consistent definitions and standards for data selection and sampling, regular
    reviews of data quality and completeness, and the provision of guidance on mitigation
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: To address data selection and sampling risks, it is crucial to establish standards
    and consistent definitions for different data types and sources, ensuring uniformity
    in data selection across various sources. These standards should encompass considerations
    for relevancy, data gaps, bias, and representation, providing guidance on mitigation
    approaches such as extending data sources, employing synthetic data, and utilizing
    appropriate sampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For mitigating data quality risks, stringent measures should be taken to verify
    that the minimum data quality standards are met to support high-quality data processing
    and model training. This includes the establishment of rules and sample data reviews
    for accuracy, completeness, consistency, and the valid representation of the target
    population.
  prefs: []
  type: TYPE_NORMAL
- en: To address data labeling risks, controls should be implemented in the data labeling
    process to ensure consistency and mitigate subjective bias. Additionally, sample
    testing the validity of dataset labels can further enhance the quality and accuracy
    of the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of regulatory compliance and privacy checks, a robust set of checks
    should be integrated into the MRM process. This involves establishing enhanced
    controls around data access, ownership, collection, storage, transmission, and
    assessment to satisfy regulatory requirements. Additionally, comprehensive regulatory
    compliance checks for data privacy protection should be embedded in the MRM process,
    linking these controls to the enterprise access and authentication platform for
    centralized governance. The enforcement of data encryption and data masking should
    be applied where necessary to bolster privacy
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the mitigation strategy and mechanisms for data-related risks should
    be tailored to the specific needs of the organization and continuously reviewed
    and updated to keep up with the evolving landscape of AI/ML technologies and their
    associated risks.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation and model development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During this phase of the project lifecycle, data scientists utilize various
    algorithms and datasets to experiment and develop models to address business problems.
    The risks associated with this phase of the project lifecycle are mostly specific
    to AI/ML. They encompass a wide range of topics, such as algorithm selection and
    associated assumptions, limitations, model validation and robustness, model transparency
    and explainability, model fairness, compliance, and intended model use.
  prefs: []
  type: TYPE_NORMAL
- en: Risk considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The risk associated with model assumptions and limitations arises from the potential
    inaccuracies, incompleteness, or inconsistencies in assumptions and limitations,
    leading the model to inadequately fit the situation.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the linear regression algorithm assumes a linear relationship
    between predictor and response variables, and if such a relationship doesn’t exist,
    the predictions may be incorrect or biased. Certain algorithms may also have limitations
    on data sample size, impacting their performance with small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection introduces risks related to overfitting when models are chosen
    based solely on training performance and a lack of interpretability when there’s
    a requirement for explainability. Inadequate sensitivity and scenario analysis
    pose risks to model robustness, as a failure to understand the sensitivity of
    a credit risk model may lead to incorrect predictions. Similarly, financial forecast
    models that consider only a limited range of economic scenarios may fail to function
    correctly during unexpected events.
  prefs: []
  type: TYPE_NORMAL
- en: Model transparency risk arises from a lack of transparency hindering the explainability
    and verification of model decisions, potentially leaving organizations legally
    vulnerable if they cannot justify AI-driven decisions. Model fairness risk acknowledges
    that both data and the model itself can introduce bias, impacting the fairness
    of the AI system. For instance, Naïve Bayes algorithms may introduce bias if they
    assume independence among features, leading to incorrect predictions when features
    are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation risk stems from inadequate independent validation or the use
    of incorrect validation methods or metrics, posing the threat of unexpected behavior
    when models are not thoroughly tested. Model use and impact risk encompass the
    potential negative consequences arising from real-world deployment, as models
    trained on historical data may perform poorly if the future differs significantly
    from the past.
  prefs: []
  type: TYPE_NORMAL
- en: Missing lineage risk emphasizes the importance of understanding the lineage
    from the data source to model artifacts, including all transformations and experimentations
    during the modeling process, to comprehend model behavior and identify the root
    cause of issues.
  prefs: []
  type: TYPE_NORMAL
- en: Risk mitigations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To mitigate these risks, organizations must establish comprehensive MRM standards
    that cover model evaluation, validation, selection, and fairness. Additionally,
    the organization should enhance its capabilities and best practices for recognizing
    assumptions and limitations, addressing known gaps, and ensuring model transparency
    and lineage through the following approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: For model assumption and limitation risk, it is crucial to clearly define and
    validate the underlying assumptions of algorithms. Test difficult-to-validate
    assumptions with various techniques, ensuring completeness, and calculate model
    uncertainty to determine confidence levels in outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To mitigate risks associated with model selection, data scientists should develop
    a robust set of candidate models, undergo diverse team reviews, and involve technical,
    business, and target audience representatives to ensure the selected model effectively
    addresses problems. The modeling approach should be assessed on whether it is
    fit for purpose, explainable, reproducible, and robust, with documented decisions
    and supporting evidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address deficiencies in sensitivity and scenario testing, standards for model
    sensitivity and scenario testing should be established within the MRM framework.
    These procedures, integral to the development process, offer insights into boundary
    conditions impacting model robustness, minimize errors, and enhance comprehension
    of input-output interplay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To tackle model transparency risks, establish standards promoting communication
    and feedback within the development team, ensuring transparency throughout the
    model development process. Thorough documentation, including model validation
    techniques, serves as corroborating evidence for transparency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For model fairness issues, define and incorporate model fairness standards.
    Embed fairness checks in the model lifecycle, involve stakeholders in issue mitigation,
    enhance governance techniques, and recognize the ongoing process of fixing discrimination
    in algorithmic systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate model performance evaluation standards within MRM for validation,
    involving business and technical stakeholders in issue discovery and mitigation.
    Employ standardized validation tools and techniques for consistent procedures,
    and conduct an end-to-end evaluation against agreed-upon standards, monitoring
    metrics during retraining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate model use and impact risks within MRM by verifying the understanding
    of decisions during design/deployment/validation. Conduct an impact assessment
    to assess risks against preset thresholds, and verify model outcomes for precision,
    consistency, relevance, and alignment with trustworthy AI criteria.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish mechanisms and technology capabilities to track model lineage from
    data source to deployment. Implement comprehensive metadata management, version
    control for relevant artifacts, a model registry, and auditing/logging mechanisms
    to understand changes made, by whom, and for what purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, a comprehensive MRM framework that includes technical standards, ongoing
    monitoring and updates, and a culture of ethical decision-making is crucial for
    organizations to effectively manage the risks associated with model development
    so they can be used in a trustworthy manner.
  prefs: []
  type: TYPE_NORMAL
- en: AI system deployment and operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During this stage of the lifecycle, organizations design and build technology
    environments for AI system/model deployment in production to handle real-world
    business workflows in the broader application ecosystem and establish operational
    processes and standards to monitor the environments and remediate production issues
    ranging from basic system failure to model performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Risk considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human supervision risk involves the crucial practice of subjecting AI models
    to human review before deployment into production to ensure ongoing suitability
    for their intended purpose, preventing the deployment of inadequately prepared
    systems that may lead to production problems or unforeseen outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Technology integration risk arises as AI systems are typically integrated into
    the broader technology ecosystem, supporting multiple business functions. Challenges
    may emerge when integrating with upstream and downstream systems for data transfer,
    model integration, or API integration, potentially causing compatibility issues,
    such as the wrong model version impacting different systems.
  prefs: []
  type: TYPE_NORMAL
- en: Technology scalability risk is associated with unexpected spikes in data volumes,
    business users, and customers after AI system/model deployment. Failure to handle
    scalability scenarios can negatively impact business and user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance and behavior change risk stems from AI systems being developed
    using historical data. Unexpected changes in real-world environments, like data
    drift and outlier conditions, can cause the model to behave differently from the
    original assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Fallback procedure risk underscores the importance of well-established fallback
    procedures when production issues are detected. Inadequate fallback protocols
    can jeopardize system operation continuity.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks present new threats to AI systems. Adversarial attacks such
    as feeding bad data to AI systems can lead to incorrect predictions and faulty
    downstream decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Risk mitigations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To effectively manage risks during deployment and operations, robust mitigation
    mechanisms and technological capabilities are essential. Rigorous testing and
    operational checks, integration standards, model performance monitoring, established
    issue resolution processes, and adversarial monitoring and remediation throughout
    the AI lifecycle are key focus areas.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore specific recommendations in depth.
  prefs: []
  type: TYPE_NORMAL
- en: A crucial element for effective AI risk management is a model management system
    with a model registry. Providing details about models, performance metrics, uses,
    and related metadata, this system should incorporate MRM standards and embed operationalization
    checks. Stress testing and scaling simulation are vital to understanding behavior
    under heavy loads. Organizations should establish processes and tools for model
    owners to monitor, manage, govern, and analyze results.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate AI integration risks, organizations need to incorporate integration
    standards and requirements in risk management. Ensuring interoperability among
    platforms and conducting robust integration testing is crucial. Verification of
    proper configuration and integration into the production environment is essential
    to prevent errors during migration or upgrading.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing model deployment review and approval standards is crucial, encompassing
    a detailed review of design, algorithms, testing results, and performance metrics.
    It should outline steps for mitigating potential deployment risks.
  prefs: []
  type: TYPE_NORMAL
- en: For operational continuity regarding performance and behavior changes, MRM should
    include model monitoring, performance issue tracking, and resolution standards.
    Continuous monitoring of statistical, technical, and business metrics, along with
    real-time circuit breakers, helps ensure the model operates as intended. Pre-specifying
    benchmark or legacy models for fallback options is useful in case performance
    boundaries are breached.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attack monitoring standards in MRM are crucial to prevent malicious
    input from causing model malfunctions. Effective testing, auditing techniques,
    and certification programs are needed to address AI model vulnerabilities. Leveraging
    research on adversarial attacks and model data leakage for vulnerability testing
    and ensuring robustness and resilience to various attacks is essential. Proactive
    cyber threat hunting should be instituted to detect and isolate advanced threats
    in networks.
  prefs: []
  type: TYPE_NORMAL
- en: What we have covered so far does not include all the risks we might encounter
    throughout the AI lifecycle and new emergent risks are coming up regularly. It
    is also not possible and potentially counterproductive to mitigate all the risks
    in practice. Organizations should determine the tolerance levels for the different
    risks, which are going to be highly contextual and application- and use case-specific.
    Other factors such as policies established by system owners and regulators, organizational
    priority, and resource considerations can also influence risk tolerance. It is
    also worth noting that risk tolerance is likely to change over time as the influencing
    factors evolve.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to prioritize the risk identified in the AI lifecycle.
    Organizations should recognize that not all risks are the same, and scarce resources
    should be allocated appropriately to address the different risks. The assessed
    risk levels and potential impact of an AI system should be used to prioritize
    the resource allocation to mitigate these risks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, AI risks are not isolated risks and should be considered and incorporated
    into the broader enterprise risk management strategies and processes. The roles
    and responsibilities of different players in managing risks will span different
    functional domains such as engineering, data science, cybersecurity, audit, and
    compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Designing ML platforms with governance and risk management considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML technology systems play a crucial role in the AI risk management process
    and activities. To begin with, these systems must be developed and constructed
    to comply with both internal and external policies and guidelines. Additionally,
    technology can aid in streamlining and automating ML governance procedures. The
    following figure illustrates the different ML governance touchpoints in an enterprise
    ML platform. It is important to know that ML technology alone can only help address
    a subset of AI risks; other enterprise security technology needs to be incorporated
    to form a more comprehensive governance and defense mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: ML platform and ML governance'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the ML governance touchpoints have been integrated
    into the MLOps architecture depicted in *Figure 9.4* of *Chapter 9*, *Designing
    an Enterprise ML Architecture with AWS ML Services*.
  prefs: []
  type: TYPE_NORMAL
- en: When an ML platform is built with AI risk management and governance in mind,
    it can gather and furnish the information to support the MRM programs while optimizing
    the risk management workflows. Online data stores, workflow applications, document-sharing
    systems, and model inventory databases are among the technology solutions employed
    for AI governance. In the following sections, let’s delve deeper into some of
    the core ML governance components and see where an ML platform or technology can
    fit in.
  prefs: []
  type: TYPE_NORMAL
- en: Data and model documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the essential elements of AI governance is documentation. All models
    used for decision-making should be properly documented. The scope of the documentation
    may include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data overview, data quality report on the valuation, and assessment of the input
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model development document including methodology and assumption, model usage
    instructions, performance and validation results, and other qualitative and quantitative
    analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validation strategy and report by the second and third lines of defense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance monitoring results and data drift reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model implementation and user acceptance testing reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The role of the ML platform in ML governance documentation is usually to provide
    data points that feed into the formal risk management documentation or generate
    some ready-to-use reports. Specifically, an ML platform should be able to track,
    store, and report the following data points:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality metrics such as data description, statistics, bias, and errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model metrics and validation results in development and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model bias and explainability reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance monitoring results in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model description and intended use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk rating and classification details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different ML platforms have varying capabilities supporting the AI governance
    documentation requirements. Here, we will discuss the various capabilities of
    SageMaker in supporting these requirements. SageMaker can produce data and documents
    to be incorporated into model risk documentation. This includes tracking and producing
    information that is relevant for AI governance documentation, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model metrics**: The SageMaker training service tracks model metrics such
    as training errors and validation errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data and model bias reports**: SageMaker Clarify is the bias detection component
    in SageMaker. If you enable SageMaker Clarify, you can get data and model bias
    reports for the training data and trained model. The data and model bias reports
    have details such as imbalances in training data and prediction behavior across
    different age groups and genders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model explainability reports**: SageMaker Clarify also provides a model explainability
    feature. It uses SHAP to explain the contribution of each input to the final decision.
    You can get more details about SHAP at [https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Card**: SageMaker Model Card can be used to document critical information
    about ML models, such as the intended use of the model, risk rating, and detailed
    descriptions of model training and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both open-source and managed model registry platforms are available for managing
    model registries. For example, the MLflow Model Registry is an open-source option,
    while Amazon SageMaker offers a managed model registry service. The SageMaker
    Model Registry has several key capabilities that can aid in ML governance activities
    and processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model inventory**: All versions of the different models belong to a respective
    model group in the SageMaker Model Registry. You can view all the model groups
    and different versions of a model here. Metadata such as model metrics, training
    job details, hyperparameters used for training, and training data sources are
    important data points for the model reviews and model audit processes. Depending
    on specific business requirements, you can set up a central model registry for
    a single enterprise view, or distributed model registries if that will meet the
    governance and audit requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model approval and lifecycle tracking**: You can track the approval of models
    and model stages directly inside the SageMaker Model Registry. This detail helps
    the business operations and audit to ensure the proper processes are followed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitoring models after deployment is crucial to identify any potential failures
    and take prompt remedial action to mitigate risks. To ensure smooth functioning,
    models must be monitored for system availability and errors, as well as data and
    model drift and prediction failure. Amazon SageMaker offers a model monitoring
    feature that can detect both data drift and model drift. SageMaker Model Monitor
    provides the following capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift**: With SageMaker Model Monitor, you can monitor data quality
    issues and data distribution skews (aka data drift) in production. To use this
    feature, you create a baseline using a baseline dataset, such as a model training
    dataset, to get data statistics and data types and suggest constraints for monitoring.
    SageMaker Model Monitor can capture live inference traffic, calculate data statistics,
    examine data types, verify them against constraints, and trigger alerts. For example,
    if a feature’s mean and standard deviation change significantly from the baseline,
    an alert can be triggered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance drift**: You can use SageMaker Model Monitor to detect
    model performance changes in production. To use this feature, you create a model
    performance baseline job using a baseline dataset that contains both the inputs
    and labels. The baseline job will suggest constraints, which are the metrics thresholds
    that Model Monitor will monitor against the metrics to be calculated with ground
    truth data collected in production. Metrics can be optionally sent to CloudWatch
    for visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature attribution drift**: When enabled with SageMaker Clarify, SageMaker
    Model Monitor can report feature attribution drift. Feature attributions are indicators
    of feature importance to the prediction output. Similar to data and model drift,
    you create a SHAP baseline job using baseline data to generate constraint suggestions.
    The separate monitoring job is then scheduled to monitor predictions in production
    against the baseline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lineage and reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MRM requires establishing lineage across data and models to ensure reproducibility.
    Lineage is the systematic tracking and documentation of the origin, transformations,
    and dependencies of data, as well as the development and evolution of ML models,
    providing transparency and accountability throughout the entire process. Lineage
    information includes training data sources, algorithm selection, hyperparameter
    configurations, and the model training script. SageMaker offers several features
    that aid in establishing lineage:'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker training jobs keep lineage data such as the training data source,
    training job container (contains the algorithm and training script), hyperparameter
    configuration, and model artifact location. Historical training job data is immutable
    in the SageMaker environment for record retention purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Experiments and ML Lineage Tracking can contain additional component
    details such as data processing for more complete lineage tracking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker hosting has information on the location of the original model artifact
    and the inference container to trace the lineage from the model to the endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lineage data points, such as training data source and training configurations,
    are available by calling the SageMaker API; an external application can call the
    SageMaker API directly to extract this data for review purposes. Alternatively,
    a data extraction job can be developed to extract these data points and load them
    into a purpose-built risk management store for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The significance of ML privacy is growing rapidly in the implementation of
    ML systems. To comply with data privacy regulations or internal data privacy controls,
    ML systems must have fundamental infrastructure security features, such as data
    encryption, network isolation, compute isolation, and private connectivity. By
    utilizing a SageMaker-based ML platform, you can enable the following essential
    security controls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Private networking**: As SageMaker is a fully managed service, it runs in
    an AWS-owned account. By default, resources in your own AWS account communicate
    with SageMaker APIs via the public internet. To enable private connectivity to
    SageMaker components from your own AWS environment, you can attach them to a subnet
    in your own **virtual private cloud** (**VPC**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage encryption**: Data-at-rest encryption can be enabled by providing
    an encryption key when you create a SageMaker notebook, a training job, a processing
    job, or a hosting endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disabling internet access**: By default, the SageMaker notebook, training
    job, and hosting service have access to the internet. Internet access can be disabled
    via configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability and auditing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auditing primarily concentrates on process verification and artifact collection
    to support audit activities. The platform on which the process takes place usually
    functions as an information source for collecting artifacts. For instance, suppose
    there is an MRM policy that necessitates approval before deploying a model into
    production. In that case, the audit will require access to the system of records
    to ensure that the required data is collected and retained.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker and other related services can be a data source in support of the
    overall audit process. Specifically, it provides the following information that
    can be relevant for auditing purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Activity and access audit trail**: SageMaker sends all audit trail data to
    CloudWatch Logs, which can be retained and analyzed for audit purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model approval tracking**: Model deployment approvals are tracked in the
    SageMaker Model Registry. This can be provided to the auditor as evidence that
    required approval processes are followed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lineage tracking**: SageMaker Experiments and ML Lineage Tracking components
    can track and retain model lineages such as data processing, model training, and
    model deployment. Lineage Tracking information helps the auditor verify that the
    model can be reproduced using its original data and configuration dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration changes**: System configuration data is captured in AWS CloudTrail
    as change events. For example, when a SageMaker endpoint is deleted, there will
    be an entry in CloudTrail indicating this change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability and performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To mitigate the potential scalability risk, AI systems should be designed to
    handle dynamic and unexpected loads. For an ML platform, this usually means that
    the training infrastructure is designed and implemented to support a single large
    training job as well as many training jobs running in parallel. Similarly, the
    model hosting infrastructure should be capable of handling a large number of models
    running in parallel as well as running a large number of model instances across
    many nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your platform of choice is SageMaker, then the following capabilities can
    help mitigate training and hosting scaling challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training infrastructure scaling**: SageMaker has support for large-scale
    distributed training, with the ability to utilize hundreds of nodes and thousands
    of CPUs/GPUs. Additionally, SageMaker provides a purpose-built library for running
    both data-parallel and model-parallel training jobs. For storage scaling, high-performance
    storage solutions like **Elastic File System** (**EFS**) and FSx can be mounted
    onto SageMaker training nodes to accommodate training jobs that require a large-scale
    dataset exceeding 1 TB. AWS accounts can run multiple training jobs in parallel,
    and the soft limit can be increased upon request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosting infrastructure scaling:** SageMaker offers several options to scale
    model hosting needs. The **Multi-Model Endpoint** (**MME**) capability allows
    you to host multiple models behind a single endpoint while reducing costs. SageMaker’s
    automatic scaling feature enables you to define a scaling policy based on metrics
    such as the number of invocations per host, which can increase the number of instances
    running the same model automatically when the traffic increases. Additionally,
    the serverless inference option allows you to run a single model concurrently
    up to the maximum number supported by SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data quality checks should take place in multiple phases of the lifecycle, including
    data acquisition and processing, exploratory data analysis and data wrangling,
    feature engineering, and model inference. The checks should cover many aspects
    of data quality, such as missing data, data accuracy, inconsistency across different
    sources, incorrect format, incompleteness, imbalanced data, and duplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an AWS technology perspective, there are several purpose-built tools and
    features that can help with data quality management:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Glue DataBrew offers a range of data quality features that can help ensure
    the accuracy and reliability of data used for analysis or model training. DataBrew
    is mainly used by data engineers who are responsible for sourcing and cleaning
    the data during the data acquisition and processing phase for downstream users
    such as data scientists. Some of these features include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data profiling**: DataBrew can automatically profile datasets to identify
    data quality issues, such as missing or inconsistent values, outliers, or duplicates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleaning**: DataBrew provides a range of data cleaning transformations
    that can be applied to address common data quality issues, such as filling in
    missing values, removing duplicates, or standardizing data formats.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: DataBrew can perform data validation checks to ensure
    that data values fall within expected ranges or conform to predefined standards
    or formats.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lineage**: DataBrew tracks the lineage of data transformations to help
    ensure that data is being processed correctly and that any changes can be traced
    back to their source.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data versioning**: DataBrew supports the versioning of datasets, making it
    easy to track changes and roll back to previous versions if necessary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Data Wrangler offers some similar data quality capabilities, targeting
    mainly data scientists who are doing data exploratory analysis and feature engineering
    in the SageMaker environment. The Data Quality and Insights report in Data Wrangler
    can automatically verify data quality (such as missing values, duplicate rows,
    and data types) and help detect anomalies (such as outliers, class imbalance,
    and data leakage) in your data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, ML technology systems serve as pivotal assets in the AI risk
    management landscape. The foundational step involves aligning these systems with
    internal and external policies, ensuring robust compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, leveraging tools like SageMaker and MLOps systems emerges as a
    strategic approach, offering substantial support in documentation, lineage tracking,
    data management, and quality assurance. By enhancing observability and enabling
    thorough auditing, these technologies enable organizations to navigate the complexities
    of AI risk management with efficiency and precision.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter delved into several areas related to AI risk management and the
    technology platforms that support it. By now, you should have a solid understanding
    of the key AI-related risk scenarios, why AI risk management is critical, and
    how to detect and address potential risks throughout the AI lifecycle. Additionally,
    you should be aware of the significance of ML platforms in supporting AI risk
    management. It is worth noting that AI risk is a vast and complex domain with
    many unresolved risk challenges and new emergent risks arising rapidly. Moreover,
    the fast advancement in AI technology and adoption is also creating new risk exposure
    that risk management professionals must constantly address.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into several specific AI risk topics
    and mitigation techniques, including bias, model explainability, model robustness,
    and adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  prefs: []
  type: TYPE_NORMAL
