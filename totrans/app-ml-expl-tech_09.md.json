["```py\n!pip install --upgrade numpy pandas matplotlib seaborn sklearn lightgbm shap\n```", "```py\nimport numpy as np\n```", "```py\nimport pandas as pd\n```", "```py\nimport seaborn as sns\n```", "```py\nimport matplotlib.pyplot as plt\n```", "```py\nimport sklearn\n```", "```py\nimport lightgbm as lgb\n```", "```py\nimport shap\n```", "```py\nshap.initjs()\n```", "```py\ndata  = pd.read_csv('datasets/german_credit_data.csv', index_col=0)\n```", "```py\ndata.head()\n```", "```py\nsns.displot(\n```", "```py\n    data=data.isna().melt(value_name=\"missing\"),\n```", "```py\n    y=\"variable\",\n```", "```py\n    hue=\"missing\",\n```", "```py\n    multiple=\"fill\",\n```", "```py\n    aspect=1.5,\n```", "```py\n    palette='seismic'\n```", "```py\n)\n```", "```py\nplt.show()\n```", "```py\ndata.fillna('Unknown', inplace=True)\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n```", "```py\nle = LabelEncoder()\n```", "```py\nfor feat in ['Sex', 'Housing', 'Saving accounts', 'Checking account', 'Purpose', 'Risk']:\n```", "```py\n    le.fit(data[feat])\n```", "```py\n    data[feat] = le.transform(data[feat])\n```", "```py\nfrom sklearn.model_selection import train_test_split\n```", "```py\nfeatures = data.drop(columns=['Risk'])\n```", "```py\nlabels = data['Risk']\n```", "```py\n# Dividing the data into training-test set with 80:20 split ratio\n```", "```py\nx_train,x_test,y_train,y_test = \\\n```", "```py\ntrain_test_split(features,labels,test_size=0.2, \n```", "```py\n                 random_state=123)\n```", "```py\ndata_train = lgb.Dataset(x_train, label=y_train, \n```", "```py\n                         categorical_feature=cat_features)\n```", "```py\ndata_test = lgb.Dataset(x_test, label=y_test, \n```", "```py\n                        categorical_feature=cat_features)\n```", "```py\nparams = {\n```", "```py\n    'boosting_type': 'gbdt',\n```", "```py\n    'objective': 'binary',\n```", "```py\n    'metric': 'auc',\n```", "```py\n    'num_leaves': 20,\n```", "```py\n    'learning_rate': 0.05,\n```", "```py\n    'feature_fraction': 0.9,\n```", "```py\n    'bagging_fraction': 0.8,\n```", "```py\n    'bagging_freq': 5,\n```", "```py\n    'verbose': -1,\n```", "```py\n    'lambda_l1': 1,\n```", "```py\n    'lambda_l2': 1,\n```", "```py\n    'seed': 123\n```", "```py\n}\n```", "```py\nmodel = lgb.train(params,\n```", "```py\n                  data_train,\n```", "```py\n                  num_boost_round=100,\n```", "```py\n                  verbose_eval=100,\n```", "```py\n                  valid_sets=[data_test, data_train])\n```", "```py\nexplainer = shap.TreeExplainer(model)\n```", "```py\nshap_values = explainer.shap_values(features)\n```", "```py\n# Local explainability with force plots\n```", "```py\nshap.force_plot(explainer.expected_value[1], shap_values[1][0,:], features.iloc[0,:])\n```", "```py\n# Local explainability with force plots\n```", "```py\nshap.decision_plot(explainer.expected_value[1], shap_values[1][0,:], features.iloc[0,:])\n```", "```py\n# For feature wise global interpretability\n```", "```py\nfor col in ['Purpose', 'Age']:\n```", "```py\n    print(f\"Feature Dependence plot for: {col}\")\n```", "```py\n    shap.dependence_plot(col, shap_values[1], features, display_features=features)\n```", "```py\nX,y = shap.datasets.imagenet50(resolution=224)\n```", "```py\ninference_image = X[[46]] \n```", "```py\nfrom tensorflow.keras.applications.vgg19 import VGG19\n```", "```py\nmodel = VGG19(weights='imagenet')\n```", "```py\nlayer_num = 10\n```", "```py\n# explain how the input to the 10th layer of the model explains the top two classes\n```", "```py\ndef map2layer(x, layer):\n```", "```py\n    '''\n```", "```py\n    Source : https://github.com/slundberg/shap\n```", "```py\n    '''\n```", "```py\n    feed_dict = dict(zip([model.layers[0].input],\n```", "```py\n                         [preprocess_input(x.copy())]))\n```", "```py\n    return K.get_session().run(model.layers[layer].input, feed_dict)\n```", "```py\nmodel_input = (model.layers[layer_num].input, \n```", "```py\n               model.layers[-1].output)\n```", "```py\nexplainer = shap.GradientExplainer(model_input,\n```", "```py\n                                   map2layer(X, layer_num),\n```", "```py\n                                   local_smoothing=0)\n```", "```py\nshap_values, ind = explainer.shap_values(\n```", "```py\n    map2layer(inference_image, layer_num), \n```", "```py\n    ranked_outputs=4)\n```", "```py\n# plot the explanations\n```", "```py\nshap.image_plot(shap_values, inference_image, class_names)\n```", "```py\nbackground = x_train[np.random.choice(len(x_train), 1000, replace=False)]\n```", "```py\nexplainer = shap.DeepExplainer(model, background)\n```", "```py\nshap_values = explainer.shap_values(sample_x_test)\n```", "```py\nshap.image_plot(shap_values, sample_x_test, labels, labelpad= 1)\n```", "```py\nexplainer = shap.KernelExplainer(model.predict, x_train)\n```", "```py\nshap_values = explainer.shap_values(x_test, nsamples=100)\n```", "```py\nshap.summary_plot(shap_values, x_test, plot_type='violin', show=False)\n```", "```py\nshap.force_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])\n```", "```py\nshap.decision_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])\n```", "```py\nexplainer = shap.LinearExplainer(model, x_train, feature_dependence=\"independent\")\n```", "```py\nshap_values = explainer.shap_values(x_test)\n```", "```py\nshap.summary_plot(shap_values, x_test, plot_type='violin', show=False)\n```", "```py\nshap.force_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])\n```", "```py\nshap.dependence_plot(\"alcohol\", shap_values, x_test, show=False)\n```", "```py\n!pip install --upgrade transformers\n```", "```py\nimport transformers\n```", "```py\nmodel = transformers.pipeline('sentiment-analysis', return_all_scores=True)\n```", "```py\ntext_data = \"Hugging Face transformers are absolutely brilliant!\"\n```", "```py\nmodel(text_data)[0]\n```", "```py\n[{'label': 'NEGATIVE', 'score': 0.00013269631017465144},\n```", "```py\n {'label': 'POSITIVE', 'score': 0.99986732006073}]\n```", "```py\nexplainer = shap.Explainer(model) \n```", "```py\nshap_values = explainer([text_data])\n```", "```py\nshap.plots.text(shap_values[0,:,'POSITIVE'])\n```", "```py\nshap.plots.bar(shap_values[0,:,'POSITIVE'])\n```", "```py\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\", use_fast=True)\n```", "```py\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\").cuda()\n```", "```py\n# build a pipeline object to do predictions\n```", "```py\npipeline = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)\n```", "```py\nexplainer = shap.Explainer(pipeline)\n```", "```py\nshap_values = explainer([text_data])\n```", "```py\nshap.plots.text(shap_values[0])\n```", "```py\nmodel = AutoModelForSequenceClassification.from_pretrained(\"valhalla/distilbart-mnli-12-3\")\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(\"valhalla/distilbart-mnli-12-3\")\n```", "```py\nclass ZSLPipeline(ZeroShotClassificationPipeline):\n```", "```py\n    # Overwrite the __call__ method\n```", "```py\n    def __call__(self, *args):\n```", "```py\n        out = super().__call__(args[0], self.set_labels)[0]\n```", "```py\n        return [[{\"label\":x[0], \"score\": x[1]}  for x in zip(out[\"labels\"], out[\"scores\"])]]\n```", "```py\n    def set_labels(self, labels: Union[str,List[str]]):\n```", "```py\n        self.set_labels = labels\n```", "```py\ntext = [\"I love playing cricket!\"]\n```", "```py\nlabels = [\"insect\", \"sports\", \"animal\"]\n```", "```py\nmodel.config.label2id.update({v:k for k,v in enumerate(labels)})\n```", "```py\nmodel.config.id2label.update({k:v for k,v in enumerate(labels)})\n```", "```py\npipe = ZSLPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n```", "```py\npipe.set_labels(labels)\n```", "```py\nexplainer = shap.Explainer(pipe)\n```", "```py\nshap_values = explainer(text)\n```", "```py\nshap.plots.text(shap_values)\n```", "```py\nshap.plots.bar(shap_values[0,:,'sports'])\n```"]