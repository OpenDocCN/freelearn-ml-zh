<html><head></head><body>
<div id="_idContainer279">
<h1 class="chapter-number" id="_idParaDest-136"><a id="_idTextAnchor839"/><a id="_idTextAnchor840"/><a id="_idTextAnchor841"/><span class="koboSpan" id="kobo.1.1">13</span></h1>
<h1 id="_idParaDest-137"><a id="_idTextAnchor842"/><span class="koboSpan" id="kobo.2.1">Evaluating Performance Metrics</span></h1>
<p><span class="koboSpan" id="kobo.3.1">No model of a real-world phenomenon is perfect. </span><span class="koboSpan" id="kobo.3.2">There are countless statistical assumptions made about the underlying data, there is noise in the measurements, and there are unknown and unmodeled factors that contribute to the output. </span><span class="koboSpan" id="kobo.3.3">But even though it is not perfect, a good model is still informative and valuable. </span><span class="koboSpan" id="kobo.3.4">So, how do you know whether you have such a good model? </span><span class="koboSpan" id="kobo.3.5">How can you be sure your predictions for the future can be trusted? </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Cross-validation</span></strong><span class="koboSpan" id="kobo.5.1"> got </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.6.1">us part of the way there, by providing a technique to compare unbiased predictions to actual values. </span><span class="koboSpan" id="kobo.6.2">This chapter is all about how to compare </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">different models.</span></span></p>
<p><span class="koboSpan" id="kobo.8.1">Prophet features several different metrics that are used for comparing your actual values with your predicted values, so you can quantify the performance of your model. </span><span class="koboSpan" id="kobo.8.2">This tells you how good your model actually is and whether you can trust the predictions, and helps you compare the performance of different models so you can choose which one </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">is best.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">This chapter will teach you about </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">Prophet’s metrics</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Creating a Prophet performance </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">metrics DataFrame</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Handling </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">irregular cut-offs</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Tuning hyperparameters with </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">grid search</span></span><a id="_idTextAnchor843"/><a id="_idTextAnchor844"/></li>
</ul>
<h1 id="_idParaDest-138"><a id="_idTextAnchor845"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">The data files and code for examples in this chapter can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">at </span></span><a href="https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.24.1">.</span></span><a id="_idTextAnchor846"/><a id="_idTextAnchor847"/></p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor848"/><span class="koboSpan" id="kobo.25.1">Understanding Prophet’s metrics</span></h1>
<p><span class="koboSpan" id="kobo.26.1">Prophet’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">diagnostics</span></strong><span class="koboSpan" id="kobo.28.1"> package </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.29.1">provides six different metrics you can use to evaluate your model. </span><span class="koboSpan" id="kobo.29.2">Those metrics are mea</span><a id="_idTextAnchor849"/><span class="koboSpan" id="kobo.30.1">n squared error, root mean squared error, mean absolute error, mean absolute percent error, median absolute percent error, and coverage. </span><span class="koboSpan" id="kobo.30.2">We’ll discuss each of these </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">in turn</span><a id="_idTextAnchor850"/><a id="_idTextAnchor851"/><span class="koboSpan" id="kobo.32.1">.</span></span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor852"/><span class="koboSpan" id="kobo.33.1">Mean squared error</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.34.1">Mean squared error</span></strong><span class="koboSpan" id="kobo.35.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.36.1">MSE</span></strong><span class="koboSpan" id="kobo.37.1">) is the</span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.38.1"> sum of the</span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.39.1"> squ</span><a id="_idTextAnchor853"/><span class="koboSpan" id="kobo.40.1">ared difference between e</span><a id="_idTextAnchor854"/><span class="koboSpan" id="kobo.41.1">ach predicted value and the actual value, as can be seen in the </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">following equation:</span></span></p>
<table class="No-Table-Style" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<div>
<div class="IMG---Figure" id="_idContainer255">
<span class="koboSpan" id="kobo.43.1"><img alt="" src="image/B19630_13_F01.jpg"/></span>
</div>
</div>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.44.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">1)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.46.1">The number of samples is represented in the preceding equation by </span><span class="koboSpan" id="kobo.47.1"><img alt="" src="image/B19630_13_F02.png"/></span><span class="koboSpan" id="kobo.48.1">, where </span><span class="koboSpan" id="kobo.49.1"><img alt="" src="image/B19630_13_F03.png"/></span><span class="koboSpan" id="kobo.50.1"> is an actual value and </span><span class="koboSpan" id="kobo.51.1"><img alt="" src="image/B19630_13_F14.png"/></span><span class="koboSpan" id="kobo.52.1"> is a </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">forecasted value.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">MSE may be the most used performance metric, but it does have its downside. </span><span class="koboSpan" id="kobo.54.2">Because it is not scaled to the data, its value is not easy to interpret – the unit of MSE is the square of your </span><strong class="source-inline"><span class="koboSpan" id="kobo.55.1">y</span></strong><span class="koboSpan" id="kobo.56.1"> unit. </span><span class="koboSpan" id="kobo.56.2">It is also sensitive to outliers, although this may be either desirable or undesirable, depending upon your data </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">and interpretation.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">However, it remains popular because it can be proven that MSE is equal to the bias squared plus the variance, so minimizing this metric can reduce both bias and variance. </span><span class="koboSpan" id="kobo.58.2">MSE is never negative and the closer it is to zero, the better </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">the model.</span></span></p>
<h2 id="_idParaDest-141"><span class="koboSpan" id="kobo.60.1">Root m</span><a id="_idTextAnchor855"/><a id="_idTextAnchor856"/><span class="koboSpan" id="kobo.61.1">ean squared error</span></h2>
<p><span class="koboSpan" id="kobo.62.1">If you scale </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.63.1">MSE to the same units as that of your data by takin</span><a id="_idTextAnchor857"/><span class="koboSpan" id="kobo.64.1">g</span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.65.1"> the squa</span><a id="_idTextAnchor858"/><span class="koboSpan" id="kobo.66.1">re root, you arrive at the </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">root mean squared </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.68.1">error</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.69.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.70.1">RMSE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">):</span></span></p>
<table class="No-Table-Style" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.72.1"><img alt="" src="image/B19630_13_F05.png"/></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.73.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">2)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.75.1">RMSE shares the same advantages and </span><a id="_idTextAnchor859"/><span class="koboSpan" id="kobo.76.1">disadvantages as MSE, although its units are more interpretable. </span><span class="koboSpan" id="kobo.76.2">As with MSE, it places more importance on</span><a id="_idTextAnchor860"/><span class="koboSpan" id="kobo.77.1"> points with large errors than those with </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">small errors.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor861"/><a id="_idTextAnchor862"/><span class="koboSpan" id="kobo.79.1">Mean absolute error</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.80.1">Mean absolute error</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">MAE</span></strong><span class="koboSpan" id="kobo.83.1">) is</span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.84.1"> similar</span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.85.1"> t</span><a id="_idTextAnchor863"/><span class="koboSpan" id="kobo.86.1">o MSE except that it tak</span><a id="_idTextAnchor864"/><span class="koboSpan" id="kobo.87.1">es the absolute value of the error, not </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">the square:</span></span></p>
<table class="No-Table-Style" id="table003-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.89.1"><img alt="" src="image/B19630_13_F06.png"/></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.90.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">3)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.92.1">MAE, in contrast with MSE and RMSE, weighs each error equally; it does not place more importance on outliers or points with uncommonly high errors. </span><span class="koboSpan" id="kobo.92.2">Like MSE though, MAE is not scaled to the data. </span><span class="koboSpan" id="kobo.92.3">So, if you find that your model reports an MAE of, say, 10, is this good or bad? </span><span class="koboSpan" id="kobo.92.4">If the average value in your dataset is 1,000, then an error of 10 would be just 1%. </span><span class="koboSpan" id="kobo.92.5">If the average of your data is 1, though, then an MAE of 10 would mean your predictions are off </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">by 1,000%!</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">In order to scale MAE to the data, it will often be divided by the data’s mean value, to arrive at </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">a percentage:</span></span></p>
<table class="No-Table-Style" id="table004-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.96.1"><img alt="" src="image/B19630_13_F07.png"/></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.97.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">4)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.99.1">This format for MAE is not</span><a id="_idTextAnchor865"/><span class="koboSpan" id="kobo.100.1"> supported in Prophet, although </span><a id="_idTextAnchor866"/><span class="koboSpan" id="kobo.101.1">you can create </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">it yourself.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor867"/><a id="_idTextAnchor868"/><span class="koboSpan" id="kobo.103.1">Mean absolute percent error</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.104.1">Mean absolute percent error</span></strong><span class="koboSpan" id="kobo.105.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.106.1">MAPE</span></strong><span class="koboSpan" id="kobo.107.1">) is </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.108.1">another </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.109.1">very common metric despite its poor ability to represent the performan</span><a id="_idTextAnchor869"/><span class="koboSpan" id="kobo.110.1">ce of a model. </span><span class="koboSpan" id="kobo.110.2">Not to b</span><a id="_idTextAnchor870"/><span class="koboSpan" id="kobo.111.1">e confused with the total MAE divided by the mean value, MAPE divides each error by the value of the data point at </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">that error:</span></span></p>
<table class="No-Table-Style" id="table005-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.113.1"><img alt="" src="image/B19630_13_F08.png"/></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.114.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">5)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.116.1">This makes the metric skewed to overly represent errors that occur when the data values are low. </span><span class="koboSpan" id="kobo.116.2">For this reason, MAPE is considered asymmetric—it puts a heavier penalty on negative errors (when the forecast is higher than the actual result) than on positive errors. </span><span class="koboSpan" id="kobo.116.3">Optimizing for MAPE will often leave your model undershooting the values it is targeting. </span><span class="koboSpan" id="kobo.116.4">Furthermore, because you are dividing by each </span><span class="koboSpan" id="kobo.117.1"><img alt="" src="image/B19630_13_F09.png"/></span><span class="koboSpan" id="kobo.118.1"> value, if any of them are zero, then the calculation will produce a division-by-zero error. </span><span class="koboSpan" id="kobo.118.2">Very small values of </span><span class="koboSpan" id="kobo.119.1"><img alt="" src="image/B19630_13_F10.png"/></span><span class="koboSpan" id="kobo.120.1"> will also cause floating-point calculation problems. </span><span class="koboSpan" id="kobo.120.2">Prophet will detect whether any </span><span class="koboSpan" id="kobo.121.1"><img alt="" src="image/B19630_13_F11.png"/></span><span class="koboSpan" id="kobo.122.1"> values are at or near zero and if found, it will simply skip MAPE calculations and proceed to the other metrics called for. </span><span class="koboSpan" id="kobo.122.2">The upside to MAPE, though, is that it has natural inter</span><a id="_idTextAnchor871"/><span class="koboSpan" id="kobo.123.1">pretability – it is easy to </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">intuitively understand.</span></span></p>
<h2 id="_idParaDest-144"><span class="koboSpan" id="kobo.125.1">Median</span><a id="_idTextAnchor872"/><a id="_idTextAnchor873"/><span class="koboSpan" id="kobo.126.1"> absolute percent error</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.127.1">Median absolute percent error</span></strong><span class="koboSpan" id="kobo.128.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.129.1">MdAPE</span></strong><span class="koboSpan" id="kobo.130.1">) is the</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.131.1"> same as MA</span><a id="_idTextAnchor874"/><span class="koboSpan" id="kobo.132.1">PE, except it uses the median instead of the mean. </span><span class="koboSpan" id="kobo.132.2">It</span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.133.1"> can be useful with noisy data when MAP</span><a id="_idTextAnchor875"/><span class="koboSpan" id="kobo.134.1">E may be the preferred metric but too many outliers are swaying it. </span><span class="koboSpan" id="kobo.134.2">For example, significant holidays can create large spikes in data and the median is able to smooth out predictions if MAPE </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">experiences issues.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor876"/><span class="koboSpan" id="kobo.136.1">Symmetric mean absolute percent error</span></h2>
<p><span class="koboSpan" id="kobo.137.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">symmetric mean absolute percent error</span></strong><span class="koboSpan" id="kobo.139.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.140.1">SMAPE</span></strong><span class="koboSpan" id="kobo.141.1">) attempts to overcome the asymmetric</span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.142.1"> deficiency</span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.143.1"> of MAPE </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">described previously.</span></span></p>
<table class="No-Table-Style" id="table006-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.145.1"><img alt="" src="image/B19630_13_F12.png"/></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.146.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">6)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.148.1">SMAPE is expressed as a percentage, which allows it to compare performance between datasets of varying magnitude. </span><span class="koboSpan" id="kobo.148.2">A shortcoming of SMAPE, though, is that it becomes unstable when both the actual value and forecast value are close to zero. </span><span class="koboSpan" id="kobo.148.3">The upper limit of the equation is 200%, which can feel a bit bizarre intuitively. </span><span class="koboSpan" id="kobo.148.4">For this reason, some formulations of the equation leave out the division by 2 in </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">the denominator.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor877"/><a id="_idTextAnchor878"/><span class="koboSpan" id="kobo.150.1">Coverage</span></h2>
<p><span class="koboSpan" id="kobo.151.1">The final </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.152.1">Prophet metric</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.153.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.154.1">coverage</span></strong><span class="koboSpan" id="kobo.155.1">. </span><span class="koboSpan" id="kobo.155.2">Coverage is simpl</span><a id="_idTextAnchor879"/><span class="koboSpan" id="kobo.156.1">y the percentage of actual values that lie between the predicted upper and lower uncertainty bounds. </span><span class="koboSpan" id="kobo.156.2">By default, the uncertainty limit</span><a id="_idTextAnchor880"/><span class="koboSpan" id="kobo.157.1">s cover 80% of the data, so your coverage value should </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">be 0.8.</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">If you find a coverage value that does not equal the </span><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">interval_width</span></strong><span class="koboSpan" id="kobo.161.1"> set during model instantiation, it means your model is not well calibrated to the uncertainty. </span><span class="koboSpan" id="kobo.161.2">In practice, this simply means that you probably cannot trust the stated uncertainty intervals in the future portions of your forecast and may want to adjust them based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">coverage value.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">And of </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.164.1">course, the cross-validation DataFrame contains all of your </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.165.1">actual </span><span class="koboSpan" id="kobo.166.1"><img alt="" src="image/B19630_13_F13.png"/></span><span class="koboSpan" id="kobo.167.1"> values and your model’s predicted </span><span class="koboSpan" id="kobo.168.1"><img alt="" src="image/B19630_13_F14.png"/></span><span class="koboSpan" id="kobo.169.1"> values, so any other metric you can come up with to compare those values, you can calculate and </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">use yourself.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor881"/><a id="_idTextAnchor882"/><span class="koboSpan" id="kobo.171.1">Choosing the best metric</span></h2>
<p><span class="koboSpan" id="kobo.172.1">Deciding which </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.173.1">performance</span><a id="_idTextAnchor883"/><span class="koboSpan" id="kobo.174.1"> metric to optimize your model with is not a trivial choice. </span><span class="koboSpan" id="kobo.174.2">It can have a significant impact on your final model, depending on the characteristics of the data. </span><span class="koboSpan" id="kobo.174.3">When worked out mathematically, it can be shown that optimizing your model for MSE will create a model predicting values close to the mean of your data, and optimizing for MAE will create predictions close to the median value. </span><span class="koboSpan" id="kobo.174.4">Optimizing for MAPE will tend to produce abnormally low forecasts because it applies such a high weight to errors occurring at low points in </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.176.1">So, between MSE (or RMSE) and MAE, which is better? </span><span class="koboSpan" id="kobo.176.2">RMSE aims to be correct to the average data point and MAE aims to overshoot the actual value as often as it undershoots. </span><span class="koboSpan" id="kobo.176.3">This difference will only materialize when the mean and median of your data are different – in highly skewed data. </span><span class="koboSpan" id="kobo.176.4">As the median will be further from the tail in skewed data than the mean will be, the MAE will introduce bias toward the bulk of data and away from the tail. </span><span class="koboSpan" id="kobo.176.5">A biased model is the greatest disadvantage </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">of MAE.</span></span></p>
<p><span class="koboSpan" id="kobo.178.1">MSE’s disadvantage is its</span><a id="_idTextAnchor884"/><span class="koboSpan" id="kobo.179.1"> sensitivity to outliers. </span><span class="koboSpan" id="kobo.179.2">Imagine a time series that is generally flat except for a couple of extreme outliers. </span><span class="koboSpan" id="kobo.179.3">MSE will really focus on the forecast errors of those outliers, so it will tend to miss the mark more often than MAE will. </span><span class="koboSpan" id="kobo.179.4">In general, the median is more robust to outliers than </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">the mean.</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">So, should we consider robustness to outliers a good thing? </span><span class="koboSpan" id="kobo.181.2">Not necessarily. </span><span class="koboSpan" id="kobo.181.3">If your time series is intermittent – that is, if most dates have a </span><span class="koboSpan" id="kobo.182.1"><img alt="" src="image/B19630_03_F02.png"/></span><span class="koboSpan" id="kobo.183.1"> value of 0 – you don’t want to target the median value but the mean. </span><span class="koboSpan" id="kobo.183.2">The median will be 0! </span><span class="koboSpan" id="kobo.183.3">In this case, you would desire MSE precisely because it is sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">to outliers.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">Unfortunately, there is no easy answer to which is the best metric to use. </span><span class="koboSpan" id="kobo.185.2">The analyst must pay attention to bias, skewness, and outliers to determine which metric will work best. </span><span class="koboSpan" id="kobo.185.3">And there is no reason you can’t try multiple metrics and see which forecast seems the most reasonable </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">to you!</span></span></p>
<h1 id="_idParaDest-148"><span class="koboSpan" id="kobo.187.1">C</span><a id="_idTextAnchor885"/><a id="_idTextAnchor886"/><span class="koboSpan" id="kobo.188.1">reating a Prophet performance metrics DataFrame</span></h1>
<p><span class="koboSpan" id="kobo.189.1">Now that you’ve </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.190.1">learned what </span><a id="_idTextAnchor887"/><span class="koboSpan" id="kobo.191.1">the different options are for performance metrics in Prophet, let’s start coding and see how to access them. </span><span class="koboSpan" id="kobo.191.2">We’ll use the same online retail sales data we used in </span><a href="B19630_12.xhtml#_idTextAnchor794"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.192.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.193.1">, </span><em class="italic"><span class="koboSpan" id="kobo.194.1">Performing Cross-Validation</span></em><span class="koboSpan" id="kobo.195.1">. </span><span class="koboSpan" id="kobo.195.2">Along with our usual imports, we are going to add the </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.197.1"> function from Prophet’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">diagnostics</span></strong><span class="koboSpan" id="kobo.199.1"> package and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">plot_cross_validation_metric</span></strong><span class="koboSpan" id="kobo.201.1"> function from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">plot</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.203.1"> package:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.204.1">
import pandas as pd
import matplotlib.pyplot as plt
from prophet import Prophet
from prophet.plot import add_changepoints_to_plot
from prophet.diagnostics import cross_validation
from prophet.diagnostics import performance_metrics
from prophet.plot import plot_cross_validation_metric</span></pre>
<p><span class="koboSpan" id="kobo.205.1">Next, let’s load the data, create our forecast, and plot </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">the results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.207.1">
df = pd.read_csv('online_retail.csv')
df.columns = ['ds', 'y']
model = Prophet(yearly_seasonality=4)
model.fit(df)
forecast = model.predict()
fig = model.plot(forecast)
add_changepoints_to_plot(fig.gca(), model, forecast)
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.208.1">Because we’re not interested in any</span><a id="_idTextAnchor888"/><span class="koboSpan" id="kobo.209.1"> future predictions, we don’t need to create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">future</span></strong><span class="koboSpan" id="kobo.211.1"> DataFrame. </span><span class="koboSpan" id="kobo.211.2">We’ll just focus on the 3 years of data </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">we’ve got:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer270">
<span class="koboSpan" id="kobo.213.1"><img alt="Figure 13.1 – Online retail sales forecast" src="image/Fig_13.1.jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><a id="_idTextAnchor889"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.214.1">Figure 13.1 – Online retail sales forecast</span></p>
<p><span class="koboSpan" id="kobo.215.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.217.1"> function </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.218.1">requires a cross-validation DataFrame as input, so we’ll create one in the same manner as you learned in </span><a href="B19630_12.xhtml#_idTextAnchor794"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.219.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.220.1">, </span><em class="italic"><span class="koboSpan" id="kobo.221.1">Performing Cross-Validation</span></em><span class="koboSpan" id="kobo.222.1">. </span><span class="koboSpan" id="kobo.222.2">We’ll set </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">horizon</span></strong><span class="koboSpan" id="kobo.224.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">90 days</span></strong><span class="koboSpan" id="kobo.226.1">, so each fold in the cross-validation will be </span><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">90 days</span></strong><span class="koboSpan" id="kobo.228.1">. </span><span class="koboSpan" id="kobo.228.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">period</span></strong><span class="koboSpan" id="kobo.230.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">30 days</span></strong><span class="koboSpan" id="kobo.232.1"> is how often to begin a new fold and </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">initial</span></strong><span class="koboSpan" id="kobo.234.1"> being set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">730 days</span></strong><span class="koboSpan" id="kobo.236.1"> is our first 2-year training period, untouched </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">by validation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.238.1">
df_cv = cross_validation(model,
                         horizon='90 days',
                         period='30 days',
                         initial='730 days',
                         parallel='processes')</span></pre>
<p><span class="koboSpan" id="kobo.239.1">Next, we’ll send </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">df_cv</span></strong><span class="koboSpan" id="kobo.241.1"> to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.243.1"> function. </span><span class="koboSpan" id="kobo.243.2">By default, this function will calculate each of the five available metrics. </span><span class="koboSpan" id="kobo.243.3">You can specify a subset of these by passing a list of metric </span><a id="_idTextAnchor890"/><span class="koboSpan" id="kobo.244.1">names to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">metrics</span></strong><span class="koboSpan" id="kobo.246.1"> argument. </span><span class="koboSpan" id="kobo.246.2">Let’s include all five and display the first few rows of the </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">resulting DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.248.1">
df_p = performance_metrics(df_cv)
df_p.head()</span></pre>
<p><span class="koboSpan" id="kobo.249.1">The</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.250.1"> output DataFrame is indexed by days in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">horizon</span></strong><span class="koboSpan" id="kobo.252.1">, so each row represents the values of those metrics when the model is asked to forecast that many days out. </span><span class="koboSpan" id="kobo.252.2">This is just the first five rows (your results may vary slightly due to randomness in the </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">optimization algorithm):</span></span></p>
<p class="IMG---Figure"><a id="_idTextAnchor891"/></p>
<div>
<div class="IMG---Figure" id="_idContainer271">
<span class="koboSpan" id="kobo.254.1"><img alt="Figure 13.2 – Performance metrics DataFrame" src="image/Fig_13.2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.255.1">Figure 13.2 – Performance metrics DataFrame</span></p>
<p><span class="koboSpan" id="kobo.256.1">You may be wondering why the first row in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">horizon</span></strong><span class="koboSpan" id="kobo.258.1"> column is </span><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">9 days</span></strong><span class="koboSpan" id="kobo.260.1">. </span><span class="koboSpan" id="kobo.260.2">Each metric value in the DataFrame is the rolling average of its calculation up to the day specified. </span><span class="koboSpan" id="kobo.260.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.262.1"> function takes a </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">rolling_window</span></strong><span class="koboSpan" id="kobo.264.1"> argument where you can change the window size, but the default is </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">0.1</span></strong><span class="koboSpan" id="kobo.266.1">. </span><span class="koboSpan" id="kobo.266.2">This number is the fraction of </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">horizon</span></strong><span class="koboSpan" id="kobo.268.1"> to include in the window. </span><span class="koboSpan" id="kobo.268.2">With 10% of our 90-day </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">horizon</span></strong><span class="koboSpan" id="kobo.270.1"> being 9 days, this is the first row of </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">the DataFrame.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">You can use this DataFrame on its own or you can visualize it with Prophet’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">plot_cross_validation_metric</span></strong><span class="koboSpan" id="kobo.274.1"> function. </span><span class="koboSpan" id="kobo.274.2">This function actually calls the </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.276.1"> function itself, so you do not need to create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">df_p</span></strong><span class="koboSpan" id="kobo.278.1"> first, just a </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">df_cv</span></strong><span class="koboSpan" id="kobo.280.1">. </span><span class="koboSpan" id="kobo.280.2">Here, we’ll plot the MAE by passing </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">'mae'</span></strong><span class="koboSpan" id="kobo.282.1"> to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">metric</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.284.1"> argument:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.285.1">
fig = plot_cross_validation_metric(df_cv, metric='mae')
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.286.1">The resulting plot shows each MAE measurement along the horizon and the rolling average value of </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">those</span><a id="_idTextAnchor892"/><span class="koboSpan" id="kobo.288.1"> measurements:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer272">
<span class="koboSpan" id="kobo.289.1"><img alt="Figure 13.3 – Cross-validation plot" src="image/Fig_13.3.jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><a id="_idTextAnchor893"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.290.1">Figure 13.3 – Cross-validation plot</span></p>
<p><span class="koboSpan" id="kobo.291.1">Our </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.292.1">cross-validation settings were </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">horizon='90 days', period='30 days', initial='730 days'</span></strong><span class="koboSpan" id="kobo.294.1">, which, for the 1 year of data remaining after the initial training period, resulted in a total of ten 90-day forecasts. </span><span class="koboSpan" id="kobo.294.2">So, for each day in our horizon, the preceding plot will have 10 MAE measurements. </span><span class="koboSpan" id="kobo.294.3">If you counted up all the dots on that plot, it should be 900. </span><span class="koboSpan" id="kobo.294.4">The solid line is the rolling average value, with the window size being the same default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">0.1</span></strong><span class="koboSpan" id="kobo.296.1">, as the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">performance_metrics</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.298.1"> DataFrame.</span></span></p>
<p><span class="koboSpan" id="kobo.299.1">You can specify this by using the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">rolling_window</span></strong><span class="koboSpan" id="kobo.301.1"> argument in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">plot_cross_validation_metric</span></strong><span class="koboSpan" id="kobo.303.1"> function. </span><span class="koboSpan" id="kobo.303.2">Just to make it very clear how this window size affects the plot, let’s compare</span><a id="_idTextAnchor894"/><span class="koboSpan" id="kobo.304.1"> two RMSE plots, one with a 1% window size and one with a </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">10% size:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.306.1">
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111)
plot_cross_validation_metric(df_cv,
                             metric='rmse',
                             rolling_window=.01,
                             ax=ax)
plot_cross_validation_metric(df_cv,
                             metric='rmse',
                             rolling_window=.1,
                             ax=ax)
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.307.1">We </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.308.1">use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">ax</span></strong><span class="koboSpan" id="kobo.310.1"> argument to plot both lines on the </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">same chart:</span></span></p>
<p class="IMG---Figure"><a id="_idTextAnchor895"/></p>
<div>
<div class="IMG---Figure" id="_idContainer273">
<span class="koboSpan" id="kobo.312.1"><img alt="Figure 13.4 – Comparing different window sizes" src="image/Fig_13.4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.313.1">Figure 13.4 – Comparing different window sizes</span></p>
<p><span class="koboSpan" id="kobo.314.1">The smoother line is the one with </span><a id="_idTextAnchor896"/><span class="koboSpan" id="kobo.315.1">a wider window size, the default window size. </span><span class="koboSpan" id="kobo.315.2">Because the window is not centered but set to the right edge, the first 8 days do not show the rolling average line when using 10% of the horizon. </span><span class="koboSpan" id="kobo.315.3">Setting the window to 1% will include all data at the cost of </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">being noisier.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">Now that you’ve learned how to use the cross-validation plot, let’s use it to see a problem that can arise when letting Prophet automatically select the cut-off dates to begin each </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">cross-validation fold.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor897"/><a id="_idTextAnchor898"/><span class="koboSpan" id="kobo.319.1">Handling irregular cut-offs</span></h1>
<p><span class="koboSpan" id="kobo.320.1">We’ll be </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.321.1">using </span><a id="_idTextAnchor899"/><span class="koboSpan" id="kobo.322.1">a new dataset for this example. </span><span class="koboSpan" id="kobo.322.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.323.1">World Food Programme</span></strong><span class="koboSpan" id="kobo.324.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.325.1">WFP</span></strong><span class="koboSpan" id="kobo.326.1">) is the</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.327.1"> branch of the United Nations focused on hunger and food security. </span><span class="koboSpan" id="kobo.327.2">One of the greatest contributing factors to food security issues in developing countries that the WFP tracks is rainfall because it can affect agricultural production. </span><span class="koboSpan" id="kobo.327.3">Thus, predicting rainfall is of critical importance in planning </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">aid delivery.</span></span></p>
<p><span class="koboSpan" id="kobo.329.1">This data represents the rainfall received over 30 years in one of the regions the WFP monitors. </span><span class="koboSpan" id="kobo.329.2">What makes this dataset unique is that the WFP recorded the amount of rain that accumulated three times per month, on the 1st, the 11th, and the 21st. </span><span class="koboSpan" id="kobo.329.3">The accumulation from the 1st to the 11th is a 10-day period. </span><span class="koboSpan" id="kobo.329.4">It’s the same from the 11th to the 21st. </span><span class="koboSpan" id="kobo.329.5">But the period from the 21st of one month to the 1st of the next varies depending upon the month. </span><span class="koboSpan" id="kobo.329.6">In a normal February, it will be 8 days. </span><span class="koboSpan" id="kobo.329.7">In a leap year, 9 days. </span><span class="koboSpan" id="kobo.329.8">Months of 30 and 31 days will see a period of 10 and 11 </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">days respectively.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">Let’s perform cross-validation as you’ve learned so far and see what effect this will have. </span><span class="koboSpan" id="kobo.331.2">First, we need to train a Prophet model on the data. </span><span class="koboSpan" id="kobo.331.3">You should have everything already imported if you’re continuing from the </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">previous example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.333.1">
df = pd.read_csv('rainfall.csv')
df.columns = ['ds', 'y']
model = Prophet(yearly_seasonality=4)
model.fit(df)
future = model.make_future_dataframe(periods=365 * 5)
future = future[future['ds'].dt.day.isin([1, 11, 21])]
forecast = model.predict(future)
fig = model.plot(forecast)
a = add_changepoints_to_plot(fig.gca(), model, forecast)
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.334.1">If you remember, cross-validation is not concerned with any future, unknown periods. </span><span class="koboSpan" id="kobo.334.2">Therefore, it’s </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.335.1">unnecessary to build a </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">future</span></strong><span class="koboSpan" id="kobo.337.1"> DataFrame and predict on it. </span><span class="koboSpan" id="kobo.337.2">I did so in this example merely to remind you of the first potential pitfall you learned about in </span><a href="B19630_04.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.338.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.339.1">, </span><em class="italic"><span class="koboSpan" id="kobo.340.1">Handling Non-Daily Data</span></em><span class="koboSpan" id="kobo.341.1">, when we used data with regular gaps. </span><span class="koboSpan" id="kobo.341.2">We needed to adjust our </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">future</span></strong><span class="koboSpan" id="kobo.343.1"> DataFrame to avo</span><a id="_idTextAnchor900"/><span class="koboSpan" id="kobo.344.1">id unconstrained predictions, and we’ve done that again here by restricting future dates only to those on the 1st, 11th, and 21st of each month. </span><span class="koboSpan" id="kobo.344.2">Here’s what the forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">looks like</span><a id="_idTextAnchor901"/><span class="koboSpan" id="kobo.346.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer274">
<span class="koboSpan" id="kobo.347.1"><img alt="Figure 13.5 – Rainfall forecast" src="image/Fig_13.5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.348.1">Figure 13.5 – Rainfall forecast</span></p>
<p><span class="koboSpan" id="kobo.349.1">It has a nearly flat trend, rising slightly until 2010 and then turning downward. </span><span class="koboSpan" id="kobo.349.2">As you may have expected, the model is dominated by yearly seasonality, with rainfall in December (summer in the Southern Hemisphere) at almost zero and rainfall at its maximum </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">in June.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">Now let’s build a cross-validation plot. </span><span class="koboSpan" id="kobo.351.2">We’ll forecast </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">90 days</span></strong><span class="koboSpan" id="kobo.353.1"> (the horizon) and create a new fold every </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">30 days</span></strong><span class="koboSpan" id="kobo.355.1"> (the period). </span><span class="koboSpan" id="kobo.355.2">Our initial training period will be </span><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">1826 days</span></strong><span class="koboSpan" id="kobo.357.1">, or 5 years. </span><span class="koboSpan" id="kobo.357.2">Finally, let’s plot </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">the RMSE:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.359.1">
df_cv = cross_validation(model,
                         horizon='90 days',
                         period='30 days',
                         initial='1826 days',
                         parallel='processes')
df_p = performance_metrics(df_cv)
fig = plot_cross_validation_metric(df_cv, metric='rmse')
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.360.1">Prophet </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.361.1">uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">horizon</span></strong><span class="koboSpan" id="kobo.363.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">period</span></strong><span class="koboSpan" id="kobo.365.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">initial</span></strong><span class="koboSpan" id="kobo.367.1"> to calculate a set of evenly spaced cut-offs. </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">horizon</span></strong><span class="koboSpan" id="kobo.369.1"> is then used again to set the length of each fold’s forecast but </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">period</span></strong><span class="koboSpan" id="kobo.371.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">initial</span></strong><span class="koboSpan" id="kobo.373.1"> are not needed after choosing </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">the cut-offs.</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">The effect of letting Prop</span><a id="_idTextAnchor902"/><span class="koboSpan" id="kobo.376.1">het automatically set the cut-offs is that they are inconveniently located compared to our data. </span><span class="koboSpan" id="kobo.376.2">We only have data for 3 days per month, and those 3 days are not consistently spaced. </span><span class="koboSpan" id="kobo.376.3">This means that each fold in our cross-validation starts effectively randomly somewhere in the data, producing a plot that seems to suggest each day in the horizon </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">has data</span><a id="_idTextAnchor903"/><span class="koboSpan" id="kobo.378.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer275">
<span class="koboSpan" id="kobo.379.1"><img alt="Figure 13.6 – Cross-validation with automatic cut-offs" src="image/Fig_13.6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.380.1">Figure 13.6 – Cross-validation with automatic cut-offs</span></p>
<p><span class="koboSpan" id="kobo.381.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">cross_validation</span></strong><span class="koboSpan" id="kobo.383.1"> function</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.384.1"> will accept a </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">cutoffs</span></strong><span class="koboSpan" id="kobo.386.1"> argument that takes a list of user-specified cut-off dates to use. </span><span class="koboSpan" id="kobo.386.2">This also means that </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">initial</span></strong><span class="koboSpan" id="kobo.388.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">period</span></strong><span class="koboSpan" id="kobo.390.1"> are no longer necessary. </span><span class="koboSpan" id="kobo.390.2">This co</span><a id="_idTextAnchor904"/><span class="koboSpan" id="kobo.391.1">de block will use a list comprehension to iterate over each year, then each month, then each day of either the 1st, 11th, or 21st, and create a list of </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">pandas </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">Timestamp</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.395.1">
cutoffs = [pd.Timestamp('{}-{}-{}'.format(year, month,
                                          day))
           for year in range(2005, 2019)
           for month in range(1, 13)
           for day in [1, 11, 21]]</span></pre>
<p><span class="koboSpan" id="kobo.396.1">Now, if we replot our cross-validation but send this list of cut-off dates, we’ll see something </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">dramatically different:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.398.1">
df_cv = cross_validation(model,
                         horizon='90 days',
                         parallel='processes',
                         cutoffs=cutoffs)
df_p = performance_metrics(df_cv)
fig = plot_cross_validation_metric(df_cv, metric='rmse')
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.399.1">Now, each </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.400.1">fold is begun o</span><a id="_idTextAnchor905"/><span class="koboSpan" id="kobo.401.1">n a day for which we have data. </span><span class="koboSpan" id="kobo.401.2">The next day for which data exists will be either 8, 9, 10, or 11 days later. </span><span class="koboSpan" id="kobo.401.3">Hence, the plot shows 4 discrete days in </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">horizon</span></strong><span class="koboSpan" id="kobo.403.1"> where a forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">took plac</span><a id="_idTextAnchor906"/><span class="koboSpan" id="kobo.405.1">e:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer276">
<span class="koboSpan" id="kobo.406.1"><img alt="Figure 13.7 – Cross-validation with custom cut-offs" src="image/Fig_13.7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.407.1">Figure 13.7 – Cross-validation with custom cut-offs</span></p>
<p><span class="koboSpan" id="kobo.408.1">Both </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.409.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.410.1">.6</span></em><span class="koboSpan" id="kobo.411.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.412.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.413.1">.7</span></em><span class="koboSpan" id="kobo.414.1"> show an average RMSE of just above 20, so the results are very similar. </span><span class="koboSpan" id="kobo.414.2">The difference is simply the ease of interpretation and consistency. </span><span class="koboSpan" id="kobo.414.3">You may encounter this situat</span><a id="_idTextAnchor907"/><span class="koboSpan" id="kobo.415.1">ion often if your data is recorded monthly or in any increment of months because they have </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">inconsistent duration</span><a id="_idTextAnchor908"/><a id="_idTextAnchor909"/><span class="koboSpan" id="kobo.417.1">s.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor910"/><span class="koboSpan" id="kobo.418.1">Tuning hyperparameters with grid search</span></h1>
<p><span class="koboSpan" id="kobo.419.1">For the </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.420.1">final section of this chapter, we’ll look at grid search and work through an example, continuing with this rainfall data. </span><span class="koboSpan" id="kobo.420.2">If you’re not familiar </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.421.1">with</span><a id="_idTextAnchor911"/><span class="koboSpan" id="kobo.422.1"> the concept of grid search, it’s a way to exhaustively check all reasonable combinations of hyperparameters against a performance indicator and choose the best combination to train your final model. </span><span class="koboSpan" id="kobo.422.2">With Prophet, you might decide to select the following hyperparameters </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">and valu</span><a id="_idTextAnchor912"/><span class="koboSpan" id="kobo.424.1">es:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer277">
<span class="koboSpan" id="kobo.425.1"><img alt="Figure 13.8 – Prophet grid search parameters" src="image/Fig_13.8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.426.1">Figure 13.8 – Prophet grid search parameters</span></p>
<p><span class="koboSpan" id="kobo.427.1">With these parameters, a grid search will iterate through each unique combination, use cross-validation to calculate and save a performance metric, and then output the set of parameter values that resulted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">best performance.</span></span></p>
<p><span class="koboSpan" id="kobo.429.1">Prophet </span><a id="_idTextAnchor913"/><span class="koboSpan" id="kobo.430.1">does not have a grid search method the way, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">sklearn</span></strong><span class="koboSpan" id="kobo.432.1"> does. </span><span class="koboSpan" id="kobo.432.2">One is easy enough to build yourself in Python though, so le</span><a id="_idTextAnchor914"/><span class="koboSpan" id="kobo.433.1">t’s see how to set it up. </span><span class="koboSpan" id="kobo.433.2">The first step is to define our parameter grid. </span><span class="koboSpan" id="kobo.433.3">We’ll use the grid shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.434.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.435.1">.8</span></em><span class="koboSpan" id="kobo.436.1">, but we’re not including holidays in our model (the weather doesn’t regularly check its calendar and adjust rainfall if it finds a holiday!), so we’ll leave </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">that out:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.438.1">
param_grid = {'changepoint_prior_scale': [0.5, 0.1, 0.01,
                                          0.001],
              'seasonality_prior_scale': [10.0, 1.0, 0.1,
                                          0.01],
              'seasonality_mode': ['additive',
                                   'multiplicative']}</span></pre>
<p><span class="koboSpan" id="kobo.439.1">Next, we’ll use Python’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.440.1">itertools</span></strong><span class="koboSpan" id="kobo.441.1"> package to iterate over that grid and create a list of each unique combination. </span><span class="koboSpan" id="kobo.441.2">We’ll need to import </span><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">itertools</span></strong><span class="koboSpan" id="kobo.443.1"> first; and while we’re at it, let’s import </span><strong class="source-inline"><span class="koboSpan" id="kobo.444.1">numpy</span></strong><span class="koboSpan" id="kobo.445.1"> as well, because we’ll be using it later. </span><span class="koboSpan" id="kobo.445.2">We’ll also create an empty list to hold all of the RMSE values, assuming that’s our chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">performance metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.447.1">
import numpy as np
import itertools
all_params = [dict(zip(param_grid.keys(), value))
              for value in itertools.product(
                                  *param_grid.values())]
rmse_values= []</span></pre>
<p><span class="koboSpan" id="kobo.448.1">We</span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.449.1"> could allow Prophet to define our cut</span><a id="_idTextAnchor915"/><span class="koboSpan" id="kobo.450.1">-off periods, but</span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.451.1"> because</span><a id="_idTextAnchor916"/><span class="koboSpan" id="kobo.452.1"> we’re using this rainfall data, let’s set </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">cutoffs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.454.1"> ourselves:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.455.1">
cutoffs = [pd.Timestamp('{}-{}-{}'.format(year, month,
                                          day))
           for year in range(2010, 2019)
           for month in range(1, 13)
           for day in [1, 11, 21]]</span></pre>
<p><span class="koboSpan" id="kobo.456.1">The final step in running our grid search, before we evaluate the results, is to iterate over each combination we saved in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">all_params</span></strong><span class="koboSpan" id="kobo.458.1"> list, and build a model, a cross-validation DataFrame, and a performance </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">metrics DataFrame.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Let’s say that we know we want </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">yearly_seasonality=4</span></strong><span class="koboSpan" id="kobo.462.1"> to keep the curve smooth, and we’ll complete model instantiation with the parameter combination for that iteration. </span><span class="koboSpan" id="kobo.462.2">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">performance_metrics</span></strong><span class="koboSpan" id="kobo.464.1"> function, we are using </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">rolling_window=1</span></strong><span class="koboSpan" id="kobo.466.1">. </span><span class="koboSpan" id="kobo.466.2">This means that we are averaging 100% of the data in that fold to calculate the metric, so instead of a series of values, we only </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">get one:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.468.1">
for params in all_params:
    model = Prophet(yearly_seasonality=4, **params).fit(df)
    df_cv = cross_validation(model,
                             cutoffs=cutoffs,
                             horizon='30 days',
                             parallel='processes')
    df_p = performance_metrics(df_cv, rolling_window=1)
    rmse_values.append(df_p['rmse'].values[0])</span></pre>
<p><span class="koboSpan" id="kobo.469.1">That</span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.470.1"> code is going to take a long time to run. </span><span class="koboSpan" id="kobo.470.2">The length of our </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">all_params</span></strong><span class="koboSpan" id="kobo.472.1"> list, after all, is 32, which means you’ll be training and </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.473.1">cross-validating 32 total models. </span><span class="koboSpan" id="kobo.473.2">I did say grid se</span><a id="_idTextAnchor917"/><span class="koboSpan" id="kobo.474.1">arch was exhaustive! </span><span class="koboSpan" id="kobo.474.2">(On a typical laptop, you can expect it will take around 8-12 hours to complete; to speed up the example, you may consider reducing the number of parameters in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">param_grid</span></strong><span class="koboSpan" id="kobo.476.1"> dictionary, such as, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">param_grid = {'changepoint_prior_scale': [0.1, 0.01]</span></strong><span class="koboSpan" id="kobo.478.1">,</span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1"> 'seasonality_prior_scale': [1.0, 0.1]}</span></strong><span class="koboSpan" id="kobo.480.1">, which will train and cross-validate only four total models. </span><span class="koboSpan" id="kobo.480.2">Be sure to recreate your </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">all_params</span></strong><span class="koboSpan" id="kobo.482.1"> dictionary after changing </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">param_grid</span></strong><span class="koboSpan" id="kobo.484.1">.) To inspect the results, we’ll </span><a id="_idTextAnchor918"/><span class="koboSpan" id="kobo.485.1">build a DataFrame with the parameter combinations and their associated RMSEs, and then display a portion </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">of it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.487.1">
results = pd.DataFrame(all_params)
results['rmse'] = rmse_values
results.head()</span></pre>
<p><span class="koboSpan" id="kobo.488.1">The full DataFrame has 32 rows, one for each combination of parameters, but here we see the first </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">five</span><a id="_idTextAnchor919"/><span class="koboSpan" id="kobo.490.1"> rows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<span class="koboSpan" id="kobo.491.1"><img alt="Figure 13.9 – Grid search DataFrame" src="image/Fig_13.9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.492.1">Figure 13.9 – Grid search DataFrame</span></p>
<p><span class="koboSpan" id="kobo.493.1">Finally, let’s use NumPy to find the parameters with the lowest RMSE value and then </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">print them:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.495.1">
best_params = all_params[np.argmin(rmse_values)]
print(best_params)</span></pre>
<p><span class="koboSpan" id="kobo.496.1">Printing </span><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">best_params</span></strong><span class="koboSpan" id="kobo.498.1"> should display </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">this output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.500.1">
'changepoint_prior_scale': 0.01,
'seasonality_prior_scale': 1.0,
'seasonality_mode': 'additive'}</span></pre>
<p><span class="koboSpan" id="kobo.501.1">The biggest difference between the best parameters found with grid search and those we’ve used so far is that the changepoint regularization would be better set to a much stronger level. </span><span class="koboSpan" id="kobo.501.2">With a lower prior scale, the magnitudes of changepoints would be less, and the trend curve would</span><a id="_idTextAnchor920"/><span class="koboSpan" id="kobo.502.1"> be even flatter. </span><span class="koboSpan" id="kobo.502.2">Intuitively, this seems appropriate; especially for longer forecasts, where allowing larger trend changes would create unrealistic rainfall forecasts far into </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">the future.</span></span></p>
<p><span class="koboSpan" id="kobo.504.1">Probably </span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.505.1">the most critical</span><a id="_idTextAnchor921"/><span class="koboSpan" id="kobo.506.1"> parameter to tune is </span><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">changepoint_prior_scale</span></strong><span class="koboSpan" id="kobo.508.1">. </span><span class="koboSpan" id="kobo.508.2">If this value is too small, the trend will underfit the </span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.509.1">variance. </span><span class="koboSpan" id="kobo.509.2">Variance that should be modeled with the trend will instead be modeled in the noise term. </span><span class="koboSpan" id="kobo.509.3">If the prior scale is too large, the trend will exhibit too much flexibility and may start to capture some of the yearly seasonality. </span><span class="koboSpan" id="kobo.509.4">A range between </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">0.5</span></strong><span class="koboSpan" id="kobo.511.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.512.1">0.001</span></strong><span class="koboSpan" id="kobo.513.1"> will work in </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">most cases.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">seasonality_prior_scale</span></strong><span class="koboSpan" id="kobo.517.1"> parameter is probably the second most impactful parameter. </span><span class="koboSpan" id="kobo.517.2">A typical range is usually </span><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">10</span></strong><span class="koboSpan" id="kobo.519.1">, with essentially no regularization, down to </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">0.01</span></strong><span class="koboSpan" id="kobo.521.1">. </span><span class="koboSpan" id="kobo.521.2">Anything smaller and the seasonality is likely to be regularized to a negligible effect. </span><span class="koboSpan" id="kobo.521.3">You also have the option of setting each seasonality to </span><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">False</span></strong><span class="koboSpan" id="kobo.523.1"> and using </span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">add_seasonality</span></strong><span class="koboSpan" id="kobo.525.1"> to choose prior scales individually, but this causes your grid search to increase in computing </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">time exponentially.</span></span></p>
<p><span class="koboSpan" id="kobo.527.1">You may also want to add </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">fourier_order</span></strong><span class="koboSpan" id="kobo.529.1"> to your grid search, but I’ve found it works great to build a quick model with defaults, inspect the components, and choose Fourier orders myself that fit my intuition. </span><span class="koboSpan" id="kobo.529.2">In a fully automated setup, keeping Fourier orders at their defaults will probably </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">be fine.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">holidays_prior_scale</span></strong><span class="koboSpan" id="kobo.532.1"> is also a tunable parameter, with many of the same characteristics as </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">seasonality_prior_scale</span></strong><span class="koboSpan" id="kobo.534.1">. </span><span class="koboSpan" id="kobo.534.2">Just keep in mind that many models won’t have holidays and so there will be no need to include </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">this parameter.</span></span></p>
<p><span class="koboSpan" id="kobo.536.1">The last of the critical parameters that should always be considered is </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">seasonality_mode</span></strong><span class="koboSpan" id="kobo.538.1">. </span><span class="koboSpan" id="kobo.538.2">In this book, you have learned a few rules of thumb to help decide which mode to use, but more oft</span><a id="_idTextAnchor922"/><span class="koboSpan" id="kobo.539.1">en than not, it isn’t clear. </span><span class="koboSpan" id="kobo.539.2">The best thing to do is simply inspect a plot of your time series and see whether the magnitude of seasonal fluctuations grows with the trend or stays constant. </span><span class="koboSpan" id="kobo.539.3">If you can’t tell, go ahead and add </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">seasonality_mode</span></strong><span class="koboSpan" id="kobo.541.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">the grid.</span></span></p>
<p><span class="koboSpan" id="kobo.543.1">Usually, the default value of 80% for </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">changepoint_range</span></strong><span class="koboSpan" id="kobo.545.1"> will be good. </span><span class="koboSpan" id="kobo.545.2">It provides a nice balance of allowing t</span><a id="_idTextAnchor923"/><span class="koboSpan" id="kobo.546.1">he trend to change where appropriate but not allowing it to overfit in the last 20% of data where errors cannot be corrected. </span><span class="koboSpan" id="kobo.546.2">If you’re the analyst and paying </span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.547.1">close attention, it’s easy to see if the default range is not appropriate. </span><span class="koboSpan" id="kobo.547.2">But in a fully automated setting, it’s probably better to be conservative and leave it </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">at 80%.</span></span></p>
<p><span class="koboSpan" id="kobo.549.1">The</span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.550.1"> remaining parameters are best left out of your grid search. </span><span class="koboSpan" id="kobo.550.2">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">'growth'</span></strong><span class="koboSpan" id="kobo.552.1">, it is either </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">'linear'</span></strong><span class="koboSpan" id="kobo.554.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">'logistic'</span></strong><span class="koboSpan" id="kobo.556.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">'flat'</span></strong><span class="koboSpan" id="kobo.558.1">, and you as the analyst should choose. </span><span class="koboSpan" id="kobo.558.2">Setting it to </span><strong class="source-inline"><span class="koboSpan" id="kobo.559.1">'logistic'</span></strong><span class="koboSpan" id="kobo.560.1"> will require setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">'cap'</span></strong><span class="koboSpan" id="kobo.562.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">'floor'</span></strong><span class="koboSpan" id="kobo.564.1"> as well. </span><span class="koboSpan" id="kobo.564.2">Many of the remaining parameters, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">n_changepoints</span></strong><span class="koboSpan" id="kobo.566.1"> and the yearly, weekly, and daily seasonalities, are better controlled with parameters already included in the search: </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">changepoint_prior_scale</span></strong><span class="koboSpan" id="kobo.568.1"> in the case of changepoints and </span><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">seasonality_prior_scale</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.570.1">with seasonalities.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">The final parameters, </span><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">mcmc_samples</span></strong><span class="koboSpan" id="kobo.573.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">interval_width</span></strong><span class="koboSpan" id="kobo.575.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">uncertainty_samples</span></strong><span class="koboSpan" id="kobo.577.1">, don’t affect your </span><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">yhat</span></strong><span class="koboSpan" id="kobo.579.1"> in any way and therefore have no effect on your performance metric. </span><span class="koboSpan" id="kobo.579.2">They only control the </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">uncertainty intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.581.1">Use common sense with grid search – it is a very long process, so don’t include every parameter and every possible value in your hyperparameter grid. </span><span class="koboSpan" id="kobo.581.2">Often the best approach an analyst can take is to provide intuition and human touch to the process and let the computer do </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">the number-crunchi</span><a id="_idTextAnchor924"/><a id="_idTextAnchor925"/><span class="koboSpan" id="kobo.583.1">ng.</span></span></p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor926"/><span class="koboSpan" id="kobo.584.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.585.1">In this chapter, you learned how to use Prophet’s performance metrics to extend the usefulness of cross-validation. </span><span class="koboSpan" id="kobo.585.2">You learned about the six metrics Prophet has out of the box, namely, MSE, RMSE, MAE, MAPE, MdAPE, and coverage. </span><span class="koboSpan" id="kobo.585.3">You learned about many of the advantages and disadvantages of these metrics, and situations where you may want to use or avoid any one </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">of them.</span></span></p>
<p><span class="koboSpan" id="kobo.587.1">Next, you learned how to create Prophet’s performance metrics DataFrame and use it to create a plot of your preferred cross-validation metric so as to be able to evaluate the performance of your model on unseen data across a range of forecast horizons. </span><span class="koboSpan" id="kobo.587.2">You then used this plot with the WFP’s rainfall data to see a situation where Prophet’s automatic cut-off date selection is not ideal, and how to create custom </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">cut-off dates.</span></span></p>
<p><span class="koboSpan" id="kobo.589.1">Finally, you brought all of this together in an exhaustive grid search of Prophet hyperparameters. </span><span class="koboSpan" id="kobo.589.2">This process enabled you to use a data-driven technique to fine-tune your model and optimize it for a metric of </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">your choice.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">In the next chapter, the final chapter of this book, you will learn about a few more tricks in Prophet’s bag to help put your models into a </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">production environment.</span></span></p>
</div>
<div>
<div class="IMG---Figure" id="_idContainer280">
</div>
</div>
</body></html>