<html><head></head><body>
		<div id="_idContainer082">
			<h1 id="_idParaDest-92"><em class="italic"><a id="_idTextAnchor094"/>Chapter 7</em>: Using the Many Models Solution Accelerator</h1>
			<p>Now that you have experienced building regression, classification, and forecasting models with AutoML, it's time for you to learn how to deploy and utilize those models in actual business scenarios. Before you tackle this, however, we will first introduce you to a new, very powerful solution, that is, the <strong class="bold">Many Models Solution Accelerator </strong>(<strong class="bold">MMSA</strong>). </p>
			<p>The MMSA lets you build hundreds to thousands of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models at once and easily scales to hundreds of thousands of models. It's an advanced technology at the cutting edge of ML. Not only can you build hundreds of thousands of models, but you can also use the MMSA to easily deploy them into production.</p>
			<p>In this chapter, you will begin by installing the accelerator and understanding the various use cases to which it applies. You will then run the three sections of the accelerator notebook-by-notebook: prepping data, training models, and forecasting new data. </p>
			<p>In each section, you will use both sample data found within the accelerator as well as sample data that you generate with Python code. This will give you examples of using the MMSA with both file and tabular datasets. Finally, you will go over tips and tricks to maximize performance using the accelerator and you will be introduced to concepts such as <strong class="bold">hierarchical forecasting</strong>.</p>
			<p>By the end of this chapter, you will have mastered using the MMSA with AutoML. You will be able to bring your own data into the MMSA, get it into the right shape, and train thousands of models. This solution is ideal for scenarios in which you wish to train similar models over a large number of products or stores, for example, building a separate forecasting model for each combination of product and store. Large companies all over the USA use it, and, by the end of this chapter, you will be able to use it too.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Installing the many models solution accelerator</li>
				<li>Prepping data for many models</li>
				<li>Training many models simultaneously</li>
				<li>Scoring new data for many models</li>
				<li>Improving your many models results </li>
			</ul>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/>Technical requirements</h1>
			<p>Within this chapter, you will log in to your <strong class="bold">Azure Machine Learning studio</strong> (<strong class="bold">AMLS</strong>), open up a Jupyter notebook on a compute instance, and install the MMSA from its location on GitHub. You will then run all three pieces of the MMSA sequentially, prepping the data, training the models remotely, and forecasting the data. As such, you need to have an Azure account, a compute instance for writing Python code, and a compute cluster for remote training. The full list of requirements is as follows: </p>
			<ul>
				<li>Access to the internet.</li>
				<li>A web browser, preferably Google Chrome or Microsoft Edge Chromium.</li>
				<li>A Microsoft Azure account.</li>
				<li>You should have created an AMLS workspace.</li>
				<li>You should have created a compute instance in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service.</em></li>
				<li>You should have created the compute cluster in <a href="B16595_02_ePub.xhtml#_idTextAnchor023"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with Azure Machine Learning Service.</em></li>
				<li>You should understand how to navigate to the Jupyter environment from an AMLS compute instance, as demonstrated in <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution.</em> </li>
			</ul>
			<p>The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter07">https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter07</a>.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor096"/>Installing the many models solution accelerator</h1>
			<p>The MMSA <a id="_idIndexMarker367"/>was built by Microsoft in 2019 to address the needs of a growing number of customers who wanted to train hundreds of thousands of similar ML models simultaneously. This is particularly important for product demand forecasting, where you are trying to make forecasts for many different products at many different locations. </p>
			<p>The impetus for the accelerator is <strong class="bold">model accuracy</strong>. While you could train a single model to predict product demand across all of your product lines and all of your stores, you will find that training individual models for each combination of product and store tends to yield superior performance. This is because a multitude of factors are dependent on both your algorithm and your data. It can be very difficult for some algorithms to find meaningful patterns when you're dealing with hundreds of thousands of different products distributed across the globe.</p>
			<p>Additionally, the same columns can have different or even opposite relationships with the target column you are trying to predict. Imagine weather and product sales. When it snows outside, it's very likely that certain products, such as winter hats, gloves, or boots, will experience a spike in sales. Other products, such as ice cream, will very likely experience a decline in sales. While some algorithms can handle these opposite relationships across product lines, many cannot, and using the MMSA to build a separate model for each product will often result in better metrics. </p>
			<p>Other common use cases besides forecasting product demand for the MMSA include predictive maintenance across thousands of devices and machines on factory plant floors, workforce optimization models across hundreds of stores, text analytics use cases and legal document search models per state in the United States, and many other similar scenarios. Still, forecasting is the most common use case. Chain stores, in particular, find the MMSA attractive. </p>
			<p>Technically speaking, the key factor in determining whether to use the MMSA or not is the presence of one or more columns in your data, which you can use to split the data into multiple files. Columns such as store, product, and region are prime targets to split. If no such columns exist in your data, there's no reason to use the MMSA. </p>
			<p>Likewise, if you expect that patterns in your data should be relatively stable across columns such as product, group, and region, you should train a single ML model with AutoML as you would for any other problem.</p>
			<p>In this section, you will install the MMSA on your Jupyter environment. First, you will create a new Jupyter notebook used only for the installation. Then, you will install the MMSA on your Jupyter environment. This will create a series of files and folders. Lastly, you will be<a id="_idIndexMarker368"/> able to confirm that the MMSA has been successfully installed and identify the notebooks you will use for the remainder of this chapter.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>Creating a new notebook in your Jupyter environment</h2>
			<p>Perform the<a id="_idIndexMarker369"/> following steps to create a new Jupyter<a id="_idIndexMarker370"/> notebook on your compute instance:</p>
			<ol>
				<li>First, open up your AML studio instance by navigating to <a href="http://ml.azure.com">http://ml.azure.com</a>. </li>
				<li>Click <strong class="bold">Compute</strong>, start up a compute instance, and open a Jupyter environment. </li>
				<li>Create a new Jupyter notebook and name it <strong class="source-inline">Accelerator_Installation</strong>. If you need a refresher, please review <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution</em>.</li>
			</ol>
			<p>With your notebook created, you are now ready to install the accelerator from GitHub. </p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/>Installing the MMSA from GitHub</h2>
			<p>The MMSA <a id="_idIndexMarker371"/>is a publicly<a id="_idIndexMarker372"/> available solution developed by Microsoft hosted on a GitHub repository. Use the following code to install the MMSA on your Jupyter environment: </p>
			<ol>
				<li value="1">Clone the MMSA repo into your Jupyter notebook filesystem using the following code:<p class="source-code">!git clone https://github.com/microsoft/solution-accelerator-many-models</p></li>
				<li>All of the files have now been loaded into a new folder named <strong class="source-inline">solution-accelerator-many-models</strong>. Click the Jupyter graphic at the top of your screen to navigate to your file directory, as shown in <em class="italic">Figure 7.1</em>:<div id="_idContainer078" class="IMG---Figure"><img src="image/Figure_7.1_B16595.jpg" alt="Figure 7.1 – Navigating to your file directory "/></div><p class="figure-caption">Figure 7.1 – Navigating to your file directory</p></li>
				<li>Ope<a id="_idTextAnchor099"/>n<a id="_idIndexMarker373"/> the <strong class="source-inline">solution-accelerator-many-models</strong> folder <a id="_idIndexMarker374"/>by clicking it.</li>
				<li>Upon opening the folder, you will find many files and folders. The first file you will use in the <em class="italic">Prepping data for many models</em> section is <strong class="source-inline">01_Data_Preparation.ipynb</strong>. If you wish to set up a new compute cluster for the MMSA, you should first run <strong class="source-inline">00_Setup_AML_Workspace.ipynb</strong>. Make a note of these.</li>
				<li>Moving on, open the <strong class="source-inline">Automated_ML</strong> folder. This folder contains two subfolders, called <strong class="source-inline">02_AutoML_Training_Pipeline</strong> and <strong class="source-inline">03_AutoML_Forecasting_Pipeline</strong>.</li>
				<li>Open each of the AutoML pipelines. Each one contains a Jupyter notebook with the same name as the folder. Make a note of these.</li>
			</ol>
			<p>For the rest of this chapter, these will be the only three Jupyter notebooks you will interact with, <strong class="source-inline">01_Data_Preparation.ipynb</strong>, <strong class="source-inline">02_AutoML_Training_Pipeline</strong>, and <strong class="source-inline">03_AutoML_Forecasting_Pipeline</strong>. In each case, first you will run the notebook as is, running with a default sample dataset. Then, you will make another notebook and use <a id="_idIndexMarker375"/>similar <a id="_idIndexMarker376"/>code to train a different dataset. This will teach you how to use both file and tabular datasets with the MMSA, and how to work with your own data. You will begin by prepping data.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor100"/>Prepping data for many models</h1>
			<p>While <a id="_idIndexMarker377"/>training thousands of ML models simultaneously sounds complicated, the MMSA makes it easy. The example included in the notebooks uses the <strong class="source-inline">OJ Sales</strong> data you used in <a href="B16595_06_ePub.xhtml#_idTextAnchor081"><em class="italic">Chapter 6</em></a>, <em class="italic">Building an AutoML Forecasting Solution</em>. You will prepare the data simply by opening and running <strong class="source-inline">01_Data_Preparation.ipynb</strong>. By reading the instructions carefully step by step and working through the notebook slowly, you will be able to understand what each section is about. </p>
			<p>Once you're able to understand what each section is doing and you have the <strong class="source-inline">OJ Sales</strong> data loaded, you will be able to load the new dataset into your Jupyter notebook. This way, by the end of this section, you will be able to load your own data into Azure, modify it for the MMSA, and master the ability to use this powerful solution.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor101"/>Prepping the sample OJ dataset</h2>
			<p>To <a id="_idIndexMarker378"/>understand how the<a id="_idIndexMarker379"/> first notebook works, follow these instructions in order: </p>
			<ol>
				<li value="1">Open <strong class="source-inline">01_Data_Preparation.ipynb</strong>.</li>
				<li>Run all of the cells in <strong class="source-inline">section</strong> <strong class="source-inline">1.0</strong> of the notebook. These cells create a folder called <strong class="source-inline">oj_sales_data</strong> in your file directory and download the <strong class="source-inline">OJ Sales</strong> data there. After running <strong class="source-inline">section</strong> <strong class="source-inline">1.0</strong>, examine the files in your new folder; <strong class="source-inline">oj_sales_data</strong> will be located in the same directory level as <strong class="source-inline">01_Data_Preparation.ipynb</strong>. </li>
				<li>Run the single cell in <strong class="source-inline">section</strong> <strong class="source-inline">2.0</strong> of the notebook. This cell splits the data into training data and inference data based on a date. It creates two folders in the <strong class="source-inline">oj_sales_data</strong> folder, one called <strong class="source-inline">upload_train_data</strong>, and another called <strong class="source-inline">upload_inference_data</strong>. Look inside each of these folders after you run the cell. You should see files with names such as <strong class="source-inline">Store1000_dominicks.csv</strong>. Click one of the files to see what the data looks like.</li>
				<li>Run all of<a id="_idIndexMarker380"/> the cells<a id="_idIndexMarker381"/> in <strong class="source-inline">section</strong> <strong class="source-inline">3.0</strong> of the notebook. These cells copy the data from your file directory on your compute instance to a file directory on your datastore. This copies the file structure and you end up with the <strong class="source-inline">oj_sales_data</strong> folder as well as your <strong class="source-inline">upload_train_data</strong> and <strong class="source-inline">upload_inference data</strong> subfolders on your datastore. If you'd like to, open up your Azure storage account and try to locate these folders. Remember that they will be in the container beginning with <strong class="source-inline">azureml-blobstore</strong>.</li>
				<li>Run the single cell in <strong class="source-inline">section</strong> <strong class="source-inline">4.0</strong> of the notebook. This cell creates two file datasets, one named <strong class="source-inline">oj_data_small_train</strong> and the other called <strong class="source-inline">oj_data_small_inference</strong>. These are the datasets you will use in <strong class="source-inline">02_AutoML_Training_Pipeline</strong> and <strong class="source-inline">03_AutoML_Forecasting_Pipeline</strong>, respectively.</li>
				<li>Run all of the cells in <strong class="source-inline">section</strong> <strong class="source-inline">5.0</strong> of the notebook to view your data.<p class="callout-heading">Important note</p><p class="callout">If you run the notebook as is, you will train a small number of models using 10 files. You can set <strong class="source-inline">dataset_maxfiles</strong> to <strong class="source-inline">11793</strong> to train a much larger number of models in <strong class="source-inline">section</strong> <strong class="source-inline">1.0</strong> of the notebook. In this case, your datasets will be called <strong class="source-inline">oj_data_inference</strong> and <strong class="source-inline">oj_data_train</strong>.</p></li>
			</ol>
			<p>You now have the <strong class="source-inline">OJ Sales</strong> data prepped for the accelerator. In order to bring your own data into the accelerator, there are a few important caveats you need to follow. Most <a id="_idIndexMarker382"/>importantly, the <strong class="source-inline">OJ Sales</strong> data <a id="_idIndexMarker383"/>comes presplit based on store and orange juice brand. You will need to mimic this structure using your own data in a new Jupyter notebook.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor102"/>Prepping a pandas dataframe</h2>
			<p>Bringing your <a id="_idIndexMarker384"/>own <a id="_idIndexMarker385"/>data into the MMSA is unclear. <strong class="source-inline">OJ Sales</strong>, after all, is a file dataset consisting of 11,793 files. You are much more likely to use data that consists of a single file or comes from a single table within a database. Moreover, you are most likely to read it in via pandas, the most common Python package. To learn how to use pandas dataframes with the MMSA, perform the following steps:</p>
			<ol>
				<li value="1">Download the <strong class="source-inline">ManyModelsSampleData.csv</strong> file from the <em class="italic">Automated-Machine-Learning-on-Microsoft-Azure</em> GitHub repository.</li>
				<li>Navigate to your Jupyter environment.</li>
				<li>Open the <strong class="source-inline">solution-accelerator-many-models</strong> folder.</li>
				<li>Click the <strong class="bold">Upload</strong> button in the top-left corner of your screen. Upload the <strong class="source-inline">ManyModelsSampleData.csv</strong> file to your Jupyter environment.</li>
				<li>Create a new Jupyter notebook and open it. Rename it <strong class="source-inline">01_Data_PreparationMy-Data.ipynb</strong>. </li>
				<li>To load in all of the libraries, you will require the following code:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import os</p><p class="source-code">import datetime as dt</p><p class="source-code">from azureml.core import Workspace, Dataset, Datastore</p><p class="source-code">from scripts.helper import split_data</p><p>You should recognize <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">Workspace</strong>, <strong class="source-inline">Dataset</strong>, and <strong class="source-inline">Datastore</strong> from <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution</em>. You've also used <strong class="source-inline">os</strong> in <a href="B16595_06_ePub.xhtml#_idTextAnchor081"><em class="italic">Chapter 6</em></a>, <em class="italic">Building an AutoML Forecasting Solution.</em> </p><p>New to this <a id="_idIndexMarker386"/>script is <strong class="source-inline">split_data</strong>, which is a <strong class="bold">helper function</strong>. Helper functions are reusable functions written for a program to reduce complexity. The MMSA has a few helper functions and split data is used to divide data into training and inference data based on a date you pass in. </p><p>Another<a id="_idIndexMarker387"/> new<a id="_idIndexMarker388"/> package is <strong class="source-inline">datetime</strong>, which lets you convert string objects into proper Python datetime objects. This is a requirement since <strong class="source-inline">split_data</strong> requires datetime objects to function properly.</p></li>
				<li>Read the <strong class="source-inline">ManyModelsSampleData.csv</strong> file into a pandas dataframe with the following code:<p class="source-code">ManyModelsSample =\</p><p class="source-code">pd.read_csv('ManyModelsSampleData.csv', header = 0)</p><p>Setting headers to <strong class="source-inline">0</strong> will use the first row of your CSV file for column names.</p></li>
				<li>Create a folder called <strong class="source-inline">MMSA_Sample_Folder</strong> with the following code:<p class="source-code">target_path = 'MMSA_Sample_Folder' </p><p class="source-code">os.makedirs(target_path, exist_ok=True)</p></li>
				<li>View your dataset:<p class="source-code">ManyModelsSample</p><p>You will find that this dataset has three columns: <strong class="source-inline">Date</strong>, <strong class="source-inline">Store</strong>, and <strong class="source-inline">Sales</strong>. It's about as simple a forecasting dataset as you can get. There are four stores and each store has a time series that extends from January 1, 2020 until March 31, 2021. You want to forecast sales into the future.</p></li>
				<li>Convert your <strong class="source-inline">Date</strong> column into a <strong class="source-inline">datetime</strong> object with the following code:<p class="source-code">ManyModelsSample['Date'] =\</p><p class="source-code">ManyModelsSample['Date'].apply(lambda x:\</p><p class="source-code">dt.datetime.strptime(x, '%m/%d/%Y'))</p><p>This code takes your <strong class="source-inline">Date</strong> column and applies a function to it using the <strong class="source-inline">datetime</strong> package to convert it from a string into a <strong class="source-inline">datetime</strong> object.</p></li>
				<li>Split your <a id="_idIndexMarker389"/>pandas <a id="_idIndexMarker390"/>dataframe into four separate CSV files, one for each store, and place each of them in the MMSA sample folder with the following code:<p class="source-code">for x, y in ManyModelsSample.groupby('Store'):</p><p class="source-code">    y.to_csv('MMSA_Sample_Folder/{}.csv'.format(x),\</p><p class="source-code"> header=True, index_label=False)</p><p>Understand that <strong class="source-inline">x</strong> is an individual store within your <strong class="source-inline">ManyModelsSample</strong> dataframe and that <strong class="source-inline">y</strong> is a pandas dataframe with only values for that store. This code loops through all four stores and, one by one, creates a CSV file with headers inside <strong class="source-inline">MMSA_Sample_Folder</strong>. Each file will be the name of the store. In this case, the stores are named after the cities in which they are located: New York, Washington DC, San Francisco, and Seattle. </p><p class="callout-heading">Important tip</p><p class="callout">The time column that you use to split your data must absolutely be a datetime object, not a string. Leaving your time column as a string will result in failed forecasting runs later on.</p></li>
				<li>Set variables for your time column as well as the cutoff for when you are training and scoring data:<p class="source-code">timestamp_column = 'Date'</p><p class="source-code">split_date = '2021-03-01'</p><p>The MMSA documentation refers to scoring data as inference data. <strong class="bold">Inferencing</strong> is just<a id="_idIndexMarker391"/> another word for scoring; it's widely used in academic settings or within research-focused companies. </p><p>When you specify <strong class="source-inline">split_date</strong>, remember that the date you specify and every date after it will be used for scoring, while all dates prior to it will be used for training. Your <strong class="source-inline">split_date</strong> function must be in the format used here.</p></li>
				<li> Split the <a id="_idIndexMarker392"/>data<a id="_idIndexMarker393"/> into training and inference files with the following code:<p class="source-code">train_path, inference_path = split_data(target_path, \</p><p class="source-code">timestamp_column, split_date)</p><p>This code uses your <strong class="source-inline">split_data</strong> helper function. Within the <strong class="source-inline">MMSA_Sample_Folder</strong>, two new folders will be created holding four sets each of training and scoring files.</p></li>
				<li>Connect your Jupyter notebook to your AMLS workspace:<p class="source-code">ws = Workspace.from_config()</p><p>If you are prompted to log in, do so by following the instructions.</p></li>
				<li>Set your datastore to the default datastore that comes with your AMLS workspace:<p class="source-code">datastore = ws.get_default_datastore()</p><p>This code is slightly different than the one you used in <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution</em>. In the AzureML SDK, there are often functions with identical uses.</p></li>
				<li>Upload your training data to your default datastore with the following code:<p class="source-code">ds_train_path = target_path + '_train'</p><p class="source-code">datastore.upload(src_dir=train_path, \</p><p class="source-code">target_path=ds_train_path, overwrite=True)</p><p>This code will write your training files to a folder called <strong class="source-inline">MMSA_Sample_Folder_train</strong> on your default datastore.</p></li>
				<li>Upload your scoring data to your default datastore with the following code:<p class="source-code">ds_inference_path = target_path + '_inference'</p><p class="source-code">datastore.upload(src_dir=inference_path, \</p><p class="source-code">target_path=ds_inference_path, overwrite=True)</p><p>This<a id="_idIndexMarker394"/> code<a id="_idIndexMarker395"/> will write your training files to a folder called <strong class="source-inline">MMSA_Sample_Folder_inference</strong> on your default datastore.</p></li>
				<li>Create file datasets for your training and scoring data with the following code:<p class="source-code">ds_train = \</p><p class="source-code">Dataset.File.from_files(path=\</p><p class="source-code">datastore.path(ds_train_path), validate=False)</p><p class="source-code">ds_inference = Dataset.File.from_files(path=\</p><p class="source-code">datastore.path(ds_inference_path), validate=False)</p><p>The MMSA requires file datasets to work. As such, you need to register the folders on your default datastore as file datasets. This will register the entire folder and all of its contents.</p></li>
				<li>Create variables for the names for when you register the datasets:<p class="source-code">dataset_name = 'MMSA_Sample'</p><p class="source-code">train_dataset_name = dataset_name + '_train'</p><p class="source-code">inference_dataset_name = dataset_name + '_inference'</p><p>Using this code will ensure that you register your datasets with the names <strong class="source-inline">MMSA_Sample_train</strong> and <strong class="source-inline">MMSA_Sample_inference</strong>.</p></li>
				<li>Register your file datasets with the following code:<p class="source-code">ds_train.register(ws, train_dataset_name, create_new_version=True)</p><p class="source-code">ds_inference.register(ws, inference_dataset_name, create_new_version=True)</p><p>You should now have two additional datasets in your AML studio. Check by clicking <strong class="bold">Datasets</strong> on the left-hand panel.</p></li>
			</ol>
			<p>Make sure you save your notebook, as this code will come in very handy in the future when you <a id="_idIndexMarker396"/>wish to use the MMSA <a id="_idIndexMarker397"/>with your own data. You have now prepped both the <strong class="source-inline">OJ Sales</strong> data as well the simple sample data and saved them as separate training and scoring file datasets. This is step number one in using the accelerator. Now that you have prepped your data, it's time for you to train a lot of models.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor103"/>Training many models simultaneously</h1>
			<p>Like prepping <a id="_idIndexMarker398"/>data for many models, training many models is simply a matter of navigating to the correct notebook and running the cells. There's no custom code required, and you are simply required to change a few settings. </p>
			<p>Like prepping data, you will first run the notebook step by step to carefully understand how it works. Once you have that understanding, you will then create a new notebook with code that uses the datasets you made from the sample data. This will benefit you tremendously, as you will understand exactly which parts of the code you need to change<a id="_idIndexMarker399"/> to facilitate your own projects.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor104"/>Training the sample OJ dataset</h2>
			<p>To train<a id="_idIndexMarker400"/> many models <a id="_idIndexMarker401"/>using the<a id="_idIndexMarker402"/> OJ data and to understand the underlying process, follow these instructions step by step:</p>
			<ol>
				<li value="1">From the <strong class="source-inline">solution-accelerator-many-models</strong> folder, click on the <strong class="source-inline">Automated_ML</strong> folder.</li>
				<li>From the <strong class="source-inline">Automated_ML</strong> folder, click on the <strong class="source-inline">02_AutoML_Training_Pipeline</strong> folder.</li>
				<li>Open <strong class="source-inline">02_AutoML_Training_Pipeline.ipynb</strong>.</li>
				<li>Run all of the cells in <strong class="source-inline">section</strong> <strong class="source-inline">1.0</strong>. This sets your datastore and your workspace and also assigns a name to your many models experiment. Notice that this code also outputs a nice table listing your AMLS workspace details along with the name of your datastore. You can add this table to all of your code if you wish or you can use the templates in this book for a more direct, spartan approach to coding.<p class="callout-heading">Tip</p><p class="callout">If you are having trouble loading any Azure libraries, update the Azure MLSDK by running the Update <strong class="source-inline">AzureML SDK.ipynb</strong> notebook foundhere: https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb.</p></li>
				<li>Run the single cell in <strong class="source-inline">section</strong> <strong class="source-inline">2.0</strong>. This retrieves your training dataset, <strong class="source-inline">oj_data_small_train</strong>. Notice that the dataset gets set twice here, first as a typical dataset, and then as <strong class="bold">named input</strong>. Named input is simply an Azure artifact that certain ML pipelines use to work with datasets. Underlying the MMSA is a <strong class="bold">parallel run ML pipeline</strong>. This ML pipeline lets you run different types of jobs in parallel.<p class="callout-heading">Important tip</p><p class="callout">The MMSA often uses <strong class="bold">AzureML contributor packages</strong>. These are packages that are under development. You may have to uncomment out cells and install the packages in this section depending on your version of the AzureML SDK. Any packages you need to install will always be in the code. </p></li>
				<li>Run all of the cells in <strong class="source-inline">section</strong> <strong class="source-inline">3.0</strong>. These cells create a compute cluster to train all of your models, set your AutoML forecasting settings, and set your many models <strong class="bold">parallel run</strong> settings. Parallel run refers to how many model training runs you would like to perform at once. Currently, the limit is 320 although it is subject to increase in the future. <p>The key change to forecasting settings is the addition of <strong class="bold">partition columns</strong>. These are <a id="_idIndexMarker403"/>the <a id="_idIndexMarker404"/>columns <a id="_idIndexMarker405"/>on which different models are trained, in this case, <strong class="source-inline">Store</strong> and <strong class="source-inline">Brand</strong>. If your Azure subscription isn't able to use the <strong class="source-inline">STANDARD_D16S_V3</strong> <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) that the notebook uses, simply change them to <strong class="source-inline">STANDARD_DS3_V2</strong>. </p><p class="callout-heading">Important note</p><p class="callout">The <em class="italic">node count</em> for your parallel run settings should be set to the number of nodes in your compute cluster. Your <em class="italic">process count per node</em> should not exceed the number of cores on each node. If you're using a <strong class="source-inline">Standard_DS3_v2</strong> VM, for example, the process count per node should not exceed <strong class="source-inline">4</strong>.</p></li>
				<li>Run both cells in <strong class="source-inline">section</strong> <strong class="source-inline">4.0</strong>. These cells train all of your models.</li>
				<li>Run the cell in <strong class="source-inline">section</strong> <strong class="source-inline">6.0</strong> to get a list of your AutoML runs and the tags they were registered with. This is how you keep track of all of the different models you have registered. The MMSA automatically generates tags for your partition columns.</li>
				<li>Publish your many models training pipeline by uncommenting the first cell and running it. Do not run the second cell, as this will schedule your pipeline to automatically run on a cadence. Although this feature is useful in a production setting, it does rack up costs.</li>
			</ol>
			<p>Within 15 to 30 <a id="_idIndexMarker406"/>minutes, you <a id="_idIndexMarker407"/>should <a id="_idIndexMarker408"/>have all 10 models trained and registered. Unlike normal AutoML runs, the MMSA automatically registers the best model for each grouping (in this case, orange juice brand and store) you train. This feature scales exceptionally well and some Microsoft customers are using it to train and retrain hundreds of thousands of models on an ongoing basis. </p>
			<p>You can check the run in the portal by clicking the blue link, which will take you to the pipeline visualization seen in the following screenshot:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/Figure_7.2_B16595.jpg" alt="Figure 7.2 – The MMSA in action "/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – The MMSA in action</p>
			<p>Next, you<a id="_idIndexMarker409"/> will <a id="_idIndexMarker410"/>create a new<a id="_idIndexMarker411"/> notebook and train many models with the sample data you loaded in as a pandas dataframe. You will substantially simplify the code in the second notebook to achieve an identical solution. This will help you easily adapt the MMSA to your own problems in the future.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor105"/>Training your sample dataset with the MMSA</h2>
			<p>Just as you<a id="_idIndexMarker412"/> modified <a id="_idIndexMarker413"/>the first notebook, you need to modify the second notebook to use your own code. All the steps will be the same, but the code will be less busy and easier to read. Begin with the following steps:</p>
			<ol>
				<li value="1">Open the <strong class="source-inline">solution-accelerator-many-models</strong> folder.</li>
				<li>Open the <strong class="source-inline">Automated_ML</strong> folder.</li>
				<li>Open the <strong class="source-inline">02_AutoML_Training_Pipeline</strong> folder.</li>
				<li>Create a new Jupyter notebook and open it. Rename it <strong class="source-inline">02_AutoML_Training_Pipeline-My-Data.ipynb</strong>. </li>
				<li>Load in all of the familiar libraries you will need with the following code:<p class="source-code">from azureml.core import Workspace, Datastore, Dataset</p><p class="source-code">from azureml.core import Experiment</p><p class="source-code">from azureml.core.compute import ComputeTarget</p><p class="source-code">import pandas as pd</p><p class="source-code">import os</p><p>You should be familiar with most of these packages from the <em class="italic">Prepping data for many models</em> section. <strong class="source-inline">ComputeTarget</strong> is used to set a compute cluster for remote training and was covered in <a href="B16595_04_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 4</em></a>, <em class="italic">Building an AutoML Regression Solution</em>.</p></li>
				<li>Load in the new libraries you will need to train your MMSA solution with the following code:<p class="source-code">from azureml.contrib.automl.pipeline.steps import AutoMLPipelineBuilder</p><p class="source-code">from azureml.pipeline.core import Pipeline</p><p class="source-code">from scripts.helper import get_training_output</p><p class="source-code">import logging</p><p><strong class="source-inline">AzureMLPipelineBuilder</strong> lets you build out your many models training runs and is a contributor package. Make sure you use pip to install it here if you haven't already using the code that is commented out in the original MMSA training notebook. <strong class="source-inline">Pipeline</strong> lets you build ML pipelines, which are necessary for running the MMSA under the hood. </p><p>Finally, <strong class="source-inline">get_training_output</strong> is another helper function that lets you retrieve information about the models you trained and <strong class="source-inline">logging</strong> enables more detailed logs to be collected regarding your training run. </p></li>
				<li>Connect <a id="_idIndexMarker414"/>your<a id="_idIndexMarker415"/> Jupyter notebook to your AMLS workspace:<p class="source-code">ws = Workspace.from_config()</p><p>If you are prompted to log in, do so by following the instructions.</p></li>
				<li>Set your datastore to the default datastore that comes with your AMLS workspace:<p class="source-code">dstore = ws.get_default_datastore()</p><p>Be careful, as the datastore variable name is different from other Jupyter notebooks.</p></li>
				<li>Set your experiment and give it a name using the following code:<p class="source-code">experiment = Experiment(ws, 'manymodels-training-pipeline-pandas-data')</p></li>
				<li>Specify your training dataset with the following code: <p class="source-code">filedst_10_models = Dataset.get_by_name(ws, name = 'MMSA_Sample_train')</p><p class="source-code">filedst_10_models_input =\</p><p class="source-code">filedst_10_models.as_named_input('MMSA_Sample_train')</p><p>It doesn't matter what name you give to your named input. The important thing is the underlying dataset. The MMSA will find the correct dataset regardless.</p></li>
				<li>Set your compute cluster, which will be used for remote training, with the following code:<p class="source-code">compute = "compute-cluster"</p><p>If you created a different compute cluster for many models training, use that one instead. </p></li>
				<li>Set<a id="_idIndexMarker416"/> your <a id="_idIndexMarker417"/>partition column names:<p class="source-code">partition_column_names = ['Store']  </p><p>You can have as many partition columns as necessary for your business problem. <strong class="source-inline">OJ Sales</strong> has two. The sample data has one.</p></li>
				<li>Adjust your AutoML settings as needed. Specifically, set <strong class="source-inline">label_column_name</strong> to <strong class="source-inline">Sales</strong>. This is the column you are trying to predict. Change the name of <strong class="source-inline">debug_log</strong> to separate it from the training run with <strong class="source-inline">OJ Sales</strong>. Set <strong class="source-inline">time_column_name</strong> to <strong class="source-inline">Date</strong>. Set <strong class="source-inline">grain_column_names</strong> to <strong class="source-inline">Store</strong>. <p>One confusing thing about the MMSA is that you should always pass in your partition columns to <strong class="source-inline">grain_column_names</strong> too. For more information about these settings, refer to <a href="B16595_06_ePub.xhtml#_idTextAnchor081"><em class="italic">Chapter 6</em></a>, <em class="italic">Building an AutoML Forecasting Solution</em>:</p><p class="source-code">automl_settings = {</p><p class="source-code">    "task" : 'forecasting',</p><p class="source-code">    "primary_metric" : \</p><p class="source-code">    'normalized_root_mean_squared_error',</p><p class="source-code">    "iteration_timeout_minutes" : 10, </p><p class="source-code">    "iterations" : 15,</p><p class="source-code">    "experiment_timeout_hours" : 1,</p><p class="source-code">    "label_column_name" : 'Sales',</p><p class="source-code">    "n_cross_validations" : 3,</p><p class="source-code">    "verbosity" : logging.INFO, </p><p class="source-code">    "debug_log": 'automl_pandas_debug.txt',</p><p class="source-code">    "time_column_name": 'Date',</p><p class="source-code">    "max_horizon" : 31,</p><p class="source-code">    "track_child_runs": False,</p><p class="source-code">    "partition_column_names": partition_column_names,</p><p class="source-code">    "grain_column_names": ['Store'],</p><p class="source-code">    "pipeline_fetch_max_batch_size": 15</p><p class="source-code">}</p><p class="callout-heading">Important tip</p><p class="callout">The MMSA <a id="_idIndexMarker418"/>can be used for <a id="_idIndexMarker419"/>regression and classification of AutoML problems too. In that case, make sure you pass in the relevant settings specific to each problem type. </p></li>
				<li>Pass in your MMSA configurations. Make sure you adjust <strong class="source-inline">node_count</strong> and <strong class="source-inline">process_count_per_node</strong> to match the number of nodes on your compute cluster and the number of cores on a single VM, respectively, with the following code:<p class="source-code">train_steps =\</p><p class="source-code">AutoMLPipelineBuilder.get_many_models_train_steps(</p><p class="source-code">experiment=experiment,</p><p class="source-code">                   automl_settings=automl_settings,</p><p class="source-code">                   train_data=filedst_10_models_input,</p><p class="source-code">                   compute_target=compute,</p><p class="source-code">                   partition_column_names=\</p><p class="source-code">                   partition_column_names,</p><p class="source-code">                   node_count=4,</p><p class="source-code">                   process_count_per_node=4,</p><p class="source-code">                   run_invocation_timeout=3700,</p><p class="source-code">                   output_datastore=dstore)</p></li>
				<li>Submit <a id="_idIndexMarker420"/>your <a id="_idIndexMarker421"/>MMSA training run with the following code:<p class="source-code">pipeline = Pipeline(workspace=ws, steps=train_steps)</p><p class="source-code">run = experiment.submit(pipeline)</p></li>
				<li>Get additional details about your MMSA training run with the following code:<p class="source-code">run.wait_for_completion(show_output=True) </p></li>
				<li>Publish your MMSA pipeline with the following code:<p class="source-code">published_pipeline = pipeline.publish(name = \</p><p class="source-code">'MMSA_pandas', description = 'MMSA Solution using a pandas dataframe', \</p><p class="source-code">version = '1', continue_on_step_failure = False)</p><p>This code will publish your pipeline so you can schedule it later at your leisure. Setting <strong class="source-inline">continue_on_step_failure</strong> to <strong class="source-inline">False</strong> will prevent this code from publishing a pipeline that errors out.</p></li>
				<li>If you would like, you can copy code over from the original MMSA training notebook to schedule your MMSA pipeline to run on a cadence. You can also copy over code to look at the results of the overall run; this is very good for debugging errors.</li>
			</ol>
			<p>You have now successfully trained many models using both the <strong class="source-inline">OJ Sales</strong> data and a sample file read in as a pandas dataframe. Instead of modifying the MMSA code from scratch, you also have a simplified notebook on hand that you can easily use to build your own<a id="_idIndexMarker422"/> solution with <a id="_idIndexMarker423"/>the MMSA. Next, you will learn how to score new data with models trained using the accelerator. This will complete your introduction to the MMSA using AutoML. </p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor106"/>Scoring new data for many models</h1>
			<p>Scoring new<a id="_idIndexMarker424"/> data with the MMSA is a fairly straightforward task. Once you have your models trained, simply navigate to the correct notebook, change your variables to match your training notebook, and click the run button. As there are very few settings to alter compared to the training notebook, it's even easier to use with your own code. </p>
			<p>In this section, like the others, first you will run the out-of-the-box scoring notebook with <strong class="source-inline">OJ Sales</strong>. Then, you will create a new notebook to score the sample data. </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor107"/>Scoring OJ sales data with the MMSA</h2>
			<p>To<a id="_idIndexMarker425"/> score <strong class="source-inline">OJ Sales</strong> data <a id="_idIndexMarker426"/>with the multiple models you've trained, follow these steps:</p>
			<ol>
				<li value="1">From the <strong class="source-inline">solution-accelerator-many-models</strong> folder, open the <strong class="source-inline">Automated_ML</strong> folder.</li>
				<li>From the <strong class="source-inline">Automated_ML</strong> folder, open the <strong class="source-inline">03_AutoML_Forecasting_Pipeline</strong> folder.</li>
				<li>Open <strong class="source-inline">03_AutoML_Forecasting_Pipeline.ipynb</strong>.</li>
				<li>Run all of the cells in <strong class="source-inline">section</strong> <strong class="source-inline">1.0</strong>. These cells set up your AMLS workspace, compute cluster, datastore, and experiment. Like the training notebook before it, the forecasting notebook is also an ML pipeline. Information regarding your ML pipeline runs, like your training runs, is saved in experiment artifacts. <p class="callout-heading">Tip</p><p class="callout">Your <em class="italic">training experiment</em> that you ran to train many models is different from your <em class="italic">inferencing experiment</em> that you are running now. Make sure they have different names.</p></li>
				<li>Run the single cell in <strong class="source-inline">section</strong> <strong class="source-inline">2.0</strong>. This cell calls the inferencing dataset you created with the data preparation notebook; this is the data you will score with your trained model.</li>
				<li>In <strong class="source-inline">section</strong> <strong class="source-inline">3.0</strong>, set<a id="_idIndexMarker427"/> your<a id="_idIndexMarker428"/> training <strong class="bold">pipeline run ID</strong> along with the training experiment name. You can find both of these in the <strong class="source-inline">02_AutoML_Training_Pipeline.ipynb</strong> notebook you ran earlier or in the <strong class="bold">Experiments</strong> section of AMLS. Your pipeline run ID is the ID for the experiment that trained all of your models: <p class="source-code">training_pipeline_run_id ="your pipeline run id"</p><p class="source-code">training_experiment_name = "your training experiment name" </p></li>
				<li>Run the second cell in <strong class="source-inline">section</strong> <strong class="source-inline">3.0</strong>. This cell configures the settings for your many models scoring pipeline. Most importantly, this is passing in your partition columns, your target column that you're trying to predict, and your time column, which determines the cadence of your predictions.</li>
				<li>Run the two cells in <strong class="source-inline">section</strong> <strong class="source-inline">4.0</strong> to score new data. This is an ML pipeline run and your compute cluster will take some time to spin up. Once it spins up and the ML pipeline starts, however, scoring your data will be very fast. </li>
				<li>Run the single cell in <strong class="source-inline">section</strong> <strong class="source-inline">5.0</strong> to view your results, as shown in the following screenshot:<div id="_idContainer080" class="IMG---Figure"><img src="image/Figure_7.3_B16595.jpg" alt="Figure 7.3 – MMSA results "/></div><p class="figure-caption">Figure 7.3 – MMSA results </p></li>
				<li>If you would <a id="_idIndexMarker429"/>like <a id="_idIndexMarker430"/>to publish your ML pipeline for later use, run the first cell in <strong class="source-inline">section</strong> <strong class="source-inline">6.0</strong>. Avoid running the second cell, as this creates an automated schedule for your pipeline that can become quite costly over time.</li>
			</ol>
			<p>You have now completed the MMSA <strong class="source-inline">OJ Sales</strong> notebooks from start to finish. You have prepped data and shaped it into the right format, splitting it into many files. Then, you trained 10 models in parallel and used those models to score the data. </p>
			<p>It's time to do the same and score your sample dataset with a simplified notebook and see the output. Keep in mind that the output should not be especially good, as the sales numbers were randomly generated. This will, however, provide you with a template for generating <a id="_idIndexMarker431"/>results <a id="_idIndexMarker432"/>with your own data. </p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor108"/>Scoring your sample dataset with many models</h2>
			<p>In order to score <a id="_idIndexMarker433"/>data from your sample dataset, observe the following instructions:</p>
			<ol>
				<li value="1">Open the <strong class="source-inline">solution-accelerator-many-models</strong> folder.</li>
				<li>Open the <strong class="source-inline">Automated_ML</strong> folder.</li>
				<li>Open the <strong class="source-inline">03_AutoML_Forecasting_Pipeline</strong> folder.</li>
				<li>Create a new Jupyter notebook and open it. Rename it <strong class="source-inline">03_AutoML_Forecasting_Pipeline-My-Data.ipynb</strong>. </li>
				<li>Load in all of the libraries you have already used in this chapter with the following code:<p class="source-code">from azureml.core import Workspace, Datastore, Dataset</p><p class="source-code">from azureml.core import Experiment</p><p class="source-code">import pandas as pd</p><p class="source-code">import os</p><p class="source-code">from azureml.core.compute import ComputeTarget</p><p class="source-code">from azureml.contrib.automl.pipeline.steps import AutoMLPipelineBuilder</p><p class="source-code">from azureml.pipeline.core import Pipeline</p><p>If you need a refresher on any of these, please refer to the <em class="italic">Training many models simultaneously</em> section.</p></li>
				<li>Load in the libraries that are new to this section with the following code:<p class="source-code">import shutil</p><p class="source-code">import sys </p><p class="source-code">from scripts.helper import get_forecasting_output</p><p>You have another helper function here, <strong class="source-inline">get_forecasting_output</strong>. This lets you easily retrieve the predictions generated by the MMSA without any hassle. Both <strong class="source-inline">sys</strong> and <strong class="source-inline">shutil</strong> are used by <strong class="source-inline">get_forecasting_output</strong>. While <strong class="source-inline">shutil</strong> lets you manipulate files and folders similar to <strong class="source-inline">os</strong>, <strong class="source-inline">sys</strong> lets you interact with the Python runtime environment.</p></li>
				<li>Connect your Jupyter notebook to your AMLS workspace:<p class="source-code">ws = Workspace.from_config()</p><p>If you are prompted to log in, do so by following the instructions.</p></li>
				<li>Set<a id="_idIndexMarker434"/> your datastore to the default datastore that comes with your AMLS workspace:<p class="source-code">dstore = ws.get_default_datastore()</p><p>This uses the same datastore variable name as the training notebook.</p></li>
				<li>Set your experiment and give it a name with the help of the following code:<p class="source-code">experiment = Experiment(ws, 'manymodels-forecasting-pipeline-pandas-data')</p></li>
				<li>Specify your training dataset with the following code: <p class="source-code">filedst_10_models = Dataset.get_by_name(ws, name = 'MMSA_Sample_inference')</p><p class="source-code">filedst_10_models_input =\</p><p class="source-code">filedst_10_models.as_named_input('MMSA_Sample_inference')</p><p>It doesn't matter what name you give to your named input. The important thing is the underlying dataset. The MMSA will find the correct dataset regardless.</p></li>
				<li>Set your compute cluster, which will be used for remote training, with the following code:<p class="source-code">compute = "compute-cluster"</p></li>
				<li>Retrieve your experiment name and run ID from the ML pipeline you used to train the models. You can find the run ID in AML studio under <strong class="bold">Experiments</strong>: <p class="source-code">training_experiment_name = "manymodels-training-pipeline-pandas-data"</p><p class="source-code">training_pipeline_run_id ="your-run-ID"</p></li>
				<li>Set <a id="_idIndexMarker435"/>your partition column names:<p class="source-code">partition_column_names = ['Store']</p></li>
				<li>Pass in your MMSA configurations. Make sure that you set <strong class="source-inline">time_column_name</strong> to <strong class="source-inline">Date</strong>, <strong class="source-inline">target_column_name</strong> that you are trying to predict to <strong class="source-inline">Sales</strong>, <strong class="source-inline">node_count</strong> to the maximum number of nodes on your compute cluster, and <strong class="source-inline">process_count_per_node</strong> to the number of cores on a single VM with the help of the following code:<p class="source-code">inference_steps =\</p><p class="source-code">AutoMLPipelineBuilder.get_many_models_batch_inference_steps(\</p><p class="source-code">                    experiment=experiment,</p><p class="source-code">                    inference_data=\</p><p class="source-code">                    filedst_10_models_input,</p><p class="source-code">                    compute_target=compute,</p><p class="source-code">                    node_count=4,</p><p class="source-code">                    process_count_per_node=4,</p><p class="source-code">                    run_invocation_timeout=300,</p><p class="source-code">                    train_experiment_name=\</p><p class="source-code">                    training_experiment_name,</p><p class="source-code">                    train_run_id=\</p><p class="source-code">                    training_pipeline_run_id,</p><p class="source-code">                    partition_column_names=\</p><p class="source-code">                    partition_column_names,</p><p class="source-code">                    time_column_name="Date",</p><p class="source-code">                    target_column_name="Sales")</p></li>
				<li>Submit your MMSA scoring run with the following code:<p class="source-code">pipeline = Pipeline(workspace=ws, steps=train_steps)</p><p class="source-code">run = experiment.submit(pipeline)</p></li>
				<li>Get <a id="_idIndexMarker436"/>additional details about your MMSA scoring run with the following code:<p class="source-code">run.wait_for_completion(show_output=True) </p><p>This should only take a few minutes to run.</p></li>
				<li>Once your ML pipeline has finished, publish your ML pipeline with the following code:<p class="source-code">published_pipeline = pipeline.publish(name = 'automl_score_many_models_pandas',</p><p class="source-code">                   description = \</p><p class="source-code">                  'MMSA Solution using x data',</p><p class="source-code">                   version = '1',</p><p class="source-code">                   continue_on_step_failure = False)</p><p>Publishing your scoring pipeline will let you run it again very easily in the future. You will learn more about ML pipelines in <a href="B16595_09_ePub.xhtml#_idTextAnchor129"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing a Batch Scoring Solution</em>.</p></li>
				<li>View the first 10 rows of your results with the following code:<p class="source-code">forecasting_results_name = "forecasting_results"</p><p class="source-code">forecasting_output_name =\</p><p class="source-code">"many_models_inference_output"</p><p class="source-code">forecast_file = get_forecasting_output(run,\</p><p class="source-code">forecasting_results_name, forecasting_output_name)</p><p class="source-code">df = pd.read_csv(forecast_file, delimiter=" ",\</p><p class="source-code">header=None)</p><p class="source-code">df.columns = ["Date", "Store", "Sales", "Predicted" ]</p><p class="source-code">df.head(10)</p><p>For this <a id="_idIndexMarker437"/>code to work, you need to manually enter the column names of your dataset. The last column will always be the predictions generated by your solution. Your results should resemble the following screenshot. Since the data was random, it shouldn't be good:</p></li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/Figure_7.4_B16595.jpg" alt="Figure 7.4 – Many model results on the sample data  "/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Many model results on the sample data </p>
			<p>Success! You <a id="_idIndexMarker438"/>have reached the ultimate goal of creating a complete MMSA solution with sample data read in as a pandas dataframe. You are now in prime position to use the solution with your own data. </p>
			<p>The first time you run the MMSA using only the <strong class="source-inline">OJ Sales</strong> data, it seems like it's really easy and there's nothing to it. Despite being easy, you will find that it produces superior results compared to the single model you trained in <a href="B16595_06_ePub.xhtml#_idTextAnchor081"><em class="italic">Chapter 6</em></a>, <em class="italic">Building a Forecasting Solution</em>. By simply running a few notebooks in the correct order, you are able to produce a high performing model. </p>
			<p>Experience has now taught you how you need to adjust the MMSA to work with your own data and that was pretty straightforward too. However, the first time you try to apply your <a id="_idIndexMarker439"/>own data to it, you may find it a bit tricky. Getting your data into just the right format can be frustrating. To help make your experience smooth, the final portion of this chapter covers tips and tricks to improve your solution.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor109"/>Improving your many models results</h1>
			<p>Now that you <a id="_idIndexMarker440"/>have adapted all three of the notebooks to run your own code, you should be feeling pretty confident in your ability to use the MMSA. Still, it's pretty easy to get stuck. Many models is a complicated framework and small errors in your data can lead to errors. </p>
			<p>Additionally, sometimes it's really hard to know what your data will look like when you are dealing with thousands of files you wish to train. Here is some good advice to follow in order to ensure you do not come to an impasse when using your own data with the MMSA:</p>
			<ul>
				<li>Before using the accelerator, always try creating a single model first with your entire dataset. Check the performance of your model. Only use the MMSA if the single model's performance is subpar compared to your expectations or in a situation where obtaining the best accuracy is mission-critical for your project. Sometimes, the trade-off between complexity and performance isn't worth it.</li>
				<li>Spend a lot of time ensuring that your data is split correctly before using the accelerator. Each combination of partition columns needs to have its own file. Think carefully as to which columns you would like to use as your partitions. Alternatively, try out a few different combinations to see which gives the best performance.</li>
				<li>When splitting data for forecasting using a date column, absolutely make sure that it is in a datetime format as opposed to a string format. Oftentimes, data scientists will make the mistake of splitting on a string column. Sometimes when this happens, the first two notebooks will run as is and you will be able to train models. However, when you get to the third notebook to forecast data, you will get an error and have to start from the beginning.</li>
				<li>In the Data Preparation notebook, do not hardcode <strong class="source-inline">split_date</strong>. Instead, make it variable based on the current datetime, how much training data you expect to have, and how many hours, days, weeks, or months out you would like to forecast. This way, when you go to retrain the MMSA solution, you will get forecasts for the appropriate time periods. Keep in mind that this is only relevant for forecasting problems.</li>
				<li>For all problems, carefully study your data before passing it into the solution. While AutoML will handle null values and many other errors, it does less with other <a id="_idIndexMarker441"/>problems, such as large gaps in your forecasting problem's time column. Clean your data as much as possible before passing it into the MMSA.</li>
				<li>While the MMSA notebook was created using a forecasting example, it's quite easy to adapt the notebook for regression and classification problems. As these are inherently less complicated than forecasting, importing your own code for these problems is comparatively easier. You don't have to worry about dates.</li>
				<li>Become familiar with the log files. When you navigate them, make sure you first click on the pipeline step that failed. Then, click <strong class="bold">Outputs + logs</strong>. You want to look for a folder called <strong class="source-inline">logs</strong> and expand it. Then, look for a folder called <strong class="source-inline">user</strong>. Within the <strong class="source-inline">user</strong> folder, you need to search for the <strong class="source-inline">error</strong> folder. The <strong class="source-inline">error</strong> folder contains numerous folders that are a series of numbers separated by periods such as <strong class="source-inline">10.0.0.5</strong>. <p>These folders hold the most important files for debugging purposes. Each file starts with <strong class="source-inline">process</strong> and ends with <strong class="source-inline">.txt</strong>. Open up these files and use them to find any errors in your code.</p></li>
				<li>Do not be afraid to use very large compute clusters when training models with the MMSA. Although larger VMs cost more per hour to use, they also train much quicker than their cheaper counterparts.</li>
				<li>Keep in mind that when you are training models using the MMSA, the compute cluster running it will be at maximum capacity for a comparatively long time depending on how many models you are training. As such, it makes sense to make sure that the compute cluster you use to train many models isn't responsible for running other jobs at the same time.</li>
				<li>The key problem that the MMSA solves is that, when you have multiple, high-dimensional categorical columns, many traditional ML algorithms underperform for mathematical reasons. As your business expands, your products expand, your locations expand, your workforce expands, and<a id="_idIndexMarker442"/> the MMSA becomes more and more appropriate for your business.</li>
				<li>Retrain your models frequently on a schedule. It's very difficult to monitor hundreds of thousands of ML models to determine which ones need to be retrained. Instead, retrain all of them on a weekly or monthly basis to ensure high performance. </li>
			</ul>
			<p>Although you have received many tips and tricks that will help you build solutions using the MMSA, the best advice is simply to practice as much as possible. Explore the solution as much as possible and uncover as many caveats as you can. </p>
			<p>More than anything, remember that the MMSA and AutoML perform best when you pass in clean data. This section concludes this chapter. You now have the expertise, knowledge, and practice to implement a truly game-changing solution in the world of automated ML.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor110"/>Summary</h1>
			<p>Advanced solutions like the MMSA are at the bleeding edge of ML and AI. It is a truly state-of-the-art technology and now it's another tool in your belt. </p>
			<p>You've not only run all three notebooks on the <strong class="source-inline">OJ Sales</strong> data, but you have also converted the code to take in other datasets and understand how it works. Prepping data, training models, and forecasting the future using the MMSA are all things you have done and could do again. You may already have a use case to which you can apply it, or you may have to wait a few more years until your company is ready, but you are prepared.</p>
			<p><a href="B16595_08_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 8</em></a>, <em class="italic">Choosing Real-Time versus Batch Scoring</em>, continues your journey at the forefront of the ML world. Once you build a model in AutoML, the next step is to deploy it, and there are two options: batch versus real-time scoring. You will learn when to use batch scoring, when to use real-time scoring, and the main differences between the two. Mastering these concepts is key to successfully applying your AutoML models to real-world business scenarios. </p>
		</div>
</body></html>