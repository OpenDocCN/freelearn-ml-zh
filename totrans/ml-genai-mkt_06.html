<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer218">
    <h1 class="chapterNumber">6</h1>
    <h1 class="chapterTitle" id="_idParaDest-146">Leveraging Predictive Analytics and A/B Testing for Customer Engagement</h1>
    <p class="normal">There are various ways that we can benefit from data-driven and AI/ML-driven marketing techniques. To name a few, you can optimize your marketing strategy based on the key drivers behind the success and failures of your previous marketing campaigns, as we discussed in <em class="chapterRef">Chapters 2</em> and <em class="chapterRef">3</em>. You can also optimize your marketing strategy based on the trend and seasonality within your business or based on the customer sentiments around your products and business, as we have discussed in <em class="chapterRef">Chapters 4</em> and <em class="chapterRef">5</em>. Targeted product recommendation (<em class="chapterRef">Chapter 7</em>) and optimizing the marketing content with Generative AI (<em class="chapterRef">Chapters 9</em> and<em class="chapterRef"> 10</em>) are some other key benefits of applying AI/ML in marketing.</p>
    <p class="normal">Among those mentioned, we are going to experiment with predictive analytics in this chapter and how you can utilize these predictive models in your next marketing campaign. By intelligently predicting the expected behaviors of customers, you can target subgroups of the customer base that are likely to result in your favor. This way, instead of mass marketing to the entire potential customer base, you can better custom-tailor your marketing messages and also save on marketing costs as you are only targeting the group with the higher chance of success. We will also discuss how A/B testing can help decide the best predictive model for your next marketing effort.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Predicting customer conversion with tree-based algorithms</li>
      <li class="bulletList">Predicting customer conversion with deep learning algorithms</li>
      <li class="bulletList">Conducting A/B testing for optimal model choice</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-147">Predicting customer conversion with tree-based algorithms</h1>
    <p class="normal">Predictive analytics or<a id="_idIndexMarker509"/> modeling<a id="_idIndexMarker510"/> can be applied at various stages of the customer life cycle. If you recall from <em class="chapterRef">Chapter 2</em>, there are largely five stages that we can break down a <a id="_idIndexMarker511"/>customer life cycle into: <strong class="keyWord">Awareness</strong>, <strong class="keyWord">Engagement</strong>, <strong class="keyWord">Conversion</strong>, <strong class="keyWord">Retention</strong>, and <strong class="keyWord">Loyalty</strong>, as shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_01.png"/></figure>
    <p class="packt_figref">Figure 6.1: Customer life cycle diagram from Chapter 2</p>
    <p class="normal">The applicability of predictive modeling is broad, depending on your marketing goal. For example, if you have a new brand or product launch and would like to improve new product awareness via ads on social media, you can build predictive models that can help you identify the target customers who are likely to click on the ads. On the other hand, if you would like to improve product purchase conversion rates, you can build predictive models that can identify customers who are more likely to make purchases in the next X number of days and target them. This results in more effective marketing, as you can avoid creating fatigue among the customers, which happens when they are exposed to your marketing campaigns too frequently with irrelevant content. This happens often when you do mass marketing without targeting the right subgroup of customers. Also, you can reduce marketing costs by sending marketing materials only to a specific subgroup of customers. This will help you repurpose the remaining marketing budget for other marketing campaigns.</p>
    <p class="normal">Not only can you utilize predictive analytics for better brand awareness, engagement, and conversion, but predictive analytics can also be used to improve retention rates. Often, customer churn likelihood models are built to identify who is at risk of turning away from your business. Through these customer churn predictions, you can build marketing strategies <a id="_idIndexMarker512"/>and marketing content that is customized for this high-churn risk group to bring them<a id="_idIndexMarker513"/> back to engaged customers. Discounts, free subscription trials, or free plan upgrades are often offered to this high-churn risk group as part of the retention strategies.</p>
    <h2 class="heading-2" id="_idParaDest-148">Tree-based machine learning algorithms</h2>
    <p class="normal">Numerous AI/ML algorithms can <a id="_idIndexMarker514"/>be used for predictive modeling, such as linear regression, logistic regression, and decision tree models, which we have discussed in previous chapters, as well as deep learning models that are rising in usage. In this chapter, we are going to build predictive models with tree-based models, such as random forest and gradient boosted trees, and neural network models, which are the backbones of deep learning models. Underneath any tree-based ML model, there is a decision tree. As we have discussed in <em class="chapterRef">Chapter 3</em>, a decision tree is like a flowchart, where it splits into child nodes based on the information gained. Each node represents a question or criteria for a split and each branch or edge represents the outcome of the question posited at the node. The following diagram shows a high-level overview of how a decision tree may be built:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_02.png"/></figure>
    <p class="packt_figref">Figure 6.2: Illustration of a decision tree</p>
    <p class="normal">Among <a id="_idIndexMarker515"/>numerous tree-based ML models, the <strong class="keyWord">gradient-boosted decision tree</strong> (<strong class="keyWord">GBDT</strong>) and random forest are the two most popular<a id="_idIndexMarker516"/> models that are frequently used for predictive modeling. Both GBDT and random forest models are built with multiple decision trees. However, the main difference is how these decision trees are built.</p>
    <p class="normal">Simply put, a random forest model<a id="_idIndexMarker517"/> is one with lots of decision trees, where each decision tree is built with a random subsample of the dataset and a random subset of features. This way, each decision tree within a random forest learns the information or relationships within the data slightly differently and with different focus areas. </p>
    <p class="normal">The final prediction is the average of all the outcomes or predictions of these individual decision trees. The following shows an illustration of a random forest:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_03.png"/></figure>
    <p class="packt_figref">Figure 6.3: Illustration of random forest</p>
    <p class="normal">A GBDT model, on the<a id="_idIndexMarker518"/> other hand, also consists of <a id="_idIndexMarker519"/>lots of decision trees, but each decision tree is built sequentially, and each subsequent decision tree is trained based on the errors that the previous decision tree makes. The final prediction of a GBDT model is the weighted average of all of the individual decision trees’ predictions. The following shows an illustration of a GBDT model:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_04.png"/></figure>
    <p class="packt_figref">Figure 6.4: Illustration of GBDTs</p>
    <h2 class="heading-2" id="_idParaDest-149">Building random forest models</h2>
    <p class="normal">In this chapter, we<a id="_idIndexMarker520"/> will be using an online purchase dataset as an example to build a predictive model to predict whether a customer will convert or not. First, we will discuss how we can build a random forest model in Python using the <code class="inlineCode">scikit-learn</code> package.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>: <a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6"><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.6</span></a></p>
      <p class="normal"><strong class="keyWord">Data source</strong>: <a href="https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset"><span class="url">https://archive.ics.uci.edu/dataset/468/online+shoppers +purchasing+intention+dataset</span></a></p>
    </div>
    <h3 class="heading-" id="_idParaDest-150">Target and feature variables</h3>
    <p class="normal">We <a id="_idIndexMarker521"/>need to first define the target and feature variables, where the target variable is the factor that we want to predict, and the feature variables are the factors that will be learned by the models to make the predictions or decisions. To do this, you can follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Let’s first load the data into a DataFrame and examine what features we can use for our random forest model:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.read_csv(<span class="hljs-string">"./data.csv"</span>)
df.info()
</code></pre>
      </li>
    </ol>
    <p class="normal">When you run this code, you should see the following output for the information about this data:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_05.png"/></figure>
    <p class="packt_figref">Figure 6.5: Summary of the example dataset</p>
    <ol>
      <li class="numberedList" value="2">The <a id="_idIndexMarker522"/>first thing to note in the preceding output is the column, <code class="inlineCode">Revenue</code>, which is the target variable that tells us whether a customer made a purchase or converted or not, has a type of <code class="inlineCode">Boolean</code>. The other column, <code class="inlineCode">Weekend</code>, also has a <code class="inlineCode">Boolean</code> data type. We are going to encode them as <code class="inlineCode">0</code> for False and <code class="inlineCode">1</code> for True with the following code:
        <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">"Revenue"</span>] = df[<span class="hljs-string">"Revenue"</span>].astype(<span class="hljs-built_in">int</span>)
df[<span class="hljs-string">"Weekend"</span>] = df[<span class="hljs-string">"Weekend"</span>].astype(<span class="hljs-built_in">int</span>)
</code></pre>
      </li>
      <li class="numberedList">Then, two columns have an <code class="inlineCode">object</code> as the data type, <code class="inlineCode">Month</code> and <code class="inlineCode">VisitorType</code>. If you look closer, the column, <code class="inlineCode">Month</code>, has string values for the months, which we will convert into corresponding month numbers. The column, <code class="inlineCode">VisitorType</code>, has three unique values, <code class="inlineCode">New_Visitor</code>, <code class="inlineCode">Returning_Visitor</code>, and <code class="inlineCode">Other</code>. We are going to encode each as <code class="inlineCode">0</code>, <code class="inlineCode">1</code>, and <code class="inlineCode">2</code> respectively, as shown in the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> strptime
df[<span class="hljs-string">"MonthNum"</span>] = df[<span class="hljs-string">"Month"</span>].apply(<span class="hljs-keyword">lambda</span> x: strptime(x[:<span class="hljs-number">3</span>],<span class="hljs-string">'%b'</span>).tm_mon)
df[<span class="hljs-string">"VisitorTypeNum"</span>] = df[<span class="hljs-string">"VisitorType"</span>].apply(
        <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">"New_Visitor"</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">"Returning_Visitor"</span> <span class="hljs-keyword">else</span> <span class="hljs-number">2</span>
)
</code></pre>
      </li>
    </ol>
    <p class="normal">As you <a id="_idIndexMarker523"/>can see in this code, we are using the <code class="inlineCode">strptime</code> function of a <code class="inlineCode">time</code> module to encode three-letter month string values into corresponding month numbers. Then, we use the <code class="inlineCode">apply</code> function of a <code class="inlineCode">pandas</code> DataFrame to encode each value of the <code class="inlineCode">VisitorType</code> column into corresponding integer values.</p>
    <ol>
      <li class="numberedList" value="4">Now that we have converted all the column values into numeric values, we are going to define the target and feature variables as in the following:
        <pre class="programlisting code"><code class="hljs-code">TARGET = <span class="hljs-string">"Revenue"</span>
FEATURES = [
    <span class="hljs-string">'Administrative'</span>,
    <span class="hljs-string">'Administrative_Duration'</span>,
    <span class="hljs-string">'BounceRates'</span>,
    <span class="hljs-string">'Browser'</span>,
    <span class="hljs-string">'</span><span class="hljs-string">ExitRates'</span>,
    <span class="hljs-string">'Informational'</span>,
    <span class="hljs-string">'Informational_Duration'</span>,
    <span class="hljs-string">'MonthNum'</span>,
    <span class="hljs-string">'OperatingSystems'</span>,
    <span class="hljs-string">'PageValues'</span>,
    <span class="hljs-string">'ProductRelated'</span>,
    <span class="hljs-string">'ProductRelated_Duration'</span>,
    <span class="hljs-string">'Region'</span>,
    <span class="hljs-string">'SpecialDay'</span>,
    <span class="hljs-string">'</span><span class="hljs-string">TrafficType'</span>,
    <span class="hljs-string">'VisitorTypeNum'</span>,
    <span class="hljs-string">'Weekend'</span>
]
X = df[FEATURES]
Y = df[TARGET]
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see from this code, we have defined the target variable, <code class="inlineCode">TARGET</code>, to use the <code class="inlineCode">Revenue</code> column and the rest columns as the feature variables, <code class="inlineCode">FEATURES</code>. Then, we create a DataFrame, <code class="inlineCode">X</code>, which is the feature set, and <code class="inlineCode">Y</code>, which is the target series.</p>
    <ol>
      <li class="numberedList" value="5">Lastly, we will split these target and feature sets into train and test sets with the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
train_x, test_x, train_y, test_y= train_test_split(
    X, Y, test_size=<span class="hljs-number">0.2</span>
)
</code></pre>
      </li>
    </ol>
    <p class="normal">We are using the <code class="inlineCode">train_test_split</code> function within the <code class="inlineCode">sklearn.model_selection</code> module. As you can see from the <code class="inlineCode">test_size</code> parameter, we are using 80% of the dataset for training and the other 20% for testing.</p>
    <p class="normal">With these <a id="_idIndexMarker524"/>train and test sets, we are now ready to train a random forest model.</p>
    <h3 class="heading-" id="_idParaDest-151">Training a random forest model</h3>
    <p class="normal">Python’s <code class="inlineCode">scikit-learn</code> package<a id="_idIndexMarker525"/> provides a handy way to a random forest model. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
rf_model = RandomForestClassifier(
    n_estimators=<span class="hljs-number">250</span>, max_depth=<span class="hljs-number">5</span>, class_weight=<span class="hljs-string">"balanced"</span>, n_jobs=-<span class="hljs-number">1</span>
)
rf_model.fit(train_x, train_y)
</code></pre>
    <p class="normal">Let’s take a closer look at this code. We are using the <code class="inlineCode">RandomForestClassifier</code> class from the <code class="inlineCode">sklearn.ensemble</code> module and initiated a Random Forest model with <code class="inlineCode">n_estimators, max_depth, class_weight, </code>and<code class="inlineCode"> n_jobs</code> parameters:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">n_estimators</code> parameter defines how many individual decision trees to build.</li>
      <li class="bulletList">The <code class="inlineCode">max_depth</code> parameter defines how deep each decision tree can grow. Along with other parameters, such as <code class="inlineCode">min_samples_split</code>, <code class="inlineCode">min_samples_leaf</code>, and <code class="inlineCode">max_features</code>, <code class="inlineCode">max_depth</code> helps prevent overfitting issues by limiting how much a decision tree can grow.</li>
      <li class="bulletList">The <code class="inlineCode">class_weight</code> parameter defines weights for each class. This parameter is useful when the dataset is imbalanced. In our example dataset, only about 15% are in the positive class, meaning only 15% of the target variable, Revenue, has a value of 1, or only 15% of customers have converted. You can give custom weights for each class as a dictionary or use the <code class="inlineCode">"balanced"</code> option to automatically adjust the weights that are inversely proportional to the actual class frequencies.</li>
      <li class="bulletList">Lastly, the <code class="inlineCode">n_jobs</code> parameter defines how many jobs to run in parallel. If you recall from our discussion of random forest and GBDTs, random forest is a bag of decision trees, so individual trees can be built in parallel without any dependency on other trees. By giving <code class="inlineCode">-1</code> as the input for this parameter, you are instructing it to use all available resources to train this random forest model.</li>
    </ul>
    <p class="normal">With these <a id="_idIndexMarker526"/>parameters, we can now train this random forest model, using the <code class="inlineCode">fit</code> function with the train set.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Overfitting versus underfitting?</strong></p>
      <p class="normal">Overfitting <a id="_idIndexMarker527"/>refers to when models are fit to the training set so closely that it does well on the training data but poorly on the data that the models have not seen before. Underfitting, on the other hand, is<a id="_idIndexMarker528"/> when models are over-generalized or do not tune well enough to the training set that they did not learn the relationships between the feature variables and the target variable. Multiple iterations of hyperparameter tuning are often required to find the sweet spot for minimal overfitting.</p>
    </div>
    <h3 class="heading-" id="_idParaDest-152">Predicting and evaluating random forest model</h3>
    <p class="normal">The <code class="inlineCode">RandomForestClassifier</code> object provides handy functions for making predictions from the<a id="_idIndexMarker529"/> trained random forest model. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">rf_pred = rf_model.predict(test_x)
rf_pred_proba = rf_model.predict_proba(test_x)[:,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">As the names suggest, the <code class="inlineCode">predict</code> function makes predictions on the given input. In our case, the results will be a list of 0s and 1s for each record in the test set, as we are predicting whether a customer has converted or not.</p>
    <p class="normal">The <code class="inlineCode">predict_proba</code> function also makes predictions on the given input, but the difference is it gives predicted probabilities running between <code class="inlineCode">0</code> and <code class="inlineCode">1</code>. It returns predicted probabilities for each record and for each class, so, in our case, it returns two values for each record, where the first element is a predicted probability to be a class of <code class="inlineCode">0</code> and the second element is a predicted probability to be a class of <code class="inlineCode">1</code>. Since we are only interested in the predicted probability of class 1, we are slicing it with <code class="inlineCode">[:,1]</code> so that we have a list of predicted probabilities of conversion.</p>
    <p class="normal">Now that we have the predicted conversion probabilities, we need to evaluate how good our<a id="_idIndexMarker530"/> predictions are. </p>
    <p class="normal">There are multiple ways to evaluate the accuracy and effectiveness of a predictive model, but we will mainly look at the overall accuracy, precision, recall, <strong class="keyWord">area under the curve</strong> (<strong class="keyWord">AUC</strong>) - <strong class="keyWord">receiver operating characteristics</strong> (<strong class="keyWord">ROC</strong>) curve, and <a id="_idIndexMarker531"/>confusion matrix. We will go deeper into these metrics with examples.</p>
    <p class="normal">As the name suggests, the <strong class="keyWord">accuracy</strong> is the percentage of correct predictions or <strong class="keyWord">true positives</strong> (<strong class="keyWord">TP</strong>) among all the predictions. The <strong class="keyWord">precision</strong> is<a id="_idIndexMarker532"/> the percentage of correct predictions among those predictive positive or the percentage of TPs among those predicted to be positive that include <strong class="keyWord">false positives</strong> (<strong class="keyWord">FP</strong>). The <strong class="keyWord">recall</strong> is the percentage of positive cases identified by the model or the percentage of TPs among the actual positives, which are TPs and false negatives. The equations for the accuracy, precision, and recall are as follows:</p>
    <p class="center"><a id="_idIndexMarker533"/><img alt="" src="../Images/B30999_06_001.png"/></p>
    <p class="center"><a id="_idIndexMarker534"/><img alt="" src="../Images/B30999_06_002.png"/></p>
    <p class="center"><a id="_idIndexMarker535"/><img alt="" src="../Images/B30999_06_003.png"/></p>
    <p class="normal">In Python, the scikit-learn package provides a handy tool for computing these metrics, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
accuracy = (test_y == rf_pred).mean()
precision = metrics.precision_score(test_y, rf_pred)
recall = metrics.recall_score(test_y, rf_pred)
</code></pre>
    <p class="normal">In our example, when these codes are run, the results of these key metrics look as in the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_06.png"/></figure>
    <p class="packt_figref">Figure 6.6: Summary of random forest model performance metrics</p>
    <p class="normal">These results suggest the random forest model we have trained has decent overall accuracy and recall rates but does not seem to do well with precision. This suggests that of those customers this model predicted to be positive or likely to convert, only about 54% of them have actually converted. However, if you recall, the actual overall conversion rate is 15%; in other words, if you randomly guess who will convert, you may only be right about 15% of the time.</p>
    <p class="normal">Thus, since this model has predicted converted customers 54% of the time, it proves to be way much more effective in selecting customers that are more likely to convert than random guesses. Also, the high recall rate suggests that about 85% of the customers who have actually converted are among those who are predicted to be highly likely to convert by this model.</p>
    <p class="normal">From an actual marketing perspective, if you have marketed only to the customers who have been predicted to be likely to convert by this model, you would have still captured most of those conversions. Also, if you have marketed to your entire customer base, 85% (100% minus 15%, which is the overall conversion rate) of your marketing spend would have been wasted. But if you have marketed only to these highly likely customers, only about 46% (100% minus 54%, which is the precision of this model) of the marketing spend would have been wasted.</p>
    <p class="normal">The other <a id="_idIndexMarker536"/>key evaluation metric we are going to look <a id="_idIndexMarker537"/>at is the <strong class="keyWord">AUC - ROC curve</strong>. The <strong class="keyWord">ROC curve</strong>, simply put, shows<a id="_idIndexMarker538"/> the trade-offs between gains in <strong class="keyWord">true positive rates</strong> (<strong class="keyWord">TPRs</strong>) for each sacrifice you make for a <strong class="keyWord">false positive rate</strong> (<strong class="keyWord">FPR</strong>). The <strong class="keyWord">AUC</strong> is, as the name suggests, the area under the ROC curve and tells us how well the model separates the positive cases from <a id="_idIndexMarker539"/>negative cases. The AUC ranges from <code class="inlineCode">0</code> to <code class="inlineCode">1</code> and the higher it is, the better the model is. At the AUC of <code class="inlineCode">0.5</code>, it suggests that the model performs the same as random guessing.</p>
    <p class="normal">The following code can be used to <a id="_idIndexMarker540"/>plot the ROC curve:</p>
    <pre class="programlisting code"><code class="hljs-code">dp = metrics.RocCurveDisplay.from_predictions(
    test_y,
    rf_pred_proba,
    name=<span class="hljs-string">"Conversion"</span>,
    color=<span class="hljs-string">"darkorange"</span>,
)
_ = dp.ax_.<span class="hljs-built_in">set</span>(
    xlabel=<span class="hljs-string">"False Positive Rate"</span>,
    ylabel=<span class="hljs-string">"True Positive Rate"</span>,
    title=<span class="hljs-string">"ROC Curve"</span>,
)
plt.grid()
plt.show()
</code></pre>
    <p class="normal">Here, we <a id="_idIndexMarker541"/>are using the <code class="inlineCode">metrics</code> module again to plot the ROC curve. The main difference between <a id="_idIndexMarker542"/>computing the accuracy, precision, and recall and computing the AUC - ROC curve is how we use the predicted probabilities, <code class="inlineCode">rf_pred_proba</code>, instead of predicted labels, <code class="inlineCode">pred</code>. This is because the ROC curve examines how TPRs and FPRs change at different probability levels. By using predicted probabilities, we can calculate how the TPR and FPR change as the decision threshold varies. The resulting chart looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_07.png"/></figure>
    <p class="packt_figref">Figure 6.7: AUC - ROC curve of the random forest model predictions</p>
    <p class="normal">As you can see from this chart, you can easily evaluate how each sacrifice in the FPR affects the TPR. For example, in this chart, at 20% FPR, we already achieve about 90% TPR, which suggests that this model does well separating positive cases from negative cases. The AUC here is <code class="inlineCode">0.93</code>, which also suggests that the model does well in identifying positive cases from negative cases.</p>
    <p class="normal">Lastly, we will<a id="_idIndexMarker543"/> look at the <strong class="keyWord">confusion matrix</strong>. As the name suggests, the confusion matrix is a good way to look at how and where the model gets confused the most. It will be easier to understand with an example. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
cf_matrix = metrics.confusion_matrix(test_y, rf_pred)
ax = plt.subplot()
sns.heatmap(
    cf_matrix,
    annot=<span class="hljs-literal">True</span>,
    annot_kws={<span class="hljs-string">"size"</span>: <span class="hljs-number">10</span>},
    fmt=<span class="hljs-string">"g"</span>,
    ax=ax
)
ax.set_xlabel(<span class="hljs-string">"Predicted"</span>)
ax.set_ylabel(<span class="hljs-string">"Actual"</span>)
ax.set_title(<span class="hljs-string">f"Confusion Matrix"</span>)
plt.show()
</code></pre>
    <p class="normal">Similar to before, we are using the <code class="inlineCode">metrics</code> module to build a confusion matrix. The <code class="inlineCode">confusion_matrix</code> function takes the actual and predicted values and builds a confusion matrix. Then, we are using the <code class="inlineCode">heatmap</code> function of the <code class="inlineCode">seaborn</code> Python package to plot a heat map. The resulting chart looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_08.png"/></figure>
    <p class="packt_figref">Figure 6.8: Confusion matrix of the random forest model predictions</p>
    <p class="normal">As you can see from this <a id="_idIndexMarker544"/>plot, the y-axis represents the actual classes and the x-axis represents the predicted classes. For example, the top left box is where the actual class was 0 or no conversion, and the predicted class was also <code class="inlineCode">0</code>. The top right box is where the actual class was 0, but the model predicted them to be a class of <code class="inlineCode">1</code> or conversion. </p>
    <p class="normal">As you can see, the confusion matrix shows you where the model is the most confused. A model that predicts the outcome with high accuracy will have large numbers or percentages in the diagonal boxes and small numbers in the other boxes.</p>
    <p class="normal">We have experimented with predicting the customer conversion likelihood with a random forest model. Here, we have observed and discussed how this random forest predictive model can help target the subset of the customer base without losing too many converted customers and how this can result in much more cost-effective marketing strategies.</p>
    <h2 class="heading-2" id="_idParaDest-153">Gradient boosted decision tree (GBDT) modeling</h2>
    <p class="normal">We will use the same dataset <a id="_idIndexMarker545"/>and train/test sets for building a GBDT model and compare its performance against the random forest model we have just built. XGBoost is the most commonly used library in Python for training a GBDT model. You can install this package using the following command in the terminal or Jupyter Notebook:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install xgboost
</code></pre>
    <h3 class="heading-" id="_idParaDest-154">Training GBDT model</h3>
    <p class="normal">The<a id="_idIndexMarker546"/> XGBoost package follows the same pattern as the <code class="inlineCode">scikit-learn</code> package. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> XGBClassifier
xgb_model = XGBClassifier(
    n_estimators=<span class="hljs-number">100</span>,
    max_depth=<span class="hljs-number">5</span>,
    scale_pos_weight=<span class="hljs-number">1</span>/train_y.mean(),
)
xgb_model.fit(train_x, train_y)
</code></pre>
    <p class="normal">As you can see from this code, we initiate an <code class="inlineCode">XGBClassifier</code> with the <code class="inlineCode">n_estimators</code>, <code class="inlineCode">max_depth</code>, and <code class="inlineCode">scale_pos_weight</code> parameters:</p>
    <ul>
      <li class="bulletList">Similar to the case of the random forest model, the <code class="inlineCode">n_estimators</code> parameter defines how many individual decision trees are to be built. You may have noticed we are using a much lower number for this parameter. If you recall, a GBDT model is sequentially built where each subsequent decision tree learns the errors the previous decision tree makes. This often results in a smaller number of individual trees performing as well as or better than the random forest model. </li>
    </ul>
    <p class="normal">Also, because a GBDT model is sequentially built, a large number of decision trees results in much longer training time than a random forest model, which can build individual decision trees in parallel.</p>
    <ul>
      <li class="bulletList">The other parameter, <code class="inlineCode">max_depth</code>, is the same as in the case of the random forest model. This parameter restricts how deep each decision tree can grow.</li>
      <li class="bulletList">Lastly, the <code class="inlineCode">scale_pos_weight</code> parameter defines the balance of the positive and negative class weights. As you may recall we have an imbalanced dataset where the positive class is only about 15% of the data. By giving inversely<a id="_idIndexMarker547"/> proportionate weight for the positive class with <code class="inlineCode">1/train_y.mean()</code>, we are instructing the model to adjust for the imbalanced dataset. This enforces the GBDT model to penalize more for incorrect predictions of the positive class, which makes the model more sensitive to the positive class.</li>
    </ul>
    <div class="note">
      <p class="normal">There are various ways to handle potential overfitting issues that often occur when you have large individual decision trees within a GBDT model. On top of the <code class="inlineCode">max_depth</code> parameter, the XGBoost package provides various other parameters, such as <code class="inlineCode">max_leaves</code>, <code class="inlineCode">colsample_bytree</code>, <code class="inlineCode">subsample</code>, and <code class="inlineCode">gamma</code>, that can help reduce overfitting.</p>
      <p class="normal">We suggest that you look at the documentation and experiment with various parameters and see how they help you prevent overfitting.</p>
      <p class="normal">Here is the documentation: <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier"><span class="url">https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier</span></a>.</p>
    </div>
    <h3 class="heading-" id="_idParaDest-155">Predicting and Evaluating GBDT Model</h3>
    <p class="normal">Similar <a id="_idIndexMarker548"/>to the <code class="inlineCode">RandomForestClassifier</code> of the random forest model, the <code class="inlineCode">XGBClassifier </code>object also provides the same syntax for predicting with the trained GBDT model. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">xgb_pred = xgb_model.predict(test_x)
xgb_pred_proba = xgb_model.predict_proba(test_x)[:,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">As you can see from this code, you can use the <code class="inlineCode">predict</code> function to predict the positive versus negative label for each record of the test set and the <code class="inlineCode">predict_proba</code> function to predict the probabilities for each class of individual records of the test set. As before, we are retrieving the second column of the predicted probabilities by slicing the output with <code class="inlineCode">[:,1]</code> to get the predicted probabilities of the positive class, which is the predicted probability of a conversion, for each record of the test set.</p>
    <p class="normal">To compare the performance of this GBDT model against the random forest model, we will use the <a id="_idIndexMarker549"/>same evaluation metrics and approaches. As you may recall, we can use the following code to get the key metrics of accuracy, precision, and recall:</p>
    <pre class="programlisting code"><code class="hljs-code">accuracy = (test_y == xgb_pred).mean()
precision = metrics.precision_score(test_y, xgb_pred)
recall = metrics.recall_score(test_y, xgb_pred)
</code></pre>
    <p class="normal">Due to some randomness in building these trees, there can be some variances and differences each time a GBDT model is trained, but at the time of this writing, the results look as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_09.png"/></figure>
    <p class="packt_figref">Figure 6.9: Summary of GBDT model performance metrics</p>
    <p class="normal">These key metrics look very similar to the random forest model. The overall accuracy and precision of this GBDT model are slightly higher than the random forest model. However, the recall is slightly lower than the random forest model.</p>
    <p class="normal">Similarly, the AUC-ROC curve can be plotted using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">dp = metrics.RocCurveDisplay.from_predictions(
    test_y,
    xgb_pred_proba,
    name=<span class="hljs-string">"Conversion"</span>,
    color=<span class="hljs-string">"darkorange"</span>,
)
_ = dp.ax_.<span class="hljs-built_in">set</span>(
    xlabel=<span class="hljs-string">"False Positive Rate"</span>,
    ylabel=<span class="hljs-string">"True Positive Rate"</span>,
    title=<span class="hljs-string">"ROC Curve"</span>,
)
plt.grid()
plt.show()
</code></pre>
    <p class="normal">The resulting AUC-ROC Curve looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_10.png"/></figure>
    <p class="packt_figref">Figure 6.10: AUC- ROC Curve of the GBDT model predictions</p>
    <p class="normal">When you<a id="_idIndexMarker550"/> compare this against the random forest model, the results are almost identical. AUC is about the same with <code class="inlineCode">0.93</code> and at the FPR of <code class="inlineCode">0.2</code>, the TPR of the GBDT model is also about <code class="inlineCode">0.9</code>, which was also the case of the previously built random forest model.</p>
    <p class="normal">Lastly, let’s take a look at the confusion matrix with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
cf_matrix = metrics.confusion_matrix(test_y, xgb_pred)
ax = plt.subplot()
sns.heatmap(
    cf_matrix,
    annot=<span class="hljs-literal">True</span>,
    annot_kws={<span class="hljs-string">"size"</span>: <span class="hljs-number">10</span>},
    fmt=<span class="hljs-string">"g"</span>,
    ax=ax
)
ax.set_xlabel(<span class="hljs-string">"Predicted"</span>)
ax.set_ylabel(<span class="hljs-string">"Actual"</span>)
ax.set_title(<span class="hljs-string">f"Confusion Matrix"</span>)
plt.show()
</code></pre>
    <p class="normal">The resulting confusion matrix looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_11.png"/></figure>
    <p class="packt_figref">Figure 6.11: Confusion matrix of the GBDT model predictions</p>
    <p class="normal">When you<a id="_idIndexMarker551"/> compare this against the confusion matrix of the random forest model, you will notice that the TPs are lower with the GBDT model, but false negatives are also lower with the GBDT model. This is expected as we have seen that the precision of the GBDT model is higher, but recall is lower compared to the random forest model.</p>
    <p class="normal">Overall, the performances of random forest and GBDT models are very similar and hard to distinguish from<a id="_idIndexMarker552"/> these examples. Depending on how you fine-tune your model, you may end up with a better random forest or GBDT model.</p>
    <div class="packt_tip">
      <p class="normal">There are numerous ways and parameters you can fine-tune the tree-based models. We suggest you experiment with various sets of parameters and see how they affect the model’s performance!</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-156">Predicting customer conversion with deep learning algorithms</h1>
    <p class="normal">Deep learning<a id="_idIndexMarker553"/> has become a hot topic and its popularity and usage are rising, as deep learning models<a id="_idIndexMarker554"/> are proven to work well when data have complex relationships within the variables <a id="_idIndexMarker555"/>and learn or extract features autonomously from the data, even though tree-based models are also very frequently used and powerful for predictive modeling. We touched on deep learning in <em class="chapterRef">Chapter 5</em> when we used pre-trained language models for sentiment analysis and classification. In this section, we are going to build on that knowledge and experiment with developing deep learning models for predictive modeling and, more specifically, for making predictions on which customers are likely to convert.</p>
    <p class="normal">Deep learning is <a id="_idIndexMarker556"/>basically an <strong class="keyWord">artificial neural network</strong> (<strong class="keyWord">ANN</strong>) model with lots of hidden and complex layers of neurons, or, in other words, a deep ANN. An ANN is a model inspired by the biological neural networks in animal and human brains. An ANN learns the data through layers of interconnected neurons that resemble animal and human brains. The following diagram shows the high-level structure of an ANN:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_12.png"/></figure>
    <p class="packt_figref">Figure 6.12: Example ANN architecture</p>
    <p class="normal">As this <a id="_idIndexMarker557"/>diagram shows, there<a id="_idIndexMarker558"/> are three layers in any deep learning or ANN model: the input layer, hidden layer, and output layer. Detailed<a id="_idIndexMarker559"/> explanations of how ANN models learn or build the hidden layer weights are beyond the scope of this book, but at a high level, they go through iterations of forward propagations and backward propagations:</p>
    <ul>
      <li class="bulletList">Forward propagation<a id="_idIndexMarker560"/> is where the data is fed through a network from the input layer to the output layer and the activation function is applied regardless of whether each neuron is activated or not and returns the output of each node.</li>
      <li class="bulletList">Backward propagation <a id="_idIndexMarker561"/>is the process of moving from the output layer to the input layer and adjusting the weights of the network by analyzing the losses or errors from the previous iteration.</li>
    </ul>
    <p class="normal">Through<a id="_idIndexMarker562"/> iterations of these forward and backward propagations, a neural network learns the weights of each neuron that minimizes the error of the predictions.</p>
    <p class="normal">A number <a id="_idIndexMarker563"/>of hidden layers and a number of neurons in each layer should be experimented with to find the right neural network architecture that works best for each case of predictive models. In this section, we are going to experiment with how wide neural network architecture performs against deep neural network architecture.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">When to use wide versus deep neural networks?</strong></p>
      <p class="normal">A wide neural network<a id="_idIndexMarker564"/> refers to an ANN model that has fewer layers but more neurons in each layer, whereas a deep neural network <a id="_idIndexMarker565"/>refers to an ANN model with lots of hidden layers. Aside from the model performance comparisons to see which architecture works better for your case, there are some key factors you may want to consider or limit which approach to take:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Training time and compute resources</strong>: As the number of layers grows, it takes more time to train as backpropagation through each layer is computationally intensive, which also results in higher compute costs. The wide neural network may have an advantage in shortening the training time and reducing the compute costs.</li>
        <li class="bulletList"><strong class="keyWord">Model interpretability</strong>: Shallower architecture may offer better interpretability compared to deep neural networks. If explainability is a requirement, shallow architecture may be a better fit.</li>
        <li class="bulletList"><strong class="keyWord">Ability to generalize</strong>: Deep neural network models tend to be able to capture more abstract patterns through more layers and learn higher-order features compared to wide neural networks. This may result in the deep architecture model having better performance on new and unseen data.</li>
      </ul>
    </div>
    <h2 class="heading-2" id="_idParaDest-157">Train and test sets for deep learning models</h2>
    <p class="normal">For the best <a id="_idIndexMarker566"/>performances of neural network models, you need to normalize the data before you train the models. We are going to use the same train and test sets that we used previously for tree-based models, but normalize them. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">mean = train_x.mean()
std = train_x.std()
normed_train_x = (train_x - mean)/std
normed_test_x = (test_x - mean)/std
</code></pre>
    <p class="normal">As this code suggests, we first get the mean and standard deviation from the train set and normalize both train and test sets by subtracting the mean and dividing by the standard deviation. After the standardization, the <code class="inlineCode">normed_train_x</code> DataFrame should have means of <code class="inlineCode">0</code> and standard deviations of <code class="inlineCode">1</code> for all the columns.</p>
    <h2 class="heading-2" id="_idParaDest-158">Wide neural network modeling</h2>
    <p class="normal">We<a id="_idIndexMarker567"/> will first <a id="_idIndexMarker568"/>look at building <strong class="keyWord">wide neural network</strong> models using the <code class="inlineCode">keras</code> package in Python.</p>
    <div class="note">
      <p class="normal">To install <code class="inlineCode">keras</code> on your machine, run the following command in your terminal:</p>
      <pre class="programlisting con"><code class="hljs-con">pip install keras
</code></pre>
    </div>
    <h3 class="heading-" id="_idParaDest-159">Training the wide neural network model</h3>
    <p class="normal">With the <a id="_idIndexMarker569"/>normalized train and test sets, let’s start building neural network models. Take a look at the following code to see how a neural network model can be initiated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> keras
model = keras.Sequential(
    [
        keras.Input(shape=normed_train_x.shape[<span class="hljs-number">1</span>:]),
        keras.layers.Dense(<span class="hljs-number">2048</span>, activation=<span class="hljs-string">"relu"</span>),
        keras.layers.Dropout(<span class="hljs-number">0.2</span>),
        keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    ]
)
</code></pre>
    <p class="normal">As you can see from this code, we create a sequence of layers with the <code class="inlineCode">Sequential</code> class. We start with the input layer with the <code class="inlineCode">Input</code> class, where we define the shape or the number of input neurons to match the number of columns or features of the train set. Then, we have one wide hidden layer with 2,048 neurons. We added a <code class="inlineCode">Dropout</code> layer, which defines what percentage of neurons to drop and this helps reduce the overfitting issues. We are instructing the neural network model to drop 20% of the neurons <a id="_idIndexMarker570"/>between the hidden and output layers. Lastly, we have one <code class="inlineCode">Dense</code> layer for the output layer with one output neuron, which will output the predicted probability of the positive case or the likelihood of a conversion.</p>
    <p class="normal">You may have noticed there are two activation functions we use for the hidden layer and the output layer. The <strong class="keyWord">rectified linear unit</strong> (<strong class="keyWord">ReLU</strong>) activation function is one of the most frequently <a id="_idIndexMarker571"/>used activation functions, which only activates the positive values and deactivates all the negative values. The equation for the ReLU activation function looks as follows:</p>
    <p class="center"><em class="italic">ReLU</em>(<em class="italic">x</em>) = <em class="italic">max</em>(<em class="italic">0</em>, <em class="italic">x</em>)</p>
    <p class="normal">The behavior of the ReLU activation function looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_13.png"/></figure>
    <p class="packt_figref">Figure 6.13: ReLU activation function</p>
    <p class="normal">The <strong class="keyWord">sigmoid</strong> function, on<a id="_idIndexMarker572"/> the other hand, turns values into a range of <code class="inlineCode">0</code> and <code class="inlineCode">1</code>. This makes the sigmoid function the top choice to predict the probability as an output. The equation for<a id="_idIndexMarker573"/> the sigmoid function is as follows:</p>
    <p class="center"><a id="_idIndexMarker574"/><img alt="" src="../Images/B30999_06_004.png"/></p>
    <p class="normal">The behavior of the sigmoid function looks as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_14.png"/></figure>
    <p class="packt_figref">Figure 6.14: Sigmoid activation function</p>
    <div class="packt_tip">
      <p class="normal">There are various other activation functions other than ReLU and sigmoid, such as tanh, leaky ReLU, and softmax. We suggest you do some research and experiment with how different activation functions work and affect the neural network models’ performances!</p>
    </div>
    <p class="normal">There are a few more steps involved before we can train a neural network model:</p>
    <ol>
      <li class="numberedList" value="1">First, we need to define the key metrics we will track through the iterations of model training and compile the model. Take a look at the following code:
        <pre class="programlisting code"><code class="hljs-code">model_metrics = [
    keras.metrics.Accuracy(name=<span class="hljs-string">"accuracy"</span>),
    keras.metrics.Precision(name=<span class="hljs-string">"precision"</span>),
    keras.metrics.Recall(name=<span class="hljs-string">"recall"</span>),
]
model.<span class="hljs-built_in">compile</span>(
    optimizer=keras.optimizers.Adam(<span class="hljs-number">0.001</span>),
    loss=<span class="hljs-string">"binary_crossentropy"</span>,
    metrics=model_metrics
)
</code></pre>
      </li>
    </ol>
    <p class="normal">As we <a id="_idIndexMarker575"/>have discussed previously with the tree-based models, we are going to track accuracy, precision, and recall. We will be using the Adam optimizer for this exercise. Simply put, optimizers are the algorithms that are used to update the weights of the network to minimize the errors based on the loss function, which in this case is <code class="inlineCode">binary_crossentropy</code>, which measures the difference between the predicted and actual binary outcomes. Since our output or target variable for the prediction is a binary variable of conversion versus no conversion, <code class="inlineCode">binary_crossentropy </code>will be the right fit for the loss function. For multi-class predictions where the target variable is not a binary variable and has more than two possible outcomes, <code class="inlineCode">categorical_crossentropy</code> will be a better-suited loss function.</p>
    <ol>
      <li class="numberedList" value="2">Next, we are going to have a checkpoint to save the best model among all the iterations or epochs that we will run while training the model. Take a look at the following code:
        <pre class="programlisting code"><code class="hljs-code">best_model_path = <span class="hljs-string">"./checkpoint.wide-model.keras"</span>
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=best_model_path,
    monitor=<span class="hljs-string">"val_precision"</span>,
    mode=<span class="hljs-string">"</span><span class="hljs-string">max"</span>,
    save_best_only=<span class="hljs-literal">True</span>
)
</code></pre>
      </li>
    </ol>
    <p class="normal">Here, we are using precision as the metric to monitor and store the best model to a local drive based on that metric value. The best model is going to be stored as defined in the <code class="inlineCode">best_model_path</code> variable.</p>
    <ol>
      <li class="numberedList" value="3">Finally, it is now time to train this <strong class="keyWord">wide neural network</strong> model with the following code:
        <pre class="programlisting code"><code class="hljs-code">class_weight = {<span class="hljs-number">0</span>: <span class="hljs-number">1</span>, <span class="hljs-number">1</span>: <span class="hljs-number">1</span>/train_y.mean()}
history = model.fit(
    normed_train_x.to_numpy(),
    train_y.to_numpy(),
    batch_size=<span class="hljs-number">64</span>,
    epochs=<span class="hljs-number">30</span>,
    verbose=<span class="hljs-number">2</span>,
    callbacks=[
        model_checkpoint_callback,
    ],
    validation_data=(normed_test_x.to_numpy(), test_y.to_numpy()),
    class_weight=class_weight,
)
</code></pre>
      </li>
    </ol>
    <p class="normal">Similar<a id="_idIndexMarker576"/> to the class weights we have given to our tree-based models previously, we are also going to define the class weights that are inversely proportional to the class composition. As you can see from the code, we instruct the model to run for 30 iterations or epochs with a batch size of 64. We register the callback that we created to store the best model previously with the custom class weights. When you run this code, it will run for 30 epochs and report the accuracy, precision, and recall metrics that we defined previously. The output of this code should look something like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_15.png"/></figure>
    <p class="packt_figref">Figure 6.15: Sample output of the neural network model</p>
    <h3 class="heading-" id="_idParaDest-160">Predicting and evaluating wide neural network model</h3>
    <p class="normal">For making <a id="_idIndexMarker577"/>predictions from the model we have just trained, we first need to load the best model. You can use the following code to load the best model:</p>
    <pre class="programlisting code"><code class="hljs-code">wide_best_model = keras.models.load_model(best_model_path)
</code></pre>
    <p class="normal">Making predictions with this model is similar to the <code class="inlineCode">scikit-learn</code> package’s syntax. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">wide_pred_proba = wide_best_model.predict(normed_test_x).flatten()
wide_preds = [<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> wide_pred_proba]
</code></pre>
    <p class="normal">As you may notice<a id="_idIndexMarker578"/> from this code, you can use the <code class="inlineCode">predict</code> function of the <code class="inlineCode">keras</code> model to get the predicted probabilities for each record. We use the <code class="inlineCode">flatten</code> function to make the predicted probability output a list of predicted probabilities from a two-dimensional array. Then, for illustration purposes, we are going to assume any record with a predicted probability above <code class="inlineCode">0.5</code> is considered to have a high chance of converting and assume they are positive cases. The probability threshold is another factor you can fine-tune for the final predictions of conversions versus no conversions.</p>
    <p class="normal">We are going to use the same codes that we have used previously for the tree-based models for key metrics, the AUC-ROC curve, and the confusion matrix. The results look as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_16.png"/></figure>
    <p class="packt_figref">Figure 6.16: Summary of the evaluation results for the wide neural network model</p>
    <p class="normal">If you compare <a id="_idIndexMarker579"/>these results against the tree-based models, you will notice the wide neural network model performed a little worse than the tree-based models. The precision and recall are slightly lower and the AUC is also a little lower compared to those of the tree-based models. We will go deeper into the practical uses of these models and how to choose the best model to use for marketing campaigns later when we discuss A/B testing in more depth, but the model performances based on the train and test sets look better for the tree-based models.</p>
    <h2 class="heading-2" id="_idParaDest-161">Deep neural network modeling</h2>
    <p class="normal">In this section, we<a id="_idIndexMarker580"/> will discuss <a id="_idIndexMarker581"/>how to build deep neural network models and compare the results against the wide neural network model that we built in the previous section. The only difference between the two neural network models is how we structure or architect them.</p>
    <h3 class="heading-" id="_idParaDest-162">Training deep neural network model</h3>
    <p class="normal">Take a<a id="_idIndexMarker582"/> look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">model = keras.Sequential(
    [
        keras.Input(shape=normed_train_x.shape[<span class="hljs-number">1</span>:]),
        keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">"relu"</span>),
        keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">"</span><span class="hljs-string">relu"</span>),
        keras.layers.Dropout(<span class="hljs-number">0.2</span>),
        keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">"relu"</span>),
        keras.layers.Dropout(<span class="hljs-number">0.2</span>),
        keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">"sigmoid"</span>),
    ]
)
</code></pre>
    <p class="normal">As before, we are using <code class="inlineCode">keras.Sequential</code> to build a model, but this time we have more layers between the first <code class="inlineCode">Input</code> layer and the final output <code class="inlineCode">Dense</code> layer:</p>
    <ul>
      <li class="bulletList">We have four hidden layers with 128 neurons each with the ReLU activation function.</li>
      <li class="bulletList">There are also two <code class="inlineCode">Dropout</code> layers between the last and the second last hidden layers and between the last hidden layer and the output layer.</li>
    </ul>
    <p class="normal">Compared to the previous wide neural network model, each layer has a smaller number of neurons but it has a larger number of layers. As you can see from this example, the model architecture is up to the person building the model and should be based on the performance. We <a id="_idIndexMarker583"/>suggest experimenting with different architectures of the model and analyzing how it changes the model performances.</p>
    <p class="normal">Using the same Keras model training code we used previously for the wide neural network model, you can train this deep neural network model as well. Make sure you have the callback function to save the best model so that we can use it to evaluate the model’s performance.</p>
    <h3 class="heading-" id="_idParaDest-163">Predicting and evaluating the deep neural network model</h3>
    <p class="normal">As before, you<a id="_idIndexMarker584"/> can use the following code to make predictions based on the best model found from training the deep neural network model:</p>
    <pre class="programlisting code"><code class="hljs-code">deep_best_model = keras.models.load_model(best_model_path)
deep_pred_proba = deep_best_model.predict(normed_test_x).flatten()
</code></pre>
    <p class="normal">With the predicted probabilities, you can run the same evaluation metrics and charts. Due to the<a id="_idIndexMarker585"/> randomness in the models, the results may vary slightly each time you train the models. At the time of this run, the results looked as in the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_17.png"/></figure>
    <p class="packt_figref">Figure 6.17: Summary of the evaluation results for the deep neural network model</p>
    <p class="normal">Compared<a id="_idIndexMarker586"/> to the wide neural network model we have built previously, the deep neural network model performs slightly worse. The AUC is about the same, but the precision and recall are lower than those with the wide neural network. These evaluation metrics and plots help examine the model performances at the facial value and at the training time. For more practical evaluations between models, you may want to consider running A/B testing to choose the final model for your marketing campaigns.</p>
    <h1 class="heading-1" id="_idParaDest-164">Conducting A/B testing for optimal model choice</h1>
    <p class="normal"><strong class="keyWord">A/B testing</strong>, simply put, compares two versions of features or models to identify which one is better. It <a id="_idIndexMarker587"/>plays a critical role in decision-making processes across various industries. Web developers may use A/B testing to test which of the two versions of the app performs better. Marketers may use A/B testing to test which version of the <a id="_idIndexMarker588"/>marketing messages may do better in engaging potential customers. Similarly, A/B testing can be used to compare two different models in terms of their performance and effectiveness. In our example in this chapter, we can use A/B testing to choose which of the models we have built based on our train and test sets may work the best in the actual real-world setting.</p>
    <p class="normal">A/B testing is typically conducted across a predefined set of periods or until a predefined number of samples are collected. This is to ensure you have enough samples collected to make your decisions based on. For example, you may want to run A/B testing for two weeks and collect the results for the duration of the test at the end of the two weeks and analyze the results. Or, you may want to set the target of 2,000 samples for both A and B scenarios without setting a certain period.</p>
    <p class="normal">You may also want to set both the period and the number of samples as the target, whichever comes first. For example, you set the sample target at 2,000 samples and a 2-week timebox; if you end up collecting 2,000 samples before 2 weeks, you may want to terminate the test sooner as you have collected enough samples to analyze the results. The period and sample size for A/B testing should be determined based on the business needs or limitations and confidence in the test results.</p>
    <p class="normal">Examining the results of A/B testing is typically done with statistical hypothesis testing. The <strong class="keyWord">two-sample t-test</strong> is <a id="_idIndexMarker589"/>frequently used to determine whether the differences observed in A and B cases of the A/B testing are statistically significant or not. There are two important statistics in a t-test – the <strong class="keyWord">t-value</strong> and <strong class="keyWord">p-value</strong>:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">t-value</strong> measures<a id="_idIndexMarker590"/> the degree of difference between the two means relative to the variances of the two populations. The larger the <strong class="keyWord">t-value</strong> is, the more different the two groups are.</li>
    </ul>
    <p class="normal">The equation to get the <strong class="keyWord">t-value</strong> for the two-sample <strong class="keyWord">t-test</strong> is as follows:</p>
    <p class="center"><a id="_idIndexMarker591"/><img alt="" src="../Images/B30999_06_005.png"/></p>
    <p class="normal"><em class="italic">M</em><sub class="subscript">1</sub> and <em class="italic">M</em><sub class="subscript">2</sub> are the averages, <em class="italic">S</em><sub class="subscript">1</sub> and <em class="italic">S</em><sub class="subscript">2</sub> are the standard deviations, and <em class="italic">N</em><sub class="subscript">1</sub> and <em class="italic">N</em><sub class="subscript">2</sub> are the number of samples in groups 1 and 2. As you may infer from this equation, the large negative <strong class="keyWord">t-value</strong> will suggest the average of group 2 is <a id="_idIndexMarker592"/>significantly larger than the average of group 1 and the large positive t-value will suggest the average of group 1 is significantly larger than the average of group 2. This is also called a two-tailed <strong class="keyWord">t-test</strong>.</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">p-value</strong>, on <a id="_idIndexMarker593"/>the other hand, measures the probability that the results occur by chance. Thus, the smaller the <strong class="keyWord">p-value</strong> is, the more statistically significant that the two groups are different and the difference is not by a mere chance.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-165">Simulating A/B Testing</h2>
    <p class="normal">We <a id="_idIndexMarker594"/>are going to simulate A/B testing as if we are running this experiment live with our models. For illustration purposes, we are going to run A/B testing on the XGBoost or GBDT model and the wide neural network model that we have built previously. We are going to run this A/B test for 1,000 samples for each scenario and analyze the results to see which one of the two models may work better for our marketing campaign to maximize customer conversion rates.</p>
    <p class="normal">First, we are going to randomly route the incoming traffic to group A (GBDT model) and group B (wide neural network model). Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">group_a_indexes = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(np.random.choice(<span class="hljs-number">2000</span>, <span class="hljs-number">1000</span>, replace=<span class="hljs-literal">False</span>)))
group_b_indexes = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2000</span>) <span class="hljs-keyword">if</span> x <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> group_a_indexes]
</code></pre>
    <p class="normal">Here, we are using the <code class="inlineCode">numpy</code> package’s <code class="inlineCode">random.choice</code> function to randomly select 1,000 items from 2,000 items. We <a id="_idIndexMarker595"/>route these to group A and the rest to group B. Then, we simulate the A/B test as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">group_a_actuals = test_y.iloc[group_a_indexes].to_numpy()
group_b_actuals = test_y.iloc[group_b_indexes].to_numpy()
group_a_preds = []
group_b_preds = []
<span class="hljs-keyword">for</span> customer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2000</span>):
    <span class="hljs-keyword">if</span> customer <span class="hljs-keyword">in</span> group_a_indexes:
        <span class="hljs-comment"># route to XGBoost</span>
        conversion = test_y.iloc[customer]
        pred_prob = xgb_model.predict_proba(
            np.array([test_x.iloc[customer].to_numpy(),])
        )[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]
        pred = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> pred_prob &gt; <span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>
        group_a_preds.append(pred)
    <span class="hljs-keyword">elif</span> customer <span class="hljs-keyword">in</span> group_b_indexes:
        <span class="hljs-comment"># route to Wide Net</span>
        conversion = test_y.iloc[customer]
        pred_prob = wide_best_model.predict(
            np.array([normed_test_x.iloc[customer].to_numpy(),]), verbose=<span class="hljs-number">0</span>
        )[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]
        pred = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> pred_prob &gt; <span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>
        group_b_preds.append(pred)
</code></pre>
    <p class="normal">As you can see from this code, for the first 2,000 customers, we route half to group A and the rest to group B. The group A customers are predicted with their likelihood of conversions by the XGBoost or GBDT model. The group B customers are predicted with their likelihood of conversions by the wide neural network model.</p>
    <p class="normal">Then, we are going to examine the actual outcomes of these predictions. In the actual test setting, these outcomes may come days or weeks after the predictions were made, as it takes time for customers to decide whether to purchase or not. We are going to evaluate the actual conversions of those customers who are predicted to convert and the missed opportunities, which are the conversions of the customers who are predicted not to convert. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_cumulative_metrics</span>(<span class="hljs-params">actuals, preds</span>):
    cum_conversions = []
    missed_opportunities = []
   
    customer_counter = <span class="hljs-number">0</span>
    cum_conversion_count = <span class="hljs-number">0</span>
    missed_opp_count = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> actual, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(actuals, preds):
        customer_counter += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> pred == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">if</span> actual == <span class="hljs-number">1</span>:
                cum_conversion_count += <span class="hljs-number">1</span>
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">if</span> actual == <span class="hljs-number">1</span>:
                missed_opp_count += <span class="hljs-number">1</span>
        cum_conversions.append(cum_conversion_count/customer_counter)
        missed_opportunities.append(missed_opp_count/customer_counter)
    <span class="hljs-keyword">return</span> cum_conversions, missed_opportunities
</code></pre>
    <p class="normal">As you <a id="_idIndexMarker596"/>can see, we are counting the cumulative number of conversions for those customers who are predicted to convert and also counting the number of conversions for those customers who are predicted not to convert but have converted. These are essentially missed opportunities as we would not have sent our marketing messages to these customers but they would have converted. We are going to aggregate these results for each group, using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">a_cum_conv_rates, a_missed_opp_rates = get_cumulative_metrics(
    group_a_actuals, group_a_preds
)
b_cum_conv_rates, b_missed_opp_rates = get_cumulative_metrics(
    group_b_actuals, group_b_preds
)
ab_results_df = pd.DataFrame({
    <span class="hljs-string">"group_a_cum_conversion_rate"</span>: a_cum_conv_rates,
    <span class="hljs-string">"group_a_cum_missed_opportunity_rate"</span>: a_missed_opp_rates,
    <span class="hljs-string">"group_b_cum_conversion_rate"</span>: b_cum_conv_rates,
    <span class="hljs-string">"group_b_cum_missed_opportunity_rate"</span>: b_missed_opp_rates,
})
</code></pre>
    <p class="normal">Now, let’s look at the cumulative conversion rates over time with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = (
    ab_results_df[[
        <span class="hljs-string">"</span><span class="hljs-string">group_a_cum_conversion_rate"</span>, <span class="hljs-string">"group_b_cum_conversion_rate"</span>
    ]]*<span class="hljs-number">100</span>
).plot(
    style=[<span class="hljs-string">'-', '--'</span>],
<span class="hljs-string">    </span>figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>),
    grid=<span class="hljs-title">True</span>
)
ax.set_ylabel(<span class="hljs-string">"</span><span class="hljs-string">Conversion Rate (%)"</span>)
ax.set_xlabel(<span class="hljs-string">"Customer Count"</span>)
ax.set_title(
    <span class="hljs-string">"Cumulative Conversion Rates over Time (A: XGBoost, B: Wide Net)"</span>
)
plt.show()
</code></pre>
    <p class="normal">This will <a id="_idIndexMarker597"/>produce a chart that looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_18.png"/></figure>
    <p class="packt_figref">Figure 6.18: Cumulative conversion rates over time for groups A and B</p>
    <p class="normal">As this chart suggests, group B, who are treated with the wide neural network model predictions, shows overall higher conversion rates, compared to group A, who are treated with the XGBoost model predictions. Let’s also take a look at how each group does for the missed<a id="_idIndexMarker598"/> opportunities with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = (
    ab_results_df[[
        <span class="hljs-string">"group_a_cum_missed_opportunity_rate"</span>,
        <span class="hljs-string">"group_b_cum_missed_opportunity_rate"</span>
    ]]*<span class="hljs-number">100</span>
).plot(
    style=[<span class="hljs-string">'-'</span>,<span class="hljs-string">'--'</span>],
    figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>),
    grid=<span class="hljs-literal">True</span>
)
ax.set_ylabel(<span class="hljs-string">"Missed Opportunity Rate (%)"</span>)
ax.set_xlabel(<span class="hljs-string">"Customer Count"</span>)
ax.set_title(
    <span class="hljs-string">"Cumulative Missed Opportunity Rates over Time (A: XGBoost, B: Wide Net)"</span>
)
plt.show()
</code></pre>
    <p class="normal">When you run this code, you should get a chart that looks similar to the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_19.png"/></figure>
    <p class="packt_figref">Figure 6.19: Cumulative missed opportunity rates over time for groups A and B</p>
    <p class="normal">This<a id="_idIndexMarker599"/> chart suggests that the missed opportunities are higher for group A with the XGBoost model predictions than group B with the wide neural network model predictions. </p>
    <p class="normal">These visuals are a great way to examine the performances of different groups or models. However, to validate whether these differences are statistically significant, we will have to run the t-test.</p>
    <h2 class="heading-2" id="_idParaDest-166">Two-tailed T-test</h2>
    <p class="normal">The <code class="inlineCode">scipy</code> package in Python<a id="_idIndexMarker600"/> provides a handy tool to <a id="_idIndexMarker601"/>compute the t-value and p-value for a two-tailed t-test.</p>
    <div class="packt_tip">
      <p class="normal">To install <code class="inlineCode">scipy</code> on your machine, run the following command in your terminal:</p>
      <pre class="programlisting con"><code class="hljs-con">pip install scipy
</code></pre>
    </div>
    <p class="normal">Once you have installed the <code class="inlineCode">scipy</code> package, you can run the following commands to examine the statistical significance of the differences in the conversion and missed opportunity rates between groups A and B:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> ttest_ind
t, p = ttest_ind(a_cum_conv_rates, b_cum_conv_rates)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f"Conversion Rate Difference Significance -- t: </span><span class="hljs-subst">{t:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string"> &amp; p: </span><span class="hljs-subst">{p:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>
)
t, p = ttest_ind(a_missed_opp_rates, b_missed_opp_rates)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f"Missed Opportunity Rate Difference Significance -- t: </span><span class="hljs-subst">{t:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string"> &amp; p: </span><span class="hljs-subst">{p:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>
)
</code></pre>
    <p class="normal">As you<a id="_idIndexMarker602"/> can <a id="_idIndexMarker603"/>see from this code, the <code class="inlineCode">ttest_ind</code> function from the <code class="inlineCode">scipy.stats</code> module lets you easily get the t-value and p-value for the two-tailed t-test between two groups. The output of this code should look similar to the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_06_20.png"/></figure>
    <p class="packt_figref">Figure 6.20: Two-tailed t-test results</p>
    <p class="normal">Let’s take a closer look at this output. First, the t-value and p-value for the conversion rate t-test are <code class="inlineCode">-13.168</code> and <code class="inlineCode">0</code>. This suggests that the difference in the conversion rates between the two groups is statistically significant. The negative value of the t-test suggests that the mean of group A is smaller than that of group B. </p>
    <p class="normal">Since from the p-value, we have concluded that the difference is significant, this translates to the result that group A’s conversion rate is significantly lower than that of group B. In other words, the wide neural network model performs significantly better in capturing the high conversion likely customers than the XGBoost model.</p>
    <p class="normal">Secondly, the t-value and p-value for the missed opportunity rate t-test are <code class="inlineCode">2.418</code> and <code class="inlineCode">0.016</code>. This suggests that the mean of group A is significantly larger than that of group B. This translates to the result that group A’s missed opportunity is significantly larger than that of group B. In other words, the XGBoost model results in missing more opportunities than the wide neural network model.</p>
    <p class="normal">As you can see from this A/B testing simulation results, A/B testing provides powerful insights into which model performs better in the real-world setting. Everyone building data science or AI/ML models should develop a habit of not only evaluating the models based on <a id="_idIndexMarker604"/>the <a id="_idIndexMarker605"/>train and test sets but also evaluating the models via A/B testing so that they can tell how realistic and applicable the models being built are in a real-world setting. Through a short period of small sample A/B testing, you can choose the optimal model to use for the upcoming marketing campaign.</p>
    <h1 class="heading-1" id="_idParaDest-167">Summary</h1>
    <p class="normal">In this chapter, we have covered a lot about building predictive models using an Online Purchase dataset. We have explored two different tree-based models, random forest and GBDT, and how to build predictive models to forecast who is likely to convert. Using the same example, we have also discussed how we can build neural network models that are the backbone of deep learning models. There is great flexibility in how you architect the neural network model, such as wide network, deep network, or wide and deep network. We have briefly touched on the activation functions and optimizers while building neural network models, but we suggest you do some more in-depth research into how they affect the performances of neural network models. Lastly, we have discussed what A/B testing is, how to conduct A/B testing, and how to interpret the A/B testing results. We have simulated A/B testing with the models we built for a scenario where we want to choose the best model for capturing the most amount of customer conversions.</p>
    <p class="normal">In the following chapter, we are going to expand further on targeted marketing using AI/ML. More specifically, we will discuss how to build personalized product recommendations in various ways and how this can lead to micro-targeted marketing strategies.</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</p>
    <p class="normal"><a href="https://packt.link/genai"><span class="url">https://packt.link/genai</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code12856128601808671.png"/></p>
  </div>
</body></html>