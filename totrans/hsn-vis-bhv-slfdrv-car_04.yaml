- en: '*Chapter 3*: Lane Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will show one of the incredible things possible using computer
    vision in general and OpenCV in particular: lane detection. You will learn how
    to analyze an image and build more and more visual knowledge about it, one step
    after another, applying several filtering techniques, replacing noise and approximation
    with a better understanding of the image, until you will be able to detect where
    the lanes are on a straight road or on a turn, and we will apply this pipeline
    to a video to highlight the road.'
  prefs: []
  type: TYPE_NORMAL
- en: You will see that this method relies on several assumptions that might not be
    true in the real world, though it can be adjusted to correct for that. Hopefully,
    you will find this chapter quite interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting lanes in a road
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perspective correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sliding window algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to design a pipeline that is able
    to detect the lanes on a road, using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our lane detection pipeline requires quite a lot of code. We will explain the
    main concepts, and you can find the full code on GitHub at [https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter3](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter3).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the instructions and code in this chapter, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV-Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Matplotlib module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To identify the lanes, we need some images and a video. While it''s easy to
    find some open source database to use for this, they are usually only available
    for non-commercial purposes. For this reason, in this book, we will use images
    and video generated by two open source projects: CARLA, a simulator useful for
    autonomous driving tasks, and Speed Dreams, an open source video game. All the
    techniques also work with real-world footage, and you are encouraged to try them
    on some public datasets, such as CULane or KITTI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/37pjxnO](https://bit.ly/37pjxnO)'
  prefs: []
  type: TYPE_NORMAL
- en: How to perform thresholding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While for a human it is easy to follow a lane, for a computer, this is not something
    that is so simple. One problem is that an image of the road has too much information.
    We need to simplify it, selecting only the parts of the image that we are interested
    in. We will only analyze the part of the image with the lane, but we also need
    to separate the lane from the rest of the image, for example, using color selection.
    After all, the road is typically black or dark, and lanes are usually white or
    yellow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will analyze different color spaces, to see which one
    is most useful for thresholding.
  prefs: []
  type: TYPE_NORMAL
- en: How thresholding works on different color spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a practical point of view, a color space is a way to decompose the colors
    of an image. You are most likely comfortable with RGB, but there are others.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV supports several color spaces, and as part of this pipeline, we need
    to choose the two best channels from a variety of color spaces. Why do we want
    to use two different channels? For two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: A color space that is good for white lanes might not be good for yellow ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there are difficult frames (for example, with shadows on the road or if
    the lane is discolored), one channel could be less affected than another one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This might not be strictly necessary for our example, as the lanes are always
    white, but it is definitely useful in real life.
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how our test image appears in different color spaces, but bear
    in mind that your case might be different.
  prefs: []
  type: TYPE_NORMAL
- en: RGB/BGR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The starting point will be the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Reference image, from Speed Dreams](img/B16322_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Reference image, from Speed Dreams
  prefs: []
  type: TYPE_NORMAL
- en: 'The image can, of course, be decomposed into three channels: red, green, and
    blue. As we know, OpenCV stores the image as BGR (meaning, the first byte is the
    blue channel, not the red channel), but conceptually, there is no difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the three channels once separated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – BGR channels: blue, green, and red channels](img/B16322_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2 – BGR channels: blue, green, and red channels'
  prefs: []
  type: TYPE_NORMAL
- en: 'They all seem fine. We can try to separate the lane by selecting the white
    pixels. As the white color is (`255, 255, 255`), we could leave some margin and
    select the colors above 180 on the scale. To do this operation, we need to create
    a black image with the same size as the selected channel, then paint all the pixels
    that are above 180 in the original channel white:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – BGR channels: blue, green, and red channels, threshold above
    180](img/B16322_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – BGR channels: blue, green, and red channels, threshold above 180'
  prefs: []
  type: TYPE_NORMAL
- en: They all seem good. The red channel also shows part of the car, but since we
    will not analyze that part of the image, it is not a problem. As the white color
    has the same value in the red, green, and blue channels, it is kind of expected
    that the lane should be visible on all three channels. This would not be true
    for yellow lanes, though.
  prefs: []
  type: TYPE_NORMAL
- en: The value that we choose for the threshold is very important, and unfortunately,
    it is dependent on the colors used for the lane and on the situation of the road;
    light conditions and shadows will also affect it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a totally different threshold, 20-120:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – BGR channels: blue, green, and red channels, threshold in the
    range 20-120](img/B16322_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4 – BGR channels: blue, green, and red channels, threshold in the
    range 20-120'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can select the pixels in the 20-120 range with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The image is probably still usable, as long as you consider that the lane is
    black, but it would not be recommended.
  prefs: []
  type: TYPE_NORMAL
- en: HLS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The HLS color space divides the color into hue, lightness, and saturation.
    The result is sometimes surprising:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – HLS channels: hue, lightness, and saturation](img/B16322_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5 – HLS channels: hue, lightness, and saturation'
  prefs: []
  type: TYPE_NORMAL
- en: The hue channel is pretty bad, noisy, and low resolution, while the lightness
    seems to perform well. The saturation seems to be unable to detect our lane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try some thresholding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – HLS channels: hue, lightness, and saturation, threshold above
    160](img/B16322_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6 – HLS channels: hue, lightness, and saturation, threshold above
    160'
  prefs: []
  type: TYPE_NORMAL
- en: The threshold shows that lightness is still a good candidate.
  prefs: []
  type: TYPE_NORMAL
- en: HSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The HSV color space divides the color into hue, saturation, and value, and
    it is related to HLS. The result is therefore similar to HLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – HSV channels: hue, saturation, and value](img/B16322_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7 – HSV channels: hue, saturation, and value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hue and saturation are not useful to us, but value looks fine with thresholding
    applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – HSV channels: hue, saturation, and value, threshold above 160](img/B16322_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8 – HSV channels: hue, saturation, and value, threshold above 160'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the threshold of value looks good.
  prefs: []
  type: TYPE_NORMAL
- en: LAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LAB (CIELAB or CIE L*a*b*) color space divides the color into L* (lightness,
    from black to white), a* (from green to red), and b* (from blue to yellow):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – LAB channels: L*, a*, and b*](img/B16322_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9 – LAB channels: L*, a*, and b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'L* seems fine, while a* and b* are not useful to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – LAB channels: L*, a*, and b*, threshold above 160](img/B16322_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10 – LAB channels: L*, a*, and b*, threshold above 160'
  prefs: []
  type: TYPE_NORMAL
- en: YCbCr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YCbCr is the last color space that we will analyze. It divides the image into
    Luma (Y) and two chroma components (Cb and Cr):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – YCbCr channels: Y, Cb, and Cr](img/B16322_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11 – YCbCr channels: Y, Cb, and Cr'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result when we apply a threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – YCbCr channels: Y, Cb, and Cr, threshold above 160](img/B16322_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12 – YCbCr channels: Y, Cb, and Cr, threshold above 160'
  prefs: []
  type: TYPE_NORMAL
- en: The threshold confirms the validity of the Luma channel.
  prefs: []
  type: TYPE_NORMAL
- en: Our choice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After some experiments, it seems that the green channel can be used for edge
    detection, and the L channel from the HLS space could be used as additional thresholding,
    so we'll stick to these. These settings should be also fine for a yellow line,
    while different colors might require different thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Perspective correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a step back and start simple. The easiest case that we can have
    is with a straight lane. Let''s see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Straight lane, from Speed Dreams](img/B16322_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Straight lane, from Speed Dreams
  prefs: []
  type: TYPE_NORMAL
- en: If we were flying over the road, and watching it from a bird's eye view, the
    lanes would be parallel, but in the picture, they are not, because of the perspective.
  prefs: []
  type: TYPE_NORMAL
- en: The perspective depends on the focal length of the lens (lenses with a shorter
    focal length show a stronger perspective) and the position of the camera. Once
    the camera is mounted on a car, the perspective is fixed, so we can take it into
    consideration and correct the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has a method to compute the perspective transformation: `getPerspectiveTransform()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes two parameters, both arrays of four points, identifying the trapezoid
    of the perspective. One array is the source and one array is the destination.
    This means that the same method can be used to compute the inverse transformation,
    by just swapping the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to select the area around the lanes, plus a small margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Trapezoid with the region of interest around the lanes](img/B16322_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Trapezoid with the region of interest around the lanes
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the destination is a rectangle (as we want to make it straight).
    *Figure 3.14* shows the green trapezoid (the `src` variable in the previous code)
    with the original perspective and the white rectangle (the `dst` variable in the
    previous code), which is the desired perspective. Please notice that for clarity,
    they have been drawn as overlapping, but the coordinates of the rectangle passed
    as a parameter are shifted, as if it was starting at *X* coordinate 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now apply the perspective correction and get our bird''s eye view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `warpPerspective()` method accepts four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The source image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformation matrix, obtained from `getPerspectiveTransform()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the output image. In our case, the width is the same as the original
    image, but the height is only the height of the trapezoid/rectangle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some flags, to specify the interpolation. `INTER_LINEAR` is a common choice,
    but I recommend experimenting, and to give `INTER_LANCZOS4` a try.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the result of warp using `INTER_LINEAR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Warped with INTER_LINEAR](img/B16322_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Warped with INTER_LINEAR
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result using `INTER_LANCZOS4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Warped with INTER_LANCZOS4](img/B16322_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Warped with INTER_LANCZOS4
  prefs: []
  type: TYPE_NORMAL
- en: They are very similar, but a closer look shows that the interpolation performed
    with the `LANCZOS4` resampling is sharper. We will see later that at the end of
    the pipeline, the difference is significant.
  prefs: []
  type: TYPE_NORMAL
- en: What is clear in both images is that our lines are now vertical, which intuitively
    could help us.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see in the next section how to leverage this image.
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is detecting the edges, and we will use the green channel for
    that, as during our experiments, it gave good results. Please be aware that you
    need to experiment with the images and videos taken from the country where you
    plan to run the software, and with many different light conditions. Most likely,
    based on the color of the lines and the colors in the image, you might want to
    choose a different channel, possibly from another color space; you can convert
    the image into different color spaces using `cvtColor()`, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We will stick to green.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has several ways to compute edge detection, and we are going to use
    Scharr, as it performs quite well. Scharr computes a derivative, so it detects
    the difference in colors in the image. We are interested in the *X* axis, and
    we want the result to be a 64-bit float, so our call would be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As Scharr computes a derivative, the values can be both positive and negative.
    We are not interested in the sign, but only in the fact that there is an edge.
    So, we will take the absolute value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Another issue is that the values are not bounded on the 0-255 value range that
    we expect on a single channel image, and the values are floating points, while
    we need an 8-bit integer. We can fix both the issues with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Edge detection with Scharr, scaled and with absolute values](img/B16322_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Edge detection with Scharr, scaled and with absolute values
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can apply thresholding to convert the image into black and
    white, to better isolate the pixels of the lanes. We need to choose the intensity
    of the pixels to select, and in this case, we can go for 20-120; we will select
    only pixels that have at least an intensity value of 20, and not more than 120:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `zeros_like()` method creates an array full of zeros, with the same shape
    of the image, and the second line sets all the pixels with an intensity between
    20 and 120 to 255.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Result after applying a threshold of 20](img/B16322_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Result after applying a threshold of 20
  prefs: []
  type: TYPE_NORMAL
- en: 'The lanes are now very visible, but there is some noise. We can reduce that
    by increasing the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Result after applying a threshold of 50](img/B16322_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Result after applying a threshold of 50
  prefs: []
  type: TYPE_NORMAL
- en: Now, there is less noise, but we lost the lines on the top.
  prefs: []
  type: TYPE_NORMAL
- en: We will now describe a technique that can help us to retain the full line without
    having an excessive amount of noise.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolated threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In practice, we don''t have to choose between selecting the whole line with
    a lot of noise and reducing the noise while detecting only part of the line. We
    could apply a higher threshold to the bottom (where we have more resolution, a
    sharper image, and more noise) and a lower threshold on the top (there, we get
    less contrast, a weaker detection, and less noise, as the pixels are stretched
    by the perspective correction, naturally blurring them). We can just interpolate
    between the thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Result after applying an interpolated threshold, from 15 to
    60](img/B16322_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Result after applying an interpolated threshold, from 15 to 60
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can have less noise at the bottom and detect weaker signals at the top.
    However, while a human can visually identify the lanes, for the computer, they
    are still just pixels in an image, so there is still work to do. But we simplified
    the image very much, and we are making good progress.
  prefs: []
  type: TYPE_NORMAL
- en: Combined threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, we also wanted to use the threshold on another channel,
    without edge detection. We chose the L channel of HLS.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the result of thresholding above 140:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – L channel with the threshold above 140](img/B16322_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – L channel with the threshold above 140
  prefs: []
  type: TYPE_NORMAL
- en: 'Not bad. Now, we can combine it with the edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Combination of the two thresholds](img/B16322_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Combination of the two thresholds
  prefs: []
  type: TYPE_NORMAL
- en: The result is noisier, but also more robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving forward, let''s introduce a picture with a turn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Lane with a turn, from Speed Dreams](img/B16322_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Lane with a turn, from Speed Dreams
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Lane with a turn, after the threshold](img/B16322_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Lane with a turn, after the threshold
  prefs: []
  type: TYPE_NORMAL
- en: It still looks good, but we can see that, because of the turn, we no longer
    have a vertical line. In fact, at the top of the image, the lines are basically
    horizontal.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the lanes using histograms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How could we understand, more or less, where the lanes are? Visually, for a
    human, the answer is simple: the lane is a long line. But what about a computer?'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we talk about vertical lines, one way could be to count the pixels that
    are white, on a certain column. But if we check the image with a turn, that might
    not work. However, if we reduce our attention to the bottom part of the image,
    the lines are a bit more vertical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Lane with a turn, after the threshold, the bottom part](img/B16322_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Lane with a turn, after the threshold, the bottom part
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now count the pixels by column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To save the histogram as a graph, in a file, we can use Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Left: histogram of a straight lane, right: histogram of a lane
    with a turn](img/Figure__3_26__Edited.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26 – Left: histogram of a straight lane, right: histogram of a lane
    with a turn'
  prefs: []
  type: TYPE_NORMAL
- en: The X coordinates on the histogram represent the pixels; as our image has a
    resolution of 1024x600, the histogram shows 1,024 data points, with the peaks
    centered around the pixels where the lanes are.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, in the case of a straight lane, the histogram identifies the
    two lines quite clearly; with a turn, the histogram is less clear (because the
    line makes a turn and, therefore, the white pixels are spread a bit around), but
    it's still usable. We can also see that in the case of a dotted line, the peek
    in the histogram is less pronounced, but it is still there.
  prefs: []
  type: TYPE_NORMAL
- en: This looks promising!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need a way to detect the two peaks. We can use `argmax()` from NumPy,
    which returns the index of the maximum element of an array, which is one of our
    peaks. However, we need two. For this, we can split the array into two halves,
    and select one peak on each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the indexes, which represent the *X* coordinate of the peaks. The
    value itself (for example, `histogram[index]`) can be considered the confidence
    of having identified the lane, as more pixels mean more confidence.
  prefs: []
  type: TYPE_NORMAL
- en: The sliding window algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we are making progress, the image still has some noise, meaning there
    are pixels that can reduce the precision. In addition, we only know where the
    line starts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is to focus on the area around the line – after all, there is
    no reason to work on the whole warped image; we could start at the bottom of the
    line and proceed to "follow it." This is probably one case where an image is worth
    a thousand words, so this is what we want to achieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Top: sliding window, bottom: histogram](img/Figure_3.27_UPDATED.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27 – Top: sliding window, bottom: histogram'
  prefs: []
  type: TYPE_NORMAL
- en: On the upper part of *Figure 3.27*, each rectangle represents a window of interest.
    The first window on the bottom of each lane is centered on the respective peak
    of the histogram. Then, we need a way to "follow the line." The width of each
    window is dependent on the margin that we want to have, while the height depends
    on the number of windows that we want to have. These two numbers can be changed
    to reach a balance between a better detection (reducing the unwanted points and
    therefore the noise) and the possibility to detect more difficult turns, with
    a smaller radius (which will require the windows to be repositioned faster).
  prefs: []
  type: TYPE_NORMAL
- en: As this algorithm requires quite some code, we will focus on the left lane for
    clarity, but the same computations need to also be performed for the right lane.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are only interested in the pixels that have been selected by the thresholding.
    We can use `nonzero()` from NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `non_zero` variable will contain the coordinates of the pixels that are
    white, then `non_zero_x` will contain the *X* coordinates, and `non_zero_y` the
    *Y* coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to set `margin`, the movement that we are allowing to the lane
    (for example, half of the window width of the sliding window), and `min_pixels`,
    the minimum number of pixels that we want to detect to accept a new position for
    the sliding window. Below this threshold, we will not update it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Coordinates of the sliding windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `left_x` variable will contain the position of the left lane, and we need
    to initialize it with the value obtained from the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the stage, we can now cycle through all the windows, and the
    variable that we will use as the index is `idx_window`. The *X* range is computed
    from the last position, adding the margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The *Y* range is determined by the index of the window that we are analyzing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to select the pixels that are white (from `non_zero_x` and `non_zero_y`)
    and constrained in the window that we are analyzing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NumPy array can be filtered using overloaded operators. To count all the
    *Y* coordinates that are above `win_y_bottom`, we can, therefore, simply use the
    following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The result is an array with `True` in the pixels selected and `False` on the
    other ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what we need is pixels between `win_y_top` and `win_y_bottom`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need the *X* coordinates, which must be between `win_x_left_min` and
    `win_x_left_max`. As we need to just count the points, we can add a `nonzero()`
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We need to select the first element because our array is inside another array
    of one single element.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also keep all these values in a variable, to draw the line above the
    lane later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just need to update the left lane position with the average of the
    positions, but only if there are enough points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Polynomial fitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we have potentially selected thousands of points, but we need to make
    sense of them and obtain a line. For this, we can use `polyfit()`, a method that
    can approximate a series of points with a polynomial of the specified degree;
    a second-degree polynomial will be enough for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please notice that `polyfit()` accepts the parameters in the order `(X, Y)`,
    while we provide them in the order `(Y, X)`. We do so because by mathematical
    convention, in a polynomial, *X* is known and *Y* is computed based on *X* (for
    example, *Y = X^2 + 3*X+5*). However, we know *Y* and we need to compute *X*,
    so we need to provide them in the opposite order.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Y* coordinates are simply a range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to compute *X* from *Y*, using the generic formula for a polynomial
    of the second degree (reversed on *X* and *Y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = Ay^2 + By + C;*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where we are now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Lanes drawn on the warped image](img/B16322_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – Lanes drawn on the warped image
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now call `perspectiveTransform()` with the inverse perspective transformation
    to move the pixels to their position in the image. This is the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Lane detected on the image](img/B16322_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 – Lane detected on the image
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! It has not been particularly easy, but you can now detect a
    lane on a frame, under the correct conditions. Unfortunately, not all the frames
    will be good enough for this. Let's see in the next section how we can use the
    temporal evolution of the video stream to filter the data and improve the precision.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing a video stream in real time can be a challenge from a computational
    point of view, but usually, it offers the possibility to improve precision, as
    we can build on knowledge from the previous frames and filter the result.
  prefs: []
  type: TYPE_NORMAL
- en: We will now see two techniques that can be used to detect lanes with better
    precision when working with video streams.
  prefs: []
  type: TYPE_NORMAL
- en: Partial histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we assume that we correctly detected a lane in the previous few frames,
    then the lane on the current frame should be in a similar position. This assumption
    is affected by the speed of the car and the frame rate of the camera: the faster
    the car, the more the lane could change. Conversely, the faster the camera, the
    less the lane could have moved between two frames. In a real self-driving car,
    both these values are known, so they can be taken into consideration if required.'
  prefs: []
  type: TYPE_NORMAL
- en: From a practical point of view, this means we can limit the part of the histogram
    that we analyze, to avoid false detections, analyzing only some histogram pixels
    (for example, 30) around the average of some of the previous frames.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main result of our detection is the three values of the polynomial fit,
    for each lane. Following the same principle of the previous section, we can deduce
    that they cannot change much between frames, so we could consider the average
    of some of the previous frames, to reduce noise.
  prefs: []
  type: TYPE_NORMAL
- en: There is a technique called the **exponentially weighted moving average** (or
    rolling average), which can be used to easily compute an approximate average on
    some of the last values of a stream of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given `beta`, a parameter greater than zero and typically close to one, the
    moving average can be computed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As an indication, the number of frames that most affect the average is given
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: So, `beta = 0.9` would average 10 frames, and `beta = 0.95` would average 20
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the chapter. I invite you to check the full code on GitHub and
    to play around with it. You can find some real-life footage and try to identify
    the lanes there.
  prefs: []
  type: TYPE_NORMAL
- en: And don't forget to apply the **camera calibration**, if you can.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a nice pipeline to detect lanes. First, we analyzed
    different color spaces, such as RGB, HLS, and HSV, to see which channels would
    be more useful to detect lanes. Then, we used perspective correction, with `getPerspectiveTransform()`,
    to obtain a *bird's eye view* and make parallel lines on the road also look parallel
    on the image we analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: We used edge detection with `Scharr()` to detect edges and make our analysis
    more robust than using only a color threshold, and we combined the two. We then
    computed a histogram to detect where the lanes start, and we used the "sliding
    window" technique to "follow" the lane in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we used `polyfit()` to fit a second-order polynomial on the pixels detected,
    making sense of them, and we used the coefficients returned by the function to
    generate our curve, after having applied reverse perspective correction on them.
    Finally, we discussed two techniques that can be applied to a video stream to
    improve the precision: partial histogram and rolling average.'
  prefs: []
  type: TYPE_NORMAL
- en: Using all these techniques together, you can now build a pipeline that can detect
    the lanes on a road.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce deep learning and neural networks, powerful
    tools that we can use to accomplish even more complex computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you name some color spaces, other than RGB?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we apply perspective correction?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we detect where the lane starts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which technique can you use to *follow the lane* in the image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have many points forming more or less a lane, how can you convert them
    into a line?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which function can you use for edge detection?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can you use to compute the average of the last *N* positions of the lane?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
