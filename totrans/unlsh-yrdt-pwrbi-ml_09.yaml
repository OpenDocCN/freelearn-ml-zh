- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating Trained and Tested ML Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B19500_08.xhtml#_idTextAnchor118) of this book, you built
    three ML models in Power BI. The models were trained and tested using FAA Wildlife
    Strike data and attempted to predict the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether wildlife striking an aircraft caused damage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the wildlife that struck the aircraft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The height at which the wildlife strike occurred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will review the results of the testing that Power BI does after
    training the ML models. After reviewing the testing results, you will make changes
    to the training data with the intent of improving predictive capabilities of the
    ML models. At the end of this chapter, all three ML models will be ready to deploy
    and configure for use with Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few key terms that you may want to research before reading this
    chapter if you are new to ML. The definitions given here are taken verbatim from
    the documentation at this link: [https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#classification-metrics](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#classification-metrics):'
  prefs: []
  type: TYPE_NORMAL
- en: '**The area under the curve** (**AUC**): The AUC can be interpreted as the proportion
    of correctly classified samples. More precisely, the AUC is the probability that
    the classifier will rank a randomly chosen positive sample higher than a randomly
    chosen negative sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Recall is the ability of a model to detect all positive samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: Precision is the ability of a model to avoid labeling negative
    samples as positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As with the previous chapters, you’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: FAA Wildlife Strike data files from either the FAA website or the Packt GitHub
    site
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Power BI Pro license
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the following Power BI licensing options for access to Power BI dataflows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power BI Premium
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Power BI Premium Per User
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the following options for getting data into the Power BI cloud service:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft OneDrive (with connectivity to the Power BI cloud service)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Access + Power BI Gateway
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Data Lake (with connectivity to the Power BI cloud service)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating test results for the Predict Damage ML model in Power BI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the three ML models have completed training, you can take a look at the
    testing results for each of those models using a pre-built report in Power BI.
    The training report will provide metrics to help you determine whether the models
    have some feedback about the value of the predictions. You start with **Predict
    Damage ML Model**, which is a binary prediction ML model. While in your workspace,
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Predict Damage ML** **Model** dataflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the ribbon, select **Machine** **learning models**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under the **ACTIONS** column, click the clipboard to access **View training
    report**, per the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Navigation for the Predict Damage ML Model training report](img/B19500_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Navigation for the Predict Damage ML Model training report
  prefs: []
  type: TYPE_NORMAL
- en: 'The report should open to look like the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The training report for Predict Damage ML Model](img/B19500_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The training report for Predict Damage ML Model
  prefs: []
  type: TYPE_NORMAL
- en: Note that your metrics on this page may differ due to random sampling of the
    testing data and any changes to the scope of source data used for training and
    testing. Also, Power BI ML will randomly split the data into testing and training
    subsets, which may be different every time it runs.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance for Predict Damage ML Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Predict Damage ML Model** testing has yielded some interesting results.
    In the example provided above, you can see that the model performance, or AUC,
    is listed at 91%. The closer that the AUC is to 100%, the better an ML model is
    at overall correct predictions. Your first inclination might be to claim success,
    but there are some additional details to consider.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a binary prediction ML model, the prediction is for either a value of 1
    (yes) or 0 (no). When predicting whether damage happened due to a wildlife strike,
    there are four possible outcomes for each row of data and comparing it to the
    real result:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Prediction** | **What** **really happened** | **Outcome** |'
  prefs: []
  type: TYPE_TB
- en: '| Damage happened | Damage really happened | True positive |'
  prefs: []
  type: TYPE_TB
- en: '| Damage happened | Damage didn’t happen | False positive |'
  prefs: []
  type: TYPE_TB
- en: '| Damage didn’t happen | Damage really happened | False negative |'
  prefs: []
  type: TYPE_TB
- en: '| Damage didn’t happen | Damage didn’t happen | True negative |'
  prefs: []
  type: TYPE_TB
- en: Figure 9.3 – Four possible outcomes of testing the ML model versus reality
  prefs: []
  type: TYPE_NORMAL
- en: As seen in *Figure 9**.2*, there is a sliding bar filter in the bottom-right
    portion of the report called **Probability Threshold**. The probability threshold
    is a value assigned to each prediction between zero and one to indicate the certainty
    of the prediction. A probability score of 99 could be interpreted as “*The ML
    model is 99% certain that this incident caused damage*.” Can you rely on the certainty
    of the ML model, and does that number reflect reality? The testing results can
    help you find out!
  prefs: []
  type: TYPE_NORMAL
- en: 'You move `0.50` and view the results. Notice that other values on the page
    change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Model performance report with Probability Threshold set to 0.50](img/B19500_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Model performance report with Probability Threshold set to 0.50
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the grid having four boxes, you note that 484 incidents were correctly
    predicted to have had damage. However, 69 incidents that had real damage were
    missed by the ML model. 484/(484 + 69) is about an `0.50`, only **30%** of the
    incidents that are flagged as causing damage will actually have caused damage.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `0.80`, the metrics change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – The model performance report with Probability Threshold set
    to 0.80](img/B19500_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – The model performance report with Probability Threshold set to
    0.80
  prefs: []
  type: TYPE_NORMAL
- en: With `0.80`, `0.80` might eliminate false positives but 33% (1 – **Recall**
    of 0.67) of actual real damage events would be missed.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, **Probability Threshold** functions as a cut-off value that can
    impact the four possible outcomes of this binary prediction ML model, as illustrated
    in the following charts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Impact of Probability Threshold on results versus real tested
    results](img/B19500_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Impact of Probability Threshold on results versus real tested results
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down *Figure 9**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: The top-left chart shows the real results of each incident with **ML Prediction
    Score** on the *x* axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-right chart shows the impact of setting `0.50` on **False Negative**
    (**FN**), **False Positive** (**FP**), **True Negative** (**TN**), and **True**
    **Positive** (**TP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-left chart shows the impact of setting `0.40`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-right chart shows the impact of setting `0.70`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 9**.7* sums up the impact of changing **Probability Threshold** for
    this ML model in practical terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Testing outcome** | **Increase** **Probability Threshold** | **Decrease**
    **Probability Threshold** |'
  prefs: []
  type: TYPE_TB
- en: '| True Positive | Fewer | More |'
  prefs: []
  type: TYPE_TB
- en: '| True Negative | More | Fewer |'
  prefs: []
  type: TYPE_TB
- en: '| False Positive | Fewer | More |'
  prefs: []
  type: TYPE_TB
- en: '| False Negative | More | Fewer |'
  prefs: []
  type: TYPE_TB
- en: Figure 9.7 – Impact of Probability Threshold on testing outcomes
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll need to go back to your stakeholders to ask for feedback about the importance
    of the different possible outcomes in *Figures 9.6* and *9.7*. Verbalizing this
    impact to non-technical stakeholders can be a challenge. The optimal **Probability
    Threshold** will depend upon the requirements of your stakeholders. You’ll need
    to ask something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Do you want to flag incidents more frequently to capture as many real incidents
    with damage as possible, at the expense of having more incorrectly flagged incidents
    that did not cause damage? Or do you want fewer flagged incidents, with fewer
    incorrectly flagged incidents, but at the expense of not flagging and missing
    some incidents* *with damage?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The screen has been split in half to improve visibility. *Figure 9**.8* shows
    the left half of the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Top predictors for the Predict Damage ML binary prediction model](img/B19500_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Top predictors for the Predict Damage ML binary prediction model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upper-right portion of the report page (see *Figure 9**.9*), you click
    a yellow box labeled **See top predictors**. A list of the top predictive features
    is shown, and you click on **Size** **is Small**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Top predictors for the Predict Damage ML binary prediction model](img/B19500_09_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Top predictors for the Predict Damage ML binary prediction model
  prefs: []
  type: TYPE_NORMAL
- en: You see that when you click on a predictor (feature) value, a chart pops up
    on the right to visualize that feature. You can also hover over bars on these
    charts for greater detail. For **Size is Small**, the chart shows that when **Size
    is Large**, damage happened about 45% of the time. When **Size is Small**, damage
    happened about 2.5% of the time. That’s a big difference! You can click on each
    of the top predictors and take notes on the findings.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the bottom of the **MODEL PERFORMANCE** page, you find a **Cost-Benefit
    Analysis** chart with filters, as shown in *Figure 9**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Cost-Benefit Analysis for Predict Damage ML](img/B19500_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Cost-Benefit Analysis for Predict Damage ML
  prefs: []
  type: TYPE_NORMAL
- en: The best way to summarize this chart, in practical terms, is that it allows
    you to determine the impact of different `10,000`, the cost of a prediction (`1`,
    and the relative benefit of a correct prediction is set to `2`, then the maximum
    profit per the testing would be `202.09` if the threshold were set to `0.94`,
    and only `4.73%` of the population would be targeted. With this particular use
    case, the setting of **Probability Threshold** will probably require a conversation
    with your stakeholders. While an interesting discussion point, there are other
    factors to consider for the implications of wildlife striking aircraft, such as
    passenger safety, impact on wildlife, end user use of the predictions, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy report for Predict Damage ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, you move to the next page of the training report, named **ACCURACY REPORT**,
    as you can see in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Accuracy report for the Predict Damage ML binary prediction
    model](img/B19500_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Accuracy report for the Predict Damage ML binary prediction model
  prefs: []
  type: TYPE_NORMAL
- en: 'The top of the accuracy report provides detailed explanations for **Cumulative
    Gains Chart** and **ROC Curve** breakdowns. Scroll down the page to view those
    charts. **Cumulative Gains Chart** is on the left and shows how the ML model performs
    compared to a perfect model and random guessing. As you hover over the line and
    move to the right, you can see the performance as **Probability** **Threshold**
    decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Cumulative Gains Chart (left) displays testing results against
    a perfect model and random guesses](img/B19500_09_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Cumulative Gains Chart (left) displays testing results against
    a perfect model and random guesses
  prefs: []
  type: TYPE_NORMAL
- en: '**ROC Curve** on the right side of the page shows how well the model predicts
    positives as positives (true positive) and negatives as negatives (true negative).
    A curve elevated upward and to the left indicates good performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – ROC Curve (right) visualizes the ML model’s ability to distinguish
    the target outcome](img/B19500_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – ROC Curve (right) visualizes the ML model’s ability to distinguish
    the target outcome
  prefs: []
  type: TYPE_NORMAL
- en: Training Details for Predict Damage ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, you move to the last page of the report, named **TRAINING DETAILS**.
    The top of this page displays details about the testing such as the number of
    rows sampled, the number of rows used for testing, the type of model that was
    selected as having the best results, and the number of iterations run to determine
    the best-fit model. The page can be seen in *Figure 9**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 9.14 – TRAINING DETAILS for the Predict Damage ML binary prediction
    ML model](img/B19500_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – TRAINING DETAILS for the Predict Damage ML binary prediction ML
    model
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll also notice the **Model quality over iterations** chart, showing ML
    model quality comparisons during the iterative training and testing. Scrolling
    down the page, you can view details about the ML model such as the features in
    the model, the data types and imputation of those features, and the parameters
    that were used to create the model. Here’s a screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Details about the ML model including features and parameters](img/B19500_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Details about the ML model including features and parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrolling down to the bottom third of the **TRAINING DETAILS** page, you will
    see a donut chart of the different ML algorithms used as part of an ensemble model.
    Hovering over an algorithm, you can see details about how it has been used. With
    Power BI, you use a SaaS ML tool, so there isn’t a need to dive deeper into these
    details. If a data science team wants to extend your findings by building custom
    ML models in a tool such as Azure ML, this information might be of value to them
    as they plan their follow-up project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Ensemble machine learning model algorithms for the Predict
    Damage ML model](img/B19500_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Ensemble machine learning model algorithms for the Predict Damage
    ML model
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the process to create the **Predict Damage ML** model, you included
    a long list of features that you considered possible candidates for having predictive
    capabilities. In [*Chapter 10*](B19500_10.xhtml#_idTextAnchor139), you will revisit
    this ML model to retrain it with a more succinct and carefully chosen list of
    features before deploying the ML model. Now, you are ready to move on to the next
    ML model for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: "Evaluating test results for Predict Size ML Model in \LPower BI"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your next ML model to review is a classification model for predicting the size
    of wildlife that struck an aircraft. These predictions don’t necessarily indicate
    the size of the actual animal. For example, a large flock of smaller birds might
    also be considered a large impact. Predicting these values could help understand
    the likelihood of incidents that are perceived to be more severe.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance for Predict Size ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Moving on to the test results for your ML model to predict the size of an animal
    or animals that struck an aircraft, you follow similar steps as in the previous
    section to open the report. From your Power BI workspace, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Predict Size ML** **Model** dataflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the ribbon, select **Machine** **learning models**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **ACTIONS** column, click the clipboard to access **View** **training
    report**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The report should look like the following figure, and on the bar chart, you
    can click on the **Small** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.17 – The training report for the Predict Size ML classification
    model with the Small class selected](img/B19500_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – The training report for the Predict Size ML classification model
    with the Small class selected
  prefs: []
  type: TYPE_NORMAL
- en: First, you notice that this report looks slightly different since it is for
    a different type of ML model, a classification model. The AUC, at the top right
    of the report page, is `60%`. As a general rule of thumb, an AUC under 70% is
    not very good. An AUC of 50% generally represents random guessing, and 60% is
    only slightly above that value. You’ll need to dive deeper into the testing results
    to find opportunities for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The **Small** class (you clicked on in *Figure 9**.16*) had a **Precision**
    rating of **88%**, which means that when a prediction of **Small** was made, it
    turned out to be true 88% of the time. **Recall** for **Small** was only **65%**,
    which means that only 65% of actual small incidents were captured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click on the **Medium** class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Training report for the Predict Size ML classification model
    with the Medium class selected](img/B19500_09_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Training report for the Predict Size ML classification model with
    the Medium class selected
  prefs: []
  type: TYPE_NORMAL
- en: The results for predicting **Medium** are not very good. With a **Precision**
    rating of **25%**, most of the predictions for **Medium** turned out to be wrong.
    Adding to the disappointment, only **36%** of actual medium strike incidents were
    captured by the **Medium** prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to the **Large** class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – The training report for the Predict Size ML classification
    model with the Large class selected](img/B19500_09_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – The training report for the Predict Size ML classification model
    with the Large class selected
  prefs: []
  type: TYPE_NORMAL
- en: The **Large** class had a **Precision** rating of **22%**, indicating that a
    prediction of **Large** is only correct about 1 out of 5 times. **66%** of actual
    large-sized incidents were captured in those predictions. In summary, a prediction
    of **Large** is usually inaccurate but about two-thirds of actual large events
    are captured in that prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrolling to the bottom of the **MODEL PERFORMANCE** page, you can click on
    the different classes that were predicted to see the top predictors. Within the
    **Large** class, you click on the bar for **Indicated Damage** to pop up a chart
    on the right, which shows details about that feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Top Predictors for the Large class with Indicated Damage details](img/B19500_09_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Top Predictors for the Large class with Indicated Damage details
  prefs: []
  type: TYPE_NORMAL
- en: When the class is **Large**, the features on the left side chart of *Figure
    9**.18* were determined to influence that class. You’ve clicked on the **Indicated
    Damage** feature and can see that when that value is **1**, the class will be
    **Large** 35% of the time. When set to **0**, a classification of **Large** occurs
    less than 5% of the time. You can click through the different classes (**Large**,
    **Medium**, and **Small**) to view the features that were top influencers and
    take notes for your next iteration of the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Training details for Predict Size ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Moving on to the **TRAINING DETAILS** page for **Predict Size ML**, you’ll
    see a similar page to that for **Predict** **Damage ML**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – TRAINING DETAILS includes details about training the ML model](img/B19500_09_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – TRAINING DETAILS includes details about training the ML model
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the quality of the ML model declined with an increase in the
    number of iterations. With machine learning, more data is not always better, and
    greater complexity does not always yield better results. As a novice with these
    tools, sometimes, trial and error is the best way to familiarize yourself with
    these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrolling to the middle of the **TRAINING DETAILS** page, you can view details
    about the features used in the ML model and the final parameters. Again, this
    is a similar page to the **Predict Damage** **ML** report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Features and parameters for the Predict Size ML model](img/B19500_09_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – Features and parameters for the Predict Size ML model
  prefs: []
  type: TYPE_NORMAL
- en: You can also scroll down to the bottom third of the page and view the **Ensemble
    machine learning** information. As stated for **Predict Damage ML**, this information
    is interesting but mostly useful for data scientists who might want to extend
    the findings of this project into a tool such as Azure ML. You’ll revisit this
    ML model in the next chapter, but for now, you can move on to the **Predict Height**
    **ML** model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating test results for the Predict Height ML model in Power BI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, you review the test results for the **Predict Height ML** model. This
    was a regression model attempting to predict the height at which wildlife strikes
    happened to aircraft. This model does not predict a yes/no answer or a categorical
    value, but rather a numeric value representing the height in feet from the ground.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance for Predict Height ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start by navigating to the **MODEL PERFORMANCE** page of the training report:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Predict Height ML** **Model** dataflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the ribbon, select **Machine** **learning models**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **ACTIONS** column, click the clipboard to access **View** **training
    report**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The report should look like the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Performance for the Predict Height ML regression model](img/B19500_09_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – Performance for the Predict Height ML regression model
  prefs: []
  type: TYPE_NORMAL
- en: First, you notice at the top of the page that **Model Performance** is **80%**.
    With a regression model, the numeric differences between predicted and real results
    are represented by this value. The features provide predictability, but you’d
    like to get that number even higher for better results.
  prefs: []
  type: TYPE_NORMAL
- en: The chart on the left side of the page, **Predicted vs Actual Height** plots
    **Actual value** versus **Predicted value** for the height at which an impact
    occurred. Variation is expected due to different aircraft models, weather, species,
    and many other factors. Perfect predictability would display the dots on the line
    separating the red and blue portions of the chart. You can see that the regression
    line generally follows through results.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right side of the page in *Figure 9**.22*, the chart for **Residual
    error by Height** shows the average error for different values that were tested.
    A value of -5% would mean that the prediction is usually 5% too low for a range
    of values. You notice that the first bubble on the *x* axis appears close to 0%.
    When you hover over it, you see the values, and you can click it to filter the
    chart on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – The lowest range in height had an extremely high number of
    residual errors](img/B19500_09_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – The lowest range in height had an extremely high number of residual
    errors
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the **Predicted vs Actual Height** chart, you can see that a low
    height above the ground had a residual error of **1%**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the third bubble from the left, a height of 6,500-7,800 feet,
    the average residual error is now **-40%**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – The bins covering heights of 3,200-4,500 feet have a lower
    average number of residual errors](img/B19500_09_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – The bins covering heights of 3,200-4,500 feet have a lower average
    number of residual errors
  prefs: []
  type: TYPE_NORMAL
- en: On the **Predicted vs Actual Height** chart for the third bin, notice that many
    actual values are reported at a height of 6,500, 7,000, and 7,500 when you click
    on the bubble to filter the chart on the left. Looking into reasons for this conformity
    might help you understand the nuances in the source data. Are pilots estimating
    the height of a wildlife strike incident? Are these common stable altitudes for
    aircraft flight plans when they are not increasing or decreasing height? Is the
    conformity just a coincidence? The root cause of this pattern can only be discovered
    with additional investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Training details for Predict Height ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Moving on to the final page of the training report for **Predict Height ML,**
    named **TRAINING DETAILS**, you see a similar report structure to the previous
    two ML model training reports. The top of the page displays the number of rows
    sampled, the number of rows used for training, the final model used, and the number
    of iterations that were run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – TRAINING DETAILS for the Predict Height ML regression model](img/B19500_09_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – TRAINING DETAILS for the Predict Height ML regression model
  prefs: []
  type: TYPE_NORMAL
- en: Scrolling down the page, you will again see similar charts and information about
    the features used, the parameters, and the algorithms used by automated ML in
    Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed your review of the testing and training report for all three
    ML models, you are ready to move on to the next chapter of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter took a deep dive into the training and testing reports for your
    **Predict Damage ML**, **Predict Size ML**, and **Predict Height ML** models.
    In doing so, you reviewed the reports for all three types of ML models in Power
    BI: binary prediction, classification, and regression. You evaluated how well
    each of these models made predictions by reviewing the testing data in Power BI.
    You also explored lists of features that were highly predictive.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will modify the filter criteria and features selected
    for your ML models, with the goal of improving the predictive capabilities. Iterative
    training and testing are the best way to improve your ML models, and this process
    will help you prepare for your own Power BI ML projects beyond the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
