<html><head></head><body><div class="calibre1" title="Chapter&#xA0;9.&#xA0;Describing and Matching Interest Points"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title"><a id="ch09" class="calibre6"/>Chapter 9. Describing and Matching Interest Points</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem">Matching local templates</li><li class="listitem">Describing and matching local intensity patterns</li><li class="listitem">Matching keypoints with binary descriptors</li></ul></div><div class="calibre1" title="Introduction"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch09lvl1sec57" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">In the previous chapter, we learned how to detect special points in an image with the objective of subsequently performing local image analysis. These keypoints are chosen to be distinctive enough so that if a keypoint is detected on the image of an object, then the same point is expected to be detected in other images depicting the same object. We also described some more sophisticated interest point detectors that can assign a representative scale factor and/or an orientation to a keypoint. As we will see in this chapter, this additional information can be useful to normalize scene representations with respect to viewpoint variations.</p><p class="calibre8">In order to perform image analysis based on interest points, we now need to build rich representations that uniquely describe each of these keypoints. This chapter looks at different approaches that have been proposed to extract descriptors from interest points. These descriptors are generally 1D or 2D vectors of binary, integer, or floating-point numbers that describe a keypoint and its neighborhood. A good descriptor should be distinctive enough to uniquely represent each keypoint of an image; it should be robust enough to have the same points represented similarly in spite of possible illumination changes or viewpoint variations. Ideally, it should also be compact to reduce memory load and improve computational efficiency.</p><p class="calibre8">One of the most common operations accomplished with keypoints is image matching. This task could be performed, for example, to relate two images of the same scene or to detect the occurrence of a target object in an image. Here, we will study some basic matching strategies, a subject that will be further discussed in the next chapter.</p></div></div>
<div class="calibre1" title="Matching local templates"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch09lvl1sec58" class="calibre6"/>Matching local templates</h1></div></div></div><p class="calibre8">Feature point matching is the operation by which one can put in correspondence points from one image to points from another image (or points from an image set). Image points should match when they correspond to the image of the same scene element in the real world.</p><p class="calibre8">A single pixel is certainly not sufficient to make a decision on the similarity of two keypoints. This is why an image patch around each keypoint must be considered during the matching process. If two patches correspond to the same scene element, then one might expect their pixels to exhibit similar values. A direct pixel-by-pixel comparison of pixel patches is the solution presented in this recipe. This is probably the simplest approach to feature point matching, but as we will see, it is not the most reliable one. Nevertheless, in several situations, it can give good results.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec170" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Most often, patches are defined as squares of odd sizes centered at the keypoint position. The similarity between two square patches can then be measured by comparing the corresponding pixel intensity values inside the patches. A simple <span><strong class="calibre15">Sum of Squared Differences</strong></span> (<span><strong class="calibre15">SSD</strong></span>) is a popular solution. The feature matching strategy then works as follows. First, the keypoints are detected in each image. Here, we use the FAST detector:</p><pre class="programlisting">    // Define feature detector 
    cv::Ptr&lt;cv::FeatureDetector&gt; ptrDetector;   // generic detector 
    ptrDetector= // we select the FAST detector 
                cv::FastFeatureDetector::create(80);    
 
    // Keypoint detection 
    ptrDetector-&gt;detect(image1,keypoints1); 
    ptrDetector-&gt;detect(image2,keypoints2); 
</pre><p class="calibre8">Note how we used the generic <code class="literal">cv::Ptr&lt;cv::FeatureDetector&gt;</code> pointer type, which can refer to any feature detector. One can then test this code on different interest point detectors just by changing the detector to be used when calling the <code class="literal">detect</code> function.</p><p class="calibre8">The second step is to define a rectangle of, for example, size <code class="literal">11x11</code> that will be used to define patches around each keypoint:</p><pre class="programlisting">    // Define a square neighborhood 
    const int nsize(11);                       // size of the neighborhood 
    cv::Rect neighborhood(0, 0, nsize, nsize); // 11x11 
    cv::Mat patch1; 
    cv::Mat patch2; 
</pre><p class="calibre8">The keypoints in one image are compared with all the keypoints in the other image. For each keypoint of the first image, the most similar patch in the second image is identified. This process is implemented using two nested loops, as shown in the following code:</p><pre class="programlisting">    // For all keypoints in first image 
    // find best match in second image 
    cv::Mat result; 
    std::vector&lt;cv::DMatch&gt; matches; 
 
    // for all keypoints in image 1 
    for (int i=0; i&lt;keypoints1.size(); i++) { 
 
      // define image patch 
      neighborhood.x = keypoints1[i].pt.x-nsize/2; 
      neighborhood.y = keypoints1[i].pt.y-nsize/2; 
 
      // if neighborhood of points outside image, 
      // then continue with next point 
      if (neighborhood.x&lt;0 || neighborhood.y&lt;0 ||   
          neighborhood.x+nsize &gt;= image1.cols || 
          neighborhood.y+nsize &gt;= image1.rows) 
      continue; 
 
      // patch in image 1 
      patch1 = image1(neighborhood); 
 
      // to contain best correlation value; 
      cv::DMatch bestMatch; 
 
      // for all keypoints in image 2 
      for (int j=0; j&lt;keypoints2.size(); j++) { 
 
        // define image patch 
        neighborhood.x = keypoints2[j].pt.x-nsize/2; 
        neighborhood.y = keypoints2[j].pt.y-nsize/2; 
 
        // if neighborhood of points outside image, 
        // then continue with next point 
        if (neighborhood.x&lt;0 || neighborhood.y&lt;0 ||   
            neighborhood.x + nsize &gt;= image2.cols ||   
            neighborhood.y + nsize &gt;= image2.rows) 
        continue; 
 
       // patch in image 2 
       patch2 = image2(neighborhood); 
 
       // match the two patches 
       cv::matchTemplate(patch1,patch2,result, cv::TM_SQDIFF); 
 
       // check if it is a best match 
       if (result.at&lt;float&gt;(0,0) &lt; bestMatch.distance) { 
 
         bestMatch.distance= result.at&lt;float&gt;(0,0); 
         bestMatch.queryIdx= i; 
         bestMatch.trainIdx= j; 
       } 
     } 
 
     // add the best match 
     matches.push_back(bestMatch); 
   } 
</pre><p class="calibre8">Note the use of the <code class="literal">cv::matchTemplate</code> function, which we will describe in the next section and that computes the patch similarity score. When a potential match is identified, this match is represented through the use of a <code class="literal">cv::DMatch</code> object. This utility class stores the index of the two matching <code class="literal">keypoints</code> as well as their similarity score.</p><p class="calibre8">The more similar the two image patches are, the higher the probability that these patches correspond to the same scene point. This is why it is a good idea to sort the resulting match points by their similarity scores:</p><pre class="programlisting">    // extract the 25 best matches 
    std::nth_element(matches.begin(),   
                     matches.begin() + 25,matches.end()); 
    matches.erase(matches.begin() + 25,matches.end()); 
</pre><p class="calibre8">You can then simply retain the matches that pass a given similarity threshold. Here, we chose to keep only the <code class="literal">N</code> best matching points (we use <code class="literal">N=25</code> to facilitate the visualization of the matching results).</p><p class="calibre8">Interestingly, there is an OpenCV function that can display the matching results by concatenating the two images and joining each corresponding point by a line. The function is used as follows:</p><pre class="programlisting">    //Draw the matching results 
    cv::Mat matchImage; 
    cv::drawMatches(image1,keypoints1,         // first image 
                    image2,keypoints2,         // second image 
                    matches,                   // vector of matches 
                    cv::Scalar(255,255,255),   // color of lines 
                    cv::Scalar(255,255,255));  // color of points 
</pre><p class="calibre8">Here are the match results:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_09_001-1.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec171" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The results obtained are certainly not perfect, but a visual inspection of the matched image points shows a number of successful matches. It can also be observed that the symmetry of the two towers of the church causes some confusion. Also, since we tried to match all the points in the left image with the ones in the right image, we obtained cases where a point in the right image was matched with multiple points in the left image. This is an asymmetrical matching situation that can be corrected by, for example, keeping only the match with the best score for each point in the right image.</p><p class="calibre8">To compare the image patches from each image, here we used a simple criterion, that is, a pixel-per-pixel sum of the squared difference specified using the <code class="literal">cv::TM_SQDIFF</code> flag. If we compare the point <code class="literal">(x,y)</code> of image <code class="literal">I1</code> with a putative match at <code class="literal">(x',y')</code> in image <code class="literal">I2</code>, then the similarity measure is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_09_002-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, the sum of the <code class="literal">(i,j)</code> point provides the offset to cover the square template centered at each point. Since the difference between adjacent pixels in similar patches should be small, the best-matching patches should be the ones with the smallest sum. This is what is done in the main loop of the matching function; that is, for each keypoint in one image, we identify the keypoint in the other image that gives the lowest sum of the squared difference. We can also reject matches for which this sum is over a certain threshold value. In our case, we simply sort them from the most similar to the least similar ones.</p><p class="calibre8">In our example, the matching was done with square patches of size <code class="literal">11x11</code>. A larger neighborhood creates more distinctive patches, but it also makes them more sensitive to local scene variations.</p><p class="calibre8">Comparing two image windows from a simple sum of square differences will work relatively well as long as the two images show the scene from similar points of views and similar illumination. Indeed, a simple lighting change will increase or decrease all the pixel intensities of a patch, resulting in a large square difference. To make matching more invariant to lighting changes, other formulae could be used to measure the similarity between two image windows. OpenCV offers a number of these. A very useful formula is the normalized sum of square differences (the <code class="literal">cv::TM_SQDIFF_NORMED</code> flag):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_09_003-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Other similarity measures are based on the concept of correlation, defined in the signal processing theory, as follows (with the <code class="literal">cv::TM_CCORR</code> flag):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_09_004-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This value will be maximal when two patches are similar.</p><p class="calibre8">The identified matches are stored in a vector of <code class="literal">cv::DMatch</code> instances. Essentially, the <code class="literal">cv::DMatch</code> data structure contains a first index that refers to an element in the first vector of keypoints and a second index that refers to the matching feature in the second vector of keypoints. It also contains a real value that represents the distance between the two matched descriptors. This distance value is used in the definition of <code class="literal">operator&lt;</code> when comparing two <code class="literal">cv::DMatch</code> instances.</p><p class="calibre8">When we drew the matches in the previous section, we wanted to limit the number of lines to make the results more readable. Therefore, we only displayed the <code class="literal">25</code> matches that had the lowest distance. To do this, we used the <code class="literal">std::nth_element</code> function, which positions the Nth element at the Nth position, with all smaller elements placed before this element. Once this is done, the vector is simply purged of its remaining elements.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec172" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The <code class="literal">cv::matchTemplate</code> function is at the heart of our feature matching method. We used it here in a very specific way, which is to compare two image patches. However, this function has been designed to be used in a more generic way.</p><div class="calibre1" title="Template matching"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch09lvl3sec36" class="calibre6"/>Template matching</h3></div></div></div><p class="calibre8">A common task in image analysis is to detect the occurrence of a specific pattern or object in an image. This can be done by defining a small image of the object, a template, and searching for a similar occurrence in a given image. In general, the search is limited to a region of interest inside which we think the object can be found. The template is then slid over this region, and a similarity measure is computed at each pixel location. This is the operation performed by the <code class="literal">cv::matchTemplate</code> function. The input is a template image of a small size and an image over which the search is performed.</p><p class="calibre8">The result is a <code class="literal">cv::Mat</code> function of floating-point values that correspond to the similarity score at each pixel location. If the template is of size <code class="literal">MxN</code> and the image is of size <code class="literal">WxH</code>, then the resulting matrix will have a size of <code class="literal">(W-M+1)x(H-N+1)</code>. In general, you will be interested in the location of the highest similarity; so, the typical template-matching code will look as follows (assuming that the target variable is our template):</p><pre class="programlisting">    // define search region 
    cv::Mat roi(image2, // here top half of the image 
    cv::Rect(0,0,image2.cols,image2.rows/2)); 
 
    // perform template matching 
    cv::matchTemplate(roi,            // search region 
                      target,         // template 
                      result,         // result 
                      cv::TM_SQDIFF); // similarity measure 
 
    // find most similar location 
    double minVal, maxVal; 
    cv::Point minPt, maxPt; 
    cv::minMaxLoc(result, &amp;minVal, &amp;maxVal, &amp;minPt, &amp;maxPt); 
 
    // draw rectangle at most similar location 
    // at minPt in this case 
    cv::rectangle(roi, cv::Rect(minPt.x, minPt.y,  
                                target.cols, target.rows), 255); 
</pre><p class="calibre8">Remember that this is a costly operation, so you should limit the search area and use a template having a size of only a few pixels.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec173" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The next recipe, <span><em class="calibre16">Describing and matching local intensity patterns</em></span>, describes the <code class="literal">cv::BFMatcher</code> class, which implements the matching strategy that was used in this recipe</li></ul></div></div></div>
<div class="calibre1" title="Describing and matching local intensity patterns"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch09lvl1sec59" class="calibre6"/>Describing and matching local intensity patterns</h1></div></div></div><p class="calibre8">The SURF and SIFT keypoint detection algorithms, discussed in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <span><em class="calibre16">Detecting Interest Points</em></span>, define a location, an orientation, and a scale for each of the detected features. The scale factor information is useful for defining the size of a window of analysis around each feature point. Thus, the defined neighborhood would include the same visual information no matter at what scale of the object to which the feature belongs has been pictured. This recipe will show you how to describe an interest point's neighborhood using feature descriptors. In image analysis, the visual information included in this neighborhood can be used to characterize each feature point in order to make each point distinguishable from the others. Feature descriptors are usually N-dimensional vectors that describe a feature point in a way that is invariant to change in lighting and to small perspective deformations. Generally, descriptors can be compared using simple distance metrics, for example, the Euclidean distance. Therefore, they constitute a powerful tool that can be used in object matching applications.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec174" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The <code class="literal">cv::Feature2D</code> abstract class defines a number of member functions that are used to compute the descriptors of a list of keypoints. As most feature-based methods include both a detector and a descriptor component, the associated classes include both a <code class="literal">detect</code> function (to detect the interest points) and a <code class="literal">compute</code> function (to compute their descriptors). This is the case of the <code class="literal">cv::SURF</code> and <code class="literal">cv::SIFT</code> classes. Here is, for example, how you can detect and describe feature points in two images using one instance of <code class="literal">cv::SURF</code>:</p><pre class="programlisting">    // Define keypoints vector 
    std::vector&lt;cv::KeyPoint&gt; keypoints1; 
    std::vector&lt;cv::KeyPoint&gt; keypoints2; 
 
    // Define feature detector 
    cv::Ptr&lt;cv::Feature2D&gt; ptrFeature2D =     
                          cv::xfeatures2d::SURF::create(2000.0); 
 
    // Keypoint detection 
    ptrFeature2D-&gt;detect(image1,keypoints1); 
    ptrFeature2D-&gt;detect(image2,keypoints2); 
 
    // Extract the descriptor 
    cv::Mat descriptors1; 
    cv::Mat descriptors2; 
    ptrFeature2D-&gt;compute(image1,keypoints1,descriptors1); 
    ptrFeature2D-&gt;compute(image2,keypoints2,descriptors2); 
</pre><p class="calibre8">For SIFT, you simply call the <code class="literal">cv::SIFT::create</code> function. The result of the computation of the interest point descriptors is a matrix (that is, a <code class="literal">cv::Mat</code> instance) that will contain as many rows as the number of elements in the keypoint vector. Each of these rows is an N-dimensional descriptor vector. In the case of the SURF descriptor, it has a default size of <code class="literal">64</code>, and for SIFT, the default dimension is <code class="literal">128</code>. This vector characterizes the intensity pattern surrounding a feature point. The more similar the two feature points, the closer their descriptor vectors should be. Note that you do not have to necessarily use the SURF (SIFT) descriptor with SURF (SIFT) points; detectors and descriptors can be used in any combination.</p><p class="calibre8">These descriptors will now be used to match our keypoints. Exactly as we did in the previous recipe, each feature descriptor vector in the first image is compared to all the feature descriptors in the second image. The pair that obtains the best score (that is, the pair with the lowest distance between the two descriptor vectors) is then kept as the best match for that feature. This process is repeated for all the features in the first image. Very conveniently, this process is implemented in OpenCV in the <code class="literal">cv::BFMatcher</code> class, so we do not need to re-implement the double loops that we previously built. This class is used as follows:</p><pre class="programlisting">    // Construction of the matcher  
    cv::BFMatcher matcher(cv::NORM_L2); 
    // Match the two image descriptors 
    std::vector&lt;cv::DMatch&gt; matches; 
    matcher.match(descriptors1,descriptors2, matches); 
</pre><p class="calibre8">This class is a subclass of the <code class="literal">cv::DescriptorMatcher</code> class that defines the common interface for different matching strategies. The result is a vector of the <code class="literal">cv::DMatch</code> instances.</p><p class="calibre8">With the current Hessian threshold for SURF, we obtained <code class="literal">74</code> keypoints for the first image and <code class="literal">71</code> for the second. The brute-force approach will then produce <code class="literal">74</code> matches. Using the <code class="literal">cv::drawMatches</code> class as in the previous recipe produces the following image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05388_09_02.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As it can be seen, several of these matches correctly link a point on the left-hand side with its corresponding point on the right-hand side. You might notice some errors; some of these are due to the fact that the observed building has a symmetrical facade, which makes some of the local matches ambiguous. For SIFT, with the same number of keypoints, we obtained the following match result:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05388_09_03.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec175" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Good feature descriptors must be invariant to small changes in illumination and viewpoint and to the presence of image noise. Therefore, they are often based on local intensity differences. This is the case for the SURF descriptors, which locally apply the following simple kernels around a keypoint:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_09_007.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The first kernel simply measures the local intensity difference in the horizontal direction (designated as <code class="literal">dx</code>), and the second measures this difference in the vertical direction (designated as <code class="literal">dy</code>). The size of the neighborhood used to extract the descriptor vector is generally defined as <code class="literal">20</code> times the scale factor of the feature (that is, <code class="literal">20σ</code>). This square region is then split into <code class="literal">4x4</code> smaller square subregions. For each subregion, the kernel responses (<code class="literal">dx</code> and <code class="literal">dy</code>) are computed at <code class="literal">5x5</code> regularly-spaced locations (with the kernel size being <code class="literal">2σ</code>). All of these responses are summed up as follows in order to extract four descriptor values for each subregion:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_09_008-1.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Since there are <code class="literal">4x4=16</code> subregions, we have a total of <code class="literal">64</code> descriptor values. Note that in order to give more importance to the neighboring pixels, that is, values closer to the keypoint, the kernel responses are weighted by a Gaussian centered at the keypoint location (with <code class="literal">σ=3.3</code>).</p><p class="calibre8">The <code class="literal">dx</code> and <code class="literal">dy</code> responses are also used to estimate the orientation of the feature. These values are computed (with a kernel size of <code class="literal">4σ</code>) within a circular neighborhood of radius <code class="literal">6σ</code> at locations regularly spaced by intervals of <code class="literal">σ</code>. For a given orientation, the responses inside a certain angular interval (<code class="literal">π/3</code>) are summed, and the orientation giving the longest vector is defined as the dominant orientation.</p><p class="calibre8">SIFT is a richer descriptor that uses an image gradient instead of simple intensity differences. It also splits the square neighborhood around each keypoint into <code class="literal">4x4</code> subregions (it is also possible to use <code class="literal">8x8</code> or <code class="literal">2x2</code> subregions). Inside each of these regions, a histogram of gradient orientations is built. The orientations are discretized into <code class="literal">8</code> bins, and each gradient orientation entry is incremented by a value proportional to the gradient magnitude.</p><p class="calibre8">This is illustrated by the following figure, inside which each star-shaped arrow set represents a local histogram of gradient orientations:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/B05388_09_05.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These <code class="literal">16</code> histograms of <code class="literal">8</code> bins each concatenated together then produce a descriptor of <code class="literal">128</code> dimensions. Note that as for SURF, the gradient values are weighted by a Gaussian filter centered at the keypoint location in order to make the descriptor less sensitive to sudden changes in gradient orientations at the perimeter of the defined neighborhood. The final descriptor is then normalized to make the distance measurement more consistent.</p><p class="calibre8">With SURF and SIFT features and descriptors, scale-invariant matching can be achieved. Here is an example that shows the SURF match result for two images at different scales (here, the <code class="literal">50</code> best matches have been displayed):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/B05388_09_06.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that the <code class="literal">cv::Feature2D</code> class includes a convenient member function that detects the interest points and compute their descriptors at the same time, for example:</p><pre class="programlisting">    ptrFeature2D-&gt;detectAndCompute(image, cv::noArray(),  
                                   keypoints, descriptors); 
</pre></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec176" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The match result produced by any matching algorithm always contains a significant number of incorrect matches. In order to improve the quality of the match set, there exist a number of strategies. Three of them are discussed here.</p><div class="calibre1" title="Cross-checking matches"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch09lvl3sec37" class="calibre6"/>Cross-checking matches</h3></div></div></div><p class="calibre8">A simple approach to validating the matches obtained is to repeat the same procedure a second time, but this time, each keypoint of the second image is compared with all the keypoints of the first image. A match is considered valid only if we obtain the same pair of keypoints in both directions (that is, each keypoint is the best match of the other). The <code class="literal">cv::BFMatcher</code> function gives the option to use this strategy. It is indeed included as a flag; when set to true, it forces the function to perform the reciprocal match cross-check:</p><pre class="programlisting">    cv::BFMatcher matcher2(cv::NORM_L2,    // distance measure 
                           true);          // cross-check flag 
</pre><p class="calibre8">The improved match results are as shown in the following image (in the case of SURF):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Cross-checking matches" src="graphics/B05388_09_07.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="The ratio test"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch09lvl3sec38" class="calibre6"/>The ratio test</h3></div></div></div><p class="calibre8">We have already noted that repetitive elements in scene objects create unreliable results because of the ambiguity in matching visually similar structures. What happens in such cases is that a keypoint will match well with more than one other keypoint. Since the probability of selecting the wrong correspondence is high, it might be preferable to reject a match in these ambiguous cases.</p><p class="calibre8">To use this strategy, we then need to find the best two matching points of each keypoint. This can be done by using the <code class="literal">knnMatch</code> method of the <code class="literal">cv::DescriptorMatcher</code> class. Since we want only two best matches, we specify <code class="literal">k=2</code>:</p><pre class="programlisting">    // find the best two matches of each keypoint 
    std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt; matches; 
    matcher.knnMatch(descriptors1,descriptors2,  
                     matches, 2);  // find the k best matches 
</pre><p class="calibre8">The next step is to reject all the best matches with a matching distance similar to that of their second best match. Since <code class="literal">knnMatch</code> produces a <code class="literal">std::vector</code> class of <code class="literal">std::vector</code> (this second vector is of size <code class="literal">k</code>), we do this by looping over each keypoint match and perform a ratio test, that is, computing the ratio of the second best distance over the best distance (this ratio will be one if the two best distances are equal). All matches that have a high ratio are judged ambiguous and are therefore rejected. Here is how we can do it:</p><pre class="programlisting">    //perform ratio test 
    double ratio= 0.85; 
    std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt;::iterator it; 
    for (it= matches.begin(); it!= matches.end(); ++it) { 
 
      // first best match/second best match 
      if ((*it)[0].distance/(*it)[1].distance &lt; ratio) { 
        // it is an acceptable match 
        newMatches.push_back((*it)[0]); 
      } 
    } 
    // newMatches is the updated match set 
</pre><p class="calibre8">The initial match set made up of <code class="literal">74</code> pairs is now reduced to <code class="literal">23</code> pairs:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="The ratio test" src="graphics/B05388_09_08.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="Distance thresholding"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch09lvl3sec39" class="calibre6"/>Distance thresholding</h3></div></div></div><p class="calibre8">An even simpler strategy consists of rejecting matches for which the distance between their descriptors is too high. This is done using the <code class="literal">radiusMatch</code> method of the <code class="literal">cv::DescriptorMatcher</code> class:</p><pre class="programlisting">    // radius match 
    float maxDist= 0.4; 
    std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt; matches2; 
    matcher.radiusMatch(descriptors1, descriptors2, matches2, maxDist); 
                       // maximum acceptable distance 
                       // between the 2 descriptors 
</pre><p class="calibre8">The result is again a <code class="literal">std::vector</code> instance of <code class="literal">std::vector</code> because the method will retain all the matches with a distance smaller than the specified threshold. This means that a given keypoint might have more than one matching point in the other image. Conversely, other keypoints will not have any matches associated with them (the corresponding inner <code class="literal">std::vector</code> class will then have a size of <code class="literal">0</code>). For our example, the result is a match set of <code class="literal">50</code> pairs:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Distance thresholding" src="graphics/B05388_09_09.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Obviously, you can combine all these strategies in order to improve your matching results.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec177" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Detecting scale-invariant features</em></span> recipe in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <span><em class="calibre16">Detecting Interest Points</em></span>, presents the associated SURF and SIFT feature detectors and provides more references on the subject</li><li class="listitem">The <span><em class="calibre16">Matching images using random sample consensus</em></span> recipe in <a href="ch10.html" title="Chapter 10. Estimating Projective Relations in Images">
Chapter 10
</a>, <span><em class="calibre16">Estimating Projective Relations in Images</em></span>, explains how to use the image and the scene geometry in order to obtain a match set of even better quality</li><li class="listitem">The <span><em class="calibre16">Detecting objects and peoples with Support Vector Machines and histograms of oriented gradients</em></span> recipe in <a href="ch14.html" title="Chapter 14. Learning from Examples">
Chapter 14
</a>, <span><em class="calibre16">Learning from Examples</em></span>, describes the HOG, another descriptor similar to SIFT</li><li class="listitem">The article <span><em class="calibre16">Matching feature points in stereo pairs: A comparative study of some matching strategies</em></span> by <span><em class="calibre16">E. Vincent</em></span> and <span><em class="calibre16">R. Laganière</em></span> in <span><em class="calibre16">Machine, Graphics and Vision</em></span>, pp. 237-260, 2001, describes other simple matching strategies that could be used to improve the quality of the match set</li></ul></div></div></div>
<div class="calibre1" title="Matching keypoints with binary descriptors"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch09lvl1sec60" class="calibre6"/>Matching keypoints with binary descriptors</h1></div></div></div><p class="calibre8">In the previous recipe, we learned how to describe a keypoint using rich descriptors extracted from the image intensity gradient. These descriptors are floating-point vectors that have a dimension of <code class="literal">64</code>, <code class="literal">128</code>, or sometimes even longer. This makes them costly to manipulate. In order to reduce the memory and computational load associated with these descriptors, the idea of using descriptors composed of a simple sequence of bits (0s and 1s) has been introduced. The challenge here is to make them easy to compute and yet keep them robust to scene and viewpoint changes. This recipe describes some of these binary descriptors. In particular, we will look at the ORB and BRISK descriptors for which we presented their associated feature point detectors in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <span><em class="calibre16">Detecting Interest Points</em></span>.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec178" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Thanks to the common interface of the OpenCV detectors and descriptors, using a binary descriptor such as ORB is no different from using descriptors such as SURF and SIFT. The complete feature-based image matching sequence is then as follows:</p><pre class="programlisting">    // Define keypoint vectors and descriptors 
    std::vector&lt;cv::KeyPoint&gt; keypoints1; 
    std::vector&lt;cv::KeyPoint&gt; keypoints2; 
    cv::Mat descriptors1; 
    cv::Mat descriptors2; 
 
    // Define feature detector/descriptor 
    // Construct the ORB feature object 
    cv::Ptr&lt;cv::Feature2D&gt; feature = cv::ORB::create(60); 
                           // approx. 60 feature points 
 
    // Keypoint detection and description 
    // Detect the ORB features 
    feature-&gt;detectAndCompute(image1, cv::noArray(),  
                              keypoints1, descriptors1); 
    feature-&gt;detectAndCompute(image2, cv::noArray(),  
                              keypoints2, descriptors2); 
 
    // Construction of the matcher  
    cv::BFMatcher matcher(cv::NORM_HAMMING); // always use hamming norm 
    // for binary descriptors 
    // Match the two image descriptors 
    std::vector&lt;cv::DMatch&gt; matches; 
    matcher.match(descriptors1, descriptors2, matches); 
</pre><p class="calibre8">The only difference resides in the use of the Hamming norm (the <code class="literal">cv::NORM_HAMMING</code> flag), which measures the distance between two binary descriptors by counting the number of bits that are dissimilar. On many processors, this operation is efficiently implemented by using an exclusive OR operation, followed by a simple bit count. The matching results are the following:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B05388_09_10.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Similar results will be obtained with another popular binary feature detector/descriptor: BRISK. In this case, the <code class="literal">cv::Feature2D</code> instance is created by the <code class="literal">cv::BRISK::create</code> call. As we learned in the previous chapter, its first parameter is a threshold that controls the number of detected points.</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec179" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The ORB algorithm detects oriented feature points at multiple scales. Based on this result, the ORB descriptor extracts a representation of each keypoint by using simple intensity comparisons. In fact, ORB builds on a previously proposed descriptor called BRIEF. This later creates a binary descriptor by simply selecting a random pair of points inside a defined neighborhood around the keypoint. The intensity values of the two pixel points are then compared, and if the first point has a higher intensity, then the value <code class="literal">1</code> is assigned to the corresponding descriptor bit value. Otherwise, the value <code class="literal">0</code> is assigned. Repeating this test on a number of random pairs generates a descriptor that is made up of several bits; typically, <code class="literal">128</code> to <code class="literal">512</code> bits (pairwise tests) are used.</p><p class="calibre8">This is the scheme used by ORB. Then, the decision to be made is which set of point pairs should be used to build the descriptor. Indeed, even if the point pairs are randomly chosen, once they have been selected, the same set of binary tests must be performed to build the descriptor of all the keypoints in order to ensure consistency of the results. To make the descriptor more distinctive, intuition tells us that some choices must be better than others. Also, the fact that the orientation of each keypoint is known introduces some bias in the intensity pattern distribution when this one is normalized with respect to this orientation (that is, when the point coordinates are given relative to this keypoint orientation). From these considerations and the experimental validation, ORB has identified a set of <code class="literal">256</code> point pairs with high variance and minimal pairwise correlation. In other words, the selected binary tests are the ones that have an equal chance of being <code class="literal">0</code> or <code class="literal">1</code> over a variety of keypoints and also those that are as independent from each other as possible.</p><p class="calibre8">The descriptor of BRISK is very similar. It is also based on pairwise intensity comparisons with two differences. First, instead of randomly selecting the points from the<code class="literal"> 31x31</code> points of the neighborhood, the chosen points are selected from a sampling pattern of a set of concentric circles (made up of <code class="literal">60</code> points) with locations that are equally spaced. Second, the intensity at each of these sample points is a Gaussian-smoothed value with a <code class="literal">σ</code> value proportional to the distance from the central keypoint. From these points, BRISK selects <code class="literal">512</code> point pairs.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec180" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Several other binary descriptors exist, and interested readers should take a look at the scientific literature to learn more on this subject. Since it is also available in the OpenCV contrib module, we will describe one additional descriptor here.</p><div class="calibre1" title="FREAK"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch09lvl3sec40" class="calibre6"/>FREAK</h3></div></div></div><p class="calibre8">
<span><strong class="calibre15">FREAK</strong></span> stands for <span><strong class="calibre15">Fast Retina Keypoint</strong></span>. This is also a binary descriptor, but it does not have an associated detector. It can be applied on any set of keypoints detected, for example, SIFT, SURF, or ORB.</p><p class="calibre8">Like BRISK, the FREAK descriptor is also based on a sampling pattern defined on concentric circles. However, to design their descriptor, the authors used an analogy of the human eye. They observed that on the retina, the density of the ganglion cells decreases as the distance to the fovea increase. Consequently, they built a sampling pattern made of <code class="literal">43</code> points in which the density of a point is much greater near the central point. To obtain its intensity, each point is filtered with a Gaussian kernel that has a size that also increases with the distance to the center.</p><p class="calibre8">In order to identify the pairwise comparisons that should be performed, an experimental validation has been performed by following a strategy similar to the one used for ORB. By analyzing several thousands of keypoints, the binary tests with the highest variance and lowest correlation are retained, resulting in <code class="literal">512</code> pairs.</p><p class="calibre8">FREAK also introduced the idea of performing the descriptor comparisons in cascade. That is, the first <code class="literal">128</code> bits representing coarser information (corresponding to the tests performed at the periphery on larger Gaussian kernels) are performed first. Only if the compared descriptors pass this initial step will the remaining tests be performed.</p><p class="calibre8">Using the keypoints detected with ORB, we extract the FREAK descriptors by simply creating the <code class="literal">cv::DescriptorExtractor</code> instance, as follows:</p><pre class="programlisting">    // to describe with FREAK  
    feature = cv::xfeatures2d::FREAK::create(); 
</pre><p class="calibre8">The match result is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="FREAK" src="graphics/B05388_09_11.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The following figure illustrates the sampling pattern used for the three descriptors presented in this recipe:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="FREAK" src="graphics/B05388_09_12.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The first square is the ORB/BRIEF descriptor in which point pairs are randomly selected on a square grid. Each pair of points linked by a line represents a possible test to compare the two pixel intensities. Here, we show only eight such pairs; the default ORB uses <code class="literal">256</code> pairs. The middle square corresponds to the BRISK sampling pattern. Points are uniformly sampled on the circles shown (for clarity, we only identify the points on the first circle here). Finally, the third square shows the log-polar sampling grid of FREAK. While BRISK has a uniform distribution of points, FREAK has a higher density of points closer to the center. For example, in BRISK, you find <code class="literal">20</code> points on the outer circle, while in the case of FREAK, its outer circle includes only six points.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch09lvl2sec181" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Detecting FAST features at multiple scales</em></span> recipe in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <span><em class="calibre16">Detecting Interest Points</em></span>, presents the associated BRISK and ORB feature detectors and provides more references on the subject</li><li class="listitem">The <span><em class="calibre16">BRIEF: Computing a Local Binary Descriptor Very Fast</em></span> article by <span><em class="calibre16">E. M. Calonder</em></span>, <span><em class="calibre16">V. Lepetit</em></span>, <span><em class="calibre16">M. Ozuysal</em></span>, <span><em class="calibre16">T. Trzcinski</em></span>, <span><em class="calibre16">C. Strecha</em></span>, and <span><em class="calibre16">P. Fua</em></span> in <span><em class="calibre16">IEEE Transactions on Pattern Analysis and Machine Intelligence</em></span>, 2012, describes the BRIEF feature descriptor that inspires the presented binary descriptors</li><li class="listitem">The <span><em class="calibre16">FREAK: Fast Retina Keypoint</em></span> article by <span><em class="calibre16">A. Alahi</em></span>, <span><em class="calibre16">R. Ortiz</em></span>, and <span><em class="calibre16">P. Vandergheynst</em></span> in <span><em class="calibre16">IEEE Conference on Computer Vision and Pattern Recognition</em></span>, 2012, describes the FREAK feature descriptor</li></ul></div></div></div></body></html>