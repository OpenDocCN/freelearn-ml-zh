- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MATLAB Tools for Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommender system is a model that’s designed to anticipate the preferences
    of a specific user. When applied to the domain of movies, it transforms into a
    movie recommendation engine. The process involves filtering items in a database
    by predicting the user’s potential ratings and facilitating the connection of
    users with the most suitable content in the dataset. This holds significance because,
    in extensive catalogs, users might not discover all pertinent content. Effective
    recommendations enhance content consumption and major platforms such as Netflix
    heavily depend on them to maintain user engagement. In this chapter, we will learn
    the basic concepts of recommender systems and how to build a **network intrusion
    detection system** (**NIDS**) using MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the basic concepts of recommender systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding similar users in data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating recommender systems for network intrusion detection using MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic machine learning concepts. To understand
    these topics, a basic knowledge of algebra and mathematical modeling is needed.
    You will also required working knowledge of MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you’ll need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`CreditCardData.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CreditCardFraudDet.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NDISdata.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NDISEnsemble.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the basic concepts of recommender systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **recommender system** is a type of information filtering system that’s designed
    to suggest items or content to users based on their preferences, historical behavior,
    or other relevant factors. These systems are widely used in various online platforms
    to help users discover products, services, content, and more. Recommender systems
    involve two primary entities: **users** and **items**. Users are individuals for
    whom recommendations are generated, and items are the products, content, or services
    to be recommended. These items can include movies, books, products, news articles,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Recommender systems rely on data that captures the interaction between users
    and items. This interaction data can include user ratings, purchase history, clicks,
    views, likes, and any other form of user engagement with items.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of recommender systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative filtering** (**CF**): CF methods make recommendations based
    on the preferences and behavior of other users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content-based filtering**: This approach recommends items to users based
    on the attributes of the items and the user’s historical preferences. It focuses
    on the content and descriptions of items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid recommender systems**: These systems integrate various recommendation
    techniques to furnish recommendations that are both more accurate and diverse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CF is a popular technique that’s used in recommender systems to make personalized
    recommendations to users based on their interactions and behaviors, as well as
    the behaviors of similar users. CF assumes that users who have interacted with
    items in a similar way in the past will have similar preferences in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also two main types of CF:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-based**: This type of CF recommends items to a user based on the preferences
    and behaviors of users who are like them. Compute a similarity score between the
    target user and all other users in the system. Common similarity metrics include
    cosine similarity or Pearson correlation. Identify a set of neighbor users who
    are most like the target user. Recommend items that the target user’s neighbors
    have interacted with, but the target user has not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Item-based**: This type of CF recommends items to a user based on the similarity
    of items they have interacted with in the past. It calculates the similarity between
    all pairs of items in the system based on user interactions. Common similarity
    metrics include cosine similarity and the Jaccard index. For a target user, it
    can identify the items they have interacted with. It can recommend items that
    are like the items the user has interacted with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CF matrices are often sparse because users interact with only a small fraction
    of available items. Techniques such as matrix factorization can help address this
    issue by finding latent factors in the data. As the number of users and items
    grows, the computation of user-user or item-item similarity becomes more computationally
    expensive. Optimizations are required for scalability. CF can struggle to make
    recommendations for new users or items with little to no interaction history.
    Techniques such as content-based recommendations or hybrid models are often used
    to address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: CF often relies on user behavior data, which raises privacy concerns. Privacy-preserving
    methods such as differential privacy may be employed to protect user information.
    Various metrics, including **mean absolute error** (**MAE**), **root mean squared
    error** (**RMSE**), and ranking-based metrics such as precision and recall, are
    used to evaluate the performance of CF algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: CF has been widely used in recommendation systems in various domains, including
    e-commerce, movie and music recommendations, social networks, and more. While
    it is effective at capturing user preferences, it does have limitations, such
    as the cold start problem and the need for a sufficient volume of user interactions.
    Hybrid approaches that combine CF with other recommendation techniques can overcome
    some of these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Content-based filtering** is a recommendation technique that’s used in recommender
    systems to provide personalized recommendations to users based on the characteristics
    and features of the items and the user’s preferences. Unlike CF, which relies
    on user-item interactions, content-based filtering focuses on the content of items
    and attempts to match them with user profiles.'
  prefs: []
  type: TYPE_NORMAL
- en: Each item in the system is described by a set of features or attributes. These
    features can vary widely based on the domain but may include things such as genres,
    keywords, actors, directors (for movies), authors (for books), and more. For text-based
    content, natural language processing techniques may be used to extract keywords
    or topics. The system maintains a user profile or preference vector for each user,
    which reflects their preferences for different features or attributes. This user
    profile is built based on the items the user has interacted with or explicitly
    rated.
  prefs: []
  type: TYPE_NORMAL
- en: To generate recommendations for a user, the system calculates a similarity score
    between the user’s profile and the features of items. The similarity score is
    often computed using techniques such as cosine similarity, **Term Frequency-Inverse
    Document Frequency** (**TF-IDF**), or other distance metrics. Items with the highest
    similarity scores are recommended to the user. These are the items that are most
    in line with the user’s historical preferences and interests.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering is effective at providing personalized recommendations
    because it considers the individual user’s preferences based on item features.
    It doesn’t rely on user-to-user or item-to-item comparisons, making it suitable
    for new or less active users who might not have much interaction history. The
    quality of recommendations heavily depends on the accuracy and richness of item
    descriptions and features. Note that ensuring high-quality metadata is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: While content-based filtering is good at recommending items such as those a
    user has interacted with before, it may not introduce as much serendipity or novelty
    in recommendations compared to CF. It can also help address the cold start problem
    for new items if there is sufficient information available about their features.
    Many recommender systems combine content-based filtering with CF or other recommendation
    techniques to improve recommendation quality and address the limitations of each
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering is commonly used in various domains such as news recommendation,
    music recommendation, and e-commerce for suggesting products. When implemented
    correctly, it can offer valuable recommendations tailored to individual user preferences
    and needs.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid recommender systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hybrid recommender systems are recommendation systems that combine multiple
    recommendation techniques to provide more accurate and diverse recommendations.
    These systems aim to leverage the strengths of different recommendation approaches,
    such as CF, content-based filtering, and more, while mitigating their weaknesses.
    Hybrid recommender systems are commonly used in various applications to improve
    recommendation quality and address the limitations of individual methods.
  prefs: []
  type: TYPE_NORMAL
- en: In weighted hybrid systems, different recommendation techniques are assigned
    weights that determine their influence on the final recommendations. For example,
    you might assign a higher weight to CF if historical user interactions are more
    critical, and a lower weight to content-based filtering for item features. Recommendations
    are generated by combining the scores from each technique, weighted by their importance.
  prefs: []
  type: TYPE_NORMAL
- en: In switching hybrid systems, the recommendation approach is dynamically chosen
    based on certain conditions or user characteristics. For instance, if a user has
    a substantial interaction history, CF might be used, but if they are a new user
    with minimal history, content-based filtering might be employed. In this approach,
    the features or scores generated by different recommendation techniques are combined
    to create a unified feature vector for items or users. This combined feature vector
    is then used to generate recommendations using any single recommendation method.
  prefs: []
  type: TYPE_NORMAL
- en: Cascade hybrids use the output of one recommendation technique as input to another.
    For example, content-based filtering may be used to generate an initial set of
    recommendations. These recommendations can be further refined using CF to improve
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In meta-level hybrid systems, different recommendation methods are applied independently,
    and their outputs are combined using a meta-learner or meta-classifier. The meta-learner
    takes the outputs of individual recommendation methods as inputs and provides
    the final recommendations. Machine learning algorithms such as decision trees,
    neural networks, or ensemble methods such as stacking can be used as meta-learners.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of data analysis and user profiling, the process of finding similar
    users holds immense significance. Let’s delve into the techniques and methodologies
    that are employed for identifying and categorizing users with similar patterns
    and behaviors within a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finding similar users in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fraud has consistently been a pervasive issue in various forms, but the emergence
    of new technological tools, such as **virtual intelligence** (**VI**), has expanded
    the avenues for fraudulent activities. In today’s world, the use of credit and
    debit cards has become the standard for making purchases, and as a result, fraud
    associated with these payment methods is on the rise. The repercussions of such
    fraud extend beyond impacting just merchants and banks, who are often left shouldering
    the financial burden.
  prefs: []
  type: TYPE_NORMAL
- en: When a customer falls victim to fraud, they may find themselves burdened with
    higher interest rates imposed by the bank as they could be categorized as a higher
    risk profile. Additionally, fraud incidents can tarnish a merchant’s reputation
    and image. If a customer experiences fraud during a transaction, it can erode
    their trust in the seller, potentially driving them to seek alternatives from
    competitors for future purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of credit card transaction data, fraud recognition is the process
    of identifying whether a new transaction belongs to the class of fraudulent or
    legitimate transactions. Such a system should not only detect fraudulent transactions
    but should do so in a cost-effective manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining a transaction with the goal of classification can be carried out
    through distinct methods, and two specific approaches are suggested:'
  prefs: []
  type: TYPE_NORMAL
- en: User-level analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single transaction analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two approaches represent unique strategies, each having its own set of
    advantages and limitations. Each approach entails certain assumptions, varying
    in their degree of validity. Regardless, both approaches can yield valuable outcomes.
    There are situations where the choice between these methods is determined by the
    nature of the available data as it may not permit an alternative approach.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing transactions at the individual transaction level refers to an approach
    where the classification of a transaction is determined by its relationship with
    all the transactions within the dataset. Various machine learning algorithms can
    be employed for this purpose. For instance, consider the **k-nearest neighbors**
    (**k-NN**) algorithm, which classifies a transaction based on its similarity to
    other transactions in the dataset. If a transaction closely resembles known fraudulent
    transactions, there’s a likelihood it will be categorized as fraudulent, and conversely,
    it would be deemed a legitimate transaction. This mechanism facilitates the identification
    of data patterns by leveraging similarities among objects of the same class.
  prefs: []
  type: TYPE_NORMAL
- en: In a nearest neighbors algorithm, a record is categorized by examining all the
    data in the training set and assigning it the same class as the nearest element.
    The underlying assumption is that in a multidimensional space, if two records
    are “close,” they likely belong to the same class. To gauge this proximity, it’s
    important to employ a distance metric. One example is the Euclidean distance or,
    more broadly, the Minkowski metric, as we introduced in [*Chapter 4*](B21156_04.xhtml#_idTextAnchor084),
    *Clustering Analysis and* *Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The nearest neighbor rule results in a space division known as **Voronoi tessellation**.
    Each element in the training set delineates a region in which patterns will be
    classified into the same class. To enhance the robustness of this mechanism, one
    approach is to classify a record by considering the k closest records. The record
    can then be assigned to the class that has the largest representation among the
    selected examples. To reduce sensitivity to the choice of k, each record can contribute
    to the classification based on a weighted scheme determined by its distance from
    the element to be classified. For this process to work effectively, the attributes
    must possess consistent value scales, which typically necessitates prior normalization
    during the data preprocessing phase. Let’s learn how to build a credit card fraud
    detection system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we will start by importing the data into the MATLAB workspace. For
    this, we will utilize the credit card fraud detection dataset, which comprises
    anonymized credit card transactions classified as fraudulent or legitimate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset exclusively consists of numerical input variables, derived from
    a PCA transformation. Among the features, `V1` to `V28` represent the principal
    components obtained through PCA. The `Class` feature serves as the response variable
    and holds a value of `1` in cases of fraud and `0` otherwise. To understand how
    the data is distributed, we can count the occurrences in the `class` column:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `groupcounts()` function provides the distinct combinations of grouping
    variables for the table or timetable denoted as `T`. It also offers the count
    of members within each group and the corresponding data percentage, ranging from
    `0` to `100`. Groups are established based on rows in the variables contained
    within `groupvars` that share identical unique value combinations. Each entry
    in the result table corresponds to an individual group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dataset is unbalanced on one of the binary classes, we should take this
    into account when evaluating the results. Classifying imbalanced data is a common
    challenge in machine learning when one class significantly outnumbers the other(s).
    The primary class (majority class) often dominates, making it challenging for
    the model to correctly predict the minority class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To check how the data is distributed, we can draw a boxplot of the features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following graph will be output (*Figure 10**.1*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Boxplot of the features](img/B21156_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Boxplot of the features
  prefs: []
  type: TYPE_NORMAL
- en: From the analysis of *Figure 10**.1*, we can see that the variables are skewed.
    We can also see that some of them present potential outliers. It is recommended
    to conduct data scaling. Keep in mind that it’s a best practice to standardize
    or normalize the data before training a machine learning model. Through scaling,
    the data’s units are standardized, making it simpler to compare data from various
    sources or locations.
  prefs: []
  type: TYPE_NORMAL
- en: Data scaling, also known as data normalization or standardization, is a preprocessing
    technique in machine learning and data analysis that involves transforming the
    data into a standardized range or distribution. The primary purpose of data scaling
    is to make different features or variables in your dataset comparable and to help
    machine learning algorithms perform better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The choice between scaling and standardization depends on the specific requirements
    of your dataset and the machine learning algorithm you plan to use. In general,
    it is advisable to apply data scaling to your features to prevent certain variables
    from dominating the learning process, especially in algorithms that are sensitive
    to feature scales, such as kNN or support vector machines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data scaling helps in improving the convergence of optimization algorithms,
    makes features more interpretable, and can also enhance the performance of some
    machine learning models. However, it’s important to note that not all algorithms
    require data scaling, and there are cases where the natural scale of the data
    is meaningful and should not be altered. The decision to scale the data should
    be made with careful consideration of your specific problem and the characteristics
    of your dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will perform a standardization with a range of `-1` to `1`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Standardization with a range of `-1` to `1`, also known as min-max scaling with
    a specific range, is a method of data scaling that transforms your data to fit
    within the range of `-1` to `1`. This approach is useful when you want to normalize
    your data while maintaining the possibility of negative values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To check how the data is distributed after we performed data scaling, we can
    draw a boxplot of the features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following graph is shown (*Figure 10**.2*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Boxplot of the scaled data](img/B21156_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Boxplot of the scaled data
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is clear that the data has been scaled so that we have the same existence
    intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training the model based on machine learning, it is necessary to carry
    out data splitting. Data splitting refers to the process of dividing a dataset
    into separate subsets for training, testing, and validation purposes. We will
    perform train-test splitting, which involves dividing the dataset into two parts,
    typically with a 70-30 or 80-20 split. The larger portion is used for training
    the model, while the smaller portion is used for testing its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We initiated the process by obtaining the number of observations within our
    dataset using the `length()` function. This function returns the size of the largest
    dimension in the array, `X`. In the context of vectors, this size corresponds
    to the total number of elements. Subsequently, we employed the `cvpartition()`
    function to create a random partition for the dataset. This partition serves as
    the foundation for constructing essential training and test subsets, which are
    instrumental in evaluating a statistical model. To extract the training data index
    and the test data index from the original dataset, we utilized the `training`
    and `test` object functions. These indices were then applied to extract the corresponding
    data subsets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'k-NN is a versatile algorithm that’s used not only for classification and regression
    tasks but also in recommender systems. It can be adapted for building CF-based
    recommender systems. We will use the `fitcknn()` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following arguments were passed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`TrainData(:,1:28)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainData(:,29)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cosine` (the expression represents the complement of the cosine of the angle
    included between observations, treating them as vectors):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Minkowski distance exponent
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of nearest neighbors to find: `10`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance weighting function: `SquaredInverse` (`Weight` is 1/distance)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We applied weighted k-NN, a variation of the k-NN algorithm that assigns different
    weights to the neighbors when making predictions or classifications. In traditional
    k-NN, each neighbor has an equal influence on the final decision, but in weighted
    k-NN, the influence of each neighbor is adjusted based on certain factors, typically
    the proximity or similarity between the neighbors and the query point. This allows
    for more accurate and context-aware predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We start by choosing a distance metric to measure the similarity or dissimilarity
    between data points. We used the `cosine` metric, also known as cosine similarity,
    which is a similarity measure that’s used to determine the similarity between
    two non-zero vectors in a high-dimensional space. It is widely used in various
    fields, including information retrieval, natural language processing, and recommendation
    systems. The `cosine` metric measures the cosine of the angle between two vectors,
    and it provides a value between -1 and 1, where 1 indicates that the vectors are
    identical, 0 means that they are orthogonal (completely dissimilar), and -1 implies
    they are opposed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The algorithm then determines the value of *k*, which represents the number
    of nearest neighbors to consider when making a prediction. The choice of k depends
    on your dataset and the specific problem you’re trying to solve. You must compute
    the distances between the query point (the point you want to classify or predict)
    and all other data points in your dataset, then calculate weights for each neighbor
    based on their proximity or similarity to the query point. Common methods for
    assigning weights include inverse distance, where closer neighbors have higher
    weights, or similarity-based weighting, where more similar neighbors are given
    higher weights. You can also choose the k neighbors with the highest weights.
    These neighbors will have a more significant impact on the final prediction. For
    classification tasks, assign class labels to the query point based on the majority
    class among the selected neighbors, with the weights influencing the voting process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the advantages of using weighted k-NN:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Weighted k-NN can provide more accurate predictions because it considers the
    influence of each neighbor on the final decision
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows for better handling of imbalanced datasets, where some neighbors may
    be more informative than others
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted k-NN is particularly useful when the neighbors are not equally informative
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted k-NN is a flexible and widely used machine learning technique that
    can be applied to a variety of problems, including classification, regression,
    and recommendation systems. It allows you to adapt the algorithm to the specific
    characteristics of your data and the problem you are trying to solve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After training the model, we must evaluate its performance. We will start by
    using the trained model to predict data labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we employed the `predict()` function, which furnishes the anticipated
    response values produced by the generalized linear regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can assess the model’s accuracy, which gauges the extent to which a
    predictive model’s forecasts match the real or observed values. It serves as an
    indicator of the model’s performance in accurately predicting outcomes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is excellent, confirming that the choice of the algorithm and training
    parameters were correct.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the ever-evolving landscape of cybersecurity, the development of effective
    tools for network intrusion detection is paramount. The next section will explore
    how to utilize MATLAB to create advanced recommender systems tailored for enhancing
    network intrusion detection capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Creating recommender systems for network intrusion detection using MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A NIDS serves as a security mechanism that’s employed to identify and prevent
    unauthorized access, malicious activities, and potential threats within a computer
    network. It involves monitoring network traffic and analyzing it to identify any
    suspicious or abnormal behaviors. The main objective of network intrusion detection
    is to protect the network from various types of attacks, such as **denial-of-service**
    (**DoS**) attacks, malware infections, data leakage, unauthorized access, and
    other cyber threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two primary methods of network intrusion detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Signature-based detection**: This method involves comparing network traffic
    patterns with a database of known signatures or patterns of known attacks. If
    a match is found, an alert is generated to notify the network administrator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly-based detection**: This method focuses on identifying abnormal or
    suspicious network behavior that deviates from the normal patterns. It uses machine
    learning algorithms to analyze network traffic and detect any anomalies that could
    potentially indicate an ongoing or imminent attack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A NIDS can be implemented at different levels within a network architecture,
    such as at the perimeter, on individual hosts, or within specific network segments.
    They collect and analyze network packets, logs, and other network data to identify
    and alert the system administrators about potential intrusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some common techniques that are used in network intrusion
    detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Packet analysis**: For examining individual network packets to identify specific
    attack signatures or anomalies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Protocol analysis**: For analyzing network protocols to detect any abnormal
    or unauthorized activities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic analysis**: For monitoring network traffic patterns to identify any
    sudden spikes or unusual patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behavior analysis**: For analyzing user or host behavior to detect any unusual
    or malicious activities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NIDS play a crucial role in safeguarding computer networks against unauthorized
    access and potential threats. They help identify and respond to security incidents
    promptly, minimizing any potential damage or data breaches.
  prefs: []
  type: TYPE_NORMAL
- en: Recommender system for NIDS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will adopt a new approach to identifying and preventing
    network attacks: network intrusion detection using a recommender system. Traditional
    NIDS rely on fixed rules and signatures to detect known attack patterns. However,
    attackers are continually evolving their techniques, making it challenging for
    these systems to keep up. By incorporating a recommender system into the network
    intrusion detection process, it becomes possible to leverage machine learning
    and data mining techniques to enhance detection capabilities. The primary objective
    is to utilize historical network traffic data to build a model that can predict
    whether a particular network event is normal or malicious.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of how network intrusion detection using a recommender
    system works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: Network traffic data is collected and stored for analysis.
    This data consists of various network parameters, such as IP addresses, port numbers,
    protocols, packet sizes, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: Relevant features are extracted from the collected
    data. These features can include traffic volume, connection duration, packet headers,
    and other characteristics that provide insights into network behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preprocessing**: The gathered data undergoes preprocessing to eliminate
    noise, address missing values, and normalize the features. This step ensures that
    the data is in a suitable format for analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training the recommender system**: The preprocessed data is used to train
    a recommender system algorithm, such as CF, matrix factorization, or association
    rule mining. This algorithm learns the patterns and relationships within the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building a recommendation model**: The trained algorithm generates a recommendation
    model based on the network traffic data. This model can identify the normal network
    behavior and detect any deviations from it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time monitoring**: The recommendation model is then used to monitor
    incoming network traffic in real time. It analyzes the network events and predicts
    whether they are normal or potentially malicious.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alert generation**: When the recommendation model detects a potentially malicious
    network event, it triggers an alert to notify network administrators. The alert
    can include information about the detected attack type and recommended countermeasures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous learning**: The recommendation model can continuously update itself
    over time by incorporating new data and adjusting its detection capabilities.
    This ensures that the system remains effective against emerging threats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, network intrusion detection using a recommender system offers a more
    dynamic and adaptive approach to identifying network attacks. It leverages machine
    learning techniques to learn from historical data and make intelligent predictions
    about the network’s security status. This can enhance the accuracy and efficiency
    of network security operations by reducing false positives and detecting emerging
    attack patterns.
  prefs: []
  type: TYPE_NORMAL
- en: NIDS using a recommender system in MATLAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this practical example, we will build a NIDS-adopting recommender system
    using **ensemble methods**. Ensemble methods are techniques that combine multiple
    individual models to form a more powerful and accurate predictor. These individual
    models, also known as base models or weak models, can be of any type, such as
    decision trees, support vector machines, or neural networks. By combining the
    predictions of these base models, ensemble methods aim to improve the overall
    performance and generalization ability of the model. There are several popular
    ensemble methods, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Bagging, short for **bootstrap aggregating**, involves training
    multiple base models on different subsets of the training data, with replacement.
    The final prediction is made by averaging or voting the predictions of the individual
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: Boosting algorithms train base models sequentially, with each
    subsequent model focusing more on the samples that were previously misclassified.
    The predictions of multiple models are combined using weighted voting or averaging
    to make the final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest**: Random forest is an ensemble method that amalgamates bagging
    and decision trees. Numerous decision trees, each trained on a random subset of
    the features, are integrated through majority voting to formulate the final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive boosting** (**AdaBoost**): AdaBoost is a boosting algorithm that
    assigns weights to the training samples based on their classification errors.
    Weak models are trained iteratively, with each subsequent model focusing more
    on the misclassified samples. The final prediction is made by combining the predictions
    of multiple models using weighted voting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosting**: Gradient boosting is another boosting algorithm that
    sequentially trains weak models, with each subsequent model minimizing the loss
    function using gradient descent. The predictions of multiple models are combined
    using weighted averaging to make the final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods have proven to be effective in improving the performance and
    robustness of predictive models in various domains, including classification,
    regression, and anomaly detection. They are widely used in machine learning and
    data mining applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we start by importing the dataset into the MATLAB environment. We
    will use the Network Intrusion Detection dataset, which is available in the Kaggle
    dataset repository ([https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection](https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection)).
    The dataset contains a diverse range of simulated intrusions in a military network
    setting. The objective was to create a realistic environment resembling a US Air
    Force LAN, which was then exposed to multiple attacks. Each connection in this
    environment represents a sequence of TCP packets, with data flowing between a
    source IP address and a target IP address under a predefined protocol. Every connection
    is classified as either normal or as an attack, with a specific attack type assigned
    to it. A connection record contains approximately 100 bytes of data. For each
    TCP/IP connection, a total of 41 features, both quantitative and qualitative,
    are collected from both normal and attack data. These features include three qualitative
    features and 38 quantitative features. The class variable in the dataset has two
    categories – normal and anomalous:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the dataset into the MATLAB environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function prints a summary of the table, displaying the properties description
    of the variables and some statistics such as min, median, and max for numeric
    features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As we did in the *Finding similar users in data* section, we have to split
    the data into two subsets: train and test. We will use the `cvpartition()` function
    for this, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We began by using the `length()` function to determine the number of observations
    in our dataset. This function gives us the size of the largest dimension in the
    array, `X`, which in the case of vectors represents the total number of elements.
    Next, we utilized the `cvpartition()` function to randomly split the dataset into
    training and test subsets. This partition forms the basis for evaluating a statistical
    model. We extracted the indices for the training and test data using the training
    and test object functions and then used these indices to obtain the respective
    data subsets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, it’s time to train the algorithm for NDIS. To do this, we will use an
    algorithm based on ensemble methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We started by setting the data used for the training and divided the subset
    into predictors and response variables. These terms are used in statistical analysis
    to describe the relationship between independent variables (predictors) and dependent
    variables (response). Predictors are variables that are used to predict, explain,
    or account for the variation in the response variable. They can be continuous
    or categorical and may have various levels or values. The response variable, on
    the other hand, is the outcome or variable that is being predicted or explained
    by the predictors. It is also known as the dependent variable. In our case, it
    is the label of the network intrusion detection classification (anomaly, normal).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As a classification method, we used `AdaBoostM1`. `AdaBoostM1`, also known as
    adaptive boosting, is a machine learning algorithm that can be used for both classification
    and regression problems. It is particularly effective in handling complex datasets
    and improving weak learning algorithms. The fundamental concept behind `AdaBoostM1`
    is to amalgamate multiple weak classifiers to form a robust classifier. A weak
    classifier is a basic model that performs marginally better than random guessing.
    `AdaBoostM1` accomplishes this by iteratively training weak classifiers on varied
    weighted versions of the dataset. The main idea behind `AdaBoostM1` is to combine
    multiple weak classifiers to create a strong classifier. A weak classifier is
    a simple model that performs slightly better than random guessing. `AdaBoostM1`
    achieves this by iteratively training weak classifiers on different weighted versions
    of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In each iteration, the algorithm assigns increased weights to instances misclassified
    in the previous iteration. This compels the weak classifiers to prioritize the
    most challenging instances, enhancing their focus on difficult cases. These weighted
    weak classifiers are then combined using a weighted majority voting scheme to
    make final predictions. In classification tasks, the final prediction is obtained
    by assigning a class label based on the weighted majority vote of the weak classifiers.
    In regression tasks, the final prediction is obtained by averaging the weighted
    predictions of the weak classifiers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`AdaBoostM1` has several advantages. It is robust to overfitting and can handle
    datasets with many features. It can also handle imbalanced datasets by adjusting
    the weights of the instances. Additionally, `AdaBoostM1` can be easily parallelized,
    making it suitable for distributed computing environments. However, `AdaBoostM1`
    may be sensitive to noisy data and outliers, as it assigns higher weights to misclassified
    instances. It also requires careful selection of weak classifiers, as too complex
    or too weak classifiers may not yield good results. Overall, `AdaBoostM1` is a
    powerful algorithm that has been widely used in many applications, including face
    detection, object recognition, and financial forecasting.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s take a look at the model that was trained:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Some information about the model is shown in the preceding code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After training the algorithm, it is time to evaluate its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `predict()` function was utilized to obtain the expected response values
    generated by the generalized linear regression model. By evaluating the accuracy
    of the model, we can measure how well the predictive model’s predictions align
    with the actual observed values. This assessment serves as a performance indicator
    in accurately forecasting outcomes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These are amazing results that demonstrate that ensemble methods are very effective
    in classifying network intrusion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Embarking on the final frontier of the data science journey, the deployment
    of machine learning models marks the critical phase where theoretical prowess
    transforms into real-world impact. The next section delves into the intricacies
    and best practices of deploying machine learning models, ensuring their seamless
    integration into operational environments.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying machine learning models refers to the process of making a trained
    model available for making predictions on new, unseen data. It involves taking
    the trained model and integrating it into a production environment where it can
    receive input data, perform predictions, and return the results. The trained model
    needs to be organized and packaged into a format suitable for deployment. This
    may involve exporting the model into a file format that can be easily loaded and
    used by other systems. An **application programming interface** (**API**) is typically
    created to expose the machine learning model’s functionality. The API acts as
    the interface that other systems or applications can use to send data and receive
    predictions from the model.
  prefs: []
  type: TYPE_NORMAL
- en: If the model is expected to handle many concurrent requests, the deployment
    environment may need to be scaled to accommodate the increased load. This may
    involve setting up clusters of servers or using cloud-based infrastructure. Once
    the model has been deployed, it is important to monitor its performance and behavior
    to ensure it continues to provide accurate predictions. Monitoring can involve
    tracking metrics such as response time, throughput, or error rates to identify
    any issues or performance degradation. Machine learning models often need updates
    or retraining as new data becomes available. Therefore, it is important to have
    processes in place for continuous integration and delivery to easily deploy new
    versions of the model and ensure it stays up to date.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model compression refers to the process of diminishing the size of a machine
    learning model without significantly compromising its performance. The need for
    model compression arises in scenarios where the size or computational requirements
    of a model become critical, such as when deploying models on resource-constrained
    devices such as smartphones or edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several techniques for model compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning**: Pruning involves removing unnecessary connections or parameters
    from the model. Typically, connections with small weights are pruned based on
    a certain threshold, resulting in a sparser model with fewer parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization**: Quantization is the process of reducing the precision of
    the weights and activations in a model. For example, instead of using 32-bit floating-point
    values, weights can be represented using 8-bit integers. This reduces memory requirements
    and improves computational efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge distillation**: Knowledge distillation involves training a smaller,
    “student” model to mimic the predictions of a larger, “teacher” model. The teacher
    model’s knowledge is transferred to the student model, allowing for a compact
    representation of the original model while maintaining performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low-rank approximation**: Low-rank approximation techniques aim to approximate
    the weights of a model using low-rank matrices or tensors. This reduces the number
    of parameters required to represent the model, leading to a smaller memory footprint
    and faster computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compact architectures**: This involves designing or using compact architectures,
    such as MobileNet or SqueezeNet, that are specifically built to be lightweight
    and efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These compression techniques enable models to be deployed on devices with limited
    resources or used in scenarios with strict latency or memory constraints. However,
    it is important to consider trade-offs as model compression may lead to slight
    performance degradation in terms of accuracy or inference speed.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering model pruning techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model pruning is a machine learning technique that aims to decrease the size
    and complexity of a trained model by eliminating unnecessary or redundant parameters,
    connections, or nodes. The objective of model pruning is to enhance the efficiency
    and computational performance of the model without significantly compromising
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several methods of model pruning, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight pruning**: In weight pruning, individual weights in the model are
    set to zero or removed entirely based on their magnitude. This reduces the number
    of parameters and can result in a sparser model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neuron pruning**: Neuron pruning entails the removal of entire neurons from
    the model, guided by their contribution to the overall performance. Neurons with
    low activation or minimal impact on the output are pruned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter pruning**: This technique is commonly used in **convolutional neural
    networks** (**CNNs**). Filters are groups of feature detectors that are applied
    across an image during the convolution operation. Filter pruning involves removing
    unnecessary filters that do not contribute significantly to the model’s accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured pruning**: Structured pruning involves removing entire layers
    or blocks of the model, rather than individual weights or neurons. This method
    often results in more efficient implementations since removing entire layers reduces
    both computational complexity and memory requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model pruning can be performed during training or as a post-training optimization
    step. It is often combined with other techniques such as regularization methods
    or quantization to further improve the efficiency of the pruned model. Pruning
    can provide significant benefits in terms of model size, speed, and memory usage,
    making it a useful technique for deploying models on resource-constrained devices
    or in real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MATLAB, you can perform model pruning using various techniques and tools
    such as Neural Network Toolbox. The following steps outline a general approach
    to model pruning in MATLAB:'
  prefs: []
  type: TYPE_NORMAL
- en: Load or create your initial neural network model using Neural Network Toolbox
    in MATLAB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the neural network model to achieve a reasonably good performance on a
    given task or dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the pruning algorithm or technique of your choice to identify and remove
    unnecessary weights, connections, or nodes from the trained model. Some commonly
    used pruning techniques include magnitude-based pruning, sensitivity-based pruning,
    and weight-decay pruning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the pruned model’s performance by testing it on a validation or test
    dataset. Make sure that the pruning process does not significantly degrade the
    model’s performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune or retrain the pruned model, if necessary, to recover any performance
    degradation due to pruning. Adjust the learning rate or other training parameters
    to optimize the pruned model’s performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *s**teps 3* to *5* until the desired level of model compression or performance
    trade-off is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember that model pruning is a dynamic and iterative process, and it requires
    careful consideration of the trade-off between model size, performance, and computational
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing quantization for efficient inference on edge devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization is a technique that’s used in model compression to reduce the memory
    footprint and computational requirements of deep neural networks. It involves
    converting the weights and activations of a network from floating-point representation
    into lower precision representation, such as 8-bit or even lower.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization reduces the precision of the model, which can lead to a loss in
    model accuracy. However, it has been observed that many neural networks are robust
    to quantization and can still achieve similar performance with reduced precision
    representation. This is especially true for deep neural networks with large numbers
    of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different approaches to quantization in model compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight-only quantization**: In this approach, only the weights of the neural
    network are quantized, while the activations remain in the original precision.
    This reduces the memory requirements significantly as weights typically consume
    the majority of the memory in a neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full quantization**: In this approach, both the weights and activations are
    quantized to lower precision. This provides further reduction in memory requirements
    and computational complexity but can result in a larger loss in model accuracy
    compared to weight-only quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic quantization**: Dynamic quantization techniques adaptively adjust
    the precision of weights and activations during inference based on the input data.
    This allows for more flexibility in the precision used, leading to potential accuracy
    improvements compared to static quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization-aware training**: Instead of quantizing the model after training,
    quantization-aware training incorporates the quantization process during the training
    phase itself. This ensures that the model learns to be robust to quantization
    and results in better accuracy when quantized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization is a widely used technique in model compression and is particularly
    useful when deploying deep neural networks on resource-constrained devices such
    as edge devices or mobile phones. It allows for efficient deployment of models
    with reduced memory requirements and improved computational efficiency, without
    sacrificing significant model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with knowledge distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowledge distillation is a technique that’s used in model compression that
    refers to the process of reducing the size and complexity of a machine learning
    model without significant loss in performance. In knowledge distillation, a large
    “teacher” model is trained on a dataset and used as a reference to train a smaller
    “student” model. The goal is to transfer the knowledge, or the learned representations,
    from the teacher model to the student model.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge transfer is achieved by training the student model to mimic the outputs
    of the teacher model. Typically, this involves using the teacher model’s soft
    targets or logits instead of the hard labels during training. Soft targets refer
    to the probabilities assigned to each class by the teacher model, which can provide
    more nuanced information compared to the one-hot encoded hard labels.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, the student model tries to minimize the difference
    between its predictions and the soft targets provided by the teacher model. This
    enables the student model to learn from the richer information provided by the
    teacher, improving its understanding of the data and increasing its performance.
    Knowledge distillation can lead to significant model compression as the smaller
    student model can capture the knowledge of the larger teacher model, often with
    fewer parameters. Additionally, the student model can be more efficient in terms
    of inference time and resource usage, making it suitable for deployment on resource-constrained
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, knowledge distillation is a technique that’s used in model compression
    to transfer knowledge from a larger teacher model to a smaller student model,
    thereby compressing the model size while maintaining performance.
  prefs: []
  type: TYPE_NORMAL
- en: Learning low-rank approximation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Low-rank approximation is a technique that’s used in model compression to reduce
    the size of a given model. It involves approximating a high-dimensional weight
    matrix or tensor by a lower-rank approximation, which significantly reduces the
    number of parameters needed to represent the model. In low-rank approximation,
    a factorization of the weight matrix is performed to decompose it into two or
    more smaller matrices or tensors. The rank of the approximation determines the
    number of smaller matrices or tensors used. By choosing a lower rank, the resulting
    approximation will have fewer parameters, making it more compact and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank approximation can be applied to various types of models, including
    neural networks, deep learning models, and other machine learning algorithms.
    It is especially useful for reducing the computational and memory requirements
    of large models. This enables their deployment on resource-constrained devices,
    such as mobile phones or embedded systems. One common approach for low-rank approximation
    is **singular value decomposition** (**SVD**), which decomposes a matrix into
    three matrices representing the left singular vectors, singular values, and right
    singular vectors. Selecting a subset of the singular values and their corresponding
    singular vectors allows for the creation of a low-rank approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques for low-rank approximation include Tucker decomposition, which
    decomposes a tensor into smaller tensors, and tensor-train decomposition, which
    represents a tensor as a series of matrix products. These techniques can be applied
    to higher-order tensors typically found in deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, low-rank approximation is a powerful technique for model compression,
    enabling the reduction of model size without sacrificing too much performance.
    It allows for the efficient deployment of models on resource-constrained devices
    and faster inference times.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned the basic concepts of recommender systems, starting
    with the definition of these systems and then understanding how the problem is
    approached. We analyzed the different types of recommender systems: CF, content-based
    filtering, and hybrid recommender systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we saw how to use similarities in the data to identify possible fraudulent
    uses of credit cards. To do this, we trained a model based on the nearest neighbor
    algorithm but using a modified version of the traditional k-NN algorithm, where
    neighbors are given varying weights during the prediction or classification process.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we saw how to implement a NIDS based on ensemble methods in MATLAB. Specifically,
    we adopted an AdaBoost algorithm to identify intrusions in a LAN network.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced the techniques of deploying machine learning models regarding
    model compression. We analyzed the most popular model compression techniques,
    including pruning, quantization, knowledge distillation, and low-rank approximation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn the basic concepts of anomaly detection and
    fault diagnosis systems. We will understand how to identify anomaly functioning
    using deep learning, as well as how to implement a fault diagnosis system in MATLAB.
    Finally, we will discover dropout, L1 and L2 regularization, and early stopping.
  prefs: []
  type: TYPE_NORMAL
