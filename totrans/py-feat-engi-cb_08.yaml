- en: <st c="0">8</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Creating New Features</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="23">Adding new features to a dataset can help machine learning models
    learn patterns and important details in the data.</st> <st c="140">For example,
    in finance, the</st> **<st c="169">disposable income</st>**<st c="186">, which
    is the</st> *<st c="201">total income</st>* <st c="213">minus the</st> *<st c="224">acquired
    debt</st>* <st c="237">for any</st> <st c="245">one month, might be more relevant
    for credit risk than just the income or the acquired debt.</st> <st c="339">Similarly,
    the</st> *<st c="354">total acquired debt</st>* <st c="373">of a person across
    financial products, such as a car loan, a mortgage, and credit cards, might be
    more important to estimate the credit risk than any debt considered individually.</st>
    <st c="554">In these examples, we use domain knowledge to craft new variables,
    and these variables are created by adding or subtracting</st> <st c="678">existing
    features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="696">In some cases, a variable may not have a linear or monotonic relationship
    with the target, but a polynomial combination might.</st> <st c="824">For example,
    if our variable has a quadratic relationship with the target,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mrow></math>](img/36.png)<st
    c="899"><st c="900">, we can convert that into a linear relationship by squaring
    the original variable.</st> <st c="984">We can also help linear models better
    understand the relationships between variables and targets by transforming the
    predictors through splines, or by using</st> <st c="1141">decision trees.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1156">The advantage of crafting additional features to train simpler
    models, such as linear or logistic regression, is that both the features and the
    models remain interpretable.</st> <st c="1330">We can explain the reasons driving
    a model’s output to management, clients, and regulators, adding a layer of transparency
    to our machine learning pipelines.</st> <st c="1488">In addition, simpler models
    tend to be faster to train and easier to deploy</st> <st c="1564">and maintain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1577">In this chapter, we will create new features by transforming or
    combining variables with mathematical functions, splines, and</st> <st c="1704">decision
    trees.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1719">This chapter will cover the</st> <st c="1748">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1766">Combining features with</st> <st c="1791">mathematical functions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1813">Comparing features to</st> <st c="1836">reference variables</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1855">Performing</st> <st c="1867">polynomial expansion</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1887">Combining features with</st> <st c="1912">decision trees</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1926">Creating periodic features from</st> <st c="1959">cyclical variables</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1977">Creating</st> <st c="1987">spline features</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2002">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2025">In this chapter, we will use the</st> `<st c="2059">pandas</st>`<st
    c="2065">,</st> `<st c="2067">numpy</st>`<st c="2072">,</st> `<st c="2074">matplotlib</st>`<st
    c="2084">,</st> `<st c="2086">scikit-learn</st>`<st c="2098">, and</st> `<st c="2104">feature-engine</st>`
    <st c="2118">Python libraries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2136">Combining features with mathematical functions</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2183">New features</st> <st c="2196">can be created by combining existing
    variables with mathematical and statistical functions.</st> <st c="2289">Taking
    an example from</st> <st c="2311">the finance industry, we can calcula</st><st
    c="2348">te the total debt of a person by summing up their debt across individual
    financial products, such as car loan, mortgage, or credit</st> <st c="2480">card
    debt:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="2490">Total debt = car loan debt + credit card debt +</st>* *<st c="2539">mortgage
    debt</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2552">We can also derive other insightful features using alternative
    statistical operations.</st> <st c="2640">For example, we can determine the maximum
    debt of a customer across financial products or the average time a user spends
    on</st> <st c="2764">a website:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="2774">maximum debt = max(car loan balance, credit card balance,</st>*
    *<st c="2833">mortgage balance)</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="2850">average time on website = mean(time spent on homepage, time spent
    on about page, time spent on</st>* *<st c="2946">FAQ page)</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2955">We can, in principle, use any mathematical or statistical operation
    to create new features, such as the product, mean, standard deviation, or maximum
    or minimum values.</st> <st c="3125">In this recipe, we will implement these mathematical
    operations using</st> `<st c="3195">pandas</st>` <st c="3201">and</st> `<st c="3206">feature-engine</st>`<st
    c="3220">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3221">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3226">While, in the recipe, we can show you how to combine features with
    mathematical functions, we can’t do justice to the use of domain knowledge in
    deciding which function to apply, as that varies with every domain.</st> <st c="3440">So,
    we will leave that</st> <st c="3463">with yo</st><st c="3470">u.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3473">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3487">In this recipe, we</st> <st c="3507">will use the breast cancer
    dataset from</st> `<st c="3547">scikit-learn</st>`<st c="3559">. The features
    are computed from digitized images of</st> <st c="3612">breast cells and describe
    the characteristics of their cell nuclei, in terms of smoothness, concavity, symmetry,
    and compactness, among others.</st> <st c="3756">Each row contains information
    about the morphology of cell nuclei in a tissue sample.</st> <st c="3842">The
    target variable indicates whether the tissue sample corresponds to cancerous cells.</st>
    <st c="3930">The idea is to predict whether the tissue samples belong to benign
    or malignant breast cells, based on their cell</st> <st c="4044">nuclei morphology.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4062">To become familiar with the dataset, run the following commands
    in a Jupyter notebook or</st> <st c="4152">Python console:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="4261">The preceding code block should print out a description of the
    dataset and an interpretation of</st> <st c="4358">its variab</st><st c="4368">les.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4373">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="4389">In</st> <st c="4393">this recipe, we will create new features by
    combining variables using multiple</st> <st c="4472">mathematical operations:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4496">Let’s begin by loading the necessary libraries, classes,</st> <st
    c="4554">and data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4680">Next, load the breast cancer dataset into a</st> `<st c="4725">pandas</st>`
    <st c="4731">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4827">In the following code lines, we will create new features by combining
    variables using multiple</st> <st c="4923">mathematical operations.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="4947">Let’s</st> <st c="4953">begin</st> <st c="4959">by creating a list
    with the subset of the features that we want</st> <st c="5024">to combine:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5147">The features in</st> *<st c="5164">step 3</st>* <st c="5170">represent
    the mean characteristics of cell nuclei in the images.</st> <st c="5236">It might
    be useful to obtain the mean across all</st> <st c="5285">examined characteristics.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="5310">Let</st><st c="5314">’s get the mean value of the features and
    then display the</st> <st c="5374">resulting feature:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5467">The following output shows the mean value of the features from</st>
    *<st c="5531">step 3</st>*<st c="5537">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5624">Similarly, to</st> <st c="5638">capture the general</st> <st c="5659">variability
    of the cell nuclei, let’s determine the standard deviation of the mean characteristics,
    and then display the</st> <st c="5780">resulting feature:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5870">The following output shows the standard deviation of the features
    from</st> <st c="5942">step 3:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6039">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6044">When we craft new features based on domain knowledge, we know exactly
    how we want to combine the variables.</st> <st c="6153">We could also combine
    features with multiple operations and then evaluate whether they are predictive,
    using, for example, a feature selection algorithm or deriving feature importance
    from the machine</st> <st c="6354">learning model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6369">Le</st><st c="6372">t’s make a list containing mathematical functions
    that we want to use to combine</st> <st c="6454">the features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6525">Now, let’s apply the functions from</st> *<st c="6562">step 6</st>*
    <st c="6568">to combine the features from</st> *<st c="6598">step 3</st>*<st c="6604">,
    capturing the</st> <st c="6619">resulting variables in a</st> <st c="6645">new
    DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6710">If we</st> <st c="6716">execute</st> `<st c="6725">df_t.head()</st>`<st
    c="6736">, we will see the DataFrame with the newly</st> <st c="6779">created</st>
    <st c="6787">features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A DataFrame with the newly created features](img/B22396_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7074">Figure 8.1 – A DataFrame with the newly created features</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7130">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7135">pandas</st>` `<st c="7142">agg</st>` <st c="7146">can apply multiple
    functions to combine features.</st> <st c="7197">It can take a list of strings
    with the function names, as we did in</st> *<st c="7265">step 7</st>*<st c="7271">;
    a list of NumPy functions, such as</st> `<st c="7309">np.log</st>`<st c="7315">;
    and Python functions that</st> <st c="7344">you creat</st><st c="7353">e.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7356">We can create the same features that we created with</st> `<st
    c="7410">pandas</st>` <st c="7416">automatically by</st> <st c="7434">using</st>
    `<st c="7440">feature-engine</st>`<st c="7454">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7455">Let’s create a list by using the name of the</st> <st c="7501">output
    features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7595">Let’s set up</st> `<st c="7609">MathFeatures()</st>` <st c="7623">to
    apply the functions in</st> *<st c="7650">step 6</st>* <st c="7656">to the features
    from</st> *<st c="7678">step 3</st>*<st c="7684">, naming the new features with
    the strings from</st> *<st c="7732">step 8</st>*<st c="7738">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7840">Let’s add</st> <st c="7851">the new features</st> <st c="7867">to
    the original DataFrame, capturing the result in a</st> <st c="7921">new variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7966">We can display the input and output features by executing</st>
    `<st c="8025">df_t[features +</st>` `<st c="8041">new_feature_name</st><st c="8057">s].head()</st>`<st
    c="8067">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – DataFrame with the input features and the newly created variables](img/B22396_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8621">Figure 8.2 – DataFrame with the input features and the newly created
    variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8699">While</st> `<st c="8706">pandas</st>` `<st c="8712">agg</st>` <st
    c="8716">returns a DataFrame with the features resulting from the operation,</st>
    `<st c="8785">feature-engine</st>` <st c="8799">goes one step further, by concatenating
    the new features to the</st> <st c="8864">original</st> <st c="8873">DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8883">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="8899">pandas</st>` <st c="8906">has many built-in operations to apply
    mathematical and statistical computations to a group of variables.</st> <st c="9012">To
    combine features mathematically, we first made a list containing the names of
    the features we wanted to combine.</st> <st c="9128">Then, we determined the mean
    and standard deviation of those features by using</st> `<st c="9207">pandas</st>`
    `<st c="9213">mean()</st>` <st c="9220">and</st> `<st c="9225">std()</st>`<st
    c="9230">. We could also apply any of the</st> `<st c="9263">sum()</st>`<st c="9268">,</st>
    `<st c="9270">prod()</st>`<st c="9276">,</st> `<st c="9278">max()</st>`<st c="9283">,
    and</st> `<st c="9289">min()</st>` <st c="9294">methods, which return the sum,
    product, maximum, and minimum values of those features, respectively.</st> <st
    c="9396">To perform these operations across the columns, we added the</st> `<st
    c="9457">axis=1</st>` <st c="9463">argument within</st> <st c="9480">the methods.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9492">With pandas</st> `<st c="9505">agg()</st>`<st c="9510">, we</st>
    <st c="9515">applied several mathematical functions simultaneously.</st> <st c="9570">It
    takes as arguments a list of strings, corresponding to the functions to apply
    and the</st> `<st c="9659">axis</st>` <st c="9663">that the functions should be
    applied to, which can be either</st> `<st c="9725">1</st>` <st c="9726">for columns
    or</st> `<st c="9742">0</st>` <st c="9743">for rows.</st> <st c="9754">As a result,
    pandas</st> `<st c="9774">agg()</st>` <st c="9779">returned a</st> `<st c="9791">pandas</st>`
    <st c="9797">DataFrame, resulting from applying the mathematical functions to
    the groups</st> <st c="9874">of features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9886">Finally,</st> <st c="9895">we</st> <st c="9899">created the same
    features by combining variables with</st> `<st c="9953">feature-engine</st>`<st
    c="9967">. We used the</st> `<st c="9981">MathFeatures()</st>` <st c="9995">transformer,
    which takes the features to combine and the functions to apply as input; it also
    has the option to indicate the names of the resulting features.</st> <st c="10154">When
    we used</st> `<st c="10167">fit()</st>`<st c="10172">, the transformer did not
    learn parameters but checked that the variables were indeed numerical.</st> <st
    c="10269">The</st> `<st c="10273">transform()</st>` <st c="10284">method triggered
    the use of</st> `<st c="10313">pandas.agg</st>` <st c="10323">under the hood,
    applying the mathematical functions to create the</st> <st c="10390">ne</st><st
    c="10392">w variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10405">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10414">To find out more about the mathematical operations supported by</st>
    `<st c="10479">pandas</st>`<st c="10485">,</st> <st c="10487">visit</st> [<st
    c="10493">https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats</st>](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats)<st
    c="10589">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10590">To learn more about</st> `<st c="10611">pandas</st>` `<st c="10617">aggregate</st>`<st
    c="10627">, check</st> <st c="10635">out</st> [<st c="10639">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg</st><st
    c="10718">regate.html</st>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html)<st
    c="10730">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10731">Comparing features to reference vari</st><st c="10768">ables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="10774">In the</st> <st c="10782">previous recipe,</st> *<st c="10799">Combining
    features with mathematical functions</st>*<st c="10845">, we created new features
    by applying mathematical</st> <st c="10895">or statistical functions, such as
    the sum or the mean, to a group of variables.</st> <st c="10976">Some mathematical
    operations, however, such as subtraction or division, are performed</st> *<st
    c="11062">between</st>* <st c="11069">features.</st> <st c="11080">Th</st><st
    c="11082">ese operations are useful to derive ratios, such as the</st> *<st c="11139">debt-to-income
    ratio</st>*<st c="11159">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="11161">debt-to-income ratio = total debt /</st>* *<st c="11197">total
    income</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11209">These operations</st> <st c="11227">are also useful to compute
    differences, such as the</st> *<st c="11279">disposable income</st>*<st c="11296">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="11298">disposable income = income -</st>* *<st c="11327">total debt</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11337">In this recipe, we</st> <st c="11356">will learn how to create
    new features by subtracting or dividing variables with</st> `<st c="11437">pandas</st>`
    <st c="11443">and</st> `<st c="11448">feature-engine</st>`<st c="11462">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11463">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11468">In the recipe, we will show you how to create features with subtraction
    and division.</st> <st c="11555">We hope that the examples, relating to the financial
    sector, shed some light on how to use domain knowledge to decide which features
    to</st> <st c="11691">combine</st> <st c="11699">and how.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11707">How to do</st> <st c="11717">it…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="11721">Let’s begin by loading the necessary Python libraries and the
    breast cancer dataset</st> <st c="11806">from scikit-learn:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11824">Load the necessary libraries, classes,</st> <st c="11864">and
    data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11994">Load the breast cancer dataset into a</st> `<st c="12033">pandas</st>`
    <st c="12039">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12135">In the breast cancer dataset, some features capture the worst
    and the mean characteristics of the cell nuclei of breast cells.</st> <st c="12263">For
    example, for each image (that is, for each row), we have the worst compactness
    observed in all nuclei and the mean compactness of all nuclei.</st> <st c="12409">A</st>
    <st c="12411">feature that captures the difference between the worst and the mean
    value</st> <st c="12484">could</st> <st c="12491">predict malignancy.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="12510">Let’s capture the difference between two features, the</st> `<st
    c="12566">worst compactness</st>` <st c="12583">and</st> `<st c="12588">mean compactness</st>`
    <st c="12604">of cel</st><st c="12611">l nuclei, in a new variable and display</st>
    <st c="12652">its values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12760">In the following output, we can see the difference between these</st>
    <st c="12826">feature values:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12924">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12929">We can perform the same calculation by executing</st> `<st c="12979">df["difference"]
    = df["worst compactness"] - (</st>``<st c="13025">df["mean compactness"])</st>`<st
    c="13049">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13050">Similarly, the ratio between the worst and the average characteristic
    of the cell nuclei might be indicative</st> <st c="13160">of malignancy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13174">Let’s create a new feature with the ratio between the worst and
    mean radius of the nuclei, and then display</st> <st c="13283">its values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13376">In the</st> <st c="13384">following output, we can see the</st>
    <st c="13417">values corresponding to the ratio between</st> <st c="13459">the
    features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13558">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13563">We can calculate the ratio by executing an alternative command,</st>
    `<st c="13628">df["quotient"] = df["worst radius"] / (</st>``<st c="13667">df["me</st><st
    c="13674">an radius"])</st>`<st c="13687">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13688">We can also capture the ratio and difference between every nuclei
    morphology characteristic and the mean radius or mean area of the nuclei.</st>
    <st c="13829">Let’s begin by capturing these subsets of variables</st> <st c="13881">into
    lists.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13892">Let’s make a list of the features in</st> <st c="13930">the numerator:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14032">Let’s make a list of the features in</st> <st c="14070">the denominator:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14127">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14132">We can create features by dividing the features in</st> *<st c="14184">step
    5</st>* <st c="14190">by one of the features in</st> *<st c="14217">step 6</st>*
    <st c="14223">with</st> `<st c="14229">pandas</st>`<st c="14235">, by executing</st>
    `<st c="14250">df[features].div(df["mean radius"])</st>`<st c="14285">. For subtraction,
    we’d execute</st> `<st c="14317">df[features].sub(df["mean radius"])</st>`<st
    c="14352">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14353">Let’s set</st> <st c="14364">up the</st> `<st c="14371">feature-engine</st>`
    <st c="14385">library’s</st> `<st c="14396">RelativeFeatures()</st>` <st c="14414">so
    that it subtracts or divides every feature</st> <st c="14461">from</st> *<st c="14466">step
    5</st>* <st c="14472">with respect to the features from</st> *<st c="14507">step
    6</st>*<st c="14513">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14607">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14612">Subtracting the features from</st> *<st c="14643">step 5</st>*
    <st c="14649">and</st> *<st c="14654">step 6</st>* <st c="14660">does not make
    biological sense, but we will do it anyway to demonstrate the use of the</st>
    `<st c="14748">RelativeFeatures()</st>` <st c="14766">transformer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14779">Let’s add the new features to the DataFrame and capture the result
    in a</st> <st c="14852">new variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14898">Let’s capture the names of the new features in</st> <st c="14946">a
    list:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="15053">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15058">feature_names_in_</st>` <st c="15076">is a</st> <st c="15082">common
    attribute in</st> `<st c="15102">scikit-learn</st>` <st c="15114">and</st> `<st
    c="15119">feature-engine</st>` <st c="15133">transformers and stores the name
    of the variables from the DataFrame used to fit the transformer.</st> <st c="15232">In
    other words, it stores the names of the input features.</st> <st c="15291">When
    using</st> `<st c="15302">transform()</st>`<st c="15313">, the transformers check
    that the features from the new input dataset match those used during training.</st>
    <st c="15417">In</st> *<st c="15420">step 9</st>*<st c="15426">, we leverage this
    attribute to find the additional variables added to the data after</st> <st c="15512">the
    transformation.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15531">If we</st> <st c="15537">execute</st> `<st c="15546">print(new_features)</st>`<st
    c="15565">, we</st> <st c="15569">will see a list with the names of the features
    created by</st> `<st c="15628">ReferenceFeatures()</st>`<st c="15647">. Note</st>
    <st c="15654">that the features contain the variables on the left- and right-hand
    sides of the mathematical equation, plus the function that was applied to them
    to create the</st> <st c="15815">new feature:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: <st c="16364">Finally, we can display the first five rows of the resulting variables
    by</st> <st c="16439">executing</st> `<st c="16449">df</st><st c="16451">_t[new_features].head()</st>`<st
    c="16475">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – A DataFrame with the newly created features](img/B22396_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16983">Figure 8.3 – A DataFrame with the newly created features</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="17039">feature-engine</st>` <st c="17054">adds</st> <st c="17060">new
    features as columns</st> <st c="17083">at the right of the original DataFrame
    and automatically adds variable names to those features.</st> <st c="17180">By
    doing so,</st> `<st c="17193">feature-engine</st>` <st c="17207">automates much
    of the manual work tha</st><st c="17245">t we would do</st> <st c="17260">with</st>
    `<st c="17265">pandas</st>`<st c="17271">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17272">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="17288">pandas</st>` <st c="17295">has many built-in operations to compare
    a feature or a group of features to a reference variable.</st> <st c="17394">In
    this recipe, we used pandas</st> `<st c="17425">sub()</st>` <st c="17430">and</st>
    `<st c="17435">div()</st>` <st c="17440">to determine the difference or the ratio
    between two variables, or a subset of variables and one</st> <st c="17538">refe</st><st
    c="17542">rence feature.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17557">To subtract one variable from another, we applied</st> `<st c="17608">sub()</st>`
    <st c="17613">to a</st> `<st c="17619">pandas</st>` <st c="17625">series with
    the first variable, passing the</st> `<st c="17670">pandas</st>` <st c="17676">series
    with the second variable as an argument to</st> `<st c="17727">sub()</st>`<st
    c="17732">. This operation returned a third</st> `<st c="17766">pandas</st>` <st
    c="17772">series with the difference between the first and second variables.</st>
    <st c="17840">To divide one variable from another, we used</st> `<st c="17885">div()</st>`<st
    c="17890">, which works identically to</st> `<st c="17919">sub()</st>` <st c="17924">–
    that is, it divides the variable on the left by the variable passed as an argument</st>
    <st c="18010">of</st> `<st c="18013">div()</st>`<st c="18018">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18019">Then, we combined several variables with two reference variables
    automatically via subtraction or division, by utilizing</st> `<st c="18141">ReferenceFeatures()</st>`
    <st c="18160">from</st> `<st c="18166">Feature-engine</st>`<st c="18180">. The</st>
    `<st c="18186">ReferenceFeatures()</st>` <st c="18205">transformer takes the variables
    to be combined, the reference variables, and the functions to use to combine them.</st>
    <st c="18321">When using</st> `<st c="18332">fit()</st>`<st c="18337">, the transformer
    did not learn about parameters but checked that the variables were numerical.</st>
    <st c="18433">Executing</st> `<st c="18443">transform()</st>` <st c="18454">added
    the new features to</st> <st c="18481">the DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18495">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="18500">ReferenceFeatures()</st>` <st c="18520">can also add, multiply,
    get the modulo, or get the power of a group of variables relating to a second
    group of reference variables.</st> <st c="18653">You can find out more in its</st>
    <st c="18682">documentation:</st> [<st c="18697">https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html</st>](https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html)<st
    c="18783">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18784">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18793">To learn more about the binary operations supported by</st> `<st
    c="18849">pandas</st>`<st c="18855">,</st> <st c="18857">visit</st> [<st c="18863">https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions</st>](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions)<st
    c="18954">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18955">Performing polynomial expansion</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="18987">Simple</st> <st c="18995">models, such as linear and logistic
    regression, can capture complex patterns if we feed them the right features.</st>
    <st c="19108">Sometimes, we can create powerful features by combining the variables
    in our datasets with themselves or with other variables.</st> <st c="19235">For
    example, in the following figure, we can see that the target,</st> *<st c="19301">y</st>*<st
    c="19302">, has a quadratic relation with the variable,</st> *<st c="19348">x</st>*<st
    c="19349">, and as shown in the left panel, a linear model is not able to capture
    that</st> <st c="19426">relationship accurately:</st>
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.4 – A linear model fit to predict a target, y, from a feature, x,\
    \ which has a quadratic relationship to the target, before and after squaring\
    \ x. In the left panel: the model offers a poor fit by using the original variable;\
    \ in the right panel, the model offers a better fit, based on the squar\uFEFF\
    e of the original variable](img/B22396_08_4.jpg)"
  prefs: []
  type: TYPE_IMG
- en: '<st c="19506">Figure 8.4 – A linear model fit to predict a target, y, from
    a feature, x, which has a quadratic relationship to the target, before and after
    squaring x.</st> <st c="19660">In the left panel: the model offers a poor fit
    by using the original variable; in the right panel, the model offers a better
    fit, based on the squar</st><st c="19808">e of the original variable</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19835">This linear</st> <st c="19848">model has a quadratic relationship
    to the target, before and after squaring</st> *<st c="19924">x</st>*<st c="19925">.
    However, if we square</st> *<st c="19949">x</st>*<st c="19950">, or, in other
    words, if we create a second-degree polynomial of the feature, the linear model
    can accurately predict the target,</st> *<st c="20080">y</st>*<st c="20081">,
    from the square of</st> *<st c="20102">x</st>*<st c="20103">, as we see in the</st>
    <st c="20122">right panel.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20134">Another classical example in which a simple feature can make a
    simple model, such as logistic regression, understand the underlying relationship
    in the data is the</st> **<st c="20299">XOR</st>** <st c="20302">situation.</st>
    <st c="20314">In</st> <st c="20316">the left panel of the following diagram, we
    see how the target class is distributed across the values of</st> *<st c="20422">x1</st>*
    <st c="20424">and</st> *<st c="20429">x2</st>* <st c="20431">(the class is highlighted
    with different</st> <st c="20473">color shades):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – An illustration of the XOR relationship and how combining features
    allows a full class separation](img/B22396_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="20592">Figure 8.5 – An illustration of the XOR relationship and how combining
    features allows a full class separation</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20702">If both features are positive, or both features are negative,
    then the class is 1, but if the features take different signs, then the class
    is 0 (left panel).</st> <st c="20862">Logistic regression will not be able to
    pick this pattern from each individual feature because, as we can see in the middle
    panel, there</st> <st c="20998">is significant class overlap across the values
    of the feature – in this case, x1\.</st> <st c="21081">However, multiplying x1
    by x2 creates a feature that allows a logistic regression to predict the classes
    accurately because x3, as can we see in the right panel, allows the classes to
    be</st> <st c="21268">clearly separated.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21286">With similar logic, polynomial combinations of the same or different
    variables can return new variables that convey additional information and capture
    feature interaction thereby resulting in useful inputs for linear models.</st>
    <st c="21512">With huge datasets, analyzing every possible variable combination
    is not always possible.</st> <st c="21602">But we can create several polynomial
    variables automatically, using, for example,</st> `<st c="21684">scikit-learn</st>`<st
    c="21696">, and we can let the model decide which variables are useful.</st> <st
    c="21758">In this recipe, we will learn how to create multiple features through
    polynomial comb</st><st c="21843">inations</st> <st c="21853">using scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21872">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="21886">Polynomial expansion serves to automate the creation of new features,
    capture feature interaction, and potential non-linear relationships between the
    original variables and the target.</st> <st c="22072">To create polynomial features,
    we need to determine which features to combine and which polynomial degree</st>
    <st c="22178">to use.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22185">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22190">While determining the features to combine or the degree of the
    polynomial combination is not an easy task, keep in mind that high polynomial
    degrees will result in a lot of new features and may lead to overfitting.</st>
    <st c="22406">In general, we keep the degree low, to a maximum of 2</st> <st c="22460">or
    3.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22465">The</st> `<st c="22470">PolynomialFeatures()</st>` <st c="22490">transformer
    from</st> `<st c="22508">scikit-learn</st>` <st c="22520">creates polynomial combinations
    of the features with a degree less than or equal to a user-specified</st> <st
    c="22622">degree, automatically.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22644">To follow up easily with this recipe, let’s first understand the
    output of</st> `<st c="22720">PolynomialFeatures()</st>` <st c="22740">when used
    to create second- and third-degree polynomial combinations of</st> <st c="22813">th</st><st
    c="22815">ree variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22830">Second-degree polynomial combinations of three variables –</st>
    *<st c="22890">a</st>*<st c="22891">,</st> *<st c="22893">b</st>*<st c="22894">,
    and</st> *<st c="22900">c</st>* <st c="22901">– will return the following</st>
    <st c="22930">new features:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="22943">1, a, b, c, ab, ac, bc, a2,</st>* *<st c="22972">b2, c2</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22978">From</st> <st c="22983">the previous features,</st> *<st c="23007">a</st>*<st
    c="23008">,</st> *<st c="23010">b</st>*<st c="23011">, and</st> *<st c="23017">c</st>*
    <st c="23018">are the original variables;</st> *<st c="23047">ab</st>*<st c="23049">,</st>
    *<st c="23051">ac</st>*<st c="23053">, and</st> *<st c="23059">bc</st>* <st c="23061">are
    the products of those features; and</st> *<st c="23102">a2</st>*<st c="23104">,</st>
    *<st c="23106">b2</st>*<st c="23108">, and</st> *<st c="23114">c2</st>* <st c="23116">are
    the squared values of the original features.</st> `<st c="23166">PolynomialFeatures()</st>`
    <st c="23186">also returns the bias term</st> *<st c="23214">1</st>*<st c="23215">,
    which we would probably exclude when</st> <st c="23254">creating features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23272">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23277">The resulting features –</st> *<st c="23303">ab</st>*<st c="23305">,</st>
    *<st c="23307">ac</st>*<st c="23309">, and</st> *<st c="23315">bc</st>* <st c="23317">–
    are</st> <st c="23324">called</st> **<st c="23331">interactions</st>** <st c="23343">or
    feature interactions of</st> **<st c="23371">degree 2</st>**<st c="23379">. The
    degree</st> <st c="23392">reflects the number of variables combined.</st> <st
    c="23435">The result combines a maximum of two variables because we indicated
    a second-degree polynomial as the maximum</st> <st c="23545">allowed combination.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23565">Third-degree polynomial combinations of the three variables –</st>
    *<st c="23628">a</st>*<st c="23629">,</st> *<st c="23631">b</st>*<st c="23632">,
    and</st> *<st c="23638">c</st>* <st c="23639">– will return the following</st>
    <st c="23668">new features:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="23681">1, a, b, c, ab, ac, bc, abc, a2b, a2c, b2a, b2c, c2a, c2b, a3,</st>*
    *<st c="23745">b3, c3</st>*'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23751">Among the returned features, in addition to those returned by
    the second-degree polynomial combination, we now have the third-degree combinations
    of the features with themselves (</st>*<st c="23931">a3</st>*<st c="23934">,</st>
    *<st c="23936">b3</st>*<st c="23938">, and</st> *<st c="23944">c3</st>*<st c="23946">),
    the squared values of every feature combined linearly with a second feature (</st>*<st
    c="24027">a2b</st>*<st c="24031">,</st> *<st c="24033">a2c</st>*<st c="24036">,</st>
    *<st c="24038">b2a</st>*<st c="24041">,</st> *<st c="24043">b2c</st>*<st c="24046">,</st>
    *<st c="24048">c2a</st>*<st c="24051">, and</st> *<st c="24057">c2b</st>*<st c="24060">),
    and the product of the three features (</st>*<st c="24103">abc</st>*<st c="24107">).</st>
    <st c="24111">Note how we have all possible interactions of degrees 1, 2, and
    3 and the bias</st> <st c="24190">term</st> *<st c="24194">1</st>*<st c="24196">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24197">Now that we understand the output of the polynomial expansion
    implemented by</st> `<st c="24275">scikit-lea</st><st c="24285">rn</st>`<st c="24288">,
    let’s jump into</st> <st c="24306">the recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24317">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="24333">In this</st> <st c="24342">recipe, we will create features with
    polynomial expansion using a toy dataset to become familiar with the resulting
    variables.</st> <st c="24469">Creating features with the polynomial expansion
    of a real dataset is identical to what we will discuss in</st> <st c="24575">this
    recipe:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24587">Let’s import the required libraries, classes,</st> <st c="24634">and
    data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24798">Let’s set</st> `<st c="24809">scikit-learn</st>` <st c="24821">library’s</st>
    `<st c="24832">set_output</st>` <st c="24842">API globally so that all transformers
    return a DataFrame as a result of the</st> `<st c="24919">transform()</st>` <st
    c="24930">method:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24976">Let’s</st> <st c="24983">create a DataFrame containing one variable,
    with values from 1</st> <st c="25046">to 10:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25112">Let’s set up</st> `<st c="25126">PolynomialFeatures()</st>` <st
    c="25146">to create all possible combinations up to a third-degree polynomial
    of the single variable and exclude the bias term from the result – that is, we
    will exclude the</st> <st c="25311">value</st> *<st c="25317">1</st>*<st c="25318">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25400">Now, let’s create the</st> <st c="25423">polynomial combinations:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25476">If we execute</st> `<st c="25491">dft</st>`<st c="25494">, we’ll</st>
    <st c="25502">see a DataFrame with the original feature, followed by its values
    squared, and then its values to the power</st> <st c="25610">of three:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A DataFrame with the polynomial expansion of the third degree
    of a single variable](img/B22396_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25812">Figure 8.6 – A DataFrame with the polynomial expansion of the
    third degree of a single variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25907">If, instead</st> <st c="25920">of returning a DataFrame,</st>
    `<st c="25946">PolynomialFeatures()</st>` <st c="25966">returns a NumPy array
    and you want to obtain the names of the features in the array, you can do so by
    executing</st> `<st c="26079">poly.get_feature_names_out()</st>`<st c="26107">,
    which returns</st> `<st c="26123">array(['var', 'var^2', '</st>``<st c="26147">var^3'],
    dtype=object)</st>`<st c="26170">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26171">Now, let’s plot the new feature values against the</st> <st c="26223">original
    variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26424">In the following diagram, we can see the relationship between
    the polynomial fea</st><st c="26505">tures and the</st> <st c="26520">original
    variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The relationship between the features resulting from polynomial
    expansion and the original variable](img/B22396_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26645">Figure 8.7 – The relationship between the features resulting from
    polynomial expansion and the original variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26757">Let’s add</st> <st c="26768">two additional variables to our toy
    dataset, with values from 1</st> <st c="26832">to 10:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26907">Next, let’s combine the three features in the</st> <st c="26954">dataset
    with polynomial expansion up to the second degree, but this time, we will only
    return features produced by combining at least two different variables – that
    is, the</st> <st c="27127">interaction features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27257">If we execute</st> `<st c="27272">dft</st>`<st c="27275">, we
    will see the features resulting from the polynomial expansion, which contain the
    original features, plus all possible combinations of the three variables but without
    the quadratic terms, as we set the transformer to return only</st> <st c="27508">the
    interaction</st> <st c="27524">between features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.8 – A DataFrame with the result of creating features with polynomial\
    \ expansion but retaining only the interactio\uFEFFn between variables](img/B22396_08_8.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="27849">Figure 8.8 – A DataFrame with the result of creating features
    with polynomial expansion but retaining only the interactio</st><st c="27970">n
    between variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27990">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27995">Go ahead and create third-degree polynomial combinations of the
    features, returning only the interactions or all possible features to get a better
    sense of the output</st> <st c="28163">of</st> `<st c="28166">PolynomialFeatures()</st>`<st
    c="28186">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28187">With that, we’ve</st> <st c="28205">learned how to create new
    features by combining existing variables with themselves or other features in
    data.</st> <st c="28315">Creating features via polynomial expansion using a real
    dataset is, in</st> <st c="28386">essence, identical.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28405">If you want to combine only a subset of features, you can select
    the features to combine by utilizing</st> `<st c="28508">ColumnTransformer()</st>`<st
    c="28527">, as we will demonstrate in the</st> *<st c="28559">There’s more…</st>*
    <st c="28572">section ahead in this recipe, or by using</st> `<st c="28615">SklearnTransformerWrapper()</st>`
    <st c="28642">from</st> `<st c="28648">feature-engine</st>`<st c="28662">, as
    you can see in the accompanying GitHub</st> <st c="28706">repository:</st> [<st
    c="28718">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/</st><st
    c="28827">Recipe3-PolynomialExpansion.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/Recipe3-PolynomialExpansion.ipynb)<st
    c="28861">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28862">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28878">In this recipe, we</st> <st c="28898">created features by using
    polynomial combinations of a feature with itself or among three variables.</st>
    <st c="28999">To create these polynomial features, we used</st> `<st c="29044">PolynomialFeatures()</st>`
    <st c="29064">from</st> `<st c="29070">scikit-learn</st>`<st c="29082">. By default,</st>
    `<st c="29096">Polynomia</st><st c="29105">lFeatures()</st>` <st c="29117">generates
    a new feature matrix consisting of all polynomial combinations of the features
    in the data, with a degree less than or equal to the user-specified</st> `<st
    c="29275">degree</st>`<st c="29281">. By setting</st> `<st c="29294">degree</st>`
    <st c="29300">to</st> `<st c="29304">3</st>`<st c="29305">, we created all possible
    polynomial combinations of a degree of 3 or smaller.</st> <st c="29384">To retain
    the combination of a feature with itself, we set the</st> `<st c="29447">interaction_only</st>`
    <st c="29463">parameter to</st> `<st c="29477">False</st>`<st c="29482">. To avoid
    returning the bias term, we set the</st> `<st c="29529">include_bias</st>` <st
    c="29541">parameter</st> <st c="29552">to</st> `<st c="29555">False</st>`<st c="29560">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29561">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29566">Setting the</st> `<st c="29579">interaction_only</st>` <st c="29595">parameter
    to</st> `<st c="29609">True</st>` <st c="29613">returns the interaction terms
    only – that is, the variables resulting from combinations of two or</st> <st c="29712">more
    variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29727">The</st> `<st c="29732">fit()</st>` <st c="29737">method determined
    all of the possible feature combinations based on the parameters specified.</st>
    <st c="29832">At this stage, the transformer did not perform actual mathematical
    computations.</st> <st c="29913">The</st> `<st c="29917">transform()</st>` <st
    c="29928">method performed the mathematical computations with the features to
    create the new variables.</st> <st c="30023">With the</st> `<st c="30032">get_feature_names()</st>`
    <st c="30051">method, we could identify the terms of the expansion – that is,
    how each new feature</st> <st c="30137">was calculated.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30152">In</st> *<st c="30156">step 2</st>*<st c="30162">, we set</st>
    `<st c="30171">scikit-learn</st>` <st c="30183">library’s</st> `<st c="30194">set_output</st>`
    <st c="30204">API</st> `<st c="30228">pandas</st>` <st c="30234">DataFrames as
    a result of the</st> `<st c="30265">transform()</st>` <st c="30276">method.</st>
    <st c="30285">scikit-learn transformers return</st> `<st c="30318">NumPy</st>`
    <st c="30323">arrays by default.</st> <st c="30343">The new</st> `<st c="30351">set_output</st>`
    <st c="30361">API allows us to change the container of the result to a</st> `<st
    c="30419">pandas</st>` <st c="30425">or a</st> `<st c="30431">polars</st>` <st
    c="30437">DataFrame.</st> <st c="30449">We can set the output individually every
    time we set up a transformer – for example, by using</st> `<st c="30543">poly
    = PolynomialFeatures().set_output(transform="pandas")</st>`<st c="30601">. Alternatively,
    as we did in this recipe, we can set the global configuration, and then every
    time we set up a new transformer,</st> <st c="30729">it will return a</st> `<st
    c="30747">pandas</st>` <st c="30753">DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30764">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30780">Let’s create features by</st> <st c="30805">performing polynomial
    expansion on a subset of variables in the breast</st> <st c="30877">cancer dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30892">First, import the necessary libraries, classes,</st> <st c="30941">and
    data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31170">Then, load the data and separate it into train and</st> <st c="31222">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31419">Make a list with the features</st> <st c="31450">to combine:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31531">Set up</st> `<st c="31539">PolynomialFeatures()</st>` <st c="31559">to
    create all possible combinations up to the</st> <st c="31606">third degree:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31700">Set up the column transformer to create features only from those
    specified in</st> *<st c="31779">step 3</st>*<st c="31785">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31838">Create the</st> <st c="31850">polynomial features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31936">And that’s it.</st> <st c="31952">By</st> <st c="31955">executing</st>
    `<st c="31965">ct.get_featur</st><st c="31978">e_names_out()</st>`<st c="31992">,
    we obtain the names of the</st> <st c="32021">new features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32034">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="32039">ColumnTransformer()</st>` <st c="32059">will append the word</st>
    `<st c="32081">poly</st>` <st c="32085">to the resulting variables, which is the
    name we gave to the step within</st> `<st c="32159">ColumnTransformer()</st>`
    <st c="32178">in</st> *<st c="32182">step 5</st>*<st c="32188">. I am not a huge
    fan of this behavior because it makes data analysis harder, as you need to keep
    track of the variable name changes.</st> <st c="32322">To avoid variable name
    changes, you can use</st> `<st c="32366">feature-engine</st>`<st c="32380">’s</st>
    `<st c="32383">SklearnTransformerWrapper()</st>` <st c="32411">instead.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32420">Combining features with decision trees</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="32459">In the</st> <st c="32466">winning solution of the</st> **<st c="32491">Knowledge
    Discovery and Data</st>** <st c="32519">Mining (</st>**<st c="32528">KDD</st>**<st
    c="32532">) competition in 200</st><st c="32553">9, the authors created new features
    by combining two or more variables using decision trees.</st> <st c="32647">When
    examining the</st> <st c="32665">variables, they noticed that some features had
    a high level of mutual information with the target yet low correlation, indicating
    that the relationship with the target was not linear.</st> <st c="32850">While
    these features</st> <st c="32871">were predictive when used in tree-based algorithms,
    linear models could not take advantage of them.</st> <st c="32971">Hence, to use
    these features in linear models, they replaced the features with the outputs of
    decision trees trained on the individual features, or combinations of two or three
    variables, to return new features with a monotonic relationship with</st> <st
    c="33217">the target.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33228">In short, combining features with decision trees is useful for
    creating features that show a monotonic relationship with the target, which is
    useful for making accurate predictions</st> <st c="33409">using linear models.</st>
    <st c="33431">The procedure consists of training a decision tree using a subset
    of the</st> <st c="33504">features – typically, one, two, or three at a time –
    and then using the prediction of the tree as a</st> <st c="33604">new feature.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33616">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33621">You can find more details about this procedure and the overall
    winning solution of the 2009 KDD data competition in this</st> <st c="33743">article:</st>
    [<st c="33752">http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf</st>](http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf)<st
    c="33811">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33812">The good news is that we can automate the creation of features
    using trees, using</st> `<st c="33895">feature-engine</st>`<st c="33909">, and
    i</st><st c="33916">n this recipe, we will learn how to</st> <st c="33953">do
    so.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33959">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33975">In this recipe, we’ll combine features with decision trees, using
    the California</st> <st c="34057">housing dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34073">Let’s begin by importing</st> `<st c="34099">pandas</st>` <st
    c="34105">and the required functions, classes,</st> <st c="34143">and dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="34391">Let’s load the California housing dataset into a</st> `<st c="34441">pandas</st>`
    <st c="34447">DataFrame and remove the</st> `<st c="34473">Latitude</st>` <st
    c="34481">and</st> `<st c="34486">Longitude</st>` <st c="34495">variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="34635">Separate the dataset into train and</st> <st c="34672">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="34772">Check out</st> <st c="34783">Pearson’s correlation coefficient</st>
    <st c="34817">between the features and the target, which is a measure of a</st>
    <st c="34878">linear relationship:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="35051">In the following output, we can see that, apart from</st> `<st
    c="35105">MedInc</st>`<st c="35111">, most variables do not show a strong linear
    relationship with the target; the correlation coefficient is smaller</st> <st
    c="35225">than 0.5:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="35518">Create a grid of hyperparameters to optimize each</st> <st c="35569">decision
    tree:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="35627">The</st> `<st c="35632">feature-engine</st>` <st c="35646">library’s</st>
    `<st c="35657">DecisionTreeFeatures()</st>` <st c="35679">allows us to add features
    resulting from the predictions of a decision tree, trained on one or more features.</st>
    <st c="35790">There</st> <st c="35795">are many ways in which we can instruct
    the transformer to combine the features.</st> <st c="35876">We’ll start by creating
    all possible combinations between</st> <st c="35934">two variables.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="35948">Make a list</st> <st c="35960">with the two features that we want
    to use</st> <st c="36003">as inputs:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36051">Set up</st> `<st c="36059">DecisionTreeFeatures()</st>` <st c="36081">to
    create all possible combinations between the features from</st> *<st c="36144">step
    6</st>*<st c="36150">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36309">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36314">We set</st> `<st c="36322">regression</st>` <st c="36332">to</st>
    `<st c="36336">True</st>` <st c="36340">because the target in this dataset is
    continuous.</st> <st c="36391">If you have a binary target or are performing classification,
    set it to</st> `<st c="36463">False</st>`<st c="36468">. Make sure to select an
    evaluation metric (</st>`<st c="36512">scoring</st>`<st c="36520">) that is suitable
    for</st> <st c="36544">your model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36555">Fit the transformer so that it trains the decision trees on the</st>
    <st c="36620">input features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36661">If you wonder</st> <st c="36676">which features have been used
    to train decision trees, you can inspect them</st> <st c="36752">like this:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36782">In the</st> <st c="36789">following output, we can see that</st>
    `<st c="36824">DecisionTreeFeatures()</st>` <st c="36846">has trained three decision
    trees – two by using the single features,</st> `<st c="36916">AveRooms</st>` <st
    c="36924">and</st> `<st c="36929">AveBedrms</st>`<st c="36938">, and one by using</st>
    <st c="36957">both features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37024">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="37029">DecisionTreeFeatures()</st>` <st c="37052">also stores the decision
    trees.</st> <st c="37085">You can check them out by executing</st> `<st c="37121">dtf.estimators_</st>`<st
    c="37136">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37137">Now, add the features to the training and</st> <st c="37180">testing
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37257">Make a list with the name of the new features (the transformer
    appends the word</st> `<st c="37338">tree</st>` <st c="37342">to the</st> <st
    c="37350">feature names):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37432">Finally, display the features that were added to the</st> <st
    c="37486">test set:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37524">In the following output, we can see the first five rows of the
    new features, resulting from the decision trees trained in</st> *<st c="37647">step
    8</st>*<st c="37653">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![    Figure 8.9 – A portion of the testing set containing the features derived
    from the decision trees](img/B22396_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37883">Figure 8.9 – A portion of the testing set containing the features
    derived from the decision trees</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37980">To check the</st> <st c="37994">power of this transformation,
    calculate</st> <st c="38034">Pearson’s correlation coefficient between the new
    features and</st> <st c="38097">the target:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38257">In the following output, we can see that the correlation between
    the new variables and the target is greater than the correlation shown by the
    original features (compare these values with those of</st> *<st c="38455">step
    4</st>*<st c="38461">):</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38590">If you want to combine specific features instead of getting all
    possible combinations between variables, you can do so by specifying the input
    features</st> <st c="38743">in tuples.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="38753">Create a tuple of tuples with the different features that we want
    to use as input for</st> <st c="38840">decision trees:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38950">Now, we</st> <st c="38959">need</st> <st c="38964">to pass these
    tuples to the</st> `<st c="38992">features_to_combine</st>` <st c="39011">parameter</st>
    <st c="39022">of</st> `<st c="39025">DecisionTreeFeatures()</st>`<st c="39047">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39213">We fitted the transformer in the previous step, so we can go ahead
    and add the features to training and</st> <st c="39318">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39392">Display the</st> <st c="39405">new features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39513">In the following output, we can see the new features derived from
    predictions of decision trees in the</st> <st c="39617">test set:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![    Figure 8.10 – A portion of the testing set containing the features derived
    from the decision trees](img/B22396_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="39886">Figure 8.10 – A portion of the testing set containing the features
    derived from the decision trees</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39984">To wrap up</st> <st c="39995">the recipe, we’ll compare</st> <st
    c="40022">the performance of a Lasso linear regression model trained using the
    original features with one using the features derived from the</st> <st c="40154">decision
    trees.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40169">Import</st> `<st c="40177">Lasso</st>` <st c="40182">and the</st>
    `<st c="40191">cross_validate</st>` <st c="40205">function</st> <st c="40215">from</st>
    `<st c="40220">scikit-learn</st>`<st c="40232">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40324">Set up a Lasso</st> <st c="40340">regression model:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40401">Train and evaluate the model using the original data with cross-validation,
    and then print out the</st> <st c="40501">resulting</st> *<st c="40511">r</st>*<st
    c="40512">-squared:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40692">In the following output, we can see the</st> *<st c="40733">r</st>*<st
    c="40734">-squared of the Lasso regression model trained using the</st> <st c="40791">original
    features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40862">Finally, train a Lasso regression model with the features derived
    from the decision trees</st> <st c="40952">and evaluate it</st> <st c="40969">with
    cross-validation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="41256">In the</st> <st c="41263">following output, we can see that the
    performance of the Lasso regression model trained based of the tree-derived features
    is b</st><st c="41391">etter; the</st> *<st c="41403">r</st>*<st c="41404">-square
    is greater than that from</st> *<st c="41438">step 20</st>*<st c="41445">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="41499">I hope I’ve given you a flavor of the power of combining features
    wit</st><st c="41569">h decision trees and how to do so</st> <st c="41604">with</st>
    `<st c="41609">feature-engine</st>`<st c="41623">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41624">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="41640">In this recipe, we created new features based on the predictions
    of decision trees trained on one or more variables.</st> <st c="41758">We used</st>
    `<st c="41766">DecisionTreeFeatures()</st>` <st c="41788">from</st> `<st c="41793">Feature-engine</st>`
    <st c="41808">to automate the process of training the decision trees with cross-validation
    and</st> <st c="41890">hyperparameter optimization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="41918">DecisionTreeFeatures()</st>` <st c="41941">trains decision trees
    using grid-search under the hood.</st> <st c="41998">Hence, you can pass a grid
    of hyperparameters to optimize the tree, or the transformer will optimize just
    the depth, which, in any case, is the most important parameter in a decision tree.</st>
    <st c="42187">You can also change the metric you want to optimize through the</st>
    `<st c="42251">scoring</st>` <st c="42258">parameter and the cross-validation
    scheme you want to use through the</st> `<st c="42329">cv</st>` <st c="42331">parameter.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42342">The most exciting feature of</st> `<st c="42372">DecisionTreeFeatures()</st>`
    <st c="42394">is its ability to infer the feature combinations to create tree-derived
    features, which is regulated through the</st> `<st c="42508">features_to_combine</st>`
    <st c="42527">parameter.</st> <st c="42539">If you pass an integer to this parameter
    – say, for example,</st> `<st c="42600">3</st>`<st c="42601">,</st> `<st c="42603">DecisionTreeFeatures()</st>`
    <st c="42625">will create all possible combinations of 1, 2, and 3 features and
    use these to train the decision trees.</st> <st c="42731">Instead of an integer,
    you can pass a list of integers – say,</st> `<st c="42793">[2,3]</st>` <st c="42798">–
    in which case,</st> `<st c="42816">DecisionTreeFeatures()</st>` <st c="42838">will
    create all</st> <st c="42854">possible combinations of 2 and 3 features.</st>
    <st c="42898">You</st> <st c="42901">can also specify which features you want
    to combine and how by passing the feature combinations in tuples, as we did in</st>
    *<st c="43022">step 14</st>*<st c="43029">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43030">With</st> `<st c="43036">fit()</st>`<st c="43041">,</st> `<st
    c="43043">DecisionTreeFeatures()</st>` <st c="43065">finds the feature combinations
    and trains the decision trees.</st> <st c="43128">With</st> `<st c="43133">transform()</st>`<st
    c="43144">,</st> `<st c="43146">DecisionTreeFeatures()</st>` <st c="43168">adds
    the features resulting from the decision trees to</st> <st c="43224">the DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43238">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43243">If you are training regression or multi-class classification,
    the new features will be either the prediction of the continuous target or the
    class.</st> <st c="43392">If you are training a binary classification model, the
    new features will result from the probability of</st> <st c="43496">class 1.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43504">After adding the new features, we compared their relationship
    to the target by analyzing Pearson’s correlation coefficient, which returned a
    measure of linear association.</st> <st c="43677">We saw that the features derived
    from trees had a greater</st> <st c="43735">correlation coefficient.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43759">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43768">If you want to know more about what mutual information is and
    how to calculate it, check out this</st> <st c="43867">article:</st> [<st c="43876">https://www.blog.trainindata.com/mutual-information-with-python/</st>](https://www.blog.trainindata.com/mutual-information-with-python/)<st
    c="43940">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43941">Creating periodic features from cyclical variables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="43992">Some features</st> <st c="44006">are periodic – for e</st><st
    c="44027">xample, the</st> <st c="44039">hours in</st> <st c="44048">a day, the
    months in a year, and the days in a week.</st> <st c="44102">They all start at
    a certain value (say, January), go up to a certain other value (say, December),
    and then start over from the beginning.</st> <st c="44240">Some features are numeric,
    such as the hours, and some can be represented with numbers, such as the months,
    with values of 1 to 12\.</st> <st c="44372">Yet, this numeric representation does
    not capture the periodicity or cyclical nature of the variable.</st> <st c="44474">For
    example, December (12) is closer to January (1) than June (6); however, this relationship
    is not captured by the numerical representation of the feature.</st> <st c="44632">But
    we could change it if we transformed these variables with the sine and cosine,
    two naturally</st> <st c="44729">periodic functions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44748">Encoding</st> <st c="44758">cyclical features</st> <st c="44776">with
    the sine and cosine functions allows linear models to leverage the cyclical nature
    of features and reduce their modeling error.</st> <st c="44909">In this recipe,
    we will create new features from</st> <st c="44958">periodic variables that capture
    the cyclical nature</st> <st c="45010">of time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45018">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="45032">Trigonometric functions</st><st c="45056">, such as sine and cosine,
    are periodic, with valu</st><st c="45106">es cycling between -1 and 1 every 2π
    cycles, as</st> <st c="45155">s</st><st c="45156">hown here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Sine and cosine functions](img/B22396_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="45336">Figure 8.11 – Sine and cosine functions</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45375">We can capture the periodicity of a cyclical variable by applying
    a trigonometric transformation after normalizing the variable values between 0</st>
    <st c="45521">and 2π:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>sin</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mi>sin</mi><mrow><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mfrac><mi>X</mi><msub><mi>X</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/37.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="45554">Dividing the variable’s values by its maximum will normalize it
    between 0 and 1 (assuming that the minimum value is 0), and multiplying it by
    2π will rescale the variable between 0</st> <st c="45735">and 2π.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45742">Should we</st> <st c="45753">use sine?</st> <st c="45763">Or should
    we use cosine?</st> <st c="45788">The thing is, we need to use both to encode
    all the values of the variables unequivocally.</st> <st c="45879">Since sine and
    cosine circle between 0 and 1, they will take a value of 0 for more than one value
    of</st> *<st c="45980">x</st>*<st c="45981">. For example, the sine of 0 returns
    0, and so does the sine of π.</st> <st c="46048">So, if we encode a variable with
    just the sine, we wouldn’t be able to distinguish between the values 0 and π anymore.</st>
    <st c="46167">However, because the sine and the cosine are out of phase, the cosine
    of 0 returns 1, whereas the</st> <st c="46265">cosine of π returns -1\.</st> <st
    c="46289">Hence, by encoding the variable with the two functions, we are now able
    to distinguish between 0 and 1, which would take (0,1) and (0,-</st><st c="46424">1)
    as values for the sine and cosine</st> <st c="46462">functions, respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46486">How to do it…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="46500">In this recipe, we will first</st> <st c="46530">transform the</st>
    `<st c="46545">hour</st>` <st c="46549">variable in a toy DataFrame with the sine
    and the cosine to get a sense of the new variable representation.</st> <st c="46658">Then,
    we will automate feature creation from multiple cyclical variables</st> <st c="46731">using</st>
    `<st c="46737">feature-engine</st>`<st c="46751">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46753">Begin by importing the</st> <st c="46776">necessary libraries:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="46867">Create a toy DataFrame with one variable –</st> `<st c="46911">hour</st>`
    <st c="46915">– with values between 0</st> <st c="46940">and 23:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47007">Next, create two features using the sine and cosine transformations,
    after normalizing the variable values between 0</st> <st c="47125">and 2π:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47268">If we e</st><st c="47276">xecute</st> `<st c="47284">df.head()</st>`<st
    c="47293">, we will see the original and</st> <st c="47324">new features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.12 – A DataFrame with the hour variable and the new features obtaine\uFEFF\
    d through the sine and cosine transformations](img/B22396_08_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="47462">Figure 8.12 – A DataFrame with the hour variable and the new features
    obtaine</st><st c="47539">d through the sine and cosine transformations</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47585">Make</st> <st c="47590">a scatter plot between the</st> <st c="47618">hour
    and its</st> <st c="47631">sine-transformed values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47774">In the following plot, we can see how the values of the hour circle
    between -1</st> <st c="47853">and 1, just like the sine function after</st> <st
    c="47895">the transformation:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.13 – A sca\uFEFFtter plot of th\uFEFFe hour versus its sine transformed\
    \ values](img/B22396_08_13.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="48018">Figure 8.13 – A sca</st><st c="48037">tter plot of th</st><st
    c="48053">e hour versus its sine transformed values</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48095">Now, make</st> <st c="48105">a scatter plot between</st> <st c="48129">the
    hour and its</st> <st c="48146">cosine transformation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="48291">In the following plot, we can see how the values of the hour circle
    between -1</st> <st c="48371">and 1, just like the cosine function after</st>
    <st c="48414">the transformation:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.14 – A scatter plot of the hour versus its cosine-transformed values](img/B22396_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="48541">Figure 8.14 – A scatter plot of the hour versus its cosine-transformed
    values</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48618">Finally, we</st> <st c="48630">can reconstitute the cyclical nature
    o</st><st c="48669">f the hour,</st> <st c="48681">which is now captured by the
    two</st> <st c="48715">new features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48728">Plot the</st> <st c="48738">values of the sine versus the cosine
    of the hour, and overlay the original values of the hour using a</st> <st c="48840">color
    map:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49025">In the following plot, we can see how the two trigonometric transformations
    of the hour reflect</st> <st c="49121">the cy</st><st c="49128">clical</st> <st
    c="49136">nature of the hour, in a plot that reminds us of</st> <st c="49185">a
    clock:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15 – A scatter plot of the trigonometric transformation of the hour](img/B22396_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49357">Figure 8.15 – A scatter plot of the trigonometric transformation
    of the hour</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49433">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49438">The code implementation and idea for this plot were taken from
    scikit-learn’s</st> <st c="49517">documentation:</st> [<st c="49532">https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features</st>](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features)<st
    c="49652">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49653">Now that we understand the n</st><st c="49682">ature and effect
    of the transformation, let’s create new features using the sine and cosine transformations
    from multiple variables automatically.</st> <st c="49830">We will use the</st>
    `<st c="49846">feature-engine</st>` <st c="49860">library’s</st> `<st c="49871">CyclicalFeatures()</st>`<st
    c="49889">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49890">Import</st> `<st c="49898">CyclicalFeatures()</st>`<st c="49916">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49971">Let’s create</st> <st c="49984">a toy DataFrame that contains
    the</st> `<st c="50019">hour</st>`<st c="50023">,</st> `<st c="50025">month</st>`<st
    c="50030">, and</st> `<st c="50036">week</st>` <st c="50040">variables, whose</st>
    <st c="50057">values vary between 0 and 23, 1 and 12, and 0 and</st> <st c="50108">6,
    respectively:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="50292">If we execut</st><st c="50305">e</st> `<st c="50308">df.head()</st>`<st
    c="50317">, we will see the first five rows of the</st> <st c="50358">toy DataFra</st><st
    c="50369">me:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.16 – The\uFEFF toy DataFrame with three cyclical features](img/B22396_08_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="50390">Figure 8.16 – The</st> <st c="50407">toy DataFrame with three
    cyclical features</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50450">Set up the transformer to create the sine and cosine features
    from</st> <st c="50518">these variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="50600">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50605">By setting</st> `<st c="50617">variables</st>` <st c="50626">to</st>
    `<st c="50630">None</st>`<st c="50634">,</st> `<st c="50636">CyclicalFeatures()</st>`
    <st c="50654">will create trigonometric features from all numerical variables.</st>
    <st c="50720">To create trigonometric features from a subset of variables, we
    can pass the variables’ names in a list to the</st> `<st c="50831">variables</st>`
    <st c="50840">parameter.</st> <st c="50852">We can retain or drop the original
    variables after creating the cyclical features using the</st> `<st c="50944">drop_original</st>`
    <st c="50957">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50968">To finish, add the</st> <st c="50988">features to the DataFrame
    and capture the result in a</st> <st c="51042">new variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51086">If</st> <st c="51089">we execute</st> `<st c="51101">dft.head()</st>`<st
    c="51111">, we will see the original and</st> <st c="51142">new features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17 – DataFrame with cyclical features plus the features created
    through the sine and cosine functions](img/B22396_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51559">Figure 8.17 – DataFrame with cyclical features plus the features
    created through the sine and cosine functions</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51669">And that’s it – we’ve</st> <st c="51692">created features by using
    the sine and cosine transformation automatically from mu</st><st c="51774">ltiple
    variables and added them directly to the</st> <st c="51823">original DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51842">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="51856">In this recipe, we enco</st><st c="51880">ded cyclical features
    with</st> <st c="51907">values obtained from the sine and cosine functions, applied
    to the normalized values of the variable.</st> <st c="52010">First, we normalized
    the variable values between 0 and 2 π.</st> <st c="52070">To do this, we divided
    the variable values by the variable maximum value, which we obtained with</st>
    `<st c="52167">pandas.max()</st>`<st c="52179">, to scale the variables between
    0 and 1\.</st> <st c="52221">Then, we multiplied those values by 2π, using</st>
    `<st c="52267">numpy.pi</st>`<st c="52275">. Finally, we used</st> `<st c="52294">np.sin</st>`
    <st c="52300">and</st> `<st c="52305">np.cos</st>` <st c="52311">to apply the
    sine and cosine</st> <st c="52341">transformations, respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52371">To automate this procedure for multiple variables, we used the</st>
    `<st c="52435">Feature-engine</st>` <st c="52449">library’s</st> `<st c="52460">CyclicalFeatures()</st>`<st
    c="52478">. With</st> `<st c="52485">fit()</st>`<st c="52490">, the transformer
    learned the maximum values of each variable, and with</st> `<st c="52562">transform()</st>`<st
    c="52573">, it added the features resulting from the sine</st> <st c="52621">and
    cosine transformations to</st> <st c="52651">the DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52665">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52670">In theory, to apply the sine and cosine transformation, we need
    to scale the original variable between 0 and 1\.</st> <st c="52783">Dividing by
    the maximum of the variable will only result in this scaling if the minimum value
    is 0\.</st> <st c="52883">scikit-learn’s documentation and</st> `<st c="52916">Feature-engine</st>`<st
    c="52930">’s current implementation divide the variable by its maximum value (or
    an arbitrary period), without paying too much attention to whether the variable
    starts at 0\.</st> <st c="53095">In practice, you won’t see a big difference in
    the resulting variables if you divide the hour feature by 23 or 24, or the month
    feature by 12 or 11\.</st> <st c="53244">Discussions are underway on whether Feature-engine’s
    implementation should be updated, so the default behavior might change by the
    time t</st><st c="53381">his book is published.</st> <st c="53405">Check out the
    documentation for</st> <st c="53437">more details.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53450">Creating spline features</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="53475">Linear models</st> <st c="53489">expect a linear relationship
    between the predictor variables and the target.</st> <st c="53567">However, we
    can use linear models to model non-linear effects if we first transform the f</st><st
    c="53656">eatures.</st> <st c="53666">In the</st> *<st c="53673">Performing polynomial
    expansion</st>* <st c="53704">recipe, we saw how we can unmask linear patterns
    by creating features with polynomial functions.</st> <st c="53802">In this recipe,
    we will discuss the use</st> <st c="53842">of splines.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53853">Splines are used to mathematically reproduce flexible shapes.</st>
    <st c="53916">They consist of piecewise low-degree polynomial functions.</st>
    <st c="53975">To create splines, we must place knots at several values of</st>
    *<st c="54035">x</st>*<st c="54036">. These knots indicate where the pieces of
    the function join.</st> <st c="54098">Then, we fit low-degree polynomials to the
    data between two</st> <st c="54158">consecutive knots.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54176">There are several types of splines, such as smoothing splines,
    regression splines, and B-splines.</st> <st c="54275">scikit-learn supports the
    use of B-splines to create features.</st> <st c="54338">The procedure to fit and,
    therefore, return the spline values for a certain variable, based on a polynomial
    degree and the number of knots, exceeds the scope of this recipe.</st> <st c="54512">For
    more details, check out the resources in the</st> *<st c="54561">See also</st>*
    <st c="54569">section of this recipe.</st> <st c="54594">In this recipe, we’ll
    get a sense of what splines are</st> <st c="54647">and how we can use them to
    improve the performance of</st> <st c="54702">linear models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54716">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="54730">Let’s get a sense of what splines are.</st> <st c="54770">In the
    following figure, on the left, we can see a spline with a degree of 1\.</st> <st
    c="54848">It consists of two linear pieces – one from 2.5 to 5 and the other from
    5 to 7.5\.</st> <st c="54930">There are three knots – 2.5, 5, and 7.5\.</st> <st
    c="54971">Outside the interval between 2.5 and 7.5, the spline takes a value of
    0\.</st> <st c="55044">The latter is characteristic of splines; they are only
    non-negative between certain values.</st> <st c="55136">On the right panel of
    the figure, we can see three</st> <st c="55186">splines of degree 1\.</st> <st
    c="55208">W</st><st c="55209">e can construct as many splines as we want by introducing</st>
    <st c="55267">more knots:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – The splines with a degree of 1](img/B22396_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="55401">Figure 8.18 – The splines with a degree of 1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55445">In the following figure, on the left, we can see</st> <st c="55494">a
    quadratic spline, also known as a spline with a degree of 2\.</st> <st c="55558">It
    is based on four adjacent knots – 0, 2.5, 5, and 7.5\.</st> <st c="55615">On the
    ri</st><st c="55624">ght-hand side of the figure, we can see several splines of</st>
    <st c="55684">degree 2:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – The splines with a degree of 2](img/B22396_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="55793">Figure 8.19 – The splines with a degree of 2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55837">We can use splines to model non-lin</st><st c="55873">ear functions,
    and we will learn how to do this in the</st> <st c="55929">next section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55942">How to do it…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="55956">In this recipe, we</st> <st c="55975">will use splines to model
    the sine function.</st> <st c="56021">Once we get a sense of what splines are
    and how we can use them to fit non-linear relationships through a linear model,
    we will use splines for regression in a</st> <st c="56181">real dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56194">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56199">The idea to model the sine function with splines was taken from
    scikit-learn’s</st> <st c="56279">documentation:</st> [<st c="56294">https://scikit-learn.org/stable/auto_</st><st
    c="56331">examples/linear_model/plot_polynomial_interpolation.html</st>](https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html)<st
    c="56388">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56389">Let’s begin by importing the necessary libraries</st> <st c="56439">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56613">Create a training set,</st> `<st c="56637">X</st>`<st c="56638">,
    with 20 values between -1 and 11, and the target variable,</st> `<st c="56699">y</st>`<st
    c="56700">, which is the sine</st> <st c="56720">of</st> `<st c="56723">X</st>`<st
    c="56724">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56767">Plot the relationship between</st> `<st c="56798">X</st>` <st
    c="56799">and</st> `<st c="56803">y</st>`<st c="56804">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56853">In the following plot, we can see the sine function</st> <st c="56906">of</st>
    `<st c="56909">X</st>`<st c="56910">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.20 – The relationship \uFEFFbetween the predictor and the target\
    \ variable, where y = sine(x)](img/B22396_08_20.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="56980">Figure 8.20 – The relationship</st> <st c="57011">between the
    predictor and the target variable, where y = sine(x)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57075">Fit a linear</st> <st c="57088">model to predict</st> `<st c="57106">y</st>`
    <st c="57107">from</st> `<st c="57113">X</st>` <st c="57114">by utilizing a Ridge
    regression, and then obtain the predictions of</st> <st c="57183">the model:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="57297">Now, plot the relationship between</st> `<st c="57333">X</st>`
    <st c="57334">and</st> `<st c="57339">y</st>`<st c="57340">, and overlay</st>
    <st c="57354">the predictions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="57505">In the following figure, we can see that the linear model m</st><st
    c="57565">akes a very poor fit of the non-linear relationship between</st> `<st
    c="57626">X</st>` <st c="57627">and</st> `<st c="57631">y</st>`<st c="57632">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21 – The linear fit between X and y](img/B22396_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="57716">Figure 8.21 – The linear fit between X and y</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57760">Now, set</st> <st c="57769">up</st> `<st c="57773">SplineTransformer()</st>`
    <st c="57792">to obtain spline features from</st> `<st c="57824">X</st>`<st c="57825">,
    by utilizing third-degree polynomials and five knots at equidistant places within
    the values</st> <st c="57920">of</st> `<st c="57923">X</st>`<st c="57924">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="57970">Obtain the spline features and convert the NumPy array into a</st>
    `<st c="58033">pandas</st>` <st c="58039">DataFrame, adding the names of the spline</st>
    <st c="58082">basis functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="58212">By executing</st> `<st c="58226">X_df.head()</st>`<st c="58237">,
    we can see the</st> <st c="58254">spline features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.22 – A DataFrame with the splines](img/B22396_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="58585">Figure 8.22 – A DataFrame with the splines</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58627">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="58632">SplineTransformer()</st>` <st c="58652">returns a</st> <st c="58662">feature
    matrix consisting of</st> `<st c="58692">n_splines = n_knots + degree –</st>`
    `<st c="58723">1</st>`<st c="58724">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58725">Now, plot</st> <st c="58736">the splines against the values</st>
    <st c="58767">of</st> `<st c="58770">X</st>`<st c="58771">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="58955">In the following figure, we can see the relationship betwee</st><st
    c="59015">n the different splines and the values of the predictor</st> <st c="59072">variable,</st>
    `<st c="59082">X</st>`<st c="59083">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.23 – Spli\uFEFFnes plotted against the values of the predictor variable,\
    \ X](img/B22396_08_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="59216">Figure 8.23 – Spli</st><st c="59234">nes plotted against the values
    of the predictor variable, X</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59294">Now, fit a</st> <st c="59306">linear model to predict</st> `<st
    c="59330">y</st>` <st c="59331">from the spline features obtained from</st> `<st
    c="59371">X</st>`<st c="59372">, by utilizing a Ridge regression, and then obtain
    the predictions of</st> <st c="59442">the model:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59530">Now, plot the relationship between</st> `<st c="59566">X</st>`
    <st c="59567">and</st> `<st c="59572">y</st>`<st c="59573">, and overlay</st>
    <st c="59587">the predictions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59739">In the following figure, we can see that by utilizing spline featur</st><st
    c="59807">es as input, the Ridge regression can better predict the shape</st>
    <st c="59871">of</st> `<st c="59874">y</st>`<st c="59875">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.24 – The predictions of a linear model, based on splines overlaid
    over the true relationship between X and y](img/B22396_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="59961">Figure 8.24 – The predictions of a linear model, based on splines
    overlaid over the true relationship between X and y</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60078">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60083">Increasing the number of knots or the degree of the polynomial
    increases the flexibility of the spline curves.</st> <st c="60195">Try creating
    splines from higher polynomial degrees and see how the Ridge regression</st> <st
    c="60280">predictions change.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60299">Now that we</st> <st c="60312">understand what the spline features
    are and how we can use them to predict non-linear effects, let’s try them out
    on a</st> <st c="60431">real dataset.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60444">Import some additional classes and functions</st> <st c="60490">from</st>
    `<st c="60495">scikit-learn</st>`<st c="60507">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60661">Load the California housing dataset and drop two of the variables,
    which we won’t use</st> <st c="60748">for modeling:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60882">First, we</st> <st c="60893">wil</st><st c="60896">l fit a Ridge
    regression to predict house prices based on the existing variables, by utilizing
    cross-validation, and then obtain the performance of the model to set up</st>
    <st c="61065">the benchmark:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61253">In the following output, we can see the model performance, where
    the values are</st> <st c="61334">the</st> *<st c="61338">R</st>*<st c="61339">-squared:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: spl = SplineTransformer(degree=3, n_knots=50)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ct = ColumnTransformer(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[("splines", spl, ['
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"AveRooms", "AveBedrms", "Population",'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '"AveOccup"]'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )],
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: remainder="passthrough",
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ct.fit(X, y)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61756">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61761">Remember that we need to use</st> `<st c="61791">ColumnTransformer()</st>`
    <st c="61810">to obtain features from a subset of variables in the data.</st>
    <st c="61870">With</st> `<st c="61875">remainder=passthrough</st>`<st c="61896">,
    we ensure that the variables that are not used as templates for the splines –
    that is,</st> `<st c="61985">MedInc</st>` <st c="61991">and</st> `<st c="61996">HouseAge</st>`
    <st c="62004">– are also returned in the resulting DataFrame.</st> <st c="62053">To
    check out the features resulting from this step,</st> <st c="62105">execute</st>
    `<st c="62113">ct.get_feature_names_out()</st>`<st c="62139">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62140">Now, fit a Ridge</st> <st c="62158">regression to predict house
    prices based on</st> `<st c="62202">MedInc</st>`<st c="62208">,</st> `<st c="62210">HouseAge</st>`<st
    c="62218">, and the spline features, using cross-validation, and then obtain the
    performance of</st> <st c="62304">the model:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="62470">In the following output, we can see the model performance, where
    the values are</st> <st c="62551">the</st> *<st c="62555">R</st>*<st c="62556">-squared:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="62620">As we can see, by using splines in place of some of the original
    va</st><st c="62688">riables, we can improve the performance of the linear</st>
    <st c="62743">regression model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62760">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62774">In this recipe, we created new features based on splines.</st>
    <st c="62833">First, we used a toy variable, with values from -1 to 11, and then
    we obtained splines from a real dataset.</st> <st c="62941">The procedure in both
    cases was identical – we used</st> `<st c="62993">SplineTransformer()</st>` <st
    c="63012">from</st> `<st c="63018">scikit-learn</st>`<st c="63030">. The</st>
    `<st c="63036">SplineTransformer()</st>` <st c="63055">transformer takes the</st>
    `<st c="63078">degree</st>` <st c="63084">property of the polynomial and the number
    of knots (</st>`<st c="63137">n_knots</st>`<st c="63145">) as input and returns
    the splines that better fit the data.</st> <st c="63207">The knots are placed
    at equidistant values of</st> `<st c="63253">X</st>` <st c="63254">by default,
    but through the</st> `<st c="63283">knots</st>` <st c="63288">parameter, we can
    choose to uniformly distribute them to the quantiles of</st> `<st c="63363">X</st>`
    <st c="63364">instead, or we can pass an array with the specific values of</st>
    `<st c="63426">X</st>` <st c="63427">that should be used</st> <st c="63448">as
    knots.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63457">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63462">The number, spacing, and position of the knots are arbitrarily
    set by the user and are the parameters that influence the shape of the splines
    the most.</st> <st c="63615">When using splines in regression models, we can optimize
    these parameters in a randomized search</st> <st c="63712">with cross-validation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63734">With</st> `<st c="63740">fit()</st>`<st c="63745">, the</st> <st
    c="63751">transformer computes the knots of the splines.</st> <st c="63798">With</st>
    `<st c="63803">transform()</st>`<st c="63814">, it returns the array of B-splines.</st>
    <st c="63851">The transformer returns</st> `<st c="63875">n_splines=n_knots +
    degree –</st>` `<st c="63904">1</st>`<st c="63905">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63906">Remember that, like most scikit-learn transformers,</st> `<st
    c="63959">SplineTransformer()</st>` <st c="63978">also now has the option to return</st>
    `<st c="64013">pandas</st>` <st c="64019">and polars DataFrames in addition to
    NumPy arrays, a behavior that can be modified through the</st> `<st c="64115">set_output()</st>`
    <st c="64127">method.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64135">Finally, we used</st> `<st c="64153">ColumnTransformer()</st>`
    <st c="64172">to derive splines from a subset of features.</st> <st c="64218">Because
    we set</st> `<st c="64233">remainder</st>` <st c="64242">to</st> `<st c="64246">passthrough</st>`<st
    c="64257">,</st> `<st c="64259">ColumnTransformer</st><st c="64276">()</st>` <st
    c="64280">concatenated the features that were not used to obtain splines to the
    resulting matrix of splines.</st> <st c="64379">By doing this, we fitted a Ridge
    regression with the splines, plus the</st> `<st c="64450">MedInc</st>` <st c="64456">and</st>
    `<st c="64461">Ho</st><st c="64463">useAge</st>` <st c="64470">variables, and
    managed to improve the linear</st> <st c="64516">model’s performance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64536">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="64545">To find out more about the math underlying B-splines, check out
    the</st> <st c="64614">following articles:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64633">Perperoglou, et al.</st> *<st c="64654">A review of spline function
    procedures in R</st>* <st c="64697">(</st>[<st c="64699">https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3</st>](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3)<st
    c="64777">).</st> <st c="64781">BMC Med Res Methodol 19,</st> <st c="64806">46
    (2019).</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="64816">Eilers and Marx.</st> *<st c="64834">Flexible Smoothing with B-splines
    and</st>* *<st c="64872">Penalties</st>* <st c="64881">(</st>[<st c="64883">https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full</st>](https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full)<st
    c="65030">).</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="65033">For an example of how to use B-splines to model time series data,
    check out the following page in the</st> `<st c="65136">scikit-learn</st>` <st
    c="65148">library’s</st> <st c="65159">documentation:</st> [<st c="65174">https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features</st>](https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features)<st
    c="65296">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
