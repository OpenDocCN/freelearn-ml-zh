- en: '*Chapter 9*: Semantic Segmentation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：语义分割'
- en: This is probably the most advanced chapter concerning deep learning, as we will
    go as far as classifying an image at a pixel level with a technique called semantic
    segmentation. We will use plenty of what we have learned so far, including data
    augmentation with generators.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是关于深度学习最先进的章节，因为我们将会深入到使用一种称为语义分割的技术，对图像进行像素级的分类。我们将充分利用到目前为止所学的内容，包括使用生成器进行数据增强。
- en: We will study a very flexible and efficient neural network architecture called
    DenseNet in great detail, as well as its extension for semantic segmentation,
    FC-DenseNet, and then we will write it from scratch and train it with a dataset
    built with Carla.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将非常详细地研究一个非常灵活且高效的神经网络架构，称为 DenseNet，以及其用于语义分割的扩展，FC-DenseNet，然后我们将从头编写它，并使用由
    Carla 构建的数据集进行训练。
- en: I hope you will find this chapter inspiring and challenging. And be prepared
    for a long training session because our task can be quite demanding!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您会发现这一章既鼓舞人心又具有挑战性。并且准备好进行长时间的训练，因为我们的任务可能相当有挑战性！
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing semantic segmentation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍语义分割
- en: Understanding DenseNet for classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 DenseNet 用于分类
- en: Semantic segmentation with CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 进行语义分割
- en: Adapting DenseNet for semantic segmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 DenseNet 应用于语义分割
- en: Coding the blocks of FC-DenseNet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 FC-DenseNet 的模块
- en: Improving bad semantic segmentation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进糟糕的语义分割
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To be able to use the code explained in this chapter, you will need to have
    the following tools and modules installed:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中解释的代码，您需要安装以下工具和模块：
- en: The Carla simulator
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carla 模拟器
- en: Python 3.7
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7
- en: The NumPy module
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 模块
- en: The TensorFlow module
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 模块
- en: The Keras module
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 模块
- en: The OpenCV-Python module
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV-Python 模块
- en: A GPU (recommended)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 GPU（推荐）
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars](https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars](https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars)找到。
- en: 'The Code in Action videos for this chapter can be found here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的“代码实战”视频可以在以下位置找到：
- en: '[https://bit.ly/3jquo3v](https://bit.ly/3jquo3v)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3jquo3v](https://bit.ly/3jquo3v)'
- en: Introducing semantic segmentation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍语义分割
- en: 'In the previous chapters, we implemented several classifiers, where we provided
    an image as input and the network said what it was. This can be excellent in many
    situations, but to be very useful, it usually needs to be combined with a method
    that can identify the region of interest. We did this in [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158),
    *Detecting Pedestrians and Traffic Lights*, where we used SSD to identify a region
    of interest with a traffic light and then our neural network was able to tell
    the color. But even this would not be very useful to us, because the regions of
    interest produced by SSD are rectangles, and therefore a network telling us that
    there is a road basically as big as the image would not provide much information:
    is the road straight? Is there a turn? We cannot know. We need more precision.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们实现了几个分类器，我们提供了一个图像作为输入，网络告诉我们它是什么。这在许多情况下可能是非常好的，但要非常有用，通常需要结合一种可以识别感兴趣区域的方法。我们在[*第7章*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158)，*检测行人和交通灯*中做到了这一点，我们使用了
    SSD 来识别交通灯的感兴趣区域，然后我们的神经网络能够说出颜色。但即使这样，对我们来说也不会非常有用，因为 SSD 生成的感兴趣区域是矩形，因此一个告诉我们基本上和图像一样大的道路的网络不会提供太多信息：道路是直的？有转弯吗？我们无法知道。我们需要更高的精度。
- en: If object detectors such as SSD brought classification to the next level, now
    we need to reach the level after that, and maybe more. In fact, we want to classify
    every pixel of the image, which is called **semantic** **segmentation**, and is
    quite a demanding task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对象检测器如 SSD 将分类提升到了下一个层次，现在我们需要达到那个层次之后，也许还有更多。实际上，我们想要对图像的每个像素进行分类，这被称为**语义**分割，这是一个相当有挑战性的任务。
- en: 'To understand this better, let''s look at an example taken from Carla. The
    following is the original image:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们看看一个来自 Carla 的例子。以下是原始图像：
- en: '![Figure 9.1 – A frame from Carla](img/Figure_9.1_B16322.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – Carla 的一帧](img/Figure_9.1_B16322.jpg)'
- en: Figure 9.1 – A frame from Carla
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – Carla 的一帧
- en: 'Now let''s look at the same frame produced by the semantic segmentation camera:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看语义分割相机产生的同一帧：
- en: '![Figure 9.2 – Semantic segmentation](img/Figure_9.2_B16322.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 语义分割](img/Figure_9.2_B16322.jpg)'
- en: Figure 9.2 – Semantic segmentation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 语义分割
- en: This is simply great! Not only is the image very simplified, but every color
    has a specific meaning—the road is purple, the sidewalk is magenta, the trees
    are dark green, the lane lines are bright green, and so on. Just to set your expectations,
    we will not be able to achieve such a perfect result, and we will also work at
    a much lower resolution, but we will still achieve interesting results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太好了！不仅图像非常简化，而且每种颜色都有特定的含义——道路是紫色，人行道是洋红色，树木是深绿色，车道线是亮绿色，等等。为了设定你的期望，我们可能无法达到如此完美的结果，我们也将以更低的分辨率工作，但我们仍然会取得有趣的结果。
- en: To be precise, this image is not the real output of the network, but it has
    been converted to show colors; `rgb(7,0,0)`, where the `7` will then be converted
    to purple.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更精确，这张图片并不是网络的真正输出，但它已经被转换为显示颜色；`rgb(7,0,0)`，其中 `7` 将被转换为紫色。
- en: Carla's ability to create images with semantic segmentation is extremely helpful,
    and can allow you to experiment at will, without relying on premade and limited
    datasets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Carla 创建具有语义分割的图像的能力非常有帮助，可以让你随意实验，而无需依赖于预制的和有限的数据库。
- en: Before we start to collect the dataset, let's discuss what the plan is in a
    bit more detail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始收集数据集之前，让我们更详细地讨论一下计划。
- en: Defining our goal
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义我们的目标
- en: Our goal is to use a dataset collected by us to train a neural network from
    scratch to perform semantic segmentation so that it can detect roads, sidewalks,
    pedestrians, traffic signs, and more at a pixel level.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使用我们收集的数据集从头开始训练一个神经网络进行语义分割，以便它可以在像素级别检测道路、人行道、行人、交通标志等等。
- en: 'The steps required for this are as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的步骤如下：
- en: '**Creating the dataset**: We will use Carla to save the original image, the
    raw segmented image (black image with dark colors), and the converted image to
    use better colors for our convenience.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建数据集**：我们将使用 Carla 保存原始图像、原始分割图像（黑色图像，颜色较深）以及转换为便于我们使用的更好颜色的图像。'
- en: '**Building the neural network**: We will study an architecture called **DenseNet**
    in great depth, and then we will see how networks performing semantic segmentations
    are usually structured. After this, we will look at an adaptation of DenseNet
    for semantic segmentation called **FC-DenseNet**, and we will implement it.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建神经网络**：我们将深入研究一个称为 **DenseNet** 的架构，然后我们将看到执行语义分割的网络通常是如何构建的。在此之后，我们将查看用于语义分割的
    DenseNet 的一个变体，称为 **FC-DenseNet**，然后我们将实现它。'
- en: '**Training the neural network**: Here, we will train the network and evaluate
    the result; the training could easily take several hours.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练神经网络**：在这里，我们将训练网络并评估结果；训练可能需要几个小时。'
- en: We will now see the changes required to collect the dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到收集数据集所需的更改。
- en: Collecting the dataset
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据集
- en: 'We have already seen how to record images from Carla and modify `manual_control.py`
    in [*Chapter 8*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182), *Behavioral
    Cloning*, and you could do that, but we have an issue: we really want the RGB
    and raw cameras to be the exact same frame to avoid movements that would make
    our dataset less effective. This problem can be solved using synchronous mode,
    where Carla waits for all the sensors to be ready before sending them to the client,
    which ensures perfect correspondence between the three cameras that we are going
    to save: RGB, raw segmentation, and colored segmentation.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何从 Carla 记录图像并修改 [*第 8 章*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182)
    中的 `manual_control.py`，*行为克隆*，你也能做到这一点，但我们有一个问题：我们真的希望 RGB 和原始相机能够完全同步，以避免使我们的数据集变得不那么有效的运动。这个问题可以通过同步模式来解决，其中
    Carla 等待所有传感器都准备好后再将它们发送到客户端，这确保了我们将要保存的三台相机之间完美的对应关系：RGB、原始分割和彩色分割。
- en: This time, we will modify another file, `synchronous_mode.py`, as it is more
    suitable for this task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将修改另一个文件，`synchronous_mode.py`，因为它更适合这项任务。
- en: I will be specifying where each block of code is located in the file, but it
    is recommended that you go to GitHub and check out the full code there.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我将指定每个代码块在文件中的位置，但建议你前往 GitHub 并查看那里的完整代码。
- en: 'This file is much simpler than `manual_control.py,` and there are basically
    two interesting parts:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件比 `manual_control.py` 简单得多，基本上有两个有趣的方面：
- en: '`CarlaSyncMode`, a class that enables the synchronized mode'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CarlaSyncMode`，一个使能同步模式的类'
- en: '`main()`, which initializes the world (the objects representing the track,
    the weather, and the vehicles) and the cameras, and then moves the car, drawing
    it on the screen'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main()`，它初始化世界（代表轨道、天气和车辆的物体）和相机，然后移动汽车，在屏幕上绘制它'
- en: 'If you run it, you will see that this file self-drives the car, possibly at
    a very high speed, merging the RGB camera and semantic segmentation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行它，你会看到这个文件可以自动驾驶汽车，可能速度非常快，合并RGB相机和语义分割：
- en: '![Figure 9.3 – Output of synchronous_mode.py](img/Figure_9.3_B16322.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – `synchronous_mode.py`的输出](img/Figure_9.3_B16322.jpg)'
- en: Figure 9.3 – Output of synchronous_mode.py
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – `synchronous_mode.py`的输出
- en: Don't be too impressed by the self-driving algorithm because, while very handy
    for us, it's also very limited.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不要对自动驾驶算法过于印象深刻，因为虽然对我们来说非常方便，但它也非常有限。
- en: 'Carla has a high number of **waypoints**, which are 3D-directed points. These
    points, which number in the thousands per track, follow the road and are taken
    from the OpenDRIVE map; OpenDRIVE is an open file format that Carla uses to describe
    the road. These points are oriented with the road, so if you move the car toward
    the points while also applying the orientations of these points, the car effectively
    moves as if it was self-driving. Brilliant! Until you add cars and walkers; then
    you start to get frames like these ones, because the car will move into other
    vehicles:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 卡拉拉（Carla）有大量的**路标**，这些是3D指向的点。这些点在每个轨道上数以千计，沿着道路排列，并来自OpenDRIVE地图；OpenDRIVE是一个开放文件格式，卡拉拉（Carla）使用它来描述道路。这些点与道路方向一致，因此如果你在移动汽车的同时也应用这些点的方向，汽车实际上就像自动驾驶一样移动。太棒了！直到你添加汽车和行人；然后你开始得到这样的帧，因为汽车会移动到其他车辆中：
- en: '![Figure 9.4 – Frames with a collision](img/Figure_9.4_B16322.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 发生碰撞的帧](img/Figure_9.4_B16322.jpg)'
- en: Figure 9.4 – Frames with a collision
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 发生碰撞的帧
- en: This might be a bit surprising when you see it, but it is still fine for our
    task, so it is not a big problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到这个时可能会有些惊讶，但对我们来说这仍然很好，所以这不是一个大问题。
- en: Let's now see how we need to modify `synchronous_mode.py`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看现在我们需要如何修改`synchronous_mode.py`。
- en: Modifying synchronous_mode.py
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改`synchronous_mode.py`
- en: 'All the following changes need to be made in the `main()` function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所有后续的更改都需要在`main()`函数中进行：
- en: 'We will change the camera position to be the same one that we used with the
    behavioral cloning, although this is not required. This involves changing the
    two calls to `carla.Transform()` with this line (it''s the same line for both
    the locations):'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将改变相机位置，使其与我们在行为克隆中使用的相同，尽管这不是必需的。这涉及到将两个`carla.Transform()`调用改为这一行（对两个位置都是相同的行）：
- en: '[PRE0]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Just after moving the car, we can save the RGB camera and the raw semantic
    segmentation image:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在移动汽车后，我们可以保存RGB相机和原始语义分割图像：
- en: '[PRE1]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the code, the line immediately after calls `image_semseg.convert()` to convert
    the raw image to the colored version, according to the CityScapes palette; now
    we can save the image with semantic segmentation so that it is properly colored:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在代码中，调用`image_semseg.convert()`的下一行将原始图像转换为彩色版本，根据CityScapes调色板；现在我们可以保存带有语义分割的图像，使其正确着色：
- en: '[PRE2]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are almost done. We just need to write the `save_img()` function:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们几乎完成了。我们只需要编写`save_img()`函数：
- en: '[PRE3]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first lines of the preceding steps of code convert the image of Carla from
    a buffer to a NumPy array and select the first three channels, dropping the fourth
    one (the transparency channel). Then we resize the image to 160 X 160 using the
    `INTER_NEAREST` algorithm to avoid smoothing the image while resizing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的前几行将卡拉拉（Carla）的图像从缓冲区转换为NumPy数组，并选择前三个通道，丢弃第四个通道（透明度通道）。然后我们使用`INTER_NEAREST`算法将图像调整大小为160
    X 160，以避免在调整大小时平滑图像。
- en: The last line saves the image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行保存了图像。
- en: 'Tip: Resize segmentation masks using the nearest-neighbor algorithm'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：使用最近邻算法调整分割掩码的大小
- en: Are you wondering why we resize using `INTER_NEAREST`, the nearest-neighbor
    algorithm, which is the most basic interpolation? The reason is that it does not
    interpolate the color but chooses the color of the pixel closer to the interpolated
    position, and this is important for the raw semantic segmentation. For example,
    let's say we are scaling four pixels down to one. Two of the pixels have a value
    of 7 (roads) and the other two pixels have a value of 9 (vegetation). We might
    be happy with the output being either 7 or 9, but we surely don't want it to be
    8 (sidewalks)!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否好奇为什么我们使用`INTER_NEAREST`，即最近邻算法进行缩放，这是最基础的插值方法？原因在于它不是进行颜色插值，而是选择接近插值位置的像素颜色，这对于原始语义分割非常重要。例如，假设我们将四个像素缩小到一点。其中两个像素的值为7（道路），另外两个像素的值为9（植被）。我们可能对输出为7或9都感到满意，但肯定不希望它为8（人行道）！
- en: But for RGB and colored segmentation, you can use more advanced interpolations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于RGB和彩色分割，你可以使用更高级的插值。
- en: This is everything that is required to collect images. The 160 X 160 resolution
    is the one I chose for my network, and we will discuss this choice later. If you
    use another resolution, please adjust the settings accordingly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是收集图像所需的一切。160 X 160的分辨率是我为我的网络选择的，我们稍后会讨论这个选择。如果你使用另一个分辨率，请相应地调整设置。
- en: You can also save at full resolution, but then you have to either write a program
    to change it later or do this when you train the neural network, and since we
    will be using a generator, this means that we need to use this convention for
    every image and for every epoch—so more than 50,000 times in our case—plus it
    will make loading the JPEG slower, which also needs to be performed 50,000 times
    in our case.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以以全分辨率保存，但这样你就必须编写一个程序在之后更改它，或者在你训练神经网络时进行此操作，由于我们将使用生成器，这意味着我们需要为每张图像和每个epoch使用这个约定——在我们的案例中超过50,000次——此外，它还会使加载JPEG变慢，在我们的案例中这也需要执行50,000次。
- en: Now that we have the dataset, we can build the neural network. Let's start with
    the architecture of DenseNet, which is the foundation of our model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据集，我们可以构建神经网络。让我们从DenseNet的架构开始，这是我们模型的基础。
- en: Understanding DenseNet for classification
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DenseNet在分类中的应用
- en: '**DenseNet** is a fascinating architecture of neural networks that is designed
    to be flexible, memory efficient, effective, and also relatively simple. There
    are really a lot of things to like about DenseNet.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**DenseNet**是一个令人着迷的神经网络架构，旨在具有灵活性、内存效率、有效性和相对简单性。关于DenseNet有很多值得喜欢的地方。'
- en: The DenseNet architecture is designed to build very deep networks, solving the
    problem of the *vanishing gradient* with techniques derived from ResNet. Our implementation
    will reach 50 layers, but you can easily build a deeper network. In fact, Keras
    has three types of DenseNet trained on ImageNet, with 121, 169, and 201 layers,
    respectively. DenseNet also solves the problem of *dead neurons*, when you have
    neurons that are basically not active.The next section will show a high-level
    overview of DenseNet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet架构旨在构建非常深的网络，通过从ResNet中提取的技术解决了**梯度消失**的问题。我们的实现将达到50层，但你很容易构建一个更深层的网络。实际上，Keras有三种在ImageNet上训练的DenseNet，分别有121、169和201层。DenseNet还解决了**神经元死亡**的问题，即当你有基本不活跃的神经元时。下一节将展示DenseNet的高级概述。
- en: DenseNet from a bird's-eye view
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从鸟瞰角度看DenseNet
- en: 'For the moment, we will focus on DenseNet as a classifier, which is not what
    we are going to implement, but it is useful as a concept to start to understand
    it. The high-level architecture of DenseNet is illustrated in the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将关注DenseNet作为一个分类器，这并不是我们即将实现的内容，但作为一个概念来开始理解它是很有用的。DenseNet的高级架构在以下图中展示：
- en: '![Figure 9.5 – High-level view of DenseNet as a classifier, with three dense
    blocks](img/Figure_9.5_B16322.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – DenseNet作为分类器的高级视图，包含三个密集块](img/Figure_9.5_B16322.jpg)'
- en: Figure 9.5 – High-level view of DenseNet as a classifier, with three dense blocks
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – DenseNet作为分类器的高级视图，包含三个密集块
- en: The figure only shows three dense blocks, but there are usually a few more.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图中只显示了三个密集块，但实际上通常会有更多。
- en: 'As you can see from the diagram, it is quite simple to understand the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如从图中所示，理解以下内容相当简单：
- en: The input is an RGB image.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是一个RGB图像。
- en: There is an initial 7 X 7 convolution.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在一个初始的7 X 7卷积。
- en: There is a **dense block**, which contains some convolutions. We will describe
    this in depth soon.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在一个**密集块**，其中包含一些卷积操作。我们很快会对此进行深入描述。
- en: Every **dense block** is followed by a 1 X 1 convolution and an average pooling,
    which reduces the image size.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个密集块后面都跟着一个 1 X 1 卷积和一个平均池化，这会减小图像的大小。
- en: The last **dense bloc**k is followed directly by the average pooling.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个密集块直接跟着平均池化。
- en: At the end, there is a dense (fully connected) layer with a **softmax**.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，有一个密集（全连接）层，带有 **softmax**。
- en: The 1 X 1 convolution can be used to reduce the number of channels to speed
    up the computations. The 1 X 1 convolution followed by the average pooling is
    called a **transition layer** by the DenseNet paper, and when the number of channels
    is reduced, they call the resulting network **DenseNet-C**, where the *C* means
    *compression* and the convolution layer is called the **compression layer**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 1 X 1 卷积可以用来减少通道数以加快计算速度。DenseNet 论文中将 1 X 1 卷积后面跟着平均池化称为 **过渡层**，当通道数减少时，他们称得到的网络为
    **DenseNet-C**，其中 *C* 代表 *压缩*，卷积层被称为 **压缩层**。
- en: As a classifier, this high-level architecture is not particularly remarkable,
    but as you might have guessed, the innovation is in the dense blocks, which are
    the focus of the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分类器，这个高级架构并不特别引人注目，但正如你可能猜到的，创新在于密集块，这是下一节的重点。
- en: Understanding the dense blocks
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解密集块
- en: The dense blocks give the name to the architecture and are the main part of
    DenseNet; they contain the convolutions, and you usually have several of them,
    depending on the resolution, the precision that you want to achieve, and the performance
    and training time. Please note that they are unrelated to the dense layers that
    we have already met.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 密集块是该架构的名称，也是 DenseNet 的主要部分；它们包含卷积，通常根据分辨率、你想要达到的精度以及性能和训练时间，你会有几个这样的块。请注意，它们与我们之前遇到的密集层无关。
- en: 'The dense blocks are blocks that we can repeat to increase the depth of the
    network, and they achieve the following goals:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 密集块是我们可以通过重复来增加网络深度的块，它们实现了以下目标：
- en: They solve the *vanishing gradient* problem, allowing us to make very deep networks.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们解决了 *梯度消失* 问题，使我们能够构建非常深的网络。
- en: They are very efficient, using a relatively small number of parameters.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们非常高效，使用相对较少的参数。
- en: They solve the *dead neurons* problem, meaning that all the convolutions contribute
    to the final result, and we don't waste CPU and memory on neurons that are basically
    useless.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们解决了 *无效神经元* 问题，意味着所有的卷积都对最终结果有贡献，我们不会浪费 CPU 和内存在基本无用的神经元上。
- en: 'These are big goals, goals that many architectures struggle to achieve. So
    let''s see how DenseNet can do what many other architectures cannot. The following
    is a dense block:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是宏伟的目标，许多架构都难以实现这些目标。那么，让我们看看 DenseNet 如何做到其他许多架构无法做到的事情。以下是一个密集块：
- en: '![Figure 9.6 – Dense block with five convolutions, plus the input](img/Figure_9.6_B16322.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 带有五个卷积的密集块，以及输入](img/Figure_9.6_B16322.jpg)'
- en: Figure 9.6 – Dense block with five convolutions, plus the input
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 带有五个卷积的密集块，以及输入
- en: This is indeed remarkable, and requires some explanation. Perhaps you remember
    **ResNet** from [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158),
    *Detecting Pedestrians and Traffic Lights*, a neural network built by Microsoft
    that had a feature called *skip connections*, shortcuts that could allow a layer
    to skip other layers, helping to solve the vanishing gradient problem and therefore
    achieve deeper networks. In fact, some versions of ResNet can have more than 1,000
    layers!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实很了不起，需要一些解释。也许你还记得来自 [*第 7 章*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158)，*检测行人和交通信号灯*
    的 **ResNet**，这是一个由微软构建的神经网络，它有一个名为 *跳过连接* 的特性，这些快捷方式允许一层跳过其他层，有助于解决梯度消失问题，从而实现更深的网络。实际上，ResNet的一些版本可以有超过
    1,000 层！
- en: 'DenseNet brings this concept to the extreme, as inside every dense block, each
    convolutional layer is connected and concatenated to the other convolutional layers
    of the same block! This has two very important implications:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 将这一概念推向了极致，因为在每个密集块内部，每个卷积层都与其他卷积层连接并连接起来！这有两个非常重要的含义：
- en: The presence of the skip connections clearly achieves the same effect of the
    skip connections in ResNet, making deeper networks much easier to train.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过连接的存在显然实现了 ResNet 中跳过连接的相同效果，使得训练更深的网络变得容易得多。
- en: Thanks to the skip connections, the features of each layer can be reused by
    the following layers, making the network very efficient and greatly reducing the
    number of parameters compared to other architectures.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多亏了跳跃连接，每一层的特征可以被后续层复用，这使得网络非常高效，并且与其它架构相比，大大减少了参数数量。
- en: 'The feature''s reuse can be better appreciated with the following diagram,
    which explains the effect of the dense block, focusing on the channels instead
    of the skip connections:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能的复用效果可以通过以下图表更好地理解，该图表解释了密集块的效果，重点关注通道而不是跳跃连接：
- en: '![Figure 9.7 – Effects of the skip connections on a dense block with five layers
    and a growth rate of three](img/Figure_9.7_B16322.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 跳跃连接对具有五个层和增长率为三的密集块的影响](img/Figure_9.7_B16322.jpg)'
- en: Figure 9.7 – Effects of the skip connections on a dense block with five layers
    and a growth rate of three
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 跳跃连接对具有五个层和增长率为三的密集块的影响
- en: The first horizontal line shows the new features that are added by each convolution,
    while all the other horizontal lines are convolutions provided by the previous
    layers, and they are reused thanks to the skip connections.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条水平线显示了每个卷积添加的新特征，而所有其他水平线都是前一层提供的卷积，并且由于跳跃连接而得以复用。
- en: 'Analyzing the diagram, where the content of each layer is a column, we can
    see the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析图表，其中每一层的内容是一列，我们可以看到以下内容：
- en: The input layer has 5 channels.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层有5个通道。
- en: Layer 1 adds 3 new channels and reuses the input, so it effectively has 8 channels.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1层增加了3个新通道，并复用了输入，因此它实际上有8个通道。
- en: Layer 2 adds 3 new channels and reuses the input and layer 1, so it effectively
    has 11 channels.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2层增加了3个新通道，并复用了输入和第1层，因此它实际上有11个通道。
- en: This continues until layer 5, which adds 3 new channels and reuses the input
    and layers 1, 2, 3, and 4, so it effectively has 20 channels.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种情况一直持续到第5层，它增加了3个新通道，并复用了输入以及第1、2、3和4层，因此它实际上有20个通道。
- en: This is very powerful, because the convolutions can reuse the previous layers,
    adding only some new channels, with the result that the network is compact and
    efficient. In addition, these new channels are going to provide new information
    because they have direct access to the previous layers, which means that they
    won't somehow replicate the same information or lose contact with what was already
    computed a few layers before. The number of new channels added on each layer is
    called the **growth rate**; it was **3** in our example, while in real life it
    will probably be 12, 16, or more.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常强大，因为卷积可以复用之前的层，只添加一些新通道，结果使得网络紧凑且高效。此外，这些新通道将提供新的信息，因为它们可以直接访问之前的层，这意味着它们不会以某种方式复制相同的信息或失去与之前几层已计算内容的联系。每一层添加的新通道数量被称为**增长率**；在我们的例子中它是**3**，而在现实生活中它可能为12、16或更多。
- en: For the dense blocks to work, all the convolution need to use padding with the
    value **same**, which, as we know, keeps the resolution unchanged.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使密集块正常工作，所有卷积都需要使用**same**值进行填充，正如我们所知，这保持了分辨率不变。
- en: Every dense block is followed by a transition layer with average pooling, which
    reduces the resolution; as skip connections require the resolution of the convolutions
    to be the same, this means that we can only have skip connections inside the same
    dense block.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个密集块后面都跟着一个具有平均池化的过渡层，这降低了分辨率；由于跳跃连接需要卷积的分辨率相同，这意味着我们只能在同一密集块内部有跳跃连接。
- en: 'Each layer of a dense block is composed of the following three components:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 密集块的每一层由以下三个组件组成：
- en: A batch normalization layer
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批量归一化层
- en: A ReLU activation
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个ReLU激活
- en: The convolution
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积
- en: 'The convolution block can therefore be written like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，卷积块可以写成如下形式：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a different style to writing Keras code, where instead of using the
    model object to describe the architecture, you build a chain of layers; this is
    the style to use with skip connections, as you need the flexibility to be able
    to use the same layer more than once.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种不同的Keras代码编写风格，在这种风格中，不是使用模型对象来描述架构，而是构建一系列层；这是使用跳跃连接时应该使用的风格，因为你需要灵活性，能够多次使用相同的层。
- en: In DenseNet, at the beginning of each dense block, you can add an optional 1
    X 1 convolution, with the goal of reducing the number of channels of input and
    therefore improving performance; when this 1 X 1 convolution is present, we call
    it a **bottleneck layer** (because the number of channels is reduced), and the
    network is called **DenseNet-B**. When the network has both the bottleneck and
    the compression layer, it is called **DenseNet-BC**. As we already know, the ReLU
    activation will add nonlinearity, so having many layers can result in a network
    that can learn very complex functions, which we will definitely need for semantic
    segmentation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在DenseNet中，每个密集块的开始处，你可以添加一个可选的1 X 1卷积，目的是减少输入通道的数量，从而提高性能；当存在这个1 X 1卷积时，我们称之为**瓶颈层**（因为通道数量减少了），而网络被称为**DenseNet-B**。当网络同时具有瓶颈层和压缩层时，它被称为**DenseNet-BC**。正如我们所知，ReLU激活函数会添加非线性，因此有很多层可以导致网络学习非常复杂的函数，这对于语义分割肯定是非常需要的。
- en: If you are wondering about dropout, DenseNet can function well without it; one
    reason for this is the presence of normalization layers, which already provide
    a regularization effect, and so their combination with dropout is not particularly
    effective. In addition, the presence of dropout usually requires us to increase
    the size of the network, which is against the goals of DenseNet. That said, the
    original paper mentions using dropouts after the convolutional layers, when there
    is no data augmentation, and I think that, by extension, if there are not many
    samples, dropout can help.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对dropout有所疑问，DenseNet可以在没有dropout的情况下很好地工作；其中一个原因是存在归一化层，它们已经提供了正则化效果，因此与dropout的组合并不特别有效。此外，dropout的存在通常要求我们增加网络的大小，这与DenseNet的目标相悖。尽管如此，原始论文提到在卷积层之后使用dropout，当没有数据增强时，我认为通过扩展，如果样本不多，dropout可以帮助。
- en: Now that we have an understanding of how DenseNet works, let's learn how to
    make a neural network for semantic segmentation, which will pave the way to a
    later section about how to adapt DenseNet to perform semantic segmentation tasks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了DenseNet的工作原理，让我们学习如何构建用于语义分割的神经网络，这将为后续关于如何将DenseNet应用于语义分割任务的章节铺平道路。
- en: Segmenting images with CNN
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN进行图像分割
- en: 'A typical semantic segmentation task receives as input an RGB image and needs
    to output an image with the raw segmentation, but this solution could be problematic.
    We already know that classifiers generate their results using *one-hot encoded*
    labels, and we can do the same for semantic segmentation: instead of generating
    a single image with the raw segmentation, the network can create a series of *one-hot
    encoded* images. In our case, as we need 13 classes, the network will output 13
    RGB images, one per label, with the following features:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的语义分割任务接收一个RGB图像作为输入，并需要输出一个包含原始分割的图像，但这种方法可能存在问题。我们已经知道，分类器使用`one-hot encoded`标签生成结果，我们也可以为语义分割做同样的事情：而不是生成一个包含原始分割的单个图像，网络可以创建一系列`one-hot
    encoded`图像。在我们的案例中，由于我们需要13个类别，网络将输出13个RGB图像，每个标签一个，具有以下特征：
- en: One image describes only one label.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张图像只描述一个标签。
- en: The pixels belonging to the label have a value of 1 in the red channel, while
    all the other pixels are marked as 0.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于该标签的像素在红色通道中的值为`1`，而所有其他像素都被标记为`0`。
- en: 'Each given pixel can be `1` only in one image; it will be `0` in all the remaining
    images. This is a difficult task, but it does not necessarily require particular
    architectures: a series of convolutional layers with *same* padding can do it;
    however, their cost quickly becomes computationally expensive, and you might also
    have problems fitting the model in memory. As a consequence, there has been a
    push to improve this architecture.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每个给定的像素在一个图像中只能为`1`，在所有其他图像中都将为`0`。这是一个困难的任务，但并不一定需要特定的架构：一系列带有`same`填充的卷积层可以完成这项任务；然而，它们的成本很快就会变得计算昂贵，并且你可能还会遇到在内存中拟合模型的问题。因此，人们一直在努力改进这种架构。
- en: As we already know, a typical way to solve this problem is to use a form of
    pooling to reduce the resolution while adding layers and channels. This works
    for classification, but as we need to generate an image with the same resolution
    as the input, we need a way to *go back* to that resolution. One way to do this
    is by using a **transposed convolution**, also called **deconvolution**, which
    is a transformation going in the opposite direction of a convolution that is able
    to increase the resolution of the output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，解决此类问题的典型方法是通过一种形式的池化来降低分辨率，同时增加层和通道。这对于分类是有效的，但我们需要生成与输入相同分辨率的图像，因此我们需要一种方法来*回退*到该分辨率。实现这一目标的一种方法是通过使用**转置卷积**，也称为**反卷积**，这是一种与卷积相反方向的转换，能够增加输出分辨率。
- en: If you add a series of convolutions and a series of deconvolutions, the resulting
    network is U-shaped, with the left side starting from the input, adding convolutions
    and channels while reducing the resolution, and the right side having a series
    of deconvolutions that bring the resolution back to the original one. This can
    be more efficient than using only convolutions of the same size, but the resulting
    segmentation will effectively have a much lower resolution than the original input.
    To solve this problem, it's possible to introduce skip connections from the left
    side to the right to give the network enough information to restore the correct
    resolution not only formally, with the number of pixels, but also practically,
    at the mask level.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你添加一系列卷积和一系列反卷积，得到的网络是U形的，左侧从输入开始，添加卷积和通道同时降低分辨率，右侧有一系列反卷积将分辨率恢复到原始值。这比仅使用相同大小的卷积更有效率，但生成的分割实际上分辨率会比原始输入低得多。为了解决这个问题，可以从左侧引入跳过连接到右侧，以便网络有足够的信息来恢复正确的分辨率，不仅在形式上（像素数），而且在实际层面（掩码级别）。
- en: Now we can look at how to apply these ideas to DenseNet.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看看如何将这些想法应用到DenseNet中。
- en: Adapting DenseNet for semantic segmentation
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将DenseNet应用于语义分割
- en: DenseNet is very suitable for semantic segmentation because of its efficiency,
    accuracy, and abundance of skip layers. In fact, using DenseNet for semantic segmentation
    proves to be effective even when the dataset is limited and when a label is underrepresented.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet由于其效率、准确性和丰富的跳层而非常适合语义分割。事实上，即使在数据集有限且标签表示不足的情况下，使用DenseNet进行语义分割也已被证明是有效的。
- en: 'To use DenseNet for semantic segmentation, we need to be able to build the
    right side of the *U* network, which means that we need the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用DenseNet进行语义分割，我们需要能够构建*U*网络的右侧，这意味着我们需要以下内容：
- en: A way to increase the resolution; if we call the transition layers of DenseNet
    *transition down*, then we need *transition-up* layers.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种提高分辨率的方法；如果我们称DenseNet的过渡层为*transition down*，那么我们需要*transition-up*层。
- en: We need to build the skip layers to join the left and right side of the *U*
    network.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要构建跳层来连接*U*网络的左右两侧。
- en: Our reference network is FC-DenseNet, also known as one hundred layers tiramisu,
    but we are not trying to reach 100 layers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的参考网络是FC-DenseNet，也称为一百层提拉米苏，但我们并不试图达到100层。
- en: 'In practice, we want to achieve an architecture similar to the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们希望实现一个类似于以下架构的架构：
- en: '![Figure 9.8 – Example of FC-DenseNet architecture](img/Figure_9.8_B16322.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – FC-DenseNet架构示例](img/Figure_9.8_B16322.jpg)'
- en: Figure 9.8 – Example of FC-DenseNet architecture
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – FC-DenseNet架构示例
- en: The horizontal red arrows connecting the concatenation layers in *Figure 9.8*
    are the skip connections used to improve the resolution of the output, and they
    can only work if the output of the corresponding dense block on the left is the
    same resolution as the input of the corresponding dense block on the right; this
    is achieved using the transition-up layers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.8*中连接拼接层的水平红色箭头是用于提高输出分辨率的跳连接，并且它们只能在工作时，左侧相应的密集块的输出与右侧相应的密集块的输入具有相同的分辨率；这是通过使用过渡-up层实现的。
- en: Let's now see how to implement FC-DenseNet.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何实现FC-DenseNet。
- en: Coding the blocks of FC-DenseNet
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码FC-DenseNet的模块
- en: 'DenseNet is very flexible, so you can easily configure it in many ways. However,
    depending on the hardware of your computer, you might hit the limits of your GPU.
    The following are the values that I used on my computer, but feel free to change
    them to achieve better accuracy or to reduce the memory consumption or the time
    required to train the network:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet非常灵活，因此您可以轻松地以多种方式配置它。然而，根据您计算机的硬件，您可能会遇到GPU的限制。以下是我计算机上使用的值，但请随意更改它们以实现更好的精度或减少内存消耗或训练网络所需的时间：
- en: '**Input and output resolution**: 160 X 160'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入和输出分辨率**: 160 X 160'
- en: '**Growth rate (number of channels added by each convolutional layer in a dense
    block)**: 12'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增长率（每个密集块中每个卷积层添加的通道数）**: 12'
- en: '**Number of dense blocks**: 11: 5 down, 1 to transition between down and up,
    and 5 up'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集块数量**: 11: 5个向下，1个用于在向下和向上之间过渡，5个向上'
- en: '**Number of convolutional blocks in each dense block**: 4'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个密集块中的卷积块数量**: 4'
- en: '**Batch size**: 4'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**: 4'
- en: '**Bottleneck layer in the dense blocks**: No'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集块中的瓶颈层**: 否'
- en: '**Compression factor**: 0.6'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩因子**: 0.6'
- en: '**Dropout**: Yes, 0.2'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**: 是的，0.2'
- en: We will define some functions that you can use to build FC-DenseNet and, as
    usual, you are invited to check out the full code on GitHub.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将定义一些函数，您可以使用它们来构建FC-DenseNet，并且，像往常一样，我们邀请您查看GitHub上的完整代码。
- en: 'The first function just defines a convolution with batch normalization:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个函数只是定义了一个带有批量归一化的卷积：
- en: '[PRE5]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Nothing special—we had a batch normalization before the ReLU activation, followed
    by a convolutional layer and the optional dropout.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有什么特别的——我们在ReLU激活之前有一个批量归一化，然后是一个卷积层和可选的Dropout。
- en: 'The next function defines a dense block using the previous method:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一个函数使用先前的方法定义了一个密集块：
- en: '[PRE6]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There is a lot going on:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发生了很多事情：
- en: '`num_layers` 3 X 3 convolutional layers, adding `growth_rate` channels every
    time. In addition, if `add_bottleneck_layer` is set, before each 3 X 3 convolutions
    it adds a 1 X 1 convolution to convert the number of channels in input to `4*
    growth_rate`; I did not use the bottleneck layer in my configuration, but you
    can.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_layers` 3 X 3卷积层，每次添加`growth_rate`通道。此外，如果`add_bottleneck_layer`被设置，则在每个3
    X 3卷积之前，它添加一个1 X 1卷积来将输入的通道数转换为`4* growth_rate`；在我的配置中我没有使用瓶颈层，但您可以使用。'
- en: It returns two outputs, where the first output, layer, is the concatenation
    of all the outputs of each convolution, including the input, and the second output,
    derived from `block_layers`, is the concatenation of all the outputs of each convolution,
    excluding the input.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回两个输出，其中第一个输出，layer，是每个卷积的所有输出的连接，包括输入，第二个输出，来自`block_layers`，是每个卷积的所有输出的连接，不包括输入。
- en: The reason why we need two outputs is because the down-sampling and the up-sampling
    path are a bit different. During down sampling, we include the input of the block,
    while during up sampling we don't; this is just to keep the size of the network
    and the computation time reasonable, as, in my case, without this change, the
    network would jump from 724 K parameters to 12 M!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个输出的原因是因为下采样路径和上采样路径略有不同。在下采样过程中，我们包括块的输入，而在上采样过程中则不包括；这只是为了保持网络的大小和计算时间合理，因为，在我的情况下，如果没有这个变化，网络将从724
    K参数跳变到12 M！
- en: 'The next function defines the transition layer that is used to reduce the resolution
    in the down-sampling path:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数定义了用于在下采样路径中降低分辨率的过渡层：
- en: '[PRE7]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It just creates a 1 X 1 convolution followed by an average pooling; if you choose
    to add a compression factor, then the number of channels will be reduced; I chose
    a compression factor of `0.6` because the network was too big without any compression
    and did not fit in the RAM of my GPU.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是创建了一个1 X 1的卷积，然后是一个平均池化；如果您选择添加压缩因子，则通道数将减少；我选择了压缩因子`0.6`，因为没有压缩的网络太大，无法适应我的GPU的RAM。
- en: 'The next method is the transition layer used to increase the resolution in
    the up-sampling path:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个方法是用于在上采样路径中增加分辨率的过渡层：
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It creates a deconvolution to increase the resolution, and it adds the skip
    connection, which is, of course, important in enabling us to also increase the
    effective resolution of the segmentation mask.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 它创建了一个反卷积来增加分辨率，并添加了跳过连接，这对于使我们能够增加分割掩码的有效分辨率当然很重要。
- en: Now that we have all the building blocks, it is just a matter of assembling
    the full network.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有构建块，剩下的只是组装完整的网络。
- en: Putting all the pieces together
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有部件组合在一起
- en: 'First, a note on the resolution: I chose 160 X 160, as that was basically the
    maximum that my laptop could do, in combination with the other settings. You can
    try a different resolution, but you will see that not all the resolutions are
    possible. In fact, depending on the number of dense blocks, you might need to
    use multiples of 16, 32, or 64\. Why is this? Simple. Let''s take an example,
    assuming that we will use 160 X 160\. If, during down sampling, you reduce the
    resolution 16 times (for example, you have 4 dense blocks, each one followed by
    a *transition-down* layer), then your intermediate resolution will be an integer
    number—in this case, 10 X 10\.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，关于分辨率的说明：我选择了160 X 160，因为这基本上是我笔记本电脑能做的最大分辨率，结合其他设置。你可以尝试不同的分辨率，但你将看到并非所有分辨率都是可能的。实际上，根据密集块的数量，你可能需要使用16、32或64的倍数。为什么是这样？简单来说，让我们举一个例子，假设我们将使用160
    X 160。如果在下采样过程中，你将分辨率降低16倍（例如，你有4个密集块，每个块后面都有一个*向下转换*层），那么你的中间分辨率将是一个整数——在这种情况下，10
    X 10。
- en: When you up sample 4 times, your resolution will grow 16 fold, so your final
    resolution will still be 160 X 160\. But if you start with 170 X 170, you will
    still end up with an intermediate resolution of 10 X 10, and up sampling it will
    produce a final resolution of 160 X 160! This is a problem, because you need to
    concatenate these outputs with the skip layers taken during down sampling, and
    if the two resolutions are different, then we cannot concatenate the layers and
    Keras will generate an error. As regards the ratio, it does not need to be a square
    and it does not need to match the ratio of your images.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你上采样4次时，你的分辨率将增长16倍，所以你的最终分辨率仍然是160 X 160。但如果你从170 X 170开始，你最终仍然会得到一个中间分辨率为10
    X 10的分辨率，上采样它将产生一个最终分辨率为160 X 160的分辨率！这是一个问题，因为你需要将这些输出与下采样期间取出的跳跃层连接起来，如果两个分辨率不同，那么我们无法连接层，Keras将生成一个错误。至于比例，它不需要是平方的，也不需要匹配你图像的比例。
- en: 'The next thing that we need to do is create the input for the neural network
    and the first convolutional layer, as the dense blocks assume that there is a
    convolution before them:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们需要做的是创建神经网络的输入和第一个卷积层的输入，因为密集块假设它们之前有一个卷积：
- en: '[PRE9]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I used a 7 X 7 convolution without max pooling, but feel free to experiment.
    You could use a bigger image and introduce a max pooling or an average pooling,
    or just create a bigger network, if you can train it at all.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一个没有最大池化的7 X 7卷积，但请随意实验。你可以使用更大的图像并引入最大池化或平均池化，或者如果你能训练它，也可以创建一个更大的网络。
- en: 'Now we can generate the down-sampling path:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成下采样路径：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We simply create all the groups that we want, five in my configuration, and
    for each group we add a dense layer and a transition-down layer, and we also record
    the skip connections.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地创建我们想要的全部组，在我的配置中是五个，对于每个组我们添加一个密集层和一个向下转换层，并且我们还记录跳跃连接。
- en: 'The following step builds the up-sampling path:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤构建上采样路径：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We reverse the skip connections because when going up, we encounter the skip
    connections in the opposite order, and we add a dense layer that is not followed
    by a transition down. This is called a *bottleneck layer*, as it has a low amount
    of information. Then we simply create the transition-up and dense layer corresponding
    to the down-sampling path.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们反转了跳跃连接，因为在向上传递时，我们会以相反的顺序遇到跳跃连接，并且我们添加了一个没有跟随向下转换的密集层。这被称为*瓶颈层*，因为它包含的信息量很少。然后我们简单地创建与下采样路径对应的向上转换和密集层。
- en: 'Now that we have the last part, let''s generate the output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最后一部分，让我们生成输出：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We simply add a 1 X 1 convolution and a softmax activation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地添加一个1 X 1卷积和一个softmax激活。
- en: The difficult part is done, but we need to learn how to feed the input to the
    network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 困难的部分已经完成，但我们需要学习如何将输入馈送到网络中。
- en: Feeding the network
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向网络输入数据
- en: Feeding the neural network is not too difficult, but there are some practical
    complications because the network is quite demanding, and loading all the images
    in RAM might not be feasible, so we are going to use a generator. However, this
    time, we will also add a simple data augmentation—we will mirror half of the images.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 向神经网络输入数据并不太难，但有一些实际上的复杂性，因为网络要求很高，将所有图像加载到RAM中可能不可行，所以我们将使用一个生成器。然而，这次，我们还将添加一个简单的数据增强——我们将镜像一半的图像。
- en: 'But first, we will define a hierarchy where we have all the images in subdirectories
    of the `dataset` folder:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们将定义一个层次结构，其中所有图像都位于`dataset`文件夹的子目录中：
- en: '`rgb` contains the images.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rgb`包含图像。'
- en: '`seg` contains the segmented and colored images.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seg`包含分割并着色的图像。'
- en: '`seg_raw` contains the images in raw format (numeric labels in the red channel).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seg_raw`包含原始格式的图像（红色通道中的数值标签）。'
- en: This means that when given an image in the `rgb` folder, we can get the corresponding
    raw segmentation by just changing the path to `seg_raw`. This is useful.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当给定一个`rgb`文件夹中的图像时，我们只需更改路径到`seg_raw`即可获取相应的原始分割。这很有用。
- en: 'We will define a generic generator that is usable for data augmentation; our
    approach will be the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个通用的生成器，可用于数据增强；我们的方法如下：
- en: The generator will receive a list of IDs—in our case, the paths of the `rgb`
    images.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器将接收一个ID列表——在我们的案例中，是`rgb`图像的路径。
- en: The generator will also receive two functions—one that, given an ID, can generate
    an image and another that, given an ID, can generate the corresponding label (changing
    the path to `seg_raw`).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器还将接收两个函数——一个函数给定一个ID可以生成一个图像，另一个函数给定一个ID可以生成相应的标签（更改路径到`seg_raw`）。
- en: We will provide the index in the epoch to help with data augmentation.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在每个epoch中提供索引以帮助数据增强。
- en: 'This is the generic generator:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通用的生成器：
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is similar to what we have already seen in [*Chapter 8*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182),
    *Behavioral Cloning*. It goes through all the IDs and obtains the images and labels
    for the batch; the main difference is that we pass two additional parameters to
    the functions, in addition to the current ID:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在[*第8章*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182)中已经看到的类似，*行为克隆*。它遍历所有ID并获取批次的图像和标签；主要区别是我们向函数传递了两个额外的参数，除了当前ID之外：
- en: A flag specifying whether we want to enable data augmentation
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标志，指定我们是否想要启用数据增强
- en: The current index in the epoch to tell the function where we are
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前epoch中的索引以告诉函数我们的位置
- en: 'Now it will be relatively easy to write a function that returns the images:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将相对容易编写一个返回图像的函数：
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We load the image and resize it using the nearest-neighbor algorithm, as already
    discussed. This way, half of the time the image will be flipped.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用最近邻算法加载图像并调整大小，正如已经讨论过的。这样，一半的时间图像将被翻转。
- en: 'This is the function to extract the labels:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提取标签的函数：
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As expected, to get the label, we need to change the path from `rgb` to `seg_raw`,
    whereas when you augment the data in classifiers, the label does not change. In
    this case, the mask needs to be augmented in the same way, so we still need to
    mirror it when we also mirror the `rgb` image.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，要获取标签，我们需要将路径从`rgb`更改为`seg_raw`，而在分类器中对数据进行增强时，标签不会改变。在这种情况下，掩码需要以相同的方式进行增强，因此当我们镜像`rgb`图像时，我们仍然需要镜像它。
- en: 'The trickier part is to generate the correct label because the raw format is
    not suitable. Normally, in a classifier, you provide a one-hot encoded label,
    meaning that if you have ten possible label values, every label will be converted
    to a vector of ten elements, where only one element is `1` and all the others
    are `0`. Here, we need to do the same, but for the whole image and at pixel level:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 更具挑战性的是生成正确的标签，因为原始格式不适合。通常，在一个分类器中，你提供一个one-hot编码的标签，这意味着如果你有十个可能的标签值，每个标签都会转换为一个包含十个元素的向量，其中只有一个元素是`1`，其余都是`0`。在这里，我们需要做同样的事情，但针对整个图像和像素级别：
- en: Our label is not a single image but 13 images (as we have 13 possible label
    values).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的标签不是一个单独的图像，而是13个图像（因为我们有13个可能的标签值）。
- en: Each image is dedicated to a single label.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个图像都对应一个单独的标签。
- en: The pixels of an image are `1` only where that label is present in the segmentation
    mask and `0` elsewhere.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的像素只有在分割掩码中存在该标签时才为`1`，其他地方为`0`。
- en: In practice, we apply one-hot encoding at pixel level.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，我们在像素级别应用one-hot编码。
- en: 'This is the resulting code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的代码：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: At the beginning of the method, we create an image with 13 channels, and then
    we precompute the one-hot encoding (which contains 13 values) to speed up the
    computation. Then we simply apply the one-hot encoding to each pixel, based on
    the value of the red channel, which is where Carla stores the raw segmentation
    value.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在方法的开始阶段，我们创建了一个包含13个通道的图像，然后我们预先计算了一个包含13个值的one-hot编码（用于加速计算）。然后我们简单地根据红色通道的值将one-hot编码应用于每个像素，红色通道是卡拉拉存储原始分割值的地方。
- en: Now you can start the training. You might consider running it overnight, as
    it might take a while, especially if you use dropout or if you decide to record
    additional images.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以开始训练了。你可能考虑让它过夜运行，因为它可能需要一段时间，特别是如果你使用dropout或者决定记录额外的图像。
- en: 'This is the graph with the training:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练的图表：
- en: '![Figure 9.9 – Training FC-DenseNet](img/Figure_9.9_B16322.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 训练FC-DenseNet](img/Figure_9.9_B16322.jpg)'
- en: Figure 9.9 – Training FC-DenseNet
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 训练FC-DenseNet
- en: It's not great, because the validation loss has many spikes, which indicates
    that the training is unstable, and sometimes the loss increases quite a lot. Ideally,
    we would like a smooth, decreasing curve as this means that the loss decreases
    at every iteration. It would probably benefit from a bigger batch size.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不理想，因为验证损失有很多峰值，这表明训练不稳定，有时损失增加得相当多。理想情况下，我们希望有一个平滑的下降曲线，这意味着损失在每次迭代时都会减少。可能需要更大的批量大小。
- en: 'But the overall performance is not bad:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 但整体表现还不错：
- en: '[PRE17]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The validation accuracy is above 90%, which is promising.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 验证准确率超过90%，这是一个好兆头。
- en: Let's now see how it behaves with the test dataset.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看它在测试数据集上的表现。
- en: Running the neural network
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行神经网络
- en: Running inference on the network is no different than the usual process, but
    we need to convert the output to a colored image that we can actually understand
    and use.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络上运行推理与常规过程没有不同，但我们需要将输出转换为我们可以理解和使用的有色图像。
- en: 'To do this, we need to define a palette of 13 colors that we are going to use
    to show the labels:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要定义一个13种颜色的调色板，我们将使用它来显示标签：
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And now we just need to derive two images using these colors—the raw segmentation
    and the colored segmentation. The following function does both:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要使用这些颜色导出两张图像——原始分割和彩色分割。以下函数执行这两个操作：
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You might remember that our output is an image with 13 channels, one per label.
    So you can see that we get the label from these channels using `argmax`; this
    label is used directly for the raw image, where it is stored in the red channel,
    whereas for the colored segmentation image, we store the color from the palette,
    using `label` as index, exchanging the blue and the red channel because OpenCV
    is in BGR.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得我们的输出是一个13通道的图像，每个标签一个通道。所以你可以看到我们使用`argmax`从这些通道中获取标签；这个标签直接用于原始图像，其中它存储在红色通道中，而对于彩色分割图像，我们使用调色板中的颜色，使用`label`作为索引，交换蓝色和红色通道，因为OpenCV是BGR格式。
- en: Let's see how it performs, bearing in mind that these images are very similar
    to the ones that the network saw during training.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的表现如何，记住这些图像与网络在训练期间看到的图像非常相似。
- en: 'The following is the result for one image, with other versions of the segmented
    picture:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一张图像的结果，以及其他分割图像的版本：
- en: '![Figure 9.10 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.10_B16322.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 从左到右：RGB图像、Carla的真实情况、彩色分割掩码和叠加分割](img/Figure_9.10_B16322.jpg)'
- en: 'Figure 9.10 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 从左到右：RGB图像、Carla的真实情况、彩色分割掩码和叠加分割
- en: 'As you can see from the image, it is not perfect, but it does a good job: the
    road is detected properly and the guard rail and trees are kind of fine, and the
    pedestrian is detected, but not very well. Surely we can improve this.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从图像中看到的，它并不完美，但做得很好：道路被正确检测到，护栏和树木也相当不错，行人也被检测到，但不是很好。当然我们可以改进这一点。
- en: 'Let'' s look at another problematic image:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一张有问题的图像：
- en: '![Figure 9.11 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.11_B16322.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图9.11 – 从左到右：RGB图像、Carla的真实情况、彩色分割掩码和叠加分割](img/Figure_9.11_B16322.jpg)'
- en: 'Figure 9.11 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 从左到右：RGB图像、Carla的真实情况、彩色分割掩码和叠加分割
- en: The previous image is quite challenging, as the road is dark and the cars are
    also dark, but the network does a decent job of detecting the road and the car
    (though the shape is not great). It does not detect the lane line, but it is actually
    not visible in the road, so here the ground truth is too optimistic.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图像相当具有挑战性，因为道路昏暗，汽车也是暗的，但网络在检测道路和汽车方面做得相当不错（尽管形状不是很好）。它没有检测到车道线，但实际上在道路上并不明显，所以这里的真实情况过于乐观。
- en: 'Let''s see another example:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看另一个例子：
- en: '![Figure 9.12 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.12_B16322.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – 从左到右：RGB图像、Carla的真实情况、彩色分割掩码和叠加分割](img/Figure_9.12_B16322.jpg)'
- en: 'Figure 9.12 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 从左至右：RGB图像、Carla的地面真实情况、彩色分割掩码和叠加分割
- en: 'Here too, the result is not bad: the road and the trees are very well detected,
    and the traffic sign is decently detected, but it does not see the lane line,
    which is challenging but visible.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，结果也不算差：道路和树木都被很好地检测到，交通标志也被相当好地检测到，但它没有看到车道线，这是一个具有挑战性但可见的线条。
- en: 'Just to be sure that it can indeed detect the lane line, let''s look at a less
    challenging image:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保它确实可以检测到车道线，让我们看看一个不那么具有挑战性的图像：
- en: '![Figure 9.13 – From the left: RGB image, colored segmentation mask, and segmentation
    in overlay](img/Figure_9.13_B16322.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – 从左至右：RGB图像，彩色分割掩码和叠加分割](img/Figure_9.13_B16322.jpg)'
- en: 'Figure 9.13 – From the left: RGB image, colored segmentation mask, and segmentation
    in overlay'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 从左至右：RGB图像、彩色分割掩码和叠加分割
- en: 'I don''t have the ground truth for this image, which also means that although
    it is taken from the same batch as the training dataset, it might be a bit more
    different. Here, the network behaves very well: the road, lane line, sidewalk,
    and vegetation are all very well detected.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有这张图像的地面真实情况，这也意味着尽管它是从与训练数据集相同的批次中拍摄的，但它可能略有不同。在这里，网络表现非常好：道路、车道线、人行道和植被都被很好地检测到。
- en: We have seen that the network performs decently, but surely we should add many
    more samples, both from the same track and also from other tracks, and with different
    kinds of weather. Unfortunately, this means that the training would be much more
    demanding.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到该网络表现尚可，但当然我们应该添加更多样本，既包括同一轨迹的样本，也包括其他轨迹的样本，以及不同天气条件下的样本。不幸的是，这意味着训练将更加耗时。
- en: Nevertheless, this kind of result with around a thousand images is, in my opinion,
    a good result. But what if you cannot get enough samples in the dataset? Let's
    learn a small trick.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我认为大约有一千张图像的这种结果是一个好结果。但如果你无法在数据集中获得足够的样本呢？让我们学习一个小技巧。
- en: Improving bad semantic segmentation
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进不良语义分割
- en: Sometimes things don't go as you hope. Maybe getting a lot of samples for the
    dataset is too expensive, or it takes too much time. Or perhaps there is no time
    because you need to try to impress some investors, or there is a technical issue
    or another type of problem, and you are stuck with a bad network and a few minutes
    to fix it. What can you do?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候事情不会像你希望的那样进行。也许为数据集获取大量样本成本太高，或者花费太多时间。或者可能没有时间，因为你需要尝试给一些投资者留下深刻印象，或者存在技术问题或其他类型的问题，你被一个不良网络和几分钟的时间困住了。你能做什么？
- en: Well, there is a small trick that can help you; it will not transform a bad
    network into a good one, but it can nevertheless be better than nothing.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，有一个小技巧可以帮助你；它不会将一个不良网络变成一个良好的网络，但它仍然可以比什么都没有好。
- en: 'Let''s look at an example from a bad network:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个来自不良网络的例子：
- en: '![Figure 9.14 – Badly trained network](img/Figure_9.14_B16322.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – 恶劣训练的网络](img/Figure_9.14_B16322.jpg)'
- en: Figure 9.14 – Badly trained network
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – 恶劣训练的网络
- en: 'It has a validation accuracy of around 80%, and it has been trained with around
    500 images. It''s quite bad, but it looks even worse than what it really is because
    of the areas that are full of dots, where the network seems to not be able to
    decide what it is looking at. Can we fix this with some postprocessing? Yes, we
    can. You might remember from [*Chapter 1*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017),
    *OpenCV Basics and Camera Calibration*, that OpenCV has several algorithms for
    blurring, and one in particular, the median blur, has a very interesting characteristic:
    it selects the median of the colors encountered, so it emits only colors that
    are already present in the few pixels that it analyzes, and it is very effective
    at reducing *salt and pepper* noise, which is what we are experiencing. So, let''s
    see the result of applying this to the previous image:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 它的验证准确率大约为80%，并且大约使用了500张图像进行训练。这相当糟糕，但由于满是点的区域，它看起来比实际情况更糟糕，因为这些区域网络似乎无法确定它在看什么。我们能通过一些后处理来修复这个问题吗？是的，我们可以。你可能还记得从[*第1章*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017)，*OpenCV基础和相机标定*，OpenCV有几种模糊算法，特别是中值模糊，它有一个非常有趣的特性：它选择遇到的颜色的中值，因此它只发出它在分析的少数像素中已经存在的颜色，并且它非常有效地减少*盐和胡椒*噪声，这正是我们正在经历的。所以，让我们看看将这个算法应用到之前图像上的结果：
- en: '![Figure 9.15 – Badly trained network, from the left: RGB image, colored segmentation,
    segmentation corrected with media blur (three pixels), and segmentation in overlay](img/Figure_9.15_B16322.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图9.15 – 训练不良的网络，从左到右：RGB图像，彩色分割，使用媒体模糊（三个像素）校正的分割，以及叠加分割](img/Figure_9.15_B16322.jpg)'
- en: 'Figure 9.15 – Badly trained network, from the left: RGB image, colored segmentation,
    segmentation corrected with media blur (three pixels), and segmentation in overlay'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 训练不良的网络，从左到右：RGB图像，彩色分割，使用媒体模糊（三个像素）校正的分割，以及叠加分割
- en: 'As you can see, while far from perfect, it makes the image more usable. And
    it is only one line of code:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虽然远非完美，但它使图像更易于使用。而且这只是一行代码：
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: I used three pixels, but you can use more, if required. I hope you don't find
    yourself in a position where your network underperforms, but if you do, then this
    will surely be worth a try.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了三个像素，但如果需要，你可以使用更多。我希望你不会发现自己处于网络表现不佳的情况，但如果确实如此，那么这肯定值得一试。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations! You completed the final chapter on deep learning.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了关于深度学习的最后一章。
- en: We started this chapter by discussing what semantic segmentation means, then
    we talked extensively about DenseNet and why it is such a great architecture.
    We quickly talked about using a stack of convolutional layers to implement semantic
    segmentation, but we focused on a more efficient way, which is using DenseNet
    after adapting it to this task. In particular, we developed an architecture similar
    to FC-DenseNet. We collected a dataset with the ground truth for semantic segmentation,
    using Carla, and then we trained our neural network on it and saw how it performed
    and when detecting roads and other objects, such as pedestrians and sidewalks.
    We even discussed a trick to improve the output of a bad semantic segmentation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章开始时讨论了语义分割的含义，然后我们广泛地讨论了DenseNet及其为何是一个如此出色的架构。我们简要地提到了使用卷积层堆叠来实现语义分割，但我们更关注一种更有效的方法，即在适应此任务后使用DenseNet。特别是，我们开发了一个类似于FC-DenseNet的架构。我们使用Carla收集了一个带有语义分割真实值的数据库，然后我们在其上训练我们的神经网络，并观察了它的表现以及它在检测道路和其他物体，如行人和人行道时的表现。我们甚至讨论了一个提高不良语义分割输出的技巧。
- en: This chapter was quite advanced, and it required a good understanding of all
    the previous chapters about deep learning. It has been quite a ride, and I think
    it is fair to say that this has been a *dense* chapter. Now that you have a good
    knowledge of how to train a network to recognize what is present in front of a
    car, it is time to take control of the car and make it steer.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容相当高级，需要很好地理解所有关于深度学习的先前章节。这是一段相当刺激的旅程，我认为可以说这是一章内容丰富的章节。现在你已经很好地了解了如何训练一个网络来识别汽车前方的物体，是时候控制汽车并让它转向了。
- en: Questions
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'After reading this chapter, you will be able to answer the following questions:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章后，你将能够回答以下问题：
- en: What is a distinguished characteristic of DenseNet?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DenseNet的一个显著特征是什么？
- en: What is the name of the family architecture such as inspired the authors of
    DenseNet?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像DenseNet的作者所受到启发的家族架构叫什么名字？
- en: What is FC-DenseNet?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是FC-DenseNet？
- en: Why do we say that FC-DenseNet is U-shaped?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么说FC-DenseNet是U形的？
- en: Do you need a fancy architecture like DenseNet to perform semantic segmentation?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要像DenseNet这样的花哨架构来执行语义分割吗？
- en: If you have a neural network that performs poorly at semantic segmentation,
    is there a quick fix that you can use sometimes, if you have no other options?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有一个在语义分割上表现不佳的神经网络，有没有一种快速修复方法，你可以在没有其他选择时使用？
- en: What are skip connections used for in FC-DenseNet and other U-shaped architectures?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在FC-DenseNet和其他U形架构中，跳过连接用于什么？
- en: Further reading
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: DenseNet paper ([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993))
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet论文([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993))
- en: FC-DenseNet paper ([https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326))
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC-DenseNet论文([https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326))
