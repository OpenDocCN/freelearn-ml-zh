- en: '*Chapter 9*: Semantic Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is probably the most advanced chapter concerning deep learning, as we will
    go as far as classifying an image at a pixel level with a technique called semantic
    segmentation. We will use plenty of what we have learned so far, including data
    augmentation with generators.
  prefs: []
  type: TYPE_NORMAL
- en: We will study a very flexible and efficient neural network architecture called
    DenseNet in great detail, as well as its extension for semantic segmentation,
    FC-DenseNet, and then we will write it from scratch and train it with a dataset
    built with Carla.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you will find this chapter inspiring and challenging. And be prepared
    for a long training session because our task can be quite demanding!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing semantic segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding DenseNet for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation with CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting DenseNet for semantic segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the blocks of FC-DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving bad semantic segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to use the code explained in this chapter, you will need to have
    the following tools and modules installed:'
  prefs: []
  type: TYPE_NORMAL
- en: The Carla simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV-Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GPU (recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars](https://github.com/PacktPublishing/Hands-On-Computer-Vision-for-Self-Driving-Cars).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3jquo3v](https://bit.ly/3jquo3v)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing semantic segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we implemented several classifiers, where we provided
    an image as input and the network said what it was. This can be excellent in many
    situations, but to be very useful, it usually needs to be combined with a method
    that can identify the region of interest. We did this in [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158),
    *Detecting Pedestrians and Traffic Lights*, where we used SSD to identify a region
    of interest with a traffic light and then our neural network was able to tell
    the color. But even this would not be very useful to us, because the regions of
    interest produced by SSD are rectangles, and therefore a network telling us that
    there is a road basically as big as the image would not provide much information:
    is the road straight? Is there a turn? We cannot know. We need more precision.'
  prefs: []
  type: TYPE_NORMAL
- en: If object detectors such as SSD brought classification to the next level, now
    we need to reach the level after that, and maybe more. In fact, we want to classify
    every pixel of the image, which is called **semantic** **segmentation**, and is
    quite a demanding task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this better, let''s look at an example taken from Carla. The
    following is the original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A frame from Carla](img/Figure_9.1_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – A frame from Carla
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the same frame produced by the semantic segmentation camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Semantic segmentation](img/Figure_9.2_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Semantic segmentation
  prefs: []
  type: TYPE_NORMAL
- en: This is simply great! Not only is the image very simplified, but every color
    has a specific meaning—the road is purple, the sidewalk is magenta, the trees
    are dark green, the lane lines are bright green, and so on. Just to set your expectations,
    we will not be able to achieve such a perfect result, and we will also work at
    a much lower resolution, but we will still achieve interesting results.
  prefs: []
  type: TYPE_NORMAL
- en: To be precise, this image is not the real output of the network, but it has
    been converted to show colors; `rgb(7,0,0)`, where the `7` will then be converted
    to purple.
  prefs: []
  type: TYPE_NORMAL
- en: Carla's ability to create images with semantic segmentation is extremely helpful,
    and can allow you to experiment at will, without relying on premade and limited
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start to collect the dataset, let's discuss what the plan is in a
    bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Defining our goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to use a dataset collected by us to train a neural network from
    scratch to perform semantic segmentation so that it can detect roads, sidewalks,
    pedestrians, traffic signs, and more at a pixel level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps required for this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating the dataset**: We will use Carla to save the original image, the
    raw segmented image (black image with dark colors), and the converted image to
    use better colors for our convenience.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Building the neural network**: We will study an architecture called **DenseNet**
    in great depth, and then we will see how networks performing semantic segmentations
    are usually structured. After this, we will look at an adaptation of DenseNet
    for semantic segmentation called **FC-DenseNet**, and we will implement it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training the neural network**: Here, we will train the network and evaluate
    the result; the training could easily take several hours.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now see the changes required to collect the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already seen how to record images from Carla and modify `manual_control.py`
    in [*Chapter 8*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182), *Behavioral
    Cloning*, and you could do that, but we have an issue: we really want the RGB
    and raw cameras to be the exact same frame to avoid movements that would make
    our dataset less effective. This problem can be solved using synchronous mode,
    where Carla waits for all the sensors to be ready before sending them to the client,
    which ensures perfect correspondence between the three cameras that we are going
    to save: RGB, raw segmentation, and colored segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: This time, we will modify another file, `synchronous_mode.py`, as it is more
    suitable for this task.
  prefs: []
  type: TYPE_NORMAL
- en: I will be specifying where each block of code is located in the file, but it
    is recommended that you go to GitHub and check out the full code there.
  prefs: []
  type: TYPE_NORMAL
- en: 'This file is much simpler than `manual_control.py,` and there are basically
    two interesting parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CarlaSyncMode`, a class that enables the synchronized mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main()`, which initializes the world (the objects representing the track,
    the weather, and the vehicles) and the cameras, and then moves the car, drawing
    it on the screen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you run it, you will see that this file self-drives the car, possibly at
    a very high speed, merging the RGB camera and semantic segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Output of synchronous_mode.py](img/Figure_9.3_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Output of synchronous_mode.py
  prefs: []
  type: TYPE_NORMAL
- en: Don't be too impressed by the self-driving algorithm because, while very handy
    for us, it's also very limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Carla has a high number of **waypoints**, which are 3D-directed points. These
    points, which number in the thousands per track, follow the road and are taken
    from the OpenDRIVE map; OpenDRIVE is an open file format that Carla uses to describe
    the road. These points are oriented with the road, so if you move the car toward
    the points while also applying the orientations of these points, the car effectively
    moves as if it was self-driving. Brilliant! Until you add cars and walkers; then
    you start to get frames like these ones, because the car will move into other
    vehicles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Frames with a collision](img/Figure_9.4_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Frames with a collision
  prefs: []
  type: TYPE_NORMAL
- en: This might be a bit surprising when you see it, but it is still fine for our
    task, so it is not a big problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how we need to modify `synchronous_mode.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying synchronous_mode.py
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the following changes need to be made in the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will change the camera position to be the same one that we used with the
    behavioral cloning, although this is not required. This involves changing the
    two calls to `carla.Transform()` with this line (it''s the same line for both
    the locations):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Just after moving the car, we can save the RGB camera and the raw semantic
    segmentation image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the code, the line immediately after calls `image_semseg.convert()` to convert
    the raw image to the colored version, according to the CityScapes palette; now
    we can save the image with semantic segmentation so that it is properly colored:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are almost done. We just need to write the `save_img()` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first lines of the preceding steps of code convert the image of Carla from
    a buffer to a NumPy array and select the first three channels, dropping the fourth
    one (the transparency channel). Then we resize the image to 160 X 160 using the
    `INTER_NEAREST` algorithm to avoid smoothing the image while resizing.
  prefs: []
  type: TYPE_NORMAL
- en: The last line saves the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip: Resize segmentation masks using the nearest-neighbor algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Are you wondering why we resize using `INTER_NEAREST`, the nearest-neighbor
    algorithm, which is the most basic interpolation? The reason is that it does not
    interpolate the color but chooses the color of the pixel closer to the interpolated
    position, and this is important for the raw semantic segmentation. For example,
    let's say we are scaling four pixels down to one. Two of the pixels have a value
    of 7 (roads) and the other two pixels have a value of 9 (vegetation). We might
    be happy with the output being either 7 or 9, but we surely don't want it to be
    8 (sidewalks)!
  prefs: []
  type: TYPE_NORMAL
- en: But for RGB and colored segmentation, you can use more advanced interpolations.
  prefs: []
  type: TYPE_NORMAL
- en: This is everything that is required to collect images. The 160 X 160 resolution
    is the one I chose for my network, and we will discuss this choice later. If you
    use another resolution, please adjust the settings accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: You can also save at full resolution, but then you have to either write a program
    to change it later or do this when you train the neural network, and since we
    will be using a generator, this means that we need to use this convention for
    every image and for every epoch—so more than 50,000 times in our case—plus it
    will make loading the JPEG slower, which also needs to be performed 50,000 times
    in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the dataset, we can build the neural network. Let's start with
    the architecture of DenseNet, which is the foundation of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DenseNet for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DenseNet** is a fascinating architecture of neural networks that is designed
    to be flexible, memory efficient, effective, and also relatively simple. There
    are really a lot of things to like about DenseNet.'
  prefs: []
  type: TYPE_NORMAL
- en: The DenseNet architecture is designed to build very deep networks, solving the
    problem of the *vanishing gradient* with techniques derived from ResNet. Our implementation
    will reach 50 layers, but you can easily build a deeper network. In fact, Keras
    has three types of DenseNet trained on ImageNet, with 121, 169, and 201 layers,
    respectively. DenseNet also solves the problem of *dead neurons*, when you have
    neurons that are basically not active.The next section will show a high-level
    overview of DenseNet.
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet from a bird's-eye view
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the moment, we will focus on DenseNet as a classifier, which is not what
    we are going to implement, but it is useful as a concept to start to understand
    it. The high-level architecture of DenseNet is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – High-level view of DenseNet as a classifier, with three dense
    blocks](img/Figure_9.5_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – High-level view of DenseNet as a classifier, with three dense blocks
  prefs: []
  type: TYPE_NORMAL
- en: The figure only shows three dense blocks, but there are usually a few more.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the diagram, it is quite simple to understand the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The input is an RGB image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an initial 7 X 7 convolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a **dense block**, which contains some convolutions. We will describe
    this in depth soon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every **dense block** is followed by a 1 X 1 convolution and an average pooling,
    which reduces the image size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last **dense bloc**k is followed directly by the average pooling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end, there is a dense (fully connected) layer with a **softmax**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1 X 1 convolution can be used to reduce the number of channels to speed
    up the computations. The 1 X 1 convolution followed by the average pooling is
    called a **transition layer** by the DenseNet paper, and when the number of channels
    is reduced, they call the resulting network **DenseNet-C**, where the *C* means
    *compression* and the convolution layer is called the **compression layer**.
  prefs: []
  type: TYPE_NORMAL
- en: As a classifier, this high-level architecture is not particularly remarkable,
    but as you might have guessed, the innovation is in the dense blocks, which are
    the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dense blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dense blocks give the name to the architecture and are the main part of
    DenseNet; they contain the convolutions, and you usually have several of them,
    depending on the resolution, the precision that you want to achieve, and the performance
    and training time. Please note that they are unrelated to the dense layers that
    we have already met.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dense blocks are blocks that we can repeat to increase the depth of the
    network, and they achieve the following goals:'
  prefs: []
  type: TYPE_NORMAL
- en: They solve the *vanishing gradient* problem, allowing us to make very deep networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are very efficient, using a relatively small number of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They solve the *dead neurons* problem, meaning that all the convolutions contribute
    to the final result, and we don't waste CPU and memory on neurons that are basically
    useless.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are big goals, goals that many architectures struggle to achieve. So
    let''s see how DenseNet can do what many other architectures cannot. The following
    is a dense block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Dense block with five convolutions, plus the input](img/Figure_9.6_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Dense block with five convolutions, plus the input
  prefs: []
  type: TYPE_NORMAL
- en: This is indeed remarkable, and requires some explanation. Perhaps you remember
    **ResNet** from [*Chapter 7*](B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158),
    *Detecting Pedestrians and Traffic Lights*, a neural network built by Microsoft
    that had a feature called *skip connections*, shortcuts that could allow a layer
    to skip other layers, helping to solve the vanishing gradient problem and therefore
    achieve deeper networks. In fact, some versions of ResNet can have more than 1,000
    layers!
  prefs: []
  type: TYPE_NORMAL
- en: 'DenseNet brings this concept to the extreme, as inside every dense block, each
    convolutional layer is connected and concatenated to the other convolutional layers
    of the same block! This has two very important implications:'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of the skip connections clearly achieves the same effect of the
    skip connections in ResNet, making deeper networks much easier to train.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to the skip connections, the features of each layer can be reused by
    the following layers, making the network very efficient and greatly reducing the
    number of parameters compared to other architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The feature''s reuse can be better appreciated with the following diagram,
    which explains the effect of the dense block, focusing on the channels instead
    of the skip connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Effects of the skip connections on a dense block with five layers
    and a growth rate of three](img/Figure_9.7_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Effects of the skip connections on a dense block with five layers
    and a growth rate of three
  prefs: []
  type: TYPE_NORMAL
- en: The first horizontal line shows the new features that are added by each convolution,
    while all the other horizontal lines are convolutions provided by the previous
    layers, and they are reused thanks to the skip connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing the diagram, where the content of each layer is a column, we can
    see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer has 5 channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 1 adds 3 new channels and reuses the input, so it effectively has 8 channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 2 adds 3 new channels and reuses the input and layer 1, so it effectively
    has 11 channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This continues until layer 5, which adds 3 new channels and reuses the input
    and layers 1, 2, 3, and 4, so it effectively has 20 channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is very powerful, because the convolutions can reuse the previous layers,
    adding only some new channels, with the result that the network is compact and
    efficient. In addition, these new channels are going to provide new information
    because they have direct access to the previous layers, which means that they
    won't somehow replicate the same information or lose contact with what was already
    computed a few layers before. The number of new channels added on each layer is
    called the **growth rate**; it was **3** in our example, while in real life it
    will probably be 12, 16, or more.
  prefs: []
  type: TYPE_NORMAL
- en: For the dense blocks to work, all the convolution need to use padding with the
    value **same**, which, as we know, keeps the resolution unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Every dense block is followed by a transition layer with average pooling, which
    reduces the resolution; as skip connections require the resolution of the convolutions
    to be the same, this means that we can only have skip connections inside the same
    dense block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer of a dense block is composed of the following three components:'
  prefs: []
  type: TYPE_NORMAL
- en: A batch normalization layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ReLU activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The convolution block can therefore be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is a different style to writing Keras code, where instead of using the
    model object to describe the architecture, you build a chain of layers; this is
    the style to use with skip connections, as you need the flexibility to be able
    to use the same layer more than once.
  prefs: []
  type: TYPE_NORMAL
- en: In DenseNet, at the beginning of each dense block, you can add an optional 1
    X 1 convolution, with the goal of reducing the number of channels of input and
    therefore improving performance; when this 1 X 1 convolution is present, we call
    it a **bottleneck layer** (because the number of channels is reduced), and the
    network is called **DenseNet-B**. When the network has both the bottleneck and
    the compression layer, it is called **DenseNet-BC**. As we already know, the ReLU
    activation will add nonlinearity, so having many layers can result in a network
    that can learn very complex functions, which we will definitely need for semantic
    segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering about dropout, DenseNet can function well without it; one
    reason for this is the presence of normalization layers, which already provide
    a regularization effect, and so their combination with dropout is not particularly
    effective. In addition, the presence of dropout usually requires us to increase
    the size of the network, which is against the goals of DenseNet. That said, the
    original paper mentions using dropouts after the convolutional layers, when there
    is no data augmentation, and I think that, by extension, if there are not many
    samples, dropout can help.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of how DenseNet works, let's learn how to
    make a neural network for semantic segmentation, which will pave the way to a
    later section about how to adapt DenseNet to perform semantic segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting images with CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical semantic segmentation task receives as input an RGB image and needs
    to output an image with the raw segmentation, but this solution could be problematic.
    We already know that classifiers generate their results using *one-hot encoded*
    labels, and we can do the same for semantic segmentation: instead of generating
    a single image with the raw segmentation, the network can create a series of *one-hot
    encoded* images. In our case, as we need 13 classes, the network will output 13
    RGB images, one per label, with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: One image describes only one label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pixels belonging to the label have a value of 1 in the red channel, while
    all the other pixels are marked as 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each given pixel can be `1` only in one image; it will be `0` in all the remaining
    images. This is a difficult task, but it does not necessarily require particular
    architectures: a series of convolutional layers with *same* padding can do it;
    however, their cost quickly becomes computationally expensive, and you might also
    have problems fitting the model in memory. As a consequence, there has been a
    push to improve this architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: As we already know, a typical way to solve this problem is to use a form of
    pooling to reduce the resolution while adding layers and channels. This works
    for classification, but as we need to generate an image with the same resolution
    as the input, we need a way to *go back* to that resolution. One way to do this
    is by using a **transposed convolution**, also called **deconvolution**, which
    is a transformation going in the opposite direction of a convolution that is able
    to increase the resolution of the output.
  prefs: []
  type: TYPE_NORMAL
- en: If you add a series of convolutions and a series of deconvolutions, the resulting
    network is U-shaped, with the left side starting from the input, adding convolutions
    and channels while reducing the resolution, and the right side having a series
    of deconvolutions that bring the resolution back to the original one. This can
    be more efficient than using only convolutions of the same size, but the resulting
    segmentation will effectively have a much lower resolution than the original input.
    To solve this problem, it's possible to introduce skip connections from the left
    side to the right to give the network enough information to restore the correct
    resolution not only formally, with the number of pixels, but also practically,
    at the mask level.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can look at how to apply these ideas to DenseNet.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting DenseNet for semantic segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DenseNet is very suitable for semantic segmentation because of its efficiency,
    accuracy, and abundance of skip layers. In fact, using DenseNet for semantic segmentation
    proves to be effective even when the dataset is limited and when a label is underrepresented.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use DenseNet for semantic segmentation, we need to be able to build the
    right side of the *U* network, which means that we need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A way to increase the resolution; if we call the transition layers of DenseNet
    *transition down*, then we need *transition-up* layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to build the skip layers to join the left and right side of the *U*
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our reference network is FC-DenseNet, also known as one hundred layers tiramisu,
    but we are not trying to reach 100 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we want to achieve an architecture similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Example of FC-DenseNet architecture](img/Figure_9.8_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Example of FC-DenseNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal red arrows connecting the concatenation layers in *Figure 9.8*
    are the skip connections used to improve the resolution of the output, and they
    can only work if the output of the corresponding dense block on the left is the
    same resolution as the input of the corresponding dense block on the right; this
    is achieved using the transition-up layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how to implement FC-DenseNet.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the blocks of FC-DenseNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DenseNet is very flexible, so you can easily configure it in many ways. However,
    depending on the hardware of your computer, you might hit the limits of your GPU.
    The following are the values that I used on my computer, but feel free to change
    them to achieve better accuracy or to reduce the memory consumption or the time
    required to train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input and output resolution**: 160 X 160'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Growth rate (number of channels added by each convolutional layer in a dense
    block)**: 12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of dense blocks**: 11: 5 down, 1 to transition between down and up,
    and 5 up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of convolutional blocks in each dense block**: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bottleneck layer in the dense blocks**: No'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression factor**: 0.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: Yes, 0.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will define some functions that you can use to build FC-DenseNet and, as
    usual, you are invited to check out the full code on GitHub.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first function just defines a convolution with batch normalization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Nothing special—we had a batch normalization before the ReLU activation, followed
    by a convolutional layer and the optional dropout.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next function defines a dense block using the previous method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There is a lot going on:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`num_layers` 3 X 3 convolutional layers, adding `growth_rate` channels every
    time. In addition, if `add_bottleneck_layer` is set, before each 3 X 3 convolutions
    it adds a 1 X 1 convolution to convert the number of channels in input to `4*
    growth_rate`; I did not use the bottleneck layer in my configuration, but you
    can.'
  prefs: []
  type: TYPE_NORMAL
- en: It returns two outputs, where the first output, layer, is the concatenation
    of all the outputs of each convolution, including the input, and the second output,
    derived from `block_layers`, is the concatenation of all the outputs of each convolution,
    excluding the input.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we need two outputs is because the down-sampling and the up-sampling
    path are a bit different. During down sampling, we include the input of the block,
    while during up sampling we don't; this is just to keep the size of the network
    and the computation time reasonable, as, in my case, without this change, the
    network would jump from 724 K parameters to 12 M!
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function defines the transition layer that is used to reduce the resolution
    in the down-sampling path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It just creates a 1 X 1 convolution followed by an average pooling; if you choose
    to add a compression factor, then the number of channels will be reduced; I chose
    a compression factor of `0.6` because the network was too big without any compression
    and did not fit in the RAM of my GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method is the transition layer used to increase the resolution in
    the up-sampling path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It creates a deconvolution to increase the resolution, and it adds the skip
    connection, which is, of course, important in enabling us to also increase the
    effective resolution of the segmentation mask.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have all the building blocks, it is just a matter of assembling
    the full network.
  prefs: []
  type: TYPE_NORMAL
- en: Putting all the pieces together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, a note on the resolution: I chose 160 X 160, as that was basically the
    maximum that my laptop could do, in combination with the other settings. You can
    try a different resolution, but you will see that not all the resolutions are
    possible. In fact, depending on the number of dense blocks, you might need to
    use multiples of 16, 32, or 64\. Why is this? Simple. Let''s take an example,
    assuming that we will use 160 X 160\. If, during down sampling, you reduce the
    resolution 16 times (for example, you have 4 dense blocks, each one followed by
    a *transition-down* layer), then your intermediate resolution will be an integer
    number—in this case, 10 X 10\.'
  prefs: []
  type: TYPE_NORMAL
- en: When you up sample 4 times, your resolution will grow 16 fold, so your final
    resolution will still be 160 X 160\. But if you start with 170 X 170, you will
    still end up with an intermediate resolution of 10 X 10, and up sampling it will
    produce a final resolution of 160 X 160! This is a problem, because you need to
    concatenate these outputs with the skip layers taken during down sampling, and
    if the two resolutions are different, then we cannot concatenate the layers and
    Keras will generate an error. As regards the ratio, it does not need to be a square
    and it does not need to match the ratio of your images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing that we need to do is create the input for the neural network
    and the first convolutional layer, as the dense blocks assume that there is a
    convolution before them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: I used a 7 X 7 convolution without max pooling, but feel free to experiment.
    You could use a bigger image and introduce a max pooling or an average pooling,
    or just create a bigger network, if you can train it at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can generate the down-sampling path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We simply create all the groups that we want, five in my configuration, and
    for each group we add a dense layer and a transition-down layer, and we also record
    the skip connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following step builds the up-sampling path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We reverse the skip connections because when going up, we encounter the skip
    connections in the opposite order, and we add a dense layer that is not followed
    by a transition down. This is called a *bottleneck layer*, as it has a low amount
    of information. Then we simply create the transition-up and dense layer corresponding
    to the down-sampling path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the last part, let''s generate the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We simply add a 1 X 1 convolution and a softmax activation.
  prefs: []
  type: TYPE_NORMAL
- en: The difficult part is done, but we need to learn how to feed the input to the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding the network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feeding the neural network is not too difficult, but there are some practical
    complications because the network is quite demanding, and loading all the images
    in RAM might not be feasible, so we are going to use a generator. However, this
    time, we will also add a simple data augmentation—we will mirror half of the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, we will define a hierarchy where we have all the images in subdirectories
    of the `dataset` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rgb` contains the images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seg` contains the segmented and colored images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seg_raw` contains the images in raw format (numeric labels in the red channel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that when given an image in the `rgb` folder, we can get the corresponding
    raw segmentation by just changing the path to `seg_raw`. This is useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a generic generator that is usable for data augmentation; our
    approach will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator will receive a list of IDs—in our case, the paths of the `rgb`
    images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator will also receive two functions—one that, given an ID, can generate
    an image and another that, given an ID, can generate the corresponding label (changing
    the path to `seg_raw`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will provide the index in the epoch to help with data augmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the generic generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is similar to what we have already seen in [*Chapter 8*](B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182),
    *Behavioral Cloning*. It goes through all the IDs and obtains the images and labels
    for the batch; the main difference is that we pass two additional parameters to
    the functions, in addition to the current ID:'
  prefs: []
  type: TYPE_NORMAL
- en: A flag specifying whether we want to enable data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current index in the epoch to tell the function where we are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now it will be relatively easy to write a function that returns the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We load the image and resize it using the nearest-neighbor algorithm, as already
    discussed. This way, half of the time the image will be flipped.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the function to extract the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As expected, to get the label, we need to change the path from `rgb` to `seg_raw`,
    whereas when you augment the data in classifiers, the label does not change. In
    this case, the mask needs to be augmented in the same way, so we still need to
    mirror it when we also mirror the `rgb` image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trickier part is to generate the correct label because the raw format is
    not suitable. Normally, in a classifier, you provide a one-hot encoded label,
    meaning that if you have ten possible label values, every label will be converted
    to a vector of ten elements, where only one element is `1` and all the others
    are `0`. Here, we need to do the same, but for the whole image and at pixel level:'
  prefs: []
  type: TYPE_NORMAL
- en: Our label is not a single image but 13 images (as we have 13 possible label
    values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each image is dedicated to a single label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pixels of an image are `1` only where that label is present in the segmentation
    mask and `0` elsewhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, we apply one-hot encoding at pixel level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the resulting code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: At the beginning of the method, we create an image with 13 channels, and then
    we precompute the one-hot encoding (which contains 13 values) to speed up the
    computation. Then we simply apply the one-hot encoding to each pixel, based on
    the value of the red channel, which is where Carla stores the raw segmentation
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can start the training. You might consider running it overnight, as
    it might take a while, especially if you use dropout or if you decide to record
    additional images.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the graph with the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Training FC-DenseNet](img/Figure_9.9_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Training FC-DenseNet
  prefs: []
  type: TYPE_NORMAL
- en: It's not great, because the validation loss has many spikes, which indicates
    that the training is unstable, and sometimes the loss increases quite a lot. Ideally,
    we would like a smooth, decreasing curve as this means that the loss decreases
    at every iteration. It would probably benefit from a bigger batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the overall performance is not bad:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The validation accuracy is above 90%, which is promising.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how it behaves with the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Running the neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running inference on the network is no different than the usual process, but
    we need to convert the output to a colored image that we can actually understand
    and use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we need to define a palette of 13 colors that we are going to use
    to show the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we just need to derive two images using these colors—the raw segmentation
    and the colored segmentation. The following function does both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You might remember that our output is an image with 13 channels, one per label.
    So you can see that we get the label from these channels using `argmax`; this
    label is used directly for the raw image, where it is stored in the red channel,
    whereas for the colored segmentation image, we store the color from the palette,
    using `label` as index, exchanging the blue and the red channel because OpenCV
    is in BGR.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how it performs, bearing in mind that these images are very similar
    to the ones that the network saw during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the result for one image, with other versions of the segmented
    picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.10_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the image, it is not perfect, but it does a good job: the
    road is detected properly and the guard rail and trees are kind of fine, and the
    pedestrian is detected, but not very well. Surely we can improve this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let'' s look at another problematic image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.11_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  prefs: []
  type: TYPE_NORMAL
- en: The previous image is quite challenging, as the road is dark and the cars are
    also dark, but the network does a decent job of detecting the road and the car
    (though the shape is not great). It does not detect the lane line, but it is actually
    not visible in the road, so here the ground truth is too optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – From the left: RGB image, ground truth from Carla, colored
    segmentation mask, and segmentation in overlay](img/Figure_9.12_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12 – From the left: RGB image, ground truth from Carla, colored segmentation
    mask, and segmentation in overlay'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here too, the result is not bad: the road and the trees are very well detected,
    and the traffic sign is decently detected, but it does not see the lane line,
    which is challenging but visible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to be sure that it can indeed detect the lane line, let''s look at a less
    challenging image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – From the left: RGB image, colored segmentation mask, and segmentation
    in overlay](img/Figure_9.13_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13 – From the left: RGB image, colored segmentation mask, and segmentation
    in overlay'
  prefs: []
  type: TYPE_NORMAL
- en: 'I don''t have the ground truth for this image, which also means that although
    it is taken from the same batch as the training dataset, it might be a bit more
    different. Here, the network behaves very well: the road, lane line, sidewalk,
    and vegetation are all very well detected.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that the network performs decently, but surely we should add many
    more samples, both from the same track and also from other tracks, and with different
    kinds of weather. Unfortunately, this means that the training would be much more
    demanding.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, this kind of result with around a thousand images is, in my opinion,
    a good result. But what if you cannot get enough samples in the dataset? Let's
    learn a small trick.
  prefs: []
  type: TYPE_NORMAL
- en: Improving bad semantic segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes things don't go as you hope. Maybe getting a lot of samples for the
    dataset is too expensive, or it takes too much time. Or perhaps there is no time
    because you need to try to impress some investors, or there is a technical issue
    or another type of problem, and you are stuck with a bad network and a few minutes
    to fix it. What can you do?
  prefs: []
  type: TYPE_NORMAL
- en: Well, there is a small trick that can help you; it will not transform a bad
    network into a good one, but it can nevertheless be better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example from a bad network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Badly trained network](img/Figure_9.14_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Badly trained network
  prefs: []
  type: TYPE_NORMAL
- en: 'It has a validation accuracy of around 80%, and it has been trained with around
    500 images. It''s quite bad, but it looks even worse than what it really is because
    of the areas that are full of dots, where the network seems to not be able to
    decide what it is looking at. Can we fix this with some postprocessing? Yes, we
    can. You might remember from [*Chapter 1*](B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017),
    *OpenCV Basics and Camera Calibration*, that OpenCV has several algorithms for
    blurring, and one in particular, the median blur, has a very interesting characteristic:
    it selects the median of the colors encountered, so it emits only colors that
    are already present in the few pixels that it analyzes, and it is very effective
    at reducing *salt and pepper* noise, which is what we are experiencing. So, let''s
    see the result of applying this to the previous image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Badly trained network, from the left: RGB image, colored segmentation,
    segmentation corrected with media blur (three pixels), and segmentation in overlay](img/Figure_9.15_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15 – Badly trained network, from the left: RGB image, colored segmentation,
    segmentation corrected with media blur (three pixels), and segmentation in overlay'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, while far from perfect, it makes the image more usable. And
    it is only one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: I used three pixels, but you can use more, if required. I hope you don't find
    yourself in a position where your network underperforms, but if you do, then this
    will surely be worth a try.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You completed the final chapter on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: We started this chapter by discussing what semantic segmentation means, then
    we talked extensively about DenseNet and why it is such a great architecture.
    We quickly talked about using a stack of convolutional layers to implement semantic
    segmentation, but we focused on a more efficient way, which is using DenseNet
    after adapting it to this task. In particular, we developed an architecture similar
    to FC-DenseNet. We collected a dataset with the ground truth for semantic segmentation,
    using Carla, and then we trained our neural network on it and saw how it performed
    and when detecting roads and other objects, such as pedestrians and sidewalks.
    We even discussed a trick to improve the output of a bad semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was quite advanced, and it required a good understanding of all
    the previous chapters about deep learning. It has been quite a ride, and I think
    it is fair to say that this has been a *dense* chapter. Now that you have a good
    knowledge of how to train a network to recognize what is present in front of a
    car, it is time to take control of the car and make it steer.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a distinguished characteristic of DenseNet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the name of the family architecture such as inspired the authors of
    DenseNet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is FC-DenseNet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we say that FC-DenseNet is U-shaped?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you need a fancy architecture like DenseNet to perform semantic segmentation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have a neural network that performs poorly at semantic segmentation,
    is there a quick fix that you can use sometimes, if you have no other options?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are skip connections used for in FC-DenseNet and other U-shaped architectures?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DenseNet paper ([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FC-DenseNet paper ([https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
