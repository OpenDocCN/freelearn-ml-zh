["```py\nstrategy = tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver) \n```", "```py\nwith strategy.scope()\n    model = <model architecture definition> \n```", "```py\nstrategy = tf.distribute.MirroredStrategy() \n```", "```py\nstrategy = tf.distribute.MultiWorkerMirroredStrategy() \n```", "```py\ntorch.distributed.init_process_group(...)\nmodel = torch.nn.parallel.DistributedDataParallel(model, ...) \n```", "```py\n    from sagemaker.pytorch import PyTorch \n\n    output_path = f\"s3://{bucket}/{prefix}\" \n\n    estimator = PyTorch(\n        entry_point=\"train-dis.py\",\n        source_dir=\"code\",\n        role=role,\n        framework_version=\"1.6\",\n        py_version=\"py3\",\n        instance_count=2,  \n        instance_type= \"ml.g4dn.12xlarge\", \n        output_path=output_path,\n        hyperparameters={\n            \"epochs\": 10,\n            \"lr\" : 5e-5,\n            \"num_labels\": 3,\n            \"train_file\": \"train.csv\",\n            \"test_file\" : \"test.csv\",\n            \"MAX_LEN\" : 315,\n            \"batch_size\" : 64,\n            \"test_batch_size\" : 10,\n            \"backend\": \"nccl\"\n        },\n\n    )\n    estimator.fit({\"training\": inputs_train, \"testing\": inputs_test}) \n    ```"]