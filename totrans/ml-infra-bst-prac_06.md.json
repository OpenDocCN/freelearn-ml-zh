["```py\n# create the feature extractor, i.e., BOW vectorizer\n# please note the argument - max_features\n# this argument says that we only want three features\n# this will illustrate that we can get problems - e.g. noise\n# when using too few features\nvectorizer = CountVectorizer(max_features = 3)\n# simple input data - two sentences\nsentence1 = 'printf(\"Hello world!\");'\nsentence2 = 'return 1'\n# creating the feature vectors for the input data\nX = vectorizer.fit_transform([sentence1, sentence2])\n# creating the data frame based on the vectorized data\ndf_bow_sklearn = pd.DataFrame(X.toarray(),\n                              columns=vectorizer.get_feature_names(),\n                              index=[sentence1, sentence2])\n# take a peek at the featurized data\ndf_bow_sklearn.head()\n```", "```py\n# read the file with gerrit code reviews\ndfReviews = pd.read_csv('./gerrit_reviews.csv', sep=';')\n# just checking that we have the right columns\n# and the right data\ndfReviews.head()\n```", "```py\nimport numpy as np\n# before we use the feature extractor, let's check if the data contains NANs\nprint(f'The data contains {dfReviews.LOC.isnull().sum()} empty rows')\n# remove the empty rows\ndfReviews = dfReviews.dropna()\n# checking again, to make sure that it does not contain them\nprint(f'The data contains {dfReviews.LOC.isnull().sum()} empty rows')\n```", "```py\n# now, let's convert the code (LOC) column to the vector of features\n# using BOW from the example above\nvectorizer = CountVectorizer(min_df=2,\n                             max_df=10)\ndfFeatures = vectorizer.fit_transform(dfReviews.LOC)\n# creating the data frame based on the vectorized data\ndf_bow_sklearn = pd.DataFrame(dfFeatures.toarray(),\n                              columns=vectorizer.get_feature_names(),index=dfReviews.LOC)\n# take a peek at the featurized data\ndf_bow_sklearn.head()\n```", "```py\n# read data with NaNs to a dataframe\ndfNaNs = pd.read_csv('./gerrit_reviews_nan.csv', sep='$')\n# before we use the feature extractor, let's check if the data contains NANs\nprint(f'The data contains {dfNaNs.isnull().sum()} NaN values')\n```", "```py\nyangresourcesnametocontentmap     213\nyangtextschemasourceset           205\nyangtextschemasourcesetbuilder    208\nyangtextschemasourcesetcache      207\nyangutils                         185\n```", "```py\n# in order to use the imputer, we need to remove the index from the data\n# we remove the index by first re-setting it (so that it becomes a regular column)\n# and then by removing this column.\ndfNaNs_features = dfNaNs.reset_index()\ndfNaNs_features.drop(['LOC', 'index'], axis=1, inplace=True)\ndfNaNs_features.head()\n```", "```py\n# let's use iterative imputed to impute data to the dataframe\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# create the instance of the imputer\nimp = IterativeImputer(max_iter=3,\n                       random_state=42,\n                       verbose = 2)\n# train the imputer on the features in the dataset\nimp.fit(dfNaNs_features)\n```", "```py\n# now, we fill in the NaNs in the original dataset\nnpNoNaNs = imp.transform(dfNaNs_features)\ndfNoNaNs = pd.DataFrame(npNoNaNs)\n```", "```py\n# importing the libraries to vectorize text\n# and to manipulate dataframes\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n# create the feature extractor, i.e., BOW vectorizer\n# please note the argument - max_features\n# this argument says that we only want three features\n# this will illustrate that we can get problems - e.g. noise\n# when using too few features\nvectorizer = CountVectorizer()\n# read the file with gerrit code reviews\ndfReviews = pd.read_csv('./gerrit_reviews.csv', sep=';')\n```", "```py\n# now, let's convert the code (LOC) column to the vector of features\n# using BOW from the example above\nvectorizer = CountVectorizer(min_df=2,\n                             max_df=10)\ndfFeatures = vectorizer.fit_transform(dfReviews.LOC)\n# creating the data frame based on the vectorized data\ndf_bow_sklearn = pd.DataFrame(dfFeatures.toarray(),\n                              columns=vectorizer.get_feature_names(),index=dfReviews.LOC)\n```", "```py\n# using a classifier from the Hugging Face hub is quite straightforward\n# we import the package and create the sentiment analysis pipeline\nfrom transformers import pipeline\n# when we create the pipeline, and do not provide the model\n# then the huggingface hub will choose one for us\n# and download it\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\n# now we are ready to get the sentiment from our reviews.\n# let's supply it to the sentiment analysis pipeline\nlstSentiments = sentiment_pipeline(list(dfReviewComments))\n# transform the list to a dataframe\ndfSentiments = pd.DataFrame(lstSentiments)\n# and then we change the textual value of the sentiment to\n# a numeric one – which we will use for the random forest\ndfSentiment = dfSentiments.label.map({'NEGATIVE': 0, 'POSITIVE': 1})\n```", "```py\n# now, we train the RandomForest classifier to get the most important features\n# Note! This training does not use any data split, as we only want to find\n# which features are important.\nX = df_bow_sklearn.drop(['sentiment'], axis=1)\nY = df_bow_sklearn['sentiment']\n# import the classifier – Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# create the classifier\nclf = RandomForestClassifier(max_depth=10, random_state=42)\n# train the classifier\n# please note that we do not check how good the classifier is\n# only train it to find the features that are important.\nClf.fit(X,Y)\n```", "```py\n# now, let's check which of the features are the most important ones\n# first we create a dataframe from this list\n# then we sort it descending\n# and then filter the ones that are not important\ndfImportantFeatures = pd.DataFrame(clf.feature_importances_, index=X.columns, columns=['importance'])\n# sorting values according to their importance\ndfImportantFeatures.sort_values(by=['importance'],\n                                ascending=False,\n                                inplace=True)\n# choosing only the ones that are important, skipping\n# the features which have importance of 0\ndfOnlyImportant = dfImportantFeatures[dfImportantFeatures['importance'] != 0]\n# print the results\nprint(f'All features: {dfImportantFeatures.shape[0]}, but only {dfOnlyImportant.shape[0]} are used in predictions. ')\n```", "```py\n# we use matplotlib and seaborn to make the plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Define size of bar plot\n# We make the x axis quite much larger than the y-axis since\n# there is a lot of features to visualize\nplt.figure(figsize=(40,10))\n# plot seaborn bar chart\n# we just use the blue color\nsns.barplot(y=dfOnlyImportant['importance'],\n            x=dfOnlyImportant.index,\n            color='steelblue')\n# we make the x-labels rotated so that we can fit\n# all the features\nplt.xticks(rotation=90)\n# add chart labels\nplt.title('Importance of features, in descending order')\nplt.xlabel('Feature importance')\nplt.ylabel('Feature names')\n```", "```py\n# then we read the dataset\ndfData = pd.read_csv('./bow_sentiment.csv', sep='$')\n# now, let's split the data into train and test\n# using the random split\nfrom sklearn.model_selection import train_test_split\nX = dfData.drop(['LOC', 'sentiment'], axis=1)\ny = dfData.sentiment\n# now we are ready to split the data\n# test_size parameter says that we want 1/3rd of the data in the test set\n# random state allows us to replicate the same split over and over again\nX_train, X_test, y_train, y_test =\n                train_test_split(X, y,\n                                 test_size=0.33,\n                                 random_state=42)\n```", "```py\n# import plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# we make the figure a bit larger\n# and the font a bit more visible\nplt.figure(figsize=(10,7))\nsns.set(font_scale=1.5)\n# here we visualize the histogram using seaborn\n# we take only one of the variables, please see the list of columns\n# above, or use print(X_train.columns) to get the list\n# I chose the one that was the most important one\n# for the prediction algorithm\nsns.histplot(data=X_train['dataresponse'],\n             binwidth=0.2)\n```", "```py\nplt.figure(figsize=(10,7))\nsns.set(font_scale=1.5)\nsns.histplot(data=X_test['dataresponse'],\n             binwidth=0.2)\n```", "```py\n# we can even check the count of each of these values\nX_train_one_feature = X_train.groupby(by='dataresponse').count()\nX_train_one_feature\n# we can even check the count of each of these values\nX_test_one_feature = X_test.groupby(by='dataresponse').count()\nX_test_one_feature\n```", "```py\n# we make the figure a bit larger\n# and the font a bit more visible\nplt.figure(figsize=(10,7))\nsns.set(font_scale=1.5)\nsns.histplot(data=y_train, binwidth=0.5)\nsns.histplot(data=y_test,  binwidth=0.5)\n```"]