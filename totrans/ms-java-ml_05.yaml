- en: Chapter 5. Real-Time Stream Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, and [Chapter 4](ch04.html "Chapter 4. Semi-Supervised and
    Active Learning"), *Semi-Supervised and Active Learning*, we discussed various
    techniques of classification, clustering, outlier detection, semi-supervised,
    and active learning. The form of learning done from existing or historic data
    is traditionally known as batch learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these algorithms or techniques assume three things, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: Finite training data is available to build different models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learned model will be static; that is, patterns won't change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data distribution also will remain the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many real-world data scenarios, there is either no training data available
    a priori or the data is dynamic in nature; that is, changes continuously with
    respect to time. Many real-world applications may also have data which has a transient
    nature to it and comes in high velocity or volume such as IoT sensor information,
    network monitoring, and Twitter feeds. The requirement here is to learn from the
    instance immediately and then update the learning.
  prefs: []
  type: TYPE_NORMAL
- en: The nature of dynamic data and potentially changing distribution renders existing
    batch-based algorithms and techniques generally unsuitable for such tasks. This
    gave rise to adaptable or updatable or incremental learning algorithms in machine
    learning. These techniques can be applied to continuously learn from the data
    streams. In many cases, the disadvantage of learning from Big Data due to size
    and the need to fit the entire data into memory can also be overcome by converting
    the Big Data learning problem into an incremental learning problem and inspecting
    one example at a time.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the assumptions and discuss different techniques
    in supervised and unsupervised learning that facilitate real-time or stream machine
    learning. We will use the open source library **Massive Online Analysis** (**MOA**)
    for performing a real-world case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major sections of this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions and mathematical notation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic stream processing and computational techniques. A discussion of stream
    computations, sliding windows including the ADWIN algorithm, and sampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concept drift and drift detection: Introduces learning evolving systems and
    data management, detection methods, and implicit and explicit adaptation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incremental supervised learning: A discussion of learning from labeled stream
    data, modeling techniques including linear, non-linear, and ensemble algorithms.
    This is followed by validation, evaluation, and model comparison methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Incremental unsupervised learning: Clustering techniques similar to those discussed
    in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"),
    *Unsupervised Machine Learning Techniques*, including validation and evaluation
    techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised learning using outlier detection: Partition-based and distance-based,
    and the validation and evaluation techniques used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study for stream-based learning: Introduces the MOA framework, presents
    the business problem, feature analysis, mapping to machine learning blueprint;
    describes the experiments, and concludes with the presentation and analysis of
    the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions and mathematical notations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some key assumptions made by many stream machine learning techniques
    and we will state them explicitly here:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of features in the data is fixed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data has small to medium dimensions, or number of features, typically in the
    hundreds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of examples or training data can be infinite or very large, typically
    in the millions or billions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of class labels in supervised learning or clusters are small and
    finite, typically less than 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally, there is an upper bound on memory; that is, we cannot fit all the
    data in memory, so learning from data must take this into account, especially
    lazy learners such as K-Nearest-Neighbors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally, there is an upper bound on the time taken to process the event or
    the data, typically a few milliseconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The patterns or the distributions in the data can be evolving over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning algorithms must converge to a solution in finite time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let *D*[t] = {**x**[i], y[i] : *y = f(x)*} be the given data available at time
    *t* ∈ {1, 2, … *i*}.'
  prefs: []
  type: TYPE_NORMAL
- en: An incremental learning algorithm produces sequences of models/hypotheses {..,
    *G*[j-1], G[j], *G*[j+1]..} for the sequence of data {.., *D*[j-1], *D*[j], *D*[j+1]..}
    and model/hypothesis *G*[i] depends only on the previous hypothesis *G*[i-1] and
    current data *D*[i].
  prefs: []
  type: TYPE_NORMAL
- en: Basic stream processing and computational techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now describe some basic computations that can be performed on the stream
    of data. If we must run summary operations such as aggregations or histograms
    with limits on memory and speed, we can be sure that some kind of trade-off will
    be needed. Two well-known types of approximations in these situations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ϵ* Approximation: The computation is close to the exact value within the fraction
    *ϵ* of error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(*ϵ**, δ*) Approximation: The computation is close to the exact value within
    1 ± *ϵ* with probability within 1 – *δ*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream computations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will illustrate some basic computations and aggregations to highlight the
    difference between batch and stream-based calculations when we must compute basic
    operations with constraints on memory and yet consider the entire data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequency count or point queries**: The generic technique of Count-Min Sketch
    has been successfully applied to perform various summarizations on the data streams.
    The primary technique is creating a window of size *w* x *d*. Then, given a desired
    probability (δ) and admissible error (*ϵ*), the size of data in memory can be
    created using *w* = 2/ *ϵ* and ![Stream computations](img/B05137_05_019.jpg).
    Associated with each row is a hash function: *h*(.). This uniformly transforms
    a value *x* to a value in the interval [1, 2 … *w*]. This method of lookup and
    updates can be used for performing point queries of values or dot products or
    frequency counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distinct count**: The generic technique of Hash-Sketch can be used to perform
    "distinct values" queries or counts. Given the domain of incoming stream values
    x ∈ [0,1,2….N-1], the hash function *h*(*x*) maps the values uniformly across
    [0,1,….2L-1], where *L=O(log N)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean**: Computing the mean without the need for storing all the values is
    very useful and is normally employed using a recursive method where only the number
    of observations *(n)* and sum of values seen so far (∑*x*[n]) is needed:![Stream
    computations](img/B05137_05_029.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation**: Like the mean, standard deviation can be computed using
    the memoryless option with only the number of observations (*n*), sum of values
    seen so far (∑*x*[n]), and sum of squares of the values (∑*x*[n]²):![Stream computations](img/B05137_05_031.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation coefficient**: Given a stream of two different values, many algorithms
    need to compute the correlation coefficient between the two which can be done
    by maintaining the running sum of each stream (∑*x*[n] and ∑*y*[n]), the sum of
    squared values (∑*x*[n]² and ∑*y*[n]²), and the cross-product (∑*x*[n]x *y*[n]).
    The correlation is given by:![Stream computations](img/B05137_05_036.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sliding windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, you don't need the entire data for computing statistics or summarizations
    but only the "recent past". In such cases, sliding window techniques are used
    to calculate summary statistics by keeping the window size either fixed or adaptable
    and moving it over the recent past.
  prefs: []
  type: TYPE_NORMAL
- en: '**ADaptable sliding WINdow** (**ADWIN**) is one well-known technique used to
    detect change as well as estimating values needed in the computation. The idea
    behind ADWIN is to keep a variable-length window of last seen values with the
    characteristic that the window has a maximum length statistically consistent with
    the fact that there has been no change in the average value within the window.
    In other words, the older values are dropped if and only if a new incoming value
    would change the average. This has the two-fold advantage of recording change
    and maintaining the dynamic value, such as aggregate, over the recent streams.
    The determination of the subjective notion "large enough" for dropping items can
    be determined using the well-known Hoeffding bound as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sliding windows](img/B05137_05_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Sliding windows](img/B05137_05_038.jpg) is the harmonic mean between
    two windows *W*[0] and *W*[1] of size |*W*[0]| and |*W*[1]| respectively, with
    *W*[1] containing the more recent elements. Further, let ![Sliding windows](img/B05137_05_043.jpg)
    and ![Sliding windows](img/B05137_05_044.jpg) be the respective calculated averages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be generalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ADWIN (*x: inputstream, δ: confidence*)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: init (*W*) //Initialize Window *W*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: while (*x*){
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: W ← W ∪ {*x*[t]} //add new instance *x*[t] to the head of Window *W*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: repeat W ← W – *x*old //drop elements from tail of the window
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Sliding windows](img/B05137_05_056.jpg) < ![Sliding windows](img/B05137_05_057.jpg)
    holds for every split of *W*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: output ![Sliding windows](img/B05137_05_058.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ADWIN has also shown that it provides theoretical bounds on false positives
    and false negatives, which makes it a very promising technique to use.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many stream-based algorithms, there is a need to reduce the data or select
    a subset of data for analysis. The normal methodology of sampling on the whole
    data must be augmented for stream-based data.
  prefs: []
  type: TYPE_NORMAL
- en: The key concerns in sampling that must be addressed are how unbiased the samples
    are and how representative they are of the population from which streams are being
    generated. In a non-streaming environment, this depends completely on the sample
    size and the sampling method. Uniform random sampling ([Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning"), *Practical Approach to Real-World
    Supervised Learning*) is one of the most well-known techniques employed to reduce
    the data in the batch data world. The reservoir sampling technique is considered
    to be a very effective way of reducing the data given the memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea of reservoir sampling is to keep a reservoir or sample of fixed
    size, say *k*, and every element that enters the stream has a probability *k/n*
    of replacing an older element in the reservoir. The detailed algorithm is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are extensions to these such as the Min-wise Sampling and Load Shedding
    that overcome some issues associated with the base method.
  prefs: []
  type: TYPE_NORMAL
- en: Concept drift and drift detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in the introduction of the chapter, the dynamic nature of infinite
    streams stands in direct opposition to the basic principles of stationary learning;
    that is, that the distribution of the data or patterns remain constant. Although
    there can be changes that are *swift* or *abrupt*, the discussion here is around
    slow, gradual changes. These slow, gradual changes are fairly hard to detect and
    separating the changes from the noise becomes tougher still:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Concept drift and drift detection](img/B05137_05_071.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 Concept drift illustrated by the gradual change in color from yellow
    to blue in the bottom panel. Sampled data reflects underlying change in data distribution,
    which must be detected and a new model learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'There have been several techniques described in various studies in the last
    two decades that can be categorized as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Concept drift and drift detection](img/B05137_05_072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 Categories of drift detection techniques
  prefs: []
  type: TYPE_NORMAL
- en: Data management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main idea is to manage a model in memory that is consistent with the dynamic
    nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Partial memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These techniques use the most recently used data in a memory buffer to learn
    or derive summary information. The key question as discussed previously is: what
    is the right window size to be effective in detecting the change and learning
    effectively? In Fixed Window Size based techniques, we use the idea of a queue
    where a new instance with a recent timestamp comes in and the one with the oldest
    is evicted. The window thus contains all the recent enough examples and the size
    is generally chosen based on physical availability of memory and size of data
    elements in the queue. In Adaptive Window Size, the queue is used in conjunction
    with a detection algorithm. When the detection algorithm indicates signs of drifts
    based on performance evaluation, the window size can be reduced to effectively
    remove old examples which no longer help the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Full memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea is to store sufficient statistics over all the examples or data seen.
    One way to do this is to put weights on the data and weights decay over time.
    Exponential weighting using the rate factor given by λ can be very effective:'
  prefs: []
  type: TYPE_NORMAL
- en: w[λ] (*x*) = *exp*(– λ * i)
  prefs: []
  type: TYPE_NORMAL
- en: Detection methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the probability *P(X)* that the given data is observed, the probability
    of patterns/class *P(C)*, and the probability of data given class *P(X|C)*—which
    is the model—the detection method can be divided into two categories, at a high
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring evolution or performance of the model, classifier, or *P(C|X)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring distributions from the environment or observing *P(X)*, *P(C)*, and
    *P(X|C)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring model evolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although this method is based on the assumption that all the learning of models
    is stationary and data is coming from **independent, identical distributions**
    (**i.i.d.**), which doesn't hold true in many applications, it has nevertheless
    been shown to be effective. Some of the well-known techniques are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Widmer and Kubat
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is one of the earliest methods which observed the error rates or misclassification
    rates and the changes to the model such as tree structures due to new branches,
    for instance. Using these and known thresholds, the learning window size is increased
    or decreased.
  prefs: []
  type: TYPE_NORMAL
- en: Drift Detection Method or DDM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This method assumes that the parameter being observed, such as the classifier
    labeling things correctly or incorrectly, is a binary random variable which follows
    a binomial distribution. It assumes probability of misclassification at probability
    pi with standard deviation of ![Drift Detection Method or DDM](img/B05137_05_083.jpg)
    where the values are computed at the *i*^(th) point in the sequence. The method
    then uses two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning level: When *p*[i] + *s*[i] ≥ *p*[min]+ 2 * *s*[min]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Detection level: When *p*[i] + *s*[i] ≥ *p*[min]+ 3 * *s*[min]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the examples between the "warning" and "detection" levels are used to train
    a new classifier that will replace the "non-performing" classifier when the "detection"
    level is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Early Drift Detection Method or EDDM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: EDDM uses the same technique as DDM but with a slight modification. It uses
    classification rate (that is, recall) rather than error rate (1 – accuracy) and
    uses the distance between the number of right predictions and two wrong predictions
    to change the levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'EDDM computes the mean distance between the two errors *p*[i]*^''* and standard
    deviation between the two *s*[i]*''*. The levels are then:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning level: (*p*[i]*^''* + 2 * *s*[i]*^''*) ⁄ (*p^''*[max] + 2 * *s^''*[max])
    < *α*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Detection level: (*p*[i]*^''* + 2 * *s*[i]*''*) ⁄ (*p^''*[max] + 2 * *s^''*[max])
    < *β*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters *α* and *β* are normally tuned by the user to something around
    90% and 95% respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring distribution changes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When there are no models or classifiers to detect changes, we apply techniques
    that use some form of statistical tests for monitoring distribution changes. These
    tests are used to identify the distribution changes. Owing to assumptions, whether
    parametric or non-parametric, and different biases, it is difficult to say concretely
    which works best. Here we provide some of the well-known statistical tests.
  prefs: []
  type: TYPE_NORMAL
- en: Welch's t test
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This is an adaptation of the Student *t* test with two samples. The test is
    adapted to take two windows of size *N*[1] and *N*[2] with means ![Welch''s t
    test](img/B05137_05_096.jpg)and ![Welch''s t test](img/B05137_05_097.jpg) and
    variances ![Welch''s t test](img/B05137_05_348.jpg) and ![Welch''s t test](img/B05137_05_349.jpg)
    to compute the *p* value and use that to reject or accept the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Welch''s t test](img/B05137_05_100.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kolmogorov-Smirnov's test
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This statistical test is normally used to compare distances between two distributions
    and validate if they are below certain thresholds or not. This can be adapted
    for change detection by using windows of two different sample sizes, *N*[1] and
    *N*[2], with different cumulative distribution functions, *F*[1](*x*) and F[2](*x*),
    *KS* distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kolmogorov-Smirnov''s test](img/B05137_05_104.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The null hypothesis, which assumes the two distributions are similar, is rejected
    with confidence of *α* if and only if ![Kolmogorov-Smirnov's test](img/B05137_05_105.jpg),
    which is obtained by a lookup in the Kolmogorov-Smirnov's table.
  prefs: []
  type: TYPE_NORMAL
- en: CUSUM and Page-Hinckley test
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The **cumulative sum** (**CUSUM**) is designed to indicate when the mean of
    the input is significantly different from zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*[0] = 0 , *g*[t] = *max*(0, *g*[t–1]) + *ϵ*[t] – *v*)'
  prefs: []
  type: TYPE_NORMAL
- en: We raise change detection when *g*[t] > *h*, where (*h, v*) are user-defined
    parameters. Note that the CUSUM test is memoryless and is one-sided or asymmetric,
    detecting only the increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Page Hinckley test is similar to CUSUM with a small variation as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*0 = 0 , *g*[t] = *g*[t–1] + *ϵ*[t] – *v*)'
  prefs: []
  type: TYPE_NORMAL
- en: For increasing and decreasing the values, we use *G*[t] *= min(g*[t], *G*[t–1]*)
    or G*[t] *= max(g*[t], *G*[t–1]*)*, and *Gt – gt > h* for change detection.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptation methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explicit and implicit adaptation are the two well-known techniques for adapting
    to environment changes when a change is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit adaptation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In explicit adaptation, an additional technique from among the following is
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrain the model from scratch with new data so the previous model or data does
    not impact the new model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the model with the changes or new data such that the transition is smooth—assumes
    changes are gradual and not drastic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a sequence or ensemble of models that are learned over time—when a collaborative
    approach is better than any single model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implicit adaptation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In implicit adaptation, we generally use ensemble algorithms/models to adapt
    to the concept change. This can mean using different combinations ranging from
    a single classifier, to predicting in the ensemble, to using ADWIN for adaptive
    window-based with the classifier—all fall within the choices for implicit adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces several techniques used to learn from stream data when
    the true label for each instance is available. In particular, we present linear,
    non-linear, and ensemble-based algorithms adapted to incremental learning, as
    well as methods required in the evaluation and validation of these models, keeping
    in mind that learning is constrained by limits on memory and CPU time.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The modeling techniques are divided into linear algorithms, non-linear algorithms,
    and ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: Linear algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear methods described here require little to no adaptation to handle
    stream data.
  prefs: []
  type: TYPE_NORMAL
- en: Online linear models with loss functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different loss functions such as hinge, logistic, and squared error can be used
    in this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Only numeric features are used in these methods. The choice of loss function
    *l* and learning rate λ at which to apply the weight updates are taken as input
    parameters. The output is typically updatable models that give predictions accompanied
    by confidence values.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The basic algorithm assumes linear weight combinations similar to linear/logistic
    regression explained in [Chapter 2](ch02.html "Chapter 2. Practical Approach to
    Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning*. The stream or online learning algorithm can be summed up as:'
  prefs: []
  type: TYPE_NORMAL
- en: for(t=1,2,…T) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**x**[t] = *receive()*; // receive the data'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_116.jpg) ; //predict the label'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*y*[t] = *obtainTrueLabel()*; // get the true label'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*loss* = *l*(**w**[t], (**x**[t], **w**[t])); // calculate the loss'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: if(*l*(**w**t,(**x**t, **w**t )) > 0 then
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_120.jpg) ; //update the weights'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Different loss functions can be plugged in based on types of problems; some
    of the well-known types are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinge loss: *l*(**w**[t], (**x**[t], **w**[t])) = max(0, 1 – *yf*(**x**[t],
    **w**[t]))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logistic loss: ![How does it work?](img/B05137_05_123.jpg)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Squared loss: ![How does it work?](img/B05137_05_124.jpg)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** (**SGD**) can be thought of as changing the
    weights to minimize the squared loss as in the preceding loss functions but going
    in the direction of the gradient with each example. The update of weights can
    be described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_125.jpg)![How does it work?](img/B05137_05_126.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Online linear models have similar advantages and disadvantages as the linear
    models described in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable to some level as the weights of each features give insights on
    the impact of each feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumes linear relationship, additive and uncorrelated features, and hence doesn't
    model complex non-linear real-world data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very susceptible to outliers in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very fast and normally one of the first algorithms to try or baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online Naïve Bayes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bayes theorem is applied to get predictions as the posterior probability, given
    for an *m* dimensional input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Online Naïve Bayes](img/B05137_05_128.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Online Naïve Bayes can accept both categorical and continuous inputs. The categorical
    features are easier, as the algorithm must maintain counts for each class while
    computing the *P*(**X**[j]|*Y*) probability for each feature given the class.
    For continuous features, we must either assume a distribution, such as Gaussian,
    or to compute online Kernel Density estimates in an incremental way or discretize
    the numeric features incrementally. The outputs are updatable models and can predict
    the class accompanied by confidence value. Being probabilistic models, they have
    better confidence scores distributed between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: for(t = 1,2,…T) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**x**[t] = *receive()*; // receive the data'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: incrementCounters(**x**[t]); //update the *P(***X**j*|Y)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_132.jpg) //posterior probability'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is the fastest algorithm and has a low memory footprint as well as computation
    cost. It is very popular among online or fast learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumes distribution or some biases on the numeric features that can impact
    the predictive quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most popular non-linear stream learning classifiers in use is the
    Hoeffding Tree. In the following subsections, the notion of the Hoeffding bound
    is introduced, followed by the algorithm itself.
  prefs: []
  type: TYPE_NORMAL
- en: Hoeffding trees or very fast decision trees (VFDT)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key idea behind **Hoeffding Trees** (**HT**) is the concept of the Hoeffding
    bound. Given a real-valued random variable **x** whose range of values has size
    **R**, suppose we have **n** independent observations of **x** and compute the
    mean as ![Hoeffding trees or very fast decision trees (VFDT)](img/B05137_05_136.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: The Hoeffding bound states that, with a probability of 1 – δ, the actual mean
    of the variable **x** is at least ![Hoeffding trees or very fast decision trees
    (VFDT)](img/B05137_05_138.jpg) where ![Hoeffding trees or very fast decision trees
    (VFDT)](img/B05137_05_139.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: The Hoeffding bound is independent of the probability distribution generating
    the samples and gives a good approximation with just **n** examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of the Hoeffding bound is used in the leaf expansion. If *x*[1] is
    the most informative feature and *x*[2] ranks second, then split using a user-defined
    split function *G*(.) in a way such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hoeffding trees or very fast decision trees (VFDT)](img/B05137_05_144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Both categorical and continuous data can be part of the data input. Continuous
    features are discretized in many implementations. The desired probability parameter
    1 – δ and the split function common to Decision Trees *G*(.) becomes part of the
    input. The output is the interpretable Decision Tree model and can predict/learn
    with class and confidence values.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: HoeffdingTree(x:inputstream,G(.):splitFunction,δ:probabilityBound)
  prefs: []
  type: TYPE_NORMAL
- en: Let HT be a tree with single leaf(root)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: InitCounts(*n*[ijk], *root*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for(t=1,2,…T) do //all data from stream
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**x***[t] = receive();* //receive the data'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*y[t] = obtainTrueLabel();* //get the true label'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: HTGrow((**x**[t], *y*[t]), **HT**, δ)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**HTGrow**((**x**[t], *y*[t]), **HT**, *G*(.), δ)'
  prefs: []
  type: TYPE_NORMAL
- en: '*l = sort((***x**[t], *y*[t]), **HT**); //sort the data to leaf l using HT'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*updateCounts(n*[ijk], *l);* // update the counts at leaf l'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*if(examplesSoFarNotOfSameClass();*// check if there are multiple classes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*computeForEachFeature(,G(.))*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_ch5.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: 'Hoeffding Trees have interesting properties, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: They are a robust low variance model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They exhibit lower overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical guarantees with high probability on the error rate exist due to
    Hoeffding bounds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are variations to Hoeffding Trees that can adapt concept drift, known
    as Concept-Adapting VFDT. They use the sliding window concept on the stream. Each
    node in the decision tree keeps sufficient statistics; based on the Hoeffding
    test, an alternate subtree is grown and swapped in when the accuracy is better.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic HT has issues with attributes being close to the chosen *ϵ* and breaks
    the ties. Deciding the number of attributes at any node is again an issue. Some
    of it is resolved in VFDT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory constraints on expansion of the trees as well as time spent on an instance
    becomes an issue as the tree changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VFDT has issues with changes in patterns and CVFDT tries to overcome these as
    discussed previously. It is one of the most elegant, fast, interpretable algorithms
    for real-time and big data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind ensemble learning is similar to batch supervised learning where
    multiple algorithms are trained and combined in some form to predict unseen data.
    The same benefits accrue even in the online setting from different approaches
    to ensembles; for example, using multiple algorithms of different types, using
    models of similar type but with different parameters or sampled data, all so that
    different search spaces or patterns are found and the total error is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted majority algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **weighted majority algorithm** (**WMA**) trains a set of base classifiers
    and combines their votes, weighted in some way, and makes predictions based on
    the majority.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The constraint on types of inputs (categorical only, continuous only, or mixed)
    depends on the chosen base classifiers. The interpretability of the model depends
    on the base model(s) selected but it is difficult to interpret the outputs of
    a combination of models. The weights for each model get updated by a factor (*β*)
    per example/instance when the prediction is incorrect. The combination of weights
    and models can give some idea of interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*WeightedMajorityAlgorithm(x: inputstream, hm: m learner models)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*initializeWeights(w*[i]*)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for(t=1, 2,…T) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*x*[t] *= receive();*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*foreach model hk* *∈* *h*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*y*[i] ← *h*[k](**x**[t]);'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_172.jpg)![How does it work?](img/B05137_05_173.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: else ![How does it work?](img/B05137_05_173a.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: if *y* is known then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for *i* = 1 to m do
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: if *y*i ≠ *y* then
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*wi* ← *w*i ** β*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end if
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end for
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: WMA has simple implementation and theoretic bounds on ensemble errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difficulty is to choose the right base algorithm as the model and the number
    of models in the pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online Bagging algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we saw in the chapter on supervised learning, the bagging algorithm, which
    creates different samples from the training sets and uses multiple algorithms
    to learn and predict, reduces the variance and is very effective in learning.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The constraint on the types of inputs (categorical only, continuous only, or
    mixed) depends on the chosen base classifiers. The base classifier algorithm with
    parameter choices corresponding to the algorithm are also the inputs. The output
    is the learned model that can predict the class/confidence based on the classifier
    chosen.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The basic batch bagging algorithm requires the entire data to be available
    to create different samples and provide these samples to different classifiers.
    Oza''s Online Bagging algorithm changes this constraint and makes it possible
    to learn from unbounded data streams. Based on sampling, each training instance
    in the original algorithm gets replicated many times and each base model is trained
    with *k* copies of the original instances where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(k) = exp(–1)/k!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is equivalent to taking one training example and choosing for each classifier
    *k~Poisson(1)* and updating the base classifier *k* times. Thus, the dependency
    on the number of examples is removed and the algorithm can run on an infinite
    stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OnlineBagging(x: inputstream, h*[m]*: m learner models)*'
  prefs: []
  type: TYPE_NORMAL
- en: initialize base models *h*[m] for all *m* ∈ {1,2,..*M*}
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for(t=1,2,…T) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*x*[t]*=receive();*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: foreach model *m* = {1,2,..*M*}
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*w = Poisson*(1)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '*updateModel(h*[m]*, w, x*[t]*)*'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: end
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: return
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_191.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitaions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It has been empirically shown to be one of the most successful online or stream
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weight must be given to the data instance without looking at the other instances;
    this reduces the choices of different weighting schemes which are available in
    batch and are good in model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance is entirely determined by the choice of the *M* learners—the
    type of learner used for the problem domain. We can only decide on this choice
    by adopting different validation techniques described in the section on model
    validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Online Boosting algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The supervised boosting algorithm takes many *weak learners* whose accuracy
    is slightly greater than random and combines them to produce a strong learner
    by iteratively sampling the misclassified examples. The concept is identical in
    Oza's Online Boosting algorithm with modification done for a continuous data stream.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The constraint on types of inputs (categorical only, continuous only, or mixed)
    depends on the chosen base classifiers. The base classifier algorithms and their
    respective parameters are inputs. The output is the learned model that can predict
    the class/confidence based on the classifier chosen.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The modification of batch boosting to online boosting is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep two sets of weights for *M* base models, *λ*^c is a vector of dimension
    *M* which carries the sum of weights of correctly classified instances, and *λ*^w
    is a vector of dimension *M*, which carries the sum of weights of incorrectly
    classified instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weights are initialized to 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given a new instance (**x**[t], *y*[t]), the algorithm goes through the iterations
    of updating the base models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each base model, the following steps are repeated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the first iteration, *k = Poisson(λ)* is set and the learning classifier
    updates the algorithm (denoted here by *h*[1]) *k* times using (**x**[t], *y*[t]):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *h*[1] incorrectly classifies the instance, the *λ*^(w1) is incremented,
    *ϵ*[1], the weighted fraction, incorrectly classified by *h*[1], is computed and
    the weight of the example is multiplied by 1/2 *ϵ*[1].
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the performance is determined by the choice of the multiple learners,
    their types and the particular domain of the problem. The different methods described
    in the section on model validation techniques help us in choosing the learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oza's Online Boosting has been shown theoretically and empirically not to be
    "lossless"; that is, the model is different compared to its batch version. Thus,
    it suffers from performance issues and different extensions have been studied
    in recent years to improve performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation, evaluation, and comparisons in online setting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to the modes of machine learning we saw in the previous chapters,
    stream learning presents unique challenges to performing the core steps of validation
    and evaluation. The fact that we are no longer dealing with batch data means the
    standard techniques for validation evaluation and model comparison must be adapted
    for incremental learning.
  prefs: []
  type: TYPE_NORMAL
- en: Model validation techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the off-line or the batch setting, we discussed various methods of tuning
    the parameters of the algorithm or testing the generalization capability of the
    algorithms as a counter-measure against overfitting. Some of the techniques in
    the batch labeled data, such as cross-validation, are not directly applicable
    in the online or stream settings. The most common techniques used in online or
    stream settings are given next.
  prefs: []
  type: TYPE_NORMAL
- en: Prequential evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The prequential evaluation method is a method where instances are provided
    to the algorithm and the output prediction of the algorithm is then measured in
    comparison with the actual label using a loss function. Thus, the algorithm is
    always tested on the unseen data and needs no "holdout" data to estimate the generalization.
    The prequential error is computed based on the sum of the accumulated loss function
    between actual values and predicted values, given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prequential evaluation](img/B05137_05_206.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Three variations of basic prequential evaluation are done for better estimation
    on changing data, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Landmark Window (basic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Sliding Window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using forgetting mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last two methods are extensions of previously described techniques where
    you put weights or fading factors on the predictions that reduce over time.
  prefs: []
  type: TYPE_NORMAL
- en: Holdout evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the extension of the holdout mechanism or "independent test set" methodology
    of the batch learning. Here the total labeled set or stream data is separated
    into training and testing sets, either based on some fixed intervals or the number
    of examples/instances the algorithm has seen. Imagine a continuous stream of data
    and we place well-known intervals at ![Holdout evaluation](img/B05137_05_207.jpg)
    and ![Holdout evaluation](img/B05137_05_208.jpg) to compare the evaluation metrics,
    as discussed in next section, for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Controlled permutations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The issue with the aforementioned mechanisms is that they provide "average"
    behavior over time and can mask some basic issues such as the algorithm doing
    well at the start and very poorly at the end due to drift, for example. The advantage
    of the preceding methods is that they can be applied to real incoming streams
    to get estimates. One way to overcome the disadvantage is to create different
    random sets of the data where the order is shuffled a bit while maintaining the
    proximity in time and the evaluation is done over a number of these random sets.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation criteria
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most of the evaluation criteria are the same as described in the chapter on
    supervised learning and should be chosen based on the business problem, the mapping
    of the business problem to the machine learning techniques, and on the benefits
    derived. In this section, the most commonly used online supervised learning evaluation
    criteria are summarized for the reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: A measure of getting the true positives and true negatives correctly
    classified by the learning algorithm:![Evaluation criteria](img/B05137_05_209.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Balanced accuracy**: When the classes are imbalanced, balanced accuracy is
    often used as a measure. Balanced accuracy is an arithmetic mean of specificity
    and sensitivity. It can be also thought of as accuracy when positive and negative
    instances are drawn from the same probability in a binary classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area under the ROC curve** (**AUC**): Area under the ROC curve gives a good
    measure of generalization of the algorithm. Closer to 1.0 means the algorithm
    has good generalization capability while close to 0.5 means it is closer to a
    random guess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kappa statistic** (**K**): The Kappa statistic is used to measure the observed
    accuracy with the expected accuracy of random guessing in the classification.
    In online learning, the Kappa statistic is used by computing the prequential accuracy
    (*p*o) and the random classifier accuracy (*p*c) and is given by:![Evaluation
    criteria](img/B05137_05_213.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kappa Plus statistic**: The Kappa Plus statistic is a modification to the
    Kappa statistic obtained by replacing the random classifier by the persistent
    classifier. The persistent classifier is a classifier which predicts the next
    instance based on the label or outcome of the previous instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When considering "drift" or change in the concept as discussed earlier, in
    addition to these standard measures, some well-known measures given are used to
    give a quantitative measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability of true change detection**: Usually, measured with synthetic
    data or data where the changes are known. It gives the ability of the learning
    algorithm to detect the change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability of false alarm**: Instead of using the False Positive Rate in
    the off-line setting, the online setting uses the inverse of *time to detection
    or the average run* length which is computed using the expected time between false
    positive detections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Delay of detection**: This is measured as the time required, terms of instances,
    to identify the drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing algorithms and metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When comparing two classifiers or learners in online settings, the usual mechanism
    is the method of taking a performance metric, such as the error rate, and using
    a statistical test adapted to online learning. Two widely used methods are described
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**McNemar test**: McNemar''s test is a non-parametric statistical test normally
    employed to compare two classifiers'' evaluation metrics, such as "error rate",
    by storing simple statistics about the two classifiers. By computing statistic
    *a*, the number of correctly classified points by one algorithm that are incorrectly
    classified by the other, and statistic *b*, which is the inverse, we obtain the
    McNemar''s Test as:![Comparing algorithms and metrics](img/B05137_05_217.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test follows a χ2 distribution and the p-value can be used to check for
    statistical significance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Nemenyi test**: When there are multiple algorithms and multiple datasets,
    we use the Nemenyi test for statistical significance, which is based on average
    ranks across all. Two algorithms are considered to be performing differently in
    a statistically significant way if the ranks differ by a critical difference given
    by:![Comparing algorithms and metrics](img/B05137_05_218.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, K=number of algorithms, N=number of datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The critical difference values are assumed to follow a Student-T distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental unsupervised learning using clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept behind clustering in a data stream remains the same as in batch
    or offline modes; that is, finding interesting clusters or patterns which group
    together in the data while keeping the limits on finite memory and time required
    to process as constraints. Doing single-pass modifications to existing algorithms
    or keeping a small memory buffer to do mini-batch versions of existing algorithms,
    constitute the basic changes done in all the algorithms to make them suitable
    for stream or real-time unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The clustering modeling techniques for online learning are divided into partition-based,
    hierarchical-based, density-based, and grid-based, similar to the case of batch-based
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Partition based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of partition-based algorithms is similar to batch-based clustering
    where **k** clusters are formed to optimize certain objective functions such as
    minimizing the inter-cluster distance, maximizing the intra-cluster distance,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Online k-Means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: k-Means is the most popular clustering algorithm, which partitions the data
    into user-specified *k* clusters, mostly to minimize the squared error or distance
    between centroids and cluster assigned points. We will illustrate a very basic
    online adaptation of k-Means, of which several variants exist.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mainly, numeric features are considered as inputs; a few tools take categorical
    features and convert them into some form of numeric representation. The algorithm
    itself takes the parameters' number of clusters *k* and number of max iterations
    *n* as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The input data stream is considered to be infinite but of constant block size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A memory buffer of the block size is kept reserved to store the data or a compressed
    representation of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initially, the first stream of data of block size is used to find the *k* centroids
    of the clusters, the centroid information is stored and the buffer is cleared.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the next data after it reaches the block size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For either max number of iterations or until there is no change in the centroids:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute k-Means with buffer data and the present centroids.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the squared sum error between centroids and data assigned to the cluster.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After the iterations, the buffer is cleared and new centroids are obtained.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 4 until the data is no longer available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to batch-based, the shape of the detected cluster depends on the distance
    measure and is not appropriate in problem domains with irregular shapes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of parameter **k**, as in batch-based, can limit the performance
    in datasets with many distinct patterns or clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers and missing data can pose lots of irregularities in clustering behavior
    of online k-Means.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the selected buffer size or the block size of the stream on which iterative
    k-Means runs is small, it will not find the right clusters. If the chosen block
    size is large, it can result in slowdown or missed changes in the data. Extensions
    such as **Very Fast k-Means Algorithm** (**VFKM**), which uses the Hoeffding bound
    to determine the buffer size, overcome this limitation to a large extent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical based and micro clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hierarchical methods are normally based on **Clustering Features** (**CF**)
    and **Clustering Trees** (**CT**). We will describe the basics and elements of
    hierarchical clustering and the BIRCH algorithm, the extension of which the CluStream
    algorithm is based on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Clustering Feature is a way to compute and preserve a summarization statistic
    about the cluster in a compressed way rather than holding on to the whole data
    belonging to the cluster. In a **d** dimensional dataset, with **N** points in
    the cluster, two aggregates in the form of total sum **LS** for each dimensions
    and total squared sum of data **SS** again for each dimension, are computed and
    the vector representing this triplet form the Clustering Feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '*CF*[j] *= < N, LS*[j]*, SS*[j] *>*'
  prefs: []
  type: TYPE_NORMAL
- en: 'These statistics are useful in summarizing the entire cluster information.
    The centroid of the cluster can be easily computed using:'
  prefs: []
  type: TYPE_NORMAL
- en: '*centroid*[j] *= LS*[j]*/N*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The radius of the cluster can be estimated using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical based and micro clustering](img/B05137_05_231.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The diameter of the cluster can be estimated using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical based and micro clustering](img/B05137_05_232.jpg)'
  prefs: []
  type: TYPE_IMG
- en: CF vectors have great incremental and additive properties which becomes useful
    in stream or incremental updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an incremental update, when we must update the CF vector, the following
    holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical based and micro clustering](img/B05137_05_233.jpg)![Hierarchical
    based and micro clustering](img/B05137_05_234.jpg)![Hierarchical based and micro
    clustering](img/B05137_05_235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When two CFs have to be merged, the following holds true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical based and micro clustering](img/B05137_05_236.jpg)![Hierarchical
    based and micro clustering](img/B05137_05_237.jpg)![Hierarchical based and micro
    clustering](img/B05137_05_238.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Clustering Feature Tree** (**CF Tree**) represents an hierarchical tree
    structure. The construction of the CF tree requires two user defined parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Branching factor **b** which is the maximum number of sub-clusters or non-leaf
    nodes any node can have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum diameter (or radius) **T**, the number of examples that can be absorbed
    by the leaf node for a CF parent node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CF Tree operations such as insertion are done by recursively traversing the
    CF Tree and using the CF vector for finding the closest node based on distance
    metrics. If a leaf node has already absorbed the maximum elements given by parameter
    *T*, the node is split. At the end of the operation, the CF vector is appropriately
    updated for its statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical based and micro clustering](img/B05137_05_245.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 An example Clustering Feature Tree illustrating hierarchical structure.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss **BIRCH** (**Balanced Iterative Reducing and Clustering Hierarchies**)
    following this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BIRCH only accepts numeric features. CF and CF tree parameters, such as branching
    factor *b* and maximum diameter (or radius) *T* for leaf are user-defined inputs.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BIRCH, designed for very large databases, was meant to be a *two-pass* algorithm;
    that is, scan the entire data once and re-scan it again, thus being an *O(N)*
    algorithm. It can be modified easily enough for online as a single pass algorithm
    preserving the same properties:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first phase or scan, it goes over the data and creates an in-memory CF
    Tree structure by sequentially visiting the points and carrying out CF Tree operations
    as discussed previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second phase, an optional phase, we remove outliers and merge sub-clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phase three is to overcome the issue of order of data in phase one. We use agglomerative
    hierarchical clustering to refactor the CF Tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Phase four is the last phase which is an optional phase to compute statistics
    such as centroids, assign data to closest centroids, and so on, for more effectiveness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is one of the most popular algorithms that scales linearly on a large database
    or stream of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has compact memory representation in the form of the CF and CF Tree for statistics
    and operations on incoming data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It handles outliers better than most algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the major limitations is that it has been shown not to perform well when
    the shape of the clusters is not spherical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of CF vector and clustering in BIRCH were extended for efficient
    stream mining requirements by Aggarwal *et al* and named *micro-cluster and CluStream*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CluStream only accepts numeric features. Among the user-defined parameters are
    the number of micro-clusters in memory (*q*) and the threshold (*δ*) in time after
    which they can be deleted. Additionally, included in the input are time-sensitive
    parameters for storing the micro-clusters information, given by *α* and *l*.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The micro-cluster extends the CF vector and keeps two additional measures.
    They are the sum of the timestamps and sum of the squares of timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*microCluster*[j] *= < N, LS*[j]*, SS*[j]*, ST, SST>*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The algorithm stores *q* micro-clusters in memory and each micro-cluster has
    a *maximum boundary* that can be computed based on means and standard deviations
    between centroid and cluster instance distances. The measures are multiplied by
    a factor which decreases exponentially with time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each new instance, we select the closest micro-cluster based on Euclidean
    distance and decide whether it should be absorbed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the distance between the new instance and the centroid of the closest micro-cluster
    falls within the maximum boundary, it is absorbed and the micro-cluster statistics
    are updated.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If none of the micro-clusters can absorb, a new micro-cluster is created with
    the instance and based on the timestamp and threshold (*δ*), the oldest micro-cluster
    is deleted.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Assuming normal distribution of timestamps, if the relevance time—the time of
    arrival of instance found by CluStream—is below the user-specified threshold,
    it is considered an outlier and removed. Otherwise, the two closest micro-clusters
    are merged.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The micro-cluster information is stored in secondary storage from time to time
    by using a pyramidal time window concept. Each micro-cluster has time intervals
    decrease exponentially using *α*l to create snapshots. These help in efficient
    search in both time and space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CluStream has been shown to be very effective in finding clusters in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CluStream algorithm, through effective storage using a pyramidal timestamp,
    has efficient time and space usage. CluStream, like BIRCH, can find only spherical
    shaped clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to batch clustering, density-based techniques overcome the "shape" issue
    faced by distance-based algorithms. Here we will present a well-known density-based
    algorithm, DenStream, which is based on the concepts of CF and CF Trees discussed
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The extent of the neighborhood of a core micro-cluster is the user-defined radius
    *ϵ*. A second input value is the minimum total weight *µ* of the micro-cluster
    which is the sum over the weighted function of the arrival time of each instance
    in the object, where the weight decays with a time constant proportional to another
    user-defined parameter, *λ*. Finally, an input factor *β* ∈ (0,1) is used to distinguish
    potential core micro-clusters from outlier micro-clusters.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the micro-cluster concepts of CluStream, DenStream holds two data
    structures: *p-micro-cluster* for potential clusters and *o-micro-clusters* for
    outlier detection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each *p-micro-cluster* structure has:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A weight associated with it which decreases exponentially with the timestamps
    it has been updated with. If there are *j* objects in the micro-cluster: ![How
    does it work?](img/B05137_05_260.jpg) where *f(t) =* 2^(-λt)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weighted linear sum** (**WLS**) and **weighted linear sum of squares** (**WSS**)
    are stored in micro-clusters similar to linear sum and sum of squares:![How does
    it work?](img/B05137_05_262.jpg)![How does it work?](img/B05137_05_263.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The mean, radius, and diameter of the clusters are then computed using the
    weighted measures defined previously, exactly like in CF. For example, the radius
    can be given as:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_05_264.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Each *o-micro-cluster* has the same structure as *p-micro-cluster* and timestamps
    associated with it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When a new instance arrives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A closest *p-micro-cluster* is found and the instance is inserted if the new
    radius is within the user-defined boundary *ϵ*. If inserted, the *p-micro-cluster*
    statistics are updated accordingly.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, an *o-micro-cluster* is found and the instance is inserted if the
    new radius is again within the boundary. The boundary is defined by *β* × *μ*,
    the product of the user-defined parameters, and if the radius grows beyond this
    value, the *o-micro-cluster* is moved to the *p-micro-cluster*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the instance cannot be absorbed by an *o-micro-cluster*, then a new micro-cluster
    is added to the *o-micro-clusters*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: At the time interval *t* based on weights, the *o-micro-cluster* can become
    the *p-micro-cluster* or vice versa. The time interval is defined in terms of
    *λ*, *β*, and *µ* as:![How does it work?](img/B05137_05_269.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the parameters, DenStream can find effective clusters and outliers
    for real-time data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has the advantage of finding clusters and outliers of any shape or size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The house keeping job of updating the *o-micro-cluster* and *p-micro-cluster*
    can be computationally expensive if not selected properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique is based on discretizing the multi-dimensional continuous space
    into a multi-dimensional discretized version with grids. The mapping of the incoming
    instance to grid online and maintaining the grid offline results in an efficient
    and effective way of finding clusters in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Here we present D-Stream, which is a grid-based online stream clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As in density-based algorithms, the idea of decaying weight of instances is
    used in D-Stream. Additionally, as described next, cells in the grid formed from
    the input space may be deemed sparse, dense, or sporadic, distinctions that are
    central to the computational and space efficiency of the algorithm. The inputs
    to the grid-based algorithm, then, are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*λ*: The decay factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0 < *C*[l] < 1 and *C*[m] > 1: Parameters that control the boundary between
    dense and sparse cells in the grid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*β* > 0: A constant that controls one of the conditions when a sparse cell
    is to be considered sporadic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each instance arriving at time *t* has a density coefficient that decreases
    exponentially over time:![How does it work?](img/B05137_05_273.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Density of the grid cell *g* at any given time *t* is given by *D(g, t)* and
    is the sum of the adjusted density of all instances given by *E(g, t)* that are
    mapped to grid cell *g*:![How does it work?](img/B05137_05_277.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each cell in the grid captures the statistics as a characterization vector
    given by:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*CV(g) =* <*t*[g], *t*>[m], *D*, *label*, *status*> where:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*[g] = last time grid cell was updated'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*[m]= last time grid cell was removed due to sparseness'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D* = density of the grid cell when last updated'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*label* = class label of the grid cell'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*status* = {NORMAL or SPORADIC}'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When the new instance arrives, it gets mapped to a cell *g* and the characteristic
    vector is updated. If *g* is not available, it is created and the list of grids
    is updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grid cells with empty instances are removed. Also, cells that have not been
    updated over a long time can become sparse, and conversely, when many instances
    are mapped, they become dense.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At a regular time interval known as a gap, the grid cells are inspected for
    status and the cells with fewer instances than a number—determined by a density
    threshold function—are treated as outliers and removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: D-Streams have theoretically and empirically been shown to find sporadic and
    normal clusters with very high efficiency in space and time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can find clusters of any shape or size effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation and evaluation techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of the static clustering evaluation measures discussed in [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, have an assumption of static and non-evolving patterns.
    Some of these internal and external measures are used even in streaming based
    cluster detection. Our goal in this section is to first highlight problems inherent
    to cluster evaluation in stream learning, then describe different internal and
    external measures that address these, and finally, present some existing measures—both
    internal and external—that are still valid.
  prefs: []
  type: TYPE_NORMAL
- en: Key issues in stream cluster evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is important to understand some of the important issues that are specific
    to streaming and clustering, as the measures need to address them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aging**: The property of points being not relevant to the clustering measure
    after a given time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missed points**: The property of a point not only being missed as belonging
    to the cluster but the amount by which it was missed being in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misplaced points**: Changes in clusters caused by evolving new clusters.
    Merging existing or deleting clusters results in ever-misplaced points with time.
    The impact of these changes with respect to time must be taken into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster noise**: Choosing data that should not belong to the cluster or forming
    clusters around noise and its impact over time must be taken into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation measures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluation measures for clustering in the context of streaming data must provide
    a useful index of the quality of clustering, taking into consideration the effect
    of evolving and noisy data streams, overlapping and merging clusters, and so on.
    Here we present some external measures used in stream clustering. Many internal
    measures encountered in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine
    Learning Techniques"), *Unsupervised Machine Learning Techniques*, such as the
    Silhouette coefficient, Dunn's Index, and R-Squared, are also used and are not
    repeated here.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Mapping Measures (CMM)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The idea behind CMM is to quantify the connectivity of the points to clusters
    given the ground truth. It works in three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapping phase**: In this phase, clusters assigned by the stream learning
    algorithm are mapped to the ground truth clusters. Based on these, various statistics
    of distance and point connectivity are measured using the concepts of k-Nearest
    Neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The average distance of point *p* to its closest *k* neighbors in a cluster
    *C*i is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_287.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The average distance for a cluster *C*[i] is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_288.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The point connectivity of a point *p* in a cluster *C*[i] is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_289.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Class frequencies are counted for each cluster and the mapping of the cluster
    to the ground truth is performed by calculating the histograms and similarity
    in the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, a cluster *C*[i] is mapped to the ground truth class, and *Cl*[j]
    is mapped to a ground truth cluster ![Cluster Mapping Measures (CMM)](img/B05137_05_350.jpg),
    which covers the majority of class frequencies of *C*[i]. The surplus is defined
    as the number of instances from class *Cl*[i] not covered by the ground truth
    cluster ![Cluster Mapping Measures (CMM)](img/B05137_05_350.jpg) and total surplus
    for instances in classes *Cl*[1], *Cl*[2], *Cl*[3] … *Cl*[1] in cluster *C*[i]
    compared to ![Cluster Mapping Measures (CMM)](img/B05137_05_350.jpg) is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_294.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Cluster *C*[i] is mapped using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_295.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Penalty phase**: The penalty for every instance that is mapped incorrectly
    is calculated in this step using computations of fault objects; that is, objects
    which are not noise and yet incorrectly placed, using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_296.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The overall penalty of point *o* with respect to all clusters found is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_297.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**CMM calculation**: Using all the penalties weighted over the lifespan is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster Mapping Measures (CMM)](img/B05137_05_298.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *C* is found clusters, *Cl* is ground truth clusters, *F* is the fault
    objects and *w(o)* is the weight of instance.
  prefs: []
  type: TYPE_NORMAL
- en: V-Measure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Validity or V-Measure is an external measure which is computed based on two
    properties that are of interest in stream clustering, namely, **Homogeneity**
    and **Completeness**. If there are *n* classes as set *C* = {*c*[1], *c*[2] …,
    *c*[n]} and *k* clusters *K* = {*k*[1], *k*[2..]*k*[m]}, contingency tables are
    created such that *A* = {*a*[ij]} corresponds to the count of instances in class
    *c*[i] and cluster *k*[j].
  prefs: []
  type: TYPE_NORMAL
- en: '**Homogeneity**: Homogeneity is defined as a property of a cluster that reflects
    the extent to which all the data in the cluster belongs to the same class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conditional entropy and class entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V-Measure](img/B05137_05_309.jpg)![V-Measure](img/B05137_05_310.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Homogeneity is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V-Measure](img/B05137_05_311.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A higher value of homogeneity is more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Completeness**: Completeness is defined as the mirror property of Homogeneity,
    that is, having all instances of a single class belong to the same cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Homogeneity, conditional entropies and cluster entropy are defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V-Measure](img/B05137_05_312.jpg)![V-Measure](img/B05137_05_313.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Completeness is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V-Measure](img/B05137_05_314.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'V-Measure is defined as the harmonic mean of homogeneity and completeness using
    a weight factor *β*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V-Measure](img/B05137_05_316.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A higher value of completeness or V-measure is better.
  prefs: []
  type: TYPE_NORMAL
- en: Other external measures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some external measures which are quite popular in comparing the clustering
    algorithms or measuring the effectiveness of clustering when the classes are known
    are given next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Purity** and **Entropy**: They are similar to homogeneity and completeness
    defined previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other external measures](img/B05137_05_317.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Entropy is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other external measures](img/B05137_05_318.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *q* = number of classes, *k* = number of clusters, *n*r = size of cluster
    *r* and ![Other external measures](img/B05137_05_320.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**, **Recall**, and **F-Measure**: Information retrieval measures
    modified for clustering algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given, ![Other external measures](img/B05137_05_321.jpg) and ![Other external
    measures](img/B05137_05_322.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other external measures](img/B05137_05_323.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other external measures](img/B05137_05_324.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'F-measures is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other external measures](img/B05137_05_325.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised learning using outlier detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The subject of finding outliers or anomalies in the data streams is one of the
    emerging fields in machine learning. This area has not been explored by researchers
    as much as classification and clustering-based problems have. However, there have
    been some very interesting ideas extending the concepts of clustering to find
    outliers from data streams. We will provide some of the research that has been
    proved to be very effective in stream outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Partition-based clustering for outlier detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central idea here is to use an online partition-based clustering algorithm
    and based on either cluster size ranking or inter-cluster distance ranking, label
    the clusters as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Here we present one such algorithm proposed by Koupaie *et al*., using incremental
    k-Means.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Only numeric features are used, as in most k-Means algorithms. The number of
    clusters *k* and the number of windows of outliers *n*, on which offline clustering
    happens, are input parameters. The output is constant outliers (local and global)
    and an updatable model that detects these.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This algorithm works by having the k-Means algorithm in two modes, an offline
    mode and an online mode, both working in parallel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the online mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply k-Means on the given window *w* and find clusters and partitions of the
    data with clusters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank the clusters based on the cluster distances and cluster size. The clusters
    which are farthest apart and small in size are considered outliers.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the outliers in memory for the window as a set *O*[w] = {**x**[1], **x**[2..]**x**[n]}
    and regard them as local outliers.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The window is cleared and the process is repeated.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the offline mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get outliers from *n*, previous windows, and create a set: ![How does it work?](img/B05137_05_328.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster this window with set *S* using k-Means and find clusters which are farthest
    away and small in size.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: These clusters are global outliers.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The window is cleared and the process is repeated.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is very sensitive to the two parameters *k* and *n* and can generate lots
    of noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only spherical clusters/outliers are found and outliers with different shapes
    get missed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance-based clustering for outlier detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distance-based outlier detection is the most studied, researched, and implemented
    method in the area of stream learning. There are many variants of the distance-based
    methods, based on sliding windows, the number of nearest neighbors, radius and
    thresholds, and other measures for considering outliers in the data. We will try
    to give a sampling of the most important algorithms in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most algorithms take the following parameters as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Window size *w*, corresponding to the fixed size on which the algorithm looks
    for outlier patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sliding size *s*, corresponds to the number of new instances that will be added
    to the window, and old ones removed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The count threshold *k* of instances when using nearest neighbor computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance threshold *R* used to define the outlier threshold in distances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers as labels or scores (based on neighbors and distance) are outputs.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present different variants of distance-based stream outlier algorithms, giving
    insights into what they do differently or uniquely. The unique elements in each
    algorithm define what happens when the slide expires, how a new slide is processed,
    and how outliers are reported.
  prefs: []
  type: TYPE_NORMAL
- en: Exact Storm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Exact Storm stores the data in the current window *w* in a well-known index
    structure, so that the range query search or query to find neighbors within the
    distance *R* for a given point is done efficiently. It also stores *k* preceding
    and succeeding neighbors of all data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expired Slide**: Instances in expired slides are removed from the index structure
    that affects range queries but are preserved in the preceding list of neighbors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New Slide**: For each data point in the new slide, range query *R* is executed,
    results are used to update the preceding and succeeding list for the instance,
    and the instance is stored in the index structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier Reporting**: In any window, after the processing of expired and new
    slide elements is complete, any instance with at least *k* elements from the succeeding
    list and non-expired preceding list is reported as an outlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstract-C
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Abstract-C keeps the index structure similar to Exact Storm but instead of
    preceding and succeeding lists for every object it just maintains a list of counts
    of neighbors for the windows the instance is participating in:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expired Slide**: Instances in expired slides are removed from the index structure
    that affects range queries and the first element from the list of counts is removed
    corresponding to the last window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New Slide**: For each data point in the new slide, range query *R* is executed
    and results are used to update the list count. For existing instances, the count
    gets updated with new neighbors and instances are added to the index structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier Reporting**: In any window, after the processing of expired and new
    slide elements is complete, all instances with a neighbors count less than *k*
    in the current window are considered outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct Update of Events (DUE)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DUE keeps the index structure for efficient range queries exactly like the
    other algorithms but has a different assumption, that when an expired slide occurs,
    not every instance is affected in the same way. It maintains two priority queues:
    the unsafe inlier queue and the outlier list. The unsafe inlier queue has sorted
    instances based on the increasing order of smallest expiration time of their preceding
    neighbors. The outlier list has all the outliers in the current window:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expired Slide**: Instances in expired slides are removed from the index structure
    that affects range queries and the unsafe inlier queue is updated for expired
    neighbors. Those unsafe inliers which become outliers are removed from the priority
    queue and moved to the outlier list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New Slide**: For each data point in the new slide, range query *R* is executed,
    results are used to update the succeeding neighbors of the point, and only the
    most recent preceding points are updated for the instance. Based on the updates,
    the point is added to the unsafe inlier priority queue or removed from the queue
    and added to the outlier list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier Reporting**: In any window, after the processing of expired and new
    slide elements is complete, all instances in the outlier list are reported as
    outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro Clustering based Algorithm (MCOD)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Micro-clustering based outlier detection overcomes the computational issues
    of performing range queries for every data point. The micro-cluster data structure
    is used instead of range queries in these algorithms. A micro-cluster is centered
    around an instance and has a radius of *R*. All the points belonging to the micro-clusters
    become inliers. The points that are outside can be outliers or inliers and stored
    in a separate list. It also has a data structure similar to DUE to keep a priority
    queue of unsafe inliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expired Slide**: Instances in expired slides are removed from both micro-clusters
    and the data structure with outliers and inliers. The unsafe inlier queue is updated
    for expired neighbors as in the DUE algorithm. Micro-clusters are also updated
    for non-expired data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New Slide**: For each data point in the new slide, the instance either becomes
    a center of a micro-cluster, or part of a micro-cluster or added to the event
    queue and the data structure of the outliers. If the point is within the distance
    *R*, it gets assigned to an existing micro-cluster; otherwise, if there are *k*
    points within *R*, it becomes the center of the new micro cluster; if not, it
    goes into the two structures of the event queue and possible outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier Reporting**: In any window, after the processing of expired and new
    slide elements is complete, any instance in the outlier structure with less than
    *k* neighboring instances is reported as an outlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approx Storm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Approx Storm, as the name suggests, is an approximation of Exact Storm. The
    two approximations are:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of data points in the window by adding a factor *ρ* and
    changing the window to *ρW*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the number instead of the data structure of preceding neighbors by using
    the fraction of the number of neighbors which are safe inliers in the preceding
    list to the number in the current window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processing of expired and new slides and how outliers are determined based
    on these steps follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expired Slide**: Same as Exact Storm—instances in expired slides are removed
    from the index structure that affects range queries but preserved in the preceding
    list of neighbors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**New Slide**: For each data point in the new slide, range query *R* is executed,
    results are used to compute the fraction discussed previously, and the index structure
    is updated. The number of safe inliers are constrained to *ρW* by removing random
    inliers if the size exceeds that value. The assumption is that most of the points
    in safe inliers are safe.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outlier Reporting**: In any window, after the processing of expired and new
    slide elements has been completed, when an approximation (see *References* [17])
    of the number of neighbors of an instance based on the fraction, window size,
    and preceding list is a value less than *k*, it is considered as an outlier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Exact Storm is demanding in storage and CPU for storing lists and retrieving
    neighbors. Also, it introduces delays; even though they are implemented in efficient
    data structures, range queries can be slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstract-C has a small advantage over Exact Storm, as no time is spent on finding
    active neighbors for each instance in the window. The storage and time spent is
    still very much dependent on the window and slide chosen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DUE has some advantage over Exact Storm and Abstract-C as it can efficiently
    re-evaluate the "inlierness" of points (that is, whether unsafe inliers remain
    inliers or become outliers) but sorting the structure impacts both CPU and memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCOD has distinct advantages in memory and CPU owing to the use of the micro-cluster
    structure and removing the pair-wise distance computation. Storing the neighborhood
    information in micro-clusters helps memory too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approx Storm has an advantage of time over the others as it doesn't process
    the expired data points over the previous window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation and evaluation techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Validation and evaluation of stream-based outliers is still an open research
    area. In many research comparisons, we see various metrics being used, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Time to evaluate in terms of CPU times per object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of outliers detected in the streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of outliers that correlate to existing labels, TP/Precision/Recall/Area
    under PRC curve, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By varying parameters such as window-size, neighbors within radius, and so on,
    we determine the sensitivity to the performance metrics mentioned previously and
    determine the robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Case study in stream learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The case study in this chapter consists of several experiments that illustrate
    different methods of stream-based machine learning. A well-studied dataset was
    chosen as the stream data source and supervised tree based methods such as Naïve
    Bayes, Hoeffding Tree, as well as ensemble methods, were used. Among unsupervised
    methods, clustering algorithms used include k-Means, DBSCAN, CluStream, and CluTree.
    Outlier detection techniques include MCOD and SimpleCOD, among others. We also
    show results from classification experiments that demonstrate handling concept
    drift. The ADWIN algorithm for calculating statistics in a sliding window, as
    described earlier in this chapter, is employed in several algorithms used in the
    classification experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Tools and software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular and arguably the most comprehensive Java-based frameworks
    for data stream mining is the open source **Massive Online Analysis** (**MOA**)
    software created by the University of Waikato. The framework is a collection of
    stream classification, clustering, and outlier detection algorithms and has support
    for change detection and concept drift. It also includes data generators and several
    evaluation tools. The framework can be extended with new stream data generators,
    algorithms, and evaluators. In this case study, we employ several stream data
    learning methods using a file-based data stream.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Product homepage: [http://moa.cms.waikato.ac.nz/](http://moa.cms.waikato.ac.nz/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [https://github.com/Waikato/moa](https://github.com/Waikato/moa)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the series of screenshots from the MOA tool shown in *Figure 4*
    and *Figure 5*, the top-level menu lets you choose the type of learning to be
    done. For the classification experiments, for example, configuration of the tools
    consists of selecting the task to run (selected to be prequential evaluation here),
    and then configuring which learner and evaluator we want to use, and finally,
    the source of the data stream. A window width parameter shown in the **Configure
    Task** dialog can affect the accuracy of the model chosen, as we will see in the
    experiment results. Other than choosing different values for the window width,
    all base learner parameters were left as default values. Once the task is configured
    it is run by clicking the **Run** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and software](img/B05137_05_339.jpg)![Tools and software](img/B05137_05_340.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. MOA graphical interface for configuring prequential evaluation for
    classification which includes setting the window width
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and software](img/B05137_05_341.jpg)![Tools and software](img/B05137_05_342.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. MOA graphical interface for prequential classification task. Within
    the Configure task, you must choose a learner, locate the data stream (details
    not shown), and select an evaluator
  prefs: []
  type: TYPE_NORMAL
- en: After the task has completed running, model evaluation results can be exported
    to a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem for this case study is to continuously learn from a stream of electricity
    market data and predict the direction of movement of the market price. We compare
    the accuracy and average cost of different classification methods including concept
    drift as well as the performance of clustering and outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset used in this case study can be used to illustrate classical batch-based
    supervised and unsupervised learning techniques. However, here we treat it as
    a stream-based data source to show how we can employ the techniques described
    in this chapter to perform classification, clustering, and outlier detection tasks
    using the MOA framework. Within this context, we demonstrate how incremental learning
    can be achieved under assumptions of a stationary as well as an evolving data
    stream exhibiting concept drift.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset is known as the Electricity or ELEC dataset, which was collected
    by the New South Wales Electricity Market. The prices in this market are variable,
    and are adjusted every 5 minutes based on supply and demand. This dataset consists
    of 45,312 such data points obtained every half-hour between May 1996 and December
    1998\. The target is an indication of the movement of the price, whether up or
    down, relative to the 24-hour moving average.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data file is a publicly available file in the ARRF format at [http://downloads.sourceforge.net/project/moa-datastream/Datasets/Classification/elecNormNew.arff.zip?r=http%3A%2F%2Fmoa.cms.waikato.ac.nz%2Fdatasets%2F&ts=1483128450&use_mirror=cytranet](http://downloads.sourceforge.net/project/moa-datastream/Datasets/Classification/elecNormNew.arff.zip?r=http%3A%2F%2Fmoa.cms.waikato.ac.nz%2Fdatasets%2F&ts=1483128450&use_mirror=cytranet).
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the experiments conducted here, no data sampling is done; each example in
    the dataset is processed individually and no example is excluded. All numeric
    data elements have been normalized to a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis and dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ELEC dataset has 45,312 records with nine features, including the target
    class. The features class and day are nominal (categorical), all others are numeric
    (continuous). The features are listed in *Table 1* and *Table 2* and give descriptive
    statistics for the ELEC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Data Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| class | nominal | UP, DOWN—direction of price movement relative to 24-hour
    moving average |'
  prefs: []
  type: TYPE_TB
- en: '| date | continuous | Date price recorded |'
  prefs: []
  type: TYPE_TB
- en: '| day | nominal | Day of the week (1-7) |'
  prefs: []
  type: TYPE_TB
- en: '| period | continuous |   |'
  prefs: []
  type: TYPE_TB
- en: '| nswprice | continuous | Electricity price in NSW |'
  prefs: []
  type: TYPE_TB
- en: '| nswdemand | continuous | Electricity demand in NSW |'
  prefs: []
  type: TYPE_TB
- en: '| vicprice | continuous | Electricity price in Victoria |'
  prefs: []
  type: TYPE_TB
- en: '| vicdemand | continuous | Electricity demand in Victoria |'
  prefs: []
  type: TYPE_TB
- en: '| transfer | integer |   |'
  prefs: []
  type: TYPE_TB
- en: '*Table 1\. ELEC dataset features*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|   | count | mean | std | 25% | 50% | 75% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| date | 45312 | 0.49908 | 0.340308 | 0.031934 | 0.456329 | 0.880547 |'
  prefs: []
  type: TYPE_TB
- en: '| period | 45312 | 0.5 | 0.294756 | 0.25 | 0.5 | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| nswprice | 45312 | 0.057868 | 0.039991 | 0.035127 | 0.048652 | 0.074336 |'
  prefs: []
  type: TYPE_TB
- en: '| nswdemand | 45312 | 0.425418 | 0.163323 | 0.309134 | 0.443693 | 0.536001
    |'
  prefs: []
  type: TYPE_TB
- en: '| vicprice | 45312 | 0.003467 | 0.010213 | 0.002277 | 0.003467 | 0.003467 |'
  prefs: []
  type: TYPE_TB
- en: '| vicdemand | 45312 | 0.422915 | 0.120965 | 0.372346 | 0.422915 | 0.469252
    |'
  prefs: []
  type: TYPE_TB
- en: '| transfer | 45312 | 0.500526 | 0.153373 | 0.414912 | 0.414912 | 0.605702 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 2\. Descriptive statistics of ELEC dataset features*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The feature reduction step is omitted here as it is in most stream-based learning.
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The experiments are divided into classification, concept drift, clustering,
    and outlier detection. Details of the learning process for each set of experiments
    and the results of the experiments are given here.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this set of experiments, a choice of linear, non-linear, and ensemble learners
    were chosen in order to illustrate the behavior of a variety of classifiers. **Stochastic
    Gradient Descent** (**SGD**), which uses a linear SVM, and Naïve Bayes are the
    linear classifiers, while Lazy k-NN is the non-linear classifier. For ensemble
    learning, we use two meta-learners, **Leveraging Bagging** (**LB**) and OxaBag,
    with different linear and non-linear base learners such as SGD, Naïve Bayes, and
    Hoeffding Trees. The algorithm used in OxaBag is described in the section on ensemble
    algorithms. In LB, the weight factor used for resampling is variable (the default
    value of 6 is used here) whereas the weight in OxaBag is fixed at 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prequential evaluation is chosen for all the classification methods, so each
    example is first tested against the prediction with the existing model, and then
    used for training the model. This requires the selection of a window width, and
    the performance of the various models for different values of the window width
    are listed in *Table 3*. Widths of 100, 500, 1,000, and 5,000 elements were used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Window width | Evaluation time (CPU seconds) | Model cost (RAM-Hours)
    | Classifications correct (percent) | Kappa Statistic (percent) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 100 | 0.5781 | 3.76E-10 | 67 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 500 | 0.5781 | 3.76E-10 | 55.6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 1000 | 0.5469 | 3.55E-10 | 53.3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 5000 | 0.5469 | 3.55E-10 | 53.78 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| NaiveBayes | 100 | 0.7656 | 8.78E-10 | 86 | 65.7030 |'
  prefs: []
  type: TYPE_TB
- en: '| NaiveBayes | 500 | 0.6094 | 8.00E-10 | 82.2 | 62.6778 |'
  prefs: []
  type: TYPE_TB
- en: '| NaiveBayes | 1000 | 0.6719 | 7.77E-10 | 75.3 | 48.8583 |'
  prefs: []
  type: TYPE_TB
- en: '| NaiveBayes | 5000 | 0.6406 | 7.35E-10 | 77.84 | 54.1966 |'
  prefs: []
  type: TYPE_TB
- en: '| kNN | 100 | 34.6406 | 4.66E-06 | 74 | 36.3057 |'
  prefs: []
  type: TYPE_TB
- en: '| kNN | 500 | 34.5469 | 4.65E-06 | 79.8 | 59.1424 |'
  prefs: []
  type: TYPE_TB
- en: '| kNN | 1000 | 35.8750 | 4.83E-06 | 82.5 | 64.8049 |'
  prefs: []
  type: TYPE_TB
- en: '| kNN | 5000 | 35.0312 | 4.71E-06 | 80.32 | 60.4594 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-kNN | 100 | 637.8125 | 2.88E-04 | 74 | 36.3057 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-kNN | 500 | 638.9687 | 2.89E-04 | 79.8 | 59.1424 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-kNN | 1000 | 655.8125 | 2.96E-04 | 82.4 | 64.5802 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-kNN | 5000 | 667.6094 | 3.02E-04 | 80.66 | 61.0965 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-HoeffdingTree | 100 | 13.6875 | 2.98E-06 | 91 | 79.1667 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-HoeffdingTree | 500 | 13.5781 | 2.96E-06 | 93 | 85.8925 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-HoeffdingTree | 1000 | 12.5625 | 2.74E-06 | 92.1 | 84.1665 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-HoeffdingTree | 5000 | 12.7656 | 2.78E-06 | 90.74 | 81.3184 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 3\. Classifier performance for different window sizes*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For the algorithms in *Table 4*, the performance was the same for each value
    of the window width used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Evaluation time (CPU seconds) | Model cost (RAM-Hours) | Classifications
    correct (percent) | Kappa Statistic (percent) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| HoeffdingTree | 1.1562 | 3.85E-08 | 79.1953 | 57.2266 |'
  prefs: []
  type: TYPE_TB
- en: '| HoeffdingAdaptiveTree | 2.0469 | 2.84E-09 | 83.3863 | 65.5569 |'
  prefs: []
  type: TYPE_TB
- en: '| OzaBag-NaiveBayes | 2.01562 | 1.57E-08 | 73.4794 | 42.7636 |'
  prefs: []
  type: TYPE_TB
- en: '| OzaBagAdwin-HoeffdingTree | 5.7812 | 2.26E-07 | 84.3485 | 67.5221 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-SGD | 2 | 1.67E-08 | 57.6977 | 3.0887 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-NaiveBayes | 3.5937 | 3.99E-08 | 78.8753 | 55.7639 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 4\. Performance of classifiers (same for all window widths used)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Concept drift experiments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this experiment, we continue using EvaluatePrequential when configuring
    the classification task. This time we select the `DriftDetectionMethodClassifier`
    as the learner and DDM as the drift detection method. This demonstrates adapting
    to an evolving data stream. Base learners used and the results obtained are shown
    in *Table 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Evaluation time (CPU seconds) | Model cost (RAM-Hours) | Classifications
    correct (percent) | Kappa Statistic (percent) | Change detected |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 0.307368829 | 1.61E-09 | 53.3 | 0 | 132 |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve-Bayes | 0.298290727 | 1.58E-09 | 86.6 | 73.03986 | 143 |'
  prefs: []
  type: TYPE_TB
- en: '| Lazy-kNN | 10.34161893 | 1.74E-06 | 87.4 | 74.8498 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| HoeffdingTree | 0.472981754 | 5.49E-09 | 86.2 | 72.19816 | 169 |'
  prefs: []
  type: TYPE_TB
- en: '| HoeffdingAdaptiveTree | 0.598665043 | 7.19E-09 | 84 | 67.80878 | 155 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-SGD | 0.912737325 | 2.33E-08 | 53.3 | 0 | 132 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-NaiveBayes | 1.990137758 | 3.61E-08 | 85.7 | 71.24056 | 205 |'
  prefs: []
  type: TYPE_TB
- en: '| OzaBag-NaiveBayes | 1.342189725 | 2.29E-08 | 77.4 | 54.017 | 211 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-kNN | 173.3624715 | 1.14E-04 | 87.5 | 75.03296 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| LB-HoeffdingTree | 5.660440101 | 1.61E-06 | 91.3 | 82.56317 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| OzaBag-HoeffdingTree | 4.306455545 | 3.48E-07 | 85.4 | 70.60209 | 125 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 5\. Performance of classifiers with concept drift detection*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clustering experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Almost all of the clustering algorithms implemented in the MOA tool were used
    in this experiment. Both extrinsic and intrinsic evaluation results were collected
    and are tabulated in *Table 6*. CMM, homogeneity, and completeness were defined
    earlier in this chapter. We have encountered Purity and Silhouette coefficients
    before, from the discussion in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*. SSQ
    is the sum of squared distances of instances from their respective cluster centers;
    the lower the value of SSQ, the better. The use of micro-clustering is indicated
    by *m = 1* in the table. How often the macro-clusters are calculated is determined
    by the selected time horizon *h*, in instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | CMM | Homogeneity | Completeness | Purity | SSQ | Silhouette
    Coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clustream With k-Means (h = 5000; k = 2; m = 1) | 0.7168 | -1.0000 | 0.1737
    | 0.9504 | 9.1975 | 0.5687 |'
  prefs: []
  type: TYPE_TB
- en: '| Clustream With k-Means(h = 1000; k = 5) | 0.5391 | -1.0000 | 0.8377 | 0.7238
    | 283.6543 | 0.8264 |'
  prefs: []
  type: TYPE_TB
- en: '| Clustream (h = 1000; m = 1) | 0.6241 | -1.0000 | 0.4363 | 0.9932 | 7.2734
    | 0.4936 |'
  prefs: []
  type: TYPE_TB
- en: '| Denstream With DBSCAN (h = 1000) | 0.4455 | -1.0000 | 0.7586 | 0.9167 | 428.7604
    | 0.6682 |'
  prefs: []
  type: TYPE_TB
- en: '| ClusTree (h = 5000; m = 1) | 0.7984 | 0.4874 | -0.4815 | 0.9489 | 11.7789
    | 0.6879 |'
  prefs: []
  type: TYPE_TB
- en: '| ClusTree (h = 1000; m = 1) | 0.7090 | -1.0000 | 0.3979 | 0.9072 | 13.4190
    | 0.5385 |'
  prefs: []
  type: TYPE_TB
- en: '| AbstractC | 1.0000 | 1.0000 | -8.1354 | 1.0000 | 0.0000 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: '| MCOD (w = 1000) | 1.0000 | 1.0000 | -8.1354 | 1.0000 | 0.0000 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: Outlier detection experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the final set of experiments, five outlier detection algorithms were used
    to process the ELEC dataset. Results are given in *Table 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Nodes always inlier | Nodes always outlier | Nodes both inlier
    and outlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MCOD | 42449 (93.7%) | 302 (0.7%) | 2561 (5.7%) |'
  prefs: []
  type: TYPE_TB
- en: '| ApproxSTORM | 41080 (90.7%) | 358 (0.8%) | 3874 (8.5%) |'
  prefs: []
  type: TYPE_TB
- en: '| SimpleCOD | 42449 (93.7%) | 302 (0.7%) | 2561 (5.7%) |'
  prefs: []
  type: TYPE_TB
- en: '| AbstractC | 42449 (93.7%) | 302 (0.7%) | 2561 (5.7%) |'
  prefs: []
  type: TYPE_TB
- en: '| ExactSTORM | 42449 (93.7%) | 302 (0.7%) | 2561 (5.7%) |'
  prefs: []
  type: TYPE_TB
- en: '*Table 7\. Evaluation of outlier detection*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following plots (*Figure 6*) show results for three pairs of features after
    running algorithm Abstract-C on the entire dataset. In each of the plots, it is
    easy to see the outliers identified by the circles surrounding the data points.
    Although it is difficult to visualize the outliers spatially in multiple dimensions
    simultaneously, the set bi-variate plots give some indication of the result of
    outlier detection methods applied in a stream-based setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outlier detection experiments](img/B05137_05_343.jpg)![Outlier detection
    experiments](img/B05137_05_344.jpg)![Outlier detection experiments](img/B05137_05_345.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Outlier detection using Abstract-C, for three pairs of features,
    after processing all 45,300 instances
  prefs: []
  type: TYPE_NORMAL
- en: 'The image in *Figure 7* shows a screenshot from MOA when two algorithms, `Angiulli.ExactSTORM`
    and `Angiulli.ApproxSTORM`, were run simultaneously; a bivariate scatter-plot
    for each algorithm is shown side-by-side, accompanied by a comparison of the per-object
    processing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outlier detection experiments](img/B05137_05_346.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Visualization of outlier detection in MOA
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of stream learning results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the evaluation of the learned models from the classification, clustering,
    and outlier detection experiments, analysis reveals several interesting observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance improves from linear to non-linear algorithms in quite a significant
    way as shown in Table 3\. Linear SGD has the best performance using an accuracy
    metric of 67%, while KNN and Hoeffding Tree show 82.4 to 93%. This clearly shows
    that the problem is non-linear and using a non-linear algorithm will give better
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-NNs give good performance but come at the cost of evaluation time, as shown
    in Table 3\. Evaluation time as well as memory are significantly higher—about
    two orders of magnitude—than the linear methods. When the model has to perform
    in tighter evaluation cycles, extreme caution in choosing algorithms such as KNNs
    must be observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoeffding Trees gives the best classification rate and Kappa statistic. The
    evaluation time is also not as high as KNNs but is still in the order of seconds,
    which may or may not be acceptable in many real-time stream-based applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evaluation time of Naive Bayes is the lowest—though not much different from
    SGD—and with the right choice of window width can give performance second only
    to Hoeffding Trees. For example, at width 100, we have a classification rate of
    86 with Naïve Bayes, which is next best to 93 of Hoeffding Trees but compared
    to over 13 seconds, Naïve Bayes takes only 0.76 seconds, as shown in *Table 3*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the window width constant, there is a clear pattern of improvement from
    linear (SGD, Naïve Bayes) to non-linear (Hoeffding Trees) to ensemble based (OzaBag,
    Adwin, Hoeffding Tree) shown in Table 4\. This clearly shows that in theory, the
    choice of ensembles can help reduce the errors but comes at the cost of foregoing
    interpretability in the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 5*, when compared to *Table 3* and *Table 4*, shows why having drift
    protection and learning with automated drift detection increases robustness. Ensemble-based
    learning of OzaBag-NaiveBayes, OzaBag-HoeffdingTrees, and OzaBag-HoeffdingAdaptiveTree
    all show improvements over the non- drift protected runs as an example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: From the first two models in *Table 6*, we see that k-Means, with a horizon
    of 5,000 instances and *k* of 2, exhibits higher purity, higher CMM, and lower
    SSQ compared to the model with a smaller horizon and *k* of 5\. In the complete
    set of results (available on this book's website, see link below), one can see
    that the effect of the larger horizon is the predominant factor responsible for
    the differences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the clustering models using micro-clustering, the SSQ is typically significantly
    smaller than when no micro-clustering is used. This is understandable, as there
    are far more clusters and fewer instances per cluster, and SSQ is measured with
    respect to the cluster center.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN was found to be insensitive to micro-clustering, and size of the horizon.
    Compared to all other models, it ranks high on both intrinsic (Silhouette coefficient)
    as well as extrinsic measures (Completeness, Purity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two ClusTree models have among the best CMM and Purity scores, with low
    SSQ due to micro-clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final two outlier-based clustering algorithms have perfect CMM and Purity
    scores. The metrics are not significantly affected by the window size (although
    this impacts the evaluation time), or the value of k, the neighbor count threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Outlier detection experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: All the techniques in this set of experiments performed equally well, with the
    exception of ApproxSTORM, which is to be expected considering the reduced window
    used in this method compared to the exact version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio of instances, always inlier versus those always outlier, is close
    to 140 for the majority of the models. Whether this implies adequate discriminatory
    power for a given dataset depends on the goals of the real-time learning problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All MOA configuration files and results from the experiments are available
    at: [https://github.com/mjmlbook/mastering-java-machine-learning/Chapter5](https://github.com/mjmlbook/mastering-java-machine-learning/Chapter5).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The assumptions in stream-based learning are different from batch-based learning,
    chief among them being upper bounds on operating memory and computation times.
    Running statistics using sliding windows or sampling must be computed in order
    to scale to a potentially infinite stream of data. We make the distinction between
    learning from stationary data, where it is assumed the generating data distribution
    is constant, and dynamic or evolving data, where concept drift must be accounted
    for. This is accomplished by techniques involving the monitoring of model performance
    changes or the monitoring of data distribution changes. Explicit and implicit
    adaptation methods are ways to adjust to the concept change.
  prefs: []
  type: TYPE_NORMAL
- en: Several supervised and unsupervised learning methods have been adapted for incremental
    online learning. Supervised methods include linear, non-linear, and ensemble techniques,
    The HoeffdingTree is introduced which is particularly interesting due largely
    in part to its guarantees on upper bounds on error rates. Model validation techniques
    such as prequential evaluation are adaptations unique to incremental learning.
    For stationary supervised learning, evaluation measures are similar to those used
    in batch-based learning. Other measures are used in the case of evolving data
    streams.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms operating under fixed memory and time constraints typically
    use small memory buffers with standard techniques in a single pass. Issues specific
    to streaming must be considered during evaluations of clustering, such as aging,
    noise, and missed or misplaced points. Outlier detection in data streams is a
    relatively new and growing field. Extending ideas in clustering to anomaly detection
    has proved very effective.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments in the case study in this chapter use the Java framework MOA,
    illustrating various stream-based learning techniques for supervised, clustering,
    and outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we embark on a tour of the probabilistic graph modelling
    techniques that are useful in representing, eliciting knowledge, and learning
    in various domains.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'G. Cormode and S. Muthukrishnan (2010). *An improved data stream summary: The
    Count-Min sketch and its applications*. Journal of Algorithms, 55(1):58–75, 2005.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: João Gama (2010). *Knowledge Discovery from Data Streams, Chapman and Hall /
    CRC Data Mining and Knowledge Discovery Series*, CRC Press 2010, ISBN 978-1-4398-2611-9,
    pp. I-XIX, 1-237.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B. Babcock, M. Datar, R. Motwani (2002). *Sampling from a moving window over
    streaming data*, in Proceedings of the thirteenth annual ACM-SIAM symposium on
    Discrete algorithms, pp.633–634, 2002.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bifet, A. and Gavalda, R. (2007). *Learning from time-changing data with adaptive
    windowing*. In Proceedings of SIAM int. conf. on Data Mining. SDM. 443–448.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vitter, J. (1985). *Random sampling with a reservoir. ACM Trans. Math. Softw*.
    11, 1, 37–57.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gama, J., Medas, P., Castillo, G., and Rodrigues, P. (2004). *Learning with
    drift detection*. In Proceedings of the 17th Brazilian symp. on Artif. Intell.
    SBIA. 286–295\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gama, J., Sebastiao, R., and Rodrigues, P. 2013\. *On evaluating stream learning
    algorithms*. Machine Learning 90, 3, 317–346.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Domingos, P. and Hulten, G. (2000). *Mining high-speed data streams*. In Proceedings.
    of the 6th ACM SIGKDD int. conference on Knowledge discovery and data mining.
    KDD. 71–80.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oza, N. (2001). *Online ensemble learning*. Ph.D. thesis, University of California
    Berkeley.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., Bouchachia, A. (2014).
    *A Survey on Concept Drift Adaptation.ACM Computing Surveys* 46(4), Article No.
    44.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Farnstrom, F., Lewis, J., and Elkan, C. (2000). *Scalability for clustering
    algorithms revisited*. SIGKDD Exploration, 51–57.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhang, T., Ramakrishnan, R., and Livny, M. (1996). *BIRCH: An Efficient Data
    Clustering Method for Very Large Databases*. In Proceedings of the ACM SIGMOD
    International Conference on Management of Data. ACM Press, 103–114.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggarwal, C. (2003). *A Framework for Diagnosing Changes in Evolving Data Streams*.
    In ACM SIGMOD Conference. 575–586.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chen, Y. and Tu, L. (2007). *Density-based clustering for real-time stream
    data*. In KDD ''07: Proceedings of the 13th ACM SIGKDD international conference
    on knowledge discovery and data mining. ACM Press, 133–142.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kremer, H., Kranen, P., Jansen, T., Seidl, T., Bifet, A., Holmes, G., and Pfahringer,
    B. (2011). *An effective evaluation measure for clustering on evolving data streams*.
    In proceedings of the 17th ACM SIGKDD international conference on knowledge discovery
    and data mining. KDD '11\. ACM, New York, NY, USA, 868–876.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mahdiraji, A. R. (2009). *Clustering data stream: A survey of algorithms*.
    International Journal of Knowledge-Based and Intelligent Engineering Systems,
    39–44.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. Angiulli and F. Fassetti (2007). *Detecting distance-based outliers in streams
    of data*. In proceedings of the Sixteenth ACM Conference on Information and Knowledge
    Management, CIKM '07, pages 811–820, New York, NY, USA, 2007\. ACM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Yang, E. A. Rundensteiner, and M. O. Ward (2009). *Neighbor-based pattern
    detection for windows over streaming data*. In Proceedings of the 12th International
    Conference on Extending Database Technology: Advances in Database Technology,
    EDBT ''09, pages 529–540, New York, NY, USA, 2009\. ACM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M. Kontaki, A. Gounaris, A. Papadopoulos, K. Tsichlas, and Y. Manolopoulos (2011).
    *Continuous monitoring of distance-based outliers over data streams*. In Data
    Engineering (ICDE), 2011 IEEE 27th International Conference on, pages 135–146,
    April 2011.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
