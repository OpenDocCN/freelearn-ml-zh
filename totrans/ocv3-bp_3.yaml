- en: Chapter 3. Recognizing Facial Expressions with Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic facial expression recognition has attracted much attention since the
    early nineties, especially in human-computer interaction. As computers start becoming
    a part of our life, they need to become more and more intelligent. Expression
    recognition systems will enhance this intelligent interaction between the human
    and the computer.
  prefs: []
  type: TYPE_NORMAL
- en: Although humans can recognize facial expressions easily, a reliable expression
    recognition system is still a challenge. In this chapter, we will introduce a
    basic implementation of facial expression using various algorithms from the OpenCV
    library, including feature extraction and classification using the ml module.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be going through the following topics in brief:'
  prefs: []
  type: TYPE_NORMAL
- en: A simple architecture to recognize human facial expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction algorithms in the OpenCV library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning and testing stage, with various machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing facial expression recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic facial expression recognition is an interesting and challenging problem
    and has several important applications in many areas such as human-computer interaction,
    human behavior understanding, and data-driven animation. Unlike face recognition,
    facial expression recognition needs to discriminate between the same expression
    in different individuals. The problem becomes more difficult as a person may show
    the same expression in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'The current existing approaches for measuring facial expressions can be categorized
    into two types: static image and image sequence. In the static image approach,
    the system analyzes the facial expression in each image frame separately. In the
    image sequence approach, the system tries to capture the temporal pattern of the
    motion and changes seen on the face in the sequence of image frames. Recently,
    attention has been shifted toward the image sequence approach. However, this approach
    is more difficult and requires more computation than the static approach. In this
    chapter, we will follow the static image approach and compare several algorithms
    using the OpenCV 3 library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem of automatic facial expression recognition includes three sub-problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the face region in the image**: The precise position of the face
    is very important for facial analysis. In this problem, we want to find the face
    region in the image. This problem can be viewed as a detection problem. In our
    implementation, we will use the cascade classifier in OpenCV''s objdetect module
    to detect the faces. However, the cascade classifier is prone to alignment error.
    Therefore, we apply the flandmark library to extract the facial landmarks from
    the face region and use these landmarks to extract the precise face region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Flandmark is an open source C library implementing a facial landmark detector.
    You can get more information about flandmark in the following sections. Basically,
    you can use whatever library you want to extract the landmarks. In our implementation,
    we will use this library to reduce complexity while integrating the library into
    our project.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Extracting features from the face region**: Given the face region, the system
    will extract facial expression information as a feature vector. The feature vector
    encodes the relevant information from the input data. In our implementation, the
    feature vector is obtained by using the combination of the feature detector from
    the feature2d module and the kmeans algorithm from the core module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classifying the features into emotion categories**: This is a classification
    problem. The system uses classification algorithms to map the extracted feature
    from the previous step to an emotion category (such as happy, neutral, or sad).
    This is the main subject of the chapter. We will evaluate machine learning algorithms
    from the ml module, including neural networks, the support vector machine, and
    K-Nearest-Neighbor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will show you a complete process for implementing
    a facial expression system. In the next section, you will find several approaches
    to improve system performance to suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Facial expression dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to simplify the chapter, we will use a dataset to demonstrate the
    process instead of a live camera. We will use a standard dataset, **Japanese Female
    Facial Expression** (**JAFFE**).There are 214 images of 10 people in the dataset.
    Each person has three images of each expression. The dataset includes seven expressions
    (happy, sad, angry, disgust, fear, surprise, and neutral) as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You need to download the dataset from the following link: [http://www.kasrl.org/jaffe.html](http://www.kasrl.org/jaffe.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Facial expression dataset](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample image from the JAFFE dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the face region in the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you a basic approach to detect faces in an image.
    We will use the cascade classifier in OpenCV to detect the face location. This
    approach may have alignment errors. In order to obtain a precise location, we
    will also provide another advanced approach to find the face region using facial
    landmarks. In our implementation, we will only use the face region. However, many
    researchers use facial landmarks to extract facial components, such as eyes and
    mouths, and operate on these components separately.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to find out more, you should check the *Facial landmarks* section
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the face region using a face detection algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our implementation, we will use the Haar Feature-based cascade classifier
    in the objdetect module. In OpenCV, you can also extract the the face region with
    LBP-based cascade. LBP-based cascade is faster than Haar-based cascade. With the
    pre-trained model, the performance of LBP-based is lower than Haar-based cascade.
    However, it is possible to train an LBP-based cascade to attain the same performance
    as the Haar-based cascade.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to understand object detection in detail, you should check [Chapter
    5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69 "Chapter 5. Generic
    Object Detection for Industrial Applications"), *Generic Object Detection for
    Industrial Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for detecting faces is very simple. First, you need to load the pre-trained
    cascade classifier for faces into your OpenCV installation folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, load the input image in color mode, convert the image to grayscale, and
    apply histogram equalization to enhance the contrast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can find faces in the image. The `detectMultiScale` function stores
    all the detected faces in the vector as Rect(x, y, w, h):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the third parameter 1.1 is the scale factor, which specifies how
    much the image size will be resized at each scale. The following figure shows
    the scale pyramid using the scale factor. In our case, the scale factor is `1.1`.
    This means that the image size is reduced by 10%. The lower this factor is, the
    better chance we have of finding the faces. The scaling process starts with the
    original image and ends when the image resolution reaches the model dimension
    in the X or Y direction. However, the computation cost is high if we have too
    many scales. Therefore, if you want to reduce the number of scales, increase the
    scale factor to `1.2` (20%), `1.3` (30%) ,or more. If you want to increase the
    number of scales, reduce the scale factor to `1.05` (5%) or more. The fourth parameter
    `3` is the minimum number of neighbors that each candidate position should have
    to become a face position.
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting the face region using a face detection algorithm](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pyramid of image scales
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure is the result of face detection if we set the number of
    neighbors to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting the face region using a face detection algorithm](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: All the candidates for face regions
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the position of the face region can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each element of the faces vector is a `Rect` object. Therefore, we can get the
    position of the top-left corner with `faces[i].x` and `faces[i].y`. The position
    of the bottom-right corner is `faces[i].x + faces[i].width` and `faces[i].y +
    faces[i].height`. This information will be used as the initial position for the
    facial landmarks process, as described in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting facial landmarks from the face region
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One disadvantage of the face detector is that the results may have misalignment.
    The misalignment may happen in scaling or translation. Therefore, the extracted
    face regions in all images will not align with each other. This misalignment can
    lead to poor recognition performance, especially with DENSE features. With the
    help of facial landmarks, we can align all the extracted faces so that each facial
    component is in the same area over the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Many researchers make use of facial landmarks for classification with other
    emotion recognition approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the flandmark library to find the location of the eyes, nose and
    mouth. Then, we will use these facial landmarks to extract the precise facial
    bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the flandmark library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flandmark is an open source C library implementing a facial landmark detector.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can access the flandmark library main page at: [http://cmp.felk.cvut.cz/~uricamic/flandmark/](http://cmp.felk.cvut.cz/~uricamic/flandmark/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a face image, the goal of the flandmark library is to estimate an S shape
    that represents the location of the facial component. A facial shape in an S is
    an array of (x, y) positions shown as: S = [x[0]y[0]x[1]y[1]....x[n]y[n]].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pre-trained model in flandmark contains eight points, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the flandmark library](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The 8 landmarks model and the corresponding index for each landmark.
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation, we use flandmark because it is easy to integrate it into
    an OpenCV project. Besides, the flandmark library is really robust in many scenarios,
    even when the person is wearing glasses. In the following figure, we show the
    result of using the flandmark library on an image where the person is wearing
    dark glasses. The red dots indicate the facial landmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the flandmark library](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will show you the steps to download and use flandmark
    in our project.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and compiling the flandmark library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Flandmark is implemented in C and can be integrated into our project easily.
    However, we need to modify some headers in the library source to use it with OpenCV
    3\. The following are the steps to download and compile the library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the main page of the flandmark library and follow the GitHub link: [http://github.com/uricamic/flandmark](http://github.com/uricamic/flandmark)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the library to your local machine with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the `libflandmark` folder to your project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy `flandmark_model.dat,which is` in the data folder, to your project folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the `liblbp.h` file in `libflandmark` and change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: to
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit the `flandmark_detector.h` file in `libflandmark` and change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: to
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Edit `CMakeLists.txt` in your project folder to add the flandmark library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Link the executable file with the flandmark static library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the flandmark header to your source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Detecting facial landmarks with flandmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have finished the above steps, the process to extract facial components
    is very straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `FLANDMARK_Model` variable to load the pre-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we save the number of landmarks into the `num_of_landmark` variable and
    create an array to store the output result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, for each face region, we create an integer array to store the face
    location and use the `flandmark_detect` function to obtain the final result in
    the `points` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter in the `flandmark_detect` function is `IplImage` so we need
    to pass our gray image into the `IplImage` constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the landmarks in an image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This step is optional. You don''t need to implement the code in this section.
    However, we recommend that you try and understand the results. The following code
    draws a circle on the image at the location of the landmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows multiple examples of the results using the above
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the landmarks in an image](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Some examples of flandmark results on JAFFE images
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the face region
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have the location of the eyes, nose, and mouth. It is very easy to extract
    the face region.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the center of the left eye as the middle of point 2 and point
    6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, the width of the eye region is the difference between x coordinates
    of point 2 and point 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we find the center and the width of the right eye:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can assume that the width of the face is a bit larger than the distance
    between the eyes, and the height of the face is larger than the width of the face,
    so we can get the eyebrows. We can obtain a good face position with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the face region can be extracted with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows some extracted images from our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting the face region](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Some examples of extracted face regions from JAFFE images
  prefs: []
  type: TYPE_NORMAL
- en: Software usage guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the software to extract facial components from the JAFFE
    dataset. You can use the code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build the software with `cmake` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the facial_components tool, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The software for this chapter based on OpenCV 3 can be found at: [https://github.com/OpenCVBlueprints/OpenCVBlueprints/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to simplify the process, we save the image paths in a `.yaml` file,
    `list.yml`. The structure of this `.yaml` file is simple. First, we save the number
    of images in the `num_of_image` variable. After that, we save the paths of all
    the images, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Software usage guide](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An image of the list.yml file
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a dataset of face regions, we can use feature extraction to obtain the
    feature vector, which gives us the most important information from the expression.
    The following figure shows the process that we use in our implementation to extract
    features vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The feature extraction process
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand this chapter, you need to understand that the feature
    representation of the expression image is the distribution of image features over
    k clusters (k = 1000 in our implementation). We have implemented a few common
    types of features that are supported in OpenCV, such as SIFT, SURF, and some advanced
    features, such as DENSE-SIFT, KAZE, DAISY. Since these image features are computed
    at image key points such as corners, except for DENSE cases, the number of image
    features can vary between images. However, we want to have a fixed feature size
    for every image to perform classification, since we will apply machine learning
    classification techniques later. It is important that the feature size of the
    images is the same so that we can compare them to obtain the final result. Therefore,
    we apply a clustering technique (kmeans in our case) to separate the image feature
    space into a k cluster. The final feature representation for each image is the
    histogram of the image features over k bins. Moreover, in order to reduce the
    dimension of the final feature, we apply principle component analysis as a last
    step.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explain the process step by step. At the
    end of this section, we will show you how to use our implementation to obtain
    the final feature representation of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting image features from facial component regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we will assume that you have the face region for each image in
    the dataset. The next step is to extract the image features from these face regions.
    OpenCV provides good implementations of many well-known key point detection and
    feature description algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Detailed explanations for each algorithm are out of the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will show you how to use some of these algorithms in our
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a function that takes current regions, a feature type, and returns
    a matrix with image features as rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this `extractFeature` function, we will extract image features from each
    Mat and return the descriptors. The implementation of `extractFeature` is simple,
    and shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above code, we call the corresponding function for each feature. For
    simplicity, we only use one feature each time. In this chapter, we will discuss
    two types of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contributed features**: SIFT, DAISY, and DENSE SIFT. In OpenCV 3, the implementation
    of SIFT and SURF have been moved to the opencv_contrib module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: These features are patented and you should pay for them if you want to use them
    in commercial applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this chapter, we will use SIFT features and the SIFT variant, DENSE SIFT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to use the opencv_contrib module, we suggest that you go to the
    *Further reading* section and take a look at the *Compiling the opencv_contrib
    module* section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Advanced features**: BRISK and KAZE. These features are a good alternative
    to SIFT and SURF in both performance and computation time. DAISY and KAZE are
    only available in OpenCV 3\. DAISY is in opencv_contrib. KAZE is in the main OpenCV
    repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributed features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's take a look at SIFT features first.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use SIFT features in OpenCV 3, you need to compile the opencv_contrib
    module with OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will assume that you have followed the instructions in the *Further reading*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to extract SIFT features is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: First, we create the `Feature2D` variable with `xfeatures2d::SIFT::create()`
    and use the `detect` function to obtain key points. The first parameter for the
    detection function is the image that we want to process. The second parameter
    is a vector to store detected key points. The third parameter is a mask specifying
    where to look for key points. We want to find key points in every position of
    the images so we just pass an empty Mat here.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the `compute` function to extract features descriptors at these
    key points. The computed descriptors are stored in the descriptors variable.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's take a look at the SURF features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to obtain SURF features is more or less the same as that for SIFT
    features. We only change the namespace from SIFT to SURF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Let's now move on to DAISY.
  prefs: []
  type: TYPE_NORMAL
- en: 'DAISY is an improved version of the rotation-invariant BRISK descriptor and
    the LATCH binary descriptor that is comparable to the heavier and slower SURF.
    DAISY is only available in OpenCV 3 in the opencv_contrib module. The code to
    implement DAISY features is fairly similar to the Sift function. However, the
    DAISY class doesn''t have a `detect` function so we will use SURF to detect key
    points and use DAISY to extract descriptors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It is now time to take a look at dense SIFT features.
  prefs: []
  type: TYPE_NORMAL
- en: Dense collects features at every location and scale in an image. There are plenty
    of applications where dense features are used. However, in OpenCV 3, the interface
    for extracting dense features has been removed. In this section, we show a simple
    approach to extracting dense features using the function in the OpenCV 2.4 source
    code to extract the vector of key points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function to extract dense Sift is similar to the Sift function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of using the `detect` function, we can use the `createDenseKeyPoints`
    function to obtain key points. After that, we pass this dense key points vector
    to compute the function. The code for `createDenseKeyPoints` is obtained from
    the OpenCV 2.4 source code. You can find this code at `modules/features2d/src/detectors.cpp`
    in the OpenCV 2.4 repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Advanced features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV 3 comes bundled with many new and advanced features. In our implementation,
    we will only use the BRISK and KAZE features. However, there are many other features
    in OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Let us familiarize ourselves with the BRISK features.
  prefs: []
  type: TYPE_NORMAL
- en: BRISK is a new feature and a good alternative to SURF. It has been added to
    OpenCV since the 2.4.2 version. BRISK is under a BSD license so you don't have
    to worry about the patent problem, as with SIFT or SURF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is an interesting article about all this, *A battle of three descriptors:
    SURF, FREAK and BRISK,* available at [http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/](http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move on and have a look at the KAZE features.
  prefs: []
  type: TYPE_NORMAL
- en: 'KAZE is a new feature in OpenCV 3\. It produces the best results in many scenarios,
    especially with image matching problems, and it is comparable to SIFT. KAZE is
    in the OpenCV repository so you don''t need opencv_contrib to use it. Apart from
    the high performance, one reason to use KAZE is that it is open source and you
    can use it freely in any commercial applications. The code to use this feature
    is very straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The image matching comparison between KAZE, SIFT, and SURF is available at
    the author repository: [https://github.com/pablofdezalc/kaze](https://github.com/pablofdezalc/kaze)'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing key points for each feature type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following figure, we visualize the position of key points for each feature
    type. We draw a circle at each key point; the radius of the circle specifies the
    scale of the image where the key point is extracted. You can see that the key
    points and the corresponding descriptors differ between these features. Therefore,
    the performance of the system will vary, based on the quality of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend that you refer to the *Evaluation* section for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing key points for each feature type](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The feature extraction process
  prefs: []
  type: TYPE_NORMAL
- en: Computing the distribution of feature representation over k clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have followed the previous pseudo-code, you should now have a vector
    of descriptors. You can see that the size of descriptors varies between images.
    Since we want a fixed size of feature representation for each image, we will compute
    the distribution of feature representation over k clusters. In our implementation,
    we will use the kmeans clustering algorithm in the core module.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering image features space into k clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we assume that the descriptors of all the images are added to a vector,
    called `features_vector`. Then, we need to create a Mat `rawFeatureData` that
    will contain all of the image features as a row. In this case, `num_of_feature`
    is the total number of features in every image and `image_feature_size` is the
    size of each image feature. We choose the number of clusters based on experiment.
    We start with 100 and increase the number for a few iterations. It depends on
    the type of features and data, so you should try to change this variable to suit
    your situation. One downside of a large number of clusters is that the cost for
    computation with kmeans will be high. Moreover, if the number of clusters is too
    large, the feature vector will be too sparse and it may not be good for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to copy the data from the vector of descriptors (`features_vector`
    in the code) to `imageFeatureData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use the `kmeans` function to perform clustering on the data, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s discuss the parameters of the `kmeans` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**InputArray data**: It contains all the samples as a row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**int K**: The number of clusters to split the samples ( k = 1000 in our implementation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InputOutputArray bestLabels**: Integer array that contains the cluster indices
    for each sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TermCriteria criteria**: The algorithm termination criteria. This contains
    three parameters (`type`, `maxCount`, `epsilon`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: Type of termination criteria. There are three types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**COUNT**: Stop the algorithm after a number of iterations (`maxCount`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EPS**: Stop the algorithm if the specified accuracy (epsilon) is reached.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EPS+COUNT**: Stop the algorithm if the COUNT and EPS conditions are fulfilled.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**maxCount**: It is the maximum number of iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**epsilon**: It is the required accuracy needed to stop the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**int attemtps**: It is the number of times the algorithm is executed with
    different initial centroids. The algorithm returns the labels that have the best
    compactness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**int flags**: This flag specifies how initial centroids are random. There
    are three types of flags. Normally, `KMEANS_RANDOM_CENTERS` and `KMEANS_PP_CENTERS`
    are used. If you want to provide your own initial labels, you should use `KMEANS_USE_INITIAL_LABELS`.
    In this case, the algorithm will use your initial labels on the first attempt.
    For further attempts, `KMEANS_*_CENTERS` flags are applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OutputArray centers**: It contains all cluster centroids, one row per each
    centroid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**double compactness**: It is the returned value of the function. This is the
    sum of the squared distance between each sample to the corresponding centroid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing a final feature for each image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now have labels for every image feature in the dataset. The next step is
    to compute a fixed size feature for each image. With this in mind, we iterate
    through each image and create a feature vector of k elements, where k is the number
    of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we iterate through the image features in the current image and increase
    the ith element of the feature vector where i is the label of the image features.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we are trying to make a histogram representation of the features
    based on the k centroids. This method looks like a bag of words approach. For
    example, image X has 100 features and image Y has 10 features. We cannot compare
    them because they do not have the same size. However, if we make a histogram of
    1,000 dimensions for each of them, they are then the same size and we can compare
    them easily.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will use **Principle Component Analysis** (**PCA**) to reduce
    the dimension of the feature space. In the previous step, we have 1,000 dimensional
    feature vectors for each image. In our dataset, we only have 213 samples. Hence,
    the further classifiers tend to overfit the training data in high dimensional
    space. Therefore, we want to use PCA to obtain the most important dimension, which
    has the largest variance.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show you how to use PCA in our system.
  prefs: []
  type: TYPE_NORMAL
- en: First, we assume that you can store all the features in a Mat named `featureDataOverBins`.
    The number of rows of this Mat should equal to the number of images in the dataset
    and the number of columns of this Mat should be 1,000\. Each row in `featureDataOverBins`
    is a feature of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we create a PCA variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter is the data that contains all the features. We don't have
    a pre-computed mean vector so the second parameter should be an empty Mat. The
    third parameter indicates that the feature vectors are stored as matrix rows.
    The final parameter specifies the percentage of variance that PCA should retain.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to project all the features from 1,000 dimensional feature
    spaces to a lower space. After the projection, we can save these features for
    further processes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of dimensions of the new features can be obtained by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Software usage guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the previous process to extract the fixed size feature
    for the dataset. Using the software is quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build the software with `cmake` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the `feature_extraction` tool as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `feature_extraction` tool creates a YAML file in the output folder which
    contains the features and labels of every image in the dataset. The available
    parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_name`: This can be sift, surf, opponent-sift, or opponent-surf. This
    is the name of the feature type which is used in the feature extraction process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_folder`: This has the absolute path to the location of facial components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_folder`: This has the absolute path to the folder where you want to
    keep the output file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The structure of the output file is fairly simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'We store the size of the feature, cluster centers, the number of images, the
    number of train and test images, the number of labels, and the corresponding label
    names. We also store PCA means, eigenvectors, and eigenvalues. The following figure
    shows a part of the YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Software usage guide](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A part of the features.yml file
  prefs: []
  type: TYPE_NORMAL
- en: 'For each image, we store three variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image_feature_<idx>`: It is a Mat that contains features of image idx'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_label_<idx>`: It is a label of the image idx'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_is_train_<idx>`: It is a Boolean specifying whether the image is used
    for training or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have extracted the features for all the samples in the dataset, it
    is time to start the classification process. The target of this classification
    process is to learn how to make accurate predictions automatically based on the
    training examples. There are many approaches to this problem. In this section,
    we will talk about machine learning algorithms in OpenCV, including neural networks,
    support vector machines, and k-nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Classification process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Classification is considered supervised learning. In a classification problem,
    a correctly labelled training set is necessary. A model is produced during the
    training stage which makes predictions and is corrected when predictions are wrong.
    Then, the model is used for predicting in other applications. The model needs
    to be trained every time you have more training data. The following figure shows
    an overview of the classification process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification process](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the classification process
  prefs: []
  type: TYPE_NORMAL
- en: The choice of learning algorithm to use is a critical step. There are a lot
    of solutions to the classification problem. In this section, we list some of the
    popular machine learning algorithms in OpenCV. The performance of each algorithm
    can vary between classification problems. You should make some evaluations and
    select the one that is the most appropriate for your problem to get the best results.
    It is essential as feature selection may affect the performance of the learning
    algorithm. Therefore, we also need to evaluate each learning algorithm with each
    different feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset into a training set and testing set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important that the dataset is separated into two parts, the training set
    and the testing set. We will use the training set for the learning stage and the
    testing set for the testing stage. In the testing stage, we want to test how the
    trained model predicts unseen samples. In other words, we want to test the *generalization
    capability* of the trained model. Therefore, it is important that the test samples
    are different from the trained samples. In our implementation, we will simply
    split the dataset into two parts. However, it is better if you use k-fold cross
    validation as mentioned in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: There is no accurate way to split the dataset into two parts. Common ratios
    are 80:20 and 70:30\. Both the training set and the testing set should be selected
    randomly. If they have the same data, the evaluation is misleading. Basically,
    even if you achieve 99 percent accuracy on your testing set, the model can't work
    in the real world, where the data is different from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation of feature extraction, we have already randomly split
    the dataset and saved the selection in the YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The k-fold cross validation is explained in more detail at the end of the *Further
    reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**A Support Vector Machine** (**SVM**) is a supervised learning technique applicable
    to both classification and regression. Given labelled training data, the goal
    of SVM is to produce an optimal hyper plane which predicts the target value of
    a test sample with only test sample attributes. In other words, SVM generates
    a function to map between input and output based on labelled training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s assume that we want to find a line to separate two sets
    of 2D points. The following figure shows that there are several solutions to the
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machines](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A lot of hyper planes can solve a problem
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of SVM is to find a hyper plane that maximizes the distances to the
    training samples. The distances are calculated to only support those vectors that
    are closest to the hyper plane. The following figure shows an optimal hyper plane
    to separate two sets of 2D points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machines](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An optimal hyper plane that maximizes the distances to the training samples.
    R is the maximal margin
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will show you how to use SVM to train and test
    facial expression data.
  prefs: []
  type: TYPE_NORMAL
- en: Training stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most difficult parts about training an SVM is parameters selection.
    It is not possible to explain everything without some deep understanding of how
    SVM works. Luckily, OpenCV implements a `trainAuto` method for automatic parameter
    estimation. If you have enough knowledge of SVM, you should try to use your own
    parameters. In this section, we will introduce the `trainAuto` method to give
    you an overview of SVM.
  prefs: []
  type: TYPE_NORMAL
- en: SVM is inherently a technique for building an optimal hyper plane in binary
    (2-class) classification. In our facial expression problem, we want to classify
    seven expressions. One-versus-all and one-versus-one are two common approaches
    that we can follow to use SVM in this problem. One-versus-all trains one SVM for
    each class. There are seven SVMs in our case. For class i, every sample with the
    label i is considered as positive and the rest of the samples are negative. This
    approach is prone to error when the dataset samples are imbalanced between classes.
    The one-versus-one approach trains an SVM for each different pairs of classes.
    The number of SVMs in total is *N*(N-1)/2* SVMs. This means 21 SVMs in our case.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCV, you don't have to follow these approaches. OpenCV supports the training
    of one multiclass SVM. However, you should follow the above methods for better
    results. We will still use one multiclass SVM. The training and testing process
    will be simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will demonstrate our implementation to solve the facial expression
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an instance of SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to change parameters, you can call the `set` function in the `svm`
    variable, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '**Type**: It is the type of SVM formulation. There are five possible values:
    `C_SVC`, `NU_SVC`, `ONE_CLASS`, `EPS_SVR`, and `NU_SVR`. However, in our multiclass
    classification, only `C_SVC` and `NU_SVC` are suitable. The difference between
    these two lies in the mathematical optimization problem. For now, we can just
    use `C_SVC`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel**: It is the type of SVM kernel. There are four possible values: `LINEAR`,
    `POLY`, `RBF`, and `SIGMOID`. The kernel is a function to map the training data
    to a higher dimensional space that makes data linearly separable. This is also
    known as *Kernel Trick*. Therefore, we can use SVM in non-linear cases with the
    support of the kernel. In our case, we choose the most commonly-used kernel, RBF.
    You can switch between these kernels and choose the best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also set other parameters such as TermCriteria, Degree, Gamma. We are
    just using the default parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we create a variable of `ml::TrainData` to store all the training set
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`train_features`: It is a Mat that contains each features vector as a row.
    The number of rows of `train_features` is the number of training samples, and
    the number of columns is the size of one features vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SampleTypes::ROW_SAMPLE`: It specifies that each features vector is in a row.
    If your features vectors are in columns, you should use COL_SAMPLE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_labels`: It is a Mat that contains labels for each training feature.
    In SVM, `train_labels` will be a Nx1 matrix, N is the number of training samples.
    The value of each row is the truth label of the corresponding sample. At the time
    of writing, the type of `train_labels` should be `CV_32S`. Otherwise, you may
    encounter an error. The following code is what we use to create the `train_labels`
    variable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we pass `trainData` to the `trainAuto` function so that OpenCV can
    select the best parameters automatically. The interface of the `trainAuto` function
    contains many other parameters. In order to keep things simple, we will use the
    default parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Testing stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After we''ve trained the SVM, we can pass a test sample to the predict function
    of the `svm` model and receive a label prediction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the sample is a feature vector just like the feature vector in
    the training features. The response is the label of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCV implements the most common type of artificial neural network, the multi-layer
    perceptron (MLP). A typical MLP consists of an input layer, an output layer, and
    one or more hidden layers. It is known as a supervised learning method because
    it needs a desired output to train. With enough data, MLP, given enough hidden
    layers, can approximate any function to any desired accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MLP with a single hidden layer can be represented as it is in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptron](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A single hidden layer perceptron
  prefs: []
  type: TYPE_NORMAL
- en: A detailed explanation and proof of how the MLP learns are out of the scope
    of this chapter. The idea is that the output of each neuron is a function of neurons
    from previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above single hidden layer MLP, we use the following notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input layer: x[1] x[2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden layer: h[1] h[2] h[3]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output layer: y'
  prefs: []
  type: TYPE_NORMAL
- en: Each connection between each neuron has a weight. The weight shown in the above
    figure is between neuron i (that is i = 3) in the current layer and neuron j (that
    is j = 2) in the previous layer is w[ij]. Each neuron has a bias value 1 with
    a weight, w[i,bias].
  prefs: []
  type: TYPE_NORMAL
- en: 'The output at neuron i is the result of an activation function *f*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptron](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many types of activation functions. In OpenCV, there are three types
    of activation functions: Identity, Sigmoid, and Gaussian. However, the Gaussian
    function is not completely supported at the time of writing and the Identity function
    is not commonly used. We recommend that you use the default activation, Sigmoid.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will show you how to train and test a multi-layer
    perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Training stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the training stage, we first define the network and then train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Define the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will use a simple four layer neural network in our facial expression problem.
    The network has one input layer, two hidden layers, and one output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a matrix to hold the layers definition. This matrix
    has four rows and one column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we assign the number of neurons for each layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this network, the number of neurons for the input layer has to be equal to
    the number of elements of each feature vector, and number of neurons for the output
    layer is the number of facial expression labels (`feature_size` equals `train_features.cols`
    where `train_features` is the Mat that contains all features and `num_of_labels`
    equals 7 in our implementation).
  prefs: []
  type: TYPE_NORMAL
- en: The above parameters in our implementation are not optimal. You can try different
    values for different numbers of hidden layers and numbers of neurons. Remember
    that the number of hidden neurons should not be larger than the number of training
    samples. It is very difficult to choose the number of neurons in a hidden layer
    and the number of layers in your network. If you do some research, you can find
    several rules of thumb and diagnostic techniques. The best way to choose these
    parameters is experimentation. Basically, the more layers and hidden neurons there
    are, the more capacity you have in the network. However, more capacity may lead
    to overfitting. One of the most important rules is that the number of examples
    in the training set should be larger than the number of weights in the network.
    Based on our experience, you should start with one hidden layer with a small number
    of neurons and calculate the generalization error and training error. Then, you
    should modify the number of neurons and repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember to make a graph to visualize the error when you change parameters.
    Keep in mind that the number of neurons is usually between the input layer size
    and the output layer size. After a few iterations, you can decide whether to add
    an additional layer or not.
  prefs: []
  type: TYPE_NORMAL
- en: However, in this case, we don't have much data. This makes the network hard
    to train. We may not add neurons and layers to improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Train the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we create a network variable, ANN_MLP, and add the layers definition
    to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to prepare some parameters for training algorithms. There are
    two algorithms for training MLP: the back-propagation algorithm and the RPROP
    algorithm. RPROP is the default algorithm for training. There are many parameters
    for RPROP so we will use the back-propagation algorithm for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is our code for setting parameters for the back-propagation algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the `TrainMethod` to `BACKPROP` to use the back-propagation algorithm.
    Select Sigmoid as the activation function There are three types of activation
    in OpenCV: `IDENTITY`, `GAUSSIAN`, and `SIGMOID`. You can go to the overview of
    this section for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: The final parameter is `TermCriteria`. This is the algorithm termination criteria.
    You can see an explanation of this parameter in the kmeans algorithm in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a `TrainData` variable to store all the training sets. The interface
    is the same as in the SVM section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`train_features` is the Mat which stores all training samples as in the SVM
    section. However, `train_labels` is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_features`: This is a Mat that contains each features vector as a row
    as we did in the SVM. The number of rows of `train_features` is the number of
    training samples and the number of columns is the size of one features vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_labels`: This is a Mat that contains labels for each training feature.
    Instead of the Nx1 matrix in SVM, `train_labels` in MLP should be a NxM matrix,
    N is the number of training samples and M is the number of labels. If the feature
    at row i is classified as label j, the position (i, j) of `train_labels` will
    be 1\. Otherwise, the value will be zero. The code to create the `train_labels`
    variable is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we train the network with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The training process takes a few minutes to complete. If you have a lot of training
    data, it may take a few hours.
  prefs: []
  type: TYPE_NORMAL
- en: Testing stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have trained our MLP, the testing stage is very simple.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a Mat to store the response of the network. The response is
    an array, whose length is the number of labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Then, we assume that we have a Mat, called sample, which contains a feature
    vector. In our facial expression case, its size should be 1x1000.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call the `predict` function of the `mlp` model to obtain the response,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The predicted label of the input sample is the index of the maximum value in
    the response array. You can find the label by simply iterating through the array.
    The disadvantage of this type of response is that you have to apply a `softmax`
    function if you want a probability for each response. In other neural network
    frameworks, there is usually a softmax layer for this reason. However, the advantage
    of this type of response is that the magnitude of each response is retained.
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors (KNN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors** (**KNN**) is a very simple algorithm for machine learning
    but works very well in many practical problems. The idea of KNN is to classify
    an unknown example with the most common class among k-nearest known examples.
    KNN is also known as a non-parametric lazy learning algorithm. It means that KNN
    doesn''t make any assumptions about the data distribution. The training process
    is very fast since it only caches all training examples. However, the testing
    process requires a lot of computation. The following figure demonstrates how KNN
    works in a 2D points case. The green dot is an unknown sample. KNN will find k-nearest
    known samples in space, (k = 5 in this example). There are three samples of red
    labels and two samples of blue labels. Therefore, the label for the prediction
    is red.'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-Nearest Neighbors (KNN)](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An explanation of how KNN predicts labels for unknown samples
  prefs: []
  type: TYPE_NORMAL
- en: Training stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The implementation of KNN algorithms is very simple. We only need three lines
    of code to train a KNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is the same as with SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_features`: This is a Mat that contains each features vector as a row.
    The number of rows in `train_features` is the number of training samples and the
    number of columns is the size of one features vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_labels`: This is a Mat that contains labels for each training feature.
    In KNN, `train_labels` is a Nx1 matrix, N is the number of training samples. The
    value of each row is the truth label of the corresponding sample. The type of
    this Mat should be `CV_32S`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The testing stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The testing stage is very straightforward. We can just pass a feature vector
    to the `findNearest` method of the `knn` model and obtain the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The second parameter is the most important parameter. It is the number of maximum
    neighbors that may be used for classification. In theory, if there are an infinite
    number of samples available, a larger K always means a better classification.
    However, in our facial expression problem, we only have 213 samples in total and
    about 170 samples in the training set. Therefore, if we use a large K, KNN may
    end up looking for samples that are not neighbors. In our implementation, K equals
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predicted labels are stored in the `predictedLabels` variable and can be
    obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Normal Bayes classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Normal Bayes classifier is one of the simplest classifiers in OpenCV. The
    Normal Bayes classifier assumes that features vectors from each class are normally
    distributed, although not necessarily independently. This classifier is an effective
    classifier that can handle multiple classes. In the training step, the classifier
    estimates the mean and co-variance of the distribution for each class. In the
    testing step, the classifier computes the probability of the features to each
    class. In practice, we then test to see if the maximum probability is over a threshold.
    If it is, the label of the sample will be the class that has the maximum probability.
    Otherwise, we say that we can't recognize the sample.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has already implemented this classifier in the ml module. In this section,
    we will show you the code to use the Normal Bayes classifier in our facial expression
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Training stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code to implement the Normal Bayes classifier is the same as with SVM and
    KNN. We only need to call the `create` function to obtain the classifier and start
    the training process. All the other parameters are the same as with SVM and KNN.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Testing stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code to test a sample with the Normal Bayes classifier is a little different
    from previous methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create two Mats to store the output class and probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we call the `predictProb` function of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The computed probability is stored in `outputProb` and the corresponding label
    can be retrieved as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Software usage guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the above process to perform classification with a training
    set. Using the software is quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build the software with `cmake` using the follow command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can use the `train` tool as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train` tool performs the training process and outputs the accuracy on
    the console. The learned model will be saved to the output folder for further
    use as `model.yml`. Furthermore, kmeans centers and pca information from features
    extraction are also saved in `features_extraction.yml`. The available parameters
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`algorithm_name`: This can be `mlp`, `svm`, `knn`, `bayes`. This is the name
    of the learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_features`: This is the absolute path to the location of the YAML features
    file from the `prepare_dataset` tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_folder`: This is the absolute path to the folder where you want to
    keep the output model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show the performance of our facial expression recognition
    system. In our test, we will keep the parameters of each learning algorithm the
    same and only change the feature extraction. We will evaluate the feature extraction
    with the number of clusters equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.
  prefs: []
  type: TYPE_NORMAL
- en: The following table shows the accuracy of the system with the number of clusters
    equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The accuracy (%) of the system with 1,000 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 1000 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 72.7273 | 93.1818 | 81.8182 | 88.6364 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 61.3636 | 79.5455 | 72.7273 | 79.5455 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 61.3636 | 65.9091 | 59.0909 | 68.1818 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 50 | 79.5455 | 61.3636 | 77.2727 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 59.0909 | 77.2727 | 65.9091 | 81.8182 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 20.4545 | 45.4545 | 43.1818 | 40.9091 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The accuracy (%) of the system with 500 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 500 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 56.8182 | 70.4545 | 75 | 77.2727 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 54.5455 | 63.6364 | 68.1818 | 79.5455 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 36.3636 | 59.0909 | 52.2727 | 52.2727 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 47.7273 | 56.8182 | 63.6364 | 65.9091 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 54.5455 | 75 | 63.6364 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 27.2727 | 43.1818 | 38.6364 | 43.1818 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The accuracy (%) of the system with 200 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 200 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 50 | 68.1818 | 65.9091 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 43.1818 | 54.5455 | 52.2727 | 63.6364 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 29.5455 | 47.7273 | 50 | 54.5455 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 50 | 59.0909 | 72.7273 | 59.0909 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 45.4545 | 68.1818 | 65.9091 | 70.4545 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 29.5455 | 43.1818 | 40.9091 | 31.8182 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The accuracy (%) of the system with 1,500 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 1500 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 45.4545 | 84.0909 | 75 | 79.5455 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 72.7273 | 88.6364 | 79.5455 | 86.3636 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 54.5455 | 72.7273 | 56.8182 | 68.1818 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 45.4545 | 79.5455 | 72.7273 | 77.2727 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 61.3636 | 88.6364 | 65.9091 | 81.8182 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 34.0909 | 47.7273 | 38.6364 | 38.6364 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The accuracy (%) of the system with 2,000 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 2000 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 63.6364 | 88.6364 | 81.8182 | 88.6364 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 65.9091 | 84.0909 | 68.1818 | 81.8182 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 47.7273 | 68.1818 | 47.7273 | 61.3636 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 47.7273 | 77.2727 | 72.7273 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 77.2727 | 81.8182 | 72.7273 | 84.0909 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 38.6364 | 45.4545 | 36.3636 | 43.1818 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The accuracy (%) of the system with 3,000 clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '| K = 3000 | MLP | SVM | KNN | Normal Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT | 52.2727 | 88.6364 | 77.2727 | 86.3636 |'
  prefs: []
  type: TYPE_TB
- en: '| SURF | 59.0909 | 79.5455 | 65.9091 | 77.2727 |'
  prefs: []
  type: TYPE_TB
- en: '| BRISK | 52.2727 | 65.9091 | 43.1818 | 59.0909 |'
  prefs: []
  type: TYPE_TB
- en: '| KAZE | 61.3636 | 81.8182 | 70.4545 | 84.0909 |'
  prefs: []
  type: TYPE_TB
- en: '| DAISY | 72.7273 | 79.5455 | 70.4545 | 68.1818 |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE-SIFT | 27.2727 | 47.7273 | 38.6364 | 45.4545 |'
  prefs: []
  type: TYPE_TB
- en: Evaluation with different learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can create graphs with the above results to compare the performance between
    features and learning algorithms in the following figure. We can see that SVM
    and Normal Bayes have better results than the others in most cases. The best result
    is 93.1818% for SVM and SIFT in 1,000 clusters. MLP has the lowest result in almost
    every case. One reason is that MLP requires lots of data to prevent over fitting.
    We only have around 160 training images. However, the feature size for each sample
    is between 100 and 150\. Even with two hidden neurons, the number of weights is
    larger than the number of samples. KNN seems to work better than MLP but can't
    beat SVM and Normal Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation with different learning algorithms](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Relationship between the performance of features and machine algorithms under
    different numbers of clusters
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation with different learning algorithms](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Effect of the number of centroids on the performance of features according to
    different machine algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation with different features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the figure, *Relationship between the performance of features and machine
    algorithms under different numbers of clusters*, we have evaluated six features.
    SIFT gives the best results in most cases. DAISY is comparable to SIFT. In some
    cases, KAZE also gives good results. DENSE-SIFT is not a good choice for our facial
    expression problem since the results are poor. Moreover, the computation cost
    for DENSE features is really high. In conclusion, SIFT is still the most stable
    choice. However, SIFT is under patent. You may want to look at DAISY or KAZE.
    We recommend you do the evaluation on your data and choose the most suitable feature.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation with a different number of clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the figure, *Effect of the number of centroids on the performance of features
    according to different machine algorithms*, we made a graph to visualize the effects
    of the number of clusters on performance. As you can see, the number of clusters
    differs between features. In SIFT, KAZE, and BRISK, the best number of clusters
    is 1,000\. However, in SURF, DAISY, and DENSE-SIFT, 1,500 is a better choice.
    Basically, we don't want the number of clusters to be too large. The computation
    cost in kmeans increases with a larger number of clusters, especially in DENSE-SIFT.
  prefs: []
  type: TYPE_NORMAL
- en: System overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explain the process to apply the trained model in your
    application. Given a face image, we detect and process each face separately. Then,
    we find landmarks and extract the face region. The image features are extracted
    and passed to kmeans to obtain a 1,000-dimensional feature vector. PCA is applied
    to reduce the dimension of this feature vector. The learned machine learning model
    is used to predict the expression of the input face.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the complete process to predict the facial expression
    of a face in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![System overview](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The process to predict a facial expression in a new image
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have introduced a basic system for facial expression. If you are really interested
    in this topic, you may want to read this section for more guidance on how to improve
    the performance of the system. In this section, we will introduce you to compiling
    the `opencv_contrib` module, the Kaggle facial expression dataset, and the k-cross
    validation approach. We will also give you some suggestions on how to get better
    feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the opencv_contrib module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will introduce the process for compiling `opencv_contrib`
    in Linux-based systems. If you use Windows, you can use the Cmake GUI with the
    same options.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, clone the `opencv` repository to your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, clone the `opencv_contrib` repository to your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Change directory to the `opencv` folder and make a build directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Build OpenCV from source with opencv_contrib support. You should change `OPENCV_EXTRA_MODULES_PATH`
    to the location of `opencv_contrib` on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Kaggle facial expression dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kaggle is a great community of data scientists. There are many competitions
    hosted by Kaggle. In 2013, there was a facial expression recognition challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the moment, you can go to the following link to access the full dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 48x48 pixel grayscale images of faces. There are 28,709
    training samples, 3,589 public test images and 3,589 images for final test. The
    dataset contains seven expressions (Anger, Disgust, Fear, Happiness, Sadness,
    Surprise and Neutral). The winner achieved a score of 69.769 %. This dataset is
    huge so we think that our basic system may not work out of the box. We believe
    that you should try to improve the performance of the system if you want to use
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Facial landmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our facial expression system, we use face detection as a pre-processing step
    to extract the face region. However, face detection is prone to misalignment,
    hence, feature extraction may not be reliable. In recent years, one of the most
    common approaches has been the usage of facial landmarks. In this kind of method,
    the facial landmarks are detected and used to align the face region. Many researchers
    use facial landmarks to extract the facial components such as the eyes, mouth,
    and so on, and do feature extractions separately.
  prefs: []
  type: TYPE_NORMAL
- en: What are facial landmarks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Facial landmarks are predefined locations of facial components. The figure below
    shows an example of a 68 points system from the iBUG group ([http://ibug.doc.ic.ac.uk/resources/facial-point-annotations](http://ibug.doc.ic.ac.uk/resources/facial-point-annotations))
  prefs: []
  type: TYPE_NORMAL
- en: '![What are facial landmarks?](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a 68 landmarks points system from the iBUG group
  prefs: []
  type: TYPE_NORMAL
- en: How do you detect facial landmarks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several ways to detect facial landmarks in a face region. We will
    give you a few solutions so that you can start your project easily
  prefs: []
  type: TYPE_NORMAL
- en: '**Active Shape Model**: This is one of the most common approaches to this problem.
    You may find the following library useful:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stasm: [https://github.com/cxcxcxcx/asmlib-opencv](https://github.com/cxcxcxcx/asmlib-opencv)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Face Alignment by Explicit Regression by Cao et al**: This is one of the
    latest works on facial landmarks. This system is very efficient and highly accurate.
    You can find an open source implementation at the following hyperlink: [https://github.com/soundsilence/FaceAlignment](https://github.com/soundsilence/FaceAlignment)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you use facial landmarks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use facial landmarks in many ways. We will give you some guides:'
  prefs: []
  type: TYPE_NORMAL
- en: You can use facial landmarks to align the face region to a common standard and
    extract the features vectors as in our basic facial expression system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can extract features vectors in different facial components such as eyes
    and mouths separately and combine everything in one feature vector for classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the location of facial landmarks as a feature vector and ignore
    the texture in the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build classification models for each facial component and combine the
    prediction in a weighted manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature extraction is one of the most important parts of facial expression.
    It is better to choose the right feature for your problem. In our implementation,
    we have only used a few features in OpenCV. We recommend that you try every possible
    feature in OpenCV. Here is the list of supported features in Open CV: BRIEF, BRISK,
    FREAK, ORB, SIFT, SURF, KAZE, AKAZE, FAST, MSER, and STAR.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other great features in the community that might be suitable for your
    problem, such as LBP, Gabor, HOG, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-fold cross validation is a common technique for estimating the performance
    of a classifier. Given a training set, we will divide it into k partitions. For
    each fold i of k experiments, we will train the classifier using all the samples
    that do not belong to fold i and use the samples in fold i to test the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of k-fold cross validation is that all the examples in the dataset
    are eventually used for training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to divide the original dataset into the training set and the
    testing set. Then, the training set will be used for k-fold cross validation and
    the testing set will be used for the final test.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation combines the prediction error of each experiment and derives
    a more accurate estimate of the model. It is very useful, especially in cases
    where we don't have much data for training. Despite a high computation time, using
    a complex feature is a great idea if you want to improve the overall performance
    of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed the complete process of a facial expression system in OpenCV
    3\. We went through each step of the system and gave you a lot of alternative
    solutions for each step. This chapter also made an evaluation of the results based
    on features and learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter gave you a few hints for further improvement including
    a great facial expression challenge, a facial landmarks approach, some features
    suggestions, and k-fold cross validation.
  prefs: []
  type: TYPE_NORMAL
