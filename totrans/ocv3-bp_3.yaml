- en: Chapter 3. Recognizing Facial Expressions with Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 使用机器学习识别面部表情
- en: Automatic facial expression recognition has attracted much attention since the
    early nineties, especially in human-computer interaction. As computers start becoming
    a part of our life, they need to become more and more intelligent. Expression
    recognition systems will enhance this intelligent interaction between the human
    and the computer.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自九十年代初以来，自动面部表情识别引起了广泛关注，尤其是在人机交互领域。随着计算机开始成为我们生活的一部分，它们需要变得越来越智能。表情识别系统将增强人类与计算机之间的智能交互。
- en: Although humans can recognize facial expressions easily, a reliable expression
    recognition system is still a challenge. In this chapter, we will introduce a
    basic implementation of facial expression using various algorithms from the OpenCV
    library, including feature extraction and classification using the ml module.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人类可以轻易地识别面部表情，但一个可靠的表情识别系统仍然是一个挑战。在本章中，我们将介绍使用OpenCV库中的各种算法的基本面部表情实现，包括使用ml模块进行特征提取和分类。
- en: 'In this chapter, we will be going through the following topics in brief:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要介绍以下主题：
- en: A simple architecture to recognize human facial expressions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的识别人类面部表情的架构
- en: Feature extraction algorithms in the OpenCV library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV库中的特征提取算法
- en: The learning and testing stage, with various machine learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习和测试阶段，使用各种机器学习算法
- en: Introducing facial expression recognition
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍面部表情识别
- en: Automatic facial expression recognition is an interesting and challenging problem
    and has several important applications in many areas such as human-computer interaction,
    human behavior understanding, and data-driven animation. Unlike face recognition,
    facial expression recognition needs to discriminate between the same expression
    in different individuals. The problem becomes more difficult as a person may show
    the same expression in different ways.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动面部表情识别是一个有趣且具有挑战性的问题，并在许多领域如人机交互、人类行为理解和数据驱动动画中具有几个重要应用。与面部识别不同，面部表情识别需要区分不同个体中相同的表情。当一个人可能以不同的方式表现出相同的表情时，问题变得更加困难。
- en: 'The current existing approaches for measuring facial expressions can be categorized
    into two types: static image and image sequence. In the static image approach,
    the system analyzes the facial expression in each image frame separately. In the
    image sequence approach, the system tries to capture the temporal pattern of the
    motion and changes seen on the face in the sequence of image frames. Recently,
    attention has been shifted toward the image sequence approach. However, this approach
    is more difficult and requires more computation than the static approach. In this
    chapter, we will follow the static image approach and compare several algorithms
    using the OpenCV 3 library.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当前用于测量面部表情的方法可以分为两种类型：静态图像和图像序列。在静态图像方法中，系统分别分析每个图像帧中的面部表情。在图像序列方法中，系统试图捕捉图像帧序列中面部上看到的运动和变化的时序模式。最近，注意力已经转向图像序列方法。然而，这种方法比静态方法更困难，需要更多的计算。在本章中，我们将遵循静态图像方法，并使用OpenCV
    3库比较几种算法。
- en: 'The problem of automatic facial expression recognition includes three sub-problems:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自动面部表情识别的问题包括三个子问题：
- en: '**Finding the face region in the image**: The precise position of the face
    is very important for facial analysis. In this problem, we want to find the face
    region in the image. This problem can be viewed as a detection problem. In our
    implementation, we will use the cascade classifier in OpenCV''s objdetect module
    to detect the faces. However, the cascade classifier is prone to alignment error.
    Therefore, we apply the flandmark library to extract the facial landmarks from
    the face region and use these landmarks to extract the precise face region.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在图像中找到面部区域**：面部在图像中的精确位置对于面部分析非常重要。在这个问题中，我们希望在图像中找到面部区域。这个问题可以被视为一个检测问题。在我们的实现中，我们将使用OpenCV的objdetect模块中的级联分类器来检测面部。然而，级联分类器容易产生对齐错误。因此，我们应用flandmark库从面部区域中提取面部特征点，并使用这些特征点来提取精确的面部区域。'
- en: Note
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Flandmark is an open source C library implementing a facial landmark detector.
    You can get more information about flandmark in the following sections. Basically,
    you can use whatever library you want to extract the landmarks. In our implementation,
    we will use this library to reduce complexity while integrating the library into
    our project.
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Flandmark是一个开源的C库，实现了人脸关键点检测器。你可以在以下章节中了解更多关于flandmark的信息。基本上，你可以使用你想要的任何库来提取关键点。在我们的实现中，我们将使用这个库来降低复杂性，同时将库集成到我们的项目中。
- en: '**Extracting features from the face region**: Given the face region, the system
    will extract facial expression information as a feature vector. The feature vector
    encodes the relevant information from the input data. In our implementation, the
    feature vector is obtained by using the combination of the feature detector from
    the feature2d module and the kmeans algorithm from the core module.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从人脸区域提取特征**：给定人脸区域，系统将提取面部表情信息作为一个特征向量。特征向量编码了从输入数据中提取的相关信息。在我们的实现中，特征向量是通过结合特征2d模块中的特征检测器和核心模块中的kmeans算法获得的。'
- en: '**Classifying the features into emotion categories**: This is a classification
    problem. The system uses classification algorithms to map the extracted feature
    from the previous step to an emotion category (such as happy, neutral, or sad).
    This is the main subject of the chapter. We will evaluate machine learning algorithms
    from the ml module, including neural networks, the support vector machine, and
    K-Nearest-Neighbor.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将特征分类到情感类别**：这是一个分类问题。系统使用分类算法将之前步骤中提取的特征映射到情感类别（如快乐、中性或悲伤）。这是本章的主要内容。我们将评估ml模块中的机器学习算法，包括神经网络、支持向量机和K-Nearest-Neighbor。'
- en: In the following sections, we will show you a complete process for implementing
    a facial expression system. In the next section, you will find several approaches
    to improve system performance to suit your needs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将向你展示实现面部表情系统的完整过程。在下一节中，你将找到几种提高系统性能的方法来满足你的需求。
- en: Facial expression dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部表情数据集
- en: 'In order to simplify the chapter, we will use a dataset to demonstrate the
    process instead of a live camera. We will use a standard dataset, **Japanese Female
    Facial Expression** (**JAFFE**).There are 214 images of 10 people in the dataset.
    Each person has three images of each expression. The dataset includes seven expressions
    (happy, sad, angry, disgust, fear, surprise, and neutral) as shown in the following
    figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化章节，我们将使用数据集来演示过程，而不是使用实时摄像头。我们将使用标准数据集，**日本女性面部表情**（**JAFFE**）。数据集中有10个人的214张图片。每个人有每种表情的三张图片。数据集包括以下图所示的七个表情（快乐、悲伤、愤怒、厌恶、恐惧、惊讶和中性）：
- en: Note
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You need to download the dataset from the following link: [http://www.kasrl.org/jaffe.html](http://www.kasrl.org/jaffe.html)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要从以下链接下载数据集：[http://www.kasrl.org/jaffe.html](http://www.kasrl.org/jaffe.html)
- en: '![Facial expression dataset](img/00032.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![面部表情数据集](img/00032.jpeg)'
- en: Sample image from the JAFFE dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: JAFFE数据集的样本图像。
- en: Finding the face region in the image
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在图像中找到人脸区域
- en: In this section, we will show you a basic approach to detect faces in an image.
    We will use the cascade classifier in OpenCV to detect the face location. This
    approach may have alignment errors. In order to obtain a precise location, we
    will also provide another advanced approach to find the face region using facial
    landmarks. In our implementation, we will only use the face region. However, many
    researchers use facial landmarks to extract facial components, such as eyes and
    mouths, and operate on these components separately.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向你展示在图像中检测人脸的基本方法。我们将使用OpenCV中的级联分类器来检测人脸位置。这种方法可能存在对齐错误。为了获得精确的位置，我们还将提供另一种使用面部关键点来查找人脸区域的高级方法。在我们的实现中，我们只使用人脸区域。然而，许多研究人员使用面部关键点来提取面部组件，如眼睛和嘴巴，并对这些组件分别进行操作。
- en: Note
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to find out more, you should check the *Facial landmarks* section
    in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多，你应该检查本章中的*面部关键点*部分。
- en: Extracting the face region using a face detection algorithm
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用人脸检测算法提取人脸区域
- en: In our implementation, we will use the Haar Feature-based cascade classifier
    in the objdetect module. In OpenCV, you can also extract the the face region with
    LBP-based cascade. LBP-based cascade is faster than Haar-based cascade. With the
    pre-trained model, the performance of LBP-based is lower than Haar-based cascade.
    However, it is possible to train an LBP-based cascade to attain the same performance
    as the Haar-based cascade.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们将在objdetect模块中使用基于Haar特征的级联分类器。在OpenCV中，你也可以使用基于LBP的级联提取人脸区域。基于LBP的级联比基于Haar的级联更快。使用预训练模型，基于LBP的级联性能低于基于Haar的级联。然而，训练一个基于LBP的级联以获得与基于Haar的级联相同的性能是可能的。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to understand object detection in detail, you should check [Chapter
    5](part0043_split_000.html#190862-940925703e144daa867f510896bffb69 "Chapter 5. Generic
    Object Detection for Industrial Applications"), *Generic Object Detection for
    Industrial Applications*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要详细了解目标检测，你应该查看[第5章](part0043_split_000.html#190862-940925703e144daa867f510896bffb69
    "第5章. 工业应用中的通用目标检测")，*工业应用中的通用目标检测*。
- en: 'The code for detecting faces is very simple. First, you need to load the pre-trained
    cascade classifier for faces into your OpenCV installation folder:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 检测人脸的代码非常简单。首先，你需要将预训练的级联分类器加载到你的OpenCV安装文件夹中：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, load the input image in color mode, convert the image to grayscale, and
    apply histogram equalization to enhance the contrast:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，以彩色模式加载输入图像，将图像转换为灰度，并应用直方图均衡化以增强对比度：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can find faces in the image. The `detectMultiScale` function stores
    all the detected faces in the vector as Rect(x, y, w, h):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在图像中找到人脸。`detectMultiScale`函数将所有检测到的人脸存储在向量中，作为Rect(x, y, w, h)：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this code, the third parameter 1.1 is the scale factor, which specifies how
    much the image size will be resized at each scale. The following figure shows
    the scale pyramid using the scale factor. In our case, the scale factor is `1.1`.
    This means that the image size is reduced by 10%. The lower this factor is, the
    better chance we have of finding the faces. The scaling process starts with the
    original image and ends when the image resolution reaches the model dimension
    in the X or Y direction. However, the computation cost is high if we have too
    many scales. Therefore, if you want to reduce the number of scales, increase the
    scale factor to `1.2` (20%), `1.3` (30%) ,or more. If you want to increase the
    number of scales, reduce the scale factor to `1.05` (5%) or more. The fourth parameter
    `3` is the minimum number of neighbors that each candidate position should have
    to become a face position.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，第三个参数1.1是缩放因子，它指定了在每次缩放时图像大小将如何调整。以下图显示了使用缩放因子的缩放金字塔。在我们的案例中，缩放因子是`1.1`。这意味着图像大小减少了10%。这个因子越低，我们找到人脸的机会就越大。缩放过程从原始图像开始，直到图像分辨率在X或Y方向达到模型维度为止。然而，如果缩放级别太多，计算成本会很高。因此，如果你想减少缩放级别，可以将缩放因子增加到`1.2`（20%）、`1.3`（30%）或更高。如果你想增加缩放级别，可以将缩放因子减少到`1.05`（5%）或更高。第四个参数`3`是每个候选位置应具有的最小邻居数，才能成为人脸位置。
- en: '![Extracting the face region using a face detection algorithm](img/00033.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![使用人脸检测算法提取人脸区域](img/00033.jpeg)'
- en: Pyramid of image scales
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图像尺度金字塔
- en: 'The following figure is the result of face detection if we set the number of
    neighbors to zero:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将邻居数设置为零，以下图显示了人脸检测的结果：
- en: '![Extracting the face region using a face detection algorithm](img/00034.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![使用人脸检测算法提取人脸区域](img/00034.jpeg)'
- en: All the candidates for face regions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的人脸区域候选者
- en: 'Finally, the position of the face region can be obtained as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，人脸区域的位置可以按以下方式获得：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each element of the faces vector is a `Rect` object. Therefore, we can get the
    position of the top-left corner with `faces[i].x` and `faces[i].y`. The position
    of the bottom-right corner is `faces[i].x + faces[i].width` and `faces[i].y +
    faces[i].height`. This information will be used as the initial position for the
    facial landmarks process, as described in the following section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: faces向量中的每个元素都是一个`Rect`对象。因此，我们可以通过`faces[i].x`和`faces[i].y`获取顶点的位置。右下角的位置是`faces[i].x
    + faces[i].width`和`faces[i].y + faces[i].height`。这些信息将被用作面部特征点处理过程的初始位置，如以下章节所述。
- en: Extracting facial landmarks from the face region
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从人脸区域提取面部特征点
- en: One disadvantage of the face detector is that the results may have misalignment.
    The misalignment may happen in scaling or translation. Therefore, the extracted
    face regions in all images will not align with each other. This misalignment can
    lead to poor recognition performance, especially with DENSE features. With the
    help of facial landmarks, we can align all the extracted faces so that each facial
    component is in the same area over the datasets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 面部检测器的一个缺点是结果可能存在错位。错位可能发生在缩放或平移过程中。因此，所有图像中提取的面部区域不会彼此对齐。这种错位可能导致识别性能不佳，尤其是在使用DENSE特征时。借助面部特征点，我们可以对所有的提取面部进行对齐，使得每个面部组件在数据集中位于相同的位置。
- en: Many researchers make use of facial landmarks for classification with other
    emotion recognition approaches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员利用面部特征点与其他情绪识别方法进行分类。
- en: We will use the flandmark library to find the location of the eyes, nose and
    mouth. Then, we will use these facial landmarks to extract the precise facial
    bounding box.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用flandmark库来找到眼睛、鼻子和嘴巴的位置。然后，我们将使用这些面部特征点来提取精确的面部边界框。
- en: Introducing the flandmark library
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍flandmark库
- en: Flandmark is an open source C library implementing a facial landmark detector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Flandmark是一个开源的C语言库，实现了面部特征点检测器。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can access the flandmark library main page at: [http://cmp.felk.cvut.cz/~uricamic/flandmark/](http://cmp.felk.cvut.cz/~uricamic/flandmark/).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下网址访问flandmark库的主页：[http://cmp.felk.cvut.cz/~uricamic/flandmark/](http://cmp.felk.cvut.cz/~uricamic/flandmark/)。
- en: 'Given a face image, the goal of the flandmark library is to estimate an S shape
    that represents the location of the facial component. A facial shape in an S is
    an array of (x, y) positions shown as: S = [x[0]y[0]x[1]y[1]....x[n]y[n]].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张人脸图像，flandmark库的目标是估计一个代表面部组件位置的S形。S形中的面部形状是一个表示为(x, y)位置的数组：S = [x[0]y[0]x[1]y[1]....x[n]y[n]]。
- en: 'The pre-trained model in flandmark contains eight points, as shown in the following
    figure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: flandmark中的预训练模型包含八个点，如图所示：
- en: '![Introducing the flandmark library](img/00035.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![介绍flandmark库](img/00035.jpeg)'
- en: The 8 landmarks model and the corresponding index for each landmark.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 8个特征点模型以及每个特征点的对应索引。
- en: In our implementation, we use flandmark because it is easy to integrate it into
    an OpenCV project. Besides, the flandmark library is really robust in many scenarios,
    even when the person is wearing glasses. In the following figure, we show the
    result of using the flandmark library on an image where the person is wearing
    dark glasses. The red dots indicate the facial landmarks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们使用flandmark，因为它很容易集成到OpenCV项目中。此外，flandmark库在许多场景中都非常稳健，即使当人戴着眼镜时也是如此。在以下图中，我们展示了在一个人戴着深色眼镜的图像上使用flandmark库的结果。红色点表示面部特征点。
- en: '![Introducing the flandmark library](img/00036.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![介绍flandmark库](img/00036.jpeg)'
- en: In the next section, we will show you the steps to download and use flandmark
    in our project.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您展示如何在我们的项目中下载和使用flandmark。
- en: Downloading and compiling the flandmark library
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载和编译flandmark库
- en: 'Flandmark is implemented in C and can be integrated into our project easily.
    However, we need to modify some headers in the library source to use it with OpenCV
    3\. The following are the steps to download and compile the library:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Flandmark是用C语言实现的，可以轻松集成到我们的项目中。然而，我们需要修改库源代码中的某些头文件，以便与OpenCV 3兼容。以下是从下载和编译库的步骤：
- en: 'Go to the main page of the flandmark library and follow the GitHub link: [http://github.com/uricamic/flandmark](http://github.com/uricamic/flandmark)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往flandmark库的主页并遵循GitHub链接：[http://github.com/uricamic/flandmark](http://github.com/uricamic/flandmark)
- en: 'Clone the library to your local machine with the following command:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将库克隆到您的本地机器上：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Copy the `libflandmark` folder to your project folder.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`libflandmark`文件夹复制到您的项目文件夹中。
- en: Copy `flandmark_model.dat,which is` in the data folder, to your project folder.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据文件夹中的`flandmark_model.dat`复制到您的项目文件夹中。
- en: 'Edit the `liblbp.h` file in `libflandmark` and change:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`libflandmark`中的`liblbp.h`文件并更改：
- en: '[PRE5]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: to
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将
- en: '[PRE6]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Edit the `flandmark_detector.h` file in `libflandmark` and change:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`libflandmark`中的`flandmark_detector.h`文件并更改：
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: to
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将
- en: '[PRE8]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Edit `CMakeLists.txt` in your project folder to add the flandmark library:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑您项目文件夹中的`CMakeLists.txt`以添加flandmark库：
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Link the executable file with the flandmark static library.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可执行文件链接到flandmark静态库。
- en: 'Add the flandmark header to your source code:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将flandmark头文件添加到您的源代码中：
- en: '[PRE10]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Detecting facial landmarks with flandmark
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用flandmark检测面部特征点
- en: Once you have finished the above steps, the process to extract facial components
    is very straightforward.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，提取面部组件的过程非常简单。
- en: 'First, we create a `FLANDMARK_Model` variable to load the pre-trained model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`FLANDMARK_Model`变量来加载预训练模型：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we save the number of landmarks into the `num_of_landmark` variable and
    create an array to store the output result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将地标数量保存到`num_of_landmark`变量中，并创建一个数组来存储输出结果：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, for each face region, we create an integer array to store the face
    location and use the `flandmark_detect` function to obtain the final result in
    the `points` array:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于每个面部区域，我们创建一个整数数组来存储面部位置，并使用`flandmark_detect`函数在`points`数组中获得最终结果：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first parameter in the `flandmark_detect` function is `IplImage` so we need
    to pass our gray image into the `IplImage` constructor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`flandmark_detect`函数的第一个参数是`IplImage`，因此我们需要将我们的灰度图像传递给`IplImage`构造函数。'
- en: Visualizing the landmarks in an image
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在图像中可视化地标
- en: 'This step is optional. You don''t need to implement the code in this section.
    However, we recommend that you try and understand the results. The following code
    draws a circle on the image at the location of the landmarks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤是可选的。你不需要在这个部分实现代码。然而，我们建议你尝试并理解结果。以下代码在图像上绘制了地标的位置：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following figure shows multiple examples of the results using the above
    code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了使用上述代码的多个结果示例：
- en: '![Visualizing the landmarks in an image](img/00037.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![在图像中可视化地标](img/00037.jpeg)'
- en: Some examples of flandmark results on JAFFE images
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在JAFFE图像上的flandmark结果示例
- en: Extracting the face region
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取面部区域
- en: We now have the location of the eyes, nose, and mouth. It is very easy to extract
    the face region.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了眼睛、鼻子和嘴的位置。提取面部区域非常容易。
- en: 'First, we compute the center of the left eye as the middle of point 2 and point
    6:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算左眼的中心为点2和点6的中点：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Second, the width of the eye region is the difference between x coordinates
    of point 2 and point 6:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，眼区域宽度是点2和点6的x坐标之差：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we find the center and the width of the right eye:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们找到右眼的中点和宽度：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can assume that the width of the face is a bit larger than the distance
    between the eyes, and the height of the face is larger than the width of the face,
    so we can get the eyebrows. We can obtain a good face position with the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设面部宽度略大于眼睛之间的距离，面部高度大于面部宽度，因此我们可以得到眉毛。我们可以使用以下代码获得良好的面部位置：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, the face region can be extracted with the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以使用以下代码提取面部区域：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following figure shows some extracted images from our implementation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了从我们的实现中提取的一些图像：
- en: '![Extracting the face region](img/00038.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![提取面部区域](img/00038.jpeg)'
- en: Some examples of extracted face regions from JAFFE images
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从JAFFE图像中提取的一些面部区域示例
- en: Software usage guide
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件使用指南
- en: 'We have implemented the software to extract facial components from the JAFFE
    dataset. You can use the code as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了从JAFFE数据集中提取面部组件的软件。你可以按照以下方式使用代码：
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载源代码。打开终端，切换到源代码文件夹。
- en: 'Build the software with `cmake` using the following command:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令使用`cmake`构建软件：
- en: '[PRE20]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can use the facial_components tool, as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用facial_components工具，如下所示：
- en: '[PRE21]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The software for this chapter based on OpenCV 3 can be found at: [https://github.com/OpenCVBlueprints/OpenCVBlueprints/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于OpenCV 3的本章软件可以在以下位置找到：[https://github.com/OpenCVBlueprints/OpenCVBlueprints/](https://github.com/OpenCVBlueprints/OpenCVBlueprints/)
- en: 'In order to simplify the process, we save the image paths in a `.yaml` file,
    `list.yml`. The structure of this `.yaml` file is simple. First, we save the number
    of images in the `num_of_image` variable. After that, we save the paths of all
    the images, as shown in the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化过程，我们将图像路径保存在一个`.yaml`文件中，`list.yml`。此`.yaml`文件的结构很简单。首先，我们将图像数量保存到`num_of_image`变量中。之后，我们保存所有图像的路径，如下面的截图所示：
- en: '![Software usage guide](img/00039.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![软件使用指南](img/00039.jpeg)'
- en: An image of the list.yml file
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: list.yml文件的图像
- en: Feature extraction
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: 'Given a dataset of face regions, we can use feature extraction to obtain the
    feature vector, which gives us the most important information from the expression.
    The following figure shows the process that we use in our implementation to extract
    features vectors:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含面部区域的数据集，我们可以使用特征提取来获取特征向量，它提供了表情中最重要信息。以下图显示了我们在实现中用于提取特征向量的过程：
- en: '![Feature extraction](img/00040.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![特征提取](img/00040.jpeg)'
- en: The feature extraction process
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取过程
- en: In order to understand this chapter, you need to understand that the feature
    representation of the expression image is the distribution of image features over
    k clusters (k = 1000 in our implementation). We have implemented a few common
    types of features that are supported in OpenCV, such as SIFT, SURF, and some advanced
    features, such as DENSE-SIFT, KAZE, DAISY. Since these image features are computed
    at image key points such as corners, except for DENSE cases, the number of image
    features can vary between images. However, we want to have a fixed feature size
    for every image to perform classification, since we will apply machine learning
    classification techniques later. It is important that the feature size of the
    images is the same so that we can compare them to obtain the final result. Therefore,
    we apply a clustering technique (kmeans in our case) to separate the image feature
    space into a k cluster. The final feature representation for each image is the
    histogram of the image features over k bins. Moreover, in order to reduce the
    dimension of the final feature, we apply principle component analysis as a last
    step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解本章，您需要了解表情图像的特征表示是图像特征在k个簇（在我们的实现中k = 1000）上的分布。我们已经实现了一些在OpenCV中受支持的常见特征类型，例如SIFT、SURF，以及一些高级特征，如DENSE-SIFT、KAZE、DAISY。由于这些图像特征是在图像的关键点（如角点）上计算的，除了DENSE情况外，图像特征的数量可能在图像之间有所不同。然而，我们希望每个图像都有一个固定的特征大小来进行分类，因为我们将在以后应用机器学习分类技术。重要的是，图像的特征大小必须相同，这样我们才能比较它们以获得最终结果。因此，我们应用聚类技术（在我们的情况下是kmeans）将图像特征空间分离成k个簇。每个图像的最终特征表示是图像特征在k个桶上的直方图。此外，为了减少最终特征的维度，我们在最后一步应用主成分分析。 '
- en: In the following sections, we will explain the process step by step. At the
    end of this section, we will show you how to use our implementation to obtain
    the final feature representation of the dataset.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐步解释这个过程。在本节的末尾，我们将向您展示如何使用我们的实现来获取数据集的最终特征表示。
- en: Extracting image features from facial component regions
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从面部组件区域提取图像特征
- en: At this point, we will assume that you have the face region for each image in
    the dataset. The next step is to extract the image features from these face regions.
    OpenCV provides good implementations of many well-known key point detection and
    feature description algorithms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将假设您已经拥有了数据集中每个图像的面部区域。下一步是从这些面部区域中提取图像特征。OpenCV提供了许多知名的关键点检测和特征描述算法的良好实现。
- en: Note
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Detailed explanations for each algorithm are out of the scope of this chapter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每个算法的详细解释超出了本章的范围。
- en: In this section, we will show you how to use some of these algorithms in our
    implementation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示如何在我们的实现中使用这些算法中的一些。
- en: 'We will use a function that takes current regions, a feature type, and returns
    a matrix with image features as rows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个函数，该函数接受当前区域、特征类型，并返回一个矩阵，其中包含作为行的图像特征：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this `extractFeature` function, we will extract image features from each
    Mat and return the descriptors. The implementation of `extractFeature` is simple,
    and shown here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`extractFeature`函数中，我们将从每个Mat中提取图像特征并返回描述符。`extractFeature`的实现很简单，如下所示：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the above code, we call the corresponding function for each feature. For
    simplicity, we only use one feature each time. In this chapter, we will discuss
    two types of features:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们为每个特征调用相应的函数。为了简单起见，我们每次只使用一个特征。在本章中，我们将讨论两种类型的特征：
- en: '**Contributed features**: SIFT, DAISY, and DENSE SIFT. In OpenCV 3, the implementation
    of SIFT and SURF have been moved to the opencv_contrib module.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贡献特征**：SIFT、DAISY和DENSE SIFT。在OpenCV 3中，SIFT和SURF的实现已被移动到opencv_contrib模块。'
- en: Note
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: These features are patented and you should pay for them if you want to use them
    in commercial applications.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征是受专利保护的，如果您想在商业应用中使用它们，则必须付费。
- en: In this chapter, we will use SIFT features and the SIFT variant, DENSE SIFT.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用SIFT特征及其变体，DENSE SIFT。
- en: Note
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to use the opencv_contrib module, we suggest that you go to the
    *Further reading* section and take a look at the *Compiling the opencv_contrib
    module* section.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想要使用opencv_contrib模块，我们建议你查看*进一步阅读*部分，并查看*编译opencv_contrib模块*部分。
- en: '**Advanced features**: BRISK and KAZE. These features are a good alternative
    to SIFT and SURF in both performance and computation time. DAISY and KAZE are
    only available in OpenCV 3\. DAISY is in opencv_contrib. KAZE is in the main OpenCV
    repository.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级功能**：BRISK和KAZE。这些特征在性能和计算时间上都是SIFT和SURF的良好替代品。DAISY和KAZE仅在OpenCV 3中可用。DAISY在opencv_contrib中，KAZE在主要的OpenCV仓库中。'
- en: Contributed features
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贡献功能
- en: Let's take a look at SIFT features first.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看SIFT特征。
- en: In order to use SIFT features in OpenCV 3, you need to compile the opencv_contrib
    module with OpenCV.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在OpenCV 3中使用SIFT特征，你需要将opencv_contrib模块与OpenCV一起编译。
- en: Note
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We will assume that you have followed the instructions in the *Further reading*
    section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设你已经遵循了*进一步阅读*部分中的说明。
- en: 'The code to extract SIFT features is very simple:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 提取SIFT特征的代码非常简单：
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: First, we create the `Feature2D` variable with `xfeatures2d::SIFT::create()`
    and use the `detect` function to obtain key points. The first parameter for the
    detection function is the image that we want to process. The second parameter
    is a vector to store detected key points. The third parameter is a mask specifying
    where to look for key points. We want to find key points in every position of
    the images so we just pass an empty Mat here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`xfeatures2d::SIFT::create()`创建`Feature2D`变量，并使用`detect`函数来获取关键点。检测函数的第一个参数是我们想要处理的图像。第二个参数是一个存储检测到的关键点的向量。第三个参数是一个掩码，指定了查找关键点的位置。我们希望在图像的每个位置都找到关键点，所以我们在这里传递一个空的Mat。
- en: Finally, we use the `compute` function to extract features descriptors at these
    key points. The computed descriptors are stored in the descriptors variable.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`compute`函数在这些关键点上提取特征描述符。计算出的描述符存储在`descriptors`变量中。
- en: Next, let's take a look at the SURF features.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看SURF特征。
- en: 'The code to obtain SURF features is more or less the same as that for SIFT
    features. We only change the namespace from SIFT to SURF:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 获取SURF特征的代码与SIFT特征的代码大致相同。我们只是将命名空间从SIFT更改为SURF：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let's now move on to DAISY.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向DAISY。
- en: 'DAISY is an improved version of the rotation-invariant BRISK descriptor and
    the LATCH binary descriptor that is comparable to the heavier and slower SURF.
    DAISY is only available in OpenCV 3 in the opencv_contrib module. The code to
    implement DAISY features is fairly similar to the Sift function. However, the
    DAISY class doesn''t have a `detect` function so we will use SURF to detect key
    points and use DAISY to extract descriptors:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: DAISY是旋转不变BRISK描述符和LATCH二进制描述符的改进版本，与较重且较慢的SURF相当。DAISY仅在OpenCV 3的opencv_contrib模块中可用。实现DAISY特征的代码与Sift函数相当相似。然而，DAISY类没有`detect`函数，因此我们将使用SURF来检测关键点，并使用DAISY来提取描述符：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It is now time to take a look at dense SIFT features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看密集SIFT特征了。
- en: Dense collects features at every location and scale in an image. There are plenty
    of applications where dense features are used. However, in OpenCV 3, the interface
    for extracting dense features has been removed. In this section, we show a simple
    approach to extracting dense features using the function in the OpenCV 2.4 source
    code to extract the vector of key points.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 密集特征在每个图像的位置和尺度上收集特征。有很多应用都使用了密集特征。然而，在OpenCV 3中，提取密集特征的接口已被移除。在本节中，我们展示了使用OpenCV
    2.4源代码中的函数提取关键点向量的简单方法。
- en: 'The function to extract dense Sift is similar to the Sift function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 提取密集Sift函数的函数与Sift函数类似：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Instead of using the `detect` function, we can use the `createDenseKeyPoints`
    function to obtain key points. After that, we pass this dense key points vector
    to compute the function. The code for `createDenseKeyPoints` is obtained from
    the OpenCV 2.4 source code. You can find this code at `modules/features2d/src/detectors.cpp`
    in the OpenCV 2.4 repository:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不用`detect`函数，而是使用`createDenseKeyPoints`函数来获取关键点。之后，我们将这个密集关键点向量传递给计算函数。`createDenseKeyPoints`的代码是从OpenCV
    2.4源代码中获得的。你可以在OpenCV 2.4仓库中的`modules/features2d/src/detectors.cpp`找到这段代码：
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Advanced features
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级功能
- en: OpenCV 3 comes bundled with many new and advanced features. In our implementation,
    we will only use the BRISK and KAZE features. However, there are many other features
    in OpenCV.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 3捆绑了许多新的和高级特性。在我们的实现中，我们只会使用BRISK和KAZE特征。然而，OpenCV中还有许多其他特性。
- en: Let us familiarize ourselves with the BRISK features.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们熟悉一下BRISK的特点。
- en: BRISK is a new feature and a good alternative to SURF. It has been added to
    OpenCV since the 2.4.2 version. BRISK is under a BSD license so you don't have
    to worry about the patent problem, as with SIFT or SURF.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK是一个新特性，是SURF的一个很好的替代品。自2.4.2版本以来，它已被添加到OpenCV中。BRISK采用BSD许可，因此你不必担心专利问题，就像SIFT或SURF一样。
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'There is an interesting article about all this, *A battle of three descriptors:
    SURF, FREAK and BRISK,* available at [http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/](http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些内容有一篇有趣的文章，*三个描述符的较量：SURF、FREAK和BRISK*，可在[http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/](http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/)找到。
- en: Let's now move on and have a look at the KAZE features.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，看看KAZE特征。
- en: 'KAZE is a new feature in OpenCV 3\. It produces the best results in many scenarios,
    especially with image matching problems, and it is comparable to SIFT. KAZE is
    in the OpenCV repository so you don''t need opencv_contrib to use it. Apart from
    the high performance, one reason to use KAZE is that it is open source and you
    can use it freely in any commercial applications. The code to use this feature
    is very straightforward:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: KAZE是OpenCV 3中的一个新特性。它在许多场景下产生最佳结果，尤其是在图像匹配问题上，并且与SIFT相当。KAZE位于OpenCV仓库中，因此你不需要opencv_contrib就可以使用它。除了高性能之外，使用KAZE的另一个原因是它是开源的，你可以在任何商业应用中自由使用它。使用此特性的代码非常简单：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The image matching comparison between KAZE, SIFT, and SURF is available at
    the author repository: [https://github.com/pablofdezalc/kaze](https://github.com/pablofdezalc/kaze)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: KAZE、SIFT和SURF之间的图像匹配比较可在作者仓库中找到：[https://github.com/pablofdezalc/kaze](https://github.com/pablofdezalc/kaze)
- en: Visualizing key points for each feature type
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为每种特征类型可视化关键点
- en: In the following figure, we visualize the position of key points for each feature
    type. We draw a circle at each key point; the radius of the circle specifies the
    scale of the image where the key point is extracted. You can see that the key
    points and the corresponding descriptors differ between these features. Therefore,
    the performance of the system will vary, based on the quality of the feature.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们可视化每种特征类型的关键点位置。我们在每个关键点处画一个圆圈；圆圈的半径指定了提取关键点的图像的缩放比例。你可以看到这些特征中的关键点和相应的描述符是不同的。因此，系统的性能将根据特征的质量而变化。
- en: Note
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We recommend that you refer to the *Evaluation* section for more details.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您参考*评估*部分以获取更多详细信息。
- en: '![Visualizing key points for each feature type](img/00041.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![为每种特征类型可视化关键点](img/00041.jpeg)'
- en: The feature extraction process
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取过程
- en: Computing the distribution of feature representation over k clusters
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算特征表示在k个簇上的分布
- en: If you have followed the previous pseudo-code, you should now have a vector
    of descriptors. You can see that the size of descriptors varies between images.
    Since we want a fixed size of feature representation for each image, we will compute
    the distribution of feature representation over k clusters. In our implementation,
    we will use the kmeans clustering algorithm in the core module.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经遵循了之前的伪代码，你现在应该有一个描述符向量。你可以看到描述符的大小在不同图像之间是不同的。由于我们希望每个图像的特征表示具有固定的大小，我们将计算特征表示在k个簇上的分布。在我们的实现中，我们将在核心模块中使用kmeans聚类算法。
- en: Clustering image features space into k clusters
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将图像特征空间聚类成k个簇
- en: First, we assume that the descriptors of all the images are added to a vector,
    called `features_vector`. Then, we need to create a Mat `rawFeatureData` that
    will contain all of the image features as a row. In this case, `num_of_feature`
    is the total number of features in every image and `image_feature_size` is the
    size of each image feature. We choose the number of clusters based on experiment.
    We start with 100 and increase the number for a few iterations. It depends on
    the type of features and data, so you should try to change this variable to suit
    your situation. One downside of a large number of clusters is that the cost for
    computation with kmeans will be high. Moreover, if the number of clusters is too
    large, the feature vector will be too sparse and it may not be good for classification.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设所有图像的描述符都被添加到一个向量中，称为`features_vector`。然后，我们需要创建一个`Mat rawFeatureData`，它将包含所有图像特征作为行。在这种情况下，`num_of_feature`是每张图像中的特征总数，`image_feature_size`是每个图像特征的大小。我们根据实验选择簇的数量。我们开始于100，并在几次迭代中增加数量。这取决于特征和数据类型，因此您应该尝试更改此变量以适应您的具体情况。大量簇的一个缺点是，使用kmeans的计算成本会很高。此外，如果簇的数量太大，特性向量将过于稀疏，这可能不利于分类。
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We need to copy the data from the vector of descriptors (`features_vector`
    in the code) to `imageFeatureData`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将描述符向量（代码中的`features_vector`）中的数据复制到`imageFeatureData`：
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we use the `kmeans` function to perform clustering on the data, as
    follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`kmeans`函数对数据进行聚类，如下所示：
- en: '[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s discuss the parameters of the `kmeans` function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论`kmeans`函数的参数：
- en: '[PRE34]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**InputArray data**: It contains all the samples as a row.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InputArray data**: 它包含所有样本作为行。'
- en: '**int K**: The number of clusters to split the samples ( k = 1000 in our implementation).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**int K**: 分割样本的簇数（在我们的实现中 k = 1000）。'
- en: '**InputOutputArray bestLabels**: Integer array that contains the cluster indices
    for each sample.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InputOutputArray bestLabels**: 包含每个样本簇索引的整数数组。'
- en: '**TermCriteria criteria**: The algorithm termination criteria. This contains
    three parameters (`type`, `maxCount`, `epsilon`).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TermCriteria criteria**: 算法终止准则。这包含三个参数（`type`、`maxCount`、`epsilon`）。'
- en: '**Type**: Type of termination criteria. There are three types:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Type**: 终止准则的类型。有三种类型：'
- en: '**COUNT**: Stop the algorithm after a number of iterations (`maxCount`).'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**COUNT**: 在迭代次数达到一定数量（`maxCount`）后停止算法。'
- en: '**EPS**: Stop the algorithm if the specified accuracy (epsilon) is reached.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EPS**: 如果达到指定的精度（epsilon），则停止算法。'
- en: '**EPS+COUNT**: Stop the algorithm if the COUNT and EPS conditions are fulfilled.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EPS+COUNT**: 如果满足COUNT和EPS条件，则停止算法。'
- en: '**maxCount**: It is the maximum number of iterations.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxCount**: 这是最大迭代次数。'
- en: '**epsilon**: It is the required accuracy needed to stop the algorithm.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epsilon**: 这是停止算法所需的精度。'
- en: '**int attemtps**: It is the number of times the algorithm is executed with
    different initial centroids. The algorithm returns the labels that have the best
    compactness.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**int attempts**: 这是算法以不同初始质心执行次数的数量。算法返回具有最佳紧致性的标签。'
- en: '**int flags**: This flag specifies how initial centroids are random. There
    are three types of flags. Normally, `KMEANS_RANDOM_CENTERS` and `KMEANS_PP_CENTERS`
    are used. If you want to provide your own initial labels, you should use `KMEANS_USE_INITIAL_LABELS`.
    In this case, the algorithm will use your initial labels on the first attempt.
    For further attempts, `KMEANS_*_CENTERS` flags are applied.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**int flags**: 此标志指定初始质心如何随机。通常使用`KMEANS_RANDOM_CENTERS`和`KMEANS_PP_CENTERS`。如果您想提供自己的初始标签，应使用`KMEANS_USE_INITIAL_LABELS`。在这种情况下，算法将在第一次尝试中使用您的初始标签。对于进一步的尝试，将应用`KMEANS_*_CENTERS`标志。'
- en: '**OutputArray centers**: It contains all cluster centroids, one row per each
    centroid.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OutputArray centers**: 它包含所有簇质心，每行一个质心。'
- en: '**double compactness**: It is the returned value of the function. This is the
    sum of the squared distance between each sample to the corresponding centroid.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**double compactness**: 这是函数返回的值。这是每个样本到对应质心的平方距离之和。'
- en: Computing a final feature for each image
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为每个图像计算最终特征
- en: We now have labels for every image feature in the dataset. The next step is
    to compute a fixed size feature for each image. With this in mind, we iterate
    through each image and create a feature vector of k elements, where k is the number
    of clusters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为数据集中的每个图像特征有了标签。下一步是为每个图像计算一个固定大小的特征。考虑到这一点，我们遍历每个图像，创建一个包含 k 个元素的特性向量，其中
    k 是簇的数量。
- en: Then, we iterate through the image features in the current image and increase
    the ith element of the feature vector where i is the label of the image features.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历当前图像中的图像特征，并增加特征向量的第i个元素，其中i是图像特征的标签。
- en: Imagine that we are trying to make a histogram representation of the features
    based on the k centroids. This method looks like a bag of words approach. For
    example, image X has 100 features and image Y has 10 features. We cannot compare
    them because they do not have the same size. However, if we make a histogram of
    1,000 dimensions for each of them, they are then the same size and we can compare
    them easily.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们正在尝试根据k个质心来制作特征的历史图表示。这种方法看起来像是一个词袋方法。例如，图像X有100个特征，图像Y有10个特征。我们无法比较它们，因为它们的大小不同。然而，如果我们为它们中的每一个都制作一个1,000维度的历史图，它们的大小就相同了，我们就可以轻松地比较它们。
- en: Dimensionality reduction
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度降低
- en: In this section, we will use **Principle Component Analysis** (**PCA**) to reduce
    the dimension of the feature space. In the previous step, we have 1,000 dimensional
    feature vectors for each image. In our dataset, we only have 213 samples. Hence,
    the further classifiers tend to overfit the training data in high dimensional
    space. Therefore, we want to use PCA to obtain the most important dimension, which
    has the largest variance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**主成分分析**（**PCA**）来降低特征空间的维度。在上一个步骤中，我们为每个图像有1,000维的特征向量。在我们的数据集中，我们只有213个样本。因此，进一步分类器倾向于在高维空间中过拟合训练数据。因此，我们希望使用PCA来获取最重要的维度，这个维度具有最大的方差。
- en: Next, we will show you how to use PCA in our system.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向您展示如何在我们的系统中使用PCA。
- en: First, we assume that you can store all the features in a Mat named `featureDataOverBins`.
    The number of rows of this Mat should equal to the number of images in the dataset
    and the number of columns of this Mat should be 1,000\. Each row in `featureDataOverBins`
    is a feature of an image.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设您可以将所有特征存储在一个名为`featureDataOverBins`的Mat中。这个Mat的行数应等于数据集中的图像数量，列数应为1,000。`featureDataOverBins`中的每一行都是图像的一个特征。
- en: 'Second, we create a PCA variable:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们创建一个PCA变量：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The first parameter is the data that contains all the features. We don't have
    a pre-computed mean vector so the second parameter should be an empty Mat. The
    third parameter indicates that the feature vectors are stored as matrix rows.
    The final parameter specifies the percentage of variance that PCA should retain.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是包含所有特征的数据。我们没有预先计算的平均向量，因此第二个参数应该是一个空的Mat。第三个参数表示特征向量以矩阵行存储。最后一个参数指定PCA应保留的方差百分比。
- en: Finally, we need to project all the features from 1,000 dimensional feature
    spaces to a lower space. After the projection, we can save these features for
    further processes.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将所有特征从1,000维特征空间投影到一个较低的空间。投影后，我们可以将这些特征保存以供进一步处理。
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The number of dimensions of the new features can be obtained by:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 新特征的数量可以通过以下方式获得：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Software usage guide
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件使用指南
- en: 'We have implemented the previous process to extract the fixed size feature
    for the dataset. Using the software is quite easy:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了先前的过程来为数据集提取固定大小的特征。使用该软件相当简单：
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载源代码。打开终端并将目录更改为源代码文件夹。
- en: 'Build the software with `cmake` using the following command:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令使用`cmake`构建软件：
- en: '[PRE38]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can use the `feature_extraction` tool as follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下方式使用`feature_extraction`工具：
- en: '[PRE39]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `feature_extraction` tool creates a YAML file in the output folder which
    contains the features and labels of every image in the dataset. The available
    parameters are:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`feature_extraction`工具在输出文件夹中创建一个YAML文件，该文件包含数据集中每个图像的特征和标签。可用的参数有：'
- en: '`feature_name`: This can be sift, surf, opponent-sift, or opponent-surf. This
    is the name of the feature type which is used in the feature extraction process.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_name`: 这可以是sift、surf、opponent-sift或opponent-surf。这是在特征提取过程中使用的特征类型的名称。'
- en: '`input_folder`: This has the absolute path to the location of facial components.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_folder`: 这是指向面部组件位置的绝对路径。'
- en: '`output_folder`: This has the absolute path to the folder where you want to
    keep the output file.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_folder`: 这是指向您希望保存输出文件的文件夹的绝对路径。'
- en: The structure of the output file is fairly simple.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件的结构相当简单。
- en: 'We store the size of the feature, cluster centers, the number of images, the
    number of train and test images, the number of labels, and the corresponding label
    names. We also store PCA means, eigenvectors, and eigenvalues. The following figure
    shows a part of the YAML file:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储了特征的大小、聚类中心、图像数量、训练和测试图像数量、标签数量以及相应的标签名称。我们还存储了PCA均值、特征向量和特征值。以下图显示了YAML文件的一部分：
- en: '![Software usage guide](img/00042.jpeg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![软件使用指南](img/00042.jpeg)'
- en: A part of the features.yml file
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: features.yml文件的一部分
- en: 'For each image, we store three variables, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个图像，我们存储三个变量，如下所示：
- en: '`image_feature_<idx>`: It is a Mat that contains features of image idx'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_feature_<idx>`：这是一个包含图像idx特征的Mat'
- en: '`image_label_<idx>`: It is a label of the image idx'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_label_<idx>`：这是图像idx的标签'
- en: '`image_is_train_<idx>`: It is a Boolean specifying whether the image is used
    for training or not.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_is_train_<idx>`：这是一个布尔值，指定图像是否用于训练。'
- en: Classification
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Once you have extracted the features for all the samples in the dataset, it
    is time to start the classification process. The target of this classification
    process is to learn how to make accurate predictions automatically based on the
    training examples. There are many approaches to this problem. In this section,
    we will talk about machine learning algorithms in OpenCV, including neural networks,
    support vector machines, and k-nearest neighbors.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你从数据集的所有样本中提取了特征，就到了开始分类过程的时候了。这个分类过程的目标是学习如何根据训练示例自动进行准确的预测。对此问题有许多方法。在本节中，我们将讨论OpenCV中的机器学习算法，包括神经网络、支持向量机和k-最近邻。
- en: Classification process
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类过程
- en: 'Classification is considered supervised learning. In a classification problem,
    a correctly labelled training set is necessary. A model is produced during the
    training stage which makes predictions and is corrected when predictions are wrong.
    Then, the model is used for predicting in other applications. The model needs
    to be trained every time you have more training data. The following figure shows
    an overview of the classification process:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 分类被认为是监督学习。在分类问题中，需要一个正确标记的训练集。在训练阶段产生一个模型，该模型在预测错误时进行纠正。然后，该模型用于其他应用的预测。每次你有更多训练数据时，都需要对模型进行训练。以下图显示了分类过程概述：
- en: '![Classification process](img/00043.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![分类过程](img/00043.jpeg)'
- en: Overview of the classification process
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 分类过程概述
- en: The choice of learning algorithm to use is a critical step. There are a lot
    of solutions to the classification problem. In this section, we list some of the
    popular machine learning algorithms in OpenCV. The performance of each algorithm
    can vary between classification problems. You should make some evaluations and
    select the one that is the most appropriate for your problem to get the best results.
    It is essential as feature selection may affect the performance of the learning
    algorithm. Therefore, we also need to evaluate each learning algorithm with each
    different feature selection.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 选择要使用的机器学习算法是一个关键步骤。对于分类问题，有很多解决方案。在本节中，我们列出了OpenCV中的一些流行机器学习算法。每种算法在分类问题上的性能可能会有所不同。你应该进行一些评估，并选择最适合你问题的算法以获得最佳结果。这是非常重要的，因为特征选择可能会影响学习算法的性能。因此，我们还需要评估每个学习算法与每个不同的特征选择。
- en: Splitting the dataset into a training set and testing set
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集分割成训练集和测试集
- en: It is important that the dataset is separated into two parts, the training set
    and the testing set. We will use the training set for the learning stage and the
    testing set for the testing stage. In the testing stage, we want to test how the
    trained model predicts unseen samples. In other words, we want to test the *generalization
    capability* of the trained model. Therefore, it is important that the test samples
    are different from the trained samples. In our implementation, we will simply
    split the dataset into two parts. However, it is better if you use k-fold cross
    validation as mentioned in the *Further reading* section.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集分成两部分，即训练集和测试集，这是非常重要的。我们将使用训练集进行学习阶段，测试集用于测试阶段。在测试阶段，我们希望测试训练好的模型如何预测未见过的样本。换句话说，我们希望测试训练模型的*泛化能力*。因此，测试样本与训练样本不同是很重要的。在我们的实现中，我们将简单地将数据集分成两部分。然而，如果你使用*进一步阅读*部分中提到的k折交叉验证会更好。
- en: There is no accurate way to split the dataset into two parts. Common ratios
    are 80:20 and 70:30\. Both the training set and the testing set should be selected
    randomly. If they have the same data, the evaluation is misleading. Basically,
    even if you achieve 99 percent accuracy on your testing set, the model can't work
    in the real world, where the data is different from the training data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种准确的方法可以将数据集分成两部分。常见的比例是80:20和70:30。训练集和测试集都应该随机选择。如果它们有相同的数据，评估将是误导性的。基本上，即使你在测试集上达到了99%的准确率，模型在真实世界中也无法工作，因为真实世界中的数据与训练数据不同。
- en: In our implementation of feature extraction, we have already randomly split
    the dataset and saved the selection in the YAML file.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特征提取实现中，我们已经随机分割了数据集，并将选择保存在YAML文件中。
- en: Note
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The k-fold cross validation is explained in more detail at the end of the *Further
    reading* section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: k折交叉验证在“进一步阅读”部分的末尾有更详细的解释。
- en: Support vector machines
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**A Support Vector Machine** (**SVM**) is a supervised learning technique applicable
    to both classification and regression. Given labelled training data, the goal
    of SVM is to produce an optimal hyper plane which predicts the target value of
    a test sample with only test sample attributes. In other words, SVM generates
    a function to map between input and output based on labelled training data.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种适用于分类和回归的监督学习技术。给定标记的训练数据，SVM的目标是生成一个最佳超平面，该超平面仅根据测试样本的属性预测测试样本的目标值。换句话说，SVM基于标记的训练数据生成一个从输入到输出的函数。'
- en: 'For example, let''s assume that we want to find a line to separate two sets
    of 2D points. The following figure shows that there are several solutions to the
    problem:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要找到一条线来分离两组2D点。以下图显示了该问题的几个解决方案：
- en: '![Support vector machines](img/00044.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/00044.jpeg)'
- en: A lot of hyper planes can solve a problem
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多超平面可以解决问题
- en: 'The goal of SVM is to find a hyper plane that maximizes the distances to the
    training samples. The distances are calculated to only support those vectors that
    are closest to the hyper plane. The following figure shows an optimal hyper plane
    to separate two sets of 2D points:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的目标是找到一个超平面，该超平面最大化到训练样本的距离。这些距离仅计算最接近超平面的向量。以下图显示了分离两组2D点的最佳超平面：
- en: '![Support vector machines](img/00045.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/00045.jpeg)'
- en: An optimal hyper plane that maximizes the distances to the training samples.
    R is the maximal margin
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一个最大化到训练样本距离的最佳超平面。R是最大间隔
- en: In the following sections, we will show you how to use SVM to train and test
    facial expression data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将向您展示如何使用支持向量机（SVM）来训练和测试面部表情数据。
- en: Training stage
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: One of the most difficult parts about training an SVM is parameters selection.
    It is not possible to explain everything without some deep understanding of how
    SVM works. Luckily, OpenCV implements a `trainAuto` method for automatic parameter
    estimation. If you have enough knowledge of SVM, you should try to use your own
    parameters. In this section, we will introduce the `trainAuto` method to give
    you an overview of SVM.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 训练SVM中最困难的部分之一是参数选择。没有对SVM工作原理的深入了解，无法解释所有内容。幸运的是，OpenCV实现了`trainAuto`方法来自动估计参数。如果你对SVM有足够的了解，你应该尝试使用自己的参数。在本节中，我们将介绍`trainAuto`方法，以向您概述SVM。
- en: SVM is inherently a technique for building an optimal hyper plane in binary
    (2-class) classification. In our facial expression problem, we want to classify
    seven expressions. One-versus-all and one-versus-one are two common approaches
    that we can follow to use SVM in this problem. One-versus-all trains one SVM for
    each class. There are seven SVMs in our case. For class i, every sample with the
    label i is considered as positive and the rest of the samples are negative. This
    approach is prone to error when the dataset samples are imbalanced between classes.
    The one-versus-one approach trains an SVM for each different pairs of classes.
    The number of SVMs in total is *N*(N-1)/2* SVMs. This means 21 SVMs in our case.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: SVM本质上是构建二进制（2类）分类中最佳超平面的技术。在我们的面部表情问题中，我们想要对七个表情进行分类。一对一和一对多是我们可以使用SVM的两种常见方法。一对一方法为每个类别训练一个SVM。在我们的例子中有七个SVM。对于类别i，所有标签为i的样本被视为正样本，其余样本被视为负样本。当数据集样本在类别之间不平衡时，这种方法容易出错。一对多方法为每个不同类别的成对训练一个SVM。总的SVM数量是
    *N*(N-1)/2* 个SVM。这意味着在我们的例子中有21个SVM。
- en: In OpenCV, you don't have to follow these approaches. OpenCV supports the training
    of one multiclass SVM. However, you should follow the above methods for better
    results. We will still use one multiclass SVM. The training and testing process
    will be simpler.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenCV 中，你不必遵循这些方法。OpenCV 支持训练一个多类 SVM。然而，为了获得更好的结果，你应该遵循上述方法。我们仍然将使用一个多类
    SVM。训练和测试过程将更简单。
- en: Next, we will demonstrate our implementation to solve the facial expression
    problem.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将演示我们的实现来解决面部表情问题。
- en: 'First, we create an instance of SVM:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 SVM 的实例：
- en: '[PRE40]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you want to change parameters, you can call the `set` function in the `svm`
    variable, as shown:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更改参数，你可以在 `svm` 变量中调用 `set` 函数，如下所示：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**Type**: It is the type of SVM formulation. There are five possible values:
    `C_SVC`, `NU_SVC`, `ONE_CLASS`, `EPS_SVR`, and `NU_SVR`. However, in our multiclass
    classification, only `C_SVC` and `NU_SVC` are suitable. The difference between
    these two lies in the mathematical optimization problem. For now, we can just
    use `C_SVC`.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：它是 SVM 公式的类型。有五个可能的值：`C_SVC`、`NU_SVC`、`ONE_CLASS`、`EPS_SVR` 和 `NU_SVR`。然而，在我们的多类分类中，只有
    `C_SVC` 和 `NU_SVC` 是合适的。这两个之间的区别在于数学优化问题。目前，我们可以使用 `C_SVC`。'
- en: '**Kernel**: It is the type of SVM kernel. There are four possible values: `LINEAR`,
    `POLY`, `RBF`, and `SIGMOID`. The kernel is a function to map the training data
    to a higher dimensional space that makes data linearly separable. This is also
    known as *Kernel Trick*. Therefore, we can use SVM in non-linear cases with the
    support of the kernel. In our case, we choose the most commonly-used kernel, RBF.
    You can switch between these kernels and choose the best.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核函数**：它是 SVM 核的类型。有四个可能的值：`LINEAR`、`POLY`、`RBF` 和 `SIGMOID`。核函数是一个将训练数据映射到更高维空间的功能，使得数据线性可分。这也被称为
    *核技巧*。因此，我们可以使用核的支持在非线性情况下使用 SVM。在我们的情况下，我们选择最常用的核函数，RBF。你可以在这几个核函数之间切换并选择最佳选项。'
- en: You can also set other parameters such as TermCriteria, Degree, Gamma. We are
    just using the default parameters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以设置其他参数，如 TermCriteria、Degree、Gamma。我们只是使用默认参数。
- en: 'Second, we create a variable of `ml::TrainData` to store all the training set
    data:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们创建一个 `ml::TrainData` 变量来存储所有训练集数据：
- en: '[PRE42]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`train_features`: It is a Mat that contains each features vector as a row.
    The number of rows of `train_features` is the number of training samples, and
    the number of columns is the size of one features vector.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_features`：它是一个 Mat，其中每行包含一个特征向量。`train_features` 的行数是训练样本的数量，列数是一个特征向量的大小。'
- en: '`SampleTypes::ROW_SAMPLE`: It specifies that each features vector is in a row.
    If your features vectors are in columns, you should use COL_SAMPLE.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SampleTypes::ROW_SAMPLE`：它指定每个特征向量位于一行。如果你的特征向量位于列中，你应该使用 COL_SAMPLE。'
- en: '`train_labels`: It is a Mat that contains labels for each training feature.
    In SVM, `train_labels` will be a Nx1 matrix, N is the number of training samples.
    The value of each row is the truth label of the corresponding sample. At the time
    of writing, the type of `train_labels` should be `CV_32S`. Otherwise, you may
    encounter an error. The following code is what we use to create the `train_labels`
    variable:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_labels`：它是一个 Mat，其中包含每个训练特征的标签。在 SVM 中，`train_labels` 将是一个 Nx1 矩阵，N
    是训练样本的数量。每行的值是对应样本的真实标签。在撰写本文时，`train_labels` 的类型应该是 `CV_32S`。否则，你可能会遇到错误。以下是我们创建
    `train_labels` 变量的代码：'
- en: '[PRE43]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we pass `trainData` to the `trainAuto` function so that OpenCV can
    select the best parameters automatically. The interface of the `trainAuto` function
    contains many other parameters. In order to keep things simple, we will use the
    default parameters:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将 `trainData` 传递给 `trainAuto` 函数，以便 OpenCV 可以自动选择最佳参数。`trainAuto` 函数的接口包含许多其他参数。为了保持简单，我们将使用默认参数：
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Testing stage
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试阶段
- en: 'After we''ve trained the SVM, we can pass a test sample to the predict function
    of the `svm` model and receive a label prediction, as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练了 SVM 之后，我们可以将一个测试样本传递给 `svm` 模型的预测函数，并接收一个标签预测，如下所示：
- en: '[PRE45]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In this case, the sample is a feature vector just like the feature vector in
    the training features. The response is the label of the sample.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，样本是一个特征向量，就像训练特征中的特征向量一样。响应是样本的标签。
- en: Multi-layer perceptron
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知器
- en: OpenCV implements the most common type of artificial neural network, the multi-layer
    perceptron (MLP). A typical MLP consists of an input layer, an output layer, and
    one or more hidden layers. It is known as a supervised learning method because
    it needs a desired output to train. With enough data, MLP, given enough hidden
    layers, can approximate any function to any desired accuracy.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV实现了最常见的人工神经网络类型，即多层感知器（MLP）。一个典型的MLP由一个输入层、一个输出层和一个或多个隐藏层组成。它被称为监督学习方法，因为它需要期望的输出来进行训练。有了足够的数据，MLP，如果给定足够的隐藏层，可以近似任何函数到任何期望的精度。
- en: 'An MLP with a single hidden layer can be represented as it is in the following
    figure:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 具有一个隐藏层的多层感知器可以表示如下图所示：
- en: '![Multi-layer perceptron](img/00046.jpeg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/00046.jpeg)'
- en: A single hidden layer perceptron
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 单隐藏层感知器
- en: A detailed explanation and proof of how the MLP learns are out of the scope
    of this chapter. The idea is that the output of each neuron is a function of neurons
    from previous layers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: MLP如何学习的一个详细解释和证明超出了本章的范围。其思想是每个神经元的输出是前一层神经元的函数。
- en: 'In the above single hidden layer MLP, we use the following notation:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述单隐藏层MLP中，我们使用以下符号：
- en: 'Input layer: x[1] x[2]'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层：x[1] x[2]
- en: 'Hidden layer: h[1] h[2] h[3]'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层：h[1] h[2] h[3]
- en: 'Output layer: y'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层：y
- en: Each connection between each neuron has a weight. The weight shown in the above
    figure is between neuron i (that is i = 3) in the current layer and neuron j (that
    is j = 2) in the previous layer is w[ij]. Each neuron has a bias value 1 with
    a weight, w[i,bias].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元之间的每个连接都有一个权重。上图所示的权重是当前层中的神经元i（即i = 3）和前一层中的神经元j（即j = 2）之间的权重，表示为w[ij]。每个神经元都有一个权重为1的偏置值，表示为w[i,bias]。
- en: 'The output at neuron i is the result of an activation function *f*:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元i的输出是激活函数*f*的结果：
- en: '![Multi-layer perceptron](img/00047.jpeg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/00047.jpeg)'
- en: 'There are many types of activation functions. In OpenCV, there are three types
    of activation functions: Identity, Sigmoid, and Gaussian. However, the Gaussian
    function is not completely supported at the time of writing and the Identity function
    is not commonly used. We recommend that you use the default activation, Sigmoid.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有很多种类型。在OpenCV中，有三种类型的激活函数：恒等函数、Sigmoid和高斯。然而，在撰写本文时，高斯函数并不完全受支持，恒等函数也不常用。我们建议您使用默认的激活函数，即Sigmoid。
- en: In the following sections, we will show you how to train and test a multi-layer
    perceptron.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将向您展示如何训练和测试一个多层感知器。
- en: Training stage
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: In the training stage, we first define the network and then train the network.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，我们首先定义网络，然后训练网络。
- en: Define the network
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义网络
- en: We will use a simple four layer neural network in our facial expression problem.
    The network has one input layer, two hidden layers, and one output layer.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的面部表情问题中，我们将使用一个简单的四层神经网络。该网络有一个输入层、两个隐藏层和一个输出层。
- en: 'First, we need to create a matrix to hold the layers definition. This matrix
    has four rows and one column:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个矩阵来保存层的定义。这个矩阵有四行一列：
- en: '[PRE46]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we assign the number of neurons for each layer, as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为每一层分配神经元数量，如下所示：
- en: '[PRE47]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this network, the number of neurons for the input layer has to be equal to
    the number of elements of each feature vector, and number of neurons for the output
    layer is the number of facial expression labels (`feature_size` equals `train_features.cols`
    where `train_features` is the Mat that contains all features and `num_of_labels`
    equals 7 in our implementation).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络中，输入层的神经元数量必须等于每个特征向量的元素数量，输出层的神经元数量是面部表情标签的数量（`feature_size`等于`train_features.cols`，其中`train_features`是包含所有特征的Mat，`num_of_labels`在我们的实现中等于7）。
- en: The above parameters in our implementation are not optimal. You can try different
    values for different numbers of hidden layers and numbers of neurons. Remember
    that the number of hidden neurons should not be larger than the number of training
    samples. It is very difficult to choose the number of neurons in a hidden layer
    and the number of layers in your network. If you do some research, you can find
    several rules of thumb and diagnostic techniques. The best way to choose these
    parameters is experimentation. Basically, the more layers and hidden neurons there
    are, the more capacity you have in the network. However, more capacity may lead
    to overfitting. One of the most important rules is that the number of examples
    in the training set should be larger than the number of weights in the network.
    Based on our experience, you should start with one hidden layer with a small number
    of neurons and calculate the generalization error and training error. Then, you
    should modify the number of neurons and repeat the process.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现中的上述参数并非最优。您可以尝试为不同数量的隐藏层和神经元数量尝试不同的值。请记住，隐藏神经元的数量不应超过训练样本的数量。在隐藏层中神经元数量和网络的层数选择上非常困难。如果您做一些研究，您会发现一些经验规则和诊断技术。选择这些参数的最佳方式是实验。基本上，层和隐藏神经元越多，网络的能力就越强。然而，更多的能力可能会导致过拟合。最重要的规则之一是训练集中的示例数量应大于网络中的权重数量。根据我们的经验，您应该从一个包含少量神经元的隐藏层开始，并计算泛化误差和训练误差。然后，您应该修改神经元数量并重复此过程。
- en: Note
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Remember to make a graph to visualize the error when you change parameters.
    Keep in mind that the number of neurons is usually between the input layer size
    and the output layer size. After a few iterations, you can decide whether to add
    an additional layer or not.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在更改参数时制作图表以可视化误差。请记住，神经元的数量通常介于输入层大小和输出层大小之间。经过几次迭代后，您可以决定是否添加额外的层。
- en: However, in this case, we don't have much data. This makes the network hard
    to train. We may not add neurons and layers to improve the performance.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们没有太多数据。这使得网络难以训练。我们可能不会添加神经元和层来提高性能。
- en: Train the network
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'First, we create a network variable, ANN_MLP, and add the layers definition
    to the network:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个网络变量，ANN_MLP，并将层定义添加到网络中：
- en: '[PRE48]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then, we need to prepare some parameters for training algorithms. There are
    two algorithms for training MLP: the back-propagation algorithm and the RPROP
    algorithm. RPROP is the default algorithm for training. There are many parameters
    for RPROP so we will use the back-propagation algorithm for simplicity.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为训练算法准备一些参数。MLP的训练有两种算法：反向传播算法和RPROP算法。RPROP是默认的训练算法。RPROP有很多参数，所以我们为了简单起见将使用反向传播算法。
- en: 'Below is our code for setting parameters for the back-propagation algorithm:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们为反向传播算法设置参数的代码：
- en: '[PRE49]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We set the `TrainMethod` to `BACKPROP` to use the back-propagation algorithm.
    Select Sigmoid as the activation function There are three types of activation
    in OpenCV: `IDENTITY`, `GAUSSIAN`, and `SIGMOID`. You can go to the overview of
    this section for more details.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`TrainMethod`设置为`BACKPROP`以使用反向传播算法。选择Sigmoid作为激活函数。在OpenCV中有三种激活类型：`IDENTITY`、`GAUSSIAN`和`SIGMOID`。您可以查看本节概述以获取更多详细信息。
- en: The final parameter is `TermCriteria`. This is the algorithm termination criteria.
    You can see an explanation of this parameter in the kmeans algorithm in the previous
    section.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个参数是`TermCriteria`。这是算法终止标准。您可以在前一个部分中关于kmeans算法的解释中看到这个参数的解释。
- en: Next, we create a `TrainData` variable to store all the training sets. The interface
    is the same as in the SVM section.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`TrainData`变量来存储所有训练集。其接口与SVM部分相同。
- en: '[PRE50]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`train_features` is the Mat which stores all training samples as in the SVM
    section. However, `train_labels` is different:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_features`是一个Mat，它存储了所有训练样本，就像在SVM部分中做的那样。然而，`train_labels`是不同的：'
- en: '`train_features`: This is a Mat that contains each features vector as a row
    as we did in the SVM. The number of rows of `train_features` is the number of
    training samples and the number of columns is the size of one features vector.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_features`：这是一个Mat，它包含每个特征向量作为一行，就像我们在SVM中做的那样。`train_features`的行数是训练样本的数量，列数是一个特征向量的大小。'
- en: '`train_labels`: This is a Mat that contains labels for each training feature.
    Instead of the Nx1 matrix in SVM, `train_labels` in MLP should be a NxM matrix,
    N is the number of training samples and M is the number of labels. If the feature
    at row i is classified as label j, the position (i, j) of `train_labels` will
    be 1\. Otherwise, the value will be zero. The code to create the `train_labels`
    variable is as follows:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_labels`：这是一个包含每个训练特征标签的 Mat。与 SVM 中的 Nx1 矩阵不同，MLP 中的 `train_labels`
    应该是一个 NxM 矩阵，N 是训练样本的数量，M 是标签的数量。如果第 i 行的特征被分类为标签 j，则 `train_labels` 的位置 (i, j)
    将是 1。否则，值将是零。创建 `train_labels` 变量的代码如下：'
- en: '[PRE51]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we train the network with the following code:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用以下代码训练网络：
- en: '[PRE52]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The training process takes a few minutes to complete. If you have a lot of training
    data, it may take a few hours.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程需要几分钟才能完成。如果你有大量的训练数据，可能需要几小时。
- en: Testing stage
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试阶段
- en: Once we have trained our MLP, the testing stage is very simple.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了我们的 MLP，测试阶段就非常简单。
- en: First, we create a Mat to store the response of the network. The response is
    an array, whose length is the number of labels.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 Mat 来存储网络的响应。响应是一个数组，其长度是标签的数量。
- en: '[PRE53]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Then, we assume that we have a Mat, called sample, which contains a feature
    vector. In our facial expression case, its size should be 1x1000.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们假设我们有一个名为 sample 的 Mat，它包含一个特征向量。在我们的面部表情案例中，其大小应该是 1x1000。
- en: 'We can call the `predict` function of the `mlp` model to obtain the response,
    as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用 `mlp` 模型的 `predict` 函数来获取响应，如下所示：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The predicted label of the input sample is the index of the maximum value in
    the response array. You can find the label by simply iterating through the array.
    The disadvantage of this type of response is that you have to apply a `softmax`
    function if you want a probability for each response. In other neural network
    frameworks, there is usually a softmax layer for this reason. However, the advantage
    of this type of response is that the magnitude of each response is retained.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 输入样本的预测标签是响应数组中最大值的索引。你可以通过简单地遍历数组来找到标签。这种类型响应的缺点是，如果你想为每个响应应用一个 `softmax` 函数以获得概率，你必须这样做。在其他神经网络框架中，通常有一个
    softmax 层来处理这种情况。然而，这种类型响应的优点是保留了每个响应的大小。
- en: K-Nearest Neighbors (KNN)
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Nearest Neighbors (KNN)
- en: '**K-Nearest Neighbors** (**KNN**) is a very simple algorithm for machine learning
    but works very well in many practical problems. The idea of KNN is to classify
    an unknown example with the most common class among k-nearest known examples.
    KNN is also known as a non-parametric lazy learning algorithm. It means that KNN
    doesn''t make any assumptions about the data distribution. The training process
    is very fast since it only caches all training examples. However, the testing
    process requires a lot of computation. The following figure demonstrates how KNN
    works in a 2D points case. The green dot is an unknown sample. KNN will find k-nearest
    known samples in space, (k = 5 in this example). There are three samples of red
    labels and two samples of blue labels. Therefore, the label for the prediction
    is red.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-Nearest Neighbors** (**KNN**) 是一种非常简单的机器学习算法，但在许多实际问题中表现良好。KNN 的思想是将未知样本分类为
    k 个最近已知样本中最常见的类别。KNN 也被称为非参数懒惰学习算法。这意味着 KNN 对数据分布没有任何假设。由于它只缓存所有训练示例，因此训练过程非常快。然而，测试过程需要大量的计算。以下图示展示了
    KNN 在二维点情况下的工作原理。绿色点是一个未知样本。KNN 将在空间中找到 k 个最近的已知样本（本例中 k = 5）。有三个红色标签的样本和两个蓝色标签的样本。因此，预测的标签是红色。'
- en: '![K-Nearest Neighbors (KNN)](img/00048.jpeg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![K-Nearest Neighbors (KNN)](img/00048.jpeg)'
- en: An explanation of how KNN predicts labels for unknown samples
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 解释 KNN 如何预测未知样本的标签
- en: Training stage
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: 'The implementation of KNN algorithms is very simple. We only need three lines
    of code to train a KNN model:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法的实现非常简单。我们只需要三行代码来训练一个 KNN 模型：
- en: '[PRE55]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The preceding code is the same as with SVM:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码与 SVM 相同：
- en: '`train_features`: This is a Mat that contains each features vector as a row.
    The number of rows in `train_features` is the number of training samples and the
    number of columns is the size of one features vector.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_features`：这是一个包含每个特征向量作为行的 Mat。`train_features` 中的行数是训练样本的数量，列数是一个特征向量的大小。'
- en: '`train_labels`: This is a Mat that contains labels for each training feature.
    In KNN, `train_labels` is a Nx1 matrix, N is the number of training samples. The
    value of each row is the truth label of the corresponding sample. The type of
    this Mat should be `CV_32S`.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_labels`: 这是一个包含每个训练特征标签的Mat。在KNN中，`train_labels`是一个Nx1的矩阵，N是训练样本的数量。每一行的值是对应样本的真实标签。这个Mat的类型应该是`CV_32S`。'
- en: The testing stage
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试阶段
- en: 'The testing stage is very straightforward. We can just pass a feature vector
    to the `findNearest` method of the `knn` model and obtain the label:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 测试阶段非常直接。我们只需将一个特征向量传递给`knn`模型的`findNearest`方法，就可以获得标签：
- en: '[PRE56]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The second parameter is the most important parameter. It is the number of maximum
    neighbors that may be used for classification. In theory, if there are an infinite
    number of samples available, a larger K always means a better classification.
    However, in our facial expression problem, we only have 213 samples in total and
    about 170 samples in the training set. Therefore, if we use a large K, KNN may
    end up looking for samples that are not neighbors. In our implementation, K equals
    2.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是最重要的参数。它是用于分类可能使用的最大邻居数。理论上，如果有无限数量的样本可用，更大的K总是意味着更好的分类。然而，在我们的面部表情问题中，我们总共有213个样本，其中大约有170个样本在训练集中。因此，如果我们使用大的K，KNN最终可能会寻找非邻居的样本。在我们的实现中，K等于2。
- en: 'The predicted labels are stored in the `predictedLabels` variable and can be
    obtained as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 预测标签存储在`predictedLabels`变量中，可以按以下方式获取：
- en: '[PRE57]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Normal Bayes classifier
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正态贝叶斯分类器
- en: The Normal Bayes classifier is one of the simplest classifiers in OpenCV. The
    Normal Bayes classifier assumes that features vectors from each class are normally
    distributed, although not necessarily independently. This classifier is an effective
    classifier that can handle multiple classes. In the training step, the classifier
    estimates the mean and co-variance of the distribution for each class. In the
    testing step, the classifier computes the probability of the features to each
    class. In practice, we then test to see if the maximum probability is over a threshold.
    If it is, the label of the sample will be the class that has the maximum probability.
    Otherwise, we say that we can't recognize the sample.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 正态贝叶斯分类器是OpenCV中最简单的分类器之一。正态贝叶斯分类器假设来自每个类别的特征向量是正态分布的，尽管不一定独立。这是一个有效的分类器，可以处理多个类别。在训练步骤中，分类器估计每个类别的分布的均值和协方差。在测试步骤中，分类器计算特征向量到每个类别的概率。在实践中，我们然后测试最大概率是否超过阈值。如果是，样本的标签将是具有最大概率的类别。否则，我们说我们无法识别该样本。
- en: OpenCV has already implemented this classifier in the ml module. In this section,
    we will show you the code to use the Normal Bayes classifier in our facial expression
    problem.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV已经在ml模块中实现了这个分类器。在本节中，我们将向您展示如何在我们的面部表情问题中使用正态贝叶斯分类器的代码。
- en: Training stage
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: The code to implement the Normal Bayes classifier is the same as with SVM and
    KNN. We only need to call the `create` function to obtain the classifier and start
    the training process. All the other parameters are the same as with SVM and KNN.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 实现正态贝叶斯分类器的代码与SVM和KNN相同。我们只需要调用`create`函数来获取分类器并开始训练过程。所有其他参数与SVM和KNN相同。
- en: '[PRE58]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Testing stage
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试阶段
- en: 'The code to test a sample with the Normal Bayes classifier is a little different
    from previous methods:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正态贝叶斯分类器测试样本的代码与之前的方法略有不同：
- en: 'First, we need to create two Mats to store the output class and probability:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建两个Mat来存储输出类别和概率：
- en: '[PRE59]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Then, we call the `predictProb` function of the model:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用模型的`predictProb`函数：
- en: '[PRE60]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The computed probability is stored in `outputProb` and the corresponding label
    can be retrieved as:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算出的概率存储在`outputProb`中，相应的标签可以检索如下：
- en: '[PRE61]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Software usage guide
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件使用指南
- en: 'We have implemented the above process to perform classification with a training
    set. Using the software is quite easy:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了上述过程，使用训练集进行分类。使用该软件相当简单：
- en: Download the source code. Open the terminal and change directory to the source
    code folder.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载源代码。打开终端，切换到源代码文件夹。
- en: 'Build the software with `cmake` using the follow command:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令使用`cmake`构建软件：
- en: '[PRE62]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You can use the `train` tool as follows:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下方式使用`train`工具：
- en: '[PRE63]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The `train` tool performs the training process and outputs the accuracy on
    the console. The learned model will be saved to the output folder for further
    use as `model.yml`. Furthermore, kmeans centers and pca information from features
    extraction are also saved in `features_extraction.yml`. The available parameters
    are:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 工具执行训练过程并在控制台上输出准确率。学习到的模型将被保存到输出文件夹中，以便进一步使用，文件名为 `model.yml`。此外，特征提取的kmeans中心信息和pca信息也将保存在
    `features_extraction.yml` 文件中。可用的参数包括：'
- en: '`algorithm_name`: This can be `mlp`, `svm`, `knn`, `bayes`. This is the name
    of the learning algorithm.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`algorithm_name`：这可以是 `mlp`、`svm`、`knn`、`bayes`。这是学习算法的名称。'
- en: '`input_features`: This is the absolute path to the location of the YAML features
    file from the `prepare_dataset` tool.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`：这是 `prepare_dataset` 工具的YAML特征文件的绝对路径。'
- en: '`output_folder`: This is the absolute path to the folder where you want to
    keep the output model.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_folder`：这是您希望保存输出模型的文件夹的绝对路径。'
- en: Evaluation
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: In this section, we will show the performance of our facial expression recognition
    system. In our test, we will keep the parameters of each learning algorithm the
    same and only change the feature extraction. We will evaluate the feature extraction
    with the number of clusters equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示我们面部表情识别系统的性能。在我们的测试中，我们将保持每个学习算法的参数相同，仅更改特征提取。我们将使用聚类数量等于200、500、1,000、1,500、2,000和3,000来评估特征提取。
- en: The following table shows the accuracy of the system with the number of clusters
    equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了系统在聚类数量等于200、500、1,000、1,500、2,000和3,000时的准确率。
- en: 'Table 1: The accuracy (%) of the system with 1,000 clusters'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：系统在1,000个聚类下的准确率（%）
- en: '| K = 1000 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| K = 1000 | MLP | SVM | KNN | Normal Bayes |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 72.7273 | 93.1818 | 81.8182 | 88.6364 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 72.7273 | 93.1818 | 81.8182 | 88.6364 |'
- en: '| SURF | 61.3636 | 79.5455 | 72.7273 | 79.5455 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 61.3636 | 79.5455 | 72.7273 | 79.5455 |'
- en: '| BRISK | 61.3636 | 65.9091 | 59.0909 | 68.1818 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 61.3636 | 65.9091 | 59.0909 | 68.1818 |'
- en: '| KAZE | 50 | 79.5455 | 61.3636 | 77.2727 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 50 | 79.5455 | 61.3636 | 77.2727 |'
- en: '| DAISY | 59.0909 | 77.2727 | 65.9091 | 81.8182 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 59.0909 | 77.2727 | 65.9091 | 81.8182 |'
- en: '| DENSE-SIFT | 20.4545 | 45.4545 | 43.1818 | 40.9091 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 20.4545 | 45.4545 | 43.1818 | 40.9091 |'
- en: 'Table 2: The accuracy (%) of the system with 500 clusters'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：系统在500个聚类下的准确率（%）
- en: '| K = 500 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| K = 500 | MLP | SVM | KNN | Normal Bayes |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 56.8182 | 70.4545 | 75 | 77.2727 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 56.8182 | 70.4545 | 75 | 77.2727 |'
- en: '| SURF | 54.5455 | 63.6364 | 68.1818 | 79.5455 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 54.5455 | 63.6364 | 68.1818 | 79.5455 |'
- en: '| BRISK | 36.3636 | 59.0909 | 52.2727 | 52.2727 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 36.3636 | 59.0909 | 52.2727 | 52.2727 |'
- en: '| KAZE | 47.7273 | 56.8182 | 63.6364 | 65.9091 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 47.7273 | 56.8182 | 63.6364 | 65.9091 |'
- en: '| DAISY | 54.5455 | 75 | 63.6364 | 75 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 54.5455 | 75 | 63.6364 | 75 |'
- en: '| DENSE-SIFT | 27.2727 | 43.1818 | 38.6364 | 43.1818 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 27.2727 | 43.1818 | 38.6364 | 43.1818 |'
- en: 'Table 3: The accuracy (%) of the system with 200 clusters'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：系统在200个聚类下的准确率（%）
- en: '| K = 200 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| K = 200 | MLP | SVM | KNN | Normal Bayes |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 50 | 68.1818 | 65.9091 | 75 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 50 | 68.1818 | 65.9091 | 75 |'
- en: '| SURF | 43.1818 | 54.5455 | 52.2727 | 63.6364 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 43.1818 | 54.5455 | 52.2727 | 63.6364 |'
- en: '| BRISK | 29.5455 | 47.7273 | 50 | 54.5455 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 29.5455 | 47.7273 | 50 | 54.5455 |'
- en: '| KAZE | 50 | 59.0909 | 72.7273 | 59.0909 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 50 | 59.0909 | 72.7273 | 59.0909 |'
- en: '| DAISY | 45.4545 | 68.1818 | 65.9091 | 70.4545 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 45.4545 | 68.1818 | 65.9091 | 70.4545 |'
- en: '| DENSE-SIFT | 29.5455 | 43.1818 | 40.9091 | 31.8182 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 29.5455 | 43.1818 | 40.9091 | 31.8182 |'
- en: 'Table 4: The accuracy (%) of the system with 1,500 clusters'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：系统在1,500个聚类下的准确率（%）
- en: '| K = 1500 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| K = 1500 | MLP | SVM | KNN | Normal Bayes |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 45.4545 | 84.0909 | 75 | 79.5455 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 45.4545 | 84.0909 | 75 | 79.5455 |'
- en: '| SURF | 72.7273 | 88.6364 | 79.5455 | 86.3636 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 72.7273 | 88.6364 | 79.5455 | 86.3636 |'
- en: '| BRISK | 54.5455 | 72.7273 | 56.8182 | 68.1818 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 54.5455 | 72.7273 | 56.8182 | 68.1818 |'
- en: '| KAZE | 45.4545 | 79.5455 | 72.7273 | 77.2727 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 45.4545 | 79.5455 | 72.7273 | 77.2727 |'
- en: '| DAISY | 61.3636 | 88.6364 | 65.9091 | 81.8182 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 61.3636 | 88.6364 | 65.9091 | 81.8182 |'
- en: '| DENSE-SIFT | 34.0909 | 47.7273 | 38.6364 | 38.6364 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 34.0909 | 47.7273 | 38.6364 | 38.6364 |'
- en: 'Table 5: The accuracy (%) of the system with 2,000 clusters'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：系统在2,000个聚类下的准确率（%）
- en: '| K = 2000 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| K = 2000 | MLP | SVM | KNN | Normal Bayes |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 63.6364 | 88.6364 | 81.8182 | 88.6364 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 63.6364 | 88.6364 | 81.8182 | 88.6364 |'
- en: '| SURF | 65.9091 | 84.0909 | 68.1818 | 81.8182 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 65.9091 | 84.0909 | 68.1818 | 81.8182 |'
- en: '| BRISK | 47.7273 | 68.1818 | 47.7273 | 61.3636 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 47.7273 | 68.1818 | 47.7273 | 61.3636 |'
- en: '| KAZE | 47.7273 | 77.2727 | 72.7273 | 75 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 47.7273 | 77.2727 | 72.7273 | 75 |'
- en: '| DAISY | 77.2727 | 81.8182 | 72.7273 | 84.0909 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 77.2727 | 81.8182 | 72.7273 | 84.0909 |'
- en: '| DENSE-SIFT | 38.6364 | 45.4545 | 36.3636 | 43.1818 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 38.6364 | 45.4545 | 36.3636 | 43.1818 |'
- en: 'Table 6: The accuracy (%) of the system with 3,000 clusters'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：具有3,000个簇的系统准确率（%）
- en: '| K = 3000 | MLP | SVM | KNN | Normal Bayes |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| K = 3000 | MLP | SVM | KNN | 正常贝叶斯 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SIFT | 52.2727 | 88.6364 | 77.2727 | 86.3636 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| SIFT | 52.2727 | 88.6364 | 77.2727 | 86.3636 |'
- en: '| SURF | 59.0909 | 79.5455 | 65.9091 | 77.2727 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| SURF | 59.0909 | 79.5455 | 65.9091 | 77.2727 |'
- en: '| BRISK | 52.2727 | 65.9091 | 43.1818 | 59.0909 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| BRISK | 52.2727 | 65.9091 | 43.1818 | 59.0909 |'
- en: '| KAZE | 61.3636 | 81.8182 | 70.4545 | 84.0909 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| KAZE | 61.3636 | 81.8182 | 70.4545 | 84.0909 |'
- en: '| DAISY | 72.7273 | 79.5455 | 70.4545 | 68.1818 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| DAISY | 72.7273 | 79.5455 | 70.4545 | 68.1818 |'
- en: '| DENSE-SIFT | 27.2727 | 47.7273 | 38.6364 | 45.4545 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| DENSE-SIFT | 27.2727 | 47.7273 | 38.6364 | 45.4545 |'
- en: Evaluation with different learning algorithms
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用不同学习算法的评估
- en: We can create graphs with the above results to compare the performance between
    features and learning algorithms in the following figure. We can see that SVM
    and Normal Bayes have better results than the others in most cases. The best result
    is 93.1818% for SVM and SIFT in 1,000 clusters. MLP has the lowest result in almost
    every case. One reason is that MLP requires lots of data to prevent over fitting.
    We only have around 160 training images. However, the feature size for each sample
    is between 100 and 150\. Even with two hidden neurons, the number of weights is
    larger than the number of samples. KNN seems to work better than MLP but can't
    beat SVM and Normal Bayes.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用上述结果创建图表，比较以下图中特征与学习算法之间的性能。我们可以看到，在大多数情况下，SVM和正常贝叶斯比其他算法有更好的结果。在1,000个簇的情况下，SVM和SIFT的最佳结果是93.1818%。MLP在几乎所有情况下都有最低的结果。一个原因是MLP需要大量数据以防止过拟合。我们只有大约160个训练图像。然而，每个样本的特征大小在100到150之间。即使有两个隐藏神经元，权重的数量也大于样本的数量。KNN似乎比MLP表现更好，但无法击败SVM和正常贝叶斯。
- en: '![Evaluation with different learning algorithms](img/00049.jpeg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![使用不同学习算法的评估](img/00049.jpeg)'
- en: Relationship between the performance of features and machine algorithms under
    different numbers of clusters
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 不同簇数量下特征性能与机器算法之间的关系
- en: '![Evaluation with different learning algorithms](img/00050.jpeg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![使用不同学习算法的评估](img/00050.jpeg)'
- en: Effect of the number of centroids on the performance of features according to
    different machine algorithms
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 不同机器算法下特征性能与聚类中心数量之间的关系
- en: Evaluation with different features
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用不同特征的评估
- en: In the figure, *Relationship between the performance of features and machine
    algorithms under different numbers of clusters*, we have evaluated six features.
    SIFT gives the best results in most cases. DAISY is comparable to SIFT. In some
    cases, KAZE also gives good results. DENSE-SIFT is not a good choice for our facial
    expression problem since the results are poor. Moreover, the computation cost
    for DENSE features is really high. In conclusion, SIFT is still the most stable
    choice. However, SIFT is under patent. You may want to look at DAISY or KAZE.
    We recommend you do the evaluation on your data and choose the most suitable feature.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，*不同簇数量下特征性能与机器算法之间的关系*，我们评估了六个特征。在大多数情况下，SIFT给出了最佳结果。DAISY与SIFT相当。在某些情况下，KAZE也给出了良好的结果。由于结果较差，DENSE-SIFT不是我们面部表情问题的好选择。此外，DENSE特征的计算成本非常高。总之，SIFT仍然是最佳选择。然而，SIFT受专利保护。您可能想看看DAISY或KAZE。我们建议您在自己的数据上评估并选择最合适的特征。
- en: Evaluation with a different number of clusters
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用不同簇数量的评估
- en: In the figure, *Effect of the number of centroids on the performance of features
    according to different machine algorithms*, we made a graph to visualize the effects
    of the number of clusters on performance. As you can see, the number of clusters
    differs between features. In SIFT, KAZE, and BRISK, the best number of clusters
    is 1,000\. However, in SURF, DAISY, and DENSE-SIFT, 1,500 is a better choice.
    Basically, we don't want the number of clusters to be too large. The computation
    cost in kmeans increases with a larger number of clusters, especially in DENSE-SIFT.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，“不同机器算法下特征性能与聚类数量的影响”，我们绘制了一个图表来可视化聚类数量对性能的影响。如您所见，不同特征之间的聚类数量不同。在 SIFT、KAZE
    和 BRISK 中，最佳聚类数量是 1,000。然而，在 SURF、DAISY 和 DENSE-SIFT 中，1,500 是更好的选择。基本上，我们不希望聚类数量太大。在
    kmeans 中的计算成本随着聚类数量的增加而增加，尤其是在 DENSE-SIFT 中。
- en: System overview
  id: totrans-451
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统概述
- en: In this section, we will explain the process to apply the trained model in your
    application. Given a face image, we detect and process each face separately. Then,
    we find landmarks and extract the face region. The image features are extracted
    and passed to kmeans to obtain a 1,000-dimensional feature vector. PCA is applied
    to reduce the dimension of this feature vector. The learned machine learning model
    is used to predict the expression of the input face.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何在您的应用程序中应用训练好的模型。给定一张人脸图像，我们分别检测和处理每个面部。然后，我们找到特征点并提取面部区域。从图像中提取特征并传递给
    kmeans 以获得一个 1,000 维的特征向量。对该特征向量应用 PCA 以降低其维度。使用学习到的机器学习模型来预测输入面部表情。
- en: 'The following figure shows the complete process to predict the facial expression
    of a face in an image:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了预测图像中面部表情的完整过程：
- en: '![System overview](img/00051.jpeg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![系统概述](img/00051.jpeg)'
- en: The process to predict a facial expression in a new image
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 预测新图像中面部表情的过程
- en: Further reading
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: We have introduced a basic system for facial expression. If you are really interested
    in this topic, you may want to read this section for more guidance on how to improve
    the performance of the system. In this section, we will introduce you to compiling
    the `opencv_contrib` module, the Kaggle facial expression dataset, and the k-cross
    validation approach. We will also give you some suggestions on how to get better
    feature extraction.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一个基本的面部表情系统。如果您对这个主题真正感兴趣，您可能想阅读本节以获取更多关于如何提高系统性能的指导。在本节中，我们将向您介绍编译 `opencv_contrib`
    模块、Kaggle 面部表情数据集和 k-交叉验证方法。我们还将提供一些关于如何获得更好的特征提取的建议。
- en: Compiling the opencv_contrib module
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译 opencv_contrib 模块
- en: In this section, we will introduce the process for compiling `opencv_contrib`
    in Linux-based systems. If you use Windows, you can use the Cmake GUI with the
    same options.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍在基于 Linux 的系统中编译 `opencv_contrib` 的过程。如果您使用 Windows，可以使用具有相同选项的 Cmake
    GUI。
- en: 'First, clone the `opencv` repository to your local machine:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将 `opencv` 仓库克隆到您的本地机器上：
- en: '[PRE64]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Second, clone the `opencv_contrib` repository to your local machine:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，将 `opencv_contrib` 仓库克隆到您的本地机器上：
- en: '[PRE65]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Change directory to the `opencv` folder and make a build directory:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到 `opencv` 文件夹并创建一个构建目录：
- en: '[PRE66]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Build OpenCV from source with opencv_contrib support. You should change `OPENCV_EXTRA_MODULES_PATH`
    to the location of `opencv_contrib` on your machine:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 opencv_contrib 支持从源代码构建 OpenCV。您应将 `OPENCV_EXTRA_MODULES_PATH` 修改为您的机器上 `opencv_contrib`
    的位置：
- en: '[PRE67]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Kaggle facial expression dataset
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 面部表情数据集
- en: Kaggle is a great community of data scientists. There are many competitions
    hosted by Kaggle. In 2013, there was a facial expression recognition challenge.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 是一个优秀的数据科学家社区。Kaggle 主办了许多比赛。2013 年，有一个面部表情识别挑战。
- en: Note
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'At the moment, you can go to the following link to access the full dataset:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，您可以通过以下链接访问完整数据集：
- en: '[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/)'
- en: The dataset consists of 48x48 pixel grayscale images of faces. There are 28,709
    training samples, 3,589 public test images and 3,589 images for final test. The
    dataset contains seven expressions (Anger, Disgust, Fear, Happiness, Sadness,
    Surprise and Neutral). The winner achieved a score of 69.769 %. This dataset is
    huge so we think that our basic system may not work out of the box. We believe
    that you should try to improve the performance of the system if you want to use
    this dataset.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含48x48像素的灰度人脸图像。共有28,709个训练样本，3,589个公开测试图像和3,589个最终测试图像。数据集包含七种表情（愤怒、厌恶、恐惧、快乐、悲伤、惊讶和中性）。获胜者获得了69.769
    %的分数。由于这个数据集非常大，所以我们认为我们的基本系统可能无法直接使用。我们相信，如果您想使用这个数据集，您应该尝试提高系统的性能。
- en: Facial landmarks
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部特征点
- en: In our facial expression system, we use face detection as a pre-processing step
    to extract the face region. However, face detection is prone to misalignment,
    hence, feature extraction may not be reliable. In recent years, one of the most
    common approaches has been the usage of facial landmarks. In this kind of method,
    the facial landmarks are detected and used to align the face region. Many researchers
    use facial landmarks to extract the facial components such as the eyes, mouth,
    and so on, and do feature extractions separately.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的面部表情系统中，我们使用人脸检测作为预处理步骤来提取面部区域。然而，人脸检测容易发生错位，因此特征提取可能不可靠。近年来，最常见的方法之一是使用面部特征点。在这种方法中，检测面部特征点并用于对齐面部区域。许多研究人员使用面部特征点来提取面部组件，如眼睛、嘴巴等，并分别进行特征提取。
- en: What are facial landmarks?
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是面部特征点？
- en: Facial landmarks are predefined locations of facial components. The figure below
    shows an example of a 68 points system from the iBUG group ([http://ibug.doc.ic.ac.uk/resources/facial-point-annotations](http://ibug.doc.ic.ac.uk/resources/facial-point-annotations))
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 面部特征点是面部组件的预定义位置。下面的图显示了iBUG组的一个68点系统的示例 ([http://ibug.doc.ic.ac.uk/resources/facial-point-annotations](http://ibug.doc.ic.ac.uk/resources/facial-point-annotations))
- en: '![What are facial landmarks?](img/00052.jpeg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![什么是面部特征点？](img/00052.jpeg)'
- en: An example of a 68 landmarks points system from the iBUG group
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: iBUG组的一个68个特征点系统的示例
- en: How do you detect facial landmarks?
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你如何检测面部特征点？
- en: There are several ways to detect facial landmarks in a face region. We will
    give you a few solutions so that you can start your project easily
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在面部区域内检测面部特征点有几种方法。我们将为您提供一些解决方案，以便您能够轻松开始您的项目。
- en: '**Active Shape Model**: This is one of the most common approaches to this problem.
    You may find the following library useful:'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动形状模型**：这是解决此问题最常见的方法之一。您可能会发现以下库很有用：'
- en: 'Stasm: [https://github.com/cxcxcxcx/asmlib-opencv](https://github.com/cxcxcxcx/asmlib-opencv)'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Stasm: [https://github.com/cxcxcxcx/asmlib-opencv](https://github.com/cxcxcxcx/asmlib-opencv)'
- en: '**Face Alignment by Explicit Regression by Cao et al**: This is one of the
    latest works on facial landmarks. This system is very efficient and highly accurate.
    You can find an open source implementation at the following hyperlink: [https://github.com/soundsilence/FaceAlignment](https://github.com/soundsilence/FaceAlignment)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cao等人通过显式回归进行人脸对齐**：这是关于面部特征点的最新工作之一。这个系统非常高效且非常准确。您可以在以下超链接中找到开源实现：[https://github.com/soundsilence/FaceAlignment](https://github.com/soundsilence/FaceAlignment)'
- en: How do you use facial landmarks?
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你如何使用面部特征点？
- en: 'You can use facial landmarks in many ways. We will give you some guides:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用面部特征点以多种方式。我们将为您提供一些指南：
- en: You can use facial landmarks to align the face region to a common standard and
    extract the features vectors as in our basic facial expression system.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用面部特征点将面部区域对齐到共同的标准，并提取特征向量，就像我们在基本面部表情系统中做的那样。
- en: You can extract features vectors in different facial components such as eyes
    and mouths separately and combine everything in one feature vector for classification.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在不同的面部组件（如眼睛和嘴巴）中分别提取特征向量，并将它们组合成一个特征向量进行分类。
- en: You can use the location of facial landmarks as a feature vector and ignore
    the texture in the image.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用面部特征点的位置作为特征向量，并忽略图像中的纹理。
- en: You can build classification models for each facial component and combine the
    prediction in a weighted manner.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以为每个面部组件构建分类模型，并以加权方式组合预测结果。
- en: Improving feature extraction
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高特征提取
- en: 'Feature extraction is one of the most important parts of facial expression.
    It is better to choose the right feature for your problem. In our implementation,
    we have only used a few features in OpenCV. We recommend that you try every possible
    feature in OpenCV. Here is the list of supported features in Open CV: BRIEF, BRISK,
    FREAK, ORB, SIFT, SURF, KAZE, AKAZE, FAST, MSER, and STAR.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是面部表情分析中最重要的部分之一。最好为你的问题选择合适的特征。在我们的实现中，我们只在OpenCV中使用了少数几个特征。我们建议你尝试OpenCV中所有可能的特征。以下是Open
    CV支持的特性列表：BRIEF, BRISK, FREAK, ORB, SIFT, SURF, KAZE, AKAZE, FAST, MSER, 和 STAR。
- en: There are other great features in the community that might be suitable for your
    problem, such as LBP, Gabor, HOG, and so on.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 社区中还有其他一些非常适合你问题的优秀特性，例如LBP, Gabor, HOG等。
- en: K-fold cross validation
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: K-fold cross validation is a common technique for estimating the performance
    of a classifier. Given a training set, we will divide it into k partitions. For
    each fold i of k experiments, we will train the classifier using all the samples
    that do not belong to fold i and use the samples in fold i to test the classifier.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证是估计分类器性能的常用技术。给定一个训练集，我们将将其划分为k个分区。对于k次实验中的每个折i，我们将使用不属于折i的所有样本来训练分类器，并使用折i中的样本来测试分类器。
- en: The advantage of k-fold cross validation is that all the examples in the dataset
    are eventually used for training and validation.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: K折交叉验证的优势在于数据集中的所有示例最终都会用于训练和验证。
- en: It is important to divide the original dataset into the training set and the
    testing set. Then, the training set will be used for k-fold cross validation and
    the testing set will be used for the final test.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据集划分为训练集和测试集是很重要的。然后，训练集将用于k折交叉验证，而测试集将用于最终测试。
- en: Cross validation combines the prediction error of each experiment and derives
    a more accurate estimate of the model. It is very useful, especially in cases
    where we don't have much data for training. Despite a high computation time, using
    a complex feature is a great idea if you want to improve the overall performance
    of the system.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证结合了每个实验的预测误差，从而得到模型更准确的估计。它非常有用，尤其是在我们训练数据不多的情况下。尽管计算时间较长，但如果你想提高系统的整体性能，使用复杂特征是一个很好的主意。
- en: Summary
  id: totrans-499
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter showed the complete process of a facial expression system in OpenCV
    3\. We went through each step of the system and gave you a lot of alternative
    solutions for each step. This chapter also made an evaluation of the results based
    on features and learning algorithms.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了OpenCV 3中面部表情系统的完整过程。我们走过了系统的每一步，并为每一步提供了许多替代方案。本章还基于特性和学习算法对结果进行了评估。
- en: Finally, this chapter gave you a few hints for further improvement including
    a great facial expression challenge, a facial landmarks approach, some features
    suggestions, and k-fold cross validation.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章为你提供了一些进一步改进的提示，包括一个面部表情挑战，面部特征点方法，一些特性建议，以及k折交叉验证。
