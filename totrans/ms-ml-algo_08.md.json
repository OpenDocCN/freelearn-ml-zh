["```py\nimport numpy as np\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nX, Y = load_wine(return_X_y=True)\n\nlr = LogisticRegression(max_iter=1000, random_state=1000)\nprint(np.mean(cross_val_score(lr, X, Y, cv=10)))\n0.956432748538\n\ndt = DecisionTreeClassifier(criterion='entropy', random_state=1000)\nprint(np.mean(cross_val_score(dt, X, Y, cv=10)))\n0.933298933609\n\nsvm = SVC(kernel='poly', random_state=1000)\nprint(np.mean(cross_val_score(svm, X, Y, cv=10)))\n0.961403508772\n```", "```py\nfrom multiprocessing import cpu_count\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50, n_jobs=cpu_count(), random_state=1000)\nprint(np.mean(cross_val_score(rf, X, Y, cv=10)))\n0.983333333333\n```", "```py\nfrom sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(estimator=rf, prefit=True, threshold=0.02)\nX_sfm = sfm.transform(X)\n\nprint(X_sfm.shape)\n(178, 10)\n```", "```py\nimport numpy as np\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score \n\nscores_ne = []\n\nfor ne in range(10, 201, 10):\n    adc = AdaBoostClassifier(n_estimators=ne, learning_rate=0.8, random_state=1000)\n    scores_ne.append(np.mean(cross_val_score(adc, X, Y, cv=10)))\n```", "```py\nimport numpy as np\n\nscores_eta_adc = []\n\nfor eta in np.linspace(0.01, 1.0, 100):\n    adc = AdaBoostClassifier(n_estimators=50, learning_rate=eta, random_state=1000)\n    scores_eta_adc.append(np.mean(cross_val_score(adc, X, Y, cv=10)))\n```", "```py\nimport numpy as np\n\nfrom sklearn.decomposition import PCA, FactorAnalysis\n\nscores_pca = []\n\nfor i in range(13, 1, -1):\n    if i < 12:\n        pca = PCA(n_components=i, random_state=1000)\n        X_pca = pca.fit_transform(X)\n    else:\n        X_pca = X\n\n    adc = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1000)\n    scores_pca.append(np.mean(cross_val_score(adc, X_pca, Y, cv=10)))  \n\nscores_fa = []\n\nfor i in range(13, 1, -1):\n    if i < 12:\n        fa = FactorAnalysis(n_components=i, random_state=1000)\n        X_fa = fa.fit_transform(X)\n    else:\n        X_fa = X\n\n    adc = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1000)\n    scores_fa.append(np.mean(cross_val_score(adc, X_fa, Y, cv=10)))\n```", "```py\nimport numpy as np\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\nscores_md = []\neta = 0.8\n\nfor md in range(2, 13):\n    gbc = GradientBoostingClassifier(n_estimators=50, learning_rate=eta, max_depth=md, random_state=1000)\n    scores_md.append(np.mean(cross_val_score(gbc, X, Y, cv=10)))\n```", "```py\nimport numpy as np\n\nscores_eta = []\n\nfor eta in np.linspace(0.01, 1.0, 100):\n    gbr = GradientBoostingClassifier(n_estimators=50, learning_rate=eta, max_depth=2, random_state=1000)\n    scores_eta.append(np.mean(cross_val_score(gbr, X, Y, cv=10)))\n```", "```py\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\n\nX, Y = load_digits(return_X_y=True)\nX /= np.max(X)\n```", "```py\nimport numpy as np\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\ndt = DecisionTreeClassifier(criterion='entropy', random_state=1000)\nprint(np.mean(cross_val_score(dt, X, Y, cv=10)))\n0.830880960443\n\nlr = LogisticRegression(C=2.0, random_state=1000)\nprint(np.mean(cross_val_score(lr, X, Y, cv=10)))\n0.937021649942\n```", "```py\nimport numpy as np\n\nfrom sklearn.ensemble import VotingClassifier\n\nvc = VotingClassifier(estimators=[\n    ('LR', LogisticRegression(C=2.0, random_state=1000)),\n    ('DT', DecisionTreeClassifier(criterion='entropy', random_state=1000))], \n     voting='soft', weights=(0.9, 0.1))\n\nprint(np.mean(cross_val_score(vc, X, Y, cv=10)))\n0.944835154373\n```"]