<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Bayesian Classification Models" id="aid-1ENBI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Bayesian Classification Models</h1></div></div></div><p>We introduced <a id="id281" class="indexterm"/>the classification machine learning task in <a class="link" title="Chapter 4. Machine Learning Using Bayesian Inference" href="part0034.xhtml#aid-10DJ42">Chapter 4</a>, <span class="emphasis"><em>Machine Learning Using Bayesian Inference</em></span>, and said that the objective of classification is to assign a data record into one of the predetermined classes. Classification is one of the most studied machine learning tasks and there are several well-established state of the art methods for it. These include logistic regression models, support vector machines, random forest models, and neural network models. With sufficient labeled training data, these models can achieve accuracies above 95% in many practical problems.</p><p>Then, the obvious question is, why would you need to use Bayesian methods for classification? There are two answers to this question. One is that often it is difficult to get a large amount of labeled data for training. When there are hundreds or thousands of features in a given problem, one often needs a large amount of training data for these supervised methods to avoid overfitting. Bayesian methods can overcome this problem through Bayesian averaging and hence require only a small to medium size training data. Secondly, most of the methods, such as SVM or NN, are like black box machines. They will give you very accurate results, but little insight as to which variables are important for the example. Often, in many practical problems, for example, in the diagnosis of a disease, it is important to identify leading causes. Therefore, a black box approach would not be sufficient. Bayesian methods have an inherent feature called <span class="strong"><strong>Automatic Relevance Determination</strong></span> (<span class="strong"><strong>ARD</strong></span>)<a id="id282" class="indexterm"/> by which important variables in a problem can be identified.</p><p>In this chapter, two Bayesian classification models will be discussed. The first one is the popular Naïve Bayes method for text classification. The second is the Bayesian logistic regression model. Before we discuss each of these models, let's review some of the performance metrics that are commonly used in the classification task.</p><div class="section" title="Performance metrics for classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Performance metrics for classification</h1></div></div></div><p>To <a id="id283" class="indexterm"/>understand the concepts easily, let's take the case of binary classification, where the task is to classify an input feature vector into one of the two states: -1 or 1. Assume that 1 is the positive class and -1 is the negative class. The<a id="id284" class="indexterm"/> predicted output contains only -1 or 1, but there can be two types of errors. Some of the -1 in the test set could be predicted as 1. This is called<a id="id285" class="indexterm"/> a <span class="strong"><strong>false positive or type I</strong></span> error. Similarly, some of the 1 in the test set could be predicted as -1. This is called a <a id="id286" class="indexterm"/>
<span class="strong"><strong>false negative or type II</strong></span> error. These two types of errors can be represented in the case of binary classification as a confusion matrix as shown below.</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th rowspan="2" colspan="2" valign="bottom" style="text-align: center">
<p>Confusion Matrix</p>
</th><th colspan="2" valign="bottom" style="text-align: center">
<p>Predicted Class</p>
</th></tr><tr><th valign="bottom">
<p>Positive</p>
</th><th valign="bottom">
<p>Negative</p>
</th></tr></thead><tbody><tr><td rowspan="2" valign="top">
<p>Actual Class</p>
</td><td valign="top">
<p>Positive</p>
</td><td valign="top">
<p>TP</p>
</td><td valign="top">
<p>FN</p>
</td></tr><tr><td valign="top">
<p>Negative</p>
</td><td valign="top">
<p>FP</p>
</td><td valign="top">
<p>TN</p>
</td></tr></tbody></table></div><p>From the confusion matrix, we can derive the following performance metrics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Precision</strong></span>: <span class="inlinemediaobject"><img src="../Images/image00453.jpeg" alt="Performance metrics for classification"/></span> This gives the percentage of correct answers in the output predicted as positive
</li><li class="listitem"><span class="strong"><strong>Recall</strong></span>: <span class="inlinemediaobject"><img src="../Images/image00454.jpeg" alt="Performance metrics for classification"/></span> This gives the percentage of positives in the test data set that have been correctly predicted
</li><li class="listitem"><span class="strong"><strong>F-Score</strong></span>: <span class="inlinemediaobject"><img src="../Images/image00455.jpeg" alt="Performance metrics for classification"/></span> This is the geometric mean of precision and recall
</li><li class="listitem"><span class="strong"><strong>True positive rate</strong></span>: <span class="inlinemediaobject"><img src="../Images/image00456.jpeg" alt="Performance metrics for classification"/></span> This is the same as recall
</li><li class="listitem"><span class="strong"><strong>False positive rate</strong></span>: <span class="inlinemediaobject"><img src="../Images/image00457.jpeg" alt="Performance metrics for classification"/></span> This gives the percentage of negative classes classified as positive
</li></ul></div><p>Also, <span class="emphasis"><em>Tpr</em></span> is called <span class="emphasis"><em>sensitivity</em></span> and <span class="emphasis"><em>1 - Fpr</em></span> is called <span class="emphasis"><em>specificity</em></span> of the classifier. A plot of Tpr versus Fpr (<span class="emphasis"><em>sensitivity</em></span> versus <span class="emphasis"><em>1 - specificity</em></span>) is called an <a id="id287" class="indexterm"/>
<span class="strong"><strong>ROC</strong></span> curve (it stands for <span class="strong"><strong>receiver operating characteristic</strong></span> curve). This is used to find the best threshold (operating point of the classifier) for deciding whether a predicted output (usually a score or probability) belongs to class 1 or -1.</p><p>Usually, the threshold is taken as the inflation point of the ROC curve that gives the best performance with the least false predictions. The area under the ROC curve or AUC is another measure of classifier performance. For a purely random model, the ROC curve will be a straight line <a id="id288" class="indexterm"/>along the diagonal. The corresponding value of AUC will be 0.5. Classifiers with AUC above 0.8 will be considered as good, though this very much depends on the problem to be solved.</p></div></div>
<div class="section" title="The Na&#xEF;ve Bayes classifier"><div class="titlepage" id="aid-1FLS42"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>The Naïve Bayes classifier</h1></div></div></div><p>The name <a id="id289" class="indexterm"/>Naïve Bayes comes from the basic assumption in the model that the probability of a particular feature <span class="inlinemediaobject"><img src="../Images/image00458.jpeg" alt="The Naïve Bayes classifier"/></span> is independent of any other feature <span class="inlinemediaobject"><img src="../Images/image00459.jpeg" alt="The Naïve Bayes classifier"/></span> given the class label <span class="inlinemediaobject"><img src="../Images/image00460.jpeg" alt="The Naïve Bayes classifier"/></span>. This implies the following:</p><div class="mediaobject"><img src="../Images/image00461.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>Using this assumption and the Bayes rule, one can show that the probability of class <span class="inlinemediaobject"><img src="../Images/image00460.jpeg" alt="The Naïve Bayes classifier"/></span>, given features <span class="inlinemediaobject"><img src="../Images/image00462.jpeg" alt="The Naïve Bayes classifier"/></span>, is given by:</p><div class="mediaobject"><img src="../Images/image00463.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00464.jpeg" alt="The Naïve Bayes classifier"/></span> is the normalization term obtained by summing the numerator on all the values of <span class="emphasis"><em>k</em></span>. It is also called Bayesian evidence or partition function Z. The classifier selects a class label as the target class that maximizes the posterior class probability <span class="inlinemediaobject"><img src="../Images/image00465.jpeg" alt="The Naïve Bayes classifier"/></span>:</p><div class="mediaobject"><img src="../Images/image00466.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>The Naïve Bayes classifier <a id="id290" class="indexterm"/>is a baseline classifier for document classification. One reason for this is that the underlying assumption that each feature (words or m-grams) is independent of others, given the class label typically holds good for text. Another reason is that the Naïve Bayes classifier scales well when there is a large number of documents.</p><p>There are two implementations of Naïve Bayes. In Bernoulli Naïve Bayes, features are binary variables that encode whether a feature (m-gram) is present or absent in a document. In multinomial Naïve Bayes, the features are frequencies of m-grams in a document. To avoid issues when the frequency is zero, a Laplace smoothing is done on the feature vectors by adding a 1 to each count. Let's look at multinomial Naïve Bayes in some detail.</p><p>Let <span class="inlinemediaobject"><img src="../Images/image00467.jpeg" alt="The Naïve Bayes classifier"/></span> be the number of times the feature <span class="inlinemediaobject"><img src="../Images/image00458.jpeg" alt="The Naïve Bayes classifier"/></span> occurred in the class <span class="inlinemediaobject"><img src="../Images/image00468.jpeg" alt="The Naïve Bayes classifier"/></span> in the training data. Then, the likelihood function of observing a feature vector <span class="inlinemediaobject"><img src="../Images/image00469.jpeg" alt="The Naïve Bayes classifier"/></span>, given a class label <span class="inlinemediaobject"><img src="../Images/image00468.jpeg" alt="The Naïve Bayes classifier"/></span>, is given by:</p><div class="mediaobject"><img src="../Images/image00470.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00471.jpeg" alt="The Naïve Bayes classifier"/></span> is the <a id="id291" class="indexterm"/>probability of observing the feature <span class="inlinemediaobject"><img src="../Images/image00458.jpeg" alt="The Naïve Bayes classifier"/></span> in the class <span class="inlinemediaobject"><img src="../Images/image00468.jpeg" alt="The Naïve Bayes classifier"/></span>.</p><p>Using Bayesian rule, the posterior probability of observing the class <span class="inlinemediaobject"><img src="../Images/image00468.jpeg" alt="The Naïve Bayes classifier"/></span>, given a feature vector <span class="emphasis"><em>X</em></span>, is given by:</p><div class="mediaobject"><img src="../Images/image00472.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>Taking logarithm on both the sides and ignoring the constant term <span class="emphasis"><em>Z</em></span>, we get the following:</p><div class="mediaobject"><img src="../Images/image00473.jpeg" alt="The Naïve Bayes classifier"/></div><p style="clear:both; height: 1em;"> </p><p>So, by taking logarithm of posterior distribution, we have converted the problem into a linear regression model with <span class="inlinemediaobject"><img src="../Images/image00474.jpeg" alt="The Naïve Bayes classifier"/></span> as the coefficients to be determined from data. This can be easily solved. Generally, instead of term frequencies, one uses TF-IDF (term frequency multiplied by inverse frequency) with the document length normalized to improve the performance of the model.</p><p>The <a id="id292" class="indexterm"/>R package <span class="strong"><strong>e1071</strong></span> (<span class="emphasis"><em>Miscellaneous Functions of the Department of Statistics</em></span>) by T.U. Wien contains an R implementation of Naïve Bayes. For this chapter, we will use the SMS spam dataset from the UCI Machine Learning repository (reference 1 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset consists of 425 SMS spam messages collected from the UK forum Grumbletext, where consumers can submit spam SMS messages. The dataset also contains 3375 normal (ham) SMS messages from the NUS SMS corpus maintained by the National University of Singapore.</p><p>The dataset can be downloaded <a id="id293" class="indexterm"/>from the UCI Machine Learning repository (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a>). Let's say that we have saved this as file <code class="literal">SMSSpamCollection.txt</code> in the working directory of R (actually, you need to open it in Excel and save it is as tab-delimited file for it to read in R properly).</p><p>Then, the command to read the file into the tm (text mining) package would be the following:</p><div class="informalexample"><pre class="programlisting">&gt;spamdata &lt;-read.table("SMSSpamCollection.txt",sep="\t",stringsAsFactors = default.stringsAsFactors())</pre></div><p>We will first separate the dependent variable <code class="literal">y</code> and independent variables <code class="literal">x</code> and split the dataset into training and testing sets in the ratio 80:20, using the following R commands:</p><div class="informalexample"><pre class="programlisting">&gt;samp&lt;-sample.int(nrow(spamdata),as.integer(nrow(spamdata)*0.2),replace=F)
&gt;spamTest &lt;-spamdata[samp,]
&gt;spamTrain &lt;-spamdata[-samp,]
&gt;ytrain&lt;-as.factor(spamTrain[,1])
&gt;ytest&lt;-as.factor(spamTest[,1])
&gt;xtrain&lt;-as.vector(spamTrain[,2])
&gt;xtest&lt;-as.vector(spamTest[,2])</pre></div><p>Since we are dealing with text documents, we need to do some standard preprocessing before we can use the data for any machine learning models. We can use the tm package in R for this purpose. In the next section, we will describe this in some detail.</p><div class="section" title="Text processing using the tm package"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec35"/>Text processing using the tm package</h2></div></div></div><p>The <a id="id294" class="indexterm"/>
<span class="strong"><strong>tm</strong></span> package <a id="id295" class="indexterm"/>has methods for data import, corpus handling, preprocessing, metadata management, and creation of term-document matrices. Data can be imported into the tm package either from a directory, a vector with each component a document, or a data frame. The fundamental data structure in tm is an abstract collection of text documents called Corpus. It has two implementations; one is where data is stored in memory and is called <a id="id296" class="indexterm"/>
<span class="strong"><strong>VCorpus</strong></span> (<span class="strong"><strong>volatile corpus</strong></span>) and the second is where data is stored in the hard disk and is called <a id="id297" class="indexterm"/>
<span class="strong"><strong>PCorpus</strong></span> (<span class="strong"><strong>permanent corpus</strong></span>).</p><p>We can create a corpus of our SMS spam dataset by using the following R commands; prior to this, you need to install the tm package and <a id="id298" class="indexterm"/>
<span class="strong"><strong>SnowballC</strong></span> package by using the <code class="literal">install.packages("packagename")</code> command in R:</p><div class="informalexample"><pre class="programlisting">&gt;library(tm)
&gt;library(SnowballC)
&gt;xtrain &lt;- VCorpus(VectorSource(xtrain))</pre></div><p>First, we <a id="id299" class="indexterm"/>need to do some basic text processing, such as removing extra white space, changing all words to lowercase, removing stop words, and stemming the words. This can be achieved by using the following functions in the tm package:</p><div class="informalexample"><pre class="programlisting">&gt;#remove extra white space
&gt;xtrain &lt;- tm_map(xtrain,stripWhitespace)
&gt;#remove punctuation
&gt;xtrain &lt;- tm_map(xtrain,removePunctuation)
&gt;#remove numbers
&gt;xtrain &lt;- tm_map(xtrain,removeNumbers)
&gt;#changing to lower case
&gt;xtrain &lt;- tm_map(xtrain,content_transformer(tolower))
&gt;#removing stop words
&gt;xtrain &lt;- tm_map(xtrain,removeWords,stopwords("english"))
&gt;#stemming the document
&gt;xtrain &lt;- tm_map(xtrain,stemDocument)</pre></div><p>Finally, the data is transformed into a form that can be consumed by machine learning models. This is the so called document-term matrix form where each document (SMS in this case) is a row, the terms appearing in all documents are the columns, and the entry in each cell denotes how many times each word occurs in one document:</p><div class="informalexample"><pre class="programlisting">&gt;#creating Document-Term Matrix
&gt;xtrain &lt;- as.data.frame.matrix(DocumentTermMatrix(xtrain))</pre></div><p>The same set of processes is done on the <code class="literal">xtest</code> dataset as well. The reason we converted <span class="emphasis"><em>y</em></span> to factors and <span class="emphasis"><em>xtrain</em></span> to a data frame is to match the input format for the Naïve Bayes classifier in the e1071 package.</p></div><div class="section" title="Model training and prediction"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Model training and prediction</h2></div></div></div><p>You need <a id="id300" class="indexterm"/>to first install the e1071 package from CRAN. The <code class="literal">naiveBayes()</code> function can be used to train the Naïve Bayes model. The function can be called using two methods. The following is the first method:</p><div class="informalexample"><pre class="programlisting">&gt;naiveBayes(formula,data,laplace=0,…,subset,na.action=na.pass)</pre></div><p>Here <code class="literal">formula</code> stands for the linear combination of independent variables to predict the following class:</p><div class="informalexample"><pre class="programlisting">&gt;class ~ x1+x2+…   </pre></div><p>Also, <code class="literal">data</code> stands for either a data frame or contingency table consisting of categorical and numerical variables.</p><p>If we have the class labels as a vector <span class="emphasis"><em>y</em></span> and dependent variables as a data frame <span class="emphasis"><em>x</em></span>, then we can use the second method of calling the function, as follows:</p><div class="informalexample"><pre class="programlisting">&gt;naiveBayes(x,y,laplace=0,…)</pre></div><p>We will use the second method of calling in our example. Once we have a trained model, which is an R object of class <code class="literal">naiveBayes</code>, we can predict the classes of new instances as follows:</p><div class="informalexample"><pre class="programlisting">&gt;predict(object,newdata,type=c(class,raw),threshold=0.001,eps=0,…)</pre></div><p>So, we can train the Naïve Bayes model on our training dataset and score on the test dataset by using the following commands:</p><div class="informalexample"><pre class="programlisting">&gt;#Training the Naive Bayes Model
&gt;nbmodel &lt;- naiveBayes(xtrain,ytrain,laplace=3)
&gt;#Prediction using trained model
&gt;ypred.nb &lt;- predict(nbmodel,xtest,type = "class",threshold = 0.075)
&gt;#Converting classes to 0 and 1 for plotting ROC
&gt;fconvert &lt;- function(x){
     if(x == "spam"){ y &lt;- 1}
   else {y &lt;- 0}
   y
}

&gt;ytest1 &lt;- sapply(ytest,fconvert,simplify = "array")
&gt;ypred1 &lt;- sapply(ypred.nb,fconvert,simplify = "array")
&gt;roc(ytest1,ypred1,plot = T)</pre></div><p>Here, the ROC curve for this model and dataset is shown. This is generated using the pROC package in CRAN:</p><div class="mediaobject"><img src="../Images/image00475.jpeg" alt="Model training and prediction"/></div><p style="clear:both; height: 1em;"> </p><div class="informalexample"><pre class="programlisting">&gt;#Confusion matrix
&gt;confmat &lt;- table(ytest,ypred.nb)
&gt;confmat
pred.nb
ytest  ham spam
  ham  143  139
  spam   9   35</pre></div><p>From the <a id="id301" class="indexterm"/>ROC curve and confusion matrix, one can choose the best threshold for the classifier, and the precision and recall metrics. Note that the example shown here is for illustration purposes only. The model needs be to tuned further to improve accuracy.</p><p>We can also print some of the most frequent words (model features) occurring in the two classes and their posterior probabilities generated by the model. This will give a more intuitive feeling for the model exercise. The following R code does this job:</p><div class="informalexample"><pre class="programlisting">&gt;tab &lt;- nbmodel$tables
&gt;fham &lt;- function(x){
  y &lt;- x[1,1]
  y
}
&gt;hamvec &lt;- sapply(tab,fham,simplify = "array")
&gt;hamvec &lt;- sort(hamvec,decreasing = T)

&gt;fspam &lt;- function(x){
  y &lt;- x[2,1]
  y
}
&gt;spamvec &lt;- sapply(tab,fspam,simplify = "array")
&gt;spamvec &lt;- sort(spamvec,decreasing = T)
&gt;prb &lt;- cbind(spamvec,hamvec)
&gt;print.table(prb)</pre></div><p>The output table is as follows:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>word</p>
</th><th valign="bottom">
<p>Prob(word|spam)</p>
</th><th valign="bottom">
<p>Prob(word|ham)</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>call</p>
</td><td valign="top">
<p>0.6994</p>
</td><td valign="top">
<p>0.4084</p>
</td></tr><tr><td valign="top">
<p>free</p>
</td><td valign="top">
<p>0.4294</p>
</td><td valign="top">
<p>0.3996</p>
</td></tr><tr><td valign="top">
<p>now</p>
</td><td valign="top">
<p>0.3865</p>
</td><td valign="top">
<p>0.3120</p>
</td></tr><tr><td valign="top">
<p>repli</p>
</td><td valign="top">
<p>0.2761</p>
</td><td valign="top">
<p>0.3094</p>
</td></tr><tr><td valign="top">
<p>text</p>
</td><td valign="top">
<p>0.2638</p>
</td><td valign="top">
<p>0.2840</p>
</td></tr><tr><td valign="top">
<p>spam</p>
</td><td valign="top">
<p>0.2270</p>
</td><td valign="top">
<p>0.2726</p>
</td></tr><tr><td valign="top">
<p>txt</p>
</td><td valign="top">
<p>0.2270</p>
</td><td valign="top">
<p>0.2594</p>
</td></tr><tr><td valign="top">
<p>get</p>
</td><td valign="top">
<p>0.2209</p>
</td><td valign="top">
<p>0.2182</p>
</td></tr><tr><td valign="top">
<p>stop</p>
</td><td valign="top">
<p>0.2086</p>
</td><td valign="top">
<p>0.2025</p>
</td></tr></tbody></table></div><p>The table shows, for <a id="id302" class="indexterm"/>example, that given a document is spam, the probability of the word <span class="emphasis"><em>call</em></span> appearing in it is 0.6994, whereas the probability of the same word appearing in a normal document is only 0.4084.</p></div></div>
<div class="section" title="The Bayesian logistic regression model"><div class="titlepage" id="aid-1GKCM2"><div><div><h1 class="title"><a id="ch06lvl1sec45"/>The Bayesian logistic regression model</h1></div></div></div><p>The<a id="id303" class="indexterm"/> name logistic regression comes from the fact that the dependent variable of the regression is a logistic function. It is one of the widely used models in problems where the response is a binary variable (for example, fraud or not-fraud, click or no-click, and so on). </p><p>A logistic function is defined by the following equation:</p><div class="mediaobject"><img src="../Images/image00476.jpeg" alt="The Bayesian logistic regression model"/></div><p style="clear:both; height: 1em;"> </p><p>It has the particular feature that, as <span class="emphasis"><em>y</em></span> varies from <span class="inlinemediaobject"><img src="../Images/image00477.jpeg" alt="The Bayesian logistic regression model"/></span> to <span class="inlinemediaobject"><img src="../Images/image00478.jpeg" alt="The Bayesian logistic regression model"/></span>, the function value varies from 0 to 1. Hence, the logistic function is ideal for modeling any binary response as the input signal is varied.</p><p>The inverse of the logistic function is called <span class="emphasis"><em>logit</em></span>. It is defined as follows:</p><div class="mediaobject"><img src="../Images/image00479.jpeg" alt="The Bayesian logistic regression model"/></div><p style="clear:both; height: 1em;"> </p><p>In logistic regression, <span class="emphasis"><em>y</em></span> is treated as a linear function of explanatory variables <span class="emphasis"><em>X</em></span>. Therefore, the logistic regression model can be defined as follows:</p><div class="mediaobject"><img src="../Images/image00480.jpeg" alt="The Bayesian logistic regression model"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00481.jpeg" alt="The Bayesian logistic regression model"/></span> is the set of basis functions and <span class="inlinemediaobject"><img src="../Images/image00482.jpeg" alt="The Bayesian logistic regression model"/></span> are the model parameters as explained in the case of linear regression in <a class="link" title="Chapter 4. Machine Learning Using Bayesian Inference" href="part0034.xhtml#aid-10DJ42">Chapter 4</a>, <span class="emphasis"><em>Machine Learning Using Bayesian Inference</em></span>. From the definition of GLM in <a class="link" title="Chapter 5. Bayesian Regression Models" href="part0041.xhtml#aid-173721">Chapter 5</a>, <span class="emphasis"><em>Bayesian Regression Models</em></span>, one can immediately recognize that logistic regression is a special case of GLM with the <a id="id304" class="indexterm"/>
<span class="strong"><strong>logit</strong></span> function as the link function.</p><p>Bayesian treatment <a id="id305" class="indexterm"/>of logistic regression is more difficult compared to the case of linear regression. Here, the likelihood function consists of a product of logistic functions; one for each data point. To compute the posterior, one has to normalize this function multiplied by the prior (to get the denominator of the Bayes formula). One approach is to use Laplace approximation as explained in <a class="link" title="Chapter 3. Introducing Bayesian Inference" href="part0030.xhtml#aid-SJGS2">Chapter 3</a>, <span class="emphasis"><em>Introducing Bayesian Inference</em></span>. Readers might recall that in Laplace approximation, the posterior is approximated as a Gaussian (normal) distribution about the maximum of the posterior. This is achieved by finding the <span class="strong"><strong>maximum a posteriori</strong></span> (<span class="strong"><strong>MAP</strong></span>) solution first and computing the second derivative of the negative log likelihood around the MAP solution. Interested readers can find the details of Laplace approximation to logistic regression in the paper by D.J.C. MacKay (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>Instead of using an analytical approximation, Polson and Scott recently proposed a fully Bayesian treatment of this problem using a data augmentation strategy (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). The authors have implemented their method in the R package: BayesLogit. We will use this package to illustrate Bayesian logistic regression in this chapter.</p><div class="section" title="The BayesLogit R package"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec37"/>The BayesLogit R package</h2></div></div></div><p>The package<a id="id306" class="indexterm"/> can be <a id="id307" class="indexterm"/>downloaded from the CRAN website at <a class="ulink" href="http://cran.r-project.org/web/packages/BayesLogit/index.html">http://cran.r-project.org/web/packages/BayesLogit/index.html</a>. The package contains the <code class="literal">logit</code> function that can be used to perform a Bayesian logistic regression. The syntax for calling this function is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;logit(Y,X,n=rep(1,length(Y) ),m0=rep(0,ncol(X) ),P0=matrix(0,nrow=ncol(X),ncol=ncol(X) ),samp=1000,burn=500)</pre></div><p>Here, <span class="emphasis"><em>Y</em></span> is an <span class="emphasis"><em>N</em></span>-dimensional vector containing response values; <span class="emphasis"><em>X</em></span> is an <span class="emphasis"><em>N x P</em></span> dimensional matrix containing values of independent variables, <span class="emphasis"><em>n</em></span> is an <span class="emphasis"><em>N</em></span>-dimensional vector, <span class="inlinemediaobject"><img src="../Images/image00483.jpeg" alt="The BayesLogit R package"/></span> is a <span class="emphasis"><em>P</em></span>-dimensional prior mean, and <span class="inlinemediaobject"><img src="../Images/image00484.jpeg" alt="The BayesLogit R package"/></span> is a <span class="emphasis"><em>P x P</em></span> dimensional prior precision. The other two arguments are related to MCMC simulation parameters. The number of MCMC simulations saved is denoted by <code class="literal">samp</code> and the number of MCMC simulations discarded at the beginning of the run before saving samples is denoted by <code class="literal">burn</code>.</p></div><div class="section" title="The dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec38"/>The dataset</h2></div></div></div><p>To illustrate<a id="id308" class="indexterm"/> Bayesian logistic regression, we use the Parkinsons dataset from the UCI Machine Learning repository (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Parkinsons">https://archive.ics.uci.edu/ml/datasets/Parkinsons</a>). The dataset was used by Little et.al. to detect Parkinson's disease by analyzing voice disorder (reference 4 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset consists of voice measurements from 31 people, of which 23 people have Parkinson's disease. There are 195 rows corresponding to multiple measurements from a single individual. The measurements can be grouped into the following sets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The vocal fundamental frequency</li><li class="listitem">Jitter</li><li class="listitem">Shimmer</li><li class="listitem">The ratio of noise to tonal components</li><li class="listitem">The nonlinear dynamical complexity measures</li><li class="listitem">The signal fractal scaling exponent</li><li class="listitem">The nonlinear measures of fundamental frequency variation</li></ul></div><p>In total, there are 22 numerical attributes.</p></div><div class="section" title="Preparation of the training and testing datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Preparation of the training and testing datasets</h2></div></div></div><p>Before we can train the<a id="id309" class="indexterm"/> Bayesian logistic model, we need to do some preprocessing of the data. The dataset contains multiple measurements from<a id="id310" class="indexterm"/> the same individual. Here, we take all observations; each from a sampled set of individuals in order to create the training and test sets. Also, we need to separate the dependent variable (class label <span class="emphasis"><em>Y</em></span>) from the independent variables (<span class="emphasis"><em>X</em></span>). The following R code does this job:</p><div class="informalexample"><pre class="programlisting">&gt;#install.packages("BayesLogit") #One time installation of package
&gt;library(BayesLogit)
&gt;PDdata &lt;- read.table("parkinsons.csv",sep=",",header=TRUE,row.names = 1)
&gt;rnames &lt;- row.names(PDdata)
&gt;cnames &lt;- colnames(PDdata,do.NULL = TRUE,prefix = "col")
&gt;colnames(PDdata)[17] &lt;- "y"
&gt;PDdata$y &lt;- as.factor(PDdata$y)

&gt;rnames.strip &lt;- substr(rnames,10,12)
&gt;PDdata1 &lt;- cbind(PDdata,rnames.strip)
&gt;rnames.unique &lt;- unique(rnames.strip)
&gt;set.seed(123)
&gt;samp &lt;- sample(rnames.unique,as.integer(length(rnames.unique)*0.2),replace=F)
&gt;PDtest &lt;- PDdata1[PDdata1$rnames.strip %in% samp,-24]  # -24 to remove last column
&gt;PDtrain &lt;- PDdata1[!(PDdata1$rnames.strip %in% samp),-24] # -24 to remove last column
&gt;xtrain &lt;- PDtrain[,-17]
&gt;ytrain &lt;- PDtrain[,17]
&gt;xtest &lt;- PDtest[,-17]
&gt;ytest&lt;- PDtest[,17]</pre></div></div><div class="section" title="Using the Bayesian logistic model"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec40"/>Using the Bayesian logistic model</h2></div></div></div><p>We can use <span class="emphasis"><em>xtrain</em></span> and <span class="emphasis"><em>ytrain</em></span> to <a id="id311" class="indexterm"/>train the Bayesian logistic regression model using the <code class="literal">logit( )</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;blmodel &lt;- logit(ytrain,xtrain,n=rep(1,length(ytrain)),m0 = rep(0,ncol(xtrain)),P0 = matrix(0,nrow=ncol(xtrain),ncol=ncol(xtrain)),samp = 1000,burn = 500)</pre></div><p>The <code class="literal">summary( )</code> function will give a high-level summary of the fitted model:</p><div class="informalexample"><pre class="programlisting">&gt;summary(blmodel)</pre></div><p>To predict <a id="id312" class="indexterm"/>values of <span class="emphasis"><em>Y</em></span> for a new dataset, we need to write a custom script as follows:</p><div class="informalexample"><pre class="programlisting">&gt;psi &lt;- blmodel$beta %*% t(xtrain)  # samp x n
&gt;p   &lt;- exp(psi) / (1 + exp(psi) )  # samp x n
&gt;ypred.bayes &lt;- colMeans(p)</pre></div><p>The error of prediction can be computed by comparing it with the actual values of <span class="emphasis"><em>Y</em></span> present in <span class="emphasis"><em>ytest</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt;table(ypred.bayes,ytest)</pre></div><p>One can plot the ROC curve using the pROC package as follows:</p><div class="informalexample"><pre class="programlisting">&gt;roc(ytrain,ypred.bayes,plot = T)</pre></div><div class="mediaobject"><img src="../Images/image00485.jpeg" alt="Using the Bayesian logistic model"/></div><p style="clear:both; height: 1em;"> </p><p>The ROC curve <a id="id313" class="indexterm"/>has an AUC of 0.942 suggesting a good classification accuracy. Again, the model is presented here to illustrate the purpose and is not tuned to obtain maximum performance.</p></div></div>
<div class="section" title="Exercises" id="aid-1HIT81"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In this exercise, we <a id="id314" class="indexterm"/>will use the DBWorld e-mails dataset from the UCI Machine Learning repository to compare the relative performance of Naïve Bayes and BayesLogit methods. The dataset contains 64 e-mails from the DBWorld newsletter and the task is to classify the e-mails into either <span class="emphasis"><em>announcements of conferences</em></span> or <span class="emphasis"><em>everything else</em></span>. The reference for this dataset is a course by Prof. Michele Filannino (reference 5 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset can be downloaded from the UCI website at <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#">https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#</a>.<p>Some preprocessing of the dataset would be required to use it for both the methods. The dataset is in the ARFF format. You need to download the <span class="strong"><strong>foreign</strong></span> R package (<a class="ulink" href="http://cran.r-project.org/web/packages/foreign/index.html">http://cran.r-project.org/web/packages/foreign/index.html</a>) and use the <code class="literal">read.arff( )</code> method  in it to read the file into an R data frame.</p></li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-1IHDQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Almeida T.A., Gómez Hidalgo J.M., and Yamakami A. "Contributions to the Study of SMS Spam Filtering: New Collection and Results". In: 2011 ACM Symposium on Document Engineering (DOCENG'11). Mountain View, CA, USA. 2011</li><li class="listitem">MacKay D.J.C. "The Evidence Framework Applied to Classification Networks". Neural Computation 4(5)</li><li class="listitem">"Bayesian Inference for Logistic Models Using Pólya-Gamma Latent Variables". Journal of the American Statistical Association. Volume 108, Issue 504, Page 1339. 2013</li><li class="listitem">Costello D.A.E., Little M.A., McSharry P.E., Moroz I.M., and Roberts S.J. "Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection". BioMedical Engineering OnLine. 2007</li><li class="listitem">Filannino M. "DBWorld e-mail Classification Using a Very Small Corpus". Project of Machine Learning Course. University of Manchester. 2011</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-1JFUC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Summary</h1></div></div></div><p>In this chapter, we discussed the various merits of using Bayesian inference for the classification task. We reviewed some of the common performance metrics used for the classification task. We also learned two basic and popular methods for classification, Naïve Bayes and logistic regression, both implemented using the Bayesian approach. Having learned some important Bayesian-supervised machine learning techniques, in the next chapter, we will discuss some unsupervised Bayesian models.</p></div></body></html>