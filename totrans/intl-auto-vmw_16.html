<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Cloud Application Scaling</h1>
                </header>
            
            <article>
                
<p>This chapter will guide you through how to support next-gen cloud app development by providing developers with access to traditional, cloud-native, and modern application development frameworks and resources, including production-grade container services and open APIs. These will be used on a common vSphere platform and will also support legacy or traditional applications side by side with cloud-native and containerized apps, across a virtualized environment.</p>
<p><span>You will learn how to optimize resources to get maximum output by defining parameters and what-if scenarios. These will be considered for future scalability, so that we can configure and autoscale parameters across different clouds.</span></p>
<p><span>In this chapter, we will cover the following topics:</span></p>
<ul>
<li class="h1">Cloud-native applications</li>
<li class="h1">The <strong>Pivotal Container Service</strong> (<strong>PKS</strong>) on vSphere</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can download VMware Enterprise PKS from <a href="https://cloud.vmware.com/vmware-enterprise-pks/resources">https://cloud.vmware.com/vmware-enterprise-pks/resources</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud-native applications</h1>
                </header>
            
            <article>
                
<p>Digital technologies are always changing, due to today's dynamic business objectives. Everything is connected via mobile, social networks, wearable devices, connected cars, and so on, and they are all influencing the way that we behave and engage with technologies today. Customers are demanding more innovative, flexible, and fast ways to experience products and services, due to this innovation in technology.</p>
<p>Let's look at the systems operating in isolation from each other, responsibility, and skills set.<strong> </strong>We are going through a digital transformation and need all of these operations across various segments. Digital transformation redesigns organizational structures in many environments, so that they're collaborative. Technology can enhance performance and an organization's reach across the globe.</p>
<p>Cloud-native applications have four characteristics:</p>
<ul>
<li><strong>Cloud-native apps are composed of microservices</strong>: Cloud-native apps adopt a microservices architecture, where each application is a collection of small services that can be operated independent of one another. Microservices are often owned by individual development teams that operate on their own schedules to develop, deploy, scale, and upgrade their services.</li>
<li><strong>Cloud-native apps are packaged in containers</strong>: Containers provide isolation contexts for microservices. They are highly accessible, scalable, easily portable from one environment to another, and fast to create or tear down, making them ideal for building and running applications that are composed of microservices.</li>
<li><strong>Cloud-native apps are running in a continuous delivery model</strong>: Software developers and IT operations teams collaborate under this model to build, test, and release software updates as soon as they are ready, without affecting end users or developers on other teams.</li>
<li><strong>Cloud-native applications are dynamically managed in the cloud</strong>: They are often built and run on modern cloud-native platforms, which offer easy scale-out and hardware decoupling, helping in terms of the orchestration, management, and automation of the application.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automation with containers</h1>
                </header>
            
            <article>
                
<p>Customers who have a substantial deployment of the VMware automation tools can easily drive agility and streamline the consumption of IT services. VMware will help customers to deliver both application and container services. This platform will extend the benefits of BOSH (autoscaling, self-healing, load balancing, and so on) to <strong>container as a service</strong> (<strong>CaaS</strong>) solutions (PKS). BOSH is an open source tool which helps in deployment and life cycle management of distributed systems. PKS is the only CaaS solution that can deliver fully managed Kubernetes clusters on premise, as well as a public <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>). This platform will also include <strong>functions as a service</strong> (<strong>FaaS</strong>). This will allow organizations to secure their abstraction planning, regardless of IaaS, by providing application deployment and runtime constructs on one platform. Because of this, we have to plan with various teams that are responsible for the app's rationalization and subsequent migration related to business and technical requirements, in detail.</p>
<p>The <strong>Pivotal Cloud Foundry</strong> (<strong>PCF</strong>) includes both <strong>Pivotal Application Service</strong> (<strong>PAS</strong>) and PKS as critical components. PAS is the cloud-native platform for deploying and operating modern applications. PKS enables customers and service providers to deliver a production-ready Kubernetes on a VMware SDDC and other public cloud environments.</p>
<p>As an example, if we have a system of 10 apps running in containers, those 10 apps will have 10 instances of isolated user spaces. Imagine that two applications are installed on the same operating system, but each needs a different version of that file. We can manage this condition with containers by using a common shared library file. Containers <span>(more </span>specifically, Linux containers) have been around for a while, and companies such as Oracle, HP, and IBM have been using containers for decades. However, Docker has become more popular among users.</p>
<p>Easy-to-use API and CLI tools for deploying apps that support namespace and resource limits reduce the complexity involved in deploying and managing containers. A container is a running instance of an image that runs a container. We need to download an image to use this. An image is a layered filesystem, where each layer has its own filesystem.</p>
<p>When you want to make changes, there's no need to crack open a single, large, monolithic application and shove new changes in. If we have to make changes, then we can just add them to a new layer. </p>
<p>Containers are doing to operating systems what VMs did to server hardware. Tools and organizational processes that are required to run and operate containers are generally not defined. VMware and Pivotal are in a unique position to solve these new challenges and become the <strong>incumbent</strong>.<strong> </strong>Containers virtualize the operating system by limiting the number of application dependencies that we need to install on the OS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container use cases </h1>
                </header>
            
            <article>
                
<p>The following are the use cases for containers:</p>
<ul>
<li><strong>The need for a developer sandbox</strong>:<strong> </strong>Developers often want access to a cluster of machines running a particular framework to build or quickly test and validate their applications. Provisioning such environments is time-consuming and often involves tickets and approvals. As a result, developers either request VMs and customize them to their needs, creating snowflake deployments, or they never give up these resources, because they are worried about obtaining a new one, which could be a tedious process.</li>
<li><strong>Application repackaging</strong>:<strong> </strong>Customers can take their existing applications and package them as a container. You don't need to refractor code or make changes to the architecture. While this forms the first logical step in a customer's containerization journey, it allows customers to derive certain benefits. Patching and maintaining the application is one primary benefit, where updates can be restricted to just the individual layers of the image. This ensures that other layers are intact, reducing errors and configuration issues that could arise.</li>
<li><strong>Portability</strong>:<strong> </strong>Packaging an application as a container enables portability. The container image, by virtue of packaging not just the application code but also all of its dependencies, is guaranteed to work anywhere. We are now able to move this image from a developer's laptop to your test/dev or production environment without having to invest time and resources in getting the target environments to exactly mimic the dev environment (or vice versa) as a result. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges with containers</h1>
                </header>
            
            <article>
                
<p>We focus on enabling developer code to instantiate all the resources developers need, even for legacy systems, to provide high levels of automation during the waterfall approach and to enable the customers to self-serve their resource requirements.</p>
<p>Traditional model uses traditional application architectures, tooling, and processes, where developers have to raise tickets for resources in cloud delivery models. Resources are provided through self-service. Cloud-native applications initiate these requests through code and provide service <strong>infrastructure as code</strong> (<strong>IaC</strong>). </p>
<p>Code replaces the service tickets, and APIs play a critical role. A developer-ready infrastructure can be achieved with automated VMware SDDC tool by providing APIs that assist in running containers, such as OpenStack, PCF, and so on, as normal VM environments. Containers can be managed from existing operating models, since developers get all of the benefits. This is because IT has to manage the underlying resources in a consistent way.</p>
<p>The globally consistent infrastructure layer has benefits in the microservices architecture, as each service defines its relationship to other microservices. This can be broken if the underlying network is complex and doesn't have visibility. The network should be wide open to avoid this problem. Pivotal value VMware NSX and developer-ready infrastructure have the same code, which defines the relationships between microservices and instantiates secure micro-segmented network connectivity. Even serverless architectures can have an Internal Server Error message.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PKS on vSphere</h1>
                </header>
            
            <article>
                
<p><strong>vSphere cluster groups</strong> are sets of ESXi hosts that have a common compute entity; there are from 2 to 64 hosts per vSphere cluster when vSphere HA and DRS are activated at the cluster level. Resource pools are created under a vSphere cluster instance, and vCenter is able to manage multiple vSphere clusters instances, as there is no hard limit to the number of vSphere clusters. We can create different types of vSphere clusters, such as management clusters, compute clusters, and Edge clusters, since PKS fully leverages the vSphere cluster construct.</p>
<p>The following vSphere clusters are recommended in a typical PKS deployment:</p>
<ul>
<li><strong>Management cluster</strong>:
<ul>
<li><strong>Hosted components</strong>: vCenter, NSX manager, and controller VMs</li>
<li>vSphere HA and DRS enabled</li>
<li>ESXi hosts need to be NSX prepared, as micro-segmentation is enforced on the hosted VMs</li>
</ul>
</li>
<li><strong>Compute cluster(s)</strong>:
<ul>
<li><strong>Hosted components</strong>: Kubernetes (K8s) clusters nodes VMs</li>
<li>vSphere HA and DRS should be enabled, as BOSH will check whether DRS is turned on</li>
<li>ESXi hosts need to be NSX prepared</li>
</ul>
</li>
<li><strong>Edge cluster</strong>:
<ul>
<li><strong>Hosted components</strong>: NSX Edge Nodes VMs</li>
<li>vSphere HA and DRS enabled</li>
<li>ESXi hosts don't need to be NSX prepared</li>
</ul>
</li>
</ul>
<p>The PKS Management Plane can reside on the management cluster or compute cluster, depending on the selected design scenario. PKS Management Plane VMs are the Ops Manager, BOSH, the PKS control plane, and Harbor. </p>
<p>The PKS data plane (or compute plane) will only reside in a compute cluster. Up to three K8s master nodes and 50 worker nodes are allowed per K8s cluster, and many K8s clusters can be created in the same PKS environment.</p>
<p>The K8s master node also hosts the etcd component. vSphere DRS and HA must be enabled on the vSphere compute cluster. vSphere <span class="packt_screen">DRS Automation</span> has to be set to <span class="packt_screen">Partially Automated</span> or <span class="packt_screen">Fully Automated</span>. vSphere HA is set with <span class="packt_screen">Host failure = Restart VMs</span>.</p>
<p>The following are the compute and storage requirements for the PKS component:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>PKS Component</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>CPU</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>RAM (GB)</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Storage (GB)</strong></p>
</td>
</tr>
<tr>
<td>
<p>Ops Manager</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>HD1: 160</p>
</td>
</tr>
<tr>
<td>
<p>PKS Control Plane VM</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>HD1: 3</p>
<p>HD2: 16</p>
<p>HD3: 10</p>
</td>
</tr>
<tr>
<td>
<p>BOSH</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>HD1: 3</p>
<p>HD2: 50</p>
<p>HD3: 50</p>
</td>
</tr>
<tr>
<td>
<p>Harbor</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>HD1: 3</p>
<p>HD2: 64</p>
<p>HD3: 30</p>
</td>
</tr>
<tr>
<td>
<p>K8s master node</p>
</td>
<td>
<p>Configurable per PKS plan</p>
</td>
<td>
<p>Configurable per PKS plan</p>
</td>
<td>
<p>Ephemeral disk: 8 to 256 GB</p>
<p>Persistent disk: 1 GB to 32 TB<br/>
(Configurable per PKS plan)</p>
</td>
</tr>
<tr>
<td>
<p>K8s worker node</p>
</td>
<td>
<p>Configurable per PKS plan</p>
</td>
<td>
<p>Configurable per PKS plan</p>
</td>
<td>
<p>Ephemeral disk: 8 to 256 GB</p>
<p>Persistent disk: 1 GB to 32 TB<br/>
(Configurable per PKS plan)</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PKS availability zone </h1>
                </header>
            
            <article>
                
<p>PKS supports the concept of an <strong>availability zone</strong> (<strong>AZ</strong>), that is, <em>AZ = vSphere cluster + resource pool</em>. The AZ dictates the placement of a VM that's created by BOSH/PKS into the corresponding vSphere cluster/resource pool.</p>
<p>There are two types of AZ:</p>
<ul>
<li><strong>Management AZ</strong>: Used for BOSH, PKS control plane, and Harbor VMs</li>
<li><strong>Compute AZ</strong>: Used for K8s master and worker node VMs</li>
</ul>
<p>PKS supports multiple compute availability zones, and each PKS plan supports up to three distinct ones. Each K8s master node (for a max of three) will land in one separate AZ. K8s worker nodes will be dispatched across the three zones.</p>
<p>Three PKS plans are allowed (for a total of nine distinct compute zones). Each PKS plan can use the same three zones or a completely different set of three AZs. An AZ is commonly used to set the locality of a VM against different locations; or, we can say that AZ = physical rack (or room).</p>
<p>Following are the PKS design topologies:</p>
<ul>
<li><strong>Physical topologies (integration with vSphere)</strong>: Multiple topologies can be deployed with PKS/NSX-T integration</li>
<li><strong>PKS Management Plane in the management cluster</strong>: Multi-compute clusters:
<ul>
<li>The PKS Management Plane is hosted in a management cluster and connected to a DVS virtual switch</li>
<li>Multiple compute clusters, to support K8s cluster nodes</li>
<li>Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between the AZ and vSphere cluster).</li>
<li><strong>AZ can represent a physical location</strong>: Each compute cluster can be in a dedicated rack or room</li>
</ul>
</li>
<li><strong>PKS Management Plane in the management cluster for</strong> <strong>a single compute cluster</strong>:<br/>
<ul>
<li>The PKS Management Plane is hosted in a management cluster and connected to a DVS virtual switch</li>
<li>Single Compute Clusters to support K8s cluster nodes</li>
<li>Each AZ is mapped to a unique vSphere cluster/different resource pool</li>
<li>AZs can be used to limit the CPU/memory per PKS plan</li>
</ul>
</li>
</ul>
<ul>
<li><strong>PKS Management Plane in the compute cluster for</strong> <strong>multi-compute clusters</strong>:
<ul>
<li>The PKS Management Plane is hosted in a compute cluster and connected to an NSX-T logical switch</li>
<li>Multiple compute clusters, to support K8s cluster nodes</li>
<li>Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between AZs and vSphere clusters)</li>
<li><strong>An AZ can represent a physical location</strong>: Each compute cluster can be in a dedicated rack or room</li>
</ul>
</li>
<li><strong>PKS Management Plane in a compute cluster, or a single compute cluster</strong>:
<ul>
<li>The PKS Management Plane is hosted in a compute cluster and connected to an NSX-T logical switch</li>
<li>Single compute clusters, to support K8s cluster nodes</li>
<li>Each AZ is mapped to a unique vSphere cluster/different resource pool:</li>
<li>An AZ can be used to the limit CPU/memory per PKS plan</li>
</ul>
</li>
<li><strong>PKS AZ (Single/multiple Compute and Management Clusters) design model</strong>:
<ul>
<li><strong>PKS AZ with single vSphere compute cluster</strong>: By default, there is no guarantee that K8s master nodes land on different ESXi hosts. A workaround is to create a DRS affinity rule on the vSphere Compute Cluster.</li>
<li><strong>Type</strong>: Separate VMs.</li>
<li><strong>Members</strong>: All K8s master node VMs.</li>
<li>The vSphere cluster must have a minimum of three ESXi hosts (this is a vSAN prerequisite). However, to protect against one host failure (and to make sure that the DRS affinity rule will operate properly), a recommendation is to start with four ESXi hosts in the cluster.</li>
<li>NSX-T 2.2 supports all types of traffic on N-VDS. This means that an ESXi host in the compute cluster can start with two physical NICs.</li>
</ul>
</li>
</ul>
<p>The minimum vSphere cluster configuration for a production environment is as follows:</p>
<ul>
<li><strong>Management cluster</strong>:
<ul>
<li><strong>Non-vSAN</strong>: Min hosts: Two</li>
<li><strong>vSAN</strong>: Min hosts: Three (To guarantee data protection for vSAN objects, you must have two replicas and one witness)</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Compute cluster(s)</strong>:
<ul>
<li><strong>Single compute cluster topology</strong>:
<ul>
<li><strong>Non-vSAN</strong>: Min hosts: Three (To guarantee one K8s master node VM per ESXi host by using a DRS affinity rule)</li>
<li><strong>vSAN</strong>: Min hosts: Three (To guarantee data protection for vSAN objects, you must have two replicas and one witness)</li>
</ul>
</li>
<li><strong>Multiple compute clusters topology</strong>:
<ul>
<li><strong>Non-vSAN</strong>: Min hosts: Two per AZ, with three AZs in total (The K8s master node is instantiated across different compute clusters. Each compute cluster is 1:1 mapped with one AZ)</li>
<li><strong>vSAN</strong>: Min hosts: Three per AZ, with three AZs in total (To guarantee data protection for vSAN objects, you must use two replicas and one witness)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Edge Cluster</strong>:
<ul>
<li><strong>Non-vSAN</strong>: Min hosts: Two.</li>
<li><strong>vSAN</strong>: Min hosts: Three (To guarantee data protection for vSAN objects, you must use two replicas and one witness.) Note: An Edge Cluster can be collapsed with a compute cluster (or even a management cluster) if you need to lower the number of starting ESXi hosts.</li>
</ul>
</li>
</ul>
<p>The following table gives information about PKS/NSX-T networks:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Network</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Description</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>CIDR</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>PKS Management Network</strong></p>
</td>
<td>
<ul>
<li>This network hosts the Ops Manager, BOSH, the PKS control plane, and Harbor</li>
<li>Can be co-located with vCenter, NSX-T management, and the control plane, if desired</li>
<li>PKS Management Network is routable or non-routable, depending on NO-NAT or NAT topology</li>
</ul>
</td>
<td>
<p>192.168.1.0/28 (for instance)</p>
<p>CIDR with /28 is a good starting point.</p>
</td>
</tr>
<tr>
<td>
<p><strong>Nodes IP Block</strong></p>
</td>
<td>
<ul>
<li>This block will be carved to create a network that will host K8s cluster node VMs</li>
<li>Each K8s cluster will be allocated a /24 portion of the block</li>
<li>The nodes IP block is routable or non-routable, depending on NO-NAT or NAT topology</li>
</ul>
</td>
<td>
<p>Depends on the NAT or NO-NAT topology.</p>
<p>172.23.0.0/16 (for instance)</p>
</td>
</tr>
<tr>
<td>
<p><strong>Pods IP Block</strong></p>
</td>
<td>
<ul>
<li>This block will be carved to create a network that will host K8s pods belonging to the same K8s namespace</li>
<li>Each k8s namespace will be allocated a /24 portion of the block</li>
<li>The pods IP block is always non-routable</li>
</ul>
</td>
<td>
<p>172.16.0.0/16 (for instance)</p>
</td>
</tr>
<tr>
<td>
<p><strong>Floating IP Pool</strong></p>
</td>
<td>
<ul>
<li>This pool will be used for two purposes:
<ul>
<li>SNAT rules for each K8s namespace on T0 (for pods networking)</li>
<li>LB virtual servers IP allocation</li>
</ul>
</li>
<li>The floating IP pool is always routable</li>
</ul>
</td>
<td>
<p>192.168.20.2-192.168.20.254 (for instance)</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p><strong> CIDR for nodes IP block</strong>:</p>
<ul>
<li>Must be unique in the case of a routable scenario (NO-NAT topology)</li>
<li>Can be duplicated in the case of a non-routable scenario (NAT topology)</li>
</ul>
<p>The <kbd>172.17.0.0/16</kbd><strong> </strong>CIDR must not be used in all cases, as Docker on the K8s worker node is using the subnet. </p>
<p>If PKS is deployed with Harbor, then the following CIDR must not be used, as Harbor is leveraging it for its internal Docker bridges:</p>
<pre> 172.18.0.0/16 ;172.19.0.0/16 ;172.20.0.0/16 ;172.21.0.0/16 ;172.22.0.0/16</pre>
<p>Each K8s cluster uses the following IP block for Kubernetes services, so avoid using it for a nodes IP block: <kbd>10.100.200.0/24</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PKS/NSX-T logical topologies</h1>
                </header>
            
            <article>
                
<p>PKS supports two types of topologies when it's integrated with NSX-T. NAT and NO-NAT topology selection is done in the <span class="packt_screen">PKS tile</span> | <span class="packt_screen">Networking</span> section. NAT topology is the default, but you can uncheck NAT mode to go with the NO-NAT topology. The NAT and NO-NAT terminology essentially applies to the PKS Management Network and the K8s cluster nodes network (that is, whether to use routable subnets). Irrespective of the NAT or NO-NAT topology, the same procedure is used to access the K8s API.</p>
<p>A virtual server on the NSX-T LB instance that's allocated to the K8s cluster is created for the following purpose:</p>
<ul>
<li>One IP from the PKS Floating IP Pool is extracted (<kbd>1x.x0.1x.1xx</kbd> here), and the port is <kbd>8443</kbd></li>
<li>The same IP address is shown from the output of the <kbd>pks cluster &lt;cluster name&gt;</kbd> command</li>
</ul>
<p>Following are the objectives with different NAT topologies:</p>
<ul>
<li><strong>NAT topology</strong>: For customers with a limited amount of available routable IP addresses in their DC and who want to automate PKS deployment using a concourse pipeline (for instance)</li>
<li><strong>NO-NAT topology</strong>:<strong> </strong>For customers who avoid NAT as NATs break full path visibility and having plenty of routable IP address resources</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use cases with different configurations</h1>
                </header>
            
            <article>
                
<p>The following are the use cases with different configurations:</p>
<ul>
<li>Access to PKS Management Plane components (Ops Manager, BOSH, PKS control plane VM, Harbor) from the corporate network:
<ul>
<li><strong>NO-NAT topology</strong>: No action is required as those components use routable IP addresses</li>
<li><strong>NAT topology</strong>: User needs to create DNAT rules on T0</li>
</ul>
</li>
<li>Access to K8s API (using a kubectl CLI, for instance):
<ul>
<li><strong>NO-NAT topology</strong>: 1 virtual server (on the NSX-T LB instance that's dedicated to the K8s cluster) is automatically created using 1 routable IP from the PKS Floating IP block</li>
<li><strong>NAT topology</strong>: The user needs to point to this IP to access K8s API</li>
</ul>
</li>
</ul>
<ul>
<li>One virtual server (on the NSX-T LB instance that's dedicated to the K8s cluster) is automatically created using one routable IP from the PKS floating IP block:
<ul>
<li><strong>NO-NAT topology</strong>: The user needs to point to this IP to access the K8s API</li>
<li><strong>NAT topology</strong>: The user needs access to the K8s nodes VM (like BOSH SSH, for instance)</li>
</ul>
</li>
<li>Components using routable IP addresses:<br/>
<ul>
<li><strong>NO-NAT topology</strong>: The user needs to SSH to the Ops Manager to perform BOSH commands against the K8s nodes VM</li>
<li><strong>NAT topology</strong>: An alternative is to install a jumpbox server on the same subnet, instead of the PKS Management Plane components</li>
</ul>
</li>
<li>Using the K8s nodes VM to access the corporate network (or internet):
<ul>
<li><strong>NO-NAT topology</strong>: No action is required, as those components use routable IP addresses</li>
<li><strong>NAT topology</strong>: PKS automatically creates a SNAT rule on T0 for each K8s cluster, using one IP address from the PKS Floating IP Pool</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PKS and NSX-T Edge Nodes and Edge Cluster</h1>
                </header>
            
            <article>
                
<p>PKS only supports NSX-T Edge Node VM configurations that are large in size. PKS only supports one Edge Cluster instance of T0 <span>(8 vCPU, 16 GB RAM)</span>. The T0 router must be configured in Active/Standby mode, as SNAT rules will be applied there by PKS. An NSX-T Edge Cluster can contain up to eight Edge <strong>Transport Nodes</strong> (<strong>TN</strong>). You can add new Edge Nodes (up to eight) in the Edge Cluster to increase the overall capacity (LB, for instance) and provide scalability to the NSX-T Edge Cluster. You can use two different Edge Nodes for the T0 uplinks IP addresses (two IPs in total) to provide HA to NSX-T T0 in an Edge Cluster. We should enable HA VIP on T0 so that it's always operational, even if one T0 uplink is down. The physical router will only interoperate with the T0 HA VIP.</p>
<p>The following are the NSX-T and load balancer scale numbers:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 22%"> </td>
<td style="width: 10%">
<p><strong>LB small</strong></p>
</td>
<td style="width: 14%">
<p><strong>LB medium</strong></p>
</td>
<td style="width: 10%">
<p><strong>LB large</strong></p>
</td>
<td colspan="5" style="width: 38%">
<p class="CDPAlignCenter CDPAlign"><strong>Pool members</strong></p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>NSX-T release</strong></p>
</td>
<td style="width: 10%">
<p>2.1</p>
</td>
<td style="width: 10%">
<p>2.2</p>
</td>
<td style="width: 10%">
<p>2.1</p>
</td>
<td style="width: 10%">
<p>2.2</p>
</td>
<td style="width: 10%">
<p>2.1</p>
</td>
<td style="width: 10%">
<p>2.2</p>
</td>
<td style="width: 10%">
<p>2.1</p>
</td>
<td style="width: 10%">
<p>2.2</p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Edge VM: Small</strong></p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 14%">
<p>-</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 3%">
<p>-</p>
</td>
<td style="width: 7%">
<p>-</p>
</td>
<td style="width: 8%">
<p>-</p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Edge VM: Medium</strong></p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 14%">
<p>1</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 3%">
<p>-</p>
</td>
<td style="width: 7%">
<p>30</p>
</td>
<td style="width: 8%">
<p>30</p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Edge VM: Large</strong></p>
</td>
<td style="width: 10%">
<p>4</p>
</td>
<td style="width: 14%">
<p>40</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 10%">
<p>-</p>
</td>
<td style="width: 3%">
<p>-</p>
</td>
<td style="width: 7%">
<p>120</p>
</td>
<td style="width: 8%">
<p>1,200</p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Edge: Bare Metal</strong></p>
</td>
<td style="width: 10%">
<p>100</p>
</td>
<td style="width: 14%">
<p>750</p>
</td>
<td style="width: 10%">
<p>10</p>
</td>
<td style="width: 10%">
<p>100</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 3%">
<p>1</p>
</td>
<td style="width: 7%">
<p>3,000</p>
</td>
<td style="width: 8%">
<p>22,500</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PKS and NSX-T communications</h1>
                </header>
            
            <article>
                
<p>Multiple PKS components need to communicate with the NSX-T manager. A PKS control plane VM using an NSX-T superuser principal identity certificate as an authentication mechanism is needed to create a T1/LS for each K8s cluster node network and an LB instance for each K8s cluster.</p>
<p>BOSH uses credentials as an authentication mechanism to tag all of a VM's logical ports with a special BOSH ID tag and NCP pod. It uses the NSX-T superuser principal identity certificate as an authentication mechanism to create T1/LS for each namespace, a SNAT rule on T0 for each namespace, and an LB virtual server for each K8s service of the type LB.</p>
<p>The following is a list of the NSX-T objects that are created, for each K8s cluster.</p>
<p>When a new K8s cluster is created, the following NSX-T objects are created by default:</p>
<ul>
<li><strong>NSX-T LS</strong>:
<ul>
<li>One LS for K8s master and worker nodes</li>
<li>One LS for each K8s namespace, that is, kube-public, kube-system, and pks-infrastructure</li>
<li>One LS for the NSX-T LB associated with the K8s cluster</li>
</ul>
</li>
<li><strong>NSX-T T1</strong>:
<ul>
<li>One T1 for K8s master and worker nodes (called cluster-router)</li>
<li>One T1 for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure)</li>
<li>One T1 for the NSX-T LB associated with the K8s cluster</li>
</ul>
</li>
<li><strong>NSX-T LB</strong>:
<ul>
<li>One NSX-T LB small instance, containing the following objects:
<ul>
<li>One virtual server to access the K8s control plane API (with port 8443)</li>
<li>One server pool containing the three K8s master nodes</li>
<li>One virtual server for the ingress controller (HTTP)</li>
<li>One virtual server for the ingress controller (HTTPS)</li>
<li>Each virtual server is allocated an IP address derived from the PKS Floating IP Pool</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>When a new K8s cluster is created, the following NSX-T objects are created, by default:</p>
<ul>
<li><strong>NSX-T DDI/IPAM</strong>: A /24 subnet from the nodes IP block will be extracted and allocated for the K8s master and worker nodes.</li>
<li><strong>NSX-T DDI/IPAM</strong>: A /24 subnet from the PODs IP Block will be extracted and allocated for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure).
<ul>
<li><strong>NSX-T T0 router</strong>:
<ul>
<li>One SNAT rule created for each K8s namespace (default, kube-public, kube-system, pks-infrastructure), using one IP from the Floating IP Pool as the translated IP address.</li>
<li>One SNAT rule created for each K8s cluster (in the case that NAT topology is used), using 1 IP from the Floating IP Pool as the translated IP address. The K8s cluster subnet is derived from the nodes IP block, using a /24 netmask.</li>
</ul>
</li>
<li><strong>NSX-T DFW</strong>:
<ul>
<li>One DFW rule for kubernetes-dashboard: Source=K8s worker node (hosting the dashboard POD/Destination= dashboard POD IP/Port: TCP/8443/Action: allow</li>
<li>One DFW rule for kube-dns: Source=K8s worker node (hosting the DNS POD)/ Destination = DNS POD IP/Port: TCP/8081 and TCP/10054/Action: allow</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage for K8s cluster node VMs</h1>
                </header>
            
            <article>
                
<p>You can provide storage for K8s PODs by using <strong>persistent volumes</strong> (<strong>PV</strong>). A PV can be mapped to a <strong>v<span>irtual machine disk</span></strong> (<strong>VMDK</strong>) file on vSphere by using the <strong>vCP</strong> (short for <strong>Cloud Provider</strong>) plugin. A VMDK file will then be attached to the worker node VM as a disk. We can then POD mount the volume from that disk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datastores</h1>
                </header>
            
            <article>
                
<p>The following is a table of information regarding datastores:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Deployment topology/storage technology</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>vSAN datastores</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>VMFS over NFS/iSCSI/FC datastores</strong></p>
</td>
</tr>
<tr>
<td>
<p>Single vSphere compute cluster (single AZ, or multiple AZs if using RPs) with a datastore local to a single vSphere compute cluster</p>
</td>
<td>
<ul>
<li><strong>Static PV provisioning</strong>: Yes</li>
<li><strong>Dynamic PV provisioning</strong>: Yes</li>
</ul>
</td>
<td>
<ul>
<li><strong>Static PV provisioning</strong>: Yes</li>
<li><strong>Dynamic PV provisioning</strong>: Yes</li>
</ul>
</td>
</tr>
<tr>
<td>
<p>Multi-vSphere compute clusters (multiple AZs) with datastore(s) local to each vSphere compute cluster</p>
</td>
<td>
<ul>
<li><strong>Static PV provisioning</strong>: No*</li>
<li><strong>Dynamic PV provisioning</strong>: No*</li>
</ul>
</td>
<td>
<ul>
<li><strong>Static PV provisioning</strong>: No*</li>
<li><strong>Dynamic PV provisioning</strong>: No*</li>
</ul>
</td>
</tr>
<tr>
<td>
<p>Multi-vSphere compute clusters (multiple AZs) with datastore(s) shared across all vSphere compute clusters</p>
</td>
<td>
<ul>
<li>N/A</li>
<li>vSAN does not support shared datastores across vSphere clusters</li>
</ul>
</td>
<td>
<ul>
<li><strong>Static PV provisioning</strong>: Yes</li>
<li><strong>Dynamic PV provisioning</strong>: Yes</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Following are the steps to provision Static PV:</p>
<ol>
<li>Manually create a VMDK file</li>
<li>Create a PV referencing the aforementioned VMDK file</li>
<li>Create a PVC</li>
<li>Deploy a stateful POD or StatefulSets by using a reference to the PVC</li>
</ol>
<p>Following are the steps for Dynamic PV provisioning:</p>
<ol>
<li>Create a PVC (vCP K8s storage plugin; <span>a</span> hatchway will automatically create PV and VMDK files)</li>
<li>Deploy stateful POD or StatefulSets using a reference to PVC</li>
</ol>
<p>The following are some vSAN considerations in regards to PKS/NSX-T:</p>
<ul>
<li>Using vSAN, a vSphere cluster must start with a minimum of three ESXi hosts to guarantee data protection (in this case, for RAID1 with failure to tolerate set to 1)</li>
<li>A PKS AZ does not map with the vSAN fault domain</li>
<li>A PKS with a single compute cluster is currently supported with vSAN (all ESXi hosts are located in the same site)</li>
<li><strong>Caution</strong>: A PKS with a vSAN stretched cluster is not a supported configuration, as of right now (no mapping of AZs with the vSAN fault domain)</li>
</ul>
<ul>
<li>A PKS with multiple compute clusters is not a supported configuration with a vSAN-only datastore</li>
<li>Master and worker nodes can be created across the different ESXi clusters (BOSH tile allows you to specify multiple persistent and ephemeral datastores for the VMs)</li>
<li>PV VMDK disks are created for only one vSAN datastore (and no replication across the different vSAN datastores will be performed automatically)</li>
</ul>
<p>Data centers maintain independent PKS instances, NSX deployments, Kubernetes (K8s) clusters, and vSphere infrastructures. A <strong>Global Server Load Balancer</strong> (<strong>GSLB</strong>), which is available through a third party, monitors the availability of the sites' K8s cluster API and PKS controller API. Operations and development direct API requests to the GSLB virtual server URL for creating and managing K8s clusters and deploying apps. Manually deployed apps (through kubectl, for instance) are not automatically replicated between environments and need to be redeployed following a failover to site B. </p>
<p>You can configure a CI/CD automation server to execute build pipelines against the K8s' URL in each environment or single builds against the GSLB virtual server URL. Harbor policy based replication, a built-in feature, manages cloning images to the standby location. You can replicate the datastore(s) between environments to support PV. Following a site A failure, the pods are redeployed at site B, mounting the original persistent volume's VMDK file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>There is a new IT approach called cloud-native behind this digitalization trend, which is one of the driving forces of business digitalization. The cloud-native methodology allows enterprises to greatly increase developer productivity, allowing them to deliver new apps and services to the market much more quickly than before; they can therefore improve customer experience and satisfaction. If adopted successfully, the cloud-native methodology can also help to cut operations and infrastructure costs, as well as to enhance app security. </p>
<p>In the next chapter, <a href="557b79e8-1cf1-4c07-be7d-29ad5d965c3e.xhtml" target="_blank">Chapter 14</a>, <em>High Performance Computing for Machine Learning</em>, you will learn about the specific aspects of virtualization that can enhance the productivity of a <strong>high-performance computing</strong> (<strong>HPC</strong>) environment. We will explore the capabilities which are enabled by VMware vSphere to meet the requirements for researching computing, academic, scientific, and engineering HPC workloads.</p>


            </article>

            
        </section>
    </body></html>