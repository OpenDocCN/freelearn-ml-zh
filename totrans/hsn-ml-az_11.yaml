- en: Integration with Other Azure Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to using Azure AI services directly, Azure also provides options
    for using these services from other non-AI services. Many Azure AI services provide
    REST API interfaces that can be consumed from other services. AI services can
    thus be used as subcomponents of other apps to provide insights and predictions.
    Many non-AI services in Azure have built-in integration with AI services, so that
    AI components can often be added to apps with a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: Some AI services do not include any automation features. Recurring tasks, such
    as retraining ML models or running batch workloads, require integration with other
    services that offer these features. In the following sections, we will present
    various options for launching AI jobs automatically. In addition to traditional
    time-scheduled workloads, Azure services also provide objects called triggers
    to launch tasks after a certain event has occurred. Triggers allow services to
    react to events in a specific way, for example, processing the contents of a blob
    file after it has been created or modified.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to integrate Azure AI services with four
    non-AI services:'
  prefs: []
  type: TYPE_NORMAL
- en: Logic Apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Lake Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Factory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these services, it is possible to build complex application pipelines where
    the ML model is just a part of the solution. In Azure, integration between different
    services is made as easy as possible, without compromising security. Getting results
    is therefore quick and efficient, and developing Azure applications can be a lot
    of fun!
  prefs: []
  type: TYPE_NORMAL
- en: Logic Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Logic Apps is a graphical tool for automating various types of tasks,
    such as getting data from a Web API and saving it in cloud storage. Logic Apps
    can be developed without writing a single line of code, so no programming skills
    are required. However, Logic Apps provides some basic functionality for programming
    languages, such as conditional execution and iterative loops.
  prefs: []
  type: TYPE_NORMAL
- en: Logic Apps is meant for light-weight tasks that do not require complex logic
    or lightning-fast performance. Such tasks could include sending an email when
    a SharePoint list is modified, or copying files between Dropbox and OneDrive if
    the files have been modified.
  prefs: []
  type: TYPE_NORMAL
- en: For AI development, Logic Apps provides a number of basic functionalities. There
    are built-in ...
  prefs: []
  type: TYPE_NORMAL
- en: Triggers and actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logic Apps is based on two main concepts: triggers and actions. Triggers are
    listeners that are constantly waiting for a specific kind of event. These events
    could be created when a new file is created in a `Blob` folder, or they could
    occur every day at the same time to automate daily tasks. Actions are routines
    that are executed every time the trigger fires. Actions usually take some input
    data, process it, and finally, save or send the result somewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logic Apps supports all Azure Data Storage services: Data Lake Storage, Blob
    Storage, Cosmos DB, and so on. For example, the **Blob Storage** trigger executes
    Logic Apps every time a new file is added to a directory or a file in the directory
    is modified. We can then take the file as input to Logic Apps, process the contents
    of the file, and save the results back to the Blob.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Logic Apps Designer contains many Cognitive Services actions: the Computer
    Vision API, Custom Vision, Text Analytics, and so on. To see the full list, search
    for the cognitive actions when creating an action, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7680f7ee-2d1d-4ad0-ac61-e2f4a58ee4a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are also a few Azure Machine Learning Studio actions. These can be used
    to score examples or retrain ML models, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7622647c-2a3b-44f5-a0d0-d6bb6467806d.png)'
  prefs: []
  type: TYPE_IMG
- en: The connectors might have been updated since the time of writing, so check the
    latest information in the official documentation from Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social media has become an everyday tool for many organizations. In addition
    to advertising themselves on these platforms, monitoring discussions about a company
    brand provides important information about how customers see its products and
    the image of the company.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll show how to create an app that reads tweets containing
    the keyword *Azure* and saves those tweets in cloud storage. This is a real-world
    data acquisition scenario: once the tweets get stored in the cloud permanently,
    we have started to collect history data about the company''s brand. As a next
    step, we could start creating analytics and machine learning models based on this
    data. These models could include topic analysis or looking ...'
  prefs: []
  type: TYPE_NORMAL
- en: Adding language detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we created a workflow that reads tweets with a specific
    keyword, performs sentiment analysis on them, and saves the results in Blob Storage.
    If you completed the example and looked at the results more carefully, you might
    have noticed that the tweets can contain many different languages, because our
    keyword (Azure) is not specific to any language. In this example, we show how
    to add language detection to the pipeline and filter out all tweets that are not
    in a particular language.
  prefs: []
  type: TYPE_NORMAL
- en: To begin the example, create a new Logic App in the Azure portal, open the Logic
    Apps Designer, and choose the same trigger as a starting point as we did in the
    previous example (when a new tweet appears).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the Detect Language module from the Text Analytics API. For the Text field,
    choose the Tweet text parameter to be analyzed by the Text Analytics API. The
    output of this module will contain the list of languages detected from the tweet.
    Note that the default number of detected languages is 1\. This can be changed
    in the advanced options for the module, if the text is expected to contain more
    than one language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1a2ce9f1-19b5-4228-8e47-630586c251b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the tweet language has been detected, we need to add a module that chooses
    only tweets that are in a certain language. For this purpose, create a new step
    in the pipeline, search for `Control` actions and choose the `Condition` action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the `Condition` module has been added to the pipeline, we must specify
    the filter condition using the three fields in the module. The first field indicates
    which value we would like to compare, the second field specifies the condition
    type, and the last field contains the value against which to compare. For the
    first field, choose the `detectedLanguages` item. Note that the detect language
    module returns the languages as a list, even if we are only detecting one language.
    The `detectedLanguages` item refers to each item in this list. Therefore, the
    Logic Apps Designer will add a new `For each` control module to the pipeline and
    the previously created `Condition` module will be placed inside the `For each`
    module. The `For each` module iterates through all the values in the `detectedLanguages`
    list, if there is more than one language detected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that our `Condition` module is placed inside the `For each` module, we can
    change the first field of the `Condition` module to `Language code`. The second
    field should be set to `is equal to` and the last field should contain the two-letter
    language code. To keep English language tweets only, we set the parameter to `en`.
    Note that the language detection module might not be able to detect all languages,
    so check the list of available languages in the latest Microsoft documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the condition module, the pipeline now branches into two directions,
    depending on whether the condition is `true` or not. In the `true` branch, we
    can add any processing steps for the English language tweets. In the `false` branch,
    we can specify what to do with the rest of the tweets. In this example, we use
    the sentiment detection pipeline that was developed in the previous example to
    process the English language tweets. We are not interested in the non-English
    tweets, so we can just ignore them by leaving the false branch empty. After completing
    all these steps, the pipeline appears as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a75abd3f-8d0a-4d23-a880-5f6d9212cd8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you save the app, the trigger will start to run and the tweets will be
    saved to the Blob. Since we used the same modules for sentiment detection as we
    did in the previous example, the format of the files should be identical, with
    the exception that only English tweets are saved to the Blob.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Logic Apps provides a fast way to automate tasks, its collection of actions
    is limited to pre-selected options that cannot be customized. Moreover, the programmability
    of Logic Apps is quite limited, and the development of more complex programs is
    not necessarily any easier than writing code. If more flexibility is needed, it
    might be more productive to develop applications with Azure Functions. Functions
    can be developed with multiple programming languages familiar to web developers.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Functions is a serverless coding platform in the cloud, where the underlying
    operating system has been virtualized. This means that many maintenance tasks,
    such as updating the operating system or language versions, is managed by ...
  prefs: []
  type: TYPE_NORMAL
- en: Triggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Functions triggers work with the same principle as the Logic Apps triggers
    introduced in the previous section: they listen to a certain event and initiate
    the function every time the event occurs. For example, an HTTP trigger executes
    the function every time the web URL of the function is called. A schedule trigger
    can execute the function periodically, for example, once a day. One of the biggest
    strengths of Azure Functions is the trigger collection. The trigger collection
    allows you to respond to many different types of events, and the code-based approach
    makes it possible to react to those events with the full flexibility of programming
    languages.'
  prefs: []
  type: TYPE_NORMAL
- en: For data-related tasks, the most useful triggers are the storage triggers. Similar
    to how  Logic Apps can be triggered on blob events, Azure Functions can be triggered
    when files are added or updated in storage services such as Blob Storage or Cosmos
    DB. Consider a scheme where raw data is stored in the cloud, for example, JSON
    files that are produced by a web app, and we wish to convert those files into
    a columnar format to add them to a relational database, for example. Using Azure
    Functions with Blob Storage triggers, this task can be fully automated without
    any external services. Thus, Azure Functions provides another way to perform such
    format conversions, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most interesting applications for Azure Functions is processing data
    from **Internet of Things** (**IoT**) devices. With event hub and IoT hub triggers,
    Azure Functions can be used as a downstream processing engine for data sent by
    IoT devices. The event hub and IoT Hub resources in Azure are data ingestion services
    that are capable of handling a large number of requests in a short time interval.
    They are designed for continuous data streams where the size of each message is
    small, but the number of messages can be large. Therefore, they are ideal services
    for receiving data from an array of IoT sensors that send their measurements in
    short intervals. The event hub and IoT hub triggers are set to fire every time
    a new event is received from the IoT devices, so we can use Azure Functions to
    define a processing routine for each event. This routine could include a scoring
    call to a machine learning service and saving the results in a database, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most trigger types, the trigger also passes some information about the
    event that fired the trigger. For example, a Blob Storage trigger passes the contents
    of the file that was created or changed, or a HTTP trigger passes the contents
    of a POST request. Handling different types of input data is made very simple
    in Azure Functions: depending on the source, a handle to the input data is passed
    as a parameter to the main function. In the following section, we will show an
    example how to read the contents of a blob file after the blob trigger has been
    fired.'
  prefs: []
  type: TYPE_NORMAL
- en: Blob-triggered function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we'll demonstrate how to create a simple function that reads
    the contents of a blob file. We will not do any processing on the file; instead,
    we'll just print its contents in a log file.
  prefs: []
  type: TYPE_NORMAL
- en: When you create an Azure Functions service in the portal, it will create a number
    of resources in addition to the functions app itself. A storage account is needed
    to store logs produced by the functions app. The app service plan is a container
    resource that determines the pricing and the resource scaling of the app. Optionally,
    you can also create an app insights resource for monitoring your app usage. This
    is particularly useful for error analysis and tracking how often your app is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: To begin development, ...
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Lake Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Azure Data Lake** (**ADL**) is Microsoft''s storage and analytics service
    for big data. It is capable of storing data on a petabyte scale and making efficient
    queries on the stored data. The storage and the analytics services are separate
    in Azure and the ADL service actually consists of two different products: **Azure
    Data Lake Storage** (**ADLS**) and **Azure Data Lake Analytics** (**ADLA**). In
    this section, we will focus on ADLA, but we will also touch on ADLS where appropriate.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Lake Storage is a file-based storage, with files organized into directories.
    This type of storage is called schemaless, since there are no constraints on what
    type of data can be stored in the Data Lake. Directories can contain text files
    and images, and the data type is specified only when the data is read out from
    the Data Lake. This is particularly useful in big data scenarios where the amount
    of data written to the Data Lake is large and running the data validation steps
    on the fly would be too resource-consuming. The data validation steps can be incorporated
    in the queries later when the data is read out, or they can be run periodically
    in batches.
  prefs: []
  type: TYPE_NORMAL
- en: ADLA is the query engine that enables you to make efficient queries against
    the ADLS. An ADLA account always requires an ADLS account behind it. This primary
    account is the default source for queries. ADLA queries are written with U-SQL,
    an SQL-like language with some programming features borrowed from C#. Some examples
    of U-SQL scripts that query data from ADLS and Storage Blobs follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the primary ADLS source, the ADLA instance can have secondary
    sources as well. You can add other ADLS accounts or Azure Storage Blobs from the
    Data sources tab in the portal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cae0e7ca-dd32-4a97-96c7-6ebb26417e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Once an account has been registered as a data source, it can be used in queries
    similar to the primary ADLS account. By using multiple data sources, you can avoid
    moving the data to a single Data Lake if your data resides in multiple Storage
    accounts.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, it is always possible to create an ADL Storage account without
    associating an ADLA account with it. But the ADLS would only act as a file storage
    in that case, without the ability to make queries against the file contents. Also,
    nothing would prevent you from creating multiple ADLA accounts that all use the
    same ADLS account as their primary source. This flexibility is possible because
    ADLS and ADLA are separate products in the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest strengths of ADL is its user permission controls. Permissions
    can be set separately for each file and directory. If some data in the Data Lake
    is confidential and should not be visible to all users, it can be placed in a
    separate directory and the directory can be made visible only to a select group
    of users. Data Lake is also integrated with Azure AD, and permissions can be assigned
    to Azure AD groups instead of specific users. Role-based access control will not
    be discussed further here, as the focus is on analytics. To add users to the ADLA
    account, they should be granted a Data Lake Analytics Developer role using the
    Add User Wizard in the ADLA instance in the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: The material in this section concerns Data Lake Storage Gen1 (Data Lake Store).
    ADLS Gen2 is in private preview at the time of writing. There might be changes
    in how ADLA works with Gen2, so check the latest information in the Microsoft
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Developing with U-SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, ADLA queries are written in U-SQL, a query language
    developed by Microsoft specifically for big data. Similar to many other Azure
    services, queries can be written and executed in the Azure portal, which is useful
    for small and quick tasks. For advanced development, there are extensions for
    VS Code and Visual Studio IDEs that offer more functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Although writing U-SQL queries is in many ways similar to writing SQL queries,
    there are some differences as well. ADLA does not produce interactive output to
    the query editor as it does in SQL Server Management Studio, for example. The
    results are always directed to a certain output, which is usually a file in ADLS.
    To check the results of the query, ...
  prefs: []
  type: TYPE_NORMAL
- en: U-SQL databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although ADLS is a file-based storage system, it also includes features that
    are familiar from relational databases, such as tables and views. An ADLA database
    is a collection of these objects. ADLA databases are useful for managing data
    in Data Lake Store, since they provide some useful features such as table indexes.
    On the other hand, ADLA database tables have strict schemas in place, so data
    must be validated before it can be entered into a table. Then, we have lost one
    of the biggest strengths of the Data Lake, namely the schemaless storage principle
    that was discussed at the beginning of the section.
  prefs: []
  type: TYPE_NORMAL
- en: When an ADLA resource is created, a master database is also created. This database
    is used as the default database for U-SQL queries. New databases can be created
    with the `CREATE DATABASE` command and a database can be changed with the `USE
    DATABASE` command. The contents of databases can be viewed in the catalog inside
    the ADLA Data Explorer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of tables in U-SQL databases: managed and external tables.
    Managed tables are in many ways similar to SQL tables: a table consists of metadata
    (for example, table schema) and the data itself. In practice, the data is stored
    as structured files in a Data Lake Storage directory. Managed tables enforce a
    schema-on-write, meaning that it is not possible to enter data to the table unless
    it conforms to the table schema.'
  prefs: []
  type: TYPE_NORMAL
- en: 'External tables are in some ways similar to SQL views: only the metadata is
    stored in the database, but the data itself can reside outside of Data Lake. External
    tables can refer to various Azure services: Azure SQL Database, Azure SQL Data
    Warehouse, or SQL Servers on virtual machines. In contrast to managed tables,
    external tables enforce a schema-on-read: the format of the underlying data can
    change after the schema is defined, because the schema is not enforced at the
    time of writing. If the schema of the data changes, the external table definition
    can be altered to match the new data.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a managed table instead of storing the data as files in directories has
    some advantages. For example, if your data is stored in a huge CSV file, it might
    be difficult to append new values to the file. This operation is possible with
    managed tables using the `INSERT` statement. (Note that the managed table is append
    only, without the possibility to update values.) Managed tables also have a clustered
    index defined, which can be used to make more effective queries.
  prefs: []
  type: TYPE_NORMAL
- en: Simple format conversion for blobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll demonstrate the execution of U-SQL scripts in the Azure
    portal and integration with secondary data sources (Blob Storage). We will read
    a **CSV** (short for **comma-separated value**) file from a Blob Storage and write
    the whole contents of the file to another blob file in a **TSV** (short for **tab-separated
    values**) format. In other words, we'll perform a file format conversion from
    CSV to TSV. Such format conversions are common in data management, since different
    applications often require data in different formats.
  prefs: []
  type: TYPE_NORMAL
- en: The requirements for running this example are a Blob Storage account (with some
    data), and ADLS and ADLA accounts.
  prefs: []
  type: TYPE_NORMAL
- en: To read the input data, the U-SQL module `Extractors` must be used. ...
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Cognitive Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous sections, we saw how to integrate Logic Apps with Cognitive Services.
    We used the Text Analytics API to score the sentiment of each tweet and stored
    the results in Blob Storage. In this example, we'll show how to implement the
    same steps with ADLA. In addition to getting the sentiment score, we'll also extract
    key phrases for each tweet to get more insights.
  prefs: []
  type: TYPE_NORMAL
- en: Completing this example requires a Cognitive Services Text Analytics API account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before Cognitive Services can be used in U-SQL scripts, the ADLA instance must
    be registered with Cognitive extensions. This can be done in the Azure portal
    by opening the ADLA instance and choosing the Sample scripts tab, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00917094-bc9e-4891-943c-ddc409c12f67.png)'
  prefs: []
  type: TYPE_IMG
- en: Once installation is successful, Cognitive extensions are ready to be used in
    U-SQL scripts. This means that the `REFERENCE ASSEMBLY` statement can now be used
    to import the Cognitive Services modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the input comes from a directory that contains multiple files.
    The query can be limited to match only some of the files in the directory, according
    to the filename. In this case, all CSV files in the input directory are chosen.
    The input files have the same format as the files that were produced by the Logic
    App in a previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new job and enter the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script reads the input data using the `Extractors.Text()` module.
    The module options state that the values are separated by the pipe `|` character,
    the text values are not quoted, and that the `Extractor` should ignore all erroneous
    data instead of throwing an error and exiting (silent mode). This last property
    is useful if we don't need to worry about the errors.
  prefs: []
  type: TYPE_NORMAL
- en: The Cognitive Services modules are accessed through the `PROCESS` command. Since
    we referenced the assembly `TextSentiment` in the beginning of the script, the
    `Cognition.Text.SentimentAnalyzer` module is now available to be used for processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the username, tweet text, tweet sentiment, and the sentiment score
    are saved to the output (Data Lake directory). The output file should look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding script, the output schema is defined after the `PRODUCE`
    statement. In this case, some fields in the input schema are ignored (`Timestamp`,
    `SentimentScore`). The output is enriched with the new sentiment and confidence
    values returned by Cognitive Services. The new confidence score can take values
    between `-1` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to sentiment detection, the Text Analytics API includes a number
    of other features. We can extract the most relevant key phrases from a text sample,
    in this case the tweet text. The `Cognition.Text.KeyPhraseProcessor` module can
    be used to extract the key phrases from the `Text` column, as in the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script uses the `CROSS APPLY` statement to produce a new row for
    every item in the key phrase list.
  prefs: []
  type: TYPE_NORMAL
- en: By merging the preceding script with the previous script, we have created a
    simple data pipeline that reads tweets from CSV files, calls the Cognitive Services
    Text Analytics API to find the tweet sentiment and key phrases, and finally, saves
    the results in ADLS.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Factory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Azure Data Factory** (**ADF**) is a cloud data integration platform that
    allows you to automate various data-related tasks, such as copying data between
    data stores, running analytical workloads, and retraining machine learning models.
    It supports a wide range of different data stores, including products from other
    vendors. Via its integration runtime model, ADF can also connect to on-premises
    locations such as self-hosted SQL databases.'
  prefs: []
  type: TYPE_NORMAL
- en: ADF can make use of many different types of computing resources in the Azure
    catalogue. These include Machine Learning Studio, ADLA, Databricks, and HDInsight.
    ADF can also make requests to any service that exposes a REST API, such as Cognitive
    Services.
  prefs: []
  type: TYPE_NORMAL
- en: Data Factory is developed with ADF Visual ...
  prefs: []
  type: TYPE_NORMAL
- en: Datasets, pipelines, and linked services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three main concepts in ADF are datasets, pipelines, and linked services.
    Datasets represent data that is stored in a specific location, such as a file
    in Blob Storage or a table in an SQL database. Pipelines are procedures that copy
    or modify data between datasets. Pipelines consist of a sequence of activities
    that make transformations to the input dataset and produce the output dataset
    as a result. The most simple pipeline consists of two datasets, the input and
    output datasets, and a copy activity between them. This simple pipeline could
    be used to move data between data stores, for example, from an on-premises SQL
    database to an Azure SQL database.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset definition itself contains information only about the format and
    the schema of data, not about the location of data. All the connection information
    is separated to modules called **linked services**. Linked ssrvices contain all
    the information needed for integration, such as connection strings, server addresses,
    and passwords. Every dataset must be associated to a linked service, otherwise,
    ADF would not know where the data resides. The linked services collection defines
    all the data sources that ADF can connect to. This collection includes many Azure,
    on-premises, and third-party products and services. The full list can be found
    in the ADF documentation and you can also request more connectors through the
    Azure feedback system, if your favorite service is missing.
  prefs: []
  type: TYPE_NORMAL
- en: Activities are tasks that move and transform data within the pipeline. These
    include control statements, such as loops and conditional execution, and computational
    tasks using various Azure services. The computational services include the Azure
    Batch service and many Machine Learning services, such as AML Studio, Azure Databricks,
    and Azure HDInsight. Therefore, Data Factory provides a great way to automate
    many AI-related tasks, including retraining ML models and running periodic analytical
    workloads. Similar to datasets, computational activities require a linked service
    definition that points to an existing computational resource in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: It is also good to note that the use of storage and computational resources
    is not included in the Data Factory billing model. Data Factory billing includes
    data management only, and data transfer costs and computational costs are billed
    per service on top of the Data Factory bill. As general advice, it is always good
    to test pipelines with very small datasets and keep an eye on the costs as the
    amount of data grows.
  prefs: []
  type: TYPE_NORMAL
- en: File format conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we'll show yet another way to perform a file format conversion in the
    cloud. To begin developing Data Factory pipelines, open the Data Factory Visual
    Tools portal and choose the *Author* tab from the left-hand menu. Create and configure
    the linked services for your Storage account(s), where the input and output data
    resides. After the linked services have been created successfully, create the
    datasets for the input files and output files and attach these datasets to the
    linked services.
  prefs: []
  type: TYPE_NORMAL
- en: When creating the dataset, pay attention that the configuration matches the
    format of your data. Also, make sure that you specify the data schema correctly.
  prefs: []
  type: TYPE_NORMAL
- en: When the datasets are configured, add a new pipeline to the Data Factory. ...
  prefs: []
  type: TYPE_NORMAL
- en: Automate U-SQL scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we developed U-SQL scripts to transfer and transform
    data in the cloud. The ADL Analytics engine does not include any automation functionality
    in itself, so we must use external services to automate such data workflows. ADF
    has the ability to trigger Data Lake Analytics jobs at regular intervals, so it
    is a good choice for automating U-SQL scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Completing this example requires ADLS and ADLA account, and a U-SQL script to
    run. The first step is to create an ADLS linked service. The ADLS linked service
    is used to store the U-SQL script to run. The U-SQL script should be uploaded
    to ADLS so that it can be read by Data Factory during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to authenticate with the ADLS instance: **Managed Service
    Identity** (**MSI**) and **Service Principal**. In this example, we will use the
    MSI authentication. The instructions for using the service principal authentication
    can be found in the documentation. When creating the linked service, take note
    of the service identity application ID, which is displayed on the screen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4f0064f-2d41-4581-80ca-00d3c1c72712.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to modify the Data Lake contents, the Data Factory instance must have
    the correct access rights to the Data Lake. To grant these access rights, copy
    the application ID onto the clipboard and navigate to the Data Lake Storage in
    the Azure portal. From the Data Explorer view, open the directory where you want
    to store your data. Click on Access and add new permissions using the +Add button,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f04ea11c-a300-4c90-be59-b248d6426a91.png)'
  prefs: []
  type: TYPE_IMG
- en: Use the service identity application ID to find the Data Factory account and
    grant all access rights (`rwx`) to this account. This gives Data Factory the rights
    to read and write files to the specified directory.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the destination directory, Data Factory also needs permissions
    for the parent directories. In this case, the `x` permission is sufficient. The
    easiest way to grant these permissions is to go to the root directory and grant
    the `x` access rights to the root and all its children directories. This means
    that the service account can navigate to all the subdirectories in the Data Lake,
    but it cannot read or write any files in these directories unless the `rw` rights
    are granted to these directories specifically. If you want Data Factory to be
    able to modify all files in the Data Lake, you can just grant `rwx` rights to
    the root directory and all its children directories at once. This way, you do
    not need to worry about the permissions anymore, since Data Factory has all the
    necessary permissions by default.
  prefs: []
  type: TYPE_NORMAL
- en: Next, create an ADLA linked service. For details about configuring the service
    principal authentication for ADLA, refer to the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to create a new pipeline and add a U-SQL activity to the pipeline.
    Configure the activity to use the previously created linked services for ADLS
    and ADLA, and specify the path where the U-SQL script can be found. Here is an
    example configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caaede63-35a5-4a34-ad2a-c835d681ea34.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the U-SQL module does not have any datasets associated with it. The
    input data and output data is specified in the U-SQL script, as explained in the
    section *Azure Data Lake Analytics*. It is possible to pass parameters to the
    U-SQL script, however. These can be specified under the advanced properties in
    the Script tab, and they will be available as variables in the U-SQL script. This
    is useful for passing filenames or table names for input and output data, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Running Databricks jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Databricks is a cloud-based machine learning service that is able to process
    heavy workloads with very high efficiency. Based on Apache Spark, it is designed
    for handling big data and high-throughput streams in real time. With ADF, it is
    possible to schedule Databricks jobs to run batch workloads periodically.
  prefs: []
  type: TYPE_NORMAL
- en: To complete this example, you need access to a Databricks Workspace. If you
    don't have an existing Databricks account, create one first in the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have access to a Databricks Workspace, open Data Factory Visual Tools
    and create a Linked Service for Databricks. The  Databricks linked service is
    of compute type. Choose the Databricks account to connect to and configure the
    linked services ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in this chapter, integrating Azure AI services with other non-AI
    services is easy and configuring these integrations can be done in a few simple
    steps. For codeless approach, Logic Apps and Data Factory provide tools to automate
    many data-related tasks. By leveraging AI services such as Cognitive Services
    or ML Studio Web Services, the incoming data can be enriched with insights and
    predictions produced by the AI services.
  prefs: []
  type: TYPE_NORMAL
- en: The trigger-based event handling system allows you to react to different kinds
    of events, for example when a new file is created or modified in cloud storage.
    The triggers can be used to launch data processing pipelines in scenarios where
    data moves infrequently and schedule-based data processing might introduce lags,
    since the system must wait for the scheduled time to lapse. With storage-based
    triggers, the data pipeline can be initiated automatically every time the source
    data is updated.
  prefs: []
  type: TYPE_NORMAL
- en: Data Lake Analytics is a batch processing engine that can make efficient queries
    against huge quantities of data. The U-SQL language combines SQL-style queries
    and C# commands as a highly flexible query language for Big Data. While the Data
    Lake Analytics engine does not contain an automation service in itself, Data Lake
    Analytics jobs can be launched from Data Factory by using the U-SQL module. This
    way all the triggers available in Data Factory can be used to automate Data Lake
    Analytics jobs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter will learn about Azure Machine Learning service launched by
    Microsoft.
  prefs: []
  type: TYPE_NORMAL
