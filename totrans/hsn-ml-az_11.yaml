- en: Integration with Other Azure Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与其他 Azure 服务集成
- en: In addition to using Azure AI services directly, Azure also provides options
    for using these services from other non-AI services. Many Azure AI services provide
    REST API interfaces that can be consumed from other services. AI services can
    thus be used as subcomponents of other apps to provide insights and predictions.
    Many non-AI services in Azure have built-in integration with AI services, so that
    AI components can often be added to apps with a few clicks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接使用 Azure AI 服务外，Azure 还提供了从其他非 AI 服务使用这些服务的选项。许多 Azure AI 服务提供 REST API
    接口，可以从其他服务中消费。因此，AI 服务可以用作其他应用程序的子组件，以提供洞察和预测。Azure 中的许多非 AI 服务都内置了对 AI 服务的集成，因此通常只需点击几下即可将
    AI 组件添加到应用程序中。
- en: Some AI services do not include any automation features. Recurring tasks, such
    as retraining ML models or running batch workloads, require integration with other
    services that offer these features. In the following sections, we will present
    various options for launching AI jobs automatically. In addition to traditional
    time-scheduled workloads, Azure services also provide objects called triggers
    to launch tasks after a certain event has occurred. Triggers allow services to
    react to events in a specific way, for example, processing the contents of a blob
    file after it has been created or modified.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 AI 服务不包含任何自动化功能。重复性任务，如重新训练机器学习模型或运行批量工作负载，需要与其他提供这些功能的服务集成。在接下来的章节中，我们将介绍自动启动
    AI 作业的各种选项。除了传统的按时间计划的工作负载外，Azure 服务还提供称为触发器的对象，在发生某些事件后启动任务。触发器允许服务以特定方式对事件做出反应，例如在创建或修改
    blob 文件后处理其内容。
- en: 'In this chapter, we will learn how to integrate Azure AI services with four
    non-AI services:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何将 Azure AI 服务与四个非 AI 服务集成：
- en: Logic Apps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑应用
- en: Azure Functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 函数
- en: Data Lake Analytics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖分析
- en: Data Factory
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工厂
- en: With these services, it is possible to build complex application pipelines where
    the ML model is just a part of the solution. In Azure, integration between different
    services is made as easy as possible, without compromising security. Getting results
    is therefore quick and efficient, and developing Azure applications can be a lot
    of fun!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些服务，可以构建复杂的应用程序管道，其中机器学习模型只是解决方案的一部分。在 Azure 中，不同服务之间的集成尽可能简单，不会牺牲安全性。因此，获取结果既快又高效，开发
    Azure 应用程序可以非常有趣！
- en: Logic Apps
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑应用
- en: Azure Logic Apps is a graphical tool for automating various types of tasks,
    such as getting data from a Web API and saving it in cloud storage. Logic Apps
    can be developed without writing a single line of code, so no programming skills
    are required. However, Logic Apps provides some basic functionality for programming
    languages, such as conditional execution and iterative loops.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 逻辑应用是一个用于自动化各种类型任务的图形工具，例如从 Web API 获取数据并将其保存到云存储中。逻辑应用可以在不编写任何代码的情况下进行开发，因此不需要编程技能。然而，逻辑应用为编程语言提供了一些基本功能，例如条件执行和循环。
- en: Logic Apps is meant for light-weight tasks that do not require complex logic
    or lightning-fast performance. Such tasks could include sending an email when
    a SharePoint list is modified, or copying files between Dropbox and OneDrive if
    the files have been modified.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑应用适用于轻量级任务，这些任务不需要复杂的逻辑或闪电般的性能。这些任务可能包括在 SharePoint 列表被修改时发送电子邮件，或者在 Dropbox
    和 OneDrive 之间复制文件（如果文件已被修改）。
- en: For AI development, Logic Apps provides a number of basic functionalities. There
    are built-in ...
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AI 开发，逻辑应用提供了一系列基本功能。有一些内置的 ...
- en: Triggers and actions
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 触发器和操作
- en: 'Logic Apps is based on two main concepts: triggers and actions. Triggers are
    listeners that are constantly waiting for a specific kind of event. These events
    could be created when a new file is created in a `Blob` folder, or they could
    occur every day at the same time to automate daily tasks. Actions are routines
    that are executed every time the trigger fires. Actions usually take some input
    data, process it, and finally, save or send the result somewhere.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑应用基于两个主要概念：触发器和操作。触发器是持续等待特定类型事件的监听器。这些事件可能是在`Blob`文件夹中创建新文件时产生的，或者它们可能每天在同一时间发生以自动化日常任务。操作是在触发器触发时执行的常规程序。操作通常需要一些输入数据，对其进行处理，最后将结果保存或发送到某个地方。
- en: 'Logic Apps supports all Azure Data Storage services: Data Lake Storage, Blob
    Storage, Cosmos DB, and so on. For example, the **Blob Storage** trigger executes
    Logic Apps every time a new file is added to a directory or a file in the directory
    is modified. We can then take the file as input to Logic Apps, process the contents
    of the file, and save the results back to the Blob.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Logic Apps 支持所有 Azure 数据存储服务：数据湖存储、Blob 存储、Cosmos DB 等。例如，**Blob 存储**触发器会在目录中添加新文件或目录中的文件被修改时执行
    Logic Apps。然后我们可以将文件作为输入传递给 Logic Apps，处理文件内容，并将结果保存回 Blob。
- en: 'The Logic Apps Designer contains many Cognitive Services actions: the Computer
    Vision API, Custom Vision, Text Analytics, and so on. To see the full list, search
    for the cognitive actions when creating an action, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Logic Apps 设计器包含许多认知服务操作：计算机视觉 API、自定义视觉、文本分析等。要查看完整列表，在创建操作时搜索认知操作，如下所示：
- en: '![](img/7680f7ee-2d1d-4ad0-ac61-e2f4a58ee4a8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7680f7ee-2d1d-4ad0-ac61-e2f4a58ee4a8.png)'
- en: 'There are also a few Azure Machine Learning Studio actions. These can be used
    to score examples or retrain ML models, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning Studio 中也有一些操作。这些操作可以用来评分示例或重新训练 ML 模型，如下所示：
- en: '![](img/7622647c-2a3b-44f5-a0d0-d6bb6467806d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7622647c-2a3b-44f5-a0d0-d6bb6467806d.png)'
- en: The connectors might have been updated since the time of writing, so check the
    latest information in the official documentation from Microsoft.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 连接器可能自编写时已经更新，因此请检查 Microsoft 官方文档中的最新信息。
- en: Twitter sentiment analysis
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter 情感分析
- en: Social media has become an everyday tool for many organizations. In addition
    to advertising themselves on these platforms, monitoring discussions about a company
    brand provides important information about how customers see its products and
    the image of the company.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体已成为许多组织的日常工具。除了在这些平台上进行自我推广外，监控有关公司品牌的讨论还可以提供有关客户如何看待其产品和公司形象的重要信息。
- en: 'In this section, we''ll show how to create an app that reads tweets containing
    the keyword *Azure* and saves those tweets in cloud storage. This is a real-world
    data acquisition scenario: once the tweets get stored in the cloud permanently,
    we have started to collect history data about the company''s brand. As a next
    step, we could start creating analytics and machine learning models based on this
    data. These models could include topic analysis or looking ...'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何创建一个应用程序，该应用程序读取包含关键字 *Azure* 的推文，并将这些推文保存到云存储中。这是一个真实的数据收集场景：一旦推文被永久存储在云中，我们就开始收集有关公司品牌的历史数据。作为下一步，我们可以开始基于这些数据创建分析和机器学习模型。这些模型可以包括主题分析或查看...
- en: Adding language detection
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加语言检测
- en: In the previous example, we created a workflow that reads tweets with a specific
    keyword, performs sentiment analysis on them, and saves the results in Blob Storage.
    If you completed the example and looked at the results more carefully, you might
    have noticed that the tweets can contain many different languages, because our
    keyword (Azure) is not specific to any language. In this example, we show how
    to add language detection to the pipeline and filter out all tweets that are not
    in a particular language.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们创建了一个工作流，该工作流读取包含特定关键字的推文，对这些推文进行情感分析，并将结果保存到 Blob 存储。如果您完成了示例并仔细查看结果，您可能会注意到推文中可能包含许多不同的语言，因为我们的关键字（Azure）并不特定于任何一种语言。在这个示例中，我们将展示如何将语言检测添加到管道中，并过滤掉所有不是特定语言的推文。
- en: To begin the example, create a new Logic App in the Azure portal, open the Logic
    Apps Designer, and choose the same trigger as a starting point as we did in the
    previous example (when a new tweet appears).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始示例，请在 Azure 门户中创建一个新的 Logic App，打开 Logic Apps 设计器，并选择与上一个示例相同的触发器作为起点（当出现新的推文时）。
- en: 'Add the Detect Language module from the Text Analytics API. For the Text field,
    choose the Tweet text parameter to be analyzed by the Text Analytics API. The
    output of this module will contain the list of languages detected from the tweet.
    Note that the default number of detected languages is 1\. This can be changed
    in the advanced options for the module, if the text is expected to contain more
    than one language:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文本分析 API 添加 Detect Language 模块。对于文本字段，选择要由文本分析 API 分析的 Tweet 文本参数。此模块的输出将包含从推文中检测到的语言列表。请注意，默认检测到的语言数量为
    1。如果预期文本包含多种语言，可以在模块的高级选项中更改此设置：
- en: '![](img/1a2ce9f1-19b5-4228-8e47-630586c251b3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1a2ce9f1-19b5-4228-8e47-630586c251b3.png)'
- en: Once the tweet language has been detected, we need to add a module that chooses
    only tweets that are in a certain language. For this purpose, create a new step
    in the pipeline, search for `Control` actions and choose the `Condition` action.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦检测到推文语言，我们需要添加一个模块，只选择特定语言的推文。为此，在管道中创建一个新的步骤，搜索`Control`操作并选择`Condition`操作。
- en: When the `Condition` module has been added to the pipeline, we must specify
    the filter condition using the three fields in the module. The first field indicates
    which value we would like to compare, the second field specifies the condition
    type, and the last field contains the value against which to compare. For the
    first field, choose the `detectedLanguages` item. Note that the detect language
    module returns the languages as a list, even if we are only detecting one language.
    The `detectedLanguages` item refers to each item in this list. Therefore, the
    Logic Apps Designer will add a new `For each` control module to the pipeline and
    the previously created `Condition` module will be placed inside the `For each`
    module. The `For each` module iterates through all the values in the `detectedLanguages`
    list, if there is more than one language detected.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当`Condition`模块添加到管道中时，我们必须使用模块中的三个字段指定过滤器条件。第一个字段表示我们想要比较的值，第二个字段指定条件类型，最后一个字段包含要比较的值。对于第一个字段，选择`detectedLanguages`项。请注意，检测语言模块将语言作为列表返回，即使我们只检测一种语言。`detectedLanguages`项指的是列表中的每个项。因此，Logic
    Apps 设计器将向管道添加一个新的`For each`控制模块，之前创建的`Condition`模块将被放置在`For each`模块内部。`For each`模块遍历`detectedLanguages`列表中的所有值，如果检测到多种语言。
- en: Now that our `Condition` module is placed inside the `For each` module, we can
    change the first field of the `Condition` module to `Language code`. The second
    field should be set to `is equal to` and the last field should contain the two-letter
    language code. To keep English language tweets only, we set the parameter to `en`.
    Note that the language detection module might not be able to detect all languages,
    so check the list of available languages in the latest Microsoft documentation.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们的`Condition`模块已经放置在`For each`模块内部，我们可以将`Condition`模块的第一个字段更改为`Language
    code`。第二个字段应设置为`is equal to`，最后一个字段应包含两位字母语言代码。为了仅保留英语语言的推文，我们将参数设置为`en`。请注意，语言检测模块可能无法检测到所有语言，因此请检查最新微软文档中提供的可用语言列表。
- en: 'After the condition module, the pipeline now branches into two directions,
    depending on whether the condition is `true` or not. In the `true` branch, we
    can add any processing steps for the English language tweets. In the `false` branch,
    we can specify what to do with the rest of the tweets. In this example, we use
    the sentiment detection pipeline that was developed in the previous example to
    process the English language tweets. We are not interested in the non-English
    tweets, so we can just ignore them by leaving the false branch empty. After completing
    all these steps, the pipeline appears as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在条件模块之后，管道现在根据条件是否为`true`分支为两个方向。在`true`分支中，我们可以为英语语言推文添加任何处理步骤。在`false`分支中，我们可以指定对剩余推文要执行的操作。在这个例子中，我们使用之前示例中开发的情感检测管道来处理英语语言推文。我们对非英语推文不感兴趣，因此我们可以通过留空`false`分支来忽略它们。完成所有这些步骤后，管道如下所示：
- en: '![](img/a75abd3f-8d0a-4d23-a880-5f6d9212cd8b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a75abd3f-8d0a-4d23-a880-5f6d9212cd8b.png)'
- en: Once you save the app, the trigger will start to run and the tweets will be
    saved to the Blob. Since we used the same modules for sentiment detection as we
    did in the previous example, the format of the files should be identical, with
    the exception that only English tweets are saved to the Blob.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 保存应用后，触发器将开始运行，并将推文保存到Blob中。由于我们使用了与之前示例中相同的情感检测模块，因此文件格式应相同，唯一不同的是只有英语推文被保存到Blob中。
- en: Azure Functions
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure Functions
- en: While Logic Apps provides a fast way to automate tasks, its collection of actions
    is limited to pre-selected options that cannot be customized. Moreover, the programmability
    of Logic Apps is quite limited, and the development of more complex programs is
    not necessarily any easier than writing code. If more flexibility is needed, it
    might be more productive to develop applications with Azure Functions. Functions
    can be developed with multiple programming languages familiar to web developers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Logic Apps 提供了一种快速自动化任务的方法，但其动作集合仅限于预选选项，无法进行自定义。此外，Logic Apps 的可编程性相当有限，开发更复杂的程序并不一定比编写代码更容易。如果需要更多灵活性，使用
    Azure Functions 开发应用程序可能更有效率。函数可以使用熟悉于 Web 开发者的多种编程语言进行开发。
- en: Azure Functions is a serverless coding platform in the cloud, where the underlying
    operating system has been virtualized. This means that many maintenance tasks,
    such as updating the operating system or language versions, is managed by ...
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Functions 是一个云中的无服务器编码平台，其中底层操作系统已经被虚拟化。这意味着许多维护任务，例如更新操作系统或语言版本，由 ...
    管理。
- en: Triggers
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 触发器
- en: 'Azure Functions triggers work with the same principle as the Logic Apps triggers
    introduced in the previous section: they listen to a certain event and initiate
    the function every time the event occurs. For example, an HTTP trigger executes
    the function every time the web URL of the function is called. A schedule trigger
    can execute the function periodically, for example, once a day. One of the biggest
    strengths of Azure Functions is the trigger collection. The trigger collection
    allows you to respond to many different types of events, and the code-based approach
    makes it possible to react to those events with the full flexibility of programming
    languages.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Functions 触发器的工作原理与上一节中介绍的 Logic Apps 触发器相同：它们监听特定事件，并在事件发生时启动函数。例如，HTTP
    触发器会在函数的 Web URL 被调用时执行函数。计划触发器可以定期执行函数，例如，每天执行一次。Azure Functions 最大的优势之一是其触发器集合。触发器集合允许您响应许多不同类型的事件，而基于代码的方法使得能够以编程语言的全部灵活性对这些事件做出反应。
- en: For data-related tasks, the most useful triggers are the storage triggers. Similar
    to how  Logic Apps can be triggered on blob events, Azure Functions can be triggered
    when files are added or updated in storage services such as Blob Storage or Cosmos
    DB. Consider a scheme where raw data is stored in the cloud, for example, JSON
    files that are produced by a web app, and we wish to convert those files into
    a columnar format to add them to a relational database, for example. Using Azure
    Functions with Blob Storage triggers, this task can be fully automated without
    any external services. Thus, Azure Functions provides another way to perform such
    format conversions, among other things.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据相关任务，最有用的触发器是存储触发器。类似于 Logic Apps 可以在 blob 事件上触发，Azure Functions 可以在存储服务（如
    Blob Storage 或 Cosmos DB）中文件被添加或更新时触发。考虑一种方案，其中原始数据存储在云中，例如，由 Web 应用程序生成的 JSON
    文件，我们希望将这些文件转换为列式格式以添加到关系型数据库中，例如。使用 Azure Functions 和 Blob Storage 触发器，这项任务可以完全自动化，无需任何外部服务。因此，Azure
    Functions 在其他方面提供了执行此类格式转换的另一种方式。
- en: One of the most interesting applications for Azure Functions is processing data
    from **Internet of Things** (**IoT**) devices. With event hub and IoT hub triggers,
    Azure Functions can be used as a downstream processing engine for data sent by
    IoT devices. The event hub and IoT Hub resources in Azure are data ingestion services
    that are capable of handling a large number of requests in a short time interval.
    They are designed for continuous data streams where the size of each message is
    small, but the number of messages can be large. Therefore, they are ideal services
    for receiving data from an array of IoT sensors that send their measurements in
    short intervals. The event hub and IoT hub triggers are set to fire every time
    a new event is received from the IoT devices, so we can use Azure Functions to
    define a processing routine for each event. This routine could include a scoring
    call to a machine learning service and saving the results in a database, for example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Functions最有趣的应用之一是处理来自**物联网**（**IoT**）设备的的数据。通过事件中心和IoT中心触发器，Azure Functions可以用作下游处理引擎，处理由IoT设备发送的数据。Azure中的事件中心和IoT中心资源是数据摄取服务，能够在短时间内处理大量请求。它们是为连续数据流设计的，其中每个消息的大小很小，但消息的数量可以很大。因此，它们是接收来自大量IoT传感器的数据的理想服务，这些传感器在短时间内发送它们的测量值。事件中心和IoT中心触发器被设置为在从IoT设备接收到新事件时触发，因此我们可以使用Azure
    Functions为每个事件定义一个处理程序。这个程序可以包括对机器学习服务的评分调用并将结果保存到数据库中，例如。
- en: 'For most trigger types, the trigger also passes some information about the
    event that fired the trigger. For example, a Blob Storage trigger passes the contents
    of the file that was created or changed, or a HTTP trigger passes the contents
    of a POST request. Handling different types of input data is made very simple
    in Azure Functions: depending on the source, a handle to the input data is passed
    as a parameter to the main function. In the following section, we will show an
    example how to read the contents of a blob file after the blob trigger has been
    fired.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数触发类型，触发器还会传递一些关于触发事件的详细信息。例如，Blob存储触发器会传递创建或更改的文件内容，或者HTTP触发器会传递POST请求的内容。在Azure
    Functions中处理不同类型的输入数据变得非常简单：根据来源，将输入数据的句柄作为参数传递给主函数。在下一节中，我们将展示一个示例，说明如何在blob触发器被触发后读取blob文件的内容。
- en: Blob-triggered function
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Blob触发的函数
- en: In this example, we'll demonstrate how to create a simple function that reads
    the contents of a blob file. We will not do any processing on the file; instead,
    we'll just print its contents in a log file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将演示如何创建一个简单的函数来读取blob文件的内容。我们不会对文件进行任何处理；相反，我们只需将其内容打印到日志文件中。
- en: When you create an Azure Functions service in the portal, it will create a number
    of resources in addition to the functions app itself. A storage account is needed
    to store logs produced by the functions app. The app service plan is a container
    resource that determines the pricing and the resource scaling of the app. Optionally,
    you can also create an app insights resource for monitoring your app usage. This
    is particularly useful for error analysis and tracking how often your app is triggered.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在门户中创建Azure Functions服务时，除了函数应用本身，它还会创建一些其他资源。需要一个存储帐户来存储函数应用产生的日志。应用服务计划是一个容器资源，它决定了应用的价格和资源扩展。可选地，你还可以创建一个应用洞察资源来监控你的应用使用情况。这对于错误分析和跟踪你的应用被触发的频率非常有用。
- en: To begin development, ...
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始开发，...
- en: Azure Data Lake Analytics
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure数据湖分析
- en: '**Azure Data Lake** (**ADL**) is Microsoft''s storage and analytics service
    for big data. It is capable of storing data on a petabyte scale and making efficient
    queries on the stored data. The storage and the analytics services are separate
    in Azure and the ADL service actually consists of two different products: **Azure
    Data Lake Storage** (**ADLS**) and **Azure Data Lake Analytics** (**ADLA**). In
    this section, we will focus on ADLA, but we will also touch on ADLS where appropriate.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure数据湖**（**ADL**）是微软为大数据提供的存储和分析服务。它能够存储PB级的数据，并对存储的数据进行高效的查询。在Azure中，存储和分析服务是分开的，ADL服务实际上由两个不同的产品组成：**Azure数据湖存储**（**ADLS**）和**Azure数据湖分析**（**ADLA**）。在本节中，我们将重点关注ADLA，但也会在适当的地方提及ADLS。'
- en: Data Lake Storage is a file-based storage, with files organized into directories.
    This type of storage is called schemaless, since there are no constraints on what
    type of data can be stored in the Data Lake. Directories can contain text files
    and images, and the data type is specified only when the data is read out from
    the Data Lake. This is particularly useful in big data scenarios where the amount
    of data written to the Data Lake is large and running the data validation steps
    on the fly would be too resource-consuming. The data validation steps can be incorporated
    in the queries later when the data is read out, or they can be run periodically
    in batches.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖存储是一种基于文件的存储，文件被组织到目录中。这种存储类型被称为无模式，因为在数据湖中存储的数据类型没有限制。目录可以包含文本文件和图像，数据类型仅在从数据湖读取数据时指定。这在大数据场景中特别有用，因为写入数据湖的数据量很大，实时运行数据验证步骤会消耗太多资源。数据验证步骤可以在读取数据时在查询中合并，或者可以定期批量运行。
- en: ADLA is the query engine that enables you to make efficient queries against
    the ADLS. An ADLA account always requires an ADLS account behind it. This primary
    account is the default source for queries. ADLA queries are written with U-SQL,
    an SQL-like language with some programming features borrowed from C#. Some examples
    of U-SQL scripts that query data from ADLS and Storage Blobs follow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ADLA是查询引擎，使您能够对ADLS进行高效查询。ADLA帐户始终需要一个ADLS帐户作为其后端。此主帐户是查询的默认源。ADLA查询使用U-SQL编写，这是一种类似于SQL的语言，它从C#借用了某些编程功能。以下是一些查询ADLS和存储Blob的U-SQL脚本示例。
- en: 'In addition to the primary ADLS source, the ADLA instance can have secondary
    sources as well. You can add other ADLS accounts or Azure Storage Blobs from the
    Data sources tab in the portal, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主要的ADLS源之外，ADLA实例还可以有二级源。您可以从门户的数据源选项卡添加其他ADLS帐户或Azure存储Blob，如下所示：
- en: '![](img/cae0e7ca-dd32-4a97-96c7-6ebb26417e7b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cae0e7ca-dd32-4a97-96c7-6ebb26417e7b.png)'
- en: Once an account has been registered as a data source, it can be used in queries
    similar to the primary ADLS account. By using multiple data sources, you can avoid
    moving the data to a single Data Lake if your data resides in multiple Storage
    accounts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将帐户注册为数据源，就可以像主要ADLS帐户一样在查询中使用它。通过使用多个数据源，如果您的数据位于多个存储帐户中，您可以避免将数据移动到单个数据湖。
- en: As a side note, it is always possible to create an ADL Storage account without
    associating an ADLA account with it. But the ADLS would only act as a file storage
    in that case, without the ability to make queries against the file contents. Also,
    nothing would prevent you from creating multiple ADLA accounts that all use the
    same ADLS account as their primary source. This flexibility is possible because
    ADLS and ADLA are separate products in the Azure portal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为旁注，始终可以创建一个不与ADLA帐户关联的ADL存储帐户。但在此情况下，ADLS只能作为文件存储，无法对文件内容进行查询。也不会阻止您创建多个ADLA帐户，这些帐户都使用相同的ADLS帐户作为其主源。这种灵活性是因为ADLS和ADLA在Azure门户中是独立的产品。
- en: One of the biggest strengths of ADL is its user permission controls. Permissions
    can be set separately for each file and directory. If some data in the Data Lake
    is confidential and should not be visible to all users, it can be placed in a
    separate directory and the directory can be made visible only to a select group
    of users. Data Lake is also integrated with Azure AD, and permissions can be assigned
    to Azure AD groups instead of specific users. Role-based access control will not
    be discussed further here, as the focus is on analytics. To add users to the ADLA
    account, they should be granted a Data Lake Analytics Developer role using the
    Add User Wizard in the ADLA instance in the Azure portal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ADL最大的优势之一是其用户权限控制。可以为每个文件和目录分别设置权限。如果数据湖中的某些数据是机密的，不应对所有用户可见，则可以将其放置在单独的目录中，并且该目录只能对选定用户组可见。数据湖还与Azure
    AD集成，可以将权限分配给Azure AD组而不是特定用户。基于角色的访问控制在此不再进一步讨论，因为重点是分析。要将用户添加到ADLA帐户，他们应该使用Azure门户中ADLA实例的添加用户向导授予数据湖分析开发者角色。
- en: The material in this section concerns Data Lake Storage Gen1 (Data Lake Store).
    ADLS Gen2 is in private preview at the time of writing. There might be changes
    in how ADLA works with Gen2, so check the latest information in the Microsoft
    documentation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的材料涉及数据湖存储Gen1（数据湖存储）。ADLS Gen2在撰写本文时处于私有预览阶段。ADLA与Gen2的工作方式可能会有所变化，因此请检查Microsoft文档中的最新信息。
- en: Developing with U-SQL
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用U-SQL进行开发
- en: As mentioned previously, ADLA queries are written in U-SQL, a query language
    developed by Microsoft specifically for big data. Similar to many other Azure
    services, queries can be written and executed in the Azure portal, which is useful
    for small and quick tasks. For advanced development, there are extensions for
    VS Code and Visual Studio IDEs that offer more functionality.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ADLA查询是用U-SQL编写的，这是一种由微软专门为大数据开发的查询语言。与其他许多Azure服务类似，查询可以在Azure门户中编写和执行，这对于小型和快速任务非常有用。对于高级开发，VS
    Code和Visual Studio IDEs有扩展，提供了更多功能。
- en: Although writing U-SQL queries is in many ways similar to writing SQL queries,
    there are some differences as well. ADLA does not produce interactive output to
    the query editor as it does in SQL Server Management Studio, for example. The
    results are always directed to a certain output, which is usually a file in ADLS.
    To check the results of the query, ...
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在许多方面编写U-SQL查询与编写SQL查询相似，但也存在一些差异。例如，ADLA不会像SQL Server Management Studio那样将交互式输出到查询编辑器，结果总是被定向到某个特定输出，通常是ADLS中的一个文件。要检查查询的结果，...
- en: U-SQL databases
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: U-SQL数据库
- en: Although ADLS is a file-based storage system, it also includes features that
    are familiar from relational databases, such as tables and views. An ADLA database
    is a collection of these objects. ADLA databases are useful for managing data
    in Data Lake Store, since they provide some useful features such as table indexes.
    On the other hand, ADLA database tables have strict schemas in place, so data
    must be validated before it can be entered into a table. Then, we have lost one
    of the biggest strengths of the Data Lake, namely the schemaless storage principle
    that was discussed at the beginning of the section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ADLS是一个基于文件的存储系统，但它也包含了一些来自关系数据库的熟悉功能，例如表和视图。ADLA数据库是这些对象的集合。ADLA数据库对于管理数据湖存储中的数据非常有用，因为它们提供了一些有用的功能，如表索引。另一方面，ADLA数据库表具有严格的架构，因此必须在将数据输入到表中之前对其进行验证。然后，我们就失去了数据湖最大的优势之一，即在章节开头讨论的无模式存储原则。
- en: When an ADLA resource is created, a master database is also created. This database
    is used as the default database for U-SQL queries. New databases can be created
    with the `CREATE DATABASE` command and a database can be changed with the `USE
    DATABASE` command. The contents of databases can be viewed in the catalog inside
    the ADLA Data Explorer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建ADLA资源时，也会创建一个主数据库。这个数据库用作U-SQL查询的默认数据库。可以使用`CREATE DATABASE`命令创建新数据库，并使用`USE
    DATABASE`命令更改数据库。可以在ADLA数据探索器中的目录中查看数据库的内容。
- en: 'There are two kinds of tables in U-SQL databases: managed and external tables.
    Managed tables are in many ways similar to SQL tables: a table consists of metadata
    (for example, table schema) and the data itself. In practice, the data is stored
    as structured files in a Data Lake Storage directory. Managed tables enforce a
    schema-on-write, meaning that it is not possible to enter data to the table unless
    it conforms to the table schema.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: U-SQL数据库中有两种类型的表：托管表和外部分布式表。托管表在很多方面与SQL表相似：一个表由元数据（例如，表架构）和数据本身组成。在实践中，数据以结构化文件的形式存储在数据湖存储目录中。托管表强制实施写时模式，这意味着除非数据符合表架构，否则无法向表中输入数据。
- en: 'External tables are in some ways similar to SQL views: only the metadata is
    stored in the database, but the data itself can reside outside of Data Lake. External
    tables can refer to various Azure services: Azure SQL Database, Azure SQL Data
    Warehouse, or SQL Servers on virtual machines. In contrast to managed tables,
    external tables enforce a schema-on-read: the format of the underlying data can
    change after the schema is defined, because the schema is not enforced at the
    time of writing. If the schema of the data changes, the external table definition
    can be altered to match the new data.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 外部分布式表在某种程度上类似于SQL视图：仅在数据库中存储元数据，但数据本身可以位于数据湖之外。外部分布式表可以引用各种Azure服务：Azure SQL数据库、Azure
    SQL数据仓库或虚拟机上的SQL服务器。与托管表相比，外部分布式表强制实施读时模式：在定义架构后，底层数据的格式可以更改，因为架构在写入时并未强制实施。如果数据架构发生变化，外部分布式表定义可以修改以匹配新的数据。
- en: Using a managed table instead of storing the data as files in directories has
    some advantages. For example, if your data is stored in a huge CSV file, it might
    be difficult to append new values to the file. This operation is possible with
    managed tables using the `INSERT` statement. (Note that the managed table is append
    only, without the possibility to update values.) Managed tables also have a clustered
    index defined, which can be used to make more effective queries.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管表而不是将数据作为文件存储在目录中有一些优点。例如，如果你的数据存储在一个巨大的CSV文件中，可能很难向文件中追加新值。使用`INSERT`语句，托管表可以实现此操作。（注意，托管表是追加只读的，没有更新值的可能性。）托管表还定义了一个聚集索引，可以用来进行更有效的查询。
- en: Simple format conversion for blobs
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的blob格式转换
- en: In this section, we'll demonstrate the execution of U-SQL scripts in the Azure
    portal and integration with secondary data sources (Blob Storage). We will read
    a **CSV** (short for **comma-separated value**) file from a Blob Storage and write
    the whole contents of the file to another blob file in a **TSV** (short for **tab-separated
    values**) format. In other words, we'll perform a file format conversion from
    CSV to TSV. Such format conversions are common in data management, since different
    applications often require data in different formats.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示在Azure门户中执行U-SQL脚本并与次要数据源（Blob存储）集成。我们将从Blob存储中读取**CSV**（代表**逗号分隔值**）文件并将整个文件内容写入另一个以**TSV**（代表**制表符分隔值**）格式存储的blob文件。换句话说，我们将执行从CSV到TSV的文件格式转换。这种格式转换在数据管理中很常见，因为不同的应用程序通常需要不同格式的数据。
- en: The requirements for running this example are a Blob Storage account (with some
    data), and ADLS and ADLA accounts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例的要求是有一个Blob存储账户（包含一些数据），以及ADLS和ADLA账户。
- en: To read the input data, the U-SQL module `Extractors` must be used. ...
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取输入数据，必须使用U-SQL模块`Extractors` ...
- en: Integration with Cognitive Services
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与认知服务的集成
- en: In previous sections, we saw how to integrate Logic Apps with Cognitive Services.
    We used the Text Analytics API to score the sentiment of each tweet and stored
    the results in Blob Storage. In this example, we'll show how to implement the
    same steps with ADLA. In addition to getting the sentiment score, we'll also extract
    key phrases for each tweet to get more insights.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何将逻辑应用与认知服务集成。我们使用了文本分析API来评估每条推文的情感并存储结果到Blob存储。在这个例子中，我们将展示如何使用ADLA实现相同的步骤。除了获取情感评分，我们还将提取每条推文的关键词以获得更多信息。
- en: Completing this example requires a Cognitive Services Text Analytics API account.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此示例需要一个认知服务文本分析API账户。
- en: 'Before Cognitive Services can be used in U-SQL scripts, the ADLA instance must
    be registered with Cognitive extensions. This can be done in the Azure portal
    by opening the ADLA instance and choosing the Sample scripts tab, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在U-SQL脚本中使用认知服务之前，必须将ADLA实例注册到认知扩展中。这可以通过在Azure门户中打开ADLA实例并选择示例脚本选项卡来完成，如下所示：
- en: '![](img/00917094-bc9e-4891-943c-ddc409c12f67.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00917094-bc9e-4891-943c-ddc409c12f67.png)'
- en: Once installation is successful, Cognitive extensions are ready to be used in
    U-SQL scripts. This means that the `REFERENCE ASSEMBLY` statement can now be used
    to import the Cognitive Services modules.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，认知扩展就准备好在U-SQL脚本中使用。这意味着现在可以使用`REFERENCE ASSEMBLY`语句来导入认知服务模块。
- en: 'In this example, the input comes from a directory that contains multiple files.
    The query can be limited to match only some of the files in the directory, according
    to the filename. In this case, all CSV files in the input directory are chosen.
    The input files have the same format as the files that were produced by the Logic
    App in a previous example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，输入来自一个包含多个文件的目录。查询可以限制为仅匹配目录中的某些文件，根据文件名。在这种情况下，选择了输入目录中的所有CSV文件。输入文件具有与之前示例中Logic
    App生成的文件相同的格式：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a new job and enter the following script:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的作业并输入以下脚本：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding script reads the input data using the `Extractors.Text()` module.
    The module options state that the values are separated by the pipe `|` character,
    the text values are not quoted, and that the `Extractor` should ignore all erroneous
    data instead of throwing an error and exiting (silent mode). This last property
    is useful if we don't need to worry about the errors.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The Cognitive Services modules are accessed through the `PROCESS` command. Since
    we referenced the assembly `TextSentiment` in the beginning of the script, the
    `Cognition.Text.SentimentAnalyzer` module is now available to be used for processing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the username, tweet text, tweet sentiment, and the sentiment score
    are saved to the output (Data Lake directory). The output file should look like
    the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that, in the preceding script, the output schema is defined after the `PRODUCE`
    statement. In this case, some fields in the input schema are ignored (`Timestamp`,
    `SentimentScore`). The output is enriched with the new sentiment and confidence
    values returned by Cognitive Services. The new confidence score can take values
    between `-1` and `1`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to sentiment detection, the Text Analytics API includes a number
    of other features. We can extract the most relevant key phrases from a text sample,
    in this case the tweet text. The `Cognition.Text.KeyPhraseProcessor` module can
    be used to extract the key phrases from the `Text` column, as in the following
    script:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding script uses the `CROSS APPLY` statement to produce a new row for
    every item in the key phrase list.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: By merging the preceding script with the previous script, we have created a
    simple data pipeline that reads tweets from CSV files, calls the Cognitive Services
    Text Analytics API to find the tweet sentiment and key phrases, and finally, saves
    the results in ADLS.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Factory
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Azure Data Factory** (**ADF**) is a cloud data integration platform that
    allows you to automate various data-related tasks, such as copying data between
    data stores, running analytical workloads, and retraining machine learning models.
    It supports a wide range of different data stores, including products from other
    vendors. Via its integration runtime model, ADF can also connect to on-premises
    locations such as self-hosted SQL databases.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: ADF can make use of many different types of computing resources in the Azure
    catalogue. These include Machine Learning Studio, ADLA, Databricks, and HDInsight.
    ADF can also make requests to any service that exposes a REST API, such as Cognitive
    Services.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Data Factory is developed with ADF Visual ...
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Datasets, pipelines, and linked services
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three main concepts in ADF are datasets, pipelines, and linked services.
    Datasets represent data that is stored in a specific location, such as a file
    in Blob Storage or a table in an SQL database. Pipelines are procedures that copy
    or modify data between datasets. Pipelines consist of a sequence of activities
    that make transformations to the input dataset and produce the output dataset
    as a result. The most simple pipeline consists of two datasets, the input and
    output datasets, and a copy activity between them. This simple pipeline could
    be used to move data between data stores, for example, from an on-premises SQL
    database to an Azure SQL database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The dataset definition itself contains information only about the format and
    the schema of data, not about the location of data. All the connection information
    is separated to modules called **linked services**. Linked ssrvices contain all
    the information needed for integration, such as connection strings, server addresses,
    and passwords. Every dataset must be associated to a linked service, otherwise,
    ADF would not know where the data resides. The linked services collection defines
    all the data sources that ADF can connect to. This collection includes many Azure,
    on-premises, and third-party products and services. The full list can be found
    in the ADF documentation and you can also request more connectors through the
    Azure feedback system, if your favorite service is missing.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Activities are tasks that move and transform data within the pipeline. These
    include control statements, such as loops and conditional execution, and computational
    tasks using various Azure services. The computational services include the Azure
    Batch service and many Machine Learning services, such as AML Studio, Azure Databricks,
    and Azure HDInsight. Therefore, Data Factory provides a great way to automate
    many AI-related tasks, including retraining ML models and running periodic analytical
    workloads. Similar to datasets, computational activities require a linked service
    definition that points to an existing computational resource in Azure.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: It is also good to note that the use of storage and computational resources
    is not included in the Data Factory billing model. Data Factory billing includes
    data management only, and data transfer costs and computational costs are billed
    per service on top of the Data Factory bill. As general advice, it is always good
    to test pipelines with very small datasets and keep an eye on the costs as the
    amount of data grows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: File format conversion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we'll show yet another way to perform a file format conversion in the
    cloud. To begin developing Data Factory pipelines, open the Data Factory Visual
    Tools portal and choose the *Author* tab from the left-hand menu. Create and configure
    the linked services for your Storage account(s), where the input and output data
    resides. After the linked services have been created successfully, create the
    datasets for the input files and output files and attach these datasets to the
    linked services.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: When creating the dataset, pay attention that the configuration matches the
    format of your data. Also, make sure that you specify the data schema correctly.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: When the datasets are configured, add a new pipeline to the Data Factory. ...
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Automate U-SQL scripts
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we developed U-SQL scripts to transfer and transform
    data in the cloud. The ADL Analytics engine does not include any automation functionality
    in itself, so we must use external services to automate such data workflows. ADF
    has the ability to trigger Data Lake Analytics jobs at regular intervals, so it
    is a good choice for automating U-SQL scripts.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Completing this example requires ADLS and ADLA account, and a U-SQL script to
    run. The first step is to create an ADLS linked service. The ADLS linked service
    is used to store the U-SQL script to run. The U-SQL script should be uploaded
    to ADLS so that it can be read by Data Factory during runtime.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to authenticate with the ADLS instance: **Managed Service
    Identity** (**MSI**) and **Service Principal**. In this example, we will use the
    MSI authentication. The instructions for using the service principal authentication
    can be found in the documentation. When creating the linked service, take note
    of the service identity application ID, which is displayed on the screen as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4f0064f-2d41-4581-80ca-00d3c1c72712.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'In order to modify the Data Lake contents, the Data Factory instance must have
    the correct access rights to the Data Lake. To grant these access rights, copy
    the application ID onto the clipboard and navigate to the Data Lake Storage in
    the Azure portal. From the Data Explorer view, open the directory where you want
    to store your data. Click on Access and add new permissions using the +Add button,
    as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f04ea11c-a300-4c90-be59-b248d6426a91.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Use the service identity application ID to find the Data Factory account and
    grant all access rights (`rwx`) to this account. This gives Data Factory the rights
    to read and write files to the specified directory.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the destination directory, Data Factory also needs permissions
    for the parent directories. In this case, the `x` permission is sufficient. The
    easiest way to grant these permissions is to go to the root directory and grant
    the `x` access rights to the root and all its children directories. This means
    that the service account can navigate to all the subdirectories in the Data Lake,
    but it cannot read or write any files in these directories unless the `rw` rights
    are granted to these directories specifically. If you want Data Factory to be
    able to modify all files in the Data Lake, you can just grant `rwx` rights to
    the root directory and all its children directories at once. This way, you do
    not need to worry about the permissions anymore, since Data Factory has all the
    necessary permissions by default.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Next, create an ADLA linked service. For details about configuring the service
    principal authentication for ADLA, refer to the documentation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to create a new pipeline and add a U-SQL activity to the pipeline.
    Configure the activity to use the previously created linked services for ADLS
    and ADLA, and specify the path where the U-SQL script can be found. Here is an
    example configuration:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caaede63-35a5-4a34-ad2a-c835d681ea34.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Note that the U-SQL module does not have any datasets associated with it. The
    input data and output data is specified in the U-SQL script, as explained in the
    section *Azure Data Lake Analytics*. It is possible to pass parameters to the
    U-SQL script, however. These can be specified under the advanced properties in
    the Script tab, and they will be available as variables in the U-SQL script. This
    is useful for passing filenames or table names for input and output data, for
    example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Running Databricks jobs
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Databricks is a cloud-based machine learning service that is able to process
    heavy workloads with very high efficiency. Based on Apache Spark, it is designed
    for handling big data and high-throughput streams in real time. With ADF, it is
    possible to schedule Databricks jobs to run batch workloads periodically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: To complete this example, you need access to a Databricks Workspace. If you
    don't have an existing Databricks account, create one first in the Azure portal.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Once you have access to a Databricks Workspace, open Data Factory Visual Tools
    and create a Linked Service for Databricks. The  Databricks linked service is
    of compute type. Choose the Databricks account to connect to and configure the
    linked services ...
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in this chapter, integrating Azure AI services with other non-AI
    services is easy and configuring these integrations can be done in a few simple
    steps. For codeless approach, Logic Apps and Data Factory provide tools to automate
    many data-related tasks. By leveraging AI services such as Cognitive Services
    or ML Studio Web Services, the incoming data can be enriched with insights and
    predictions produced by the AI services.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The trigger-based event handling system allows you to react to different kinds
    of events, for example when a new file is created or modified in cloud storage.
    The triggers can be used to launch data processing pipelines in scenarios where
    data moves infrequently and schedule-based data processing might introduce lags,
    since the system must wait for the scheduled time to lapse. With storage-based
    triggers, the data pipeline can be initiated automatically every time the source
    data is updated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Data Lake Analytics is a batch processing engine that can make efficient queries
    against huge quantities of data. The U-SQL language combines SQL-style queries
    and C# commands as a highly flexible query language for Big Data. While the Data
    Lake Analytics engine does not contain an automation service in itself, Data Lake
    Analytics jobs can be launched from Data Factory by using the U-SQL module. This
    way all the triggers available in Data Factory can be used to automate Data Lake
    Analytics jobs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter will learn about Azure Machine Learning service launched by
    Microsoft.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
