<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer188">
			<h1 id="_idParaDest-177"><a id="_idTextAnchor182"/><em class="italic">Chapter 8</em>: Behavioral Cloning</h1>
			<p>In this chapter, we are going to train a neural network to control the steering wheel of a car, effectively teaching it how to drive a car! Hopefully, you will be surprised by how simple the core of this task is, thanks to deep learning.</p>
			<p>To achieve our goal, we will have to modify one of the examples of the CARLA simulator, first to save the images required to create the dataset, then to use our neural network to drive. Our neural network will be inspired by the architecture of Nvidia DAVE-2, and we will also see how to better visualize where the neural network focuses its attention.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Teaching a neural network how to drive with behavioral cloning</li>
				<li>The Nvidia DAVE-2 neural network</li>
				<li>Recording images and the steering wheel from Carla</li>
				<li>Recording three video streams</li>
				<li>Creating the neural network</li>
				<li>Training a neural network for regression</li>
				<li>Visualizing the saliency maps</li>
				<li>Integrating with Carla for self-driving</li>
				<li>Training bigger datasets using generators</li>
			</ul>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor183"/>Technical requirements</h1>
			<p>To be able to use the code explained in this chapter, you need to have installed the following tools and modules:</p>
			<ul>
				<li>The Carla simulator</li>
				<li>Python 3.7</li>
				<li>The NumPy module</li>
				<li>The TensorFlow module</li>
				<li>The Keras module</li>
				<li>The <strong class="source-inline">keras-vis</strong> module</li>
				<li>The OpenCV-Python module</li>
				<li>A GPU (recommended)</li>
			</ul>
			<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter8</a>.</p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/3kjIQLA">https://bit.ly/3kjIQLA</a></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor184"/>Teaching a neural network how to drive with behavioral cloning</h1>
			<p>A self-driving car<a id="_idIndexMarker530"/> is a complicated ensemble of hardware and software. The hardware of a normal car is already very complex, usually with thousands of mechanical pieces, and a self-driving car adds many sensors to that. The software is not any simpler, and in fact, rumor has it that already 15 years ago, a world-class carmaker had to take a step back, because the complexity of the software was getting out of control. To give you an idea, a sports car can have more than 50 CPUs!</p>
			<p>Clearly, making a self-driving car that is safe and reasonably fast is an incredible challenge, but despite this, we will see how powerful a dozen of lines of code can be. For me, it was an enlightening moment to realize that something so complex as driving could be coded in such a simple way. But I should not have been surprised because, with deep learning, data is more important than the code itself, at least to a certain extent.</p>
			<p>We don't have the luxury of testing on a real self-driving car, so we will use Carla, and we will train a neural network that can generate the steering angle after having been fed the video of the camera. We are not using other sensors, though, in principle, you could use all the sensors that you can imagine, just modifying the network to accept this additional data.</p>
			<p>Our goal is to teach Carla how to make a <em class="italic">lap</em>, using a part of the <strong class="bold">Town04</strong> track, one of the tracks included in Carla. We want our neural network to drive a bit straight, and then make some turns to the right until it reaches the initial point. In principle, to teach the neural network, we just need to drive Carla, recording images of the road and the corresponding steering angle that we applied, a process called <strong class="bold">behavioral cloning</strong>.</p>
			<p>Our task is divided into three steps:</p>
			<ul>
				<li>Building the dataset</li>
				<li>Designing and training the neural network</li>
				<li>Integrating the neural network in Carla</li>
			</ul>
			<p>We are going to<a id="_idIndexMarker531"/> take inspiration from the DAVE-2 system, created by Nvidia. So, let's start describing it.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor185"/>Introducing DAVE-2</h1>
			<p>DAVE-2 is a system<a id="_idIndexMarker532"/> designed by Nvidia to train a neural network to drive a car, intended as a proof of concept to demonstrate that, in principle, a single neural network could be able to steer a car on a road. Putting it another way, our network could be trained to drive a real car on a real road, if enough data is provided. To give you an idea, Nvidia used around 72 hours of video, at 10 frames per second.</p>
			<p>The idea is very simple: we feed the neural network a video stream, and the neural network will simply generate the steering angle, or something equivalent. The training is created by a human driver, and the system collects data from the camera (training data) and from the steering wheel moved by the pilot (training labels). This is called <em class="italic">behavioral cloning</em> because the network is trying to clone the behavior of the human driver.</p>
			<p>Unfortunately, this would be a bit too simple, as most of the labels would simply be 0 (the driver going straight), so the network would have problems learning how to move to the middle of the lane. To alleviate this issue, Nvidia uses three cameras:</p>
			<ul>
				<li>One on the center of the car, which is the real human behavior</li>
				<li>One on the left, simulating what to do if the car is too much on the left</li>
				<li>One on the right, simulating what to do if the car is too much on the right</li>
			</ul>
			<p>For the left and right cameras to be useful, it is, of course, necessary to change the steering angle associated with their videos, to simulate a correction; so, the <em class="italic">left</em> camera needs to be associated with a turn <em class="italic">more to the right</em> and the <em class="italic">right</em> camera needs to be associated with a turn <em class="italic">more to the left</em>.</p>
			<p>The following diagram shows the system:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="Images/Figure_8.1_B16322.jpg" alt="Figure 8.1 – Nvidia DAVE-2 system" width="706" height="291"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Nvidia DAVE-2 system</p>
			<p>To make the system more robust, Nvidia adds random shift and rotation, adjusting the steering for it, but we will not do that. However, we are going to use three video streams, as suggested <a id="_idIndexMarker533"/>by them.</p>
			<p>How do we get the three video streams and the steering angle? Of course, from Carla, which we will use quite a lot during this chapter. Before starting to write some code, let's get familiar with <strong class="source-inline">manual_control.py</strong>, a file that we will copy and modify.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor186"/>Getting to know manual_control.py</h2>
			<p>Instead of<a id="_idIndexMarker534"/> writing a full client code to do what we need, we will change the <strong class="source-inline">manual_control.py</strong> file, from <strong class="source-inline">PythonAPI/examples</strong>.</p>
			<p>I will usually say where the code to alter is located, but you really need to check GitHub to see it.</p>
			<p>Before starting, please consider that the code of this chapter might be stricter than usual on the version requirements, in particular the visualization part, as it uses a library that has not been updated.</p>
			<p>My recommendation is to use Python 3.7 and to install TensorFlow version 2.2, Keras 2.3, and <strong class="source-inline">scipy</strong> 1.2, as follows:</p>
			<p class="source-code">pip install tensorflow==2.2.0 pip install keras==2.3.1 pip install scipy==1.2.3</p>
			<p>If you now look at <strong class="source-inline">manual_control.py</strong>, the first thing that you might notice is this block of code:</p>
			<p class="source-code">try:  sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (    sys.version_info.major,    sys.version_info.minor,    'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])except IndexError:  pass</p>
			<p>It loads an <strong class="source-inline">egg</strong> file containing<a id="_idIndexMarker535"/> the code for Carla, which is located in the <strong class="source-inline">PythonAPI/carla/dist/</strong> folder. As an alternative, you can also install Carla using a command such as the following, of course with the name of your <strong class="source-inline">egg</strong> file:</p>
			<p class="source-code">python -m easy_install carla-0.9.9-py3.7-win-amd64.egg</p>
			<p>After this, you will probably notice that the code is organized into the following classes:</p>
			<ul>
				<li><strong class="source-inline">World</strong>: The virtual world where our vehicle moves, which includes the map and all the actors (vehicles, pedestrians, and sensors).</li>
				<li><strong class="source-inline">KeyboardControl</strong>: This reacts to the keys pressed by the user, and it has some logic to convert the binary on/off keys for steering, braking, and accelerating to a wider range of values, based on how long they are pressed for, making the car much easier to control.</li>
				<li><strong class="source-inline">HUD</strong>: This renders all the information related to the simulation, such as speed, steering, and throttle, and it manages the notifications that can show some information to the user, for a few seconds.</li>
				<li><strong class="source-inline">FadingText</strong>: This class is used by the HUD class to show notifications that disappear after a few seconds.</li>
				<li><strong class="source-inline">HelpText</strong>: This class displays some text using <strong class="source-inline">pygame</strong>, a gaming library used by Carla.</li>
				<li><strong class="source-inline">CollisionSensor</strong>: This is a sensor that is able to detect collisions.</li>
				<li><strong class="source-inline">LaneInvasionSensor</strong>: This is a sensor that is able to detect that you crossed a lane line.</li>
				<li><strong class="source-inline">GnssSensor</strong>: This is a GPS/GNSS sensor that provides the GNSS position inside the OpenDRIVE map.</li>
				<li><strong class="source-inline">IMUSensor</strong>: This is the inertial measurement unit, which uses a gyroscope to detect the accelerations applied to the car.</li>
				<li><strong class="source-inline">RadarSensor</strong>: A radar, providing a two-dimensional map of the elements detected, including<a id="_idIndexMarker536"/> their speed.</li>
				<li><strong class="source-inline">CameraManager</strong>: This is a class that manages the camera and prints it.</li>
			</ul>
			<p>There are also a couple of other notable methods:</p>
			<ul>
				<li><strong class="source-inline">main()</strong>: This is mostly dedicated to parsing the arguments received by the OS.</li>
				<li><strong class="source-inline">game_loop()</strong>: This mostly initializes pygame, the Carla client, and all the related objects, and it also implements the game loop, where, 60 times per second, the keys are analyzed and the most updated image is shown on the screen.</li>
			</ul>
			<p>The visualization of the frame is triggered by <strong class="source-inline">game_loop()</strong>, with the following line:</p>
			<p class="source-code">world.render(display)</p>
			<p>The <strong class="source-inline">world.render()</strong> method calls <strong class="source-inline">CameraManager.render()</strong>, which displays the last frame available.</p>
			<p>If you checked the code, you may have noticed that Carla uses a weak reference to avoid circular <a id="_idIndexMarker537"/>references. A <strong class="bold">weak reference</strong> is a reference that does not prevent an object from being garbage-collected, which is useful in some scenarios, such as a cache.</p>
			<p>When you work with Carla, there is one important thing to consider. Some of your code runs on the server, while some of it runs on the client, and it might not be easy to draw a line between the two. This can have unintended consequences, such as your model running 10 to 30 times slower, probably because it is serialized to the server, though this is just my speculation after seeing this problem. For this reason, I run my inference in the <strong class="source-inline">game_loop()</strong> method, which surely runs on the client.</p>
			<p>This also means that the frames are computed on the server and sent to the client.</p>
			<p>An unfortunate additional thing to consider is that the API of Carla is not stable, and version 0.9.0 removed many functionalities that should be added back soon.</p>
			<p>The documentation is also not particularly updated with these missing APIs, so don't be surprised if things don't work as expected. Hopefully, this will be fixed soon. In the meantime, you can use an older version. We used Carla 0.9.9.2, and there are still some rough edges, but it is<a id="_idIndexMarker538"/> good enough for our needs.</p>
			<p>Now that we know more about CARLA, let's see how we can record our dataset, starting with only one video stream.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor187"/>Recording one video stream</h2>
			<p>In principle, recording <a id="_idIndexMarker539"/>one video stream with Carla is very simple, because there is already an option to do so. If you run <strong class="source-inline">manual_control.py</strong>, from the <strong class="source-inline">PythonAPI/examples</strong> directory, when you press <em class="italic">R</em>, it starts to record.</p>
			<p>The problem is that we also want the steering angle. Normally, you could save this data in a database of some type, a CSV file, or a pickle file. To keep things simpler and focused on the core task, we will just add the steering angle and some other data to the filename. This makes it a bit easier for you to build the dataset, as you might want to record multiple runs dedicated to fixing a specific problem, and you can just move the files to a new directory and easily preserve all the information without having to update the path on a database.</p>
			<p>But if you don't like it, feel free to use a better system.</p>
			<p>We could write a client from scratch that integrates with the Carla server and does what we need, but for simplicity and to better isolate the changes required, we will just copy <strong class="source-inline">manual_control.py</strong> to a file called <strong class="source-inline">manual_control_recording.py</strong>, and we will just add what we need. </p>
			<p>Please remember that this file should run in the <strong class="source-inline">PythonAPI/examples</strong> directory.</p>
			<p>The first thing that we want to do is change track to <strong class="source-inline">Town04</strong>, because it is more interesting than the default track:</p>
			<p class="source-code">client.load_world('Town04')</p>
			<p class="source-code">client.reload_world()</p>
			<p>The previous code needs to go in the <strong class="source-inline">game_loop()</strong> method.</p>
			<p>The variable client is clearly the client connecting to the Carla server.</p>
			<p>We also need to change the spawn point (the place where the simulation starts) to be fixed, <a id="_idIndexMarker540"/>because normally, this changes every time:</p>
			<p class="source-code">spawn_point = spawn_points[0] if spawn_points else carla.Transform()</p>
			<p>Now, we need to change the name of the file. While we are at it, we will not only save the steering angle, but also the throttle and the brake. We will not use them, but if you want to experiment, they will be there for you. The following method should be defined in the <strong class="source-inline">CameraManager</strong> class:</p>
			<p class="source-code">def set_last_controls(self, control):</p>
			<p class="source-code">    self.last_steer = control.steer</p>
			<p class="source-code">    self.last_throttle = control.throttle</p>
			<p class="source-code">    self.last_brake = control.brake</p>
			<p>Now, we can save the file as follows:</p>
			<p class="source-code">image.save_to_disk('_out/%08d_%s_%f_%f_%f.jpg' % (image.frame,     camera_name, self.last_steer, self.last_throttle,    self.last_brake))</p>
			<p>The <strong class="source-inline">image.frame</strong> variable contains the number of the current frame, and <strong class="source-inline">camera_name</strong> for now is not important, but it will have the <strong class="source-inline">MAIN</strong> value.</p>
			<p>The <strong class="source-inline">image</strong> variable also contains the current image that we want to save.</p>
			<p>You should get names similar to the following:</p>
			<p class="source-code">00078843_MAIN_0.000000_0.500000_0.000000.jpg</p>
			<p>In the previous file name, you can identify the following components:</p>
			<ul>
				<li>The frame number (<strong class="source-inline">00078843</strong>)</li>
				<li>The camera (<strong class="source-inline">MAIN</strong>)</li>
				<li>The steering angle (<strong class="source-inline">0.000000</strong>)</li>
				<li>The throttle (<strong class="source-inline">0.500000</strong>)</li>
				<li>The brake (<strong class="source-inline">0.000000</strong>)</li>
			</ul>
			<p>This is the<a id="_idIndexMarker541"/> image, in my case:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="Images/Figure_8.2_B16322.jpg" alt="Figure 8.2 – One frame from Carla, steering 0 degrees" width="480" height="320"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – One frame from Carla, steering 0 degrees</p>
			<p>This frame is fine, but not great. I should have stayed in another lane, or the steering should have been slightly pointed toward the right. In the case of behavioral cloning, the car learns from you, so how you drive is important. Controlling Carla with a keyboard is not great, and when recording, it works worse because of the time spent saving the images.</p>
			<p>The real problem is that <a id="_idIndexMarker542"/>we need to record three cameras, not just one. Let's see how to do that.</p>
			<h3>Recording three video streams</h3>
			<p>To record three <a id="_idIndexMarker543"/>video streams, the starting point is to have three cameras. </p>
			<p>By default, Carla has the following five cameras:</p>
			<ul>
				<li>A classical <em class="italic">third-person</em> view, from the back, above the car</li>
				<li>From the front of the car, toward the road (looking forward)</li>
				<li>From the front of the car, toward the car (looking backward)</li>
				<li>From far above</li>
				<li>From the left</li>
			</ul>
			<p>Here, you can see the first three cameras:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="Images/Figure_8.3_B16322.jpg" alt="Figure 8.3 – Cameras from above, toward the road, and toward the car" width="1650" height="318"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Cameras from above, toward the road, and toward the car</p>
			<p>The second camera looks very interesting to us. </p>
			<p>The following are taken from the remaining two cameras:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="Images/Figure_8.4_B16322.jpg" alt="Figure 8.4 – Carla cameras from far above and from the left" width="1650" height="480"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Carla cameras from far above and from the left</p>
			<p>The last camera is also somehow interesting, though we do not want to record the car in our frames. We are missing the camera from the right because, for some reason, the authors of Carla haven't added it to the list.</p>
			<p>Luckily, changing the cameras or adding a new one is quite simple. This is the definition of the original <a id="_idIndexMarker544"/>cameras, the <strong class="source-inline">CameraManager</strong> constructor:</p>
			<p class="source-code">bound_y = 0.5 + self._parent.bounding_box.extent.y</p>
			<p class="source-code">self._camera_transforms = [</p>
			<p class="source-code">    (carla.Transform(carla.Location(x=-5.5, z=2.5),        carla.Rotation(pitch=8.0)), Attachment.SpringArm),</p>
			<p class="source-code">    (carla.Transform(carla.Location(x=1.6, z=1.7)),         Attachment.Rigid),</p>
			<p class="source-code">    (carla.Transform(carla.Location(x=5.5, y=1.5, z=1.5)),        Attachment.SpringArm),</p>
			<p class="source-code">    (carla.Transform(carla.Location(x=-8.0, z=6.0),         carla.Rotation(pitch=6.0)), Attachment.SpringArm),</p>
			<p class="source-code">    (carla.Transform(carla.Location(x=-1, y=-bound_y, z=0.5)),        Attachment.Rigid)]</p>
			<p>As a first attempt, we can keep just the second and the fifth camera, but we want them at comparable positions. Carla has been written using a very famous engine for video-games: <em class="italic">Unreal Engine 4</em>. In <em class="italic">Unreal Engine</em>, the <em class="italic">z</em> axis is the vertical one (up and down), the <em class="italic">x</em> axis is for forward and backward, and the <em class="italic">y</em> axis is for lateral movements, left and right. So, we want the cameras to have the same <em class="italic">x</em> and <em class="italic">z</em> coordinates. We also want a third camera, from the right. For this, it is enough to change the sign of the <em class="italic">y</em> coordinates. This is the resulting code, only for the cameras:</p>
			<p class="source-code">(carla.Transform(carla.Location(x=1.6, z=1.7)), Attachment.Rigid),(carla.Transform(carla.Location(x=1.6, y=-bound_y, z=1.7)),    Attachment.Rigid),(carla.Transform(carla.Location(x=1.6, y=bound_y, z=1.7)),    Attachment.Rigid)</p>
			<p>You could probably <a id="_idIndexMarker545"/>stop here. I ended up moving the lateral cameras more to the side, which can be done by changing <strong class="source-inline">bound_y</strong>:</p>
			<p class="source-code">bound_y = 4</p>
			<p>These are the images that we get now:</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="Images/Figure_8.5_B16322.jpg" alt="Figure 8.5 – New cameras: from the left, from the front (main camera), and from the right" width="1650" height="321"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – New cameras: from the left, from the front (main camera), and from the right</p>
			<p>Now, it should be easier to understand that the left and right cameras can be used to teach the neural network how to correct the trajectory, if it is not in the correct location, compared to the main camera. This, of course, assumes that the stream recorded by the main camera is the intended position.</p>
			<p>Even if the correct cameras are now available, they are not in use. We need to add them, in <strong class="source-inline">World. restart()</strong>, as follows:</p>
			<p class="source-code">self.camera_manager.add_camera(1)self.camera_manager.add_camera(2)</p>
			<p>The <strong class="source-inline">CameraManager.add_camera()</strong> method is defined as follows:</p>
			<p class="source-code">camera_name = self.get_camera_name(camera_index)if not (camera_index in self.sensors_added_indexes):    sensor = self._parent.get_world().spawn_actor(                self.sensors[self.index][-1],                self._camera_transforms[camera_index][0],                attach_to=self._parent,                attachment_type=self._camera_transforms[camera_index][1])        self.sensors_added_indexes.add(camera_index)        self.sensors_added.append(sensor)        # We need to pass the lambda a weak reference to self to avoid         # circular reference.        weak_self = weakref.ref(self)        sensor.listen(lambda image: CameraManager._save_image(weak_self,         image, camera_name))</p>
			<p>What this code<a id="_idIndexMarker546"/> does is the following:</p>
			<ol>
				<li>Sets up a sensor, using the specified camera</li>
				<li>Adds the sensor to a list</li>
				<li>Instructs the sensor to call a lambda function that invokes the <strong class="source-inline">save_image()</strong> method </li>
			</ol>
			<p>The following <strong class="source-inline">get_camera_name()</strong> method is used to get a meaningful name to the camera, based on its index, which is dependent on the cameras that we defined earlier:</p>
			<p class="source-code">def get_camera_name(self, index):    return 'MAIN' if index == 0 else ('LEFT' if index == 1 else         ('RIGHT' if index == 2 else 'UNK'))</p>
			<p>Before looking at the code of <strong class="source-inline">save_image()</strong>, let's discuss a small issue. </p>
			<p>Recording three cameras<a id="_idIndexMarker547"/> for every frame is kind of slow, resulting in low <strong class="bold">Frames Per Second</strong> (<strong class="bold">FPS</strong>), which makes it difficult to drive the car. As a consequence, you would over-correct, recording a sub-optimal dataset where you basically teach the car how to zig-zag. To limit this problem, we will record only one camera view for each frame, then we rotate to the next camera view for the next frame, and we will cycle through all three camera views during recording. After all, consecutive frames are similar, so it is not a huge problem. </p>
			<p>The camera used by Nvidia was recording at 30 FPS, but they decided to skip most of the frames, recording only at 10 FPS, because the frames were very similar, increasing the training time without adding much information. You would not record at the highest speed, but your dataset would be better, and if you want a bigger dataset, you can always just drive more. </p>
			<p>The <strong class="source-inline">save_image()</strong> function needs to first check whether this is a frame that we want to record:</p>
			<p class="source-code">if self.recording:</p>
			<p class="source-code">    n = image.frame % 3</p>
			<p class="source-code"> </p>
			<p class="source-code">    # Save only one camera out of 3, to increase fluidity</p>
			<p class="source-code">    if (n == 0 and camera_name == 'MAIN') or (n == 1 and         camera_name == 'LEFT') or (n == 2 and camera_name ==            'RIGHT'):</p>
			<p class="source-code">       # Code to convert, resize and save the image</p>
			<p>The second step is to <a id="_idIndexMarker548"/>convert the image into a format suitable for OpenCV, as we are going to use it to save the image. We need to convert the raw buffer to NumPy, and we also need to drop one channel, because Carla produces images with BGRA, with four channels: blue, green, red, and alpha (transparency):</p>
			<p class="source-code">img = np.frombuffer(image.raw_data, dtype=np.dtype('uint8'))</p>
			<p class="source-code">img = np.reshape(img, (image.height, image.width, 4))</p>
			<p class="source-code">img = img[:, :, :3]</p>
			<p>Now, we can resize the image, crop the part that we need, and save it:</p>
			<p class="source-code">img = cv2.resize(img, (200, 133))</p>
			<p class="source-code">img = img[67:, :, :]</p>
			<p class="source-code"> </p>
			<p class="source-code">cv2.imwrite('_out/%08d_%s_%f_%f_%f.jpg' % (image.frame, camera_name,   self.last_steer, self.last_throttle, self.last_brake), img). </p>
			<p>You can see in the code repository in GitHub that I recorded a fair amount of frames, enough to drive for one or two turns, but if you want to drive along the whole track, you will need many <a id="_idIndexMarker549"/>more frames, and the better you drive, the better it is.</p>
			<p>Now that we have the cameras, we need to use them to build the dataset that we need.</p>
			<h3>Recording the dataset</h3>
			<p>To build the <a id="_idIndexMarker550"/>dataset, clearly, you need to record at least the turns that you expect your network to make. The more the better. But you should also record movements that help your car correct the trajectory. The left and right camera already help quite a lot, but you should also record a few stints where the car is close to the edge of the road, and the steering wheel is turning it toward the center.</p>
			<p>For example, consider something like the following:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="Images/Figure_8.6_B16322.jpg" alt="Figure 8.6 – Car close to the left, steering to the right" width="480" height="320"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Car close to the left, steering to the right</p>
			<p>If there are turns that don't go the way you want, you can try to record them more than once, as I did.</p>
			<p>Now, you might see the advantage of encoding the steering wheel in the name of the image. You can group these correction stints, or whatever you prefer, on dedicated directories, and take them in and out of the dataset as required.</p>
			<p>If you want, you can even manually select a part of the pictures, to correct for wrong steering angles, though this might not be necessary if there is a limited number of frames with the wrong angle.</p>
			<p>Despite saving only one camera per frame, you might still find it difficult to drive, above all in regard to the speed. I personally prefer to limit the throttle so that the car does not go too fast, but I can still slow down if I want.</p>
			<p>The throttle can usually<a id="_idIndexMarker551"/> reach the value of <strong class="source-inline">1</strong>, so to limit it, it is enough to use a line of code similar to the following, in the <strong class="source-inline">KeyboardControl ._parse_vehicle_keys()</strong> method:</p>
			<p class="source-code">self._control.throttle = min(self._control.throttle + 0.01, 0.5)</p>
			<p>To increase fluidity, you might run the client with a lower resolution:</p>
			<p class="source-code">python manual_control_packt.py --res 480x320</p>
			<p>You can also lower the resolution of the server, as follows:</p>
			<p class="source-code">CarlaUE4  -ResX=480-ResY=320</p>
			<p>Now that you have the raw dataset, it's time to create the real dataset, with the proper steering angles.</p>
			<h3>Preprocessing the dataset</h3>
			<p>The dataset that we <a id="_idIndexMarker552"/>recorded is raw, meaning that it needs some preprocessing before being ready to be used.</p>
			<p>The most important thing to do is to correct the steering angle for the left and right camera. </p>
			<p>For convenience, this is done by an additional program so that you can eventually change it without having to record the frames again.</p>
			<p>To start, we need a method to extract the data from the name (we assume the file is a JPG or a PNG):</p>
			<p class="source-code">def expand_name(file):</p>
			<p class="source-code">    idx = int(max(file.rfind('/'), file.rfind('\\')))</p>
			<p class="source-code">    prefix = file[0:idx]</p>
			<p class="source-code">    file = file[idx:].replace('.png', '').replace('.jpg', '')</p>
			<p class="source-code">    parts = file.split('_')</p>
			<p class="source-code"> </p>
			<p class="source-code">    (seq, camera, steer, throttle, brake, img_type) = parts</p>
			<p class="source-code"> </p>
			<p class="source-code">    return (prefix + seq, camera, to_float(steer),        to_float(throttle), to_float(brake), img_type)</p>
			<p>The <strong class="source-inline">to_float</strong> method<a id="_idIndexMarker553"/> is just a convenience to convert -0 to 0.</p>
			<p>Now, changing the steering angle is simple:</p>
			<p class="source-code">(seq, camera, steer, throttle, brake, img_type) = expand_name(file_name)</p>
			<p class="source-code"> </p>
			<p class="source-code">    if camera == 'LEFT':</p>
			<p class="source-code">        steer = steer + 0.25</p>
			<p class="source-code">    if camera == 'RIGHT':</p>
			<p class="source-code">        steer = steer - 0.25</p>
			<p>I added a correction of 0.25. If your camera is closer to the car, you might want to use a smaller number.</p>
			<p>While we are at it, we can also add the frames mirrored, to increase the size of the dataset a bit. </p>
			<p>Now that we have converted the dataset, we are ready to train a neural network similar to DAVE-2 to learn <a id="_idIndexMarker554"/>how to drive.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor188"/>Modeling the neural network</h2>
			<p>To create our <a id="_idIndexMarker555"/>neural network, we will take inspiration from DAVE-2, which is a surprisingly simple neural network:</p>
			<ul>
				<li>We start with a lambda layer, to confine the image pixels in the (-1, +1) range:<p class="source-code">model = Sequential()</p><p class="source-code">model.add(Lambda(lambda x: x/127.5 - 1., input_shape=(66, 200, 3)))</p></li>
				<li>Then, there are three convolutional layers with kernel size <strong class="source-inline">5</strong> and strides <strong class="source-inline">(2,2)</strong>, which halves the output resolution, and three convolutional layers with kernel size <strong class="source-inline">3</strong>:<p class="source-code">model.add(Conv2D(24, (5, 5), strides=(2, 2), activation='elu'))</p><p class="source-code">model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='relu'))</p><p class="source-code">model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='relu'))</p><p class="source-code"> </p><p class="source-code">model.add(Conv2D(64, (3, 3), activation='relu'))</p><p class="source-code">model.add(Conv2D(64, (3, 3), activation='relu'))</p></li>
				<li>Then, we have the dense layers:<p class="source-code">model.add(Flatten())</p><p class="source-code">model.add(Dense(1164, activation='relu'))</p><p class="source-code">model.add(Dense(100, activation='relu'))</p><p class="source-code">model.add(Dense(50, activation='relu'))</p><p class="source-code">model.add(Dense(10, activation='relu'))</p><p class="source-code">model.add(Dense(1, activation='tanh'))</p></li>
			</ul>
			<p>I am always amazed when I think that these few lines of code are enough to somehow allow a car to drive by itself on a real road!</p>
			<p>While it looks more or less similar to other neural networks that we saw before, there is a very important difference—the last activation is not a softmax function, because this is not a classifier, but a neural network that needs to perform a <em class="italic">regression</em> task, predicting the correct steering angle given an image. </p>
			<p>We say that a neural network is performing a regression when it is trying to predict a value in a potentially continuous interval—for example, between –1 and +1. By comparison, in a classification task, the neural network is trying to predict which label is more likely correct and probably represents the content of the image. A neural network that can distinguish between cats and dogs is therefore a classifier, while a network that tries to predict the cost<a id="_idIndexMarker556"/> of an apartment based on the size and location is performing regression.</p>
			<p>Let's see what we need to change to use a neural network for regression.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor189"/>Training a neural network for regression</h2>
			<p>As we have <a id="_idIndexMarker557"/>already seen, a difference is the lack of a softmax <a id="_idIndexMarker558"/>layer. In its place, we used Tanh (hyperbolic tangent), an activation useful to generate values in the range (-1, +1), which is the range that we need for the steering angle. However, in principle, you could not even have an activation and directly use the value of the last neuron.</p>
			<p>The following figure shows the Tanh function:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="Images/Figure_8.7_B16322.jpg" alt="Figure 8.7 – The tanh function" width="879" height="438"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – The tanh function</p>
			<p>As you can see, Tanh will limit the range of the activation to the (-1, +1) range.</p>
			<p>Usually, when we train a classifier, as in the case of MNIST or CIFAR-10, we use <strong class="source-inline">categorical_crossentropy</strong> as a loss and <strong class="source-inline">accuracy</strong> as a metric. However, for regression, we <a id="_idIndexMarker559"/>need to use <strong class="source-inline">mse</strong> for the loss and we can <a id="_idIndexMarker560"/>optionally use <strong class="source-inline">cosine_proximity</strong> as a metric.</p>
			<p>The cosine proximity is an indication of similarity for vectors. So, 1 means that they are identical, 0 that they are perpendicular, and -1 that they are opposite. The loss and metric code snippet looks as follows:</p>
			<p class="source-code">model.compile(loss=mse, optimizer=Adam(), metrics=    ['cosine_proximity'])</p>
			<p>The rest of the code is as it is for classifiers, except that we don't need to use one-hot encoding.</p>
			<p>Let's see the graph for the training:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="Images/Figure_8.8_B16322.jpg" alt="Figure 8.8 – Behavioral cloning with DAVE-2, training" width="392" height="341"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Behavioral cloning with DAVE-2, training</p>
			<p>You can <a id="_idIndexMarker561"/>see slight overfitting. This is the value of the<a id="_idIndexMarker562"/> losses:</p>
			<p class="source-code">Min Loss: 0.0026791724107401277</p>
			<p class="source-code">Min Validation Loss: 0.0006011795485392213</p>
			<p class="source-code">Max Cosine Proximity: 0.72493887</p>
			<p class="source-code">Max Validation Cosine Proximity: 0.6687041521072388</p>
			<p>The loss, in this case, is the mean square error between the steering angle recorded for training and the angle computed by the network. We can see that the validation loss is quite good. If you have time, you can try to experiment with the model, adding dropouts or even changing the whole structure.</p>
			<p>Soon, we will integrate our neural network with Carla and see how it drives, but before that, it could be legit to wonder whether the neural network is actually focusing its attention on the right parts of the road. The next section will show us how to do this, using a technique called <strong class="bold">saliency maps</strong>.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor190"/>Visualizing the saliency maps</h2>
			<p>To understand <a id="_idIndexMarker563"/>what the neural network is focusing its attention on, we should use a practical example, so let's choose an image:</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="Images/Figure_8.9_B16322.jpg" alt="Figure 8.9 – Test image" width="479" height="160"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Test image</p>
			<p>If we had to drive on this road, as humans, we would pay attention to the lanes and the wall, though admittedly, the wall is not as important as the last lane is before that.</p>
			<p>We already know<a id="_idIndexMarker564"/> how to get an idea of what a <strong class="bold">CNN</strong> (short for <strong class="bold">convolutional neural network</strong>) such as DAVE-2 is taking into consideration: as the output of a convolution layer is an image, we can visualize it as follows:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="Images/Figure_8.10_B16322.jpg" alt="Figure 8.10 – Part of the activations of the first convolutional layer" width="493" height="42"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Part of the activations of the first convolutional layer</p>
			<p>This is a good starting point, but we would like something more. We would like to understand which pixels contribute the most to the prediction. For that, we need to get a <strong class="bold">saliency map</strong>.</p>
			<p>Keras does not directly support them, but we can use <strong class="source-inline">keras-vis</strong>. You can install it with <strong class="source-inline">pip</strong>, as follows:</p>
			<p class="source-code">sudo pip install keras-vis</p>
			<p>The first step to get a saliency map is to create a model that starts with the input of our model but ends with the layer that we want to analyze. The resulting code is very similar to what we saw for the activations, except that for convenience, we also need the index of the layer:</p>
			<p class="source-code">conv_layer, idx_layer = next((layer.output, idx) for idx, layer in   enumerate(model.layers) if layer.output.name.startswith(name))</p>
			<p class="source-code">act_model = models.Model(inputs=model.input, outputs=[conv_layer])</p>
			<p>While not necessary in our case, you might want to change the activation to become linear, then reload the model:</p>
			<p class="source-code">conv_layer.activation = activations.linear</p>
			<p class="source-code">sal_model = utils.apply_modifications(act_model)</p>
			<p>Now, it is just a matter of calling <strong class="source-inline">visualize_saliency()</strong>:</p>
			<p class="source-code">grads = visualize_saliency(sal_model, idx_layer,    filter_indices=None, seed_input=img)</p>
			<p class="source-code">plt.imshow(grads, alpha=.6)</p>
			<p>We are interested in the saliency map of the last layer, the output, but as an exercise, we will go through all the convolutional layers to see what they understand.</p>
			<p>Let's see the saliency<a id="_idIndexMarker565"/> map for the first convolutional layer:</p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="Images/Figure_8.11_B16322.jpg" alt="Figure 8.11 – Saliency map of the first convolutional layer" width="1178" height="421"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Saliency map of the first convolutional layer</p>
			<p>Not very impressive, as there is no saliency and we only see the original image. </p>
			<p>Let's see how the map of the second layer looks:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="Images/Figure_8.12_B16322.jpg" alt="Figure 8.12 – Saliency map of the second convolutional layer" width="1167" height="416"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Saliency map of the second convolutional layer</p>
			<p>This is an improvement, but even if we see some attention in the middle line, on the wall and the land <a id="_idIndexMarker566"/>after the right lane, it is not very clear. Let's see the third layer:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="Images/Figure_8.13_B16322.jpg" alt="Figure 8.13 – Saliency map of the third convolutional layer" width="1174" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Saliency map of the third convolutional layer</p>
			<p>Now we are talking! We can see great attention on the central and left line, and some attention focused on the wall and the right line. The network seems to be trying to understand where the road ends. Let's also see the fourth layer:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="Images/Figure_8.14_B16322.jpg" alt="Figure 8.14 – Saliency map of the fourth convolutional layer" width="1175" height="417"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Saliency map of the fourth convolutional layer</p>
			<p>Here, we can see that the attention is mostly focused on the central line, but there are also sparks of attention on the left line and on the wall, as well as a bit on the whole road.</p>
			<p>We can also check<a id="_idIndexMarker567"/> the fifth and last convolutional layer:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="Images/Figure_8.15_B16322.jpg" alt="Figure 8.15 – Saliency map of the fifth convolutional layer" width="1170" height="416"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Saliency map of the fifth convolutional layer</p>
			<p>The fifth layer is similar to the fourth layer, plus with some more attention on the left line and on the wall.</p>
			<p>We can also visualize the saliency map for dense layers. Let's see the result for the last layer, which is what we consider the real saliency map for this image:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="Images/Figure_8.16_B16322.jpg" alt="Figure 8.16 – Saliency map of the output layer" width="1173" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Saliency map of the output layer</p>
			<p>The last saliency map, the most important one, shows great attention to the central line and the right line, plus some attention on the upper-right corner, which could be an attempt to estimate <a id="_idIndexMarker568"/>the distance from the right lane. We can also see some attention on the wall and the left lane. So, all in all, it seems promising.</p>
			<p>Let's try with another image:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="Images/Figure_8.17_B16322.jpg" alt="Figure 8.17 – Second test image" width="200" height="66"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17 – Second test image</p>
			<p>This is an interesting image, as it is taken from a part of the road where the network has not been trained, but it still behaved very well.</p>
			<p>Let's see the saliency map of the third convolutional layer:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="Images/Figure_8.18_B16322.jpg" alt="Figure 8.18 – Saliency map of the third convolutional layer" width="1177" height="419"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – Saliency map of the third convolutional layer</p>
			<p>The neural network seems very concerned with the end of the road and it seems to have detected a couple of trees as well. If it was trained for braking, I bet it would do so!</p>
			<p>Let's see the final map:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="Images/Figure_8.19_B16322.jpg" alt="Figure 8.19 – Saliency map of the output layer" width="1173" height="418"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – Saliency map of the output layer</p>
			<p>This is pretty similar<a id="_idIndexMarker569"/> to the previous one, but there is some attention to the central line and the right line, and a tiny amount on the road in general. Looks good to me.</p>
			<p>Let's try with the last image, taken from the training to teach when to turn right:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="Images/Figure_8.20_B16322.jpg" alt="Figure 8.20 – Third test image" width="200" height="66"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.20 – Third test image</p>
			<p>This is the final saliency map for it:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="Images/Figure_8.21_B16322.jpg" alt="Figure 8.21 – Saliency map of the output layer" width="1171" height="420"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.21 – Saliency map of the output layer</p>
			<p>You can see that the neural network is giving attention mostly to the right line, also keeping an eye on the whole road and with some spark of attention dedicated to the left line.</p>
			<p>As you can see, the saliency map can be a valid tool to understand the behavior of the network a bit more <a id="_idIndexMarker570"/>and do a kind of sanity check on its interpretation of the world.</p>
			<p>Now it is finally time to integrate with Carla and see how we are performing in the real world. Fasten your seatbelt, because we are going to drive, and our neural network will be in the driver's seat!</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor191"/>Integrating the neural network with Carla</h1>
			<p>We will now<a id="_idIndexMarker571"/> integrate our neural network with Carla, to<a id="_idIndexMarker572"/> achieve self-driving. </p>
			<p>As before, we start by making a copy of <strong class="source-inline">manual_control.py</strong>, which we could call <strong class="source-inline">manual_control_drive.py</strong>. For simplicity, I will only write the code that you need to change or add, but you can find the full source code on GitHub.</p>
			<p>Please remember that this file should run in the <strong class="source-inline">PythonAPI/examples</strong> directory.</p>
			<p>In principle, letting our neural network take control of the steering wheel is quite simple, as we just need to analyze the current frame and set the steering. However, we also need to apply some throttle, or the car will not move!</p>
			<p>It's also very important that you run the inference phase in the game loop, or that you are really sure that it is running on the client, else the performance will drop substantially and your network will have a hard time driving due to the excess of latency between receiving the frame and sending the instruction to drive.</p>
			<p>As the Carla client changes the car every time, the effect of the throttle will change, sometimes making your car too fast or too slow. You therefore need a way to change the throttle with a key, or you could always use the same car, which will be our solution.</p>
			<p>You can get a list of the cars available in Carla with the following line of code:</p>
			<p class="source-code">vehicles = world.get_blueprint_library().filter('vehicle.*')</p>
			<p>At the <a id="_idIndexMarker573"/>time of writing, this produces the following<a id="_idIndexMarker574"/> list:</p>
			<p class="source-code">vehicle.citroen.c3</p>
			<p class="source-code">vehicle.chevrolet.impala</p>
			<p class="source-code">vehicle.audi.a2</p>
			<p class="source-code">vehicle.nissan.micra</p>
			<p class="source-code">vehicle.carlamotors.carlacola</p>
			<p class="source-code">vehicle.audi.tt</p>
			<p class="source-code">vehicle.bmw.grandtourer</p>
			<p class="source-code">vehicle.harley-davidson.low_rider</p>
			<p class="source-code">vehicle.bmw.isetta</p>
			<p class="source-code">vehicle.dodge_charger.police</p>
			<p class="source-code">vehicle.jeep.wrangler_rubicon</p>
			<p class="source-code">vehicle.mercedes-benz.coupe</p>
			<p class="source-code">vehicle.mini.cooperst</p>
			<p class="source-code">vehicle.nissan.patrol</p>
			<p class="source-code">vehicle.seat.leon</p>
			<p class="source-code">vehicle.toyota.prius</p>
			<p class="source-code">vehicle.yamaha.yzf</p>
			<p class="source-code">vehicle.kawasaki.ninja</p>
			<p class="source-code">vehicle.bh.crossbike</p>
			<p class="source-code">vehicle.tesla.model3</p>
			<p class="source-code">vehicle.gazelle.omafiets</p>
			<p class="source-code">vehicle.tesla.cybertruck</p>
			<p class="source-code">vehicle.diamondback.century</p>
			<p class="source-code">vehicle.audi.etron</p>
			<p class="source-code">vehicle.volkswagen.t2</p>
			<p class="source-code">vehicle.lincoln.mkz2017</p>
			<p class="source-code">vehicle.mustang.mustang</p>
			<p>In <strong class="source-inline">World.restart()</strong>, you <a id="_idIndexMarker575"/>can select the car of your<a id="_idIndexMarker576"/> choice:</p>
			<p class="source-code">bp=self.world.get_blueprint_library().filter(self._actor_filter)</p>
			<p class="source-code">blueprint = next(x for x in bp if x.id == 'vehicle.audi.tt')</p>
			<p>Carla uses actors, which can represent vehicles, walkers, sensors, traffic lights, traffic signs, and so on; actors<a id="_idIndexMarker577"/> are created from templates called <strong class="bold">blueprints</strong>. Later, in the same function, the code (which you don't need to modify) creates a vehicle using <strong class="source-inline">try_spawn_actor()</strong>:</p>
			<p class="source-code">self.player = self.world.try_spawn_actor(blueprint, spawn_point)</p>
			<p>If you run the code now, you will see the car but with the wrong point of view. Pressing the <em class="italic">Tab</em> key will fix it:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="Images/Figure_8.22_B16322.jpg" alt="Figure 8.22 – Left: default initial camera, right: camera for self-driving" width="674" height="261"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.22 – Left: default initial camera, right: camera for self-driving</p>
			<p>If you want to start from the point where I was training the car, you should also set the starting point in the same method:</p>
			<p class="source-code">spawn_point = spawn_points[0] if spawn_points else carla.Transform()</p>
			<p>If you don't do <a id="_idIndexMarker578"/>that, the car will be spawned in a<a id="_idIndexMarker579"/> random position, and it might have more problems driving.</p>
			<p>In <strong class="source-inline">game_loop()</strong>, we also need to select the proper track:</p>
			<p class="source-code">client.load_world('Town04')</p>
			<p class="source-code">client.reload_world()</p>
			<p>If you run it now, after pressing <em class="italic">Tab</em>, you should see something like the following:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="Images/Figure_8.23_B16322.jpg" alt="Figure 8.23 – Image from Carla, ready for self-driving" width="1037" height="613"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.23 – Image from Carla, ready for self-driving</p>
			<p>If you press <em class="italic">F1</em>, you can remove the information on the left.</p>
			<p>For convenience, we <a id="_idIndexMarker580"/>want to be able to trigger the self-driving<a id="_idIndexMarker581"/> mode on and off, so we need a variable for that, such as the following, and one to hold the computed steering angle in the constructor of <strong class="source-inline">KeyboardControl</strong>:</p>
			<p class="source-code">self.self_driving = False</p>
			<p>Then, in <strong class="source-inline">KeyboardControl.parse_events()</strong>, we will intercept the <em class="italic">D</em> key and switch the self-driving functionality on and off:</p>
			<p class="source-code">elif event.key == K_d:</p>
			<p class="source-code">  self.self_driving = not self.self_driving</p>
			<p class="source-code">  if self.self_driving:</p>
			<p class="source-code">    world.hud.notification('Self-driving with Neural Network')</p>
			<p class="source-code">  else:</p>
			<p class="source-code">    world.hud.notification('Self-driving OFF')</p>
			<p>The next step is resizing and saving the last image received from the server, when it is still in BGR format, in <strong class="source-inline">CameraManager._parse_image()</strong>. This is shown here:</p>
			<p class="source-code">array_bgr = cv2.resize(array, (200, 133))</p>
			<p class="source-code">self.last_image = array_bgr[67:, :, :]</p>
			<p class="source-code">array = array[:, :, ::-1]  # BGR =&gt; RGB</p>
			<p>The <strong class="source-inline">array</strong> variable originally contains the image in BGR format, and <strong class="source-inline">::-1</strong> in NumPy reverses the order, so the last line of code effectively converts the image from BGR into RGB, before visualizing it.</p>
			<p>Now, we can load the model in <strong class="source-inline">game_loop()</strong>, outside of the main loop:</p>
			<p class="source-code">model = keras.models.load_model('behave.h5')</p>
			<p>Then, we <a id="_idIndexMarker582"/>can run the model in <strong class="source-inline">game_loop()</strong>, inside<a id="_idIndexMarker583"/> the main loop, and save the steering, as follows:</p>
			<p class="source-code">if world.camera_manager.last_image is not None:</p>
			<p class="source-code">  image_array = np.asarray(world.camera_manager.last_image)</p>
			<p class="source-code">  controller.self_driving_steer = model.predict(image_array[    None, :, :, :], batch_size=1)[0][0].astype(float)</p>
			<p>The last thing to do is just to use the steering that we computed, put a fix throttle, and limit the maximum speed, while we are at it:</p>
			<p class="source-code">if self.self_driving:</p>
			<p class="source-code">  self.player_max_speed = 0.3</p>
			<p class="source-code">  self.player_max_speed_fast = 0.3</p>
			<p class="source-code">  self._control.throttle = 0.3</p>
			<p class="source-code">  self._control.steer = self.self_driving_steer</p>
			<p class="source-code">  return</p>
			<p>This is all nice and good, except that it might not work because of a GPU error. Let's see what it is and how to overcome it.</p>
			<p>Making your GPU work</p>
			<p>You might get an <a id="_idIndexMarker584"/>error similar to this one:</p>
			<p class="source-code">failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED</p>
			<p>My understanding of what's happening is that there is a conflict with some component of Carla, either the server or the client, which results in the GPU not having enough memory. In particular, it is TensorFlow creating the problem, as it tries to allocate all the memory in the GPU.</p>
			<p>Luckily, this is easily fixable with a few lines of the following code:</p>
			<p class="source-code">import tensorflow</p>
			<p class="source-code">gpus = tensorflow.config.experimental.list_physical_devices('GPU')</p>
			<p class="source-code">if gpus:</p>
			<p class="source-code">  try:</p>
			<p class="source-code">    for gpu in gpus:</p>
			<p class="source-code">      tensorflow.config.experimental.set_memory_growth(gpu, True)</p>
			<p class="source-code">    print('TensorFlow allowed growth to ', len(gpus), ' GPUs')</p>
			<p class="source-code">  except RuntimeError as e:</p>
			<p class="source-code">    print(e)</p>
			<p>The call to <strong class="source-inline">set_memory_growth()</strong> instructs TensorFlow to allocate only part of the GPU RAM, and eventually allocate more if required, solving our problem.</p>
			<p>At this point, your car should be able to drive, so let's discuss a bit how it works.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor192"/>Self-driving!</h1>
			<p>Now, you could start <a id="_idIndexMarker585"/>running <strong class="source-inline">manual_control_drive.py</strong>, maybe instructing it to use a lower resolution, using the <strong class="source-inline">--res 480x320</strong> parameter.</p>
			<p>If you press the <em class="italic">D</em> key, the car should start to drive by itself. It's probably quite slow, but it should run, sometimes nicely, sometimes less nicely. It might not always take the turns that it is supposed to take. You can try to add images to the dataset or improve the architecture of the neural network – for example, by adding some dropout layers.</p>
			<p>You could try to change the car or increase the speed. You might notice that at a higher speed, the car starts to move more erratically, as if the driver was drunk! This is due to the excessive latency between the car getting in the wrong position and the neural network reacting to it. I think this could be fixed partly with a computer fast enough to process many FPS. However, I think a real fix would be to also record higher speed runs, where the corrections would be stronger; this would require a better controller than the keyboard, and you should also insert the speed in the input, or have multiple neural networks and switch between them based on the speed.</p>
			<p>Interestingly, sometimes it can somehow also drive even if we are using the outside camera, with the result that our car is part of the image! Of course, the result is not good, and you get the <em class="italic">drunk drive</em> effect even at low speed.</p>
			<p>Just out of curiosity, let's <a id="_idIndexMarker586"/>check the saliency map. This is the image that we are sending to the network:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="Images/Figure_8.24_B16322.jpg" alt="Figure 8.24 – Image from the back" width="1400" height="452"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.24 – Image from the back</p>
			<p>Now, we can check the saliency map:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="Images/Figure_8.25_B16322.jpg" alt="Figure 8.25 – Saliency maps: third convolution layer and output layer" width="1650" height="292"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25 – Saliency maps: third convolution layer and output layer</p>
			<p>The network is still able to recognize the lines and the road; however, it is very concerned about the car. My hypothesis is that the neural network <em class="italic">thinks</em> it is an obstacle and the road is ending.</p>
			<p>If you want to teach the car how to drive well with this camera, or with any other one, you will need to train it with that specific camera. If you want the car to drive properly on another track, you will need to train it on that specific track. Eventually, if you train it on many tracks and on many conditions, it should be able to drive everywhere. But this means building a huge dataset, with millions of images. Eventually, if your dataset is too big, you will run out<a id="_idIndexMarker587"/> of memory.</p>
			<p>In the next section, we will talk about generators, a technique that can help us overcome these problems.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor193"/>Training bigger datasets using generators</h2>
			<p>When training big <a id="_idIndexMarker588"/>datasets, memory consumption can be an issue. In Keras, one way to solve this problem is by using Python generators. A Python generator is a function that can lazily return a potentially infinite stream of values, with a very low memory footprint as you only need memory for one object, plus, of course, all the supporting data that you might need; a generator can be used as if it were a list. A typical generator has a loop, and for every object that needs to be part of the stream, it will use the <strong class="source-inline">yield</strong> keyword.</p>
			<p>In Keras, the generator needs to be aware of the batch size, because it needs to return a batch of samples and a batch of labels.</p>
			<p>We will keep a list of the files to process, and we will write a generator that can use this list to return the image associated with it and the label.</p>
			<p>We will write a generic generator that, hopefully, you can reuse on other cases, and it will accept four parameters:</p>
			<ul>
				<li>A list of IDs, which in our case are the filenames</li>
				<li>A function to retrieve the input (the image) from the ID</li>
				<li>A function to retrieve the label (the steering wheel) from the ID</li>
				<li>The batch size</li>
			</ul>
			<p>To start, we need a function that can return an image given a file:</p>
			<p class="source-code">def extract_image(file_name):</p>
			<p class="source-code">    return cv2.imread(file_name)</p>
			<p>We also need a function that, given a filename, can return the label, which in our case is the steering angle:</p>
			<p class="source-code">def extract_label(file_name):</p>
			<p class="source-code">  (seq, camera, steer, throttle, brake, img_type) =    expand_name(file_name)</p>
			<p class="source-code">  return steer</p>
			<p>We can now<a id="_idIndexMarker589"/> write the generator, as follows:</p>
			<p class="source-code">def generator(ids, fn_image, fn_label, batch_size=32):</p>
			<p class="source-code">  num_samples = len(ids)</p>
			<p class="source-code">  while 1: # The generator never terminates</p>
			<p class="source-code">    samples_ids = shuffle(ids) # New epoch</p>
			<p class="source-code"> </p>
			<p class="source-code">    for offset in range(0, num_samples, batch_size):</p>
			<p class="source-code">      batch_samples_ids = samples_ids[offset:offset + batch_size]</p>
			<p class="source-code">      batch_samples = [fn_image(x) for x in batch_samples_ids]</p>
			<p class="source-code">      batch_labels = [fn_label(x) for x in batch_samples_ids]</p>
			<p class="source-code"> </p>
			<p class="source-code">      yield np.array(batch_samples), np.array(batch_labels)</p>
			<p>Every iteration in the <strong class="source-inline">while</strong> loop corresponds to an epoch, while the <strong class="source-inline">for</strong> loop generates all the batches required to complete each epoch; at the beginning of each epoch, we shuffle the IDs to improve the training.</p>
			<p>In Keras, it used to be that you had to use the <strong class="source-inline">fit_generator()</strong> method, but nowadays, <strong class="source-inline">fit()</strong> is able to understand if the argument is a generator, but you still need to provide a couple of new parameters:</p>
			<ul>
				<li><strong class="source-inline">steps_per_epoch</strong>: This gives how many batches there are in a single training epoch, which is the number of training samples divided by the batch size.</li>
				<li><strong class="source-inline">validation_steps</strong>: This gives how many batches there are in a single validation epoch, which is the number of validation samples divided by the batch size.</li>
			</ul>
			<p>This is the code<a id="_idIndexMarker590"/> that you need to use the <strong class="source-inline">generator()</strong> function that we just defined:</p>
			<p class="source-code">files = shuffle(files)idx_split = int(len(files) * 0.8)</p>
			<p class="source-code">val_size = len(files) - idx_split</p>
			<p class="source-code">train_gen = generator(files[0:idx_split], extract_image,  extract_label, batch_size)</p>
			<p class="source-code">valid_gen = generator(files[idx_split:], extract_image,  extract_label, batch_size)</p>
			<p class="source-code">history_object = model.fit(train_gen, epochs=250,  steps_per_epoch=idx_split/batch_size, validation_data=valid_gen,  validation_steps=val_size/batch_size, shuffle=False, callbacks=  [checkpoint, early_stopping])</p>
			<p>Thanks to this code, you can now leverage very big datasets. However, there is also another application of generators: custom on-demand data augmentation. Let's say a few words about it.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor194"/>Augmenting data the hard way</h2>
			<p>We already saw an easy <a id="_idIndexMarker591"/>way to perform data augmentation, using <strong class="source-inline">ImageDataGenerator</strong> in <a href="B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158"><em class="italic">Chapter 7</em></a>, <em class="italic">Detecting Pedestrians and Traffic Lights</em>. This could be appropriate for classifiers, because the transformations applied to the image do not alter its classification. However, in our case, some of these transformations would require a change in the prediction. In fact, Nvidia designed a custom data augmentation, where the image is randomly shifted and the steering wheel is updated accordingly. This could be done with a generator, where we take the original image, apply the transformation, and correct the steering wheel based on the amount of shifting.</p>
			<p>But we are not limited to just replicating the same amount of images that we have in input, but we could create less (filtering) or more; for example, mirroring could be applied at runtime, and as a result, we duplicate the images in memory, without having to store double the <a id="_idIndexMarker592"/>amount of images and saving, as a consequence, half of the file access and the JPEG decompression; though of course, we would need some CPU to flip the image.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor195"/>Summary</h1>
			<p>In this chapter, we went through many interesting topics. </p>
			<p>We started by describing DAVE-2, an experiment of Nvidia with the goal to demonstrate that a neural network can learn how to drive on a road, and we decided to replicate the same experiment but on a much smaller scale. First, we collected the image from Carla, taking care of recording not only the main camera but also two additional side cameras, to teach the network how to correct errors. </p>
			<p>Then, we created our neural network, copying the architecture of DAVE-2, and we trained it for regression, which requires some changes compared to the other training that we did so far. We learned how to generate saliency maps and get a better understanding of where the neural network is focusing its attention. Then, we integrated with Carla and used the network to <em class="italic">self-drive</em> the car! </p>
			<p>At the end, we learned how to train a neural network using Python generators, and we discussed how this can be used to achieve more sophisticated data augmentations.</p>
			<p>In the next chapter, we will explore a state-of-the-art technique that can be used to detect the road at a pixel level—semantic segmentation.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor196"/>Questions</h1>
			<p>After reading the chapter, you should be able to answer the following questions:</p>
			<ol>
				<li value="1">What is the original name of the neural network that Nvidia trained for self-driving?</li>
				<li>What is the difference between a classification and a regression task?</li>
				<li>What is the Python keyword that you can use to create a generator?</li>
				<li>What is a saliency map?</li>
				<li>Why do we need to record three video streams?</li>
				<li>Why are we running inference from the <strong class="source-inline">game_loop()</strong> method?</li>
			</ol>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor197"/>Further reading</h1>
			<ul>
				<li>Nvidia DAVE-2: <a href="https://devblogs.nvidia.com/deep-learning-self-driving-cars/">https://devblogs.nvidia.com/deep-learning-self-driving-cars/</a></li>
				<li>Notes related to Carla 0.9.0 API changes: <a href="https://carla.org/2018/07/30/release-0.9.0/">https://carla.org/2018/07/30/release-0.9.0/</a></li>
				<li>Carla: <a href="https://carla.org">https://carla.org</a></li>
				<li><strong class="source-inline">keras-vis</strong>: <a href="https://github.com/raghakot/keras-vis">https://github.com/raghakot/keras-vis</a></li>
			</ul>
		</div>
	</div>



  </body></html>