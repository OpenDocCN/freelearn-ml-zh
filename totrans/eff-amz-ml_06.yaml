- en: Predictions and Performances
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is time to make some predictions! In [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml),
    *Loading and Preparing the Dataset,* we did split the `Titanic` dataset into two
    subsets, the training and held-out subsets, respectively consisting of 70% and
    30% of the original dataset randomly shuffled. We have used variations of the training
    subset extensively in [chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml) *Model
    Creation,* to train and select the best classification model. But so far, we have
    not used the held-out subset at all. In this chapter, we apply our models to this
    held-out subset to make predictions on unseen data and make a final assessment
    of the performance and robustness of our models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon ML offers two types of predictions: batch and streaming. Batch prediction
    requires a datasource. The samples you want to predict are given to the model
    all at once in batch mode. Streaming, also known as real-time or online predictions,
    requires the creation of an API endpoint and consists of submitting sequences
    of samples, one by one, via HTTP requests. Real-time predictions do not involve
    the creation of a datasource.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We will start with batch predictions on the Titanic held-out set. We will confirm
    that our different models perform similarly on the held-out dataset as they did
    on the validation subsets, assuming that all the subsets have a similar variable
    distribution. In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model
    Creation*, we concluded that out of our three datasources — suggested recipe with
    quantile binning (QB), recipe without QB, and the extended dataset — the one with extra
    variables (`deck`, `title`, `log_fare`, and so on) resulted in the best score
    on the validation subset. We will verify that this is also the case on the held-out
    subset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is organized in two parts. In the first part, we look at batch
    predictions on the `Titanic` dataset. In the second part, we look at real-time,
    streaming predictions, with a new text-based quantile binning from the UCI repository.
    The `Spam` dataset is large enough to simulate streaming data. We will create
    an Amazon ML endpoint and use the Python SDK to send and retrieve classification
    predictions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Making batch predictions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making real-time predictions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real-world classification problems or regression problems, the previously
    unseen data you want to make predictions on will not include the target values.
    In our case, the held-out datasets do contain the solution, and this allows us
    to assess the model performance on previously unseen data. But with real-world
    problems, you do not have that luxury and you will have to trust your model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Making batch predictions
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Making batch predictions on Amazon ML is straightforward and follows this process:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: From the dashboard, create a new Batch prediction.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the model.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the datasource on which to apply the model.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the prediction output folder and grant permissions.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置预测输出文件夹并授予权限。
- en: Review and launch.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查并启动。
- en: We call the `prediction` dataset or datasource, the data on which we want to
    make predictions. In this chapter, we are in a testing context and the `prediction`
    dataset is the `held-out` dataset we extracted from the whole original dataset.
    In a real-world context, the prediction dataset refers to entirely new data and
    does not include the target variable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称预测数据集或数据源为，我们想要进行预测的数据。在本章中，我们处于测试环境中，预测数据集是我们从整个原始数据集中提取的`保留`数据集。在现实世界的情况下，预测数据集指的是全新的数据，并且不包含目标变量。
- en: The prediction can only work if the distribution of the prediction dataset is
    similar to the distribution of the training dataset on which the model has been
    trained. The prediction datasource and the training datasource must also share
    the same schema, with one difference the prediction dataset does not need to include
    the target variable. Amazon ML will verify that the schema defined for your training
    data is relevant to your prediction data and will issue a warning if the datasets
    are not similar.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 预测只有在预测数据集的分布与模型训练所用的训练数据集的分布相似时才能工作。预测数据源和训练数据源还必须共享相同的模式，唯一的区别是预测数据集不需要包含目标变量。Amazon
    ML 将验证为您的训练数据定义的模式是否与您的预测数据相关，如果数据集不相似，将发出警告。
- en: For the sake of convenience, we have recreated the datasets, datasources, and
    models for this chapter. All datasets and scripts are available in the GitHub
    repository at [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6).
    Since we reshuffled the original Titanic data, the evaluation scores will be different
    from the ones obtained previously for the same dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们已重新创建了本章的数据集、数据源和模型。所有数据集和脚本均可在 GitHub 仓库中找到，网址为 [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6)。由于我们对原始的泰坦尼克号数据进行了重新排序，因此评估分数将与之前相同数据集获得的分数不同。
- en: Creating the batch prediction job
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建批量预测作业
- en: 'To create a batch prediction, go to the Amazon ML dashboard and click on Create
    new batch prediction:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建批量预测，请转到 Amazon ML 仪表板并点击创建新的批量预测：
- en: '![](img/B05028_06_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_01.png)'
- en: 'Then select the model. We choose the original model related to the `Titanic`
    dataset, the one using the Amazon ML suggested recipe with quantile binning:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后选择模型。我们选择与`泰坦尼克号`数据集相关的原始模型，使用 Amazon ML 建议的食谱进行分位数分箱：
- en: Quantile binning of all numeric variables
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数值变量的分位数分箱
- en: L2 mild regularization
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 轻度正则化
- en: '![](img/B05028_06_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_02.png)'
- en: 'After the model selection comes the datasource selection. If you have not yet
    created a datasource for the held-out set, you can do so now. First, upload your
    prediction dataset to S3 and specify the S3 path for the source of the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择之后，进行数据源选择。如果您尚未为保留集创建数据源，现在可以创建。首先，将您的预测数据集上传到 S3，并指定数据的 S3 路径：
- en: '![](img/B05028_06_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_03.png)'
- en: 'When you click on Verify, Amazon ML will check that the prediction dataset
    follows the same schema as the training dataset on which the model was trained:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击验证时，Amazon ML 将检查预测数据集是否遵循与模型训练所用的训练数据集相同的模式：
- en: '![](img/B05028_06_04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_04.png)'
- en: Interpreting prediction outputs
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释预测输出
- en: 'The output of the Amazon ML prediction job will consists of two files: the
    manifest file and the actual prediction results given in a compressed CSV file.
    Amazon ML will create the files on S3 in an S3 location, `s3://bucket/folder`,
    which you must specify. We use the same path as our data path: `s3://aml.packt/data/`.
    Amazon ML will create a `/batch_prediction` folder, where it will write the manifest
    file as well as an extra subfolder `/results`, where the CSV file with the actual
    predictions will be written. To recap, in our context, the manifest file will
    be in the  `s3://aml.packt/data/batch_prediction` folder, and the compressed CSV
    results file will be in the `s3://aml.packt/data/batch_prediction/results/` folder.The
    name given to the batch prediction will dictate the naming of the manifest and
    results files:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_05.png)**Prediction pricing**: If you just created the datasource
    for the batch prediction, Amazon ML does not yet have access to the data statistics
    that it needs to calculate the prediction costs. In that case, it will simply
    inform you of the price, of $0.10 per 1,000 predictions. If the prediction datasource
    has already been validated and Amazon ML knows the number of records, the estimated
    price will be the number of rows times the price per prediction rounded up to
    the nearest cent. You are not billed for the invalid samples Amazon ML fails to
    predict. More information is available at [http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html](http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Review and click on the Create batch prediction button. The batch prediction
    job will take a few minutes to complete. When finished, it will have created the
    manifest and results files in S3 and will show up as completed in the Batch Prediction
    section of the Amazon ML dashboard.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Reading the manifest file
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The manifest file contains JSON-formatted data that maps the input file to
    the prediction results file, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our context, the manifest file contains the following line:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Multiple input files**: If your input data is split into several files, all
    stored in the same S3 location `s3://examplebucket/input/`, all the input files
    will be considered by the batch prediction job. The manifest file will then contain
    the mapping from the different input files to the associated results files. For
    instance, if you have three input files named `data1.csv`, `data2.csv`, and `data3.csv`,
    and they are all stored in the S3 location `s3://examplebucket/input/`, you will see
    a mapping string that looks like as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Maximum size for predictions**: Amazon ML allows up to 1TB of data for prediction
    files. If the data on which you want to make predictions is larger, it is possible
    to split your data into several files, upload them to a specific S3 location,
    and Amazon ML will handle the different files and generate as many prediction
    result files as there are input files by running several batches in parallel.
    The manifest file will contain all the different input/output pairs, `{input_file.csv
    : prediction_results.csv.gz}` for your different batch prediction files.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测的最大大小**：Amazon ML允许预测文件的最大数据量为1TB。如果你想要进行预测的数据量更大，可以将数据分割成几个文件，上传到特定的S3位置，Amazon
    ML将处理不同的文件，并通过并行运行多个批次生成与输入文件数量相等的预测结果文件。清单文件将包含所有不同的输入/输出对，`{input_file.csv :
    prediction_results.csv.gz}`，针对你的不同批量预测文件。'
- en: Reading the results file
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取结果文件
- en: The output results file is compressed with gzip, originates from the UNIX world,
    and offers better compression than the more common zip compression. A simple click
    should be sufficient to open and decompress the gzipped results file into a readable
    CSV file. Alternatively, a call to the gunzip command from the command line should
    work. Take a look at [http://www.gzip.org/](http://www.gzip.org/) for installation
    on different systems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果文件使用gzip压缩，源自UNIX世界，提供的压缩效果优于更常见的zip压缩。简单点击即可打开和解压缩gzip压缩的结果文件，将其转换为可读的CSV文件。或者，可以从命令行调用gunzip命令。查看[http://www.gzip.org/](http://www.gzip.org/)获取不同系统的安装信息。
- en: 'For binary classification, the decompressed results file contains two or three
    columns, depending on whether the initial input file contained the target or not.
    In our case of binary classification, the result file has the following columns:
    `trueLabel`, `bestAnswer`, and `score`, where `trueLabel` is the initial `survived`
    column. If your initial batch prediction dataset did not include the target values,
    the results file will only have the bestAnswer and score columns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类，解压缩的结果文件包含两列或三列，具体取决于初始输入文件是否包含目标值。在我们的二元分类案例中，结果文件包含以下列：`trueLabel`、`bestAnswer`和`score`，其中`trueLabel`是初始的`survived`列。如果你的初始批量预测数据集没有包含目标值，结果文件将只包含`bestAnswer`和`score`列：
- en: '`trueLabel` is the original target value contained in the input file'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trueLabel`是输入文件中包含的原始目标值'
- en: '`bestAnswer` is the classification result: 0 or 1'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bestAnswer`是分类结果：0或1'
- en: '`Score` is the probability for that classification written in scientific notation'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Score`是以科学记数法表示的那个分类的概率'
- en: The classification cutoff threshold for the score probability is 0.5 by default,
    or set to the threshold value you chose while evaluating the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分数的概率分类截止阈值默认为0.5，或者在评估模型时设置的阈值值。
- en: For multiclass classification with *N* potential target classes, the results
    file will have *N+1* or *N+2* columns. The `trueLabel`, `bestAnswer`, and `N`
    columns each with the probability scores for each one of the N classes. The chosen
    class will be the one that bears the highest probability score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*N*个潜在目标类别的多类分类，结果文件将包含*N+1*或*N+2*列。`trueLabel`、`bestAnswer`和*N*列分别包含N个类别中每个类别的概率分数。所选的类别将是具有最高概率分数的类别。
- en: For a regression model, the results file will only contain one/two score columns
    with the predicted value, and possibly the `trueLabel` column.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归模型，结果文件将只包含一个/两个分数列，包含预测值，可能还有`trueLabel`列。
- en: Assessing our predictions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的预测
- en: Since we know the real class of our held-out samples, we can calculate the **ROC-AUC**
    score and other metrics to see how close our prediction and validation scores
    are. Assuming that our data subsets have very similar distributions, both scores
    should end up very close. The difference only comes from randomness in the samples
    for the validation and held-out sets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道保留样本的真实类别，我们可以计算**ROC-AUC**分数和其他指标，以查看我们的预测和验证分数有多接近。假设我们的数据子集具有非常相似的分布，这两个分数最终应该非常接近。差异仅来自验证和保留集样本中的随机性。
- en: 'The following Python script uses the `scikit-learn` library ([http://scikit-learn.org/](http://scikit-learn.org/))
    as well as the pandas library. It takes a few lines of Python to calculate the
    AUC score of the model on that prediction dataset. First, download the gzipped
    file from S3 and then, in a Python Notebook or console, run the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Python environment**: All the Python code in this book is for Python 3.5
    or above. For more information on the Anaconda library, take a look at [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    Anaconda is an amazingly powerful open source data science platform in Python.
    It contains the most important libraries (`numpy`, `pandas`, `scikit-learn`, `matplotlib`,
    and many others) as well as the Jupyter Notebooks environment. We use the IPython
    console for its simplicity of use and many magic commands ([http://ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html)).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Running the previous Python script on the predictions results, we obtain an
    AUC of 0.84 on our held-out dataset, which is very close to the AUC (0.85) we
    obtained on the validation set in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation*. We can conclude that our model is pretty stable and robust when
    facing new, previously unforeseen data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows both the ROC curves for the validation (dotted line)
    and held-out (solid-line) sets for the chosen model. The validation set is slightly
    better for higher values of the threshold. This difference is a reflection of
    the different data distributions in the two datasets:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_06.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Evaluating the held-out dataset
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*,
    we evaluated the performance of our different models on a slice of the training
    datasource. We obtained for each model an AUC score, and selected the AUC with
    the best AUC score. We relied on Amazon ML to create the validation set, by splitting
    the training dataset into two, with 70% for training and 30% of the data for validation.
    We could have done that split ourselves, created the validation datasource, and
    specified which datasource to use for the evaluation of the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, nothing prevents us from running a model evaluation on the held-out
    dataset. If you go to the model summary page, you will notice a Perform another
    Evaluation button in the Evaluation section:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_15.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Click on it. You are asked to select the datasource for the evaluation. Select
    the held-out dataset; Amazon ML will verify that the data follows the same schema
    and is similar to the training data. You end up with two evaluations on the model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_16.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'And as expected, the evaluation AUC for the held-out dataset is equal to the
    AUC we obtained by downloading the results and calculating the AUC in Python:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_17.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Finding out who will survive
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The true value of predictions, however, is not about validating the robustness
    of our model; it's about making predictions on our prediction dataset, in our
    context, getting survival predictions on this `new` list of passengers contained
    in the held-out dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预测的真正价值并不在于验证我们模型的鲁棒性；它在于在我们的预测数据集、我们的环境中进行预测，对保留数据集中的这个“新”乘客列表进行生存预测。
- en: 'The rows in the results file follow the exact same order as the rows in the
    prediction file. We can put side by side the first rows of the held-out file and the
    first rows of the results file, and see that the `survived` and the `trueLabel`
    columns are identical:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果文件中的行与预测文件中的行顺序完全相同。我们可以将保留文件的前几行和结果文件的前几行并排放置，并看到“survived”和“trueLabel”列是相同的：
- en: '![](img/B05028_06_13.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_06_13.png)'
- en: Multiplying trials
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 乘法试验
- en: 'The evaluation scores on the various models and dataset version are, to a certain
    extent, dependent on the samples contained in the evaluation sets. If we run the
    following experiment several times on the three datasets, we see certain variations
    in the scores:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种模型和数据集版本上的评估分数在一定程度上取决于评估集中的样本。如果我们在这三个数据集上多次运行以下实验，我们会看到分数的某些变化：
- en: Shuffle and split the dataset into three -- training, validation, and held-out
    and create the respective datasources
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱并分割数据集为三个部分——训练、验证和保留，并创建相应的数据源
- en: Train a model on the training dataset, keeping the default Amazon ML settings
    (mild L2 regularization)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练模型，保持默认的亚马逊机器学习设置（轻微的L2正则化）
- en: Evaluate the model on the evaluation and held-out datasets
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估和保留数据集上评估模型
- en: 'The following plot shows the respective performances of the three models for
    several trials. The average AUC is written on the graph. We see that on average,
    the extended dataset performs better (*AUC = 0.87*) than the original dataset
    with the default recipe (*AUC = 0.84*) and the original dataset without quantile
    binning (*AUC = 0.83*). We also notice that in some trials, the extended dataset
    performs worse than the original one. For trial 3, the default recipe is even
    slightly less performant than the no quantile binning one:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了三个模型在多次试验中的相应性能。图表上写有平均AUC。我们看到平均而言，扩展数据集的性能优于默认配方下的原始数据集（AUC = 0.84）和没有分位数分箱的原始数据集（AUC
    = 0.83）。我们还注意到，在某些试验中，扩展数据集的性能不如原始数据集。对于试验3，默认配方甚至略逊于没有分位数分箱的配方：
- en: '![](img/B05028_06_14.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_06_14.png)'
- en: This shows the importance of inner data distribution variability. When trying
    out several variants of your datasets with different features and processing,
    it's important to base your conclusions on several runs of your models. A single evaluation
    may lead to missing the best model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了内部数据分布变异性的重要性。当尝试使用不同特征和处理的多个数据集变体时，基于您模型的多次运行得出结论是很重要的。单次评估可能会导致错过最佳模型。
- en: Making real-time predictions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行实时预测
- en: With batch predictions, you submit all the samples you want the model to predict
    at once to Amazon ML by creating a datasource. With real-time predictions, also
    called streaming or online predictions, the idea is to send one sample at a time
    to an API endpoint, a URL, via HTTP queries, and receive back predictions and
    information for each one of the samples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量预测时，您可以通过创建数据源一次将所有希望模型预测的样本提交给亚马逊机器学习。使用实时预测（也称为流式或在线预测），想法是每次发送一个样本到一个API端点、一个URL，通过HTTP查询，并为每个样本接收预测和信息。
- en: Setting up real-time predictions on a model consists of knowing the prediction
    API endpoint URL and writing a script that can read your data, send each new sample to
    that API URL, and retrieve the predicted class or value. We will present a Python-based
    example in the following section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型上设置实时预测包括了解预测API端点URL和编写一个脚本，该脚本可以读取您的数据，将每个新样本发送到该API URL，并检索预测的类别或值。我们将在下一节中提供一个基于Python的示例。
- en: Amazon ML also offers a way to make predictions on data you create on the fly
    on the prediction page. We can input the profile of a would-be passenger on the
    `Titanic` and see whether that profile would have survived or not. It is a great
    way to explore the influence of the dataset variables on the outcome.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习（Amazon ML）还提供了一种方法，可以在预测页面上对您实时创建的数据进行预测。我们可以输入一个潜在的乘客在“泰坦尼克号”上的档案，并查看该档案是否能够幸存。这是一种探索数据集变量对结果影响的绝佳方式。
- en: Before setting up API for streaming, let’s see what we can gather from submitting
    several single passenger profiles. We can even try to answer the question – *W**ould
    you have survived on the Titanic?*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置流式API之前，让我们看看通过提交几个单个乘客档案我们能收集到什么信息。我们甚至可以尝试回答这个问题——*你会在大西洋号上幸存下来吗？*
- en: Manually exploring variable influence
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动探索变量影响
- en: Go to the model summary page and click on the Try real-time predictions link
    on the left side of the page. The following page shows a form where you can fill
    out values for the variables in our dataset except for the target.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前往模型摘要页面，然后点击页面左侧的“尝试实时预测”链接。接下来的页面显示一个表单，您可以在其中填写我们数据集中变量的值，除了目标变量。
- en: 'Let''s see if Alex Mr. Perrier, a first-class passenger who embarked at Southhampton
    with his family (3 sibsp and 2 parch) and who paid a fare of 100 pounds, would
    have survived. Well, in that case, the model gives a very low probability of survival
    (0.001), meaning that the model predicts with confidence that this passenger would
    not have survived. His 12 year old daughter would have had better chances of surviving
    (*probability 0.56*), though the model is less sure of it. However, if that same
    girl was traveling alone (*sibsp = parch = 0*), her chance of survival would surge
    to 0.98 under the condition that she traveled in 1st class. In 3rd class, she
    would have been less fortunate (*probability: 0.28*):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看亚历克斯·佩里埃先生，一位头等舱乘客，他在南安普顿和他的家人（3个兄弟姐妹和2个孩子）登船，并支付了100英镑的船费，他是否会幸存。好吧，在这种情况下，模型给出了非常低的生存概率（0.001），这意味着模型有信心预测这位乘客不会幸存。他的12岁女儿有更大的生存机会（*概率为0.56*），尽管模型对此不太确定。然而，如果那个女孩独自旅行（*兄弟姐妹=孩子=0*），在头等舱的条件下，她的生存机会会激增到0.98。在三等舱，她将不太幸运（*概率：0.28*）：
- en: '![](img/B05028_06_18.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_18.png)'
- en: So, by changing one variable at a time in the data, we can have a better understanding
    of the impact of each variable on the outcome.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过逐个改变数据中的变量，我们可以更好地理解每个变量对结果的影响。
- en: Setting up real-time predictions
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置实时预测
- en: 'To demonstrate real-time predictions, we will use the `Spam` dataset from the
    UCI repository. This dataset is composed of 5,574 SMS messages annotated spam
    or ham (non-spam). There are no missing values and only two variables: the nature
    of the SMS (ham or spam) and the text message of the SMS, nothing else.  The `Spam`
    dataset is available at [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) in
    its raw form, and in the book''s GitHub repository at [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6).
    We have simply transformed the target from categorical: `spam` and `ham` values
    to binary: 1 (for spam) and 0 (for ham) so that Amazon ML understands the prediction
    to be of the binary-classification type.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示实时预测，我们将使用来自UCI仓库的`Spam`数据集。这个数据集由5,574条标注为垃圾邮件或非垃圾邮件（非垃圾邮件）的短信组成。没有缺失值，只有两个变量：短信的性质（垃圾邮件或非垃圾邮件）和短信文本，没有其他内容。《Spam》数据集以原始形式可在[https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)找到，并在本书的GitHub仓库[https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6)中提供。我们简单地将目标从分类值：`垃圾邮件`和`非垃圾邮件`转换为二进制值：1（代表垃圾邮件）和0（代表非垃圾邮件），这样Amazon
    ML就能理解预测为二分类类型。
- en: AWS SDK
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS SDK
- en: AWS offers several APIs and **Software Development Kits (SDKs)** to its many
    services. You can programmatically manage your files on S3, set up EC2 instances,
    and create datasources, models, and evaluations on Amazon ML without using the
    web-based user interface. The AWS APIs are low-level endpoints. In general, it
    is simpler and more efficient to use the SDKs, which are wrappers around the APIs
    and are available for several languages (Python, Ruby, Java, C++, and so on).
    In this book, we will use the Python SDK based on the `Boto3` library. We explore
    in detail the use of the Python SDK in [Chapter 7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml),
    *Command Line and SDK.* For now, we will only use the `predict()` method necessary
    for real-time predictions. But first, we need to enable access to AWS by setting
    up AWS credentials on our local machine.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Setting up AWS credentials
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to access AWS programmatically, we first need to access AWS via the
    command line. This requires the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Creating access keys on AWS IAM for your user
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the `AWS-CLI` command-line interface on local
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring `AWS-CLI` with the AWS access keys
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS access keys
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall from [Chapter 3](5938ed0c-9243-49cc-bf67-314ffb5f9386.xhtml),
    *Overview of an Amazon Machine Learning Workflow*, we had created access keys
    for our `AML@Packt` user. Access keys are user-based and composed of two parts:
    the **Access Key ID**, which is always available in the user security credentials
    tab in IAM, and the **Secret Access Key**, which is only shown at creation time.
    When creating these access keys, you are given the possibility of downloading
    them. If you did not do so at that time, you can recreate access keys for your
    user now. Go to the IAM console at  [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam),
    click on your user profile, select the Security Credentials tab, and click on
    the Create Access Key button. This time make sure you download the keys on your
    local machine or copy them somewhere. Note that there’s a limit of two sets of
    access keys per user. You will have to delete existing keys before creating new
    ones if you already have two keys associated to your user:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_03_03.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Setting up AWS CLI
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have only worked with the AWS web interface, clicking from page to
    page on the AWS website. Another way to interact with AWS services is via the
    command line in a terminal window, using the `aws cli` library. CLI stands for
    Command Line Interface.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `aws cli` library, open a terminal window. For a Python-based
    environment (Python 2 version 2.6.5+ or Python 3 version 3.3+), installing `aws
    cli` consists of running the following command in a terminal:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Full instructions for installation in other environments are available at [http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).
     Once AWS-CLI is installed, run the following command to configure it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You will be asked for your access keys, the default region, and format. See
    [http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration)
    for more in-depth explanations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, AWS-CLI commands follow this syntax:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Test your setup by running the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see a list of your buckets, folders, and files in your s3 account.
    This is what my output looks like when I list the file and folders in the `aml.packt`
    bucket:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: We will explore in detail how to run your Amazon ML projects using CLI in [Chapter
    7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml), C*ommand Line and SDK*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Python SDK
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will not use the AWS-CLI any further in this chapter, but instead switch
    to the Python SDK. We needed to setup the credentials for the AWS CLI in order
    for our SDKs scripts to be able to access our AWS account. To use the Python SDK,
    we need to install the `Boto3` library, which comes bundled in the Anaconda distribution.
    If you use Anaconda as your Python environment, you should already have the `boto3`
    package installed. If not, you can install it using `pip` with the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Boto3** will use the credentials we configured for the AWS CLI. There’s no
    need for a specific setup. Boto3 is available for most AWS services. The full
    documentation is available at [https://boto3.readthedocs.io/](https://boto3.readthedocs.io/).
    Our minimal use of `Boto3` only requires to specify the service we want, Machine
    Learning, and then use the `predict()` method to send the proper data to the model.
    In return, we obtain the predictions we wanted. The following Python code initiates
    a client to access the machine learning service.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `predict()` method requires the following parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`MLModelId`: The ID of the model your want to use to predict'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PredictEndpoint`*:* The URL of the Amazon ML endpoint for your model'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Record`*:* A JSON-formatted version of the sample'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `MLModelId` and `PredictEndpoint` URL can be obtained from the model summary
    page. The `Record` is a JSON-formatted string. We will simulate a streaming application
    by opening a held-out set of samples, looping through each sample and sending
    it via the `predict()` method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'We have split the initial dataset into a training set of 4,400 samples and
    a held-out set of 1,174 samples. These subsets are available at the GitHub repository. We
    create a datasource for the training subset, and create a model and its evaluation
    with default settings (mild L2 regularization). We keep the inferred schema (binary
    and text), the suggested recipe (no transformation besides tokenization of the
    text variable), and use the default model parameters (10 passes and mild L2 regularization).
    The training dataset is further split by Amazon ML into a smaller training dataset
    and a validation dataset, respectively 70% and 30% of the initial *4,400* samples.
    The AUC score obtained on the validation set is very high at *0.98*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_08.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: 'To get the `ModelID` and the `endpoint` URL, go to your summary page for the
    model. Copy the `ModelID` from the top of the page. Then scroll down to get to
    the prediction section and click on the Create endpoint button:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_09.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: At that point, you will be given an estimate of the real-time prediction pricing
    for your model and asked to confirm the creation of the endpoint.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The size of your model is 502.1 KB. You will incur the reserved capacity charge
    of $0.001 for every hour your endpoint is active. The prediction charge for real-time
    predictions is $0.0001 per prediction, rounded up to the nearest penny**.**
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, the endpoint will be created and you will have the endpoint
    URL:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_10.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Now that we know the endpoint URL, we can write a simple Python code that sends
    an SMS message, a simple text, to our prediction model and see whether this message is
    predicted to be spam or ham. We send the text `Hello world, my name is Alex` to
    be classified as ham, while the text `Call now to get dating contacts for free,
    no cash no credit card` should probably be detected as spam due to the presence
    of the words *free*, *cash*, *dating*, and so forth.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'The initialization/declaration part of the code is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now use the `predict()` function of the machine learning service SDK:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, pretty print the response:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This returns the following JSON-formatted string:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The JSON response is composed of two parts: the first part is related to the
    request itself, `ResponseMetadata`, and the second is related to the `Prediction`.
    The `HTTPStatusCode` in the `*ResponseMetadata*` part tells us that our query was
    successful `("HTTPStatusCode": 200)`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The interpretation of the Prediction part is straightforward. The SMS was predicted
    to be spam with a very low probability of 0.12%, hence it was classified as ham,
    which is what we expected for the text `Hello world, my name is Alex`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'We expect the text `Call now to get dating contacts for free, no cash no credit
    card` to be classified as spam, the words `free`, `call`, and `dating` usually
    being strong indicators of spam messages. We get the following in return:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The text is classified as spam, which is what we expected. As far as we can
    tell from these two simple examples, our model seems to be working fine. Once
    we can obtain prediction via API calls on a sample-by-sample basis, it becomes
    feasible to hook an incoming stream of data into the endpoint and obtain real-time
    predictions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: To simulate that pipeline, we can use Python to read a whole file of new samples,
    send each one to the model, and capture the results. Let’s do that with the held-out
    set of Spam samples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code reads the file, loads it into a panda dataframe,
    and loops over each row of the dataframe. We use `iterrows()` to loop over each
    row of the dataframe. This method is slower than `itertuples()`, but has better
    code readability. The following code is not optimized:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The responses from Amazon ML are blazingly fast. Thousands of samples are processed
    in a few seconds. This is an extract of the results we get:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_11.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Here, each line is formatted as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that, out of the three SMS detected as spam, only two were actually spam
    SMS. The text "*Money i have won wining number 946 wot do i do next*" was probably
    detected as spam due to the presence of the words "Money" or "wining" but was
    in fact a ham message.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, across the whole predictions, the probabilities are very close to
    either 0 or 1, indicating that the model is very decisive in its classification.
    No hesitation. The ROC curve for the held-out dataset shows that high level of
    accuracy:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_06_12.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the final step in the Amazon ML workflow, the predictions.
    Amazon ML offers several ways to apply your models to new datasets in order to
    make predictions. Batch mode involves submitting all the new data at once to the
    model and returning the actual predictions in a csv file on S3\. Real-time predictions,
    on the other hand, are based on sending samples one by one to an API and getting
    prediction results in return. We looked at how to create an API on the Amazon
    ML platform. We also started using the command line and the Python SDK to interact
    with the Amazon ML service -- something we will explore in more depth in [Chapter
    7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml), *Command Line and SDK*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: As explained in the previous chapters, the Amazon ML service is built around
    the Stochastic Gradient Descent (SGD) algorithm. This algorithm has been around
    for many years and is used in many different domains and applications, from signal
    processing and adaptive filtering to predictive analysis or deep learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will present the algorithm and some if its versions,
    and bring to light its behavior when dealing with different types of data and
    prediction problems. We will explain why quantile binning of numeric values, which
    is often frowned upon, is such a performance and stability booster in our case.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
