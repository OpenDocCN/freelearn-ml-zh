- en: '*Chapter 12*: DataRobot Python API'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Users can access DataRobot's capabilities using DataRobot's Python client package.
    This lets us ingest data, create machine learning projects, make predictions from
    models, and manage models programmatically. It is easy to see the advantages that
    **Application Programming Interfaces** (**APIs**) offer users. The integrated
    use of Python and DataRobot lets us leverage the AutoML capabilities DataRobot
    presents, all while exploiting the programmatic flexibility and potential that
    Python possesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the DataRobot Python API to ingest data, create
    a project with models, evaluate the models, and make predictions against them.
    At a high level, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the DataRobot API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the DataRobot Python client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building models programmatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions programmatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the analysis and modeling that will be carried out in this chapter, you
    will need access to the DataRobot software. Jupyter Notebook is crucial for this
    chapter as most of the interactions with DataRobot will be carried out from the
    console. Your Python version should be 2.7 or 3.4+. Now, let's look at the dataset
    that will be utilized in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action at [https://bit.ly/3wV4qx5](https://bit.ly/3wV4qx5).
  prefs: []
  type: TYPE_NORMAL
- en: Automobile Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The automobile dataset can be accessed at the UCI Machine Learning Repository
    ( [https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)).
    Each row in this dataset represents a specific automobile. The features (columns)
    describe its characteristics, risk rating, and associated normalized losses. Even
    though it is a small dataset, it has many features that are numerical as well
    as categorical. Its features are described on its web page and the data is provided
    in`.csv` format.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the DataRobot API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The programmatic use of DataRobot enables data experts to leverage the platform's
    efficacies while having the flexibility associated with typical programming. With
    the API access of DataRobot, data from numerous sources can be integrated for
    analytic or modeling purposes. This capability is not only limited to the data
    that's ingested, but also the output of the outcome. For instance, API access
    makes it possible for a customer risk profiling model to get data from differing
    sources, such as Google BigQuery, local files, as well as AWS S3 buckets. And
    in a few lines of codes, the outcomes can update records on Salesforce, as well
    as those surfaced on PowerBI via a BigQuery table. The strength of this multiple
    data source integration capability is furthered as this enables the automated,
    scheduled, end-to-end periodic refresh of model outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In this preceding case, it becomes possible for the client base to be rescored
    periodically. Regarding scoring data, the DataRobot platform can only score datasets
    that are less than 1 GB in size. When problems require huge datasets, the **Batch
    Prediction API** normally chunks up the data and scores them concurrently. For
    a dataset with hundreds of millions of rows, it is possible to set up an iterative
    job to chunk up the data and score it iteratively using the Batch Prediction API.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the API access to DataRobot allows users to develop user-defined
    features that make commercial sense before analysis and those based on scored
    model outcomes. This makes the modeling process more robust as it allows human
    intelligence to be applied to outcomes. In the preceding client risk profiling
    case, it becomes possible to classify customers into risk categories for easier
    business decision making. Also, based on the explanations given, the next best
    actions could be developed.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, programmatic use of DataRobot allows users to configure differing
    visualizations as they deem fit. This also offers analysts a broader range of
    visual outcome types. The Seaborn and Matplotlib Python libraries offer a huge
    range of visualization types with differing configurations. This also allows certain
    data subgroups or splits to be visualized. Among other benefits, it becomes possible
    to even select certain aspects of the data to be visualized.
  prefs: []
  type: TYPE_NORMAL
- en: One of the big advantages of accessing DataRobot using its API is the ability
    to create multiple projects iteratively. Two easy examples come to mind here.
    One approach to improving the outcomes of multi-class modeling is to use the one
    versus all modeling paradigm. This involves creating models for each of the classes.
    When scoring, all the models are used to score the data and for each row, the
    class with the highest score is attributed to the row. To bring this to life,
    let's assume we are building models to predict wheel drive types based on other
    characteristics. First, models are created for the three main types of wheel drives;
    that is, **front-wheel drive** (**FWD**), **four-wheel drive** (**4WD**), and
    **rear-wheel drive** (**RWD**). Data is then scored against all three models,
    and the model that presents each row with the highest prediction is assumed as
    the class the row belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: The model factory is another example where multiple model projects are integrated
    into a system so that each project builds models for a subgroup in the data. In
    some problems, data tends to be nested in that certain variables tend to govern
    the way models behave generally. A point in case is modeling the performance of
    students nested in class. These features, such as the class teacher for schools,
    tend to control the effect other exogenous variables have on the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of cars, their brands typically drive their prices. For instance,
    irrespective of how similar a Skoda is to an Audi, the Audi will most likely be
    more expensive. As such, when developing models for such a case, it is ideal to
    create models for each of the car brands. In the context of programmatically accessing
    DataRobot, the idea would be to run an iteration of the project for each of the
    car brands.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to creating and scoring DataRobot models programmatically, we will
    use Jupyter Notebook's **Integrated Development Environment** (**IDE**) to build
    projects for a case of one versus all and a model factory. However, before we
    can create projects with DataRobot using an API, certain identification processes
    must be covered. Let's have a look.
  prefs: []
  type: TYPE_NORMAL
- en: 'To programmatically access DataRobot, users need to create an API key. This
    key is then used to access the platform from a client. To create an API key, open
    the **Account** menu at the top right-hand corner of the home page (see *Figure
    12.1*). From there, access the **Developer Tools** window (see *Figure 12.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Accessing Developer Tools'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Accessing Developer Tools
  prefs: []
  type: TYPE_NORMAL
- en: 'After opening the **Developer Tools** window, click on **Create New Key** and
    enter the name of the new key. On saving the new key''s name, the API key will
    be generated (see *Figure 12.2*). After this, the generated key is copied and
    secured. The API key, along with the endpoint, is necessary to establish a connection
    between the local machine and the DataRobot instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Creating an API key'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Creating an API key
  prefs: []
  type: TYPE_NORMAL
- en: The endpoint parameter is the URL of the DataRobot endpoint. [https://app.datarobot.com/api/v2](https://app.datarobot.com/api/v2)
    is the default endpoint for the US cloud-based endpoint for its US and Japanese
    users. The EU-managed cloud endpoint is [https://app.eu.datarobot.com/api/v2](https://app.eu.datarobot.com/api/v2).
    VPC, on-premises, hybrid, or private users usually have their deployment endpoint
    as their DataRobot GUI root. To enhance security, these credentials are sometimes
    stored and accessed as `.yaml` files. These two credentials enable a connection
    between a computer and a DataRobot instance to use the DataRobot Python client.
  prefs: []
  type: TYPE_NORMAL
- en: Using the DataRobot Python client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python programming language is one of the most popular programming languages
    used by data scientists. It is flexible yet powerful. Being able to integrate
    the AutoML capabilities of DataRobot and utilize the flexibility of Python offers
    data scientists various benefits, as we mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Programming in Python using the Jupyter IDE.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's explore the DataRobot Python client.
  prefs: []
  type: TYPE_NORMAL
- en: To use the DataRobot Python client, Python must be version 2.7 or 3.4+. The
    most up-to-date version of DataRobot must be installed. For the cloud version,
    the `pip` command will install the most recent version of the `DataRobot` package.
    On Python, running `!pip install datarobot` should install the `DataRobot` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having installed the `DataRobot` package, the package has been imported. The
    `Client` method of the `DataRobot` package provides the much-needed connection
    to the DataRobot instance. As shown in *Figure 12.3*, the basic format for the
    `Client` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In terms of data ingestion, data can be imported from different sources. This
    process is identical to normal data imports with Python. The local file installation
    is quite straightforward. Here, all you need is the API key and the file path.
    *Figure 12.3* presents the code for ingesting the automobile dataset. For the
    JDBC connection, to get data from platforms such as BigQuery and Snowflake, in
    addition to the API key, the identity of the data source object is required, as
    well as the user database's credentials – their usernames and passwords. The user
    database's credentials are provided by their organization's database administrators.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we established how to access the credentials necessary to programmatically
    use DataRobot. We have also imported data programmatically. Naturally, conducting
    some analysis and modeling comes after ingesting data. In the next section, we
    will create machine learning models using the Python API.
  prefs: []
  type: TYPE_NORMAL
- en: Building models programmatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have imported the data, we will start building models programmatically.
    We will look at building the most basic models, then explore how to extract and
    visualize feature impact, before evaluating the performance of our models. Then,
    we will create more complex projects. Specifically, we will build one versus all
    **multiclass** classification models and **model factories**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a DataRobot project, we must use the DataRobot `Project.start` method.
    The basic format for this is importing the necessary libraries (DataRobot, in
    the following case). Thereafter, the access credentials are presented, as described
    in the previous section. It is at the point that the `Project` method is called.
    `project_name`, `sourcedata`, and `target` are the minimal parameters that are
    required by the `Project` method for projects to be created. The `project_name`
    parameter tells DataRobot the name to give the created project. `sourcedata` provides
    information regarding the location of the data that''s required to create models.
    This could be a location or a Python object. Finally, `target` specifies the target
    variable for the models to be built, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The basic format for creating projects was shown in the preceding section and
    illustrated in *Figure 12.3*. Once the models have been created, we can use the
    `project.get_models` method to get a list of them. This list of models is presented
    in order by their validation scores by default. For this example, we will be using
    the automobile dataset, which we used to build models in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. The project''s name is `autoproject_1`. Here,
    the file''s location is specifically stored in a pandas object called `data`.
    The target variable is `price`. Note that these parameters are case-sensitive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve created the model, the `get_models` method is called to list the
    models. We can see that the best performing model is `Gradient Boosted Greedy
    Trees Regressor (Least-Square Loss)`. To evaluate this model, we need to extract
    its ID. To do so, we must create an object, `best_model_01`, to store the best-performing
    model. This metrics method is then called for this model. As shown in the following
    screenshot, the cross-validation RMSE for this model is `2107.40`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Programmatically evaluating DataRobot models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Programmatically evaluating DataRobot models
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide some insight into the price drivers, we need the feature impacts.
    These can be retrieved through the DataRobot API using the `get_or_feature_impact`
    method. To visualize the feature impacts for projects, we must define a function
    called `plot_FI` that takes in the model''s name and chart title as parameters,
    gets the feature impacts, and then normalizes and plots them using Seaborn''s
    bar plot method. Regarding the `autoproject_1` project, the following screenshot
    shows how to retrieve and present the feature impacts using the `plot_FI` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Defining a function and extracting the feature impacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Defining a function and extracting the feature impacts
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmatic access to DataRobot furthers the benefits the platform offers.
    With programmatic access, you can take advantage of the iterative process within
    Python, and users can create multiple projects for the same dataset. Now, let''s
    look at two ways to create multiple projects from the same dataset: **multi-class**
    classification and **model factory**.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification involves classifying instances into more than two
    classes. It is possible to create a single project that classifies rows into either
    of these classes. Essentially, this is a model that classes rows into one of all
    the available classes. Another way to approach this problem involves building
    different models for the different classes. Within this approach, a model is built
    for each of the classes as a target. You can see how this can be executed using
    Python's iterative process; that is, by looping through all the target levels.
    The one versus all method is better for performing classification problems with
    more than two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s demonstrate how to use the one versus all method on the auto pricing
    project. Here, we will create price classes using the pandas `qcut`. `qcut` helps
    divide data into similarly sized bins. Using this function, we can divide our
    data into price classes – low to high. The following screenshot shows this price
    discretizing process and checking the distribution of cases across the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Price discretization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Price discretization
  prefs: []
  type: TYPE_NORMAL
- en: 'Having created the classes, to allow for data **leakages**, we will drop the
    initial price variable. We will write a loop that builds models for each of the
    price classes. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Turn the `price_class` variable into dummy variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each iteration, create a DataRobot project after a dummified price class
    name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each iteration, we drop the `price_class` dummy level being modeled. This
    ensures that there are no leakages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each iteration, we must build the models for a target variable dummy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After creating the projects, the top-performing model for each project is selected
    and stored in a dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Creating a one versus all classification suite of projects'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Creating a one versus all classification suite of projects
  prefs: []
  type: TYPE_NORMAL
- en: This process involves creating projects with a suite of models with targets
    iterating through all the price classes. After creating the projects, the best
    model for each target class is selected using an iteration of all the projects
    with names starting with `Auto`, and then the top-performing model for each project.
    These best models are placed in a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes recommended, if not ideal, to create different projects with
    a subset of the data. After selecting all the cases for the target variable, you
    must create a random subset of the data for each project creation iteration. In
    the auto pricing case, however, we were unable to explore this as the out-sample
    size was limiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fuel-type`):'
  prefs: []
  type: TYPE_NORMAL
- en: First, create and store a project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select cases for the target variable (the influencer of interest). In this case,
    the variable is `fuel-type`. Here, this variable is selected, and differing levels
    of this variable are used to create DataRobot projects. In simple terms, this
    step involves, for instance, selecting all the rows with `fuel-type` set to `gas`
    as a subgroup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If necessary, define the evaluation metric. Here, we can alter aspects of the
    advanced options we encountered in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. Other advance options can be selected and altered.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If necessary, set a data limit that a class will be deselected for (for instance,
    if the number of rows is less than 20 for that class). The importance of this
    step lies in the fact that some variable levels could have very low occurrences,
    so the sample size within the subgroup is small. Therefore, creating models out
    of these becomes a challenge. This step becomes the best place to drop such variable
    levels using the count of cases within the subgroup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the models from all the projects are selected and stored in a dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of these steps are evident in creating a model factor for the auto pricing
    problem (see *Figure 12.8*). Here, `fuel-type` is selected as the feature that
    projects are created on. In this case, only two projects are created: one for
    gas automobiles and another for diesel ones. Now that we''ve created the models,
    the next step is to collect the best-performing models for each `fuel-type`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Creating model factories'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Creating model factories
  prefs: []
  type: TYPE_NORMAL
- en: The efficacy of using one versus all multiclass classification models and model
    factories lies in their ability to fit models to each level of the target variable.
    This happens in an automated fashion and considers the sample validation, all
    the preprocessing steps, and the model training process. When data cardinality
    and volume are high, these approaches would mostly outperform typical modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the model factory, multiple projects are created for the different levels
    of the feature of interest. To evaluate this, the best-performing model for each
    project is selected from the dictionary for all projects. This set of best models
    from all the projects is stored in another dictionary object. A `for` loop is
    then run across all the models of the dictionary to extract the performance of
    the model, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Evaluating the performance of models with a model factory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Evaluating the performance of models with a model factory
  prefs: []
  type: TYPE_NORMAL
- en: 'Improved model performance is only one of the reasons you should use the one
    versus all multiclass classification models, as well as model factors. Sometimes,
    understanding the drivers is equally as important. Visualizing the feature importance
    for the different fuel types could present an interesting contrast in drivers.
    This means that different factors affect the prices of different fuel types. This
    could have a bearing on strategic decisions. As shown in the following screenshot,
    the Python API can be used to plot the feature impacts by leveraging chart functions
    from Seaborn and Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Feature impacts for the differing diesel and gas automobiles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Feature impacts for the differing diesel and gas automobiles
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there are some differences in the feature impacts for the automobile
    fuel types. While `curb-weight` seems to be an important driver, its effect is
    relatively more important for diesel vehicles. Similarly, for gas cars, the power
    that's generated by these automobiles, as typified by the `engine_size` and `horsepower`
    features, carries more importance in determining price than those of diesel cars.
    You can already see the effect such preliminary findings could have on decisions
    and how this could be applied to other commercial cases. Using feature importance
    to examine multiple models can also be applied in the case of one versus all classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we created basic DataRobot projects using the Python API. After,
    we solved more complex problems by using multiple projects within a system. There,
    we created one versus all projects to solve multiclass classification problems
    and model factories to solve multi-level problems involving subgroups. We also
    explored feature impact and model evaluation. Having programmatically created
    models, we will now learn how to make predictions using these models. Specifically,
    we will learn how to deploy models, make predictions, extract explanations from
    models, and score large datasets through parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions programmatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The possibilities that programmatically using DataRobot presents are enormous.
    By using its API, models can be deployed and predictions can be made against them.
    Before making programmatical predictions within the production environment, models
    need to be deployed. DataRobot models are deployed using Portable Prediction Servers.
    These are Docker containers that can host machine learning models, which serve
    predictions and prediction explanations through a REST API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy models, we can use the DataRobot package''s `deployment` method.
    Here, we must provide a description, the DataRobot model''s ID, as well as its
    label to create the deployments. A typical Python deployment script follows this
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As per this approach, the following screenshot shows how `autoproject_1`, which
    we created in the *Building models programmatically* section, can be deployed.
    Here, the model ID is `best_model_1`. We will label `AutoBase Deployment` with
    a description of `Base Automobile Price Deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Deploying a model programmatically'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – Deploying a model programmatically
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment process can be iterated to enable those of more complex projects.
    For instance, with model factories, irrespective of the number of levels the differentiating
    variable has, with a single `for` loop, all the best models can be deployed to
    DataRobot. For each of the best models, a deployment is created, which is then
    used to score new data. The script for deploying the model factory for the automobile
    project, along with the fuel type as its differentiating variable, is shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Deploying models from a model factory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Deploying models from a model factory
  prefs: []
  type: TYPE_NORMAL
- en: 'Having deployed the models, predictions can be made against them. To make simple
    predictions within the development environment, we can use the `DataRobot BatchPredictionJob.score_to_file`
    method. To make predictions, this method requires the model ID, prediction data,
    and the location where the scored data will be stored. Here, we will use `best_model_1`
    to score the same model we used to develop the model, the `df` data object, and
    the location path, which specifies the prediction file path as `./pred.csv`. The
    `passthrough_columns_set` parameter specifies the columns from the original dataset
    that will be included in the predictions. Since this is set to `''all''`, all
    the columns are returned, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – Simple programmatic prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_12_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.13 – Simple programmatic prediction
  prefs: []
  type: TYPE_NORMAL
- en: These predictions comprise all the columns from the initial dataset, in addition
    to the predicted prices. There are cases where it is ideal to include rationales
    behind predictions. In such cases, the `max_explanations` parameter should be
    included in the job's configuration. This parameter sets the highest number of
    explanations to be provided for every data row.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataRobot provides us with a unique capability to rapidly develop models. With
    this platform, data scientists can combine the benefits of DataRobot and the flexibilities
    of open programming. In this chapter, we explored ways to access the credentials
    needed to programmatically use DataRobot. Using the Python client, we demonstrated
    ways in which data can be ingested and how basic projects can be created. We started
    building models for more complex problems. We created model factories as well
    as one versus all models. Finally, we demonstrated how models can be deployed
    and used to score data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of programmatically using DataRobot is the ability
    to ingest data from numerous sources, score them, and store them in the relevant
    sources. This makes it possible to carry out end-to-end dataset scoring. It becomes
    possible for a system to be set up to score models periodically. With this comes
    numerous data quality and model monitoring concerns. The next chapter will focus
    on how to control the quality of the models and data on the DataRobot platform,
    as well as using the Python API.
  prefs: []
  type: TYPE_NORMAL
