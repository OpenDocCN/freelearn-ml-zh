- en: '*Chapter 12*: DataRobot Python API'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：DataRobot Python API'
- en: Users can access DataRobot's capabilities using DataRobot's Python client package.
    This lets us ingest data, create machine learning projects, make predictions from
    models, and manage models programmatically. It is easy to see the advantages that
    **Application Programming Interfaces** (**APIs**) offer users. The integrated
    use of Python and DataRobot lets us leverage the AutoML capabilities DataRobot
    presents, all while exploiting the programmatic flexibility and potential that
    Python possesses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过使用 DataRobot 的 Python 客户端包来访问 DataRobot 的功能。这使得我们可以摄取数据，创建机器学习项目，从模型中进行预测，并编程管理模型。API
    为用户提供的优势很容易看出。Python 和 DataRobot 的集成使用使我们能够利用 DataRobot 提供的 AutoML 功能，同时利用 Python
    所具有的程序灵活性和潜力。
- en: 'In this chapter, we will use the DataRobot Python API to ingest data, create
    a project with models, evaluate the models, and make predictions against them.
    At a high level, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 DataRobot Python API 摄取数据，创建包含模型的工程，评估模型，并对它们进行预测。从高层次上讲，我们将涵盖以下主题：
- en: Accessing the DataRobot API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 DataRobot API
- en: Understanding the DataRobot Python client
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 DataRobot Python 客户端
- en: Building models programmatically
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程构建模型
- en: Making predictions programmatically
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程进行预测
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the analysis and modeling that will be carried out in this chapter, you
    will need access to the DataRobot software. Jupyter Notebook is crucial for this
    chapter as most of the interactions with DataRobot will be carried out from the
    console. Your Python version should be 2.7 or 3.4+. Now, let's look at the dataset
    that will be utilized in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章将要进行的分析和建模中，您将需要访问 DataRobot 软件。Jupyter Notebook 对于本章至关重要，因为与 DataRobot 的大多数交互都将通过控制台进行。您的
    Python 版本应为 2.7 或 3.4+。现在，让我们看看本章将使用的数据库集。
- en: Check out the following video to see the Code in Action at [https://bit.ly/3wV4qx5](https://bit.ly/3wV4qx5).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用效果：[https://bit.ly/3wV4qx5](https://bit.ly/3wV4qx5)。
- en: Automobile Dataset
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汽车数据集
- en: The automobile dataset can be accessed at the UCI Machine Learning Repository
    ( [https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)).
    Each row in this dataset represents a specific automobile. The features (columns)
    describe its characteristics, risk rating, and associated normalized losses. Even
    though it is a small dataset, it has many features that are numerical as well
    as categorical. Its features are described on its web page and the data is provided
    in`.csv` format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据集可在 UCI 机器学习数据库中访问（[https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile)）。该数据集中的每一行代表一辆特定的汽车。特征（列）描述了其特征、风险评级和相关的归一化损失。尽管这是一个小型数据集，但它具有许多数值和分类特征。其特征在网页上进行了描述，数据以
    `.csv` 格式提供。
- en: Dataset Citation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: 'Dua, D. and Graff, C. (2019). UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Dua, D. 和 Graff, C. (2019). UCI 机器学习数据库 ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    加州欧文：加州大学信息与计算机科学学院。
- en: Accessing the DataRobot API
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问 DataRobot API
- en: The programmatic use of DataRobot enables data experts to leverage the platform's
    efficacies while having the flexibility associated with typical programming. With
    the API access of DataRobot, data from numerous sources can be integrated for
    analytic or modeling purposes. This capability is not only limited to the data
    that's ingested, but also the output of the outcome. For instance, API access
    makes it possible for a customer risk profiling model to get data from differing
    sources, such as Google BigQuery, local files, as well as AWS S3 buckets. And
    in a few lines of codes, the outcomes can update records on Salesforce, as well
    as those surfaced on PowerBI via a BigQuery table. The strength of this multiple
    data source integration capability is furthered as this enables the automated,
    scheduled, end-to-end periodic refresh of model outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot的程序化使用使数据专家能够在保持典型编程的灵活性的同时利用平台的高效性。通过DataRobot的API访问权限，可以集成来自多个来源的数据，用于分析或建模目的。这种能力不仅限于摄入的数据，还包括结果的输出。例如，API访问使得客户风险分析模型能够从不同的来源获取数据，例如Google
    BigQuery、本地文件以及AWS S3存储桶。通过几行代码，可以在Salesforce上更新记录，以及通过BigQuery表在PowerBI上显示的记录。这种多数据源集成能力的优势进一步体现在它能够实现模型结果的自动化、计划内、端到端的定期刷新。
- en: In this preceding case, it becomes possible for the client base to be rescored
    periodically. Regarding scoring data, the DataRobot platform can only score datasets
    that are less than 1 GB in size. When problems require huge datasets, the **Batch
    Prediction API** normally chunks up the data and scores them concurrently. For
    a dataset with hundreds of millions of rows, it is possible to set up an iterative
    job to chunk up the data and score it iteratively using the Batch Prediction API.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前的案例中，客户基础可以定期重新评分。关于评分数据，DataRobot平台只能评分小于1 GB大小的数据集。当问题需要大量数据集时，**批量预测API**通常会将数据分块并并发评分。对于包含数亿行数据的集合，可以使用批量预测API设置迭代作业，分块并迭代评分数据。
- en: In addition, the API access to DataRobot allows users to develop user-defined
    features that make commercial sense before analysis and those based on scored
    model outcomes. This makes the modeling process more robust as it allows human
    intelligence to be applied to outcomes. In the preceding client risk profiling
    case, it becomes possible to classify customers into risk categories for easier
    business decision making. Also, based on the explanations given, the next best
    actions could be developed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DataRobot的API访问权限允许用户在分析之前开发具有商业意义的用户自定义特征，以及基于评分模型结果的那些特征。这使得建模过程更加稳健，因为它允许将人类智慧应用于结果。在前面的客户风险分析案例中，可以将客户分类到风险类别，以便于更轻松地进行商业决策。此外，根据所提供的解释，还可以开发下一步的最佳行动方案。
- en: Furthermore, programmatic use of DataRobot allows users to configure differing
    visualizations as they deem fit. This also offers analysts a broader range of
    visual outcome types. The Seaborn and Matplotlib Python libraries offer a huge
    range of visualization types with differing configurations. This also allows certain
    data subgroups or splits to be visualized. Among other benefits, it becomes possible
    to even select certain aspects of the data to be visualized.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DataRobot的程序化使用允许用户根据需要配置不同的可视化。这也为分析师提供了更广泛的视觉结果类型。Seaborn和Matplotlib Python库提供了大量不同配置的可视化类型。这也允许可视化某些数据子组或分割。在其他好处中，甚至可以选择要可视化的数据的一些特定方面。
- en: One of the big advantages of accessing DataRobot using its API is the ability
    to create multiple projects iteratively. Two easy examples come to mind here.
    One approach to improving the outcomes of multi-class modeling is to use the one
    versus all modeling paradigm. This involves creating models for each of the classes.
    When scoring, all the models are used to score the data and for each row, the
    class with the highest score is attributed to the row. To bring this to life,
    let's assume we are building models to predict wheel drive types based on other
    characteristics. First, models are created for the three main types of wheel drives;
    that is, **front-wheel drive** (**FWD**), **four-wheel drive** (**4WD**), and
    **rear-wheel drive** (**RWD**). Data is then scored against all three models,
    and the model that presents each row with the highest prediction is assumed as
    the class the row belongs to.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataRobot的API访问它的一个主要优势是能够迭代地创建多个项目。这里有两个简单的例子。提高多类建模结果的一种方法是用一对一的建模范式。这涉及到为每个类别创建模型。在评分时，所有模型都用于评分数据，并且对于每一行，具有最高评分的类别被分配给该行。为了使这一点更加生动，让我们假设我们正在构建基于其他特征的预测车轮驱动类型的模型。首先，为三种主要的车轮驱动类型创建模型；即，**前轮驱动**（**FWD**）、**全轮驱动**（**4WD**）和**后轮驱动**（**RWD**）。然后，数据与所有三个模型进行评分，并假设呈现每一行最高预测的模型是该行所属的类别。
- en: The model factory is another example where multiple model projects are integrated
    into a system so that each project builds models for a subgroup in the data. In
    some problems, data tends to be nested in that certain variables tend to govern
    the way models behave generally. A point in case is modeling the performance of
    students nested in class. These features, such as the class teacher for schools,
    tend to control the effect other exogenous variables have on the dependent variable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工厂是另一个例子，其中多个模型项目被集成到一个系统中，以便每个项目为数据中的子组构建模型。在某些问题中，数据往往嵌套在那些某些变量倾向于控制模型行为方式的地方。一个例子是建模嵌套在班级中的学生表现。这些特征，如学校的班主任，往往控制其他外生变量对因变量的影响。
- en: In the case of cars, their brands typically drive their prices. For instance,
    irrespective of how similar a Skoda is to an Audi, the Audi will most likely be
    more expensive. As such, when developing models for such a case, it is ideal to
    create models for each of the car brands. In the context of programmatically accessing
    DataRobot, the idea would be to run an iteration of the project for each of the
    car brands.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车的情况下，它们的品牌通常决定了价格。例如，无论斯柯达与奥迪有多相似，奥迪很可能更贵。因此，在为这种情况开发模型时，为每个汽车品牌创建模型是理想的。在程序化访问DataRobot的上下文中，这个想法是为每个汽车品牌运行项目的一个迭代。
- en: In addition to creating and scoring DataRobot models programmatically, we will
    use Jupyter Notebook's **Integrated Development Environment** (**IDE**) to build
    projects for a case of one versus all and a model factory. However, before we
    can create projects with DataRobot using an API, certain identification processes
    must be covered. Let's have a look.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了程序化创建和评分DataRobot模型外，我们还将使用Jupyter Notebook的**集成开发环境**（**IDE**）为一对一和模型工厂的情况构建项目。然而，在我们能够使用API通过DataRobot创建项目之前，必须覆盖某些识别过程。让我们看看。
- en: 'To programmatically access DataRobot, users need to create an API key. This
    key is then used to access the platform from a client. To create an API key, open
    the **Account** menu at the top right-hand corner of the home page (see *Figure
    12.1*). From there, access the **Developer Tools** window (see *Figure 12.1*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要程序化访问DataRobot，用户需要创建一个API密钥。这个密钥随后被用来从客户端访问平台。要创建API密钥，请打开主页右上角的**账户**菜单（见*图12.1*）。从那里，访问**开发者工具**窗口（见*图12.1*）：
- en: '![Figure 12.1 – Accessing Developer Tools'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – 访问开发者工具'
- en: '](img/B17159_12_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_01.jpg)'
- en: Figure 12.1 – Accessing Developer Tools
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 访问开发者工具
- en: 'After opening the **Developer Tools** window, click on **Create New Key** and
    enter the name of the new key. On saving the new key''s name, the API key will
    be generated (see *Figure 12.2*). After this, the generated key is copied and
    secured. The API key, along with the endpoint, is necessary to establish a connection
    between the local machine and the DataRobot instance:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开**开发者工具**窗口后，点击**创建新密钥**并输入新密钥的名称。在保存新密钥的名称后，将生成API密钥（见*图12.2*）。之后，生成的密钥被复制并安全存储。API密钥以及端点是建立本地机器与DataRobot实例之间连接所必需的：
- en: '![Figure 12.2 – Creating an API key'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.2 – 创建API密钥'
- en: '](img/B17159_12_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17159_12_02.jpg)'
- en: Figure 12.2 – Creating an API key
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 创建 API 密钥
- en: The endpoint parameter is the URL of the DataRobot endpoint. [https://app.datarobot.com/api/v2](https://app.datarobot.com/api/v2)
    is the default endpoint for the US cloud-based endpoint for its US and Japanese
    users. The EU-managed cloud endpoint is [https://app.eu.datarobot.com/api/v2](https://app.eu.datarobot.com/api/v2).
    VPC, on-premises, hybrid, or private users usually have their deployment endpoint
    as their DataRobot GUI root. To enhance security, these credentials are sometimes
    stored and accessed as `.yaml` files. These two credentials enable a connection
    between a computer and a DataRobot instance to use the DataRobot Python client.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 端点参数是 DataRobot 端点的 URL。[https://app.datarobot.com/api/v2](https://app.datarobot.com/api/v2)
    是美国云端点的默认端点，适用于其美国和日本用户。欧盟管理的云端点是 [https://app.eu.datarobot.com/api/v2](https://app.eu.datarobot.com/api/v2)。VPC、本地、混合或私有用户通常将他们的部署端点作为
    DataRobot GUI 根。为了增强安全性，这些凭证有时以 `.yaml` 文件的形式存储和访问。这两个凭证使计算机与 DataRobot 实例之间建立连接，以便使用
    DataRobot Python 客户端。
- en: Using the DataRobot Python client
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataRobot Python 客户端
- en: The Python programming language is one of the most popular programming languages
    used by data scientists. It is flexible yet powerful. Being able to integrate
    the AutoML capabilities of DataRobot and utilize the flexibility of Python offers
    data scientists various benefits, as we mentioned earlier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python 编程语言是数据科学家使用最流行的编程语言之一。它既灵活又强大。能够集成 DataRobot 的 AutoML 功能并利用 Python 的灵活性，为数据科学家提供了各种好处，正如我们之前提到的。
- en: Programming in Python using the Jupyter IDE.
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Jupyter IDE 在 Python 中编程。
- en: Now, let's explore the DataRobot Python client.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索 DataRobot Python 客户端。
- en: To use the DataRobot Python client, Python must be version 2.7 or 3.4+. The
    most up-to-date version of DataRobot must be installed. For the cloud version,
    the `pip` command will install the most recent version of the `DataRobot` package.
    On Python, running `!pip install datarobot` should install the `DataRobot` package.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 DataRobot Python 客户端，Python 必须是 2.7 或 3.4+ 版本。必须安装最新版本的 DataRobot。对于云版本，`pip`
    命令将安装 `DataRobot` 包的最新版本。在 Python 中运行 `!pip install datarobot` 应该会安装 `DataRobot`
    包。
- en: 'Having installed the `DataRobot` package, the package has been imported. The
    `Client` method of the `DataRobot` package provides the much-needed connection
    to the DataRobot instance. As shown in *Figure 12.3*, the basic format for the
    `Client` method is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了 `DataRobot` 包后，该包已被导入。`DataRobot` 包的 `Client` 方法提供了连接到 DataRobot 实例所需的重要连接。如图
    *图 12.3* 所示，`Client` 方法的格式如下：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In terms of data ingestion, data can be imported from different sources. This
    process is identical to normal data imports with Python. The local file installation
    is quite straightforward. Here, all you need is the API key and the file path.
    *Figure 12.3* presents the code for ingesting the automobile dataset. For the
    JDBC connection, to get data from platforms such as BigQuery and Snowflake, in
    addition to the API key, the identity of the data source object is required, as
    well as the user database's credentials – their usernames and passwords. The user
    database's credentials are provided by their organization's database administrators.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取方面，可以从不同的来源导入数据。这个过程与使用 Python 的正常数据导入相同。本地文件安装相当简单。在这里，你需要的是 API 密钥和文件路径。*图
    12.3* 展示了导入汽车数据集的代码。对于 JDBC 连接，要从 BigQuery 和 Snowflake 等平台获取数据，除了 API 密钥外，还需要数据源对象的身份以及用户数据库的凭证——它们的用户名和密码。用户数据库的凭证由其组织的数据库管理员提供。
- en: In this section, we established how to access the credentials necessary to programmatically
    use DataRobot. We have also imported data programmatically. Naturally, conducting
    some analysis and modeling comes after ingesting data. In the next section, we
    will create machine learning models using the Python API.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何访问程序化使用 DataRobot 所需的凭证。我们还以程序方式导入了数据。自然地，在导入数据之后，进行一些分析和建模是接下来的步骤。在下一节中，我们将使用
    Python API 创建机器学习模型。
- en: Building models programmatically
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以编程方式构建模型
- en: Now that we have imported the data, we will start building models programmatically.
    We will look at building the most basic models, then explore how to extract and
    visualize feature impact, before evaluating the performance of our models. Then,
    we will create more complex projects. Specifically, we will build one versus all
    **multiclass** classification models and **model factories**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了数据，我们将开始以编程方式构建模型。我们将查看构建最基础的模型，然后探索如何提取和可视化特征影响，最后评估我们模型的性能。然后，我们将创建更复杂的项目。具体来说，我们将构建一个对多个类别的**多分类**分类模型和**模型工厂**。
- en: 'To create a DataRobot project, we must use the DataRobot `Project.start` method.
    The basic format for this is importing the necessary libraries (DataRobot, in
    the following case). Thereafter, the access credentials are presented, as described
    in the previous section. It is at the point that the `Project` method is called.
    `project_name`, `sourcedata`, and `target` are the minimal parameters that are
    required by the `Project` method for projects to be created. The `project_name`
    parameter tells DataRobot the name to give the created project. `sourcedata` provides
    information regarding the location of the data that''s required to create models.
    This could be a location or a Python object. Finally, `target` specifies the target
    variable for the models to be built, as shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个DataRobot项目，我们必须使用DataRobot的`Project.start`方法。这个方法的基本格式是导入必要的库（在下面的例子中是DataRobot）。之后，展示访问凭证，如前文所述。在调用`Project`方法时，就是这一点。`project_name`、`sourcedata`和`target`是`Project`方法创建项目所需的最小参数。`project_name`参数告诉DataRobot为创建的项目命名。`sourcedata`提供有关创建模型所需数据位置的信息。这可能是一个位置或一个Python对象。最后，`target`指定要构建的模型的目标变量，如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The basic format for creating projects was shown in the preceding section and
    illustrated in *Figure 12.3*. Once the models have been created, we can use the
    `project.get_models` method to get a list of them. This list of models is presented
    in order by their validation scores by default. For this example, we will be using
    the automobile dataset, which we used to build models in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. The project''s name is `autoproject_1`. Here,
    the file''s location is specifically stored in a pandas object called `data`.
    The target variable is `price`. Note that these parameters are case-sensitive:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 创建项目的基本格式在前一节中已展示，并在*图12.3*中进行了说明。一旦创建了模型，我们可以使用`project.get_models`方法获取它们的列表。默认情况下，这些模型按验证分数顺序排列。在这个例子中，我们将使用汽车数据集，我们在[*第6章*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104)“使用DataRobot构建模型”中用它来构建模型。项目的名称是`autoproject_1`。在这里，文件的存储位置特别存储在一个名为`data`的pandas对象中。目标变量是`price`。请注意，这些参数是区分大小写的：
- en: '![Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – 以编程方式创建DataRobot模型和提取它们的列表'
- en: '](img/B17159_12_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17159_12_03.jpg](img/B17159_12_03.jpg)'
- en: Figure 12.3 – Programmatically creating DataRobot models and extracting their
    lists
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 以编程方式创建DataRobot模型并提取它们的列表
- en: 'Once you''ve created the model, the `get_models` method is called to list the
    models. We can see that the best performing model is `Gradient Boosted Greedy
    Trees Regressor (Least-Square Loss)`. To evaluate this model, we need to extract
    its ID. To do so, we must create an object, `best_model_01`, to store the best-performing
    model. This metrics method is then called for this model. As shown in the following
    screenshot, the cross-validation RMSE for this model is `2107.40`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型后，调用`get_models`方法列出模型。我们可以看到表现最好的模型是`Gradient Boosted Greedy Trees Regressor
    (Least-Square Loss)`。为了评估这个模型，我们需要提取它的ID。为此，我们必须创建一个名为`best_model_01`的对象来存储表现最好的模型。然后对这个模型调用这个度量方法。如图所示，这个模型的交叉验证RMSE为`2107.40`：
- en: '![Figure 12.4 – Programmatically evaluating DataRobot models'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.4 – 以编程方式评估DataRobot模型'
- en: '](img/B17159_12_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17159_12_04.jpg](img/B17159_12_04.jpg)'
- en: Figure 12.4 – Programmatically evaluating DataRobot models
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 以编程方式评估DataRobot模型
- en: 'To provide some insight into the price drivers, we need the feature impacts.
    These can be retrieved through the DataRobot API using the `get_or_feature_impact`
    method. To visualize the feature impacts for projects, we must define a function
    called `plot_FI` that takes in the model''s name and chart title as parameters,
    gets the feature impacts, and then normalizes and plots them using Seaborn''s
    bar plot method. Regarding the `autoproject_1` project, the following screenshot
    shows how to retrieve and present the feature impacts using the `plot_FI` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一些关于价格驱动因素的了解，我们需要特征影响。这些可以通过 DataRobot API 使用 `get_or_feature_impact` 方法检索。为了可视化项目的特征影响，我们必须定义一个名为
    `plot_FI` 的函数，该函数接受模型名称和图表标题作为参数，获取特征影响，然后使用 Seaborn 的条形图方法进行归一化和绘图。关于 `autoproject_1`
    项目，以下截图显示了如何使用 `plot_FI` 函数检索和展示特征影响：
- en: '![Figure 12.5 – Defining a function and extracting the feature impacts'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.5 – 定义函数和提取特征影响'
- en: '](img/B17159_12_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17159_12_05.jpg](img/B17159_12_05.jpg)'
- en: Figure 12.5 – Defining a function and extracting the feature impacts
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 定义函数和提取特征影响
- en: 'Programmatic access to DataRobot furthers the benefits the platform offers.
    With programmatic access, you can take advantage of the iterative process within
    Python, and users can create multiple projects for the same dataset. Now, let''s
    look at two ways to create multiple projects from the same dataset: **multi-class**
    classification and **model factory**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过程序访问 DataRobot 可以进一步扩大该平台提供的益处。有了程序访问权限，您可以利用 Python 中的迭代过程，并且用户可以为同一数据集创建多个项目。现在，让我们看看从同一数据集创建多个项目的两种方法：**多类**分类和**模型工厂**。
- en: Multi-class classification involves classifying instances into more than two
    classes. It is possible to create a single project that classifies rows into either
    of these classes. Essentially, this is a model that classes rows into one of all
    the available classes. Another way to approach this problem involves building
    different models for the different classes. Within this approach, a model is built
    for each of the classes as a target. You can see how this can be executed using
    Python's iterative process; that is, by looping through all the target levels.
    The one versus all method is better for performing classification problems with
    more than two classes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类涉及将实例分类到两个以上的类别。可以创建一个项目，将行分类到这些类别中的任何一个。本质上，这是一个将行分类到所有可用类别之一的模型。另一种处理此问题的方法是为不同的类别构建不同的模型。在此方法中，为每个类别作为目标构建一个模型。您可以看到如何使用
    Python 的迭代过程执行此操作；即通过遍历所有目标级别。一对多方法更适合执行具有两个以上类别的分类问题。
- en: 'Now, let''s demonstrate how to use the one versus all method on the auto pricing
    project. Here, we will create price classes using the pandas `qcut`. `qcut` helps
    divide data into similarly sized bins. Using this function, we can divide our
    data into price classes – low to high. The following screenshot shows this price
    discretizing process and checking the distribution of cases across the classes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们演示如何在自动定价项目中使用一对多方法。在这里，我们将使用 pandas 的 `qcut` 创建价格类别。`qcut` 帮助将数据划分为大小相似的区间。使用此函数，我们可以将我们的数据划分为价格类别——从低到高。以下截图显示了这一价格离散化过程以及检查案例在类别中的分布：
- en: '![Figure 12.6 – Price discretization'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.6 – 价格离散化'
- en: '](img/B17159_12_06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17159_12_06.jpg](img/B17159_12_06.jpg)'
- en: Figure 12.6 – Price discretization
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – 价格离散化
- en: 'Having created the classes, to allow for data **leakages**, we will drop the
    initial price variable. We will write a loop that builds models for each of the
    price classes. Perform the following steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了类别之后，为了允许数据**泄漏**，我们将丢弃初始价格变量。我们将编写一个循环，为每个价格类别构建模型。执行以下步骤：
- en: Turn the `price_class` variable into dummy variables.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `price_class` 变量转换为虚拟变量。
- en: For each iteration, create a DataRobot project after a dummified price class
    name.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一次迭代，在虚拟价格类别名称之后创建一个 DataRobot 项目。
- en: For each iteration, we drop the `price_class` dummy level being modeled. This
    ensures that there are no leakages.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一次迭代，我们都会丢弃正在建模的 `price_class` 虚拟级别。这确保了没有泄漏。
- en: For each iteration, we must build the models for a target variable dummy.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一次迭代，我们必须为目标变量虚拟构建模型。
- en: 'After creating the projects, the top-performing model for each project is selected
    and stored in a dictionary:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建项目后，每个项目的最佳模型被选中并存储在字典中：
- en: '![Figure 12.7 – Creating a one versus all classification suite of projects'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.7 – 创建一对多分类项目套件'
- en: '](img/B17159_12_07.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17159_12_07.jpg](img/B17159_12_07.jpg)'
- en: Figure 12.7 – Creating a one versus all classification suite of projects
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 创建一个单类与所有分类的项目套件
- en: This process involves creating projects with a suite of models with targets
    iterating through all the price classes. After creating the projects, the best
    model for each target class is selected using an iteration of all the projects
    with names starting with `Auto`, and then the top-performing model for each project.
    These best models are placed in a dictionary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程涉及创建一系列具有目标迭代所有价格类别的项目。在创建项目后，使用以`Auto`开头的所有项目的迭代来选择每个目标类别的最佳模型，然后选择每个项目的表现最好的模型。这些最佳模型被放置在一个字典中。
- en: It is sometimes recommended, if not ideal, to create different projects with
    a subset of the data. After selecting all the cases for the target variable, you
    must create a random subset of the data for each project creation iteration. In
    the auto pricing case, however, we were unable to explore this as the out-sample
    size was limiting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有时建议（如果不是理想的情况）使用数据子集创建不同的项目。在选择了目标变量的所有案例之后，你必须为每个项目创建迭代创建数据的随机子集。然而，在自动定价的情况下，我们无法探索这一点，因为外样本的大小有限。
- en: 'A `fuel-type`):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'A `fuel-type`):'
- en: First, create and store a project.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建并存储一个项目。
- en: Select cases for the target variable (the influencer of interest). In this case,
    the variable is `fuel-type`. Here, this variable is selected, and differing levels
    of this variable are used to create DataRobot projects. In simple terms, this
    step involves, for instance, selecting all the rows with `fuel-type` set to `gas`
    as a subgroup.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择目标变量（感兴趣的变量）的案例。在这个例子中，变量是`fuel-type`。在这里，这个变量被选中，并且使用这个变量的不同级别来创建DataRobot项目。简单来说，这一步包括，例如，选择所有将`fuel-type`设置为`gas`的行作为一个子组。
- en: If necessary, define the evaluation metric. Here, we can alter aspects of the
    advanced options we encountered in [*Chapter 6*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104),
    *Model Building with DataRobot*. Other advance options can be selected and altered.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，定义评估指标。在这里，我们可以修改我们在[*第6章*](B17159_06_Final_NM_ePub.xhtml#_idTextAnchor104)中遇到的“使用DataRobot进行模型构建”的高级选项。其他高级选项也可以选择并修改。
- en: If necessary, set a data limit that a class will be deselected for (for instance,
    if the number of rows is less than 20 for that class). The importance of this
    step lies in the fact that some variable levels could have very low occurrences,
    so the sample size within the subgroup is small. Therefore, creating models out
    of these becomes a challenge. This step becomes the best place to drop such variable
    levels using the count of cases within the subgroup.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，设置一个数据限制，即某个类别将被取消选择（例如，如果该类别的行数少于20）。这一步骤的重要性在于，某些变量级别可能发生频率非常低，因此子组内的样本量很小。因此，从这些变量中创建模型成为了一个挑战。这一步骤成为使用子组内案例计数来删除这些变量级别的最佳位置。
- en: All the models from all the projects are selected and stored in a dictionary.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有项目中选择了所有模型并将它们存储在一个字典中。
- en: 'Some of these steps are evident in creating a model factor for the auto pricing
    problem (see *Figure 12.8*). Here, `fuel-type` is selected as the feature that
    projects are created on. In this case, only two projects are created: one for
    gas automobiles and another for diesel ones. Now that we''ve created the models,
    the next step is to collect the best-performing models for each `fuel-type`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建自动定价问题的模型因子时，一些步骤是显而易见的（参见*图12.8*）。在这里，`fuel-type`被选为创建项目的基础特征。在这种情况下，只创建了两个项目：一个用于汽油汽车，另一个用于柴油汽车。现在我们已经创建了模型，下一步是收集每个`fuel-type`的最佳性能模型：
- en: '![Figure 12.8 – Creating model factories'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 – 创建模型工厂'
- en: '](img/B17159_12_08.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17159_12_08.jpg)'
- en: Figure 12.8 – Creating model factories
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 创建模型工厂
- en: The efficacy of using one versus all multiclass classification models and model
    factories lies in their ability to fit models to each level of the target variable.
    This happens in an automated fashion and considers the sample validation, all
    the preprocessing steps, and the model training process. When data cardinality
    and volume are high, these approaches would mostly outperform typical modeling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单类与所有多类分类模型以及模型工厂的有效性在于它们能够根据目标变量的每个级别来拟合模型。这个过程是自动化的，并考虑样本验证、所有预处理步骤以及模型训练过程。当数据基数和体积较高时，这些方法通常会优于典型的建模方法。
- en: 'For the model factory, multiple projects are created for the different levels
    of the feature of interest. To evaluate this, the best-performing model for each
    project is selected from the dictionary for all projects. This set of best models
    from all the projects is stored in another dictionary object. A `for` loop is
    then run across all the models of the dictionary to extract the performance of
    the model, as shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型工厂，为感兴趣的特征的不同级别创建了多个项目。为了评估这一点，从所有项目的字典中选择了每个项目的最佳性能模型。所有项目中的最佳模型集存储在另一个字典对象中。然后，通过一个`for`循环遍历字典中的所有模型以提取模型的性能，如图所示：
- en: '![Figure 12.9 – Evaluating the performance of models with a model factory'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9 – 使用模型工厂评估模型性能'
- en: '](img/B17159_12_09.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_09.jpg)'
- en: Figure 12.9 – Evaluating the performance of models with a model factory
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – 使用模型工厂评估模型性能
- en: 'Improved model performance is only one of the reasons you should use the one
    versus all multiclass classification models, as well as model factors. Sometimes,
    understanding the drivers is equally as important. Visualizing the feature importance
    for the different fuel types could present an interesting contrast in drivers.
    This means that different factors affect the prices of different fuel types. This
    could have a bearing on strategic decisions. As shown in the following screenshot,
    the Python API can be used to plot the feature impacts by leveraging chart functions
    from Seaborn and Matplotlib:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 改进模型性能只是你应该使用“一对多”多类分类模型以及模型因素的原因之一。有时，理解驱动因素同样重要。可视化不同燃料类型的特征重要性可以呈现有趣的驱动因素对比。这意味着不同的因素会影响不同燃料类型的定价。这可能会影响战略决策。如图所示，Python
    API可以通过利用Seaborn和Matplotlib的图表函数来绘制特征影响：
- en: '![Figure 12.10 – Feature impacts for the differing diesel and gas automobiles'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.10 – 不同柴油和汽油汽车的特征影响'
- en: '](img/B17159_12_10.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_10.jpg)'
- en: Figure 12.10 – Feature impacts for the differing diesel and gas automobiles
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 – 不同柴油和汽油汽车的特征影响
- en: As we can see, there are some differences in the feature impacts for the automobile
    fuel types. While `curb-weight` seems to be an important driver, its effect is
    relatively more important for diesel vehicles. Similarly, for gas cars, the power
    that's generated by these automobiles, as typified by the `engine_size` and `horsepower`
    features, carries more importance in determining price than those of diesel cars.
    You can already see the effect such preliminary findings could have on decisions
    and how this could be applied to other commercial cases. Using feature importance
    to examine multiple models can also be applied in the case of one versus all classification
    problems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，汽车燃料类型的特征影响存在一些差异。虽然“车辆重量”似乎是一个重要的驱动因素，但其对柴油车辆的影响相对更为重要。同样，对于汽油车来说，这些汽车产生的动力，由“发动机尺寸”和“马力”特征所代表，在确定价格方面比柴油车更为重要。你已能看出这些初步发现对决策的影响以及如何将其应用于其他商业案例。使用特征重要性来检查多个模型也可以应用于“一对多”分类问题。
- en: In this section, we created basic DataRobot projects using the Python API. After,
    we solved more complex problems by using multiple projects within a system. There,
    we created one versus all projects to solve multiclass classification problems
    and model factories to solve multi-level problems involving subgroups. We also
    explored feature impact and model evaluation. Having programmatically created
    models, we will now learn how to make predictions using these models. Specifically,
    we will learn how to deploy models, make predictions, extract explanations from
    models, and score large datasets through parallelization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用Python API创建了基本的DataRobot项目。之后，我们通过在一个系统中使用多个项目来解决更复杂的问题。在那里，我们创建了“一对多”项目来解决多类分类问题，并使用模型工厂来解决涉及子组的多层次问题。我们还探讨了特征影响和模型评估。在程序化创建模型后，我们将学习如何使用这些模型进行预测。具体来说，我们将学习如何部署模型、进行预测、从模型中提取解释以及通过并行化对大型数据集进行评分。
- en: Making predictions programmatically
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序化进行预测
- en: The possibilities that programmatically using DataRobot presents are enormous.
    By using its API, models can be deployed and predictions can be made against them.
    Before making programmatical predictions within the production environment, models
    need to be deployed. DataRobot models are deployed using Portable Prediction Servers.
    These are Docker containers that can host machine learning models, which serve
    predictions and prediction explanations through a REST API.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 程序化使用提供的可能性是巨大的。通过使用其 API，可以部署模型并对它们进行预测。在生产环境中进行程序化预测之前，需要部署模型。DataRobot
    模型通过便携式预测服务器进行部署。这些是 Docker 容器，可以托管机器学习模型，并通过 REST API 提供预测和预测解释。
- en: 'To deploy models, we can use the DataRobot package''s `deployment` method.
    Here, we must provide a description, the DataRobot model''s ID, as well as its
    label to create the deployments. A typical Python deployment script follows this
    format:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署模型，我们可以使用 DataRobot 包的 `deployment` 方法。在这里，我们必须提供一个描述、DataRobot 模型的 ID 以及其标签来创建部署。一个典型的
    Python 部署脚本遵循以下格式：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As per this approach, the following screenshot shows how `autoproject_1`, which
    we created in the *Building models programmatically* section, can be deployed.
    Here, the model ID is `best_model_1`. We will label `AutoBase Deployment` with
    a description of `Base Automobile Price Deployment`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种方法，以下截图显示了在 *Building models programmatically* 部分中创建的 `autoproject_1` 可以如何部署。在这里，模型
    ID 是 `best_model_1`。我们将使用 `AutoBase Deployment` 并将其描述为 `Base Automobile Price
    Deployment`：
- en: '![Figure 12.11 – Deploying a model programmatically'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.11 – 程序化部署模型'
- en: '](img/B17159_12_11.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_11.jpg)'
- en: Figure 12.11 – Deploying a model programmatically
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11 – 程序化部署模型
- en: 'The deployment process can be iterated to enable those of more complex projects.
    For instance, with model factories, irrespective of the number of levels the differentiating
    variable has, with a single `for` loop, all the best models can be deployed to
    DataRobot. For each of the best models, a deployment is created, which is then
    used to score new data. The script for deploying the model factory for the automobile
    project, along with the fuel type as its differentiating variable, is shown in
    the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程可以迭代，以使更复杂的项目成为可能。例如，使用模型工厂，无论区分变量有多少级别，只需一个 `for` 循环，就可以将所有最佳模型部署到 DataRobot。对于每个最佳模型，都会创建一个部署，然后用于评分新数据。以下截图显示了用于汽车项目的模型工厂部署脚本，其中燃料类型作为区分变量：
- en: '![Figure 12.12 – Deploying models from a model factory'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.12 – 从模型工厂部署模型'
- en: '](img/B17159_12_12.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_12.jpg)'
- en: Figure 12.12 – Deploying models from a model factory
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12 – 从模型工厂部署模型
- en: 'Having deployed the models, predictions can be made against them. To make simple
    predictions within the development environment, we can use the `DataRobot BatchPredictionJob.score_to_file`
    method. To make predictions, this method requires the model ID, prediction data,
    and the location where the scored data will be stored. Here, we will use `best_model_1`
    to score the same model we used to develop the model, the `df` data object, and
    the location path, which specifies the prediction file path as `./pred.csv`. The
    `passthrough_columns_set` parameter specifies the columns from the original dataset
    that will be included in the predictions. Since this is set to `''all''`, all
    the columns are returned, as shown here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了模型后，可以对它们进行预测。要在开发环境中进行简单预测，我们可以使用 `DataRobot BatchPredictionJob.score_to_file`
    方法。要进行预测，此方法需要模型 ID、预测数据和评分数据将存储的位置。在这里，我们将使用 `best_model_1` 来评分我们用于开发模型的相同模型，`df`
    数据对象，以及指定预测文件路径的位置路径，即 `./pred.csv`。`passthrough_columns_set` 参数指定将包含在预测中的原始数据集的列。由于此设置为
    `'all'`，因此返回所有列，如下所示：
- en: '![Figure 12.13 – Simple programmatic prediction'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.13 – 简单的程序预测'
- en: '](img/B17159_12_13.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17159_12_13.jpg)'
- en: Figure 12.13 – Simple programmatic prediction
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13 – 简单的程序预测
- en: These predictions comprise all the columns from the initial dataset, in addition
    to the predicted prices. There are cases where it is ideal to include rationales
    behind predictions. In such cases, the `max_explanations` parameter should be
    included in the job's configuration. This parameter sets the highest number of
    explanations to be provided for every data row.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测包括初始数据集的所有列，以及预测价格。在某些情况下，包括预测背后的理由是理想的。在这种情况下，应在作业配置中包含 `max_explanations`
    参数。此参数设置每行数据应提供的最高解释数量。
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: DataRobot provides us with a unique capability to rapidly develop models. With
    this platform, data scientists can combine the benefits of DataRobot and the flexibilities
    of open programming. In this chapter, we explored ways to access the credentials
    needed to programmatically use DataRobot. Using the Python client, we demonstrated
    ways in which data can be ingested and how basic projects can be created. We started
    building models for more complex problems. We created model factories as well
    as one versus all models. Finally, we demonstrated how models can be deployed
    and used to score data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DataRobot 为我们提供了一种独特的快速开发模型的能力。借助这个平台，数据科学家可以结合 DataRobot 的优势以及开放编程的灵活性。在本章中，我们探讨了获取用于程序化使用
    DataRobot 所需凭证的方法。使用 Python 客户端，我们演示了数据摄取的方式以及如何创建基本项目。我们开始构建用于更复杂问题的模型。我们创建了模型工厂以及一对多模型。最后，我们展示了如何部署模型并用于评分数据。
- en: One of the key advantages of programmatically using DataRobot is the ability
    to ingest data from numerous sources, score them, and store them in the relevant
    sources. This makes it possible to carry out end-to-end dataset scoring. It becomes
    possible for a system to be set up to score models periodically. With this comes
    numerous data quality and model monitoring concerns. The next chapter will focus
    on how to control the quality of the models and data on the DataRobot platform,
    as well as using the Python API.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataRobot 进行程序化操作的一个关键优势是能够从众多来源摄取数据，对其进行评分，并将它们存储在相关来源中。这使得进行端到端数据集评分成为可能。系统可以设置为定期进行模型评分。随之而来的是许多数据质量和模型监控问题。下一章将重点介绍如何在
    DataRobot 平台上控制模型和数据的质量，以及如何使用 Python API。
