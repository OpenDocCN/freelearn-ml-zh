- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Better Learners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a sports team falls short of meeting its goal—whether it is to obtain an
    Olympic gold medal, a league championship, or a world record time—it must search
    for possible improvements. Imagine that you’re the team’s coach. How would you
    spend your practice sessions? Perhaps you’d direct the athletes to train harder
    or train differently in order to maximize every bit of their potential. You might
    also focus on teamwork to use each athlete’s strengths and weaknesses more smartly.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine that you’re training a championship machine learning algorithm.
    Perhaps you hope to compete in machine learning competitions or maybe you simply
    need to outperform business competitors. Where do you begin? Despite the different
    context, the strategies for improving a sports team’s performance are like those
    used for improving the performance of statistical learners. As the coach, it is
    your job to find the combination of training techniques and teamwork skills that
    allow the machine learning project to meet your performance goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter builds on the material covered throughout this book to introduce
    techniques that improve the predictive ability of learning algorithms. You will
    learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for automating model performance tuning by systematically searching
    for the optimal set of training conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods for combining models into groups that use teamwork to tackle tough learning
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use and differentiate among popular variants of decision trees that have
    become popular due to their impressive performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of these methods will be successful for every problem. Yet looking at the
    winning entries to machine learning competitions, you’ll likely find at least
    one of them has been employed. To be competitive, you too will need to add these
    skills to your repertoire.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning stock models for better performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some machine learning tasks are well suited to be solved by the stock models
    presented in prior chapters. For these tasks, it may not be necessary to spend
    much time iterating and refining the model, because it may perform well enough
    without additional effort. On the other hand, many real-world tasks are inherently
    more difficult. For these tasks, the underlying concepts to be learned tend to
    be extremely complex, requiring an understanding of many subtle relationships,
    or the problem may be affected by substantial amounts of random variability, which
    makes it difficult to find the signal within the noise.
  prefs: []
  type: TYPE_NORMAL
- en: Developing models that perform extremely well on these types of challenging
    problems is every bit an art as it is a science. Sometimes a bit of intuition
    is helpful when trying to identify areas where performance can be improved. In
    other cases, finding improvements will require a brute-force, trial-and-error
    approach. Of course, this is one of the strengths of using machines that never
    tire and never become bored; searching for numerous potential improvements can
    be made easier by automated programs. As we will see, however, human effort and
    computing time are not always fungible, and creating a finely-tuned learning algorithm
    can come with its own costs.
  prefs: []
  type: TYPE_NORMAL
- en: We attempted a difficult machine learning problem *in* *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*, as we attempted
    to predict bank loans that were likely to enter default. Although we were able
    to achieve a respectable classification accuracy of 82 percent, upon more careful
    examination in *Chapter 10*, *Evaluating Model Performance*, we realized that
    the accuracy statistic was a bit misleading. The kappa statistic—a better measure
    of performance for unbalanced outcomes­—was only about 0.294 as measured via 10-fold
    **cross-validation** (**CV**), which suggested that the model was performing somewhat
    poorly, despite the high accuracy. In this section, we’ll revisit the credit scoring
    model to see whether we can improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along with the examples, download the `credit.csv` file from the
    Packt Publishing website and save it to your R working directory. Load the file
    into R using the following command: `credit <- read.csv("credit.csv")`.'
  prefs: []
  type: TYPE_NORMAL
- en: You may recall that we first used a stock C5.0 decision tree to build the classifier
    for the credit data and later attempted to improve the classifier’s performance
    by adjusting the `trials` option to increase the number of boosting iterations.
  prefs: []
  type: TYPE_NORMAL
- en: By changing the number of iterations from the default value of 1 up to the value
    of 10, we were able to increase the model’s accuracy. As defined in *Chapter 11*,
    *Being Successful with Machine Learning*, these model options, known as hyperparameters,
    are not learned automatically from the data but are instead set before training.
    The process of testing various hyperparameter settings to achieve a better model
    fit is thus called **hyperparameter tuning**, and strategies for tuning range
    from simple ad hoc trial and error to more rigorous and systematic iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning is not limited to decision trees. For instance, we tuned
    k-NN models when we searched for the best value of *k*. We also tuned neural networks
    and support vector machines as we adjusted the number of nodes and the number
    of hidden layers, or chose different kernel functions. Most machine learning algorithms
    allow the adjustment of at least one hyperparameter, and the most sophisticated
    models offer many ways to tweak the model fit. Although this allows the model
    to be tailored closely to the learning task, the complexity of the many options
    can be daunting. A more systematic approach is warranted.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the scope of hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When performing hyperparameter tuning, it is important to put bounds on the
    scope to prevent the search from proceeding endlessly. The computer provides the
    muscle, but it is up to the human to dictate where to look and for how long. Even
    as computing power is growing and cloud computing costs are shrinking, the search
    can easily get out of hand when sifting through nearly endless combinations of
    values. A narrow or shallow tuning scope may last long enough to grab a cup of
    coffee, while a wide or deep scope may give you time to get a good night of sleep—or
    more!
  prefs: []
  type: TYPE_NORMAL
- en: Time and money are often fungible, as you may be able to buy time in the form
    of additional computing resources or by enlisting additional team members to build
    models faster or in parallel. Even so, taking this for granted can lead to ruin
    in the form of budget overruns or missed deadlines because it is easy for the
    scope to balloon quickly when work proceeds down countless tangents and dead ends
    without a plan. To avoid such pitfalls, it is wise to strategize about the breadth
    and depth of the tuning process beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: You might start by thinking about tuning as a process much like playing the
    classic board game *Battleship*. In this game, your opponent has placed a fleet
    of battleships on a two-dimensional grid, which is hidden out of your view. Your
    goal is to destroy the opponent’s fleet by guessing the coordinates of all their
    ships before they do the same to yours. Because the ships are known sizes and
    shapes, a smart player will begin by broadly probing the search grid in a checkerboard
    pattern but quickly focus on a specific target once it has been hit.
  prefs: []
  type: TYPE_NORMAL
- en: This is a better strategy than guessing coordinates at random or iterating across
    each coordinate systematically, both of which are inefficient in comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: The hunt for the optimal machine learning hyperparameters can
    be much like playing the classic Battleship board game'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, there are methods for tuning that are more efficient than systematic
    iteration over endless values and combinations of values. With experience, you
    will develop an intuition for how to proceed, but for the first few attempts,
    it may be useful to think intentionally about the process. The following general
    strategy, listed as a series of steps, can be adapted to your machine learning
    project, computing and staffing resources, and work style:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Replicate the real-world evaluation criteria**: To find the single best set
    of model hyperparameters, it is important that the models are evaluated using
    the same criteria as will be used in deployment. This may mean choosing an evaluation
    metric that mirrors the final, real-world metric, or it may involve writing a
    function that simulates the deployment environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consider the resource usage for one iteration**: As tuning will be iterating
    many times on the same algorithm, you should have an estimate of the time and
    computing resources needed for a single iteration. If it takes one hour to train
    a single model, it will take 100 hours or more for 100 iterations. If the computer
    memory is already at its limit, it is likely that you will exceed the limit during
    tuning. If this is a problem, you will need to invest in additional computing
    power, run the experiment in parallel, or reduce the size of the dataset via random
    sampling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Begin with a shallow search to probe for patterns**: The initial tuning process
    should be interactive and shallow. It is intended to develop your own understanding
    of what options and values are important. When probing a single hyperparameter,
    keep increasing or decreasing its setting in reasonable increments until the performance
    stops improving (or starts decreasing). Depending on the option, this may be increments
    of one, multiples of five or ten, or incrementally small fractions, such as 0.1,
    0.01, 0.001, and so on. When tuning two or more hyperparameters, it may help to
    focus on one at a time and keep the other values static. This is a more efficient
    approach than testing all possible combinations of settings, but may ultimately
    miss important combinations that would have been discovered if all combinations
    were tested.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Narrow in on the optimal set of hyperparameter values**: Once you have a
    sense of a range of values suspected to contain the optimal settings, you can
    reduce the increments between the tested values and test a narrower range with
    greater precision or test a greater number of combinations of values. The previous
    step should have already resulted in a reasonable set of hyperparameters, so this
    step should only improve and never detract from the model’s performance; it can
    be stopped at any time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Determine a reasonable stopping point**: Deciding when to stop the tuning
    process is easier said than done—the thrill of the hunt and the possibility of
    a slightly better model can lead to a stubborn desire to keep going! Sometimes,
    the stopping point is a project deadline when time is running out. In other cases,
    the work can only stop once the desired performance level has been reached. In
    any case, because the only way to guarantee finding the optimal hyperparameter
    values is to test an infinite number of possibilities, rather than working toward
    burnout, you will need to define the point at which performance is “good enough”
    to stop the process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 14.2* illustrates the process of homing in on hyperparameter values
    for single-parameter tuning. Five potential values (1, 2, 3, 4, and 5) denoted
    by solid circles were evaluated in the initial pass, and the accuracy was highest
    when the hyperparameter was set to 3\. To check whether an even better hyperparameter
    setting might exist, eight additional values (from 2.2 to 3.8 in increments of
    0.2, denoted by vertical tick marks) were tested within the range between 2 and
    4, which led to the discovery of a higher accuracy when the hyperparameter was
    set to 3.2\. If time allows, one could test even more values in a narrower range
    around this value to possibly find an even better setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Strategies for parameter tuning home in on the optimal value by
    searching broadly and then narrowly'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuning two or more hyperparameters is more complicated, because the optimal
    value of one parameter may depend on the value of the others. Constructing a visualization
    like the one depicted in *Figure 14.3* may help in understanding how to find the
    best combinations of parameters; within hot spots where certain combinations of
    values result in better model performance, one might test more values in narrower
    and narrower ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, diagram  Description automatically generated](img/B17290_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Tuning strategies become more challenging as more hyperparameters
    are added, as the model’s best performance depends on combinations of values'
  prefs: []
  type: TYPE_NORMAL
- en: This *Battleship*-style **grid search**, in which hyperparameters and combinations
    of hyperparameters are tested systematically, is not the only approach to tuning,
    although it may be the most widely used. A more intelligent approach called **Bayesian
    optimization** treats the tuning process as a learning problem that can be solved
    using modeling. This approach is included in some automated machine learning software
    but is outside the scope of this book. Instead, for the remainder of this section,
    we will focus on applying the idea of grid search to our real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Example – using caret for automated tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thankfully, we can use R to conduct the iterative search through many possible
    hyperparameter values and combinations of values to find the best set. This approach
    is a relatively easy yet sometimes computationally expensive brute-force method
    of optimizing a learning algorithm’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `caret` package, which was used previously in *Chapter 10*, *Evaluating
    Model Performance*, provides tools to assist with this form of automated tuning.
    The core tuning functionality is provided by a `train()` function that serves
    as a standardized interface for over 200 different machine learning models for
    both classification and numeric prediction tasks. Using this function, it is possible
    to automate the search for optimal models using a choice of evaluation methods
    and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automated parameter tuning with `caret` will require you to consider three
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of machine learning algorithm (and specific R implementation of this
    algorithm) should be trained on the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which hyperparameters can be adjusted for this algorithm, and how extensively
    should they be tuned to find the optimal settings?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What criterion should be used to evaluate the candidate models to identify the
    best overall set of tuning values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering the first question involves finding a match between the machine learning
    task and one of the many models available to the `caret` package. This requires
    a general understanding of the types of machine learning models, which you may
    already have if you’ve been working through this book chronologically. It can
    also help to work through a process of elimination. Nearly half of the models
    can be eliminated depending on whether the task is classification or numeric prediction;
    others can be excluded based on the format of the training data or the need to
    avoid black box models, and so on. In any case, there’s also no reason you can’t
    create several highly tuned models and compare them across the set.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the second question is a matter largely dictated by the choice of
    model since each algorithm utilizes its own set of hyperparameters. The available
    tuning options for the predictive models covered in this book are listed in the
    following table. Keep in mind that although some models have additional options
    not shown, only those listed in the table are supported by `caret` for automatic
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Learning Task** | **Method Name** | **Hyperparameters** |'
  prefs: []
  type: TYPE_TB
- en: '| *k-Nearest Neighbors* | *Classification* | `knn` | `k` |'
  prefs: []
  type: TYPE_TB
- en: '| *Naive Bayes* | *Classification* | `nb` | `fL, usekernel` |'
  prefs: []
  type: TYPE_TB
- en: '| *Decision Trees* | *Classification* | `C5.0` | `model, trials, winnow` |'
  prefs: []
  type: TYPE_TB
- en: '| *OneR Rule Learner* | *Classification* | `OneR` | `None` |'
  prefs: []
  type: TYPE_TB
- en: '| *RIPPER Rule Learner* | *Classification* | `JRip` | `NumOpt` |'
  prefs: []
  type: TYPE_TB
- en: '| *Linear Regression* | *Regression* | `lm` | `None` |'
  prefs: []
  type: TYPE_TB
- en: '| *Regression Trees* | *Regression* | `rpart` | `cp` |'
  prefs: []
  type: TYPE_TB
- en: '| *Model Trees* | *Regression* | `M5` | `pruned, smoothed, rules` |'
  prefs: []
  type: TYPE_TB
- en: '| *Neural Networks* | *Dual Use* | `nnet` | `size, decay` |'
  prefs: []
  type: TYPE_TB
- en: '| *Support Vector Machines (Linear Kernel)* | *Dual Use* | `svmLinear` | `C`
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Support Vector Machines (Radial Basis Kernel)* | *Dual Use* | `svmRadial`
    | `C, sigma` |'
  prefs: []
  type: TYPE_TB
- en: '| *Random Forests* | *Dual Use* | `rf` | `mtry` |'
  prefs: []
  type: TYPE_TB
- en: '| *Gradient Boosting Machines (GBM)* | *Dual Use* | `gbm` | `n.trees, interaction.depth,
    shrinkage, n.minobsinnode` |'
  prefs: []
  type: TYPE_TB
- en: '| *XGBoost (XGB)* | *Dual Use* | `xgboost` | `eta, max_depth, colsample_bytree,
    subsample, nrounds, gamma, min_child_weight` |'
  prefs: []
  type: TYPE_TB
- en: For a complete list of the models and corresponding tuning options covered by
    `caret`, refer to the table provided by package author Max Kuhn at [http://topepo.github.io/caret/available-models.html](http://topepo.github.io/caret/available-models.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ever forget the tuning parameters for a particular model, the `modelLookup()`
    function can be used to find them. Simply supply the method name as illustrated
    for the C5.0 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The goal of automatic tuning is to iterate over the set of candidate models
    comprising the search grid of potential parameter combinations. As it is impractical
    to search every conceivable combination, only a subset of possibilities is used
    to construct the grid. By default, `caret` searches, at most, three values for
    each of the model’s *p* hyperparameters, which means that, at most, *3*^p candidate
    models will be tested. For example, by default, the automatic tuning of k-nearest
    neighbors will compare *3*¹ *= 3* candidate models with `k=5`, `k=7`, and `k=9`.
    Similarly, tuning a decision tree will result in a comparison of up to 27 different
    candidate models, comprising the grid of *3*³ *= 27* combinations of `model`,
    `trials`, and `winnow` settings. In practice, however, only 12 models are tested.
    This is because `model` and `winnow` can only take two values (`tree` versus `rules`
    and `TRUE` versus `FALSE`, respectively), which makes the grid size *3*2*2 = 12*.
  prefs: []
  type: TYPE_NORMAL
- en: Since the default search grid may not be ideal for your learning problem, `caret`
    allows you to provide a custom search grid defined by a simple command, which
    we will cover later.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final step in automatic model tuning involves identifying the
    best model among the candidates. This uses the methods discussed in *Chapter 10*,
    *Evaluating Model Performance*, including the choice of resampling strategy for
    creating training and test datasets, and the use of model performance statistics
    to measure the predictive accuracy. All the resampling strategies and many of
    the performance statistics we’ve learned are supported by `caret`. These include
    statistics such as accuracy and kappa for classifiers and R-squared or **root-mean-square
    error** (**RMSE**) for numeric models. Cost-sensitive measures like sensitivity,
    specificity, and AUC can also be used if desired.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `caret` will select the candidate model with the best value of the
    desired performance measure. Because this practice sometimes results in the selection
    of models that achieve minor performance improvements via large increases in model
    complexity, alternative model selection functions are provided. These alternatives
    allow us to choose simpler models that are still reasonably close to the best
    model, which may be desirable in the case where a bit of predictive performance
    is worth sacrificing for an improvement in computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Given the wide variety of options in the `caret` tuning process, it is helpful
    that many of the function’s defaults are reasonable. For instance, without specifying
    the settings manually, `caret` uses prediction accuracy or RMSE on a bootstrap
    sample to choose the best performer for classification and numeric prediction
    models, respectively. Similarly, it will automatically define a limited grid to
    search. These defaults allow us to start with a simple tuning process and learn
    to tweak the `train()` function to design a wide variety of experiments of our
    choosing.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple tuned model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate the process of tuning a model, let’s begin by observing what
    happens when we attempt to tune the credit scoring model using the `caret` package’s
    default settings. The simplest way to tune a learner requires only that you specify
    a model type via the `method` parameter. Since we used C5.0 decision trees previously
    with the credit model, we’ll continue our work by optimizing this learner. The
    basic `train()` command for tuning a C5.0 decision tree using the default settings
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First, the `set.seed()` function is used to initialize R’s random number generator
    to a set starting position. You may recall that we used this function in several
    prior chapters. By setting the `seed` parameter (in this case, to the arbitrary
    number 300), the random numbers will follow a predefined sequence. This allows
    simulations that use random sampling to be repeated with identical results—a very
    helpful feature if you are sharing code or attempting to replicate a prior result.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a tree as `default ~ .` using the R formula interface. This
    models a loan default status (`yes` or `no`) using all the other features in the
    `credit` dataset. The parameter `method = "C5.0"` tells the function to use the
    C5.0 decision tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve entered the preceding command, depending upon your computer’s capabilities,
    there may be a significant delay as the tuning process occurs. Even though this
    is a small dataset, a substantial amount of calculation must occur. R must repeatedly
    generate random bootstrap samples of data, build decision trees, compute performance
    statistics, and evaluate the result. Because there are 12 candidate models with
    varying hyperparameter values to be evaluated, and 25 bootstrap samples per candidate
    model to compute an average performance measure, there are *25*12 = 300* decision
    tree models being built using C5.0—and this doesn’t even count the additional
    decision trees being built when the boosting trials are set!
  prefs: []
  type: TYPE_NORMAL
- en: 'A list named `m` stores the result of the `train()` experiment, and the command
    `str(m)` will display the associated results, but the contents can be substantial.
    Instead, simply type the name of the object for a condensed summary of the results.
    For instance, typing `m` yields the following output (note that numbered labels
    have been added for clarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: The results of a caret experiment are separated into four components,
    as annotated in this figure'
  prefs: []
  type: TYPE_NORMAL
- en: 'The labels highlight four main components in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A brief description of the input dataset**: If you are familiar with your
    data and have applied the `train()` function correctly, this information should
    not be surprising.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A report of the preprocessing and resampling methods applied**: Here we see
    that 25 bootstrap samples, each including 1,000 examples, were used to train the
    models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A list of the candidate models evaluated**: In this section, we can confirm
    that 12 different models were tested, based on the combinations of three C5.0
    hyperparameters: `model`, `trials`, and `winnow`. The average accuracy and kappa
    statistics for each candidate model are also shown.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The choice of best model**: As the footnote describes, the model with the
    best accuracy (in other words, “largest”) was selected. This was the C5.0 model
    that used a decision tree with the settings `winnow = FALSE` and `trials = 20`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After identifying the best model, the `train()` function uses the tuned hyperparameters
    to build a model on the full input dataset, which is stored in `m` as `m$finalModel`.
    In most cases, you will not need to work directly with the `finalModel` sub-object.
    Instead, simply use the `predict()` function with the `m` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting vector of predictions works as expected, allowing us to create
    a confusion matrix that compares the predicted and actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Of the 1,000 examples used for training the final model, only two were misclassified,
    for an accuracy of 99.8 percent. However, it is very important to note that since
    the model was built on both the training and test data, this accuracy is optimistic
    and thus should not be viewed as indicative of performance on unseen data. The
    bootstrap accuracy estimate of 72.996 percent, which can be found in the last
    row of section three of the `train()`output in *Figure 14.4*, is a far more realistic
    estimate of future accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to automatic hyperparameter tuning, using the `caret` package’s
    `train()` and `predict()` functions also offers a pair of benefits beyond the
    functions found in the stock packages.
  prefs: []
  type: TYPE_NORMAL
- en: First, any data preparation steps applied by the `train()` function will be
    similarly applied to the data used for generating predictions. This includes transformations
    like centering and scaling, as well as the imputation of missing values. Allowing
    `caret` to handle the data preparation will ensure that the steps that contributed
    to the best model’s performance will remain in place when the model is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the `predict()` function provides a standardized interface for obtaining
    predicted class values and predicted class probabilities, even for model types
    that ordinarily would require additional steps to obtain this information. For
    a classification model, the predicted classes are provided by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the estimated probabilities for each class, use the `type = "prob"`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Even in cases where the underlying model refers to the prediction probabilities
    using a different string (for example, `"raw"` for a `naiveBayes` model), the
    `predict()` function will translate `type = "prob"` to the appropriate parameter
    setting automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the tuning process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The decision tree we created previously demonstrates the `caret` package’s
    ability to produce an optimized model with minimal intervention. The default settings
    allow optimized models to be created easily. However, it is also possible to change
    the default settings as desired, which may assist with unlocking the upper echelon
    of performance. Before the tuning process begins, it’s worth answering a series
    of questions that will help guide the setup of the `caret` experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: How long does it take for one iteration? In other words, how long does it take
    to train a single instance of the model being tuned?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the time it takes to train a single instance, how long will it take to
    perform the model evaluation using the chosen resampling method? For example,
    10-fold CV will require 10 times as much time as training a single model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time are you willing to spend on tuning? Based on this number, one
    can determine the total number of hyperparameter values that can be tested. For
    instance, if it takes one minute to evaluate a model using 10-fold CV, then 60
    hyperparameter settings can be tested per hour.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using time as the key limiting factor will help put bounds on the tuning process
    and prevent you from chasing better and better performance endlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve decided how much time to spend on the trials, it is easy to customize
    the process to your liking. To illustrate this flexibility, let’s modify our work
    on the credit decision tree to mirror the process we used in *Chapter 10*, *Evaluating
    Model Performance*. In that chapter, we estimated the kappa statistic using 10-fold
    CV. We’ll do the same here, using kappa to tune the boosting trials for the C5.0
    decision tree algorithm and find the optimal setting for our data. Note that decision
    tree boosting was first covered in *Chapter 5*, *Divide and Conquer – Classification
    Using Decision Trees and Rules*, and will also be covered in greater detail later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `trainControl()` function is used to create a set of configuration options
    known as a **control object**. This object guides the `train()` function and allows
    for the selection of model evaluation criteria such as the resampling strategy
    and the measure used for choosing the best model. Although this function can be
    used to modify nearly every aspect of a `caret` tuning experiment, we’ll focus
    on two important parameters: `method` and `selectionFunction`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re eager for more details about the control object, you can use the `?trainControl`
    command for a list of all the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `trainControl()` function, the `method` parameter sets the resampling
    method, such as holdout sampling or k-fold CV. The following table lists the possible
    `method` values, as well as any additional parameters for adjusting the sample
    size and the number of iterations. Although the default options for these resampling
    methods follow popular conventions, you may choose to adjust these depending on
    the size of your dataset and the complexity of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Resampling method** | **Method name** | **Additional options and default
    values** |'
  prefs: []
  type: TYPE_TB
- en: '| *Holdout sampling* | `LGOCV` | `p = 0.75` (training data proportion) |'
  prefs: []
  type: TYPE_TB
- en: '| *k-fold CV* | `cv` | `number = 10` (number of folds) |'
  prefs: []
  type: TYPE_TB
- en: '| *Repeated k-fold CV* | `repeatedcv` | `number = 10` (number of folds)`repeats
    = 10` (number of iterations) |'
  prefs: []
  type: TYPE_TB
- en: '| *Bootstrap sampling* | `boot` | `number = 25` (resampling iterations) |'
  prefs: []
  type: TYPE_TB
- en: '| *0.632 bootstrap* | `boot632` | `number = 25` (resampling iterations) |'
  prefs: []
  type: TYPE_TB
- en: '| *Leave-one-out CV* | `LOOCV` | *None* |'
  prefs: []
  type: TYPE_TB
- en: The `selectionFunction` parameter is used to specify the function that will
    choose the optimal model among the candidates. Three such functions are included.
    The `best` function simply chooses the candidate with the best value on the specified
    performance measure. This is used by default. The other two functions are used
    to choose the most parsimonious, or simplest, model that is within a certain threshold
    of the best model’s performance. The `oneSE` function chooses the simplest candidate
    within one standard error of the best performance, and `tolerance` uses the simplest
    candidate within a user-specified percentage.
  prefs: []
  type: TYPE_NORMAL
- en: Some subjectivity is involved with the `caret` package’s ranking of models by
    simplicity. For information on how models are ranked, see the help page for the
    selection functions by typing `?best` at the R command prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a control object named `ctrl` that uses 10-fold CV and the `oneSE`
    selection function, use the following command, noting that `number = 10` is included
    only for clarity; since this is the default value for `method = "cv"`, it could
    have been omitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use the result of this function shortly.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, the next step in setting up our experiment is to create the
    search grid for hyperparameter tuning. The grid must include a column named for
    each hyperparameter in the desired model, regardless of whether it will be tuned.
    It must also include a row for each desired combination of values to test. Since
    we are using a C5.0 decision tree, this means we’ll need columns named `model`,
    `trials`, and `winnow`, corresponding to the three options that can be tuned.
    For other machine learning models, refer to the table presented earlier in this
    chapter or use the `modelLookup()` function to find the hyperparameters as described
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than filling the grid data frame cell by cell—a tedious task if there
    are many possible combinations of values—we can use the `expand.grid()` function,
    which creates data frames from the combinations of all values supplied. For example,
    suppose we would like to hold constant `model = "tree"` and `winnow = FALSE` while
    searching eight different values of `trials`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be created as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `grid` data frame contains *1*8*1 = 8* rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `train()` function will build a candidate model for evaluation using each
    `grid` row’s combination of model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the search grid and the control object created previously, we are ready
    to run a thoroughly customized `train()` experiment. As before, we’ll set the
    random seed to the arbitrary number `300` in order to ensure repeatable results.
    But this time, we’ll pass our control object and tuning grid while adding a parameter
    `metric = "Kappa"`, indicating the statistic to be used by the model evaluation
    function—in this case, `"oneSE"`. The full set of commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in an object that we can view by typing its name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Although the output is similar to the automatically tuned model, there are a
    few notable differences. Because 10-fold CV was used, the sample size to build
    each candidate model was reduced to 900 rather than the 1,000 used in the bootstrap.
    Furthermore, eight candidate models were tested rather than the 12 in the prior
    experiment. Lastly, because `model` and `winnow` were held constant, their values
    are no longer shown in the results; instead, they are listed as a footnote.
  prefs: []
  type: TYPE_NORMAL
- en: The best model here differs quite significantly from the prior experiment. Before,
    the best model used `trials = 20`, whereas here, it used `trials = 1`. This change
    is because we used the `oneSE` function rather than the `best` function to select
    the optimal model. Even though the model with `trials = 35` obtained the best
    kappa, the single-trial model offers reasonably close performance with a much
    simpler algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the large number of configuration parameters, `caret` can seem overwhelming
    at first. Don’t let this deter you—there is no easier way to test the performance
    of models using 10-fold CV. Instead, think of the experiment as defined by two
    parts: a `trainControl()` object that dictates the testing criteria, and a tuning
    grid that determines what model parameters to evaluate. Supply these to the `train()`
    function and with a bit of computing time, your experiment will be complete!'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, tuning is just one possibility for building better learners. In the
    next section, you will discover that in addition to buffing up a single learner
    to make it stronger, it is also possible to combine several weaker models to form
    a more powerful team.
  prefs: []
  type: TYPE_NORMAL
- en: Improving model performance with ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as the best sports teams have players with complementary rather than overlapping
    skillsets, some of the best machine learning algorithms utilize teams of complementary
    models. Since a model brings a unique bias to a learning task, it may readily
    learn one subset of examples but have trouble with another. Therefore, by intelligently
    using the talents of several diverse team members, it is possible to create a
    strong team of multiple weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: This technique of combining and managing the predictions of multiple models
    falls into a wider set of **meta-learning methods**, which are techniques that
    involve learning how to learn. This includes anything from simple algorithms that
    gradually improve performance by iterating over design decisions—for instance,
    the automated parameter tuning used earlier in this chapter—to highly complex
    algorithms that use concepts borrowed from evolutionary biology and genetics for
    self-modifying and adapting to learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you were a contestant on a television trivia show that allowed you to
    choose a panel of five friends to assist you with answering the final question
    for the million-dollar prize. Most people would try to stack the panel with a
    diverse set of subject matter experts. A panel containing professors of literature,
    science, history, and art, along with a current pop-culture expert, would be safely
    well-rounded. Given their breadth of knowledge, it would be unlikely that a question
    would stump the group.
  prefs: []
  type: TYPE_NORMAL
- en: The meta-learning approach that utilizes a similar principle of creating a varied
    team of experts is known as an **ensemble**. For the remainder of this chapter,
    we’ll focus on meta-learning only as it pertains to ensembling—the task of modeling
    a relationship between the predictions of several models and the desired outcome.
    The teamwork-based methods covered here are quite powerful and are used often
    to build more effective classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ensemble learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All ensemble methods are based on the idea that by combining multiple weaker
    learners, a stronger learner is created. Ensembles contain two or more machine
    learning models, which can be of the same type, such as several decision trees,
    or of different types, such as a decision tree and a neural network. Though there
    are myriad ways to construct an ensemble, they tend to fall into several general
    categories, which can be distinguished, in large part, by the answers to two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How are the ensemble’s models chosen and trained?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are the models’ predictions combined to make a single final prediction?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When answering these questions, it can be helpful to imagine the ensemble in
    terms of the following process diagram, which encompasses nearly all ensembling
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Ensembles combine multiple weaker models into a single stronger
    model'
  prefs: []
  type: TYPE_NORMAL
- en: In this design pattern, input training data is used to build several models.
    The **allocation function** dictates how much and what subsets of the training
    data each model receives. Do they each receive the full training dataset or merely
    a sample? Do they each receive every feature or a subset of features? The decisions
    made here will shape the training of the weaker learners that comprise the stronger
    ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Just as you’d want a variety of experts to advise your appearance on a television
    trivia game show, ensembles depend on a **diverse** set of classifiers, which
    means that they have uncorrelated classifications but still perform better than
    random chance. In other words, each classifier must be making an independent prediction,
    but each must also be doing more than merely guessing.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity can be added to the ensemble by including a variety of machine learning
    techniques, such as an ensemble that groups a decision tree, a neural network,
    and a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, the allocation function itself can also be a source of diversity
    by acting as a **data manipulator** and artificially varying the input data to
    bias the resulting learners, even if they use the same learning algorithm. As
    we will see in practice later, the allocation and data manipulation processes
    may be automated or included as part of the ensembling algorithm itself, or they
    may be performed by hand as part of the data engineering and model-building process.
    Overall, modes of increasing the ensemble’s diversity generally fall into five
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Using assorted base learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating the training sample by taking different samples at random, often
    by using bootstrapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating a single learning algorithm by using different hyperparameter settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing how the target feature is represented, such as representing an outcome
    as binary, categorical, or numeric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning the training data into subgroups that represent different patterns
    to be learned; for instance, one might stratify the examples by key features,
    and let models in the ensemble become experts on different subsets of the training
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, in an ensemble of decision trees, the allocation function might
    use bootstrap sampling to construct unique training datasets for each tree, or
    it may pass each one a different subset of features. On the other hand, if the
    ensemble already includes a diverse set of algorithms—such as a neural network,
    a decision tree, and a k-NN classifier—then the allocation function might pass
    the training data on to each algorithm relatively unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: After the ensemble’s models are trained, they can be used to generate predictions
    on future data, but this set of multiple predictions must be reconciled somehow
    to generate a single final prediction. The **combination function** is the step
    in the ensembling process that takes each of these predictions and combines them
    into a single authoritative prediction for the set. Of course, because some of
    the models may disagree on the predicted value, the function must somehow blend
    or unify the information from the learners. The combination function is also known
    as a **composer** due to its work synthesizing the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main strategies for merging or composing final predictions. The
    simpler of the two approaches involves **weighting methods**, which assign a score
    to each prediction that dictates how heavily it will factor into the final prediction.
    These range from a simple majority vote in which each classifier is weighted evenly,
    to more complex performance-based methods that grant more authority to some models
    than others if they have proven to be more reliable on past data.
  prefs: []
  type: TYPE_NORMAL
- en: The second approach uses more complex meta-learning methods, such as the model
    stacking technique, which will be covered in depth later in this chapter. These
    use the initial set of predictions from the weak learners to train a secondary
    machine learning algorithm to make the final prediction—a process that is analogous
    to a committee making recommendations to a leader that makes the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensembling methods are used to gain better performance than what is possible
    using only a single learning algorithm—the primary goal of the ensemble is to
    turn a group of weaker learners into a stronger, unified team. Still, there are
    many additional benefits, some of which may be surprising. These suggest additional
    reasons why one might turn to an ensemble, even outside of a machine learning
    competition environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The use of independent ensembles allows work in parallel**: Training independent
    classifiers separately means that work can be divided across multiple people.
    This allows more rapid iteration and may increase creativity. Each team member
    builds their best model, and the results can be easily combined into an ensemble
    at the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved performance on massive or minuscule datasets**: Many algorithms
    run into memory or complexity limits when an extremely large set of features or
    examples are used. An ensemble of independent models can be fed subsets of features
    or examples, which are more computationally efficient to train than a single full
    model, and importantly, can often be run in parallel using distributed computing
    methods. On the other side of the spectrum, ensembles also do well on the smallest
    datasets because resampling methods like bootstrapping are inherently part of
    the allocation function of many ensemble designs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The ability to synthesize data from distinct domains**: Since there is no
    one-size-fits-all learning algorithm, and each learning algorithm has its own
    biases and heuristics, the ensemble’s ability to incorporate evidence from multiple
    types of learners is increasingly important for modeling the most challenging
    learning tasks relying on data drawn from diverse domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A more nuanced understanding of difficult learning tasks**: Real-world phenomena
    are often extremely complex, with many interacting intricacies. Methods like ensembles,
    which divide the task into smaller modeled portions, are more able to capture
    subtle patterns that a single model might miss. Some learners in the set can go
    narrower and deeper to learn a specific subset of the most challenging cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: None of these benefits would be very helpful if you weren’t able to easily apply
    ensemble methods in R, and there are many packages available to do just that.
    Let’s look at several of the most popular ensemble methods and how they can be
    used to improve the performance of the credit model we’ve been working on.
  prefs: []
  type: TYPE_NORMAL
- en: Popular ensemble-based algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thankfully, using teams of machine learners to improve the predictive performance
    doesn’t mean you’ll need to train each ensemble member separately by hand, although
    this option does exist, as you will learn later in this chapter. Instead, there
    are ensemble-based algorithms that manipulate the allocation function to train
    a very large number of simpler models in a single step automatically. In this
    way, an ensemble that includes a hundred learners or more can be trained with
    no more human time and input than training a single learner. As easily as one
    might build a single decision tree model, it is possible to build an ensemble
    with hundreds of such trees and harness the power of teamwork. Although it would
    be tempting to assume this is a magic bullet, such power, of course, comes with
    downsides such as loss of interpretability and a less diverse set of base algorithms
    from which to choose. This will be apparent in the sections that follow, which
    cover the evolution of two decades’ worth of popular ensembling algorithms—all
    of which, not coincidentally, are based on decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the first ensemble methods to gain widespread acceptance used a technique
    called **bootstrap aggregating** or **bagging** for short. As described by Leo
    Breiman in the mid-1990s, bagging begins by generating several new training datasets
    using bootstrap sampling on the original training data. These datasets are then
    used to generate a set of models using a single learning algorithm. The models’
    predictions are combined using voting for classification and averaging for numeric
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: For additional information on bagging, refer to *Bagging predictors. Breiman
    L., Machine Learning, 1996, Vol. 24, pp. 123-140*.
  prefs: []
  type: TYPE_NORMAL
- en: Although bagging is a relatively simple ensemble, it can perform quite well
    if it is used with relatively **unstable** learners, that is, those generating
    models that tend to change substantially when the input data changes only slightly.
    Unstable models are essential for ensuring the ensemble’s diversity despite only
    minor variations across the bootstrap training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, bagging is most often used with decision trees, which have
    the tendency to vary dramatically given minor changes in input data.
  prefs: []
  type: TYPE_NORMAL
- en: The `ipred` package offers a classic implementation of bagged decision trees.
    To train the model, the `bagging()` function works similarly to many of the models
    used previously. The `nbagg` parameter is used to control the number of decision
    trees voting in the ensemble, with a default value of `25`. Depending on the difficulty
    of the learning task and the amount of training data, increasing this number may
    improve the model’s performance, up to a limit. The downside is that this creates
    additional computational expense, and a large number of trees may take some time
    to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the `ipred` package, we can create the ensemble as follows.
    We’ll stick to the default value of `25` decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `mybag` model works as expected in concert with the `predict()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the preceding results, the model seems to have fit the data extremely
    well—*too well*, probably, as the results are based only on the training data
    and thus may reflect overfitting rather than true performance on future unseen
    data. To obtain a better estimate of future performance, we can use the bagged
    decision tree method in the `caret` package to obtain a 10-fold CV estimate of
    accuracy and kappa. Note that the method name for the `ipred` bagging function
    is `treebag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The kappa statistic of 0.33 for this model suggests that the bagged tree model
    performs roughly as well as the C5.0 decision tree we tuned earlier in this chapter,
    which had a kappa statistic ranging from 0.32 to 0.34, depending on the tuning
    parameters. Keep this performance in mind as you read the next section, and consider
    the differences between the simple bagging technique and the more complex methods
    that build upon it.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another common ensemble-based method is called **boosting** because it improves
    or “boosts” the performance of weak learners to attain the performance of stronger
    learners. This method is based largely on the work of Robert Schapire and Yoav
    Freund, who have published extensively on the topic since the 1990s.
  prefs: []
  type: TYPE_NORMAL
- en: 'For additional information on boosting, refer to *Boosting: Foundations and
    Algorithms, Schapire, RE, Freund, Y, Cambridge, MA: The MIT Press, 2012*.'
  prefs: []
  type: TYPE_NORMAL
- en: Like bagging, boosting uses ensembles of models trained on resampled data and
    a vote to determine the final prediction. There are two key distinctions. First,
    the resampled datasets in boosting are constructed specifically to generate complementary
    learners. This means that the work cannot occur in parallel, as the ensemble’s
    models are no longer independent from one another. Second, rather than giving
    each learner an equal vote, boosting gives each learner a vote that is weighted
    based on its past performance. Models that perform better have greater influence
    over the ensemble’s final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting will result in performance that is often somewhat better and certainly
    no worse than the best model in the ensemble. Since the models in the ensemble
    are purposely built to be complementary, it is possible to increase ensemble performance
    to an arbitrary threshold simply by adding additional classifiers to the group,
    assuming that each additional classifier performs better than random chance. Given
    the obvious utility of this finding, boosting is thought to be one of the most
    significant discoveries in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Although boosting can create a model that meets an arbitrarily low error rate,
    this may not always be reasonable in practice. One reason for this is that the
    performance gains are incrementally smaller as additional learners are gained,
    making some thresholds practically infeasible. Additionally, the pursuit of pure
    accuracy may result in the model being overfitted to the training data and not
    generalizable to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: A boosting algorithm called **AdaBoost**, short for **adaptive boosting**, was
    proposed by Freund and Schapire in 1997\. The algorithm is based on the idea of
    generating weak learners that iteratively learn a larger portion of the difficult-to-classify
    examples in the training data by paying more attention (that is, giving more weight)
    to often misclassified examples.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning from an unweighted dataset, the first classifier attempts to model
    the outcome. Examples that the classifier predicted correctly will be less likely
    to appear in the training dataset for the following classifier, and conversely,
    the difficult-to-classify examples will appear more frequently. As additional
    rounds of weak learners are added, they are trained on data with successively
    more difficult examples. The process continues until the desired overall error
    rate is reached or performance no longer improves. At that point, each classifier’s
    vote is weighted according to its accuracy on the training data on which it was
    built.
  prefs: []
  type: TYPE_NORMAL
- en: Though boosting principles can be applied to nearly any type of model, the principles
    are most often used with decision trees. We already applied the boosting technique
    earlier in this chapter, as well as in *Chapter 5*, *Divide and Conquer – Classification
    Using Decision Trees and Rules*, as a method to improve the performance of a C5.0
    decision tree. With C5.0, boosting can be enabled by simply setting a `trials`
    parameter to an integer value greater than one.
  prefs: []
  type: TYPE_NORMAL
- en: The **AdaBoost.M1** algorithm provides a standalone implementation of AdaBoost
    for classification with trees. The algorithm can be found in the `adabag` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about the `adabag` package, refer to *adabag: An R Package
    for Classification with Boosting and Bagging, Alfaro, E, Gamez, M, Garcia, N,
    Journal of Statistical Software, 2013, Vol. 54, pp. 1-35*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create an `AdaBoost.M1` classifier for the credit data. The general syntax
    for this algorithm is similar to other modeling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, the `predict()` function is applied to the resulting object to make
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Departing from convention, rather than returning a vector of predictions, this
    returns an object with information about the model. The predictions are stored
    in a sub-object called `class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A confusion matrix can be found in the `confusion` sub-object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Before you get your hopes up about the perfect accuracy, note that the preceding
    confusion matrix is based on the model’s performance on the training data. Since
    boosting allows the error rate to be reduced to an arbitrarily low level, the
    learner simply continued until it made no more errors. This likely resulted in
    overfitting on the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more accurate assessment of performance on unseen data, we need to use
    another evaluation method. The `adabag` package provides a simple function to
    use 10-fold CV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on your computer’s capabilities, this may take some time to run,
    during which it will log each iteration to the screen—on a recent MacBook Pro
    computer, it took about a minute. After it completes, we can view a more reasonable
    confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can find the kappa statistic using the `vcd` package, as demonstrated in
    *Chapter 10*, *Evaluating Model Performance*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: With a kappa of `0.3397`, the boosted model is slightly outperforming the bagged
    decision trees, which had a kappa of around `0.3319`. Let’s see how boosting compares
    to another ensemble method.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the prior results were obtained using R version 4.2.3 on a Windows
    PC and verified on Linux. At the time this was written, slightly different results
    are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro. Also note
    that the AdaBoost.M1 algorithm can be tuned with `caret` by specifying `method
    = "AdaBoost.M1"`.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yet another tree-based ensemble-based method, called **random forests**, builds
    upon the principles of bagging but adds additional diversity to the decision trees
    by only allowing the algorithm to choose from a randomly selected subset of features
    each time it attempts to split. Beginning at the root node, the random forest
    algorithm might only be allowed to choose from a small number of features selected
    at random from the full set of predictors; at each subsequent split, a different
    random subset is provided. As is the case for bagging, once the ensemble of trees
    (the forest) is generated, the algorithm performs a simple vote to make the final
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: For more detail on how random forests are constructed, refer to *Random Forests,
    Breiman L, Machine Learning, 2001, Vol. 45, pp. 5-32*. Note that the phrase “random
    forests” is trademarked by Breiman and Cutler but is used colloquially to refer
    to any type of decision tree ensemble. A pedant would use the more general term
    **decision tree forests** except when referring to their specific implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that each tree is built on different and randomly selected sets of
    features helps ensure that each tree in the ensemble is unique. It is even possible
    that two trees in the forest may have been built from completely different sets
    of features. Random feature selection limits the decision tree’s greedy heuristic
    from picking the same low-hanging fruit each time the tree is grown, which may
    help the algorithm discover subtle patterns that the standard tree-growing method
    may miss. On the other hand, the potential for overfitting is limited given that
    each tree has just one vote of many in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these strengths, it is no surprise that the random forest algorithm quickly
    grew to become one of the most popular learning algorithms—only recently has its
    hype been surpassed by a newer ensemble method, which you will learn about shortly.
    Random forests combine versatility and power into a single machine learning approach
    and are not especially prone to overfitting or underfitting. Because the tree-growing
    algorithm uses only a small, random portion of the full feature set, random forests
    can handle extremely large datasets, where the so-called curse of dimensionality
    might cause other models to fail. At the same time, its predictive performance
    on most learning tasks is as good as, if not better than, all but the most sophisticated
    methods. The following table summarizes the strengths and weaknesses of random
    forest models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: An all-purpose model that performs well on most problems, including both classification
    and numeric prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle noisy or missing data as well as categorical or continuous features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selects only the most important features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used on data with an extremely large number of features or examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a decision tree, the model is not easily interpretable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May struggle with categorical features with very large numbers of levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot be extensively tuned if greater performance is desired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Their strong performance combined with the ease of use makes random forests
    a terrific place to begin most real-world machine learning projects. The algorithm
    also provides a solid benchmark for other comparisons with highly tuned models,
    as well as the other, more complex approaches you will learn about later.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a hands-on demonstration of random forests, we’ll apply the technique to
    the credit-scoring data we’ve been using in this chapter. Although there are several
    packages with random forest implementations in R, the aptly named `randomForest`
    package is perhaps the simplest, while the `ranger` package offers much better
    performance on large datasets. Both are supported by the `caret` package for experimentation
    and automated parameter tuning. The syntax for training a model with `randomForest`
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Random forest syntax'
  prefs: []
  type: TYPE_NORMAL
- en: By default, the `randomForest()` function creates an ensemble of 500 decision
    trees that each consider `sqrt(p)` random features at each split, where `p` is
    the number of features in the training dataset and `sqrt()` refers to R’s square
    root function. For example, since the credit data has 16 features, each of the
    500 decision trees would be allowed to consider only *sqrt*(*16*) = *4* predictors
    each time the algorithm attempts to split.
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not these default `ntree` and `mtry` parameters are appropriate depends
    on the nature of the learning task and training data. Generally, more complex
    learning problems and larger datasets (both more features as well as more examples)
    warrant a larger number of trees, though this needs to be balanced with the computational
    expense of training more trees. Once the `ntree` parameter is set to a sufficiently
    large value, the `mtry` parameter can be tuned to determine the best setting;
    however, the default tends to work well in practice. Assuming the number of trees
    is large enough, the number of randomly selected features can be surprisingly
    low before performance is degraded—but trying a few values is still a good practice.
    Ideally, the number of trees should be set large enough such that each feature
    has a chance of appearing in several models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the default `randomForest()` parameters work with the credit
    data. We’ll train the model just as we have done with other learners. As usual,
    the `set.seed()` function ensures that the result can be replicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For a summary of model performance, we can simply type the resulting object’s
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the random forest included 500 trees and tried four variables
    at each split, as expected. At first glance, you might be alarmed at the seemingly
    poor performance according to the confusion matrix—the error rate of 23.3 percent
    is far worse than the resubstitution error of any of the other ensemble methods
    so far. However, this confusion matrix does not show a resubstitution error. Instead,
    it reflects the **out-of-bag error rate** (listed in the output as `OOB estimate
    of error rate`), which, unlike a resubstitution error, is an unbiased estimate
    of the test set error. This means that it should be a fair estimate of future
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The out-of-bag estimate is computed using a clever technique during the construction
    of the random forest. Essentially, any example not selected for a single tree’s
    bootstrap sample can be used to test the model’s performance on unseen data. At
    the end of the forest construction, for each of the 1,000 examples in the dataset,
    any trees that did not use the example in training are allowed to make a prediction.
    These predictions are tallied, and a vote is taken to determine the single final
    prediction for the example. The total error rate of such predictions across all
    1,000 examples becomes the out-of-bag error rate. Because each prediction uses
    only a subset of the forest, it is not equivalent to a true validation or test
    set estimation, but it is a reasonable substitute.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 10*, *Evaluating Model Performance*, it was stated that any given
    example has a 63.2 percent chance of being included in a bootstrap sample. This
    implies that an average of 36.8 percent of the 500 trees in the random forest
    voted for each of the 1,000 examples in the out-of-bag estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the kappa statistic on the out-of-bag predictions, we can use
    the function in the `vcd` package as follows. The code applies the `Kappa()` function
    to the first two rows and columns of the `confusion` object, which stores the
    confusion matrix of the out-of-bag predictions for the `rf` random forest model
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With a kappa statistic of `0.381`, the random forest is our best-performing
    model yet. Its performance was better than the bagged decision tree ensemble,
    which had a kappa of about `0.332`, as well as the AdaBoost.M1 model, which had
    a kappa of about `0.340`.
  prefs: []
  type: TYPE_NORMAL
- en: The `ranger` package, as mentioned previously, is a substantially faster implementation
    of the random forest algorithm. For a dataset as small as the credit dataset,
    optimizing for computational efficiency may be less important than ease of use,
    and by default, `ranger` sacrifices some conveniences in order to increase speed
    and reduce the memory footprint. Consequently, although the `ranger` function
    is nearly identical to `randomForest()` in syntax, in practice, you may find that
    it breaks existing code or takes a bit of digging through the help pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recreate the previous model using `ranger`, we simply change the function
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting model has quite a similar out-of-bag prediction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compute kappa much as before while noting the slight difference in how
    the model’s confusion matrix sub-object was named:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The kappa value is `0.381`, which is the same as the result from the earlier
    random forest model. Note that this is coincidental, as the two algorithms are
    not guaranteed to produce identical results.
  prefs: []
  type: TYPE_NORMAL
- en: As with AdaBoost, the prior results were obtained using R version 4.2.3 on a
    Windows PC and verified on Linux. At the time this was written, slightly different
    results are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Gradient boosting** is an evolution of the boosting algorithm based on the
    finding that it is possible to treat the boosting process as an optimization problem
    to be solved using the gradient descent technique. We first encountered gradient
    descent in *Chapter 7*, *Black-Box Methods – Neural Networks and Support Vector
    Machines*, where it was introduced as a solution to optimize the weights in a
    neural network. You may recall that a cost function—essentially, the prediction
    error—relates the input values to the target. Then, by systematically analyzing
    how changes to the weights affect the cost, it is possible to find the set of
    weights that minimizes the cost. Gradient boosting treats the process of boosting
    in much the same way, with the weak learners in the ensemble being treated as
    the parameters to optimize. Models using this technique are termed **gradient
    boosting machines** or **generalized boosting models**—both of which can be abbreviated
    as **GBMs**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more on GBMs, see *Greedy Function Approximation: A Gradient Boosting Machine,
    Friedman JH, 2001, Annals of Statistics 29(5):1189-1232*.'
  prefs: []
  type: TYPE_NORMAL
- en: The following table summarizes the strengths and weaknesses of GBMs. In short,
    gradient boosting is extremely powerful and can produce some of the most accurate
    models but may require tuning to find the balance between over- and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: An all-purpose classifier that can perform extremely well on both classification
    and numeric prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can achieve even better performance than random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well on large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: May require tuning to match the performance of the random forest algorithm and
    more extensive tuning to exceed its performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because there are several hyperparameters to tune, finding the best combination
    requires many iterations and more computing power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the `gbm()` function in the `gbm` package for creating GBMs for both
    classification and numeric prediction. You’ll need to install and load this package
    to your R session if you haven’t already. As the following box shows, the syntax
    is like the machine learning functions used previously, but it has several new
    parameters that may need to be adjusted. These parameters control the complexity
    of the model and the balance between over- and underfitting. Without tuning, the
    GBM may not perform as well as simpler methods, but it generally can surpass the
    performance of most other methods once parameter values have been optimized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Gradient boosting machine (GBM) syntax'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train a simple GBM to predict loan defaults on the `credit` dataset
    as follows. For simplicity, we set `stringsAsFactors = TRUE` to avoid recoding
    the predictors, but then the target `default` feature must be converted back to
    a binary outcome, as the `gbm()` function requires this for binary classification.
    We’ll create a random sample for training and testing, then apply the `gbm()`
    function to the training data, leaving the parameters set to their defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Typing the name of the model provides some basic information about the GBM
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'More importantly, we can evaluate the model on the test set. Note that we need
    to convert the predictions to binary, as they are given as probabilities. If the
    probability of loan default is greater than 50 percent, we will predict default,
    otherwise, we predict non-default. The table shows the agreement between the predicted
    and actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To measure the performance, we’ll apply the `Kappa()` function to this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The resulting kappa value of about `0.361` is better than what was obtained
    with the boosted decision tree, but worse than the random forest model. Perhaps
    with a bit of tuning, we can get this higher.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the `caret` package to tune the GBM model and obtain a more robust
    performance measure. Recall that tuning needs a search grid, which we can define
    for GBM as follows. This will test three values for three of the `gbm()` function
    parameters and one value for the remaining parameter, which results in *3 * 3
    * 3 * 1 = 27* models to evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set the `trainControl` object to select the best model from a 10-fold
    CV experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we read in the `credit` dataset and supply the required objects to
    the `caret()` function while specifying the `gbm` method and the `Kappa` performance
    metric. Depending on the capabilities of your computer, this may take a few minutes
    to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Typing the name of the object shows the results of the experiment. Note that
    some lines of output have been omitted for brevity, but the full output contains
    27 rows—one for each model evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we can see that the best GBM model had a kappa of `0.394`,
    which exceeds the random forest trained previously. With additional tuning, it
    may be possible to bring the kappa up even higher. Or, as you will see in the
    next section, a more intensive form of boosting can be employed in the pursuit
    of even better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme gradient boosting with XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A cutting-edge implementation of the gradient boosting technique can be found
    in the **XGBoost** algorithm (`https://xgboost.ai`), which takes boosting to the
    “extreme” by improving the algorithm’s efficiency and performance. In the time
    since the algorithm was introduced in 2014, XGBoost has been found on top of the
    leaderboards of many machine learning competitions. In fact, according to the
    algorithm’s authors, among 29 winning solutions on Kaggle in 2015, a total of
    17 used the XGBoost algorithm. Likewise, in the 2015 KDD Cup (described in *Chapter
    11*, *Being Successful with Machine Learning*), all of the top 10 winners used
    XGBoost. Today, the algorithm is still the champion for traditional machine learning
    problems involving classification and numeric prediction, whereas its closest
    challenger, deep neural networks, tends to win only on unstructured data, such
    as image, audio, and text processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on XGBoost, see *XGBoost: A Scalable Tree Boosting System,
    Chen T and Guestrin C, 2016*. [https://arxiv.org/abs/1603.02754](https://arxiv.org/abs/1603.02754).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The great power of the XGBoost algorithm comes with the downside that the algorithm
    is not quite as easy to use and requires substantially more tuning than other
    methods examined so far. On the other hand, its performance ceiling tends to be
    higher than any other approach. The strengths and weaknesses of XGBoost are found
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: An all-purpose classifier that can perform extremely well on both classification
    and numeric prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps undisputedly, the current champion of performance on traditional learning
    problems; wins virtually every machine learning competition on structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable, performs well on large datasets, and can be run in parallel
    on distributed computing platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: More challenging to use than other functions, as it relies on external frameworks
    that do not use native R data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires extensive tuning of a large set of hyperparameters that can be difficult
    to understand without a strong math background
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because there are many tuning parameters, finding the best combination requires
    many iterations and more computing power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results in a “black box” model that is nearly impossible to interpret without
    explainability tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply the algorithm, we’ll use the `xgboost()` function in the `xgboost`
    package, which provides an R interface to the XGBoost framework. Entire books
    could be written about this framework, as it includes features for many types
    of machine learning tasks, and is highly extensible and adaptable to many high-performance
    computing environments. For more information about the XGBoost framework, see
    the excellent documentation on the web at [https://xgboost.readthedocs.io](https://xgboost.readthedocs.io).
    Our work will focus on a narrow slice of its functionality, as shown in the following
    syntax box, which is much denser than those for other algorithms due to a large
    increase in complexity and hyperparameters that may be tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: XGBoost (XGB) syntax'
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges with using XGBoost in R is its need to use data in matrix
    format rather than R’s preferred formats of tibbles or data frames. Because XGBoost
    is designed for extremely large datasets, it can also use sparse matrices, such
    as those discussed in previous chapters. You may recall that a sparse matrix only
    stores non-zero values, which makes it more memory-efficient than traditional
    matrices when many feature values are zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Data in matrix form is often sparse because factors are typically one-hot or
    dummy coded during the transition between the data frame and matrix. These encodings
    create additional columns for additional levels of the factor, and all columns
    are set to zero except the one “hot” value that indicates the level for the given
    example. In the case of dummy coding, one feature level is left out of the transformation,
    so it results in one fewer column than one-hot; the missing level can be indicated
    by the presence of zeros in all of the *n* - *1* columns.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot and dummy coding generally produce the same results, with the exception
    that statistics-based models like regression require dummy coding and will present
    errors or warning messages if one-hot is used instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by reading the `credit.csv` file and creating a sparse matrix of
    data from the `credit` data frame. The `Matrix` package provides a function to
    perform this task, which uses the R formula interface to determine the columns
    to include in the matrix. Here, the formula `~ . -default` tells the function
    to use all features except `default`, which we don’t want in the matrix, as this
    is our target feature for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm our work, let’s check the dimensions of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We still have 1,000 rows, but the columns have increased from 16 features in
    the original data frame to 36 in the sparse matrix. This is due to the dummy coding
    that was applied automatically when converting to matrix form. We can see this
    if we examine the first five rows and 15 columns of the sparse matrix using the
    `print()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The matrix is depicted with the dot (`.`) character indicating cells with zero
    values. The first column (`1`, `2`, `3`, `4`, `5`) is the row number and the second
    column (`1`, `1`, `1`, `1`, `1`) is a column for the intercept term, which was
    added automatically by the R formula interface. Two columns have numbers (`6`,
    `48`, …) and (`1169`, `5951`, …) that correspond to the numeric values of the
    `months_loan_duration` and `amount` features, respectively. All other columns
    are dummy-coded versions of factor variables. For instance, the third, fourth,
    and fifth columns reflect the `checking_balance` feature, with a `1` in the third
    column indicating a value of `'> 200 DM'`, a `1` in the fourth column indicating
    `'1 – 200 DM'`, and a 1 in the fifth column indicating the `'unknown'` feature
    value. Rows showing the sequence `. . .` in columns 3, 4, and 5 fall into the
    reference category, which was the `'< 0 DM'` feature level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are not building a regression model, the intercept column full of
    `1` values is useless for this analysis and can be removed from the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll split the matrix at random into training and test sets using a
    90-10 split as we’ve done before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the work was done correctly, we’ll check the dimensions of these
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the training set has 900 rows and 35 columns, and the test set
    has 100 rows and a matching set of columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we’ll create training and test vectors of labels for `default`, the
    target to be predicted. These are transformed from factors to binary `1` or `0`
    values using an `ifelse()` function so that they can be used to train and evaluate
    the XGBoost model, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re now ready to start building the model. After installing the `xgboost`
    package, we’ll load the library and start to define the hyperparameters for training.
    Without knowing where else to begin, we’ll set the values to their defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, after setting the random seed, we’ll train the model, supplying our parameters
    object as well as the matrix of training data and the target labels. The `nrounds`
    parameter determines the number of boosting iterations. Without a better guess,
    we’ll set this to `100`, which is a common starting point due to empirical evidence
    suggesting that results tend to improve very little beyond this value. Lastly,
    the `verbose` and `print_every_n` options are used to turn on diagnostic output
    and display the progress after every 10 boosting iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should appear as the training is completed, showing that all 100
    iterations occurred and the training error (labeled `train-logloss`) continued
    to decline with additional rounds of boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Knowing whether additional iterations would help the model performance or result
    in overfitting is something we can determine via tuning later. Before doing so,
    let’s look at the performance of this trained model on the test set, which we
    held out earlier. First, the `predict()` function obtains the predicted probability
    of loan default for each row of test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use `ifelse()` to predict a default (value `1`) if the probability
    of a default is at least 0.50, or non-default (value `0`) otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Comparing the predicted to actual values, we find an accuracy of *(62 + 14)
    / 100 = 76* percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the kappa statistic suggests there is still room to improve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of `0.3766` is a bit lower than the `0.394` we obtained with the
    GBM model, so perhaps a bit of hyperparameter tuning can help. For this, we’ll
    use `caret`, starting with a tuning grid comprising a variety of options for each
    of the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting grid contains *2 * 3 * 2 * 3 * 3 * 2 * 1 = 216* different combinations
    of `xgboost` hyperparameter values. We’ll evaluate each of these potential models
    in `caret` using 10-fold CV, as we’ve done for other models. Note that the `verbosity`
    parameter is set to zero so that the `xgboost()` function output is suppressed
    for the many iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the capabilities of your computer, the experiment may take a few
    minutes to complete, but once it finishes, typing `m_xgb` will provide the results
    of all 216 models tested. We can also obtain the best model directly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The kappa value for this model can be found using the `max()` function to find
    the highest value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The kappa value of `0.406` is our best-performing model so far, exceeding the
    `0.394` of the GBM model and the `0.381` of the random forest. The fact that XGBoost
    required so little effort to train—with a bit of fine-tuning—yet still surpassed
    other powerful techniques provides examples of why it always seems to win machine
    learning competitions. Yet, with even more tuning, it may be possible to go higher
    still! Leaving that as an exercise to you, the reader, we’ll now turn our attention
    to the question of why all of these popular ensembles seem to focus exclusively
    on decision tree-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Why are tree-based ensembles so popular?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After reading the prior sections, you would not be the first person to wonder
    why ensembling algorithms seem to always be built upon decision trees. Although
    trees are not required for building an ensemble, there are several reasons why
    they are especially well-suited for this process. You may have noted some of them
    already:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles work best with diversity, and because decision trees are not robust
    to small changes in the data, random sampling the same training data can easily
    create a diverse set of tree-based models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the greedy “divide-and-conquer” based algorithm, decision trees are
    computationally efficient and perform relatively well despite this fact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees can be grown purposely large or small to overfit and underfit
    as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees can automatically ignore irrelevant features, which reduces the
    negative impact of the “curse of dimensionality”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees can be used for numeric prediction as well as classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these characteristics, it is not difficult to see why we’ve ended up
    with a wealth of tree-based ensembling approaches such as bagging, boosting, and
    random forests. The distinctions among them are subtle but important.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table may help contrast the tree-based ensembling algorithms
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ensembling Algorithm** | **Allocation Function** | **Combination Function**
    | **Other Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging | Provides each learner with a bootstrap sample of the training data
    | The learners are combined using a vote for classification or a weighted average
    for numeric prediction | Uses an independent ensemble — the learners can be run
    in parallel |'
  prefs: []
  type: TYPE_TB
- en: '| Boosting | The first learner is given a random sample; subsequent samples
    are weighted to have more difficult-to-predict cases | The learners’ predictions
    are combined as above but weighted according to their performance on training
    data | Uses a dependent ensemble — each tree in the sequence receives data that
    earlier trees found challenging |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | Like bagging, each tree receives a bootstrap sample of training
    data; however, features are also randomly selected for each tree split | Similar
    to bagging | Similar to bagging, but the added diversity via random feature selection
    allows additional benefits for larger ensembles |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient Boosting Machine (GBM) | Conceptually similar to boosting | Similar
    to boosting, but there are many more learners and they comprise a complex mathematical
    function | Uses gradient descent to make a more efficient boosting algorithm;
    the trees are generally not very deep (decision tree “stumps”) but there are many
    more of them; requires more tuning |'
  prefs: []
  type: TYPE_TB
- en: '| eXtreme Gradient Boosting (XGB) | Similar to GBM | Similar to GBM | Similar
    to GBM but more extreme; uses optimized data structures, parallel processing,
    and heuristics to create a very performant boosting algorithm; tuning is essential
    |'
  prefs: []
  type: TYPE_TB
- en: To be able to distinguish among these approaches reveals a deep understanding
    of several aspects of ensembling. Additionally, the most recent techniques, such
    as random forests and gradient boosting, are among the best-performing learning
    algorithms and are being used as off-the-shelf solutions to solve some of the
    most challenging business problems. This may help explain why companies hiring
    data scientists and machine learning engineers often ask candidates to describe
    or compare these algorithms as part of the interview process. Thus, even though
    tree-based ensembling algorithms are not the only approach to machine learning,
    it is important to be aware of their potential uses. However, as the next section
    describes, trees aren’t the only approach to building a diverse ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking models for meta-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rather than using a canned ensembling method like bagging, boosting, or random
    forests, there are situations in which a tailored approach to ensembling is warranted.
    Although these tree-based ensembling techniques combine hundreds or even thousands
    of learners into a single, stronger learner, the process is not much different
    than training a traditional machine learning algorithm, and suffers some of the
    same limitations, albeit to a lesser degree. Being based on decision trees that
    have been weakly trained and minimally tuned may, in some cases, put a ceiling
    on the ensemble’s performance relative to one composed of a more diverse set of
    learning algorithms that have been extensively tuned with the benefit of human
    intelligence. Furthermore, although it is possible to parallelize tree-based ensembles
    like random forests and XGB, this only parallelizes the computer’s effort—not
    the human effort of model building.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, it is possible to increase an ensemble’s diversity by not only adding
    additional learning algorithms but by distributing the work of model building
    to additional human teams working in parallel. In fact, many of the world’s competition-winning
    models were built by taking other teams’ best models and ensembling them together.
  prefs: []
  type: TYPE_NORMAL
- en: This type of ensemble is conceptually quite simple, offering performance boosts
    that would be otherwise unobtainable, but can become complex in practice. Getting
    the implementation details correct is crucial to avoid disastrous levels of overfitting.
    Done correctly, the ensemble will perform at least as well as the strongest model
    in the ensemble, and often substantially better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining **receiver operating characteristic** (**ROC**) curves, as introduced
    in *Chapter 10*, *Evaluating Model Performance*, provides a simple method to determine
    whether two or more models would benefit from ensembling. If two models have intersecting
    ROC curves, their **convex hull**—the outermost boundary that would be obtained
    by stretching an imaginary rubber band around the curves—represents a hypothetical
    model that can be obtained by interpolating, or combining, the predictions from
    these models. As depicted in *Figure 14.9*, two ROC curves with identical **area
    under the curve** (**AUC**) values of *0.70* might create a new model with an
    AUC of *0.72* when paired in an ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, radar chart  Description automatically generated](img/B17290_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: When two or more ROC curves intersect, their convex hull represents
    a potentially better classifier that can be generated by combining their predictions
    in an ensemble'
  prefs: []
  type: TYPE_NORMAL
- en: Because this form of ensembling is performed largely by hand, a human needs
    to provide the allocation and combination functions for the models in the ensemble.
    In their simplest form, these can be implemented quite pragmatically. For example,
    suppose that the same training dataset has been given to three different teams.
    This is the allocation function. These teams can use this dataset however they
    see fit to build the best possible model using evaluation criteria of their choosing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, each team is given the test set, and their models are used to make predictions,
    which must be combined into a single, final prediction. The combination function
    can take multiple different forms: the groups could vote, the predictions could
    be averaged, or the predictions could be weighted according to how well each group
    performed in the past. Even the simple approach of choosing one group at random
    is a viable strategy, assuming each group performs better than all others at least
    once in a while. Of course, even more intelligent approaches are possible, as
    you will soon learn.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model stacking and blending
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the most sophisticated custom ensembles apply machine learning to learn
    a combination function for the final prediction. Essentially, it is trying to
    learn which models can and cannot be trusted. This arbiter learner may realize
    that one model in the ensemble is a poor performer and shouldn’t be trusted or
    that another deserves more weight in the ensemble. The arbiter function may also
    learn more complex patterns. For example, suppose that when models *M1* and *M2*
    agree on the outcome, the prediction is almost always accurate, but otherwise
    *M3* is generally more accurate than either of the two. In this case, an additional
    arbiter model could learn to ignore the vote of *M1* and *M2* except when they
    agree. This process of using the predictions of several models to train a final
    model is called **stacking**.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart  Description automatically generated](img/B17290_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Stacking is a sophisticated ensemble that uses an arbiter learning
    algorithm to combine the predictions of a set of learners and make a final prediction'
  prefs: []
  type: TYPE_NORMAL
- en: More broadly, stacking falls within a methodology known as **stacked generalization**.
    As formally defined, the stack is constructed using first-level models that have
    been trained via CV, and a second-level model or **meta-model** that is trained
    using the predictions for the out-of-fold samples—the examples the model does
    not see during training but is tested on during the CV process.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose three first-level models are included in the stack and
    each one is trained using 10-fold CV. If the training dataset includes 1,000 rows,
    each of the three first-stage models is trained on 900 rows and tested on 100
    rows ten times. The 100-row test sets, when combined, comprise the entire training
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As all three models have made a prediction for every row of the training data,
    a new table can be constructed with four columns and 1,000 rows: the first three
    columns represent the predictions for the three models and column four represents
    the true value of the target. Note that because the predictions made for each
    of these 100 rows were made on the other 900 rows, all 1,000 rows are predictions
    on unseen data. This allows the second-stage meta-model, which is often a regression
    or logistic regression model, to learn which first-stage models perform better
    by training using the predicted values as the predictors of the true value. This
    process of finding the optimal combination of learners is sometimes called **super
    learning**, and the resulting model may be called a **super learner**. This process
    is often performed by machine learning software or packages, which train numerous
    learning algorithms in parallel and stack them together automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B17290_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: In a stacked ensemble, the second-stage meta-model or “super
    learner” learns from the predictions of the first-stage models on out-of-fold
    samples'
  prefs: []
  type: TYPE_NORMAL
- en: For a more hands-on approach, a special case of stacked generalization called
    **blending** or **holdout stacking** provides a simplified way to implement stacking
    by replacing CV with a holdout sample. This allows the work to be distributed
    across teams more easily by merely dividing the training data into a training
    set for the first-level models and using a holdout set for the second-level meta-learner.
    It may also be less prone to overfitting the CV “information leak” described in
    *Chapter 11*, *Being Successful with Machine Learning*. Thus, even though it is
    a simple approach, it can be quite effective; blending is often what competition-winning
    teams do when they take other models and ensemble them together for better results.
  prefs: []
  type: TYPE_NORMAL
- en: The terminology around stacking, blending, and super learning is somewhat fuzzy,
    and many use the terms interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Practical methods for blending and stacking in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform blending in R requires a careful roadmap, as getting the details
    wrong can lead to extreme overfitting and models that perform no better than random
    guessing. The following figure illustrates the process. Begin by imagining that
    you are tasked with predicting loan defaults and have access to one million rows
    of past data. Immediately, you should partition the dataset into training and
    test sets; of course, the test set should be kept in a vault for evaluating the
    ensemble later. Assume the training set is 750,000 rows and the test set is 250,000
    rows. The training set must then be divided yet again to create datasets for training
    the level one models and the level two meta-learner. The exact proportions are
    somewhat arbitrary, but it is customary to use a smaller set for the second-stage
    model—sometimes as low as ten percent. As *Figure 14.12* depicts, we might use
    500,000 rows for level one and 250,000 rows for level two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: The full training dataset must be divided into distinct subsets
    for training the level one and level two models'
  prefs: []
  type: TYPE_NORMAL
- en: The 500,000-row level one training dataset is used to train the first-level
    models exactly as we have done many times throughout this book. The *M1*, *M2*,
    and *M3* models may use any learning algorithm, and the work of building these
    models can even be distributed across different teams working independently.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need for the models or teams to use the same set of features from
    the training data or the same form of feature engineering, assuming each team’s
    feature engineering pipeline can be replicated or automated in the future when
    the ensemble is to be deployed. The important thing is that *M1*, *M2*, and *M3*
    should be able to take a dataset with identical features and produce a prediction
    for each row.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 250,000-row level two training dataset is then fed into the *M1*, *M2*,
    and *M3* models after being processed through their associated feature engineering
    pipelines, and three vectors of 250,000 predictions are obtained. These vectors
    are labeled *p1*, *p2*, and *p3* in the diagram. When combined with the 250,000
    true values of the target (labeled *c* in the diagram) obtained from the level
    two training dataset, a four-column data frame is produced, as depicted in *Figure
    14.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: The dataset used to train the meta-model is composed of the predictions
    from the first-level models and the actual target value from the level two training
    data'
  prefs: []
  type: TYPE_NORMAL
- en: This type of data frame is used to create a meta-model, typically using regression
    or logistic regression, which predicts the actual target value (*c* in *Figure
    14.12*) using the predictions of *M1*, *M2*, and *M3* (*p1*, *p2*, and *p3* in
    *Figure 14.12*) as predictors. In an R formula, this might be specified in a form
    like `c ~ p1 + p2 + p3`, which results in a model that weighs the input from three
    different predictions to make its own final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the future performance of this final meta-model, we must use the
    250,000-row test set, which, as illustrated in *Figure 14.12* previously, was
    held out during the training process. As shown in *Figure 14.14*, the test dataset
    is then fed to the *M1*, *M2*, and *M3* models and their associated feature engineering
    pipelines, and much like in the previous step, three vectors of 250,000 predictions
    are obtained. However, rather than *p1*, *p2*, and *p3* being used to train a
    meta-model, they are now used as predictors for the existing meta-model to obtain
    a final prediction (labeled *p4*) for each of the 250,000 test cases. This vector
    can be compared to the 250,000 true values of the target in the test set to perform
    the performance evaluation and obtain an unbiased estimate of the ensemble’s future
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: To obtain an unbiased estimate of the ensemble’s future performance,
    the test set is used to generate predictions for the level one models, which are
    then used to obtain the meta-model’s final predictions'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above methodology is flexible to create other interesting types of ensembles.
    *Figure 14.15* illustrates a blended ensemble that combines models trained on
    completely different subsets of features. Specifically, it envisions a learning
    task in which Twitter profile data is used to make a prediction about the user—perhaps
    their gender or whether they would be interested in purchasing a particular product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated](img/B17290_14_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: The stack’s first-level models can be trained on different features
    in the training set, while the second-level model is trained on their predictions'
  prefs: []
  type: TYPE_NORMAL
- en: The first model receives the profile’s picture and trains a deep learning neural
    network with the image data to predict the outcome. Model two receives a set of
    tweets for the user and uses a text-based model like Naive Bayes to predict the
    outcome. Lastly, model three is a more conventional model using a traditional
    data frame of demographic data like location, total number of tweets, last login
    date, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: All three models are combined, and the meta-model can learn whether the image,
    text, or profile data is most helpful for predicting the gender or purchasing
    behavior. Alternatively, because the meta-model is a logistic regression model
    like *M3*, it would be possible to supply the profile data directly to the second-stage
    model and skip the construction of *M3* altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from constructing blended ensembles by hand as described here, there is
    a growing set of R packages to assist with this process. The `caretEnsemble` package
    can assist with ensemble models trained with the `caret` package and ensure that
    the stack’s sampling is handled correctly for stacking or blending. The `SuperLearner`
    package provides an easy way to create a super learner; it can apply dozens of
    base algorithms to the same dataset and stack them together automatically. As
    an off-the-shelf algorithm, this may be useful for building a powerful ensemble
    with the least amount of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should now know the approaches that are used
    to win data mining and machine learning competitions. Automated tuning methods
    can assist with squeezing every bit of performance out of a single model. On the
    other hand, tremendous gains are possible by creating groups of machine learning
    models called ensembles, which work together to achieve greater performance than
    single models can by working alone. A variety of tree-based algorithms, including
    random forests and gradient boosting, provide the benefits of ensembles but can
    be trained as easily as a single model. On the other hand, learners can be stacked
    or blended into ensembles by hand, which allows the approach to be carefully tailored
    to a learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: With a variety of options for improving the performance of a model, where should
    someone begin? There is no single best approach, but practitioners tend to fall
    into one of three camps. First, some begin with one of the more sophisticated
    ensembles such as random forests or XGBoost, and spend most of their time tuning
    and feature engineering to achieve the highest possible performance for this model.
    A second group might try a variety of approaches, then collect the models into
    a single stacked or blended ensemble to create a more powerful learner. The third
    approach might be described as “throw everything at the computer and see what
    sticks.” This attempts to feed the learning algorithm as much data as possible
    and as quickly as possible, and is sometimes combined with automated feature engineering
    or dimensionality reduction techniques like those described in the previous chapters.
    With practice, you may be drawn to some of these ideas more than others, so feel
    free to use whichever works best for you.
  prefs: []
  type: TYPE_NORMAL
- en: Although this chapter was designed to help you prepare competition-ready models,
    note that your fellow competitors have access to the same techniques. You won’t
    be able to get away with stagnancy; therefore, continue to add proprietary methods
    to your bag of tricks. Perhaps you can bring unique subject-matter expertise to
    the table, or perhaps your strengths include an eye for detail in data preparation.
    In any case, practice makes perfect, so take advantage of competitions to test,
    evaluate, and improve your machine learning skillset. In the next chapter—the
    last in this book—we’ll look at ways to apply cutting-edge “big data” techniques
    to some highly specialized and difficult data tasks using R.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
