- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Building Better Learners
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建更好的学习者
- en: When a sports team falls short of meeting its goal—whether it is to obtain an
    Olympic gold medal, a league championship, or a world record time—it must search
    for possible improvements. Imagine that you’re the team’s coach. How would you
    spend your practice sessions? Perhaps you’d direct the athletes to train harder
    or train differently in order to maximize every bit of their potential. You might
    also focus on teamwork to use each athlete’s strengths and weaknesses more smartly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当一支运动队未能达到其目标时——无论是获得奥运金牌、联赛冠军还是世界纪录时间——它必须寻找可能的改进。想象一下你是这支球队的教练。你会在训练中如何安排？也许你会指导运动员更加努力或以不同的方式训练，以最大限度地发挥他们的潜力。你也可能会专注于团队合作，更明智地利用每位运动员的优势和劣势。
- en: Now imagine that you’re training a championship machine learning algorithm.
    Perhaps you hope to compete in machine learning competitions or maybe you simply
    need to outperform business competitors. Where do you begin? Despite the different
    context, the strategies for improving a sports team’s performance are like those
    used for improving the performance of statistical learners. As the coach, it is
    your job to find the combination of training techniques and teamwork skills that
    allow the machine learning project to meet your performance goals.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下你正在训练一个冠军机器学习算法。也许你希望参加机器学习竞赛，或者你可能只是需要超越商业竞争对手。你从哪里开始？尽管背景不同，提高运动队表现的战略与提高统计学习器表现的战略相似。作为教练，你的任务是找到训练技巧和团队合作技能的组合，使机器学习项目能够达到你的性能目标。
- en: 'This chapter builds on the material covered throughout this book to introduce
    techniques that improve the predictive ability of learning algorithms. You will
    learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章基于本书所涵盖的内容，介绍了提高学习算法预测能力的技巧。你将学习：
- en: Techniques for automating model performance tuning by systematically searching
    for the optimal set of training conditions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过系统地搜索最佳训练条件集来自动化模型性能调整的技术
- en: Methods for combining models into groups that use teamwork to tackle tough learning
    tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型组合成团队，利用团队合作解决困难的学习任务的方法
- en: How to use and differentiate among popular variants of decision trees that have
    become popular due to their impressive performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用并区分因表现卓越而流行的决策树的不同变体
- en: None of these methods will be successful for every problem. Yet looking at the
    winning entries to machine learning competitions, you’ll likely find at least
    one of them has been employed. To be competitive, you too will need to add these
    skills to your repertoire.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法并不适用于每个问题。然而，查看机器学习竞赛的获胜作品，你可能会发现至少其中之一已被采用。为了具有竞争力，你也需要将这些技能添加到你的技能库中。
- en: Tuning stock models for better performance
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整股票模型以获得更好的性能
- en: Some machine learning tasks are well suited to be solved by the stock models
    presented in prior chapters. For these tasks, it may not be necessary to spend
    much time iterating and refining the model, because it may perform well enough
    without additional effort. On the other hand, many real-world tasks are inherently
    more difficult. For these tasks, the underlying concepts to be learned tend to
    be extremely complex, requiring an understanding of many subtle relationships,
    or the problem may be affected by substantial amounts of random variability, which
    makes it difficult to find the signal within the noise.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习任务非常适合由前几章中介绍的股票模型来解决。对于这些任务，可能没有必要花费太多时间迭代和改进模型，因为它可能在没有额外努力的情况下表现良好。另一方面，许多现实世界的任务本质上是更困难的。对于这些任务，需要学习的基本概念往往非常复杂，需要理解许多微妙的关系，或者问题可能受到大量随机变异性的影响，这使得在噪声中找到信号变得困难。
- en: Developing models that perform extremely well on these types of challenging
    problems is every bit an art as it is a science. Sometimes a bit of intuition
    is helpful when trying to identify areas where performance can be improved. In
    other cases, finding improvements will require a brute-force, trial-and-error
    approach. Of course, this is one of the strengths of using machines that never
    tire and never become bored; searching for numerous potential improvements can
    be made easier by automated programs. As we will see, however, human effort and
    computing time are not always fungible, and creating a finely-tuned learning algorithm
    can come with its own costs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开发在这些具有挑战性的问题上表现极好的模型，既是一门艺术，也是一门科学。有时，在尝试确定可以改进性能的领域时，一点直觉是有帮助的。在其他情况下，找到改进可能需要一种暴力搜索、尝试和错误的办法。当然，这是使用机器的一个优点，因为机器永远不会感到疲倦，也永远不会感到无聊；通过自动化程序可以更容易地搜索众多潜在改进。然而，我们将看到，人力和计算时间并不总是可以互换的，创建一个精心调整的学习算法可能会带来自己的成本。
- en: We attempted a difficult machine learning problem *in* *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*, as we attempted
    to predict bank loans that were likely to enter default. Although we were able
    to achieve a respectable classification accuracy of 82 percent, upon more careful
    examination in *Chapter 10*, *Evaluating Model Performance*, we realized that
    the accuracy statistic was a bit misleading. The kappa statistic—a better measure
    of performance for unbalanced outcomes­—was only about 0.294 as measured via 10-fold
    **cross-validation** (**CV**), which suggested that the model was performing somewhat
    poorly, despite the high accuracy. In this section, we’ll revisit the credit scoring
    model to see whether we can improve the results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第5章*《分而治之 - 使用决策树和规则进行分类》*中尝试了一个困难的机器学习问题，我们试图预测可能进入违约的银行贷款。尽管我们能够实现82%的可敬的分类准确率，但在第10章*《评估模型性能》*中更仔细地检查后，我们发现准确度统计指标有些误导。kappa统计量——不平衡结果性能的更好衡量指标——通过10折**交叉验证**（**CV**）测量仅为0.294，这表明尽管准确率很高，但模型的表现有些不佳。在本节中，我们将重新审视信用评分模型，看看我们是否可以改进结果。
- en: 'To follow along with the examples, download the `credit.csv` file from the
    Packt Publishing website and save it to your R working directory. Load the file
    into R using the following command: `credit <- read.csv("credit.csv")`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随示例进行操作，请从Packt Publishing网站下载`credit.csv`文件，并将其保存到你的R工作目录中。使用以下命令将文件加载到R中：`credit
    <- read.csv("credit.csv")`。
- en: You may recall that we first used a stock C5.0 decision tree to build the classifier
    for the credit data and later attempted to improve the classifier’s performance
    by adjusting the `trials` option to increase the number of boosting iterations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们最初使用了一个标准的C5.0决策树来构建信用数据的分类器，后来尝试通过调整`trials`选项来增加提升迭代的次数，以提高分类器的性能。
- en: By changing the number of iterations from the default value of 1 up to the value
    of 10, we were able to increase the model’s accuracy. As defined in *Chapter 11*,
    *Being Successful with Machine Learning*, these model options, known as hyperparameters,
    are not learned automatically from the data but are instead set before training.
    The process of testing various hyperparameter settings to achieve a better model
    fit is thus called **hyperparameter tuning**, and strategies for tuning range
    from simple ad hoc trial and error to more rigorous and systematic iteration.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将迭代次数从默认值1增加到10，我们能够提高模型的准确度。如第11章*《用机器学习取得成功》*中定义的，这些被称为超参数的模型选项不是从数据中自动学习的，而是在训练之前设置的。因此，测试各种超参数设置以实现更好的模型拟合的过程被称为**超参数调整**，调整策略从简单的临时尝试和错误到更严格和系统的迭代。
- en: Hyperparameter tuning is not limited to decision trees. For instance, we tuned
    k-NN models when we searched for the best value of *k*. We also tuned neural networks
    and support vector machines as we adjusted the number of nodes and the number
    of hidden layers, or chose different kernel functions. Most machine learning algorithms
    allow the adjustment of at least one hyperparameter, and the most sophisticated
    models offer many ways to tweak the model fit. Although this allows the model
    to be tailored closely to the learning task, the complexity of the many options
    can be daunting. A more systematic approach is warranted.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整不仅限于决策树。例如，我们在寻找最佳 *k* 值时调整了 k-NN 模型。我们还调整了神经网络和支持向量机，当我们调整节点数和隐藏层数，或选择不同的核函数时。大多数机器学习算法至少允许调整一个超参数，而最复杂的模型提供了许多调整模型拟合的方法。虽然这允许模型紧密地适应学习任务，但众多选项的复杂性可能会令人望而却步。需要一种更系统的方法。
- en: Determining the scope of hyperparameter tuning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定超参数调整的范围
- en: When performing hyperparameter tuning, it is important to put bounds on the
    scope to prevent the search from proceeding endlessly. The computer provides the
    muscle, but it is up to the human to dictate where to look and for how long. Even
    as computing power is growing and cloud computing costs are shrinking, the search
    can easily get out of hand when sifting through nearly endless combinations of
    values. A narrow or shallow tuning scope may last long enough to grab a cup of
    coffee, while a wide or deep scope may give you time to get a good night of sleep—or
    more!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行超参数调整时，重要的是要对范围设定界限，以防止搜索无休止地进行。计算机提供力量，但决定在哪里寻找以及寻找多长时间的责任在于人类。即使计算能力在增长，云计算成本在下降，但在筛选几乎无尽的值组合时，搜索很容易失控。一个狭窄或浅层次的调整范围可能足以让你喝上一杯咖啡，而一个广泛或深层次的调整范围可能会给你足够的时间好好睡一觉——或者更多！
- en: Time and money are often fungible, as you may be able to buy time in the form
    of additional computing resources or by enlisting additional team members to build
    models faster or in parallel. Even so, taking this for granted can lead to ruin
    in the form of budget overruns or missed deadlines because it is easy for the
    scope to balloon quickly when work proceeds down countless tangents and dead ends
    without a plan. To avoid such pitfalls, it is wise to strategize about the breadth
    and depth of the tuning process beforehand.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和金钱往往是可互换的，因为你可能能够通过购买额外的计算资源或招募额外的团队成员来更快或并行地构建模型来购买时间。即便如此，这种假设可能导致预算超支或错过截止日期，因为如果没有计划，工作可能会迅速偏离无数的方向和死胡同。为了避免这样的陷阱，事先对调整过程的广度和深度进行战略规划是明智的。
- en: You might start by thinking about tuning as a process much like playing the
    classic board game *Battleship*. In this game, your opponent has placed a fleet
    of battleships on a two-dimensional grid, which is hidden out of your view. Your
    goal is to destroy the opponent’s fleet by guessing the coordinates of all their
    ships before they do the same to yours. Because the ships are known sizes and
    shapes, a smart player will begin by broadly probing the search grid in a checkerboard
    pattern but quickly focus on a specific target once it has been hit.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会首先将调整过程想象成玩经典的棋盘游戏 *Battleship*。在这个游戏中，你的对手在一个二维网格上放置了一支舰队，而你无法看到。你的目标是猜测他们所有舰船的坐标，并在他们对你做同样的事情之前摧毁他们的舰队。因为舰船的大小和形状是已知的，一个聪明的玩家会首先以棋盘格的图案广泛地探测搜索网格，但一旦被击中，就会迅速聚焦于一个特定的目标。
- en: This is a better strategy than guessing coordinates at random or iterating across
    each coordinate systematically, both of which are inefficient in comparison.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这比随机猜测坐标或系统地迭代每个坐标都要好，这两种方法在效率上都比较低。
- en: '![Diagram  Description automatically generated](img/B17290_14_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_14_01.png)'
- en: 'Figure 14.1: The hunt for the optimal machine learning hyperparameters can
    be much like playing the classic Battleship board game'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：寻找最优机器学习超参数的过程可以类似于玩经典的“战舰”棋盘游戏
- en: 'Similarly, there are methods for tuning that are more efficient than systematic
    iteration over endless values and combinations of values. With experience, you
    will develop an intuition for how to proceed, but for the first few attempts,
    it may be useful to think intentionally about the process. The following general
    strategy, listed as a series of steps, can be adapted to your machine learning
    project, computing and staffing resources, and work style:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，有一些比在无限值和值组合上系统迭代更有效的调整方法。随着经验的积累，你会发展出如何进行的感觉，但最初的几次尝试，有意识地思考这个过程可能是有用的。以下列出的一系列一般策略，可以根据你的机器学习项目、计算和人力资源以及工作风格进行调整：
- en: '**Replicate the real-world evaluation criteria**: To find the single best set
    of model hyperparameters, it is important that the models are evaluated using
    the same criteria as will be used in deployment. This may mean choosing an evaluation
    metric that mirrors the final, real-world metric, or it may involve writing a
    function that simulates the deployment environment.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复制现实世界的评估标准**：为了找到最佳的模型超参数集，重要的是模型应该使用与部署时相同的标准进行评估。这可能意味着选择一个反映最终现实指标的评估指标，或者可能涉及编写一个模拟部署环境的函数。'
- en: '**Consider the resource usage for one iteration**: As tuning will be iterating
    many times on the same algorithm, you should have an estimate of the time and
    computing resources needed for a single iteration. If it takes one hour to train
    a single model, it will take 100 hours or more for 100 iterations. If the computer
    memory is already at its limit, it is likely that you will exceed the limit during
    tuning. If this is a problem, you will need to invest in additional computing
    power, run the experiment in parallel, or reduce the size of the dataset via random
    sampling.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**考虑一次迭代的资源使用**：由于调整将在同一算法上迭代多次，你应该有一个关于单次迭代所需的时间和计算资源的估计。如果训练单个模型需要一小时，那么100次迭代将需要100小时或更多。如果计算机内存已经达到极限，那么在调整过程中很可能会超过这个限制。如果这是一个问题，你需要投资额外的计算能力，并行运行实验，或者通过随机抽样减少数据集的大小。'
- en: '**Begin with a shallow search to probe for patterns**: The initial tuning process
    should be interactive and shallow. It is intended to develop your own understanding
    of what options and values are important. When probing a single hyperparameter,
    keep increasing or decreasing its setting in reasonable increments until the performance
    stops improving (or starts decreasing). Depending on the option, this may be increments
    of one, multiples of five or ten, or incrementally small fractions, such as 0.1,
    0.01, 0.001, and so on. When tuning two or more hyperparameters, it may help to
    focus on one at a time and keep the other values static. This is a more efficient
    approach than testing all possible combinations of settings, but may ultimately
    miss important combinations that would have been discovered if all combinations
    were tested.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从浅层搜索开始以寻找模式**：初始调整过程应该是交互式的和浅层的。它的目的是发展你对自己认为哪些选项和值是重要的理解。在探测单个超参数时，应合理地增加或减少其设置，直到性能停止提高（或开始下降）。根据选项，这可能是1的增量，5或10的倍数，或者增量很小的分数，如0.1、0.01、0.001等。当调整两个或更多超参数时，一次关注一个并保持其他值不变可能会有所帮助。这种方法比测试所有可能的设置组合更有效，但最终可能会错过如果测试了所有组合本可以发现的组合。'
- en: '**Narrow in on the optimal set of hyperparameter values**: Once you have a
    sense of a range of values suspected to contain the optimal settings, you can
    reduce the increments between the tested values and test a narrower range with
    greater precision or test a greater number of combinations of values. The previous
    step should have already resulted in a reasonable set of hyperparameters, so this
    step should only improve and never detract from the model’s performance; it can
    be stopped at any time.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚焦于最优的超参数值集**：一旦你对包含最优设置的值范围有了感觉，你可以减少测试值之间的增量，并使用更高的精度测试更窄的范围，或者测试更多值的组合。前一步应该已经产生了一组合理的超参数，所以这一步应该只会提高而不会降低模型的性能；它可以在任何时候停止。'
- en: '**Determine a reasonable stopping point**: Deciding when to stop the tuning
    process is easier said than done—the thrill of the hunt and the possibility of
    a slightly better model can lead to a stubborn desire to keep going! Sometimes,
    the stopping point is a project deadline when time is running out. In other cases,
    the work can only stop once the desired performance level has been reached. In
    any case, because the only way to guarantee finding the optimal hyperparameter
    values is to test an infinite number of possibilities, rather than working toward
    burnout, you will need to define the point at which performance is “good enough”
    to stop the process.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定一个合理的停止点**：决定何时停止调整过程比说起来容易——追求更好模型的冲动和可能出现的略微改进可以导致固执地继续下去！有时，停止点是一个项目截止日期，当时时间正在流逝。在其他情况下，只有在达到期望的性能水平后，工作才能停止。在任何情况下，因为保证找到最佳超参数值的唯一方法是在无限多的可能性中进行测试，而不是朝着燃尽努力，你需要定义性能“足够好”以停止过程的点。'
- en: '*Figure 14.2* illustrates the process of homing in on hyperparameter values
    for single-parameter tuning. Five potential values (1, 2, 3, 4, and 5) denoted
    by solid circles were evaluated in the initial pass, and the accuracy was highest
    when the hyperparameter was set to 3\. To check whether an even better hyperparameter
    setting might exist, eight additional values (from 2.2 to 3.8 in increments of
    0.2, denoted by vertical tick marks) were tested within the range between 2 and
    4, which led to the discovery of a higher accuracy when the hyperparameter was
    set to 3.2\. If time allows, one could test even more values in a narrower range
    around this value to possibly find an even better setting.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.2*说明了针对单参数调整聚焦超参数值的过程。在初始遍历中，对五个潜在值（1、2、3、4和5）进行了评估，当超参数设置为3时，准确率最高。为了检查是否存在更好的超参数设置，在2到4的范围内测试了八个额外的值（从2.2到3.8，以0.2的增量，由垂直刻度表示），当超参数设置为3.2时，发现了更高的准确率。如果时间允许，可以在围绕这个值更窄的范围内测试更多的值，以可能找到更好的设置。'
- en: '![](img/B17290_14_02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_14_02.png)'
- en: 'Figure 14.2: Strategies for parameter tuning home in on the optimal value by
    searching broadly and then narrowly'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：参数调整策略通过广泛搜索然后缩小搜索范围来聚焦于最佳值
- en: 'Tuning two or more hyperparameters is more complicated, because the optimal
    value of one parameter may depend on the value of the others. Constructing a visualization
    like the one depicted in *Figure 14.3* may help in understanding how to find the
    best combinations of parameters; within hot spots where certain combinations of
    values result in better model performance, one might test more values in narrower
    and narrower ranges:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 调整两个或更多超参数更为复杂，因为一个参数的最佳值可能取决于其他参数的值。构建类似于*图14.3*所示的可视化可以帮助理解如何找到最佳参数组合；在那些某些值组合导致模型性能更好的热点区域，可以在越来越窄的范围内测试更多的值：
- en: '![Chart, diagram  Description automatically generated](img/B17290_14_03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图表，图解 自动生成描述](img/B17290_14_03.png)'
- en: 'Figure 14.3: Tuning strategies become more challenging as more hyperparameters
    are added, as the model’s best performance depends on combinations of values'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：随着更多超参数的添加，调整策略变得更加具有挑战性，因为模型的最佳性能取决于值的组合
- en: This *Battleship*-style **grid search**, in which hyperparameters and combinations
    of hyperparameters are tested systematically, is not the only approach to tuning,
    although it may be the most widely used. A more intelligent approach called **Bayesian
    optimization** treats the tuning process as a learning problem that can be solved
    using modeling. This approach is included in some automated machine learning software
    but is outside the scope of this book. Instead, for the remainder of this section,
    we will focus on applying the idea of grid search to our real-world dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类似*战舰*风格的**网格搜索**，其中系统地测试超参数及其组合，虽然不是调整的唯一方法，但可能是最广泛使用的方法。一种更智能的方法称为**贝叶斯优化**，将调整过程视为一个可以通过建模解决的问题。这种方法包含在一些自动机器学习软件中，但超出了本书的范围。相反，在本节的剩余部分，我们将专注于将网格搜索的理念应用于我们的实际数据集。
- en: Example – using caret for automated tuning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 - 使用caret进行自动调整
- en: Thankfully, we can use R to conduct the iterative search through many possible
    hyperparameter values and combinations of values to find the best set. This approach
    is a relatively easy yet sometimes computationally expensive brute-force method
    of optimizing a learning algorithm’s performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用R来通过许多可能的超参数值和值的组合进行迭代搜索，以找到最佳集。这种方法是一种相对简单但有时计算成本较高的暴力方法，用于优化学习算法的性能。
- en: The `caret` package, which was used previously in *Chapter 10*, *Evaluating
    Model Performance*, provides tools to assist with this form of automated tuning.
    The core tuning functionality is provided by a `train()` function that serves
    as a standardized interface for over 200 different machine learning models for
    both classification and numeric prediction tasks. Using this function, it is possible
    to automate the search for optimal models using a choice of evaluation methods
    and metrics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章“评估模型性能”中使用的`caret`包提供了辅助工具，以帮助进行这种形式的自动化调整。核心调整功能由一个`train()`函数提供，该函数作为分类和数值预测任务中200多个不同机器学习模型的标准化接口。使用此函数，可以使用选择的评估方法和指标自动搜索最佳模型。
- en: 'Automated parameter tuning with `caret` will require you to consider three
    questions:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`caret`进行自动参数调整需要你考虑三个问题：
- en: What type of machine learning algorithm (and specific R implementation of this
    algorithm) should be trained on the data?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该在数据上训练哪种机器学习算法（以及该算法的特定R实现）？
- en: Which hyperparameters can be adjusted for this algorithm, and how extensively
    should they be tuned to find the optimal settings?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于此算法，哪些超参数可以调整，以及应该调整到何种程度以找到最佳设置？
- en: What criterion should be used to evaluate the candidate models to identify the
    best overall set of tuning values?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用什么标准来评估候选模型，以确定最佳的整体调整值集？
- en: Answering the first question involves finding a match between the machine learning
    task and one of the many models available to the `caret` package. This requires
    a general understanding of the types of machine learning models, which you may
    already have if you’ve been working through this book chronologically. It can
    also help to work through a process of elimination. Nearly half of the models
    can be eliminated depending on whether the task is classification or numeric prediction;
    others can be excluded based on the format of the training data or the need to
    avoid black box models, and so on. In any case, there’s also no reason you can’t
    create several highly tuned models and compare them across the set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 回答第一个问题涉及在机器学习任务与`caret`包提供的许多模型之一之间找到匹配。这可能需要你对机器学习模型类型的一般理解，如果你按时间顺序阅读本书，你可能已经具备这种理解。消除过程也可能有所帮助。几乎一半的模型可以根据任务是分类还是数值预测来消除；其他模型可以根据训练数据的格式或避免黑盒模型的需求等因素排除，等等。在任何情况下，也没有理由你不能创建几个高度调整的模型并在整个集合中比较它们。
- en: Addressing the second question is a matter largely dictated by the choice of
    model since each algorithm utilizes its own set of hyperparameters. The available
    tuning options for the predictive models covered in this book are listed in the
    following table. Keep in mind that although some models have additional options
    not shown, only those listed in the table are supported by `caret` for automatic
    tuning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回答第二个问题在很大程度上取决于模型的选择，因为每个算法都使用自己的一组超参数。本书中涵盖的预测模型的可调选项列在以下表中。请注意，尽管一些模型有未显示的附加选项，但只有表中列出的选项由`caret`支持用于自动调整。
- en: '| **Model** | **Learning Task** | **Method Name** | **Hyperparameters** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **学习任务** | **方法名称** | **超参数** |'
- en: '| *k-Nearest Neighbors* | *Classification* | `knn` | `k` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| *k-最近邻* | *分类* | `knn` | `k` |'
- en: '| *Naive Bayes* | *Classification* | `nb` | `fL, usekernel` |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| *朴素贝叶斯* | *分类* | `nb` | `fL, usekernel` |'
- en: '| *Decision Trees* | *Classification* | `C5.0` | `model, trials, winnow` |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| *决策树* | *分类* | `C5.0` | `model, trials, winnow` |'
- en: '| *OneR Rule Learner* | *Classification* | `OneR` | `None` |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| *OneR规则学习器* | *分类* | `OneR` | `None` |'
- en: '| *RIPPER Rule Learner* | *Classification* | `JRip` | `NumOpt` |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| *RIPPER规则学习器* | *分类* | `JRip` | `NumOpt` |'
- en: '| *Linear Regression* | *Regression* | `lm` | `None` |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| *线性回归* | *回归* | `lm` | `None` |'
- en: '| *Regression Trees* | *Regression* | `rpart` | `cp` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| *回归树* | *回归* | `rpart` | `cp` |'
- en: '| *Model Trees* | *Regression* | `M5` | `pruned, smoothed, rules` |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| *模型树* | *回归* | `M5` | `pruned, smoothed, rules` |'
- en: '| *Neural Networks* | *Dual Use* | `nnet` | `size, decay` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| *神经网络* | *双重用途* | `nnet` | `size, decay` |'
- en: '| *Support Vector Machines (Linear Kernel)* | *Dual Use* | `svmLinear` | `C`
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| *支持向量机（线性核）* | *双重用途* | `svmLinear` | `C` |'
- en: '| *Support Vector Machines (Radial Basis Kernel)* | *Dual Use* | `svmRadial`
    | `C, sigma` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| *支持向量机（径向基核）* | *双重用途* | `svmRadial` | `C, sigma` |'
- en: '| *Random Forests* | *Dual Use* | `rf` | `mtry` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| *随机森林* | *双重用途* | `rf` | `mtry` |'
- en: '| *Gradient Boosting Machines (GBM)* | *Dual Use* | `gbm` | `n.trees, interaction.depth,
    shrinkage, n.minobsinnode` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| *梯度提升机（GBM）* | *双重用途* | `gbm` | `n.trees, interaction.depth, shrinkage, n.minobsinnode`
    |'
- en: '| *XGBoost (XGB)* | *Dual Use* | `xgboost` | `eta, max_depth, colsample_bytree,
    subsample, nrounds, gamma, min_child_weight` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| *XGBoost (XGB)* | *双重用途* | `xgboost` | `eta, max_depth, colsample_bytree,
    subsample, nrounds, gamma, min_child_weight` |'
- en: For a complete list of the models and corresponding tuning options covered by
    `caret`, refer to the table provided by package author Max Kuhn at [http://topepo.github.io/caret/available-models.html](http://topepo.github.io/caret/available-models.html).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`caret`支持的模型和相应调优选项的完整列表，请参阅包作者Max Kuhn提供的表格，链接为[http://topepo.github.io/caret/available-models.html](http://topepo.github.io/caret/available-models.html)。
- en: 'If you ever forget the tuning parameters for a particular model, the `modelLookup()`
    function can be used to find them. Simply supply the method name as illustrated
    for the C5.0 model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忘记了特定模型的调优参数，可以使用`modelLookup()`函数来查找它们。只需提供如C5.0模型所示的方法名：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The goal of automatic tuning is to iterate over the set of candidate models
    comprising the search grid of potential parameter combinations. As it is impractical
    to search every conceivable combination, only a subset of possibilities is used
    to construct the grid. By default, `caret` searches, at most, three values for
    each of the model’s *p* hyperparameters, which means that, at most, *3*^p candidate
    models will be tested. For example, by default, the automatic tuning of k-nearest
    neighbors will compare *3*¹ *= 3* candidate models with `k=5`, `k=7`, and `k=9`.
    Similarly, tuning a decision tree will result in a comparison of up to 27 different
    candidate models, comprising the grid of *3*³ *= 27* combinations of `model`,
    `trials`, and `winnow` settings. In practice, however, only 12 models are tested.
    This is because `model` and `winnow` can only take two values (`tree` versus `rules`
    and `TRUE` versus `FALSE`, respectively), which makes the grid size *3*2*2 = 12*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自动调优的目标是在由潜在参数组合的搜索网格组成的候选模型集合上进行迭代。由于搜索所有可能的组合是不切实际的，因此只使用可能性的子集来构建网格。默认情况下，`caret`为每个模型的*p*个超参数最多搜索三个值，这意味着最多将测试*3*^p个候选模型。例如，默认情况下，k-近邻自动调优将比较*3*¹
    *= 3*个候选模型，其中`k=5`、`k=7`和`k=9`。同样，调整决策树将导致最多27个不同候选模型的比较，包括`model`、`trials`和`winnow`设置的*3*³
    *= 27*组合网格。然而，在实践中，只测试了12个模型。这是因为`model`和`winnow`只能取两个值（`tree`与`rules`以及`TRUE`与`FALSE`），这使得网格大小为*3*²*²
    = 12*。
- en: Since the default search grid may not be ideal for your learning problem, `caret`
    allows you to provide a custom search grid defined by a simple command, which
    we will cover later.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于默认的搜索网格可能不适合你的学习问题，`caret`允许你通过简单的命令提供自定义搜索网格，我们将在后面进行介绍。
- en: The third and final step in automatic model tuning involves identifying the
    best model among the candidates. This uses the methods discussed in *Chapter 10*,
    *Evaluating Model Performance*, including the choice of resampling strategy for
    creating training and test datasets, and the use of model performance statistics
    to measure the predictive accuracy. All the resampling strategies and many of
    the performance statistics we’ve learned are supported by `caret`. These include
    statistics such as accuracy and kappa for classifiers and R-squared or **root-mean-square
    error** (**RMSE**) for numeric models. Cost-sensitive measures like sensitivity,
    specificity, and AUC can also be used if desired.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自动模型调优的第三步和最后一步是确定候选模型中最好的模型。这使用了在第10章“评估模型性能”中讨论的方法，包括选择重采样策略来创建训练和测试数据集，以及使用模型性能统计来衡量预测准确性。我们学到的所有重采样策略和许多性能统计都由`caret`支持。这些包括如分类器的准确率和kappa，以及数值模型的R-squared或**均方根误差**（**RMSE**）。如果需要，也可以使用成本敏感度指标，如灵敏度、特异性和AUC。
- en: By default, `caret` will select the candidate model with the best value of the
    desired performance measure. Because this practice sometimes results in the selection
    of models that achieve minor performance improvements via large increases in model
    complexity, alternative model selection functions are provided. These alternatives
    allow us to choose simpler models that are still reasonably close to the best
    model, which may be desirable in the case where a bit of predictive performance
    is worth sacrificing for an improvement in computational efficiency.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`caret` 将选择具有所需性能度量最佳值的候选模型。由于这种做法有时会导致选择通过增加模型复杂度来实现微小性能改进的模型，因此提供了替代模型选择函数。这些替代方案允许我们选择更简单的模型，这些模型仍然与最佳模型相当接近，这在需要牺牲一点预测性能以换取计算效率提高的情况下可能是可取的。
- en: Given the wide variety of options in the `caret` tuning process, it is helpful
    that many of the function’s defaults are reasonable. For instance, without specifying
    the settings manually, `caret` uses prediction accuracy or RMSE on a bootstrap
    sample to choose the best performer for classification and numeric prediction
    models, respectively. Similarly, it will automatically define a limited grid to
    search. These defaults allow us to start with a simple tuning process and learn
    to tweak the `train()` function to design a wide variety of experiments of our
    choosing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `caret` 调优过程中有各种各样的选项，许多函数的默认设置是合理的，这很有帮助。例如，不手动指定设置，`caret` 会使用在自助样本上的预测准确度或
    RMSE 来选择分类和数值预测模型的最佳表现者。同样，它将自动定义一个有限的网格进行搜索。这些默认设置使我们能够从简单的调优过程开始，并学习如何调整 `train()`
    函数来设计我们选择的广泛实验。
- en: Creating a simple tuned model
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个简单的调优模型
- en: 'To illustrate the process of tuning a model, let’s begin by observing what
    happens when we attempt to tune the credit scoring model using the `caret` package’s
    default settings. The simplest way to tune a learner requires only that you specify
    a model type via the `method` parameter. Since we used C5.0 decision trees previously
    with the credit model, we’ll continue our work by optimizing this learner. The
    basic `train()` command for tuning a C5.0 decision tree using the default settings
    is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明调优模型的过程，让我们首先观察当我们尝试使用 `caret` 包的默认设置来调优信用评分模型时会发生什么。调整学习者的最简单方法只需要你通过 `method`
    参数指定一个模型类型。由于我们之前已经使用 C5.0 决策树与信用模型一起使用，我们将通过优化这个学习者继续我们的工作。使用默认设置调优 C5.0 决策树的基本
    `train()` 命令如下：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, the `set.seed()` function is used to initialize R’s random number generator
    to a set starting position. You may recall that we used this function in several
    prior chapters. By setting the `seed` parameter (in this case, to the arbitrary
    number 300), the random numbers will follow a predefined sequence. This allows
    simulations that use random sampling to be repeated with identical results—a very
    helpful feature if you are sharing code or attempting to replicate a prior result.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用 `set.seed()` 函数初始化 R 的随机数生成器到一个固定的起始位置。你可能还记得我们在几个先前的章节中使用了这个函数。通过设置 `seed`
    参数（在这种情况下，为任意数 300），随机数将遵循预定义的序列。这允许使用随机抽样的模拟能够重复产生相同的结果——如果你正在共享代码或尝试复制先前的结果，这是一个非常有帮助的特性。
- en: Next, we define a tree as `default ~ .` using the R formula interface. This
    models a loan default status (`yes` or `no`) using all the other features in the
    `credit` dataset. The parameter `method = "C5.0"` tells the function to use the
    C5.0 decision tree algorithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 R 公式接口定义一棵树为 `default ~ .`。这使用 `credit` 数据集中的所有其他特征来模拟贷款违约状态（`是`或`否`）。参数
    `method = "C5.0"` 告诉函数使用 C5.0 决策树算法。
- en: After you’ve entered the preceding command, depending upon your computer’s capabilities,
    there may be a significant delay as the tuning process occurs. Even though this
    is a small dataset, a substantial amount of calculation must occur. R must repeatedly
    generate random bootstrap samples of data, build decision trees, compute performance
    statistics, and evaluate the result. Because there are 12 candidate models with
    varying hyperparameter values to be evaluated, and 25 bootstrap samples per candidate
    model to compute an average performance measure, there are *25*12 = 300* decision
    tree models being built using C5.0—and this doesn’t even count the additional
    decision trees being built when the boosting trials are set!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在您输入上述命令后，根据您计算机的能力，在调整过程中可能会有显著的延迟。尽管这是一个小数据集，但必须进行大量的计算。R 必须反复生成数据的随机自助样本，构建决策树，计算性能统计信息，并评估结果。因为有
    12 个候选模型具有不同的超参数值需要评估，每个候选模型有 25 个自助样本来计算平均性能指标，所以使用 C5.0 构建了 300 个决策树模型——而且这还不包括在设置提升试验时构建的额外决策树！
- en: 'A list named `m` stores the result of the `train()` experiment, and the command
    `str(m)` will display the associated results, but the contents can be substantial.
    Instead, simply type the name of the object for a condensed summary of the results.
    For instance, typing `m` yields the following output (note that numbered labels
    have been added for clarity):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 命名为 `m` 的列表存储了 `train()` 实验的结果，使用 `str(m)` 命令将显示相关结果，但内容可能相当多。相反，只需输入对象名称即可获得结果的简明摘要。例如，输入
    `m` 将产生以下输出（注意，为了清晰起见，已添加编号标签）：
- en: '![](img/B17290_14_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_14_04.png)'
- en: 'Figure 14.4: The results of a caret experiment are separated into four components,
    as annotated in this figure'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：caret 实验的结果分为四个部分，如图中所示
- en: 'The labels highlight four main components in the output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 标签突出了输出中的四个主要组成部分：
- en: '**A brief description of the input dataset**: If you are familiar with your
    data and have applied the `train()` function correctly, this information should
    not be surprising.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入数据集的简要描述**：如果您熟悉您的数据并且正确应用了 `train()` 函数，这些信息不应令人惊讶。'
- en: '**A report of the preprocessing and resampling methods applied**: Here we see
    that 25 bootstrap samples, each including 1,000 examples, were used to train the
    models.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用的预处理和重采样方法的报告**：在这里，我们可以看到使用了 25 个包含 1,000 个示例的自助样本来训练模型。'
- en: '**A list of the candidate models evaluated**: In this section, we can confirm
    that 12 different models were tested, based on the combinations of three C5.0
    hyperparameters: `model`, `trials`, and `winnow`. The average accuracy and kappa
    statistics for each candidate model are also shown.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估的候选模型列表**：在本节中，我们可以确认基于三个 C5.0 超参数（`model`、`trials` 和 `winnow`）的组合测试了 12
    个不同的模型。每个候选模型的平均准确率和 kappa 统计量也显示出来。'
- en: '**The choice of best model**: As the footnote describes, the model with the
    best accuracy (in other words, “largest”) was selected. This was the C5.0 model
    that used a decision tree with the settings `winnow = FALSE` and `trials = 20`.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最佳模型的选择**：正如脚注所述，选择了具有最佳准确率（换句话说，“最大”）的模型。这是使用设置 `winnow = FALSE` 和 `trials
    = 20` 的决策树的 C5.0 模型。'
- en: 'After identifying the best model, the `train()` function uses the tuned hyperparameters
    to build a model on the full input dataset, which is stored in `m` as `m$finalModel`.
    In most cases, you will not need to work directly with the `finalModel` sub-object.
    Instead, simply use the `predict()` function with the `m` object as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定最佳模型后，`train()` 函数使用调整后的超参数在完整输入数据集上构建一个模型，该模型存储在 `m` 中作为 `m$finalModel`。在大多数情况下，您不需要直接与
    `finalModel` 子对象一起工作。相反，只需使用 `predict()` 函数并带上 `m` 对象，如下所示：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The resulting vector of predictions works as expected, allowing us to create
    a confusion matrix that compares the predicted and actual values:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果向量按预期工作，使我们能够创建一个混淆矩阵，该矩阵比较了预测值和实际值：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Of the 1,000 examples used for training the final model, only two were misclassified,
    for an accuracy of 99.8 percent. However, it is very important to note that since
    the model was built on both the training and test data, this accuracy is optimistic
    and thus should not be viewed as indicative of performance on unseen data. The
    bootstrap accuracy estimate of 72.996 percent, which can be found in the last
    row of section three of the `train()`output in *Figure 14.4*, is a far more realistic
    estimate of future accuracy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于训练最终模型的1,000个示例中，只有两个被错误分类，准确率达到99.8%。然而，非常重要的一点是，由于模型是在训练数据和测试数据上构建的，因此这种准确率是乐观的，因此不应将其视为对未见数据的性能指标。在*图14.4*的`train()`输出的第三部分的最后一行中可以找到的72.996%的bootstrap准确率估计是一个更现实的未来准确率估计。
- en: In addition to automatic hyperparameter tuning, using the `caret` package’s
    `train()` and `predict()` functions also offers a pair of benefits beyond the
    functions found in the stock packages.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动超参数调优之外，使用`caret`包的`train()`和`predict()`函数还提供了一对超越标准包中函数的好处。
- en: First, any data preparation steps applied by the `train()` function will be
    similarly applied to the data used for generating predictions. This includes transformations
    like centering and scaling, as well as the imputation of missing values. Allowing
    `caret` to handle the data preparation will ensure that the steps that contributed
    to the best model’s performance will remain in place when the model is deployed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`train()`函数应用的所有数据准备步骤将同样应用于用于生成预测的数据。这包括中心化和缩放等转换，以及缺失值的填充。允许`caret`处理数据准备将确保在模型部署时，有助于最佳模型性能的步骤仍然保持不变。
- en: 'Second, the `predict()` function provides a standardized interface for obtaining
    predicted class values and predicted class probabilities, even for model types
    that ordinarily would require additional steps to obtain this information. For
    a classification model, the predicted classes are provided by default:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，`predict()`函数为获取预测类别值和预测类别概率提供了一个标准化的接口，即使对于通常需要额外步骤才能获取这些信息的模型类型也是如此。对于分类模型，默认提供预测类别：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To obtain the estimated probabilities for each class, use the `type = "prob"`
    parameter:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取每个类别的估计概率，请使用`type = "prob"`参数：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Even in cases where the underlying model refers to the prediction probabilities
    using a different string (for example, `"raw"` for a `naiveBayes` model), the
    `predict()` function will translate `type = "prob"` to the appropriate parameter
    setting automatically.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在底层模型使用不同的字符串（例如，对于`naiveBayes`模型使用`"raw"`）来引用预测概率的情况下，`predict()`函数也会自动将`type
    = "prob"`转换为适当的参数设置。
- en: Customizing the tuning process
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定制调优过程
- en: 'The decision tree we created previously demonstrates the `caret` package’s
    ability to produce an optimized model with minimal intervention. The default settings
    allow optimized models to be created easily. However, it is also possible to change
    the default settings as desired, which may assist with unlocking the upper echelon
    of performance. Before the tuning process begins, it’s worth answering a series
    of questions that will help guide the setup of the `caret` experiment:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的决策树展示了`caret`包产生优化模型的最小干预能力。默认设置允许轻松创建优化模型。然而，也可以根据需要更改默认设置，这可能会帮助解锁性能的上层水平。在调优过程开始之前，回答一系列问题将有助于指导`caret`实验的设置：
- en: How long does it take for one iteration? In other words, how long does it take
    to train a single instance of the model being tuned?
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个迭代需要多长时间？换句话说，训练正在调优的模型的一个实例需要多长时间？
- en: Given the time it takes to train a single instance, how long will it take to
    perform the model evaluation using the chosen resampling method? For example,
    10-fold CV will require 10 times as much time as training a single model.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到训练单个实例所需的时间，使用所选的重新采样方法进行模型评估需要多长时间？例如，10折交叉验证将比训练单个模型多花费10倍的时间。
- en: How much time are you willing to spend on tuning? Based on this number, one
    can determine the total number of hyperparameter values that can be tested. For
    instance, if it takes one minute to evaluate a model using 10-fold CV, then 60
    hyperparameter settings can be tested per hour.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你愿意在调优上花多少时间？根据这个数字，可以确定可以测试的超参数值的总数。例如，如果使用10折交叉验证评估模型需要一分钟，那么每小时可以测试60个超参数设置。
- en: Using time as the key limiting factor will help put bounds on the tuning process
    and prevent you from chasing better and better performance endlessly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时间作为关键限制因素将有助于对调优过程进行限制，并防止你无休止地追求更好的性能。
- en: Once you’ve decided how much time to spend on the trials, it is easy to customize
    the process to your liking. To illustrate this flexibility, let’s modify our work
    on the credit decision tree to mirror the process we used in *Chapter 10*, *Evaluating
    Model Performance*. In that chapter, we estimated the kappa statistic using 10-fold
    CV. We’ll do the same here, using kappa to tune the boosting trials for the C5.0
    decision tree algorithm and find the optimal setting for our data. Note that decision
    tree boosting was first covered in *Chapter 5*, *Divide and Conquer – Classification
    Using Decision Trees and Rules*, and will also be covered in greater detail later
    in this chapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定了在试验上花费多少时间，就可以很容易地根据你的喜好定制这个过程。为了说明这种灵活性，让我们修改我们在 *第 10 章*，*评估模型性能* 中使用的信用决策树的工作，以反映我们在该章中使用的过程。在那个章节中，我们使用
    10 折交叉验证来估计卡方统计量。我们在这里也将这样做，使用卡方来调整 C5.0 决策树算法的增强试验，并找到我们数据的最佳设置。请注意，决策树增强首先在第
    5 章，*分而治之 – 使用决策树和规则进行分类* 中介绍，也将在本章的后面部分进行更详细的介绍。
- en: 'The `trainControl()` function is used to create a set of configuration options
    known as a **control object**. This object guides the `train()` function and allows
    for the selection of model evaluation criteria such as the resampling strategy
    and the measure used for choosing the best model. Although this function can be
    used to modify nearly every aspect of a `caret` tuning experiment, we’ll focus
    on two important parameters: `method` and `selectionFunction`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainControl()` 函数用于创建一组称为 **控制对象** 的配置选项。此对象指导 `train()` 函数，并允许选择模型评估标准，如重抽样策略和用于选择最佳模型的度量。尽管此函数可以用来修改
    `caret` 调优实验的几乎所有方面，但我们将关注两个重要参数：`method` 和 `selectionFunction`。'
- en: If you’re eager for more details about the control object, you can use the `?trainControl`
    command for a list of all the parameters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你渴望了解更多关于控制对象的信息，你可以使用 `?trainControl` 命令来查看所有参数的列表。
- en: When using the `trainControl()` function, the `method` parameter sets the resampling
    method, such as holdout sampling or k-fold CV. The following table lists the possible
    `method` values, as well as any additional parameters for adjusting the sample
    size and the number of iterations. Although the default options for these resampling
    methods follow popular conventions, you may choose to adjust these depending on
    the size of your dataset and the complexity of your model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `trainControl()` 函数时，`method` 参数设置重抽样方法，例如保留样本法或 k 折交叉验证。下表列出了可能的 `method`
    值，以及调整样本大小和迭代次数的任何附加参数。尽管这些重抽样方法的默认选项遵循了流行的惯例，但你可能需要根据你的数据集大小和模型复杂性进行调整。
- en: '| **Resampling method** | **Method name** | **Additional options and default
    values** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **重抽样方法** | **方法名称** | **附加选项和默认值** |'
- en: '| *Holdout sampling* | `LGOCV` | `p = 0.75` (training data proportion) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| *保留样本法* | `LGOCV` | `p = 0.75` (训练数据比例) |'
- en: '| *k-fold CV* | `cv` | `number = 10` (number of folds) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| *k 折交叉验证* | `cv` | `number = 10` (折数) |'
- en: '| *Repeated k-fold CV* | `repeatedcv` | `number = 10` (number of folds)`repeats
    = 10` (number of iterations) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| *重复 k 折交叉验证* | `repeatedcv` | `number = 10` (折数)`repeats = 10` (迭代次数) |'
- en: '| *Bootstrap sampling* | `boot` | `number = 25` (resampling iterations) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| *重抽样法* | `boot` | `number = 25` (重抽样迭代次数) |'
- en: '| *0.632 bootstrap* | `boot632` | `number = 25` (resampling iterations) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| *0.632 重抽样* | `boot632` | `number = 25` (重抽样迭代次数) |'
- en: '| *Leave-one-out CV* | `LOOCV` | *None* |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| *留一法交叉验证* | `LOOCV` | *无* |'
- en: The `selectionFunction` parameter is used to specify the function that will
    choose the optimal model among the candidates. Three such functions are included.
    The `best` function simply chooses the candidate with the best value on the specified
    performance measure. This is used by default. The other two functions are used
    to choose the most parsimonious, or simplest, model that is within a certain threshold
    of the best model’s performance. The `oneSE` function chooses the simplest candidate
    within one standard error of the best performance, and `tolerance` uses the simplest
    candidate within a user-specified percentage.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`selectionFunction` 参数用于指定将选择候选模型中最佳模型的功能。包含三个这样的功能。`best` 函数简单地选择在指定性能度量上具有最佳值的候选者。这是默认使用的。其他两个函数用于选择最简约的，或最简单的模型，该模型在最佳模型性能的一定阈值内。`oneSE`
    函数选择在最佳性能的一个标准误差内的最简单候选者，而 `tolerance` 使用用户指定的百分比内的最简单候选者。'
- en: Some subjectivity is involved with the `caret` package’s ranking of models by
    simplicity. For information on how models are ranked, see the help page for the
    selection functions by typing `?best` at the R command prompt.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `caret` 包按简单性对模型进行排名时，涉及一些主观性。有关模型排名的信息，请参阅选择函数的帮助页面，在 R 命令提示符中键入 `?best`。
- en: 'To create a control object named `ctrl` that uses 10-fold CV and the `oneSE`
    selection function, use the following command, noting that `number = 10` is included
    only for clarity; since this is the default value for `method = "cv"`, it could
    have been omitted:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个名为 `ctrl` 的控制对象，该对象使用10倍交叉验证（CV）和 `oneSE` 选择函数，请使用以下命令，注意 `number = 10`
    仅用于清晰起见；由于这是 `method = "cv"` 的默认值，因此可以省略：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ll use the result of this function shortly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快使用这个函数的结果。
- en: In the meantime, the next step in setting up our experiment is to create the
    search grid for hyperparameter tuning. The grid must include a column named for
    each hyperparameter in the desired model, regardless of whether it will be tuned.
    It must also include a row for each desired combination of values to test. Since
    we are using a C5.0 decision tree, this means we’ll need columns named `model`,
    `trials`, and `winnow`, corresponding to the three options that can be tuned.
    For other machine learning models, refer to the table presented earlier in this
    chapter or use the `modelLookup()` function to find the hyperparameters as described
    previously.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，设置实验的下一步是创建超参数调整的搜索网格。网格必须包括一个列名，对应于所需模型中的每个超参数，无论它是否将被调整。它还必须包括一个行名，对应于要测试的每个所需值组合。由于我们使用的是
    C5.0 决策树，这意味着我们需要名为 `model`、`trials` 和 `winnow` 的列，对应于可以调整的三个选项。对于其他机器学习模型，请参阅本章前面提供的表格或使用
    `modelLookup()` 函数查找超参数，如之前所述。
- en: Rather than filling the grid data frame cell by cell—a tedious task if there
    are many possible combinations of values—we can use the `expand.grid()` function,
    which creates data frames from the combinations of all values supplied. For example,
    suppose we would like to hold constant `model = "tree"` and `winnow = FALSE` while
    searching eight different values of `trials`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是逐个填充网格数据帧的单元格——如果有许多可能的值组合，这将是一项繁琐的任务——我们可以使用 `expand.grid()` 函数，该函数从提供的所有值的组合中创建数据帧。例如，假设我们希望在搜索时保持
    `model = "tree"` 和 `winnow = FALSE` 不变，同时搜索 `trials` 的八个不同值。
- en: 'This can be created as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式创建：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting `grid` data frame contains *1*8*1 = 8* rows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 `grid` 数据帧包含 *1*8*1 = 8* 行：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `train()` function will build a candidate model for evaluation using each
    `grid` row’s combination of model parameters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()` 函数将使用每个 `grid` 行的模型参数组合构建一个候选模型以进行评估。'
- en: 'Given the search grid and the control object created previously, we are ready
    to run a thoroughly customized `train()` experiment. As before, we’ll set the
    random seed to the arbitrary number `300` in order to ensure repeatable results.
    But this time, we’ll pass our control object and tuning grid while adding a parameter
    `metric = "Kappa"`, indicating the statistic to be used by the model evaluation
    function—in this case, `"oneSE"`. The full set of commands is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定搜索网格和之前创建的控制对象后，我们准备运行一个彻底定制的 `train()` 实验。和之前一样，我们将随机种子设置为任意数字 `300` 以确保可重复的结果。但这次，我们将传递我们的控制对象和调整网格，同时添加参数
    `metric = "Kappa"`，指示模型评估函数将使用的统计量——在这种情况下，`"oneSE"`。完整的命令集如下：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in an object that we can view by typing its name:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个我们可以通过键入其名称来查看的对象：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Although the output is similar to the automatically tuned model, there are a
    few notable differences. Because 10-fold CV was used, the sample size to build
    each candidate model was reduced to 900 rather than the 1,000 used in the bootstrap.
    Furthermore, eight candidate models were tested rather than the 12 in the prior
    experiment. Lastly, because `model` and `winnow` were held constant, their values
    are no longer shown in the results; instead, they are listed as a footnote.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输出与自动调整的模型相似，但有一些显著的不同之处。因为使用了10折交叉验证，构建每个候选模型的样本量减少到900，而不是之前自举中使用的1,000。此外，测试了8个候选模型，而不是之前实验中的12个。最后，由于`model`和`winnow`保持不变，它们的结果中不再显示其值；相反，它们被列在脚注中。
- en: The best model here differs quite significantly from the prior experiment. Before,
    the best model used `trials = 20`, whereas here, it used `trials = 1`. This change
    is because we used the `oneSE` function rather than the `best` function to select
    the optimal model. Even though the model with `trials = 35` obtained the best
    kappa, the single-trial model offers reasonably close performance with a much
    simpler algorithm.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最好的模型与之前的实验差异很大。之前，最好的模型使用`trials = 20`，而在这里，它使用`trials = 1`。这种变化是因为我们使用了`oneSE`函数而不是`best`函数来选择最佳模型。尽管`trials
    = 35`的模型获得了最好的kappa值，但单次试验的模型提供了一个简单得多的算法，并且性能相当接近。
- en: 'Due to the large number of configuration parameters, `caret` can seem overwhelming
    at first. Don’t let this deter you—there is no easier way to test the performance
    of models using 10-fold CV. Instead, think of the experiment as defined by two
    parts: a `trainControl()` object that dictates the testing criteria, and a tuning
    grid that determines what model parameters to evaluate. Supply these to the `train()`
    function and with a bit of computing time, your experiment will be complete!'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于配置参数数量众多，`caret`在最初可能会让人感到不知所措。不要让这一点阻碍你——没有比使用10折交叉验证测试模型性能更简单的方法了。相反，将实验视为由两部分组成：一个`trainControl()`对象，它规定了测试标准；以及一个确定要评估的模型参数的调整网格。将这些提供给`train()`函数，并投入一点计算时间，你的实验就会完成！
- en: Of course, tuning is just one possibility for building better learners. In the
    next section, you will discover that in addition to buffing up a single learner
    to make it stronger, it is also possible to combine several weaker models to form
    a more powerful team.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，调整只是构建更好学习者的可能性之一。在下一节中，你将发现，除了增强单个学习者使其更强之外，还可以将几个较弱的模型结合起来形成一个更强大的团队。
- en: Improving model performance with ensembles
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成方法提高模型性能
- en: Just as the best sports teams have players with complementary rather than overlapping
    skillsets, some of the best machine learning algorithms utilize teams of complementary
    models. Since a model brings a unique bias to a learning task, it may readily
    learn one subset of examples but have trouble with another. Therefore, by intelligently
    using the talents of several diverse team members, it is possible to create a
    strong team of multiple weak learners.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如最好的运动队拥有互补而不是重叠技能的球员一样，一些最好的机器学习算法利用了互补模型的团队。由于模型为学习任务带来独特的偏差，它可能很容易学会一组示例，但可能在另一组上遇到困难。因此，通过智能地利用几个不同团队成员的才能，可以创建一个由多个弱学习者组成的强大团队。
- en: This technique of combining and managing the predictions of multiple models
    falls into a wider set of **meta-learning methods**, which are techniques that
    involve learning how to learn. This includes anything from simple algorithms that
    gradually improve performance by iterating over design decisions—for instance,
    the automated parameter tuning used earlier in this chapter—to highly complex
    algorithms that use concepts borrowed from evolutionary biology and genetics for
    self-modifying and adapting to learning tasks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结合和管理多个模型预测的技术属于更广泛的**元学习方法**范畴，这些方法是涉及学习如何学习的技术。这包括从简单的算法开始，通过迭代设计决策逐渐提高性能——例如，本章早期使用的自动化参数调整——到高度复杂的算法，这些算法借鉴了进化生物学和遗传学的概念来实现自我修改和适应学习任务。
- en: Suppose you were a contestant on a television trivia show that allowed you to
    choose a panel of five friends to assist you with answering the final question
    for the million-dollar prize. Most people would try to stack the panel with a
    diverse set of subject matter experts. A panel containing professors of literature,
    science, history, and art, along with a current pop-culture expert, would be safely
    well-rounded. Given their breadth of knowledge, it would be unlikely that a question
    would stump the group.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一个电视智力游戏节目的参赛者，这个节目允许你选择一个由五位朋友组成的团队来帮助你回答价值百万美元的最终问题。大多数人会尝试让这个团队包含各种学科领域的专家。一个包含文学、科学、历史和艺术教授，以及一个当前流行文化专家的团队将是一个安全而全面的团队。鉴于他们的知识广度，一个难以回答的问题不太可能难倒这个团队。
- en: The meta-learning approach that utilizes a similar principle of creating a varied
    team of experts is known as an **ensemble**. For the remainder of this chapter,
    we’ll focus on meta-learning only as it pertains to ensembling—the task of modeling
    a relationship between the predictions of several models and the desired outcome.
    The teamwork-based methods covered here are quite powerful and are used often
    to build more effective classifiers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 利用创建一个多样化的专家团队类似原则的元学习方法是称为**集成**。在本章的剩余部分，我们将只关注与集成相关的元学习——即建模多个模型预测与期望结果之间关系的工作。这里介绍基于团队合作的方法非常强大，并且经常被用来构建更有效的分类器。
- en: Understanding ensemble learning
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解集成学习
- en: 'All ensemble methods are based on the idea that by combining multiple weaker
    learners, a stronger learner is created. Ensembles contain two or more machine
    learning models, which can be of the same type, such as several decision trees,
    or of different types, such as a decision tree and a neural network. Though there
    are myriad ways to construct an ensemble, they tend to fall into several general
    categories, which can be distinguished, in large part, by the answers to two questions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集成方法都是基于这样一个想法：通过结合多个较弱的学习者，可以创建一个更强的学习者。集成包含两个或多个机器学习模型，这些模型可以是同一类型的，例如多个决策树，也可以是不同类型的，例如决策树和神经网络。尽管构建集成有许多方法，但它们往往可以分为几个一般类别，这些类别在很大程度上可以通过回答两个问题来区分：
- en: How are the ensemble’s models chosen and trained?
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型的模型是如何选择和训练的？
- en: How are the models’ predictions combined to make a single final prediction?
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的预测是如何组合起来以做出一个单一最终预测的？
- en: 'When answering these questions, it can be helpful to imagine the ensemble in
    terms of the following process diagram, which encompasses nearly all ensembling
    approaches:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当回答这些问题时，想象集成如下流程图可能会有所帮助，它几乎涵盖了所有集成方法：
- en: '![Diagram  Description automatically generated](img/B17290_14_05.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 描述自动生成](img/B17290_14_05.png)'
- en: 'Figure 14.5: Ensembles combine multiple weaker models into a single stronger
    model'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：集成将多个较弱的模型组合成一个更强的模型
- en: In this design pattern, input training data is used to build several models.
    The **allocation function** dictates how much and what subsets of the training
    data each model receives. Do they each receive the full training dataset or merely
    a sample? Do they each receive every feature or a subset of features? The decisions
    made here will shape the training of the weaker learners that comprise the stronger
    ensemble.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计模式中，输入训练数据被用来构建多个模型。**分配函数**决定了每个模型接收多少以及哪些训练数据子集。它们是否都接收完整的训练数据集，或者只是样本？它们是否都接收每个特征或特征子集？在这里做出的决定将塑造构成更强集成模型的较弱学习者的训练。
- en: Just as you’d want a variety of experts to advise your appearance on a television
    trivia game show, ensembles depend on a **diverse** set of classifiers, which
    means that they have uncorrelated classifications but still perform better than
    random chance. In other words, each classifier must be making an independent prediction,
    but each must also be doing more than merely guessing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你在电视智力游戏节目中希望有一群各种专家来为你提供关于外表的建议一样，集成模型依赖于一组**多样化的**分类器，这意味着它们具有不相关的分类，但仍然比随机机会表现得更好。换句话说，每个分类器都必须做出独立的预测，但每个分类器也必须做的不只是猜测。
- en: Diversity can be added to the ensemble by including a variety of machine learning
    techniques, such as an ensemble that groups a decision tree, a neural network,
    and a logistic regression model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过包括各种机器学习技术，如将决策树、神经网络和逻辑回归模型组合在一起的集成，可以增加集成的多样性。
- en: 'Alternatively, the allocation function itself can also be a source of diversity
    by acting as a **data manipulator** and artificially varying the input data to
    bias the resulting learners, even if they use the same learning algorithm. As
    we will see in practice later, the allocation and data manipulation processes
    may be automated or included as part of the ensembling algorithm itself, or they
    may be performed by hand as part of the data engineering and model-building process.
    Overall, modes of increasing the ensemble’s diversity generally fall into five
    categories:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，分配函数本身也可以通过充当**数据操作器**并人为地改变输入数据来偏置结果学习者，即使它们使用相同的学习算法，从而成为多样性的来源。正如我们稍后将在实践中看到的那样，分配和数据操作过程可能是自动化的，或者作为集成算法本身的一部分，或者它们可能是作为数据工程和模型构建过程的一部分手动执行的。总的来说，增加集成多样性的模式通常分为五类：
- en: Using assorted base learning algorithms
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种基础学习算法
- en: Manipulating the training sample by taking different samples at random, often
    by using bootstrapping
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过随机抽取不同的样本来操纵训练样本，通常通过使用自助采样
- en: Manipulating a single learning algorithm by using different hyperparameter settings
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用不同的超参数设置来操纵单个学习算法
- en: Changing how the target feature is represented, such as representing an outcome
    as binary, categorical, or numeric
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变目标特征的表示方式，例如将结果表示为二元、分类或数值
- en: Partitioning the training data into subgroups that represent different patterns
    to be learned; for instance, one might stratify the examples by key features,
    and let models in the ensemble become experts on different subsets of the training
    data
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练数据划分为代表不同学习模式的子组；例如，可以通过关键特征对示例进行分层，并让集成中的模型成为训练数据不同子集的专家
- en: For instance, in an ensemble of decision trees, the allocation function might
    use bootstrap sampling to construct unique training datasets for each tree, or
    it may pass each one a different subset of features. On the other hand, if the
    ensemble already includes a diverse set of algorithms—such as a neural network,
    a decision tree, and a k-NN classifier—then the allocation function might pass
    the training data on to each algorithm relatively unchanged.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个决策树集成中，分配函数可能使用自助采样为每棵树构建独特的训练数据集，或者它可能为每个算法传递不同的特征子集。另一方面，如果集成已经包含了一个多样化的算法集——例如神经网络、决策树和k-NN分类器——那么分配函数可能将训练数据相对不变地传递给每个算法。
- en: After the ensemble’s models are trained, they can be used to generate predictions
    on future data, but this set of multiple predictions must be reconciled somehow
    to generate a single final prediction. The **combination function** is the step
    in the ensembling process that takes each of these predictions and combines them
    into a single authoritative prediction for the set. Of course, because some of
    the models may disagree on the predicted value, the function must somehow blend
    or unify the information from the learners. The combination function is also known
    as a **composer** due to its work synthesizing the final prediction.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成模型训练完成后，它们可以用来对未来数据进行预测，但必须以某种方式协调这组多个预测以生成一个单一的最终预测。**组合函数**是集成过程中的一个步骤，它将每个预测组合成一个权威的单一预测。当然，由于一些模型可能在预测值上存在分歧，该函数必须以某种方式混合或统一学习者的信息。组合函数也被称为**作曲家**，因为它的工作是综合最终预测。
- en: There are two main strategies for merging or composing final predictions. The
    simpler of the two approaches involves **weighting methods**, which assign a score
    to each prediction that dictates how heavily it will factor into the final prediction.
    These range from a simple majority vote in which each classifier is weighted evenly,
    to more complex performance-based methods that grant more authority to some models
    than others if they have proven to be more reliable on past data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 合并或组合最终预测有两种主要策略。其中较简单的方法涉及**加权方法**，它为每个预测分配一个分数，该分数决定了它在最终预测中的重要性。这些方法从简单的多数投票（其中每个分类器被平等加权）到更复杂的基于性能的方法，后者如果某些模型在过去的证据上证明比其他模型更可靠，则赋予某些模型比其他模型更多的权威。
- en: The second approach uses more complex meta-learning methods, such as the model
    stacking technique, which will be covered in depth later in this chapter. These
    use the initial set of predictions from the weak learners to train a secondary
    machine learning algorithm to make the final prediction—a process that is analogous
    to a committee making recommendations to a leader that makes the final decision.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法使用更复杂的元学习技术，例如模型堆叠技术，这些内容将在本章后面进行深入探讨。这些方法使用弱学习器的初始预测集来训练一个二级机器学习算法以进行最终预测——这个过程类似于一个委员会向做出最终决策的领导者提供建议。
- en: 'Ensembling methods are used to gain better performance than what is possible
    using only a single learning algorithm—the primary goal of the ensemble is to
    turn a group of weaker learners into a stronger, unified team. Still, there are
    many additional benefits, some of which may be surprising. These suggest additional
    reasons why one might turn to an ensemble, even outside of a machine learning
    competition environment:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法用于获得比仅使用单个学习算法更好的性能——集成的主要目标是把一群较弱的学习者转变为一个更强、更统一的团队。尽管如此，还有很多额外的优势，其中一些可能令人惊讶。这些表明了为什么人们可能会转向集成，即使在机器学习竞赛环境之外：
- en: '**The use of independent ensembles allows work in parallel**: Training independent
    classifiers separately means that work can be divided across multiple people.
    This allows more rapid iteration and may increase creativity. Each team member
    builds their best model, and the results can be easily combined into an ensemble
    at the end.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立集成的方法允许并行工作**：分别训练独立的分类器意味着工作可以分配给多个人。这允许更快的迭代，并可能增加创造力。每个团队成员构建他们最好的模型，最终的结果可以很容易地组合成一个集成。'
- en: '**Improved performance on massive or minuscule datasets**: Many algorithms
    run into memory or complexity limits when an extremely large set of features or
    examples are used. An ensemble of independent models can be fed subsets of features
    or examples, which are more computationally efficient to train than a single full
    model, and importantly, can often be run in parallel using distributed computing
    methods. On the other side of the spectrum, ensembles also do well on the smallest
    datasets because resampling methods like bootstrapping are inherently part of
    the allocation function of many ensemble designs.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在大型或小型数据集上提高性能**：当使用大量特征或示例时，许多算法会遇到内存或复杂度限制。一个独立的模型集可以输入特征或示例的子集，这些子集比单个完整模型更容易训练，并且重要的是，通常可以使用分布式计算方法并行运行。在另一端，集成在最小数据集上也做得很好，因为重采样方法（如自助法）是许多集成设计分配函数的固有部分。'
- en: '**The ability to synthesize data from distinct domains**: Since there is no
    one-size-fits-all learning algorithm, and each learning algorithm has its own
    biases and heuristics, the ensemble’s ability to incorporate evidence from multiple
    types of learners is increasingly important for modeling the most challenging
    learning tasks relying on data drawn from diverse domains.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**能够综合来自不同领域的数据**：由于没有一种适合所有情况的算法，每个学习算法都有自己的偏见和启发式方法，因此集成能够结合来自多种类型学习者的证据，对于建模最具有挑战性的学习任务（依赖于来自不同领域的数据）变得越来越重要。'
- en: '**A more nuanced understanding of difficult learning tasks**: Real-world phenomena
    are often extremely complex, with many interacting intricacies. Methods like ensembles,
    which divide the task into smaller modeled portions, are more able to capture
    subtle patterns that a single model might miss. Some learners in the set can go
    narrower and deeper to learn a specific subset of the most challenging cases.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对困难学习任务的更细致的理解**：现实世界现象通常非常复杂，有许多相互作用的复杂性。像集成这样的方法，将任务分解成更小的建模部分，更有可能捕捉到单个模型可能错过的微妙模式。集合中的某些学习者可以更窄、更深入地学习最具挑战性的案例的特定子集。'
- en: None of these benefits would be very helpful if you weren’t able to easily apply
    ensemble methods in R, and there are many packages available to do just that.
    Let’s look at several of the most popular ensemble methods and how they can be
    used to improve the performance of the credit model we’ve been working on.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能轻松地在R中应用集成方法，那么这些好处将不会非常有帮助，而且有许多包可以做到这一点。让我们看看几种最流行的集成方法以及它们如何帮助我们提高一直在工作的信用模型的性能。
- en: Popular ensemble-based algorithms
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流行的基于集成的方法
- en: Thankfully, using teams of machine learners to improve the predictive performance
    doesn’t mean you’ll need to train each ensemble member separately by hand, although
    this option does exist, as you will learn later in this chapter. Instead, there
    are ensemble-based algorithms that manipulate the allocation function to train
    a very large number of simpler models in a single step automatically. In this
    way, an ensemble that includes a hundred learners or more can be trained with
    no more human time and input than training a single learner. As easily as one
    might build a single decision tree model, it is possible to build an ensemble
    with hundreds of such trees and harness the power of teamwork. Although it would
    be tempting to assume this is a magic bullet, such power, of course, comes with
    downsides such as loss of interpretability and a less diverse set of base algorithms
    from which to choose. This will be apparent in the sections that follow, which
    cover the evolution of two decades’ worth of popular ensembling algorithms—all
    of which, not coincidentally, are based on decision trees.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用机器学习团队来提高预测性能并不意味着你需要手动分别训练每个集成成员，尽管这个选项是存在的，正如你将在本章后面学到的那样。相反，有一些基于集成的算法可以操纵分配函数，以单步自动训练大量简单的模型。这样，包含一百个或更多学习者的集成可以在不比训练单个学习者更多的时间和输入的情况下进行训练。就像一个人可能构建一个单一的决策树模型一样，可以构建一个包含数百个此类树木的集成，并利用团队合作的力量。虽然这可能会让人联想到这是一个神奇的子弹，但这种力量当然也伴随着一些缺点，比如可解释性的损失和选择基础算法的多样性较少。这一点将在接下来的章节中变得明显，这些章节涵盖了二十年来流行的集成算法的演变——所有这些算法都不是巧合地基于决策树。
- en: Bagging
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging
- en: One of the first ensemble methods to gain widespread acceptance used a technique
    called **bootstrap aggregating** or **bagging** for short. As described by Leo
    Breiman in the mid-1990s, bagging begins by generating several new training datasets
    using bootstrap sampling on the original training data. These datasets are then
    used to generate a set of models using a single learning algorithm. The models’
    predictions are combined using voting for classification and averaging for numeric
    prediction.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最早获得广泛认可的集成方法之一使用了一种称为 **自助聚合** 或简称为 **Bagging** 的技术。正如 Leo Breiman 在 1990 年代中期所描述的，Bagging
    首先通过在原始训练数据上使用自助采样生成几个新的训练数据集。然后，使用单个学习算法使用这些数据集生成一组模型。模型的预测通过分类投票和数值预测的平均值进行组合。
- en: For additional information on bagging, refer to *Bagging predictors. Breiman
    L., Machine Learning, 1996, Vol. 24, pp. 123-140*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Bagging的更多信息，请参阅 *Bagging predictors. Breiman L., Machine Learning, 1996,
    Vol. 24, pp. 123-140*。
- en: Although bagging is a relatively simple ensemble, it can perform quite well
    if it is used with relatively **unstable** learners, that is, those generating
    models that tend to change substantially when the input data changes only slightly.
    Unstable models are essential for ensuring the ensemble’s diversity despite only
    minor variations across the bootstrap training datasets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Bagging是一种相对简单的集成方法，但如果与相对**不稳定**的学习者一起使用，即那些在输入数据仅发生微小变化时模型倾向于发生实质性变化的学习者，它可以表现得相当好。不稳定的模型对于确保集成在自助训练数据集的微小变化中保持多样性是必不可少的。
- en: For this reason, bagging is most often used with decision trees, which have
    the tendency to vary dramatically given minor changes in input data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Bagging 最常与决策树一起使用，因为决策树倾向于在输入数据发生微小变化时发生显著变化。
- en: The `ipred` package offers a classic implementation of bagged decision trees.
    To train the model, the `bagging()` function works similarly to many of the models
    used previously. The `nbagg` parameter is used to control the number of decision
    trees voting in the ensemble, with a default value of `25`. Depending on the difficulty
    of the learning task and the amount of training data, increasing this number may
    improve the model’s performance, up to a limit. The downside is that this creates
    additional computational expense, and a large number of trees may take some time
    to train.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`ipred` 包提供了一个经典的袋装决策树的实现。为了训练模型，`bagging()` 函数的工作方式与之前使用的许多模型类似。`nbagg` 参数用于控制参与集成的决策树数量，默认值为
    `25`。根据学习任务的难度和训练数据量，增加这个数量可能会提高模型性能，但有一个上限。缺点是这会增加额外的计算开销，大量树木的训练可能需要一些时间。'
- en: 'After installing the `ipred` package, we can create the ensemble as follows.
    We’ll stick to the default value of `25` decision trees:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `ipred` 包后，我们可以创建集成如下。我们将坚持默认的 `25` 个决策树值：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting `mybag` model works as expected in concert with the `predict()`
    function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`mybag`模型与`predict()`函数协同工作，如预期：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Given the preceding results, the model seems to have fit the data extremely
    well—*too well*, probably, as the results are based only on the training data
    and thus may reflect overfitting rather than true performance on future unseen
    data. To obtain a better estimate of future performance, we can use the bagged
    decision tree method in the `caret` package to obtain a 10-fold CV estimate of
    accuracy and kappa. Note that the method name for the `ipred` bagging function
    is `treebag`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的结果，模型似乎与数据拟合得非常好——*可能太好了*，因为结果仅基于训练数据，因此可能反映了过度拟合而不是对未来未见数据的真实性能。为了获得对未来性能的更好估计，我们可以使用`caret`包中的袋装决策树方法来获得10折交叉验证的准确性和kappa估计。请注意，`ipred`袋装函数的方法名称为`treebag`：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The kappa statistic of 0.33 for this model suggests that the bagged tree model
    performs roughly as well as the C5.0 decision tree we tuned earlier in this chapter,
    which had a kappa statistic ranging from 0.32 to 0.34, depending on the tuning
    parameters. Keep this performance in mind as you read the next section, and consider
    the differences between the simple bagging technique and the more complex methods
    that build upon it.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的kappa统计量为0.33，表明袋装树模型的表现大致与我们在本章早期调整的C5.0决策树相当，其kappa统计量在0.32到0.34之间变化，具体取决于调整参数。在阅读下一节时，请记住这一性能，并考虑简单袋装技术与在此基础上构建的更复杂方法之间的差异。
- en: Boosting
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升法
- en: Another common ensemble-based method is called **boosting** because it improves
    or “boosts” the performance of weak learners to attain the performance of stronger
    learners. This method is based largely on the work of Robert Schapire and Yoav
    Freund, who have published extensively on the topic since the 1990s.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的基于集合的方法被称为**提升法**，因为它提高了或“提升”了弱学习者的性能，以达到强学习者的性能。这种方法主要基于Robert Schapire和Yoav
    Freund的工作，自1990年代以来，他们在该主题上发表了大量论文。
- en: 'For additional information on boosting, refer to *Boosting: Foundations and
    Algorithms, Schapire, RE, Freund, Y, Cambridge, MA: The MIT Press, 2012*.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如需有关提升法的更多信息，请参阅*《提升法：基础与算法》，Schapire RE，Freund Y，剑桥，MA：麻省理工学院出版社，2012*。
- en: Like bagging, boosting uses ensembles of models trained on resampled data and
    a vote to determine the final prediction. There are two key distinctions. First,
    the resampled datasets in boosting are constructed specifically to generate complementary
    learners. This means that the work cannot occur in parallel, as the ensemble’s
    models are no longer independent from one another. Second, rather than giving
    each learner an equal vote, boosting gives each learner a vote that is weighted
    based on its past performance. Models that perform better have greater influence
    over the ensemble’s final prediction.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装法类似，提升法使用在重采样数据上训练的模型集合，并通过投票来确定最终预测。有两个关键的区别。首先，提升法中的重采样数据集是专门构建的，以生成互补的学习者。这意味着工作不能并行进行，因为集合中的模型不再是相互独立的。其次，提升法并不是给每个学习者平等的投票，而是根据其过去的性能给每个学习者一个加权投票。表现更好的模型对集合的最终预测有更大的影响力。
- en: Boosting will result in performance that is often somewhat better and certainly
    no worse than the best model in the ensemble. Since the models in the ensemble
    are purposely built to be complementary, it is possible to increase ensemble performance
    to an arbitrary threshold simply by adding additional classifiers to the group,
    assuming that each additional classifier performs better than random chance. Given
    the obvious utility of this finding, boosting is thought to be one of the most
    significant discoveries in machine learning.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的结果通常略好，但绝对不差于集合中最好的模型。由于集合中的模型是故意构建为互补的，因此只需向该组添加额外的分类器，假设每个额外的分类器都比随机机会表现更好，就可以通过添加额外的分类器来提高集合的性能到一个任意阈值。鉴于这一发现的明显效用，提升法被认为是机器学习中最重大的发现之一。
- en: Although boosting can create a model that meets an arbitrarily low error rate,
    this may not always be reasonable in practice. One reason for this is that the
    performance gains are incrementally smaller as additional learners are gained,
    making some thresholds practically infeasible. Additionally, the pursuit of pure
    accuracy may result in the model being overfitted to the training data and not
    generalizable to unseen data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管增强可以创建一个满足任意低错误率的模型，但在实践中这并不总是合理的。原因之一是，随着更多学习者的加入，性能增益逐渐减小，使得某些阈值实际上不可行。此外，追求纯粹的准确性可能导致模型过度拟合训练数据，无法推广到未见过的数据。
- en: A boosting algorithm called **AdaBoost**, short for **adaptive boosting**, was
    proposed by Freund and Schapire in 1997\. The algorithm is based on the idea of
    generating weak learners that iteratively learn a larger portion of the difficult-to-classify
    examples in the training data by paying more attention (that is, giving more weight)
    to often misclassified examples.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为**AdaBoost**的增强算法，全称为**自适应增强**，由Freund和Schapire于1997年提出。该算法基于生成弱学习者的想法，通过迭代地学习训练数据中难以分类的样本的更大部分，通过对经常被错误分类的样本给予更多关注（即给予更多权重）来实现。
- en: Beginning from an unweighted dataset, the first classifier attempts to model
    the outcome. Examples that the classifier predicted correctly will be less likely
    to appear in the training dataset for the following classifier, and conversely,
    the difficult-to-classify examples will appear more frequently. As additional
    rounds of weak learners are added, they are trained on data with successively
    more difficult examples. The process continues until the desired overall error
    rate is reached or performance no longer improves. At that point, each classifier’s
    vote is weighted according to its accuracy on the training data on which it was
    built.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个无权重的数据集开始，第一个分类器试图模拟结果。分类器预测正确的示例不太可能出现在下一个分类器的训练数据集中，反之亦然，难以分类的示例将更频繁地出现。随着更多弱学习者的加入，它们将在具有越来越困难示例的数据上训练。这个过程一直持续到达到所需的总体错误率或性能不再提高。在此之后，每个分类器的投票将根据其在构建时的训练数据上的准确性进行加权。
- en: Though boosting principles can be applied to nearly any type of model, the principles
    are most often used with decision trees. We already applied the boosting technique
    earlier in this chapter, as well as in *Chapter 5*, *Divide and Conquer – Classification
    Using Decision Trees and Rules*, as a method to improve the performance of a C5.0
    decision tree. With C5.0, boosting can be enabled by simply setting a `trials`
    parameter to an integer value greater than one.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然增强原理可以应用于几乎任何类型的模型，但这些原理最常与决策树一起使用。我们已经在本章前面以及*第5章*，*分而治之 - 使用决策树和规则进行分类*中应用了增强技术，作为一种提高C5.0决策树性能的方法。在C5.0中，只需将`trials`参数设置为大于一的整数值即可启用增强。
- en: The **AdaBoost.M1** algorithm provides a standalone implementation of AdaBoost
    for classification with trees. The algorithm can be found in the `adabag` package.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost.M1**算法为分类提供了AdaBoost的独立实现。该算法可在`adabag`包中找到。'
- en: 'For more information about the `adabag` package, refer to *adabag: An R Package
    for Classification with Boosting and Bagging, Alfaro, E, Gamez, M, Garcia, N,
    Journal of Statistical Software, 2013, Vol. 54, pp. 1-35*.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`adabag`包的更多信息，请参阅*adabag：用于分类的增强和Bagging的R包，Alfaro, E, Gamez, M, Garcia,
    N, 统计软件杂志，2013，第54卷，第1-35页*。
- en: 'Let’s create an `AdaBoost.M1` classifier for the credit data. The general syntax
    for this algorithm is similar to other modeling techniques:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为信用数据创建一个`AdaBoost.M1`分类器。此算法的一般语法与其他建模技术类似：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As usual, the `predict()` function is applied to the resulting object to make
    predictions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将`predict()`函数应用于结果对象以进行预测：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Departing from convention, rather than returning a vector of predictions, this
    returns an object with information about the model. The predictions are stored
    in a sub-object called `class`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规不同，它返回一个包含模型信息的对象，而不是返回一个预测的向量。预测存储在名为`class`的子对象中：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A confusion matrix can be found in the `confusion` sub-object:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在`confusion`子对象中找到混淆矩阵：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Before you get your hopes up about the perfect accuracy, note that the preceding
    confusion matrix is based on the model’s performance on the training data. Since
    boosting allows the error rate to be reduced to an arbitrarily low level, the
    learner simply continued until it made no more errors. This likely resulted in
    overfitting on the training dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在你对完美的准确率抱有希望之前，请注意，前面的混淆矩阵是基于模型在训练数据上的性能。由于提升允许错误率降低到任意低水平，学习器简单地继续直到不再犯错误。这可能导致在训练数据集上的过度拟合。
- en: 'For a more accurate assessment of performance on unseen data, we need to use
    another evaluation method. The `adabag` package provides a simple function to
    use 10-fold CV:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对未见数据上的性能进行更准确的评估，我们需要使用另一种评估方法。`adabag` 包提供了一个简单的函数来使用10折交叉验证：
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Depending on your computer’s capabilities, this may take some time to run,
    during which it will log each iteration to the screen—on a recent MacBook Pro
    computer, it took about a minute. After it completes, we can view a more reasonable
    confusion matrix:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您计算机的能力，这可能需要一些时间来运行，在此期间，它将记录每次迭代的日志到屏幕上——在最新的MacBook Pro计算机上，大约需要一分钟。完成后，我们可以查看一个更合理的混淆矩阵：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can find the kappa statistic using the `vcd` package, as demonstrated in
    *Chapter 10*, *Evaluating Model Performance*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `vcd` 包找到卡方统计量，如第10章中所示，*评估模型性能*：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With a kappa of `0.3397`, the boosted model is slightly outperforming the bagged
    decision trees, which had a kappa of around `0.3319`. Let’s see how boosting compares
    to another ensemble method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在卡方值为`0.3397`的情况下，提升模型略优于卡方值约为`0.3319`的装袋决策树。让我们看看提升与另一种集成方法相比如何。
- en: Note that the prior results were obtained using R version 4.2.3 on a Windows
    PC and verified on Linux. At the time this was written, slightly different results
    are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro. Also note
    that the AdaBoost.M1 algorithm can be tuned with `caret` by specifying `method
    = "AdaBoost.M1"`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，先前结果是在Windows PC上使用R版本4.2.3获得的，并在Linux上进行了验证。在撰写本文时，使用R 4.2.3在最新的MacBook
    Pro上的Apple硅上获得的结果略有不同。此外，请注意，AdaBoost.M1算法可以通过指定`method = "AdaBoost.M1"`来使用`caret`进行调整。
- en: Random forests
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: Yet another tree-based ensemble-based method, called **random forests**, builds
    upon the principles of bagging but adds additional diversity to the decision trees
    by only allowing the algorithm to choose from a randomly selected subset of features
    each time it attempts to split. Beginning at the root node, the random forest
    algorithm might only be allowed to choose from a small number of features selected
    at random from the full set of predictors; at each subsequent split, a different
    random subset is provided. As is the case for bagging, once the ensemble of trees
    (the forest) is generated, the algorithm performs a simple vote to make the final
    prediction.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于树的集成方法，称为**随机森林**，建立在装袋原则的基础上，但通过每次只允许算法从随机选择的特征子集中选择来为决策树增加额外的多样性。从根节点开始，随机森林算法可能只能从从完整预测器集合中随机选择的少量特征中选择；在每次后续分割时，提供不同的随机子集。与装袋类似，一旦生成树集成（森林），算法就会进行简单的投票来做出最终预测。
- en: For more detail on how random forests are constructed, refer to *Random Forests,
    Breiman L, Machine Learning, 2001, Vol. 45, pp. 5-32*. Note that the phrase “random
    forests” is trademarked by Breiman and Cutler but is used colloquially to refer
    to any type of decision tree ensemble. A pedant would use the more general term
    **decision tree forests** except when referring to their specific implementation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林如何构建的更多详细信息，请参阅*随机森林，Breiman L，机器学习，2001，第45卷，第5-32页*。请注意，“随机森林”一词是Breiman和Cutler的商标，但通常用来指任何类型的决策树集成。一个严谨的人会使用更一般的术语**决策树森林**，除非在提及它们的特定实现时。
- en: The fact that each tree is built on different and randomly selected sets of
    features helps ensure that each tree in the ensemble is unique. It is even possible
    that two trees in the forest may have been built from completely different sets
    of features. Random feature selection limits the decision tree’s greedy heuristic
    from picking the same low-hanging fruit each time the tree is grown, which may
    help the algorithm discover subtle patterns that the standard tree-growing method
    may miss. On the other hand, the potential for overfitting is limited given that
    each tree has just one vote of many in the forest.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每棵树都是基于不同且随机选择的特征集构建的，这有助于确保集成中的每棵树都是独特的。甚至可能森林中的两棵树是从完全不同的特征集构建的。随机特征选择限制了决策树在每次生长时选择相同低垂果实的贪婪启发式方法，这可能有助于算法发现标准树增长方法可能错过的微妙模式。另一方面，由于每棵树在森林中只有一票，因此过拟合的可能性有限。
- en: 'Given these strengths, it is no surprise that the random forest algorithm quickly
    grew to become one of the most popular learning algorithms—only recently has its
    hype been surpassed by a newer ensemble method, which you will learn about shortly.
    Random forests combine versatility and power into a single machine learning approach
    and are not especially prone to overfitting or underfitting. Because the tree-growing
    algorithm uses only a small, random portion of the full feature set, random forests
    can handle extremely large datasets, where the so-called curse of dimensionality
    might cause other models to fail. At the same time, its predictive performance
    on most learning tasks is as good as, if not better than, all but the most sophisticated
    methods. The following table summarizes the strengths and weaknesses of random
    forest models:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些优点，随机森林算法迅速成为最受欢迎的学习算法之一——最近，其炒作已被一种新的集成方法所超越，你将在稍后了解该方法。随机森林将多功能性和强大功能结合为单一机器学习方法，并且不太容易过拟合或欠拟合。由于树增长算法仅使用整个特征集的一小部分随机样本，随机森林可以处理极其庞大的数据集，而所谓的维度诅咒可能会使其他模型失败。同时，它在大多数学习任务上的预测性能与最复杂的方法相当，甚至更好。以下表格总结了随机森林模型的优缺点：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose model that performs well on most problems, including both classification
    and numeric prediction
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种适用于大多数问题的通用模型，在分类和数值预测方面都表现出色
- en: Can handle noisy or missing data as well as categorical or continuous features
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理噪声或缺失数据，以及分类或连续特征
- en: Selects only the most important features
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只选择最重要的特征
- en: Can be used on data with an extremely large number of features or examples
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于具有极多特征或示例的数据
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Unlike a decision tree, the model is not easily interpretable
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与决策树不同，该模型不易解释
- en: May struggle with categorical features with very large numbers of levels
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能难以处理具有非常大量级别的分类特征
- en: Cannot be extensively tuned if greater performance is desired
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要更高的性能，则无法进行大量调整
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Their strong performance combined with the ease of use makes random forests
    a terrific place to begin most real-world machine learning projects. The algorithm
    also provides a solid benchmark for other comparisons with highly tuned models,
    as well as the other, more complex approaches you will learn about later.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的强大性能与易用性使得随机森林成为大多数现实世界机器学习项目的绝佳起点。该算法还为其他与高度调整的模型以及你稍后将要学习的其他更复杂方法进行了可靠的基准测试。
- en: 'For a hands-on demonstration of random forests, we’ll apply the technique to
    the credit-scoring data we’ve been using in this chapter. Although there are several
    packages with random forest implementations in R, the aptly named `randomForest`
    package is perhaps the simplest, while the `ranger` package offers much better
    performance on large datasets. Both are supported by the `caret` package for experimentation
    and automated parameter tuning. The syntax for training a model with `randomForest`
    is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观演示随机森林，我们将应用本章中使用的信用评分数据。尽管R中有几个包含随机森林实现的包，但名为`randomForest`的包可能是最简单的，而`ranger`包在大型数据集上提供了更好的性能。这两个包都由`caret`包支持，用于实验和自动参数调整。使用`randomForest`训练模型的语法如下：
- en: '![Text  Description automatically generated](img/B17290_14_06.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17290_14_06.png)'
- en: 'Figure 14.6: Random forest syntax'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：随机森林语法
- en: By default, the `randomForest()` function creates an ensemble of 500 decision
    trees that each consider `sqrt(p)` random features at each split, where `p` is
    the number of features in the training dataset and `sqrt()` refers to R’s square
    root function. For example, since the credit data has 16 features, each of the
    500 decision trees would be allowed to consider only *sqrt*(*16*) = *4* predictors
    each time the algorithm attempts to split.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`randomForest()`函数创建了一个包含500棵决策树的集成，每棵树在每个分割点考虑`sqrt(p)`个随机特征，其中`p`是训练数据集中的特征数量，`sqrt()`指的是R的平方根函数。例如，由于信用数据有16个特征，所以每棵500棵决策树在算法尝试分割时只能考虑*sqrt*(*16*)
    = *4*个预测因子。
- en: Whether or not these default `ntree` and `mtry` parameters are appropriate depends
    on the nature of the learning task and training data. Generally, more complex
    learning problems and larger datasets (both more features as well as more examples)
    warrant a larger number of trees, though this needs to be balanced with the computational
    expense of training more trees. Once the `ntree` parameter is set to a sufficiently
    large value, the `mtry` parameter can be tuned to determine the best setting;
    however, the default tends to work well in practice. Assuming the number of trees
    is large enough, the number of randomly selected features can be surprisingly
    low before performance is degraded—but trying a few values is still a good practice.
    Ideally, the number of trees should be set large enough such that each feature
    has a chance of appearing in several models.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这些默认的`ntree`和`mtry`参数是否合适取决于学习任务和训练数据的性质。一般来说，更复杂的学习问题和更大的数据集（包括更多的特征和更多的示例）需要更多的树，尽管这需要与训练更多树的计算成本相平衡。一旦将`ntree`参数设置为一个足够大的值，就可以调整`mtry`参数以确定最佳设置；然而，在实践中默认设置往往效果良好。假设树的数量足够大，在性能下降之前，随机选择特征的数量可以出奇地低——但尝试几个值仍然是好习惯。理想情况下，应该将树的数量设置得足够大，以便每个特征都有机会出现在几个模型中。
- en: 'Let’s see how the default `randomForest()` parameters work with the credit
    data. We’ll train the model just as we have done with other learners. As usual,
    the `set.seed()` function ensures that the result can be replicated:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看默认的`randomForest()`参数如何与信用数据一起工作。我们将像对其他学习器所做的那样训练模型。像往常一样，`set.seed()`函数确保结果可以复制：
- en: '[PRE33]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For a summary of model performance, we can simply type the resulting object’s
    name:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结模型性能，我们可以简单地输入结果对象的名称：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output shows that the random forest included 500 trees and tried four variables
    at each split, as expected. At first glance, you might be alarmed at the seemingly
    poor performance according to the confusion matrix—the error rate of 23.3 percent
    is far worse than the resubstitution error of any of the other ensemble methods
    so far. However, this confusion matrix does not show a resubstitution error. Instead,
    it reflects the **out-of-bag error rate** (listed in the output as `OOB estimate
    of error rate`), which, unlike a resubstitution error, is an unbiased estimate
    of the test set error. This means that it should be a fair estimate of future
    performance.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示，随机森林包含500棵树，并在每个分割点尝试了四个变量，正如预期的那样。乍一看，根据混淆矩阵，你可能会对看似糟糕的性能感到惊讶——23.3%的错误率远高于迄今为止任何其他集成方法的重新替换错误。然而，这个混淆矩阵并没有显示重新替换错误。相反，它反映了**袋外误差率**（在输出中列为`OOB
    estimate of error rate`），与重新替换错误不同，它是对测试集误差的无偏估计。这意味着它应该是对未来性能的公平估计。
- en: The out-of-bag estimate is computed using a clever technique during the construction
    of the random forest. Essentially, any example not selected for a single tree’s
    bootstrap sample can be used to test the model’s performance on unseen data. At
    the end of the forest construction, for each of the 1,000 examples in the dataset,
    any trees that did not use the example in training are allowed to make a prediction.
    These predictions are tallied, and a vote is taken to determine the single final
    prediction for the example. The total error rate of such predictions across all
    1,000 examples becomes the out-of-bag error rate. Because each prediction uses
    only a subset of the forest, it is not equivalent to a true validation or test
    set estimation, but it is a reasonable substitute.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林构建过程中，使用一种巧妙的技术来计算袋外估计。本质上，任何未被选为单个树的自举样本的示例都可以用来测试模型在未见数据上的性能。在森林构建结束时，对于数据集中的每个1,000个示例，任何在训练中未使用该示例的树都可以进行预测。这些预测被汇总，并通过投票来确定该示例的单个最终预测。所有1,000个示例的这种预测的总错误率成为袋外错误率。因为每个预测只使用森林的子集，所以它不等同于真正的验证或测试集估计，但它是一个合理的替代品。
- en: In *Chapter 10*, *Evaluating Model Performance*, it was stated that any given
    example has a 63.2 percent chance of being included in a bootstrap sample. This
    implies that an average of 36.8 percent of the 500 trees in the random forest
    voted for each of the 1,000 examples in the out-of-bag estimate.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章*评估模型性能*中提到，任何给定的示例有63.2%的几率被包含在自举样本中。这意味着平均有36.8%的随机森林中的500棵树在袋外估计中对每个1,000个示例进行了投票。
- en: 'To calculate the kappa statistic on the out-of-bag predictions, we can use
    the function in the `vcd` package as follows. The code applies the `Kappa()` function
    to the first two rows and columns of the `confusion` object, which stores the
    confusion matrix of the out-of-bag predictions for the `rf` random forest model
    object:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要在袋外预测上计算kappa统计量，我们可以使用`vcd`包中的函数如下。代码将`Kappa()`函数应用于`confusion`对象的前两行和两列，该对象存储了`rf`随机森林模型对象的袋外预测混淆矩阵：
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With a kappa statistic of `0.381`, the random forest is our best-performing
    model yet. Its performance was better than the bagged decision tree ensemble,
    which had a kappa of about `0.332`, as well as the AdaBoost.M1 model, which had
    a kappa of about `0.340`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在kappa统计量为`0.381`的情况下，随机森林是我们迄今为止表现最好的模型。其性能优于具有约`0.332`的kappa值的袋装决策树集成，以及具有约`0.340`的kappa值的AdaBoost.M1模型。
- en: The `ranger` package, as mentioned previously, is a substantially faster implementation
    of the random forest algorithm. For a dataset as small as the credit dataset,
    optimizing for computational efficiency may be less important than ease of use,
    and by default, `ranger` sacrifices some conveniences in order to increase speed
    and reduce the memory footprint. Consequently, although the `ranger` function
    is nearly identical to `randomForest()` in syntax, in practice, you may find that
    it breaks existing code or takes a bit of digging through the help pages.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`ranger`包是一个随机森林算法的实质性快速实现。对于像信用数据集这样小的数据集，优化计算效率可能不如易用性重要，默认情况下，`ranger`牺牲了一些便利性以提高速度并减少内存占用。因此，尽管`ranger`函数在语法上几乎与`randomForest()`相同，但在实践中，你可能会发现它破坏了现有的代码或需要查阅帮助页面。
- en: 'To recreate the previous model using `ranger`, we simply change the function
    name:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`ranger`重新创建先前的模型，我们只需更改函数名：
- en: '[PRE38]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The resulting model has quite a similar out-of-bag prediction error:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型具有相当相似的袋外预测错误：
- en: '[PRE39]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can compute kappa much as before while noting the slight difference in how
    the model’s confusion matrix sub-object was named:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样计算kappa值，同时注意模型混淆矩阵子对象命名的细微差别：
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The kappa value is `0.381`, which is the same as the result from the earlier
    random forest model. Note that this is coincidental, as the two algorithms are
    not guaranteed to produce identical results.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: kappa值为`0.381`，与早期随机森林模型的结果相同。请注意，这是巧合，因为这两个算法不保证产生相同的结果。
- en: As with AdaBoost, the prior results were obtained using R version 4.2.3 on a
    Windows PC and verified on Linux. At the time this was written, slightly different
    results are obtained using R 4.2.3 for Apple silicon on a recent MacBook Pro.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 与AdaBoost一样，先前的结果是在Windows PC上的R版本4.2.3上获得的，并在Linux上进行了验证。在撰写本文时，使用R 4.2.3在最新的MacBook
    Pro上的Apple硅上获得的结果略有不同。
- en: Gradient boosting
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升
- en: '**Gradient boosting** is an evolution of the boosting algorithm based on the
    finding that it is possible to treat the boosting process as an optimization problem
    to be solved using the gradient descent technique. We first encountered gradient
    descent in *Chapter 7*, *Black-Box Methods – Neural Networks and Support Vector
    Machines*, where it was introduced as a solution to optimize the weights in a
    neural network. You may recall that a cost function—essentially, the prediction
    error—relates the input values to the target. Then, by systematically analyzing
    how changes to the weights affect the cost, it is possible to find the set of
    weights that minimizes the cost. Gradient boosting treats the process of boosting
    in much the same way, with the weak learners in the ensemble being treated as
    the parameters to optimize. Models using this technique are termed **gradient
    boosting machines** or **generalized boosting models**—both of which can be abbreviated
    as **GBMs**.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**是基于以下发现的提升算法的演变：可以将提升过程视为一个可以使用梯度下降技术解决的优化问题。我们首次在*第7章*，*黑盒方法——神经网络和支持向量机*中遇到梯度下降，它被介绍为优化神经网络权重的一种解决方案。您可能还记得，成本函数——本质上，预测误差——将输入值与目标相关联。然后，通过系统地分析权重变化如何影响成本，可以找到最小化成本的一组权重。梯度提升以类似的方式处理提升过程，将集成中的弱学习器视为需要优化的参数。使用这种技术的模型被称为**梯度提升机**或**广义提升模型**——两者都可以缩写为**GBMs**。'
- en: 'For more on GBMs, see *Greedy Function Approximation: A Gradient Boosting Machine,
    Friedman JH, 2001, Annals of Statistics 29(5):1189-1232*.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '更多关于GBMs的信息，请参阅*Greedy Function Approximation: A Gradient Boosting Machine,
    Friedman JH, 2001, Annals of Statistics 29(5):1189-1232*。'
- en: The following table summarizes the strengths and weaknesses of GBMs. In short,
    gradient boosting is extremely powerful and can produce some of the most accurate
    models but may require tuning to find the balance between over- and underfitting.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了GBMs的优缺点。简而言之，梯度提升非常强大，可以产生一些最精确的模型，但可能需要调整以找到过拟合和欠拟合之间的平衡。
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose classifier that can perform extremely well on both classification
    and numeric prediction
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种通用的分类器，可以在分类和数值预测上表现出色。
- en: Can achieve even better performance than random forests
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以实现比随机森林更好的性能。
- en: Performs well on large datasets
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型数据集上表现良好。
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: May require tuning to match the performance of the random forest algorithm and
    more extensive tuning to exceed its performance
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要调整以匹配随机森林算法的性能，并且需要更广泛的调整才能超越其性能。
- en: Because there are several hyperparameters to tune, finding the best combination
    requires many iterations and more computing power
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为有多个超参数需要调整，找到最佳组合需要多次迭代和更多的计算能力。
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: We’ll use the `gbm()` function in the `gbm` package for creating GBMs for both
    classification and numeric prediction. You’ll need to install and load this package
    to your R session if you haven’t already. As the following box shows, the syntax
    is like the machine learning functions used previously, but it has several new
    parameters that may need to be adjusted. These parameters control the complexity
    of the model and the balance between over- and underfitting. Without tuning, the
    GBM may not perform as well as simpler methods, but it generally can surpass the
    performance of most other methods once parameter values have been optimized.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`gbm()`函数在`gbm`包中创建用于分类和数值预测的GBMs。如果您还没有安装和加载此包到R会话中，您需要这样做。如下方框所示，语法类似于之前使用的机器学习函数，但它有几个可能需要调整的新参数。这些参数控制模型的复杂性和过拟合与欠拟合之间的平衡。未经调整，GBM可能不如简单方法表现得好，但一旦参数值被优化，它通常可以超越大多数其他方法的性能。
- en: '![](img/B17290_14_07.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_14_07.png)'
- en: 'Figure 14.7: Gradient boosting machine (GBM) syntax'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：梯度提升机（GBM）语法
- en: 'We can train a simple GBM to predict loan defaults on the `credit` dataset
    as follows. For simplicity, we set `stringsAsFactors = TRUE` to avoid recoding
    the predictors, but then the target `default` feature must be converted back to
    a binary outcome, as the `gbm()` function requires this for binary classification.
    We’ll create a random sample for training and testing, then apply the `gbm()`
    function to the training data, leaving the parameters set to their defaults:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练一个简单的GBM模型，在`credit`数据集上预测贷款违约，如下所示。为了简化，我们将`stringsAsFactors = TRUE`设置为避免重新编码预测变量，但随后必须将目标`default`特征转换回二进制结果，因为`gbm()`函数需要这个二进制分类。我们将创建一个用于训练和测试的随机样本，然后将`gbm()`函数应用于训练数据，参数设置为默认值：
- en: '[PRE43]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Typing the name of the model provides some basic information about the GBM
    process:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输入模型的名称会提供一些关于GBM过程的基本信息：
- en: '[PRE44]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'More importantly, we can evaluate the model on the test set. Note that we need
    to convert the predictions to binary, as they are given as probabilities. If the
    probability of loan default is greater than 50 percent, we will predict default,
    otherwise, we predict non-default. The table shows the agreement between the predicted
    and actual values:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，我们可以在测试集上评估模型。请注意，我们需要将预测转换为二进制，因为它们是以概率给出的。如果贷款违约的概率大于50%，我们将预测违约，否则，我们预测非违约。表格显示了预测值和实际值之间的协议：
- en: '[PRE46]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To measure the performance, we’ll apply the `Kappa()` function to this table:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量性能，我们将`Kappa()`函数应用于此表：
- en: '[PRE48]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The resulting kappa value of about `0.361` is better than what was obtained
    with the boosted decision tree, but worse than the random forest model. Perhaps
    with a bit of tuning, we can get this higher.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的kappa值约为`0.361`，比使用提升决策树得到的好，但比随机森林模型差。也许经过一点调整，我们可以将其提高。
- en: 'We’ll use the `caret` package to tune the GBM model and obtain a more robust
    performance measure. Recall that tuning needs a search grid, which we can define
    for GBM as follows. This will test three values for three of the `gbm()` function
    parameters and one value for the remaining parameter, which results in *3 * 3
    * 3 * 1 = 27* models to evaluate:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`caret`包来调整GBM模型，以获得更稳健的性能指标。回想一下，调整需要搜索网格，我们可以为GBM定义如下网格。这将测试`gbm()`函数三个参数的三个值以及剩余参数的一个值，从而得到*3
    * 3 * 3 * 1 = 27*个模型进行评估：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we set the `trainControl` object to select the best model from a 10-fold
    CV experiment:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`trainControl`对象设置为从10折交叉验证实验中选择最佳模型：
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Lastly, we read in the `credit` dataset and supply the required objects to
    the `caret()` function while specifying the `gbm` method and the `Kappa` performance
    metric. Depending on the capabilities of your computer, this may take a few minutes
    to run:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们读取`credit`数据集，并将所需的对象提供给`caret()`函数，同时指定`gbm`方法和`Kappa`性能指标。根据您计算机的能力，这可能需要几分钟才能运行：
- en: '[PRE52]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Typing the name of the object shows the results of the experiment. Note that
    some lines of output have been omitted for brevity, but the full output contains
    27 rows—one for each model evaluated:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输入对象的名称会显示实验结果。请注意，为了简洁，省略了一些输出行，但完整的输出包含27行——每个评估的模型一行：
- en: '[PRE53]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: From the output, we can see that the best GBM model had a kappa of `0.394`,
    which exceeds the random forest trained previously. With additional tuning, it
    may be possible to bring the kappa up even higher. Or, as you will see in the
    next section, a more intensive form of boosting can be employed in the pursuit
    of even better performance.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到最佳GBM模型的kappa值为`0.394`，超过了之前训练的随机森林。通过进一步的调整，可能将kappa值进一步提高。或者，正如您将在下一节中看到的，可以采用更加强化的提升形式来追求更好的性能。
- en: Extreme gradient boosting with XGBoost
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于XGBoost的极端梯度提升
- en: A cutting-edge implementation of the gradient boosting technique can be found
    in the **XGBoost** algorithm (`https://xgboost.ai`), which takes boosting to the
    “extreme” by improving the algorithm’s efficiency and performance. In the time
    since the algorithm was introduced in 2014, XGBoost has been found on top of the
    leaderboards of many machine learning competitions. In fact, according to the
    algorithm’s authors, among 29 winning solutions on Kaggle in 2015, a total of
    17 used the XGBoost algorithm. Likewise, in the 2015 KDD Cup (described in *Chapter
    11*, *Being Successful with Machine Learning*), all of the top 10 winners used
    XGBoost. Today, the algorithm is still the champion for traditional machine learning
    problems involving classification and numeric prediction, whereas its closest
    challenger, deep neural networks, tends to win only on unstructured data, such
    as image, audio, and text processing.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost算法（`https://xgboost.ai`）是梯度提升技术的尖端实现，它通过提高算法的效率和性能将提升技术推向了“极致”。自2014年算法推出以来，XGBoost已经在许多机器学习竞赛的排行榜上名列前茅。事实上，根据算法作者的统计，在2015年的Kaggle上，29个获胜方案中，共有17个使用了XGBoost算法。同样，在2015年的KDD
    Cup（在第11章*用机器学习取得成功*中描述），前10名获胜者都使用了XGBoost。如今，该算法仍然是传统机器学习问题（涉及分类和数值预测）的冠军，而其最接近的挑战者，深度神经网络，往往只在非结构化数据（如图像、音频和文本处理）上获胜。
- en: 'For more information on XGBoost, see *XGBoost: A Scalable Tree Boosting System,
    Chen T and Guestrin C, 2016*. [https://arxiv.org/abs/1603.02754](https://arxiv.org/abs/1603.02754).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '关于XGBoost的更多信息，请参阅 *XGBoost: A Scalable Tree Boosting System, Chen T and Guestrin
    C, 2016*。[https://arxiv.org/abs/1603.02754](https://arxiv.org/abs/1603.02754).'
- en: 'The great power of the XGBoost algorithm comes with the downside that the algorithm
    is not quite as easy to use and requires substantially more tuning than other
    methods examined so far. On the other hand, its performance ceiling tends to be
    higher than any other approach. The strengths and weaknesses of XGBoost are found
    in the following table:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost算法的强大功能伴随着算法并不那么容易使用，并且比迄今为止考察的其他方法需要更多的调整。另一方面，其性能上限往往高于任何其他方法。XGBoost的优点和缺点如下表所示：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose classifier that can perform extremely well on both classification
    and numeric prediction
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通用的分类器，能够在分类和数值预测上表现出色
- en: Perhaps undisputedly, the current champion of performance on traditional learning
    problems; wins virtually every machine learning competition on structured data
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在传统学习问题上的性能可能是无争议的冠军；在结构化数据上的几乎每个机器学习竞赛中都能获胜
- en: Highly scalable, performs well on large datasets, and can be run in parallel
    on distributed computing platforms
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展，在大数据集上表现良好，可以在分布式计算平台上并行运行
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: More challenging to use than other functions, as it relies on external frameworks
    that do not use native R data structures
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比其他函数更难使用，因为它依赖于不使用原生R数据结构的外部框架
- en: Requires extensive tuning of a large set of hyperparameters that can be difficult
    to understand without a strong math background
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要对大量超参数进行广泛的调整，这些超参数在没有强大数学背景的情况下可能难以理解
- en: Because there are many tuning parameters, finding the best combination requires
    many iterations and more computing power
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为有很多调整参数，找到最佳组合需要多次迭代和更多的计算能力
- en: Results in a “black box” model that is nearly impossible to interpret without
    explainability tools
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导致一个“黑盒”模型，没有解释性工具几乎无法解释
- en: '|'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'To apply the algorithm, we’ll use the `xgboost()` function in the `xgboost`
    package, which provides an R interface to the XGBoost framework. Entire books
    could be written about this framework, as it includes features for many types
    of machine learning tasks, and is highly extensible and adaptable to many high-performance
    computing environments. For more information about the XGBoost framework, see
    the excellent documentation on the web at [https://xgboost.readthedocs.io](https://xgboost.readthedocs.io).
    Our work will focus on a narrow slice of its functionality, as shown in the following
    syntax box, which is much denser than those for other algorithms due to a large
    increase in complexity and hyperparameters that may be tuned:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用算法，我们将使用`xgboost`包中的`xgboost()`函数，它为XGBoost框架提供了R接口。关于这个框架，可以写出整本书，因为它包括了许多类型机器学习任务的功能，并且具有高度的可扩展性和适应性强，适用于许多高性能计算环境。有关XGBoost框架的更多信息，请参阅网络上的优秀文档[https://xgboost.readthedocs.io](https://xgboost.readthedocs.io)。我们的工作将专注于其功能的一个狭窄部分，如下面的语法框所示，由于复杂性和可能调整的超参数的大量增加，它比其他算法的语法要密集得多：
- en: '![](img/B17290_14_08.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_14_08.png)'
- en: 'Figure 14.8: XGBoost (XGB) syntax'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：XGBoost（XGB）语法
- en: One of the challenges with using XGBoost in R is its need to use data in matrix
    format rather than R’s preferred formats of tibbles or data frames. Because XGBoost
    is designed for extremely large datasets, it can also use sparse matrices, such
    as those discussed in previous chapters. You may recall that a sparse matrix only
    stores non-zero values, which makes it more memory-efficient than traditional
    matrices when many feature values are zeros.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用R中的XGBoost的一个挑战是它需要使用矩阵格式而不是R首选的tibbles或数据框格式。因为XGBoost是为极大数据集设计的，它也可以使用稀疏矩阵，如前几章中讨论的。你可能记得，稀疏矩阵只存储非零值，当许多特征值为零时，这使得它比传统矩阵更节省内存。
- en: Data in matrix form is often sparse because factors are typically one-hot or
    dummy coded during the transition between the data frame and matrix. These encodings
    create additional columns for additional levels of the factor, and all columns
    are set to zero except the one “hot” value that indicates the level for the given
    example. In the case of dummy coding, one feature level is left out of the transformation,
    so it results in one fewer column than one-hot; the missing level can be indicated
    by the presence of zeros in all of the *n* - *1* columns.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵形式的数据通常是稀疏的，因为因素通常在数据框和矩阵之间的转换过程中进行one-hot或虚拟编码。这些编码为因素级别的额外级别创建了额外的列，除了表示给定示例级别的“热”值之外，所有列都设置为0。在虚拟编码的情况下，转换中省略了一个特征级别，因此它比one-hot少一个列；缺失的级别可以通过所有*n
    - 1*列中存在零来表示。
- en: One-hot and dummy coding generally produce the same results, with the exception
    that statistics-based models like regression require dummy coding and will present
    errors or warning messages if one-hot is used instead.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot编码和虚拟编码通常会产生相同的结果，但有一个例外，即基于统计的模型如回归需要虚拟编码，如果使用one-hot编码，则会显示错误或警告信息。
- en: 'Let’s begin by reading the `credit.csv` file and creating a sparse matrix of
    data from the `credit` data frame. The `Matrix` package provides a function to
    perform this task, which uses the R formula interface to determine the columns
    to include in the matrix. Here, the formula `~ . -default` tells the function
    to use all features except `default`, which we don’t want in the matrix, as this
    is our target feature for prediction:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取`credit.csv`文件并从`credit`数据框创建一个稀疏矩阵开始。`Matrix`包提供了一个执行此任务的功能，它使用R公式接口来确定矩阵中要包含的列。在这里，公式`~
    . -default`告诉函数使用除了`default`之外的所有特征，因为我们不希望在矩阵中包含这个特征，因为这是我们预测的目标特征：
- en: '[PRE55]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'To confirm our work, let’s check the dimensions of the matrix:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认我们的工作，让我们检查矩阵的维度：
- en: '[PRE56]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We still have 1,000 rows, but the columns have increased from 16 features in
    the original data frame to 36 in the sparse matrix. This is due to the dummy coding
    that was applied automatically when converting to matrix form. We can see this
    if we examine the first five rows and 15 columns of the sparse matrix using the
    `print()` function:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有1,000行，但列数从原始数据框中的16个特征增加到稀疏矩阵中的36个。这是由于在转换为矩阵形式时自动应用的虚拟编码。我们可以使用`print()`函数检查稀疏矩阵的前五行和15列来看到这一点：
- en: '[PRE58]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The matrix is depicted with the dot (`.`) character indicating cells with zero
    values. The first column (`1`, `2`, `3`, `4`, `5`) is the row number and the second
    column (`1`, `1`, `1`, `1`, `1`) is a column for the intercept term, which was
    added automatically by the R formula interface. Two columns have numbers (`6`,
    `48`, …) and (`1169`, `5951`, …) that correspond to the numeric values of the
    `months_loan_duration` and `amount` features, respectively. All other columns
    are dummy-coded versions of factor variables. For instance, the third, fourth,
    and fifth columns reflect the `checking_balance` feature, with a `1` in the third
    column indicating a value of `'> 200 DM'`, a `1` in the fourth column indicating
    `'1 – 200 DM'`, and a 1 in the fifth column indicating the `'unknown'` feature
    value. Rows showing the sequence `. . .` in columns 3, 4, and 5 fall into the
    reference category, which was the `'< 0 DM'` feature level.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵用点(`.`)字符表示零值单元格。第一列(`1`, `2`, `3`, `4`, `5`)是行号，第二列(`1`, `1`, `1`, `1`, `1`)是截距项的列，这是由R公式接口自动添加的。两列有数字(`6`,
    `48`, …)和(`1169`, `5951`, …)，分别对应于`months_loan_duration`和`amount`特征的数值。所有其他列都是因子变量的虚拟编码版本。例如，第三、第四和第五列反映了`checking_balance`特征，第三列中的`1`表示`'>
    200 DM'`的值，第四列中的`1`表示`'1 – 200 DM'`，第五列中的`1`表示`'unknown'`特征值。在第三、第四和第五列中显示序列`...`的行属于参考类别，这是`'<
    0 DM'`特征级别。
- en: 'Since we are not building a regression model, the intercept column full of
    `1` values is useless for this analysis and can be removed from the matrix:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不是构建回归模型，所以充满`1`值的截距列对于这次分析是无用的，可以从矩阵中移除：
- en: '[PRE60]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, we’ll split the matrix at random into training and test sets using a
    90-10 split as we’ve done before:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将矩阵随机分割成训练集和测试集，使用之前90-10的分割比例：
- en: '[PRE61]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To confirm the work was done correctly, we’ll check the dimensions of these
    matrices:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认工作是否正确完成，我们将检查这些矩阵的维度：
- en: '[PRE62]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As expected, the training set has 900 rows and 35 columns, and the test set
    has 100 rows and a matching set of columns.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，训练集有900行和35列，测试集有100行和与之匹配的列数。
- en: 'Lastly, we’ll create training and test vectors of labels for `default`, the
    target to be predicted. These are transformed from factors to binary `1` or `0`
    values using an `ifelse()` function so that they can be used to train and evaluate
    the XGBoost model, respectively:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将为`default`目标创建训练和测试标签向量。这些标签通过`ifelse()`函数从因子转换为二进制`1`或`0`值，以便可以分别用于训练和评估XGBoost模型：
- en: '[PRE66]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We’re now ready to start building the model. After installing the `xgboost`
    package, we’ll load the library and start to define the hyperparameters for training.
    Without knowing where else to begin, we’ll set the values to their defaults:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始构建模型。在安装了`xgboost`包之后，我们将加载库并开始定义训练的超参数。由于不知道从哪里开始，我们将值设置为默认值：
- en: '[PRE67]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, after setting the random seed, we’ll train the model, supplying our parameters
    object as well as the matrix of training data and the target labels. The `nrounds`
    parameter determines the number of boosting iterations. Without a better guess,
    we’ll set this to `100`, which is a common starting point due to empirical evidence
    suggesting that results tend to improve very little beyond this value. Lastly,
    the `verbose` and `print_every_n` options are used to turn on diagnostic output
    and display the progress after every 10 boosting iterations:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在设置随机种子之后，我们将训练模型，提供我们的参数对象以及训练数据矩阵和目标标签。`nrounds`参数决定了提升迭代的次数。由于没有更好的猜测，我们将此设置为`100`，这是一个常见的起点，因为经验证据表明结果在此值之后很少有所改善。最后，`verbose`和`print_every_n`选项用于开启诊断输出，并在每10次提升迭代后显示进度：
- en: '[PRE68]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output should appear as the training is completed, showing that all 100
    iterations occurred and the training error (labeled `train-logloss`) continued
    to decline with additional rounds of boosting:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 完成训练后，输出应该显示所有100次迭代都已发生，训练错误（标记为`train-logloss`）在额外的提升轮次中继续下降：
- en: '[PRE69]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Knowing whether additional iterations would help the model performance or result
    in overfitting is something we can determine via tuning later. Before doing so,
    let’s look at the performance of this trained model on the test set, which we
    held out earlier. First, the `predict()` function obtains the predicted probability
    of loan default for each row of test data:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 了解额外的迭代是否有助于提高模型性能或导致过拟合是我们可以在调整后确定的事情。在这样做之前，让我们看看这个训练模型在之前保留的测试集上的表现。首先，`predict()`
    函数获取测试数据每行的贷款违约预测概率：
- en: '[PRE70]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then, we use `ifelse()` to predict a default (value `1`) if the probability
    of a default is at least 0.50, or non-default (value `0`) otherwise:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `ifelse()` 函数来预测违约（值为 `1`）的概率至少为 0.50，否则预测非违约（值为 `0`）：
- en: '[PRE71]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Comparing the predicted to actual values, we find an accuracy of *(62 + 14)
    / 100 = 76* percent:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测值与实际值进行比较，我们发现准确率为 *(62 + 14) / 100 = 76* 百分比：
- en: '[PRE72]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'On the other hand, the kappa statistic suggests there is still room to improve:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，kappa 统计量表明仍有改进的空间：
- en: '[PRE74]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The value of `0.3766` is a bit lower than the `0.394` we obtained with the
    GBM model, so perhaps a bit of hyperparameter tuning can help. For this, we’ll
    use `caret`, starting with a tuning grid comprising a variety of options for each
    of the hyperparameters:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`0.3766` 的值略低于我们使用 GBM 模型获得的 `0.394`，因此可能需要一点超参数调整来帮助。为此，我们将使用 `caret`，从包含每个超参数各种选项的调整网格开始：'
- en: '[PRE76]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The resulting grid contains *2 * 3 * 2 * 3 * 3 * 2 * 1 = 216* different combinations
    of `xgboost` hyperparameter values. We’ll evaluate each of these potential models
    in `caret` using 10-fold CV, as we’ve done for other models. Note that the `verbosity`
    parameter is set to zero so that the `xgboost()` function output is suppressed
    for the many iterations:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的网格包含 *2 * 3 * 2 * 3 * 3 * 2 * 1 = 216* 种不同的 `xgboost` 超参数值组合。我们将像对其他模型所做的那样，在
    `caret` 中使用 10 折交叉验证来评估这些潜在模型。请注意，`verbosity` 参数设置为零，以便抑制 `xgboost()` 函数输出的多次迭代：
- en: '[PRE77]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Depending on the capabilities of your computer, the experiment may take a few
    minutes to complete, but once it finishes, typing `m_xgb` will provide the results
    of all 216 models tested. We can also obtain the best model directly as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你电脑的能力，实验可能需要几分钟才能完成，但一旦完成，输入 `m_xgb` 将提供所有 216 个测试模型的成果。我们也可以直接获得最佳模型，如下所示：
- en: '[PRE78]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The kappa value for this model can be found using the `max()` function to find
    the highest value as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `max()` 函数可以找到这个模型的最大值，从而找到该模型的 kappa 值如下：
- en: '[PRE80]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The kappa value of `0.406` is our best-performing model so far, exceeding the
    `0.394` of the GBM model and the `0.381` of the random forest. The fact that XGBoost
    required so little effort to train—with a bit of fine-tuning—yet still surpassed
    other powerful techniques provides examples of why it always seems to win machine
    learning competitions. Yet, with even more tuning, it may be possible to go higher
    still! Leaving that as an exercise to you, the reader, we’ll now turn our attention
    to the question of why all of these popular ensembles seem to focus exclusively
    on decision tree-based methods.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`0.406` 的 kappa 值是我们迄今为止表现最好的模型，超过了 GBM 模型的 `0.394` 和随机森林的 `0.381`。XGBoost
    在经过一点微调后仍然需要如此少的努力就能超越其他强大的技术，这提供了为什么它似乎总是赢得机器学习竞赛的原因。然而，通过更多的调整，我们可能还能进一步提高！将这个作为留给读者你的练习，我们现在将注意力转向为什么所有这些流行的集成方法似乎都专注于基于决策树的方法。'
- en: Why are tree-based ensembles so popular?
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么基于树的集成方法如此受欢迎？
- en: 'After reading the prior sections, you would not be the first person to wonder
    why ensembling algorithms seem to always be built upon decision trees. Although
    trees are not required for building an ensemble, there are several reasons why
    they are especially well-suited for this process. You may have noted some of them
    already:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了前面的章节之后，你不会是第一个想知道为什么集成算法似乎总是建立在决策树之上的。尽管构建集成算法不需要树，但有几个原因说明它们特别适合这个过程。你可能已经注意到了其中的一些：
- en: Ensembles work best with diversity, and because decision trees are not robust
    to small changes in the data, random sampling the same training data can easily
    create a diverse set of tree-based models
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法与多样性结合得最好，因为决策树对数据中的微小变化不稳健，随机采样相同的训练数据可以轻松创建一组基于树的模型
- en: Because of the greedy “divide-and-conquer” based algorithm, decision trees are
    computationally efficient and perform relatively well despite this fact
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于贪婪的“分而治之”算法，决策树在计算上效率高，尽管如此，其表现相对较好
- en: Decision trees can be grown purposely large or small to overfit and underfit
    as needed
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树可以根据需要有意地生长得很大或很小，以过度拟合和欠拟合
- en: Decision trees can automatically ignore irrelevant features, which reduces the
    negative impact of the “curse of dimensionality”
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树可以自动忽略无关特征，从而减少“维度诅咒”的负面影响
- en: Decision trees can be used for numeric prediction as well as classification
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树不仅可以用于数值预测，还可以用于分类
- en: Based on these characteristics, it is not difficult to see why we’ve ended up
    with a wealth of tree-based ensembling approaches such as bagging, boosting, and
    random forests. The distinctions among them are subtle but important.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些特性，不难看出我们为何拥有如此丰富的基于树的集成方法，如bagging、boosting和随机森林。它们之间的区别细微但很重要。
- en: 'The following table may help contrast the tree-based ensembling algorithms
    covered in this chapter:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格可能有助于对比本章所涵盖的基于树的集成算法：
- en: '| **Ensembling Algorithm** | **Allocation Function** | **Combination Function**
    | **Other Notes** |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **集成算法** | **分配函数** | **组合函数** | **其他说明** |'
- en: '| Bagging | Provides each learner with a bootstrap sample of the training data
    | The learners are combined using a vote for classification or a weighted average
    for numeric prediction | Uses an independent ensemble — the learners can be run
    in parallel |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Bagging | 为每个学习者提供一个训练数据的自助样本 | 学习者通过投票进行分类或通过加权平均进行数值预测进行组合 | 使用独立的集成——学习者可以并行运行
    |'
- en: '| Boosting | The first learner is given a random sample; subsequent samples
    are weighted to have more difficult-to-predict cases | The learners’ predictions
    are combined as above but weighted according to their performance on training
    data | Uses a dependent ensemble — each tree in the sequence receives data that
    earlier trees found challenging |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Boosting | 第一个学习者得到一个随机样本；后续样本被加权以包含更多难以预测的案例 | 学习者的预测按照上述方法组合，但根据它们在训练数据上的表现进行加权
    | 使用依赖集成——序列中的每棵树都接收早期树发现具有挑战性的数据 |'
- en: '| Random Forest | Like bagging, each tree receives a bootstrap sample of training
    data; however, features are also randomly selected for each tree split | Similar
    to bagging | Similar to bagging, but the added diversity via random feature selection
    allows additional benefits for larger ensembles |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 与bagging类似，每棵树都接收训练数据的自助样本；然而，对于每棵树的分裂，也会随机选择特征 | 类似于bagging | 类似于bagging，但通过随机特征选择增加的多样性为更大的集成提供了额外的优势
    |'
- en: '| Gradient Boosting Machine (GBM) | Conceptually similar to boosting | Similar
    to boosting, but there are many more learners and they comprise a complex mathematical
    function | Uses gradient descent to make a more efficient boosting algorithm;
    the trees are generally not very deep (decision tree “stumps”) but there are many
    more of them; requires more tuning |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升机（GBM） | 概念上类似于boosting | 类似于boosting，但学习者更多，它们构成了一个复杂的数学函数 | 使用梯度下降来制作更高效的boosting算法；树通常不是很深（决策树“桩”），但数量更多；需要更多调整
    |'
- en: '| eXtreme Gradient Boosting (XGB) | Similar to GBM | Similar to GBM | Similar
    to GBM but more extreme; uses optimized data structures, parallel processing,
    and heuristics to create a very performant boosting algorithm; tuning is essential
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 极端梯度提升（XGB） | 类似于GBM | 类似于GBM | 类似于GBM但更极端；使用优化的数据结构、并行处理和启发式方法来创建非常高效的boosting算法；调整是必要的
    |'
- en: To be able to distinguish among these approaches reveals a deep understanding
    of several aspects of ensembling. Additionally, the most recent techniques, such
    as random forests and gradient boosting, are among the best-performing learning
    algorithms and are being used as off-the-shelf solutions to solve some of the
    most challenging business problems. This may help explain why companies hiring
    data scientists and machine learning engineers often ask candidates to describe
    or compare these algorithms as part of the interview process. Thus, even though
    tree-based ensembling algorithms are not the only approach to machine learning,
    it is important to be aware of their potential uses. However, as the next section
    describes, trees aren’t the only approach to building a diverse ensemble.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 能够区分这些方法，表明了对集成几个方面的深刻理解。此外，最新的技术，如随机森林和梯度提升，是性能最好的学习算法之一，并被用作现成的解决方案来解决一些最具挑战性的商业问题。这可能有助于解释为什么雇佣数据科学家和机器学习工程师的公司通常要求候选人在面试过程中描述或比较这些算法。因此，尽管基于树的集成算法不是机器学习的唯一方法，但了解它们的潜在用途是很重要的。然而，正如下一节所描述的，树并不是构建多样化集成的唯一方法。
- en: Stacking models for meta-learning
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为元学习堆叠模型
- en: Rather than using a canned ensembling method like bagging, boosting, or random
    forests, there are situations in which a tailored approach to ensembling is warranted.
    Although these tree-based ensembling techniques combine hundreds or even thousands
    of learners into a single, stronger learner, the process is not much different
    than training a traditional machine learning algorithm, and suffers some of the
    same limitations, albeit to a lesser degree. Being based on decision trees that
    have been weakly trained and minimally tuned may, in some cases, put a ceiling
    on the ensemble’s performance relative to one composed of a more diverse set of
    learning algorithms that have been extensively tuned with the benefit of human
    intelligence. Furthermore, although it is possible to parallelize tree-based ensembles
    like random forests and XGB, this only parallelizes the computer’s effort—not
    the human effort of model building.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用诸如bagging、boosting或随机森林等现成的集成方法相比，在某些情况下，采用定制的集成方法是有必要的。尽管这些基于树的集成技术将数百甚至数千个学习器组合成一个单一、更强的学习器，但这个过程与传统机器学习算法的训练并没有太大的不同，并且存在一些相同的局限性，尽管程度较轻。基于弱训练和最小调优的决策树，在某些情况下，可能会对集成性能设置一个上限，相对于由更多样化的学习算法组成的集成，这些算法在人类智能的帮助下进行了广泛的调优。此外，尽管可以并行化基于树的集成，如随机森林和XGB，但这只并行化了计算机的努力——而不是模型构建的人类努力。
- en: Indeed, it is possible to increase an ensemble’s diversity by not only adding
    additional learning algorithms but by distributing the work of model building
    to additional human teams working in parallel. In fact, many of the world’s competition-winning
    models were built by taking other teams’ best models and ensembling them together.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，通过不仅添加额外的学习算法，还将模型构建的工作分配给并行工作的额外人类团队，可以增加集成的多样性。实际上，世界上许多获奖的模型都是通过采用其他团队的最佳模型并将它们集成在一起来构建的。
- en: This type of ensemble is conceptually quite simple, offering performance boosts
    that would be otherwise unobtainable, but can become complex in practice. Getting
    the implementation details correct is crucial to avoid disastrous levels of overfitting.
    Done correctly, the ensemble will perform at least as well as the strongest model
    in the ensemble, and often substantially better.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成在概念上非常简单，提供了在其他情况下无法获得的性能提升，但在实践中可能会变得复杂。正确实现实现细节至关重要，以避免灾难性的过拟合水平。如果正确执行，集成将至少与集成中最强的模型一样好，通常要好得多。
- en: 'Examining **receiver operating characteristic** (**ROC**) curves, as introduced
    in *Chapter 10*, *Evaluating Model Performance*, provides a simple method to determine
    whether two or more models would benefit from ensembling. If two models have intersecting
    ROC curves, their **convex hull**—the outermost boundary that would be obtained
    by stretching an imaginary rubber band around the curves—represents a hypothetical
    model that can be obtained by interpolating, or combining, the predictions from
    these models. As depicted in *Figure 14.9*, two ROC curves with identical **area
    under the curve** (**AUC**) values of *0.70* might create a new model with an
    AUC of *0.72* when paired in an ensemble:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, radar chart  Description automatically generated](img/B17290_14_09.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: When two or more ROC curves intersect, their convex hull represents
    a potentially better classifier that can be generated by combining their predictions
    in an ensemble'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Because this form of ensembling is performed largely by hand, a human needs
    to provide the allocation and combination functions for the models in the ensemble.
    In their simplest form, these can be implemented quite pragmatically. For example,
    suppose that the same training dataset has been given to three different teams.
    This is the allocation function. These teams can use this dataset however they
    see fit to build the best possible model using evaluation criteria of their choosing.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, each team is given the test set, and their models are used to make predictions,
    which must be combined into a single, final prediction. The combination function
    can take multiple different forms: the groups could vote, the predictions could
    be averaged, or the predictions could be weighted according to how well each group
    performed in the past. Even the simple approach of choosing one group at random
    is a viable strategy, assuming each group performs better than all others at least
    once in a while. Of course, even more intelligent approaches are possible, as
    you will soon learn.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model stacking and blending
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the most sophisticated custom ensembles apply machine learning to learn
    a combination function for the final prediction. Essentially, it is trying to
    learn which models can and cannot be trusted. This arbiter learner may realize
    that one model in the ensemble is a poor performer and shouldn’t be trusted or
    that another deserves more weight in the ensemble. The arbiter function may also
    learn more complex patterns. For example, suppose that when models *M1* and *M2*
    agree on the outcome, the prediction is almost always accurate, but otherwise
    *M3* is generally more accurate than either of the two. In this case, an additional
    arbiter model could learn to ignore the vote of *M1* and *M2* except when they
    agree. This process of using the predictions of several models to train a final
    model is called **stacking**.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart  Description automatically generated](img/B17290_14_10.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本和剪贴画的图片，描述自动生成](img/B17290_14_10.png)'
- en: 'Figure 14.10: Stacking is a sophisticated ensemble that uses an arbiter learning
    algorithm to combine the predictions of a set of learners and make a final prediction'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10：堆叠是一种复杂的集成，它使用仲裁学习算法来组合一组学习者的预测并做出最终预测
- en: More broadly, stacking falls within a methodology known as **stacked generalization**.
    As formally defined, the stack is constructed using first-level models that have
    been trained via CV, and a second-level model or **meta-model** that is trained
    using the predictions for the out-of-fold samples—the examples the model does
    not see during training but is tested on during the CV process.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，堆叠属于一种称为**堆叠泛化**的方法。根据正式定义，堆叠是通过使用通过交叉验证训练的一级模型构建的，以及使用超出折叠样本的预测来训练的二级模型或**元模型**。
- en: For example, suppose three first-level models are included in the stack and
    each one is trained using 10-fold CV. If the training dataset includes 1,000 rows,
    each of the three first-stage models is trained on 900 rows and tested on 100
    rows ten times. The 100-row test sets, when combined, comprise the entire training
    dataset.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设堆叠中包含三个一级模型，并且每个模型都使用10折交叉验证进行训练。如果训练数据集包括1,000行，那么每个一级模型在900行上训练，并在100行上测试十次。当这些100行的测试集合并时，构成了整个训练数据集。
- en: 'As all three models have made a prediction for every row of the training data,
    a new table can be constructed with four columns and 1,000 rows: the first three
    columns represent the predictions for the three models and column four represents
    the true value of the target. Note that because the predictions made for each
    of these 100 rows were made on the other 900 rows, all 1,000 rows are predictions
    on unseen data. This allows the second-stage meta-model, which is often a regression
    or logistic regression model, to learn which first-stage models perform better
    by training using the predicted values as the predictors of the true value. This
    process of finding the optimal combination of learners is sometimes called **super
    learning**, and the resulting model may be called a **super learner**. This process
    is often performed by machine learning software or packages, which train numerous
    learning algorithms in parallel and stack them together automatically.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有三个模型都对训练数据中的每一行进行了预测，可以构建一个包含四列和1,000行的新的表格：前三个列代表三个模型的预测，第四列代表目标的真实值。请注意，由于对这100行中的每一行所做的预测都是在其他900行上做出的，因此所有1,000行都是对未见数据的预测。这允许第二阶段的元模型，通常是一个回归或逻辑回归模型，通过使用预测值作为真实值的预测因子来训练，从而学习哪些一级模型表现更好。这个过程有时被称为**超级学习**，而得到的模型可能被称为**超级学习器**。这个过程通常由机器学习软件或包执行，它们并行训练多个学习算法并将它们自动堆叠在一起。
- en: '![Diagram  Description automatically generated with medium confidence](img/B17290_14_11.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成，置信度中等](img/B17290_14_11.png)'
- en: 'Figure 14.11: In a stacked ensemble, the second-stage meta-model or “super
    learner” learns from the predictions of the first-stage models on out-of-fold
    samples'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：在堆叠集成中，第二阶段的元模型或“超级学习器”从第一级模型对超出折叠样本的预测中学习
- en: For a more hands-on approach, a special case of stacked generalization called
    **blending** or **holdout stacking** provides a simplified way to implement stacking
    by replacing CV with a holdout sample. This allows the work to be distributed
    across teams more easily by merely dividing the training data into a training
    set for the first-level models and using a holdout set for the second-level meta-learner.
    It may also be less prone to overfitting the CV “information leak” described in
    *Chapter 11*, *Being Successful with Machine Learning*. Thus, even though it is
    a simple approach, it can be quite effective; blending is often what competition-winning
    teams do when they take other models and ensemble them together for better results.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更实际的方法，一种称为**混合**或**保留样本堆叠**的堆叠泛化特殊案例提供了一种简化堆叠实现的方法，通过用保留样本替换交叉验证。这可以通过将训练数据分为一级模型的训练集和使用保留集作为二级元学习器的集合，使工作更容易地在团队之间分配。这也可能不太容易受到第11章中描述的交叉验证“信息泄露”的影响，即*在机器学习中取得成功*。因此，尽管这是一个简单的方法，但它可以非常有效；混合通常是赢得比赛的团队在将其他模型组合在一起以获得更好的结果时所做的事情。
- en: The terminology around stacking, blending, and super learning is somewhat fuzzy,
    and many use the terms interchangeably.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Practical methods for blending and stacking in R
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform blending in R requires a careful roadmap, as getting the details
    wrong can lead to extreme overfitting and models that perform no better than random
    guessing. The following figure illustrates the process. Begin by imagining that
    you are tasked with predicting loan defaults and have access to one million rows
    of past data. Immediately, you should partition the dataset into training and
    test sets; of course, the test set should be kept in a vault for evaluating the
    ensemble later. Assume the training set is 750,000 rows and the test set is 250,000
    rows. The training set must then be divided yet again to create datasets for training
    the level one models and the level two meta-learner. The exact proportions are
    somewhat arbitrary, but it is customary to use a smaller set for the second-stage
    model—sometimes as low as ten percent. As *Figure 14.12* depicts, we might use
    500,000 rows for level one and 250,000 rows for level two:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_12.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: The full training dataset must be divided into distinct subsets
    for training the level one and level two models'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The 500,000-row level one training dataset is used to train the first-level
    models exactly as we have done many times throughout this book. The *M1*, *M2*,
    and *M3* models may use any learning algorithm, and the work of building these
    models can even be distributed across different teams working independently.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: There is no need for the models or teams to use the same set of features from
    the training data or the same form of feature engineering, assuming each team’s
    feature engineering pipeline can be replicated or automated in the future when
    the ensemble is to be deployed. The important thing is that *M1*, *M2*, and *M3*
    should be able to take a dataset with identical features and produce a prediction
    for each row.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'The 250,000-row level two training dataset is then fed into the *M1*, *M2*,
    and *M3* models after being processed through their associated feature engineering
    pipelines, and three vectors of 250,000 predictions are obtained. These vectors
    are labeled *p1*, *p2*, and *p3* in the diagram. When combined with the 250,000
    true values of the target (labeled *c* in the diagram) obtained from the level
    two training dataset, a four-column data frame is produced, as depicted in *Figure
    14.13*:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_14_13.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: The dataset used to train the meta-model is composed of the predictions
    from the first-level models and the actual target value from the level two training
    data'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: This type of data frame is used to create a meta-model, typically using regression
    or logistic regression, which predicts the actual target value (*c* in *Figure
    14.12*) using the predictions of *M1*, *M2*, and *M3* (*p1*, *p2*, and *p3* in
    *Figure 14.12*) as predictors. In an R formula, this might be specified in a form
    like `c ~ p1 + p2 + p3`, which results in a model that weighs the input from three
    different predictions to make its own final prediction.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the future performance of this final meta-model, we must use the
    250,000-row test set, which, as illustrated in *Figure 14.12* previously, was
    held out during the training process. As shown in *Figure 14.14*, the test dataset
    is then fed to the *M1*, *M2*, and *M3* models and their associated feature engineering
    pipelines, and much like in the previous step, three vectors of 250,000 predictions
    are obtained. However, rather than *p1*, *p2*, and *p3* being used to train a
    meta-model, they are now used as predictors for the existing meta-model to obtain
    a final prediction (labeled *p4*) for each of the 250,000 test cases. This vector
    can be compared to the 250,000 true values of the target in the test set to perform
    the performance evaluation and obtain an unbiased estimate of the ensemble’s future
    performance.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_14_14.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: To obtain an unbiased estimate of the ensemble’s future performance,
    the test set is used to generate predictions for the level one models, which are
    then used to obtain the meta-model’s final predictions'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'The above methodology is flexible to create other interesting types of ensembles.
    *Figure 14.15* illustrates a blended ensemble that combines models trained on
    completely different subsets of features. Specifically, it envisions a learning
    task in which Twitter profile data is used to make a prediction about the user—perhaps
    their gender or whether they would be interested in purchasing a particular product:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated](img/B17290_14_15.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: The stack’s first-level models can be trained on different features
    in the training set, while the second-level model is trained on their predictions'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: The first model receives the profile’s picture and trains a deep learning neural
    network with the image data to predict the outcome. Model two receives a set of
    tweets for the user and uses a text-based model like Naive Bayes to predict the
    outcome. Lastly, model three is a more conventional model using a traditional
    data frame of demographic data like location, total number of tweets, last login
    date, and so on.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: All three models are combined, and the meta-model can learn whether the image,
    text, or profile data is most helpful for predicting the gender or purchasing
    behavior. Alternatively, because the meta-model is a logistic regression model
    like *M3*, it would be possible to supply the profile data directly to the second-stage
    model and skip the construction of *M3* altogether.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Aside from constructing blended ensembles by hand as described here, there is
    a growing set of R packages to assist with this process. The `caretEnsemble` package
    can assist with ensemble models trained with the `caret` package and ensure that
    the stack’s sampling is handled correctly for stacking or blending. The `SuperLearner`
    package provides an easy way to create a super learner; it can apply dozens of
    base algorithms to the same dataset and stack them together automatically. As
    an off-the-shelf algorithm, this may be useful for building a powerful ensemble
    with the least amount of effort.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you should now know the approaches that are used
    to win data mining and machine learning competitions. Automated tuning methods
    can assist with squeezing every bit of performance out of a single model. On the
    other hand, tremendous gains are possible by creating groups of machine learning
    models called ensembles, which work together to achieve greater performance than
    single models can by working alone. A variety of tree-based algorithms, including
    random forests and gradient boosting, provide the benefits of ensembles but can
    be trained as easily as a single model. On the other hand, learners can be stacked
    or blended into ensembles by hand, which allows the approach to be carefully tailored
    to a learning problem.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: With a variety of options for improving the performance of a model, where should
    someone begin? There is no single best approach, but practitioners tend to fall
    into one of three camps. First, some begin with one of the more sophisticated
    ensembles such as random forests or XGBoost, and spend most of their time tuning
    and feature engineering to achieve the highest possible performance for this model.
    A second group might try a variety of approaches, then collect the models into
    a single stacked or blended ensemble to create a more powerful learner. The third
    approach might be described as “throw everything at the computer and see what
    sticks.” This attempts to feed the learning algorithm as much data as possible
    and as quickly as possible, and is sometimes combined with automated feature engineering
    or dimensionality reduction techniques like those described in the previous chapters.
    With practice, you may be drawn to some of these ideas more than others, so feel
    free to use whichever works best for you.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Although this chapter was designed to help you prepare competition-ready models,
    note that your fellow competitors have access to the same techniques. You won’t
    be able to get away with stagnancy; therefore, continue to add proprietary methods
    to your bag of tricks. Perhaps you can bring unique subject-matter expertise to
    the table, or perhaps your strengths include an eye for detail in data preparation.
    In any case, practice makes perfect, so take advantage of competitions to test,
    evaluate, and improve your machine learning skillset. In the next chapter—the
    last in this book—we’ll look at ways to apply cutting-edge “big data” techniques
    to some highly specialized and difficult data tasks using R.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章旨在帮助你准备竞赛所需的模型，请注意你的竞争对手也能接触到同样的技术。你不能仅仅依靠停滞不前；因此，继续将专有方法添加到你的技巧包中。也许你可以带来独特的专业知识，或者也许你的优势包括在数据准备中对细节的关注。无论如何，熟能生巧，所以利用竞赛来测试、评估和提升你的机器学习技能集。在本书的下一章——也是最后一章中，我们将探讨如何使用R将尖端“大数据”技术应用于一些高度专业化和困难的数据任务。
- en: Join our book’s Discord space
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人交流，并与其他4000多人一起学习：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/r.jpg)'
