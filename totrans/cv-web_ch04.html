<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" style="font-size:1.200rem;">
<head><title>Chapter&#160;4.&#160;Smile and Wave, Your Face Has Been Tracked!</title>
<link rel="stylesheet" href="../Styles/style0001.css" type="text/css"/>
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
</head>
<body id="page">
<div class="chapter" title="Chapter&#160;4.&#160;Smile and Wave, Your Face Has Been Tracked!"><div class="titlepage" id="aid-RL0A2"><div><div><h1 class="title"><a id="ch04"></a>Chapter&#160;4.&#160;Smile and Wave, Your Face Has Been Tracked!</h1>
</div>
</div>
</div>
<p>The most commonly seen object in our lives is a human face. We interact with people everywhere even when we do not meet them in person; we write a lot of messages via social networks, such as Twitter and Facebook, or e-mails and text messages using our phones. Face detection and tracking has many applications. In some cases, you might want to create a human computer interface, which will take the head position as an input or, more likely, you might want to help your users with tagging their friends. Actually, there are a lot of face detection libraries, which are written on JavaScript; these outnumber the libraries that focus on image processing itself. This is a good opportunity to choose the library that you really need. In addition to face detection, many libraries support face particle recognition and recognition of other objects.</p>
<p>In this chapter, we <a id="id163" class="indexterm"></a>will focus on the <a id="id164" class="indexterm"></a>JSFeat (<a class="ulink" href="http://inspirit.github.io/jsfeat/">http://inspirit.github.io/jsfeat/</a>), tracking.js (<a class="ulink" href="http://inspirit.github.io/jsfeat/">http://inspirit.github.io/jsfeat/</a>), and headtrackr (<a class="ulink" href="https://github.com/auduno/headtrackr">https://github.com/auduno/headtrackr</a>) libraries. The last library supports head tracking instead <a id="id165" class="indexterm"></a>of just recognition. Most of the libraries focus on Haar-like features detection.</p>
<p>With the help of several examples, we will cover the following topics in this chapter:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Face detection with JSFeat</li>
<li class="listitem">Tagging people with tracking.js</li>
<li class="listitem">Head tracking with Camshift</li>
</ul>
</div>
<div class="section" title="Face detection with JSFeat"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"></a>Face detection with JSFeat</h1>
</div>
</div>
</div>
<p>We saw detection of various objects in the previous chapter. The human face is much more complicated <a id="id166" class="indexterm"></a>than just a <a id="id167" class="indexterm"></a>regular color object, for example. More complex detectors share in common such things like the usage of brightness information and the patterns that this information forms. First of all, we need to see how face recognition is done. Without that, the tracking process will be quite difficult to understand. Actually, in most cases, the face recognition part is just the first step of face tracking algorithms.</p>
<p>We start with the JSFeat project. This awesome library provides a functionality to detect a face in two ways. Both have many applications in the real world. We will see how both of them work from the inside and discuss the API provided by JSFeat.</p>
<div class="section" title="Face detection using Haar-like features"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"></a>Face detection using Haar-like features</h2>
</div>
</div>
</div>
<p>This is probably the most popular face detector nowadays. Most of the libraries use exactly this <a id="id168" class="indexterm"></a>algorithm as<a id="id169" class="indexterm"></a> a common face detector. It is easy to implement and use. In addition to this, it can be used in any application as it gives good precision in face detection. The method itself forms a Viola-Jones object detection framework, which was proposed by Paul Viola and Michael Jones in 2001.</p>
<p>Remember the convolution kernels from <a class="link" title="Chapter&#160;2.&#160;Turn Your Browser into Photoshop" href="cv-web_ch02.html#aid-I3QM1">Chapter 2</a>, <span class="emphasis"><em>Turn Your Browser into Photoshop</em></span>? Take a look at this picture:</p>
<div class="mediaobject"><img src="../Images/image00118.jpeg" alt="Face detection using Haar-like features"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The rectangles are called Haar-like features or Haar features. They are just convolution kernels, where we subtract pixels under the white rectangle and add pixels where under the black part. To compute them fast, we use integral images. If you do not remember the concept, then you had better refresh your memory by referring to the section on integral image under the section <span class="emphasis"><em>What is filtering and how to use it</em></span> in <a class="link" title="Chapter&#160;2.&#160;Turn Your Browser into Photoshop" href="cv-web_ch02.html#aid-I3QM1">Chapter 2</a>, <span class="emphasis"><em>Turn Your Browser into Photoshop</em></span>. Briefly, the integral image provides substantial support in fast calculation of the sum of pixels in a rectangular area of an image.</p>
<p>We can use the features shown in the right-hand side of the picture too. They are rotated by 45 degrees. For that case, we use the tilted integral. They can capture more object details. The functionality for tilted features is available in most of the libraries. But there is a problem which prevents its usage in real-world applications—the Haar features are usually applied on low resolution parts of an image, for example, 20 x 20 or 24 x 24; when we rotate a feature (or integral image), we may face rounding errors. Because of this, those features are rarely used in practice.</p>
<p>How can these <a id="id170" class="indexterm"></a>features help us? Using them, we can describe an object by selecting the unique features of it. For <a id="id171" class="indexterm"></a>example, you see an ordinary female face under low resolution in the following images:</p>
<div class="mediaobject"><img src="../Images/image00119.jpeg" alt="Face detection using Haar-like features"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Usually, the part with the eyes is darker than the lower part. Furthermore, the nose is brighter than the eyes and the brows. We already found two unique face features!</p>
<p>For the input image, we use a sliding window to apply those kernels and check whether an object in the window is a face or not. We need to do this for all possible sizes and locations of kernels, which is practically impossible. Even for a 24 x 24 window, we need to check more than 160,000 features. Of course, there is a solution for this. We need to train a classifier and save only those features that are relevant to the detected object, in our case, it is a face. Unfortunately, JavaScript libraries do not provide such a functionality. Actually, they do not need to do so, since the libraries we use already contain most of necessary classifiers for face detection. Besides, the training time can take from several hours to months. However, if you need to detect something else or improve the detection accuracy, then you will probably want to see other libraries, for<a id="id172" class="indexterm"></a> example, OpenCV (<a class="ulink" href="http://opencv.org">http://opencv.org</a>). They provide the functionality to train your own classifier.</p>
<p>In short, during the training process, the algorithm checks all possible sizes and positions for features and selects the best of them that describe the object. After the first step we get several thousands of features. Still, this is too much. The next step provides a solution for this problem. We group these features into different stages of the classifier. When the algorithm checks the slide window, the algorithm evaluates it on each group one-by-one. If the first group fails the checking, then we discard that window and move on to another. This whole process structure is<a id="id173" class="indexterm"></a> called a <span class="strong"><strong>Cascade of classifiers</strong></span>. Eventually, the training<a id="id174" class="indexterm"></a> process <a id="id175" class="indexterm"></a>significantly reduces the number of features that need checking.</p>
<p>To make the algorithm scale invariant, it is applied using various window sizes. Unfortunately, the algorithm is not rotation invariant. You can try to apply this by rotating the source image but in that case, you may face incorrect results.</p>
<p>Now, you have an idea of how the whole algorithm works. Let's see how we can apply it in the JSFeat library by performing the following steps:</p>
<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we need to define an object which we want to detect. In our case it is a face. To set an object, we need to add a JavaScript file:<div class="informalexample"><pre class="programlisting">&lt;script src="/path/to/frontalface.js"&gt;&lt;/script&gt;</pre>
</div></li>
<li class="listitem">Then, set the classifier in the code. It contains cascades, the original window size, and the tilted integral flag, if it is required:<div class="informalexample"><pre class="programlisting">var classifier = jsfeat.haar.frontalface;</pre>
</div></li>
<li class="listitem">Next, we get the image data from the context:<div class="informalexample"><pre class="programlisting">var imageData = context.getImageData(0, 0, cols, rows);</pre>
</div></li>
<li class="listitem">We then define an image and convert it to grayscale:<div class="informalexample"><pre class="programlisting">var mat = new jsfeat.matrix_t(cols, rows, jsfeat.U8C1_t);
jsfeat.imgproc.grayscale(imageData.data, cols, rows, mat);</pre>
</div></li>
<li class="listitem">Sometimes, it is a good choice to increase the image contrast and remove some noise, which can be done as follows:<div class="informalexample"><pre class="programlisting">jsfeat.imgproc.equalize_histogram(mat, mat);
jsfeat.imgproc.gaussian_blur(mat, mat, 3);</pre>
</div></li>
<li class="listitem">We then predefine arrays for integrals:<div class="informalexample"><pre class="programlisting">var integralSum = new Int32Array((cols + 1) * (rows + 1));
var integralSqSum = new Int32Array((cols + 1) * (rows + 1));
var integralTilted = new Int32Array((cols + 1) * (rows + 1));
jsfeat.imgproc.compute_integral_image(mat, integralSum, integralSqSum, classifier.tilted ? integralTilted : null);</pre>
</div></li>
<li class="listitem">Take a close look at what we do here. We compute the tilted integral only if it is set to <code class="literal">true</code> in the classifier. There is a part that is not required, but in some cases, it helps to speed up the computation and remove noisy elements. We will check the edges' density using the Canny edge detector and its integral:<div class="informalexample"><pre class="programlisting">var integralCanny = new Int32Array((cols + 1) * (rows + 1));
var edges = new jsfeat.matrix_t(cols, rows, jsfeat.U8C1_t);
jsfeat.imgproc.canny(mat, edges, 10, 50);
jsfeat.imgproc.compute_integral_image(edges, integralCanny, null, null);</pre>
</div></li>
<li class="listitem">If the number of edges in a window is less than the edges' density. Then the program will skip that window without checking the Haar features. You can set the density threshold in JSFeat as follows:<div class="informalexample"><pre class="programlisting">jsfeat.haar.edges_density = 0.13;</pre>
</div></li>
<li class="listitem">Next, we set the other parameters and call the function:<div class="informalexample"><pre class="programlisting">var minScale = 2;
var scaleFactor = 1.1;
var bb = jsfeat.haar.detect_multi_scale(integralSum, integralSqSum, integralTilted, integralCanny,
        mat.cols, mat.rows, classifier, scaleFactor, minScale);</pre>
</div></li>
</ol>
<div style="height:10px; width: 1px"></div>
</div>
<p>If you look into the <a id="id176" class="indexterm"></a>frontalface.js file, you will see that the original window size is 20 x 20 pixels, but we <a id="id177" class="indexterm"></a>set the <code class="literal">minScale</code> variable in the preceding code block assuming that there will be no face that is smaller than 40x40 pixels. The <code class="literal">scaleFactor</code> variable is the factor for the scale. The process stops when the window increases to the image size.</p>
<p>The algorithm returns multiple rectangles for each face. Why? Because when the algorithm moves the window, the movement can be too small to make a big difference to the image. The JSFeat library provides a method to group those rectangles, where the last parameter indicates how many neighbors the result should have in order to be grouped with another one:</p>
<div class="informalexample"><pre class="programlisting">bb = jsfeat.haar.group_rectangles(bb, 1);</pre>
</div>
<p>Moreover, the algorithm returns confidence for each detection, and if we want to print only the best detections, then we can sort the result array and print only the most confident ones:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.math.qsort(bb, 0, bb.length - 1, function (a, b) {
    return (b.confidence &lt; a.confidence);
});
for (var i = 0; i &lt; maxFaceNumber; ++i) {
    var b = bb[i];
    context.strokeStyle = "#fff";
    context.strokeRect(b.x, b.y, b.width, b.height);
}</pre>
</div>
<p>After applying<a id="id178" class="indexterm"></a> this to an <a id="id179" class="indexterm"></a>image, we get the following result:</p>
<div class="mediaobject"><img src="../Images/image00120.jpeg" alt="Face detection using Haar-like features"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>On the first image, we painted all rectangles without grouping; see how many detections we got for the faces? The different sizes represent different window scales. On the second image, we painted the faces after grouping. Already good enough, isn't it? And for the last one, we chose the four most confident detections.</p>
<p>As we can see, the algorithm has many interesting parts and it really helps to detect faces on photos. There are various implementations of that algorithm. In addition, this method has many extensions. We will discuss one of them in the next section.</p>
</div>
<div class="section" title="Brightness binary features"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"></a>Brightness binary features</h2>
</div>
</div>
</div>
<p>From the section name, you may conclude that this method works with a change in image brightness, probably with its pixels, and that it compares those intensity values to receive some sort <a id="id180" class="indexterm"></a>of a binary check. You are totally right! In some ways, it is like getting <a id="id181" class="indexterm"></a>FAST corners, but the whole idea is a bit more complex. Let's discuss it.</p>
<p>The main difference<a id="id182" class="indexterm"></a> between brightness binary features and Haar features is that it uses distinct pixels instead of convolutions. Moreover, it uses different image pyramids not different sliding window sizes to compute the required features.</p>
<p>You can get an idea from the following image:</p>
<div class="mediaobject"><img src="../Images/image00121.jpeg" alt="Brightness binary features"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>We kept the resolution of all three images the same for a better view. But you need to keep in mind that the images are 24x24, 12x12, and 6x6 pixels. Besides, white and black points represent pixels.</p>
<p>Here, the idea is <a id="id183" class="indexterm"></a>very similar to <a id="id184" class="indexterm"></a>what you saw while learning the Haar features. For example, eyes are much darker than other face particles and because of that, we indicate them as dark or black points. The correct instance of an object must follow the rules: all white points i and black points j in a window should satisfy the expression <code class="literal">I(i) &gt; I(j)</code>, where <code class="literal">I(position)</code> is a pixel value of a window at this position.</p>
<p>The number of points may vary, it is chosen during the classifier training process. The format for the classifier is different from the format of the Haar features. The training process is much more complex, since it needs to get various point combinations. In case you want to train your own classifier, you may want to follow the <a id="id185" class="indexterm"></a>CCV library (<a class="ulink" href="http://libccv.org/doc/doc-bbf/">http://libccv.org/doc/doc-bbf/</a>). This is a C library that provides implementations of various Computer Vision algorithms.</p>
<p>It is harder to find a BBF algorithm implementation, since it is more complicated and the training process is much more difficult. Also, the JSFeat library provides the algorithm for the same. First, you need to include the classifier file:</p>
<div class="informalexample"><pre class="programlisting">&lt;script src="/path/to/bbf_face.js"&gt;&lt;/script&gt;</pre>
</div>
<p>Then, you need to preallocate some data before the computation starts:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.bbf.prepare_cascade(jsfeat.bbf.face_cascade);</pre>
</div>
<p>As usual, we work with grayscale images; we get one using the standard JSFeat functions:</p>
<div class="informalexample"><pre class="programlisting">var imageData = context.getImageData(0, 0, cols, rows);
var mat = new jsfeat.matrix_t(cols, rows, jsfeat.U8C1_t);
jsfeat.imgproc.grayscale(imageData.data, cols, rows, mat);</pre>
</div>
<p>One of the important <a id="id186" class="indexterm"></a>steps is <a id="id187" class="indexterm"></a>generating an image pyramid. The input parameters are: input image, minimum dimensions of the image in the pyramid, and an interval. It sets the number of original scale levels in the pyramid; the larger this number, the more pyramid levels you get:</p>
<div class="informalexample"><pre class="programlisting">var pyramid = jsfeat.bbf.build_pyramid(mat, minWidth, minHeight, interval);</pre>
</div>
<p>Then call the function which takes the image pyramid and the cascade as input parameters. After all this, we group the resulting rectangles together:</p>
<div class="informalexample"><pre class="programlisting">var bb = jsfeat.bbf.detect(pyramid, jsfeat.bbf.face_cascade);
bb = jsfeat.bbf.group_rectangles(bb, 1);</pre>
</div>
<p>Here is the result we get with our image:</p>
<div class="mediaobject"><img src="../Images/image00122.jpeg" alt="Brightness binary features"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>For the first part, we took the result without rectangle grouping and for the second, with it. As for the Haar features, you may select only the most confident results.</p>
<p>In the preceding image, we see that the result performed poorly compared to the Haar features. It is hard to say why it gives such result. In many cases, it highly depends on the implementation or classifier training or maybe just on the input image.</p>
<p>We saw two different algorithms, you can select one by your choice. It is probably better to stay with the Haar features, since you will find a lot of realizations of that algorithm. In <a id="id188" class="indexterm"></a>contrast, if you want to extend the Computer Vision practical boundaries, you may want to <a id="id189" class="indexterm"></a>tune the BBF implementation or just write your own. It is all in your hands!</p>
</div>
</div>
</div>


<div class="section" title="Tagging people with tracking.js" id="aid-SJGS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"></a>Tagging people with tracking.js</h1>
</div>
</div>
</div>
<p>To see more about Haar-like features and its implementation, we will discuss tracking.js library. It<a id="id190" class="indexterm"></a> provides nearly the same functionality as the JSFeat library. What is interesting is that it supplies classifiers for other different objects, for example, face particles. Eventually, we will see how to make it possible to tag friends.</p>
<div class="section" title="Haar features with tracking.js"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"></a>Haar features with tracking.js</h2>
</div>
</div>
</div>
<p>Tracking.js <a id="id191" class="indexterm"></a>provides the functionality to detect not only a face, but various face particles too. It is very<a id="id192" class="indexterm"></a> easy to do that. You need to perform the following steps:</p>
<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, you need to add object files for what you want to detect:<div class="informalexample"><pre class="programlisting">&lt;script src="/path/to/face.js"&gt;&lt;/script&gt;
&lt;script src="/path/to/eye.js"&gt;&lt;/script&gt;
&lt;script src="/path/to/mouth.js"&gt;&lt;/script&gt;</pre>
</div></li>
<li class="listitem">Next, initialize the <code class="literal">ObjectTracker</code> function. We did not discuss this in the previous chapter, since it is mostly focused on face detection, not just a regular object. Anyway, we initialize it with the names of the objects we want to track:<div class="informalexample"><pre class="programlisting">var tracker = new tracking.ObjectTracker(['face', 'eye', 'mouth']);</pre>
</div></li>
<li class="listitem">There are also custom functions that you can call. One of them is the <code class="literal">setStepSize</code> function, which sets the step size for a sliding window or how it is called in the tracking.js library block:<div class="informalexample"><pre class="programlisting">tracker.setStepSize(1.2);</pre>
</div></li>
<li class="listitem">We then define the postprocessing function. What we need is to plot our result on a canvas:<div class="informalexample"><pre class="programlisting">tracker.on('track', function (event) {
    event.data.forEach(function (rect) {
        plot(rect.x, rect.y, rect.width, rect.height);
    });
});</pre>
</div><p>We also need the <code class="literal">plot</code> function itself:</p><div class="informalexample"><pre class="programlisting">var canvas = document.getElementById('initCanvas');
var context = canvas.getContext('2d');
function plot(x, y, w, h) {
    context.lineWidth = 3;
    context.globalAlpha = 0.8;
    context.strokeStyle = "#fff";
    context.strokeRect(x, y, w, h);
}</pre>
</div></li>
</ol>
<div style="height:10px; width: 1px"></div>
</div>
<p>As we mentioned, there is<a id="id193" class="indexterm"></a> no functionality to plot different objects with distinct colors. For now, you <a id="id194" class="indexterm"></a>can operate with different objects by creating several different trackers at once. Eventually, the last thing you need to do is to call the <code class="literal">track</code> function on a canvas:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#initCanvas', tracker);</pre>
</div>
<p>There are various functions that you can use:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">setEdgesDensity</code>: This is the same as in the JSFeat library, you just set a threshold for a sliding window edge's density. It may significantly improve the result; the higher the value, the more edges a window needs to contain to be a candidate for an object we want to find.</li>
<li class="listitem"><code class="literal">setInitialScale</code>: This is the initial scale for a sliding window.</li>
<li class="listitem"><code class="literal">setScaleFactor</code>: This is the scale factor for the sliding window.</li>
</ul>
</div>
<p>Using these functions, you can tune the algorithm a bit to get a better result.</p>
<p>We tested the algorithm by applying three detectors one-by-one. For the face, eye, and mouth we used red, blue, and green colors, respectively. Here is the result:</p>
<div class="mediaobject"><img src="../Images/image00123.jpeg" alt="Haar features with tracking.js"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can see, the result for faces is much better than those for face particles. This can be due to the <a id="id195" class="indexterm"></a>bad lighting <a id="id196" class="indexterm"></a>conditions or poorly trained classifier.</p>
</div>
</div>


<div class="section" title="Tagging people in photos" id="aid-TI1E1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"></a>Tagging people in photos</h1>
</div>
</div>
</div>
<p>Tagging people in photos is a common procedure that you use a lot in social networks. If you <a id="id197" class="indexterm"></a>want to create similar functionality on your website, the JavaScript world can offer something for you. Actually, you can do that with any library which provides face detection methods, you just need to write some additional methods. To simplify the code, we will follow an example from the tracking.js library. It is easy to understand and implement:</p>
<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we need to place our image to the HTML code:<div class="informalexample"><pre class="programlisting">&lt;div id="photo"&gt;&lt;img id="img" src="/path/to/your/image.jpg"/&gt;&lt;/div&gt;</pre>
</div></li>
<li class="listitem">Here is an array that holds all the names that need to be tagged:<div class="informalexample"><pre class="programlisting">var theBeatles = ['George Harrison', 'John Lennon', 'Ringo Starr', 'Paul McCartney'];</pre>
</div></li>
<li class="listitem">Then, we start from initializing our <code class="literal">ObjectTracker</code> function with a <code class="literal">face</code> object:<div class="informalexample"><pre class="programlisting">var tracker = new tracking.ObjectTracker('face');</pre>
</div></li>
<li class="listitem">The whole magic goes on in a post processing function:<div class="informalexample"><pre class="programlisting">tracker.on('track', function (event) {
    var data = event.data;
    data.sort(function (a, b) {
        return b.x – a.x;
    });
    data = data.filter(function (el) {
        return el.width &gt;= 50;
    });
    data.forEach(function (rect) {
        tag(rect.x, rect.y, rect.width, rect.height);
    });
});</pre>
</div></li>
<li class="listitem">Let's review it a bit. First, we sort all rectangles by x coordinates, it will be much easier to plot the result when we know the order of detections.</li>
<li class="listitem">Next, we filter our object array and skip all detections in which width is less then must be "50" pixels. That will help us to omit background or noisy detections. Moreover, we <a id="id198" class="indexterm"></a>present a new <code class="literal">tag </code>function, which will tag all detections on a photo. See the following code:<div class="informalexample"><pre class="programlisting">var img = document.getElementById('img');
var tag = function (x, y, w, h) {
    var rect = document.createElement('div');    
    var input = document.createElement('input');

    input.value = theBeatles.pop();
    rect.appendChild(input);
    imageContainer.appendChild(rect);
    rect.style.width = w + 'px';
    rect.style.height = h + 'px';
    rect.style.left = (img.offsetLeft + x) + 'px';
    rect.style.top = (img.offsetTop + y) + 'px';
    rect.style.position = 'absolute';
    rect.style.border = '3px solid white';
};</pre>
</div><p>The function creates an <code class="literal">&lt;input&gt;</code> tag for a name, then takes the first element of an array and appends the input element to the <code class="literal">&lt;div&gt;</code> rectangle.</p></li>
<li class="listitem">The last thing we need to do is to call our tracker on an image:<div class="informalexample"><pre class="programlisting">tracking.track(img, tracker);</pre>
</div></li>
</ol>
<div style="height:10px; width: 1px"></div>
</div>
<p>Here is the result:</p>
<div class="mediaobject"><img src="../Images/image00124.jpeg" alt="Tagging people in photos"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can see, we have successfully removed the background detection and tagged all four people in the<a id="id199" class="indexterm"></a> correct order.</p>
</div>


<div class="section" title="Head tracking with Camshift"><div class="titlepage" id="aid-UGI02"><div><div><h1 class="title"><a id="ch04lvl1sec30"></a>Head tracking with Camshift</h1>
</div>
</div>
</div>
<p>Head tracking is another huge topic in the field of Computer Vision. It is very useful when you want to <a id="id200" class="indexterm"></a>create a<a id="id201" class="indexterm"></a> human computer interface. For example, it is usually used in web browser games to move objects or control a 3D interface. There are differences between object detection and tracking. First of all, tracking works only on videos, since you track an object (not reestimate) a new instance in each frame. Consequently, we need to assume that the object we track is the same as it was on the previous frame.</p>
<p>Tracking can be done for multiple objects but here we will focus on a single object, in our case, it is a head or more precisely—face. There is a wonderful library that can help us to track it. It is called <a id="id202" class="indexterm"></a>headtrackr (<a class="ulink" href="https://github.com/auduno/headtrackr">https://github.com/auduno/headtrackr</a>). In addition to face tracking, it provides a functionality to create an interface that helps to control your browser applications using head motion. We will not focus on the motion estimation part here, since the chapter is focused on face detection and tracking. But do not worry, we will get to that in the next chapter. First, we will see how the tracking algorithm works and then we will focus on its practical examples.</p>
<div class="section" title="The idea behind head tracking"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"></a>The idea behind head tracking</h2>
</div>
</div>
</div>
<p>There are many object tracking algorithms but most of them are not suitable for JavaScript and web<a id="id203" class="indexterm"></a> browsers due to computational complexity. For Haar features, it is very difficult to apply them in a rotation-invariant manner because when you do that for several skewed images, the algorithm becomes non-real time. The headtrackr library tends to solve that problem. It introduces a framework that can help you to track a face. Its main focus is creating a human interface, but it provides enough flexibility to use it for other tasks as well.</p>
<p>How does the tracking work? Suppose you have found an object on an initial frame, for example, using Haar features or another method. We can work with a video file or just a webcam. In that case, the difference between neighboring frames will not be that huge. These are our core assumptions, let's move on.</p>
<p>We will talk here about objects, not just a face. Let's assume that our object is a group of points and we want to find that group on the next frame. Look at the following image:</p>
<div class="mediaobject"><img src="../Images/image00125.jpeg" alt="The idea behind head tracking"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The circle (window) <span class="strong"><strong>C1</strong></span> is the location of an object on a previous frame. The circle <span class="strong"><strong>C2</strong></span> binds the group <a id="id204" class="indexterm"></a>of points that we want to find. f we get a sum of them in the <span class="strong"><strong>C2</strong></span> circle by adding the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> coordinates and dividing their sums by the number of points in that circle, we will get point <span class="strong"><strong>c_n</strong></span>, which is called <span class="strong"><strong>centroid</strong></span>. After you find the centroid, we move the start circle center <span class="strong"><strong>c_s</strong></span> to the new center <span class="strong"><strong>c_n</strong></span>. The algorithm continues the iterating process by finding a new centroid until it converges in the end center <span class="strong"><strong>c_e</strong></span>. You have found the position of our object on a new frame! This algorithm of finding centers of point densities is called <a id="id205" class="indexterm"></a>
<span class="strong"><strong>Meanshift</strong></span>.</p>
<p>How can we get the density for the Meanshift algorithm when we use a face? The common approach is to generate a skin map, as shown in the following image:</p>
<div class="mediaobject"><img src="../Images/image00126.jpeg" alt="The idea behind head tracking"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>In the right-hand side image, each pixel represents the probability of this pixel being a skin point. Let's call it a density picture. We get a centroid location using these intensity points in a window.</p>
<p>Can you see a problem with the Meanshift approach? We are not changing the size of a window. What if an object gets closer or further from the camera? We need to adapt the size of an object somehow. This issue was solved by the <span class="strong"><strong>CAMshift</strong></span> (<span class="strong"><strong>Continuously Adaptive Meanshift</strong></span>) algorithm. The first stage of the algorithm is the Meanshift<a id="id206" class="indexterm"></a> approach. When we find the window with the highest density, the Camshift algorithm updates the window size based on the sum of the intensity values in that window. The higher the intensity and the more nonzero points in a window, the larger<a id="id207" class="indexterm"></a> the output size will be. After all, the window converges to the required object. Moreover, the algorithm provides computation of a possible head rotation using the density picture.</p>
<p>See the following image:</p>
<div class="mediaobject"><img src="../Images/image00127.jpeg" alt="The idea behind head tracking"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The first one shows the original rectangle (the smaller one) and the final detection by Camshift. The right-hand side image shows the result after the rectangle angle calculation.</p>
<p>The headtrackr library can initialize the Meanshift algorithm in two ways:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The user manually selects an object on a video and the tracking is done using the user input</li>
<li class="listitem">The algorithm can use Haar features to detect the face to be tracked</li>
</ul>
</div>
<p>We will see an example with the second approach, when a face is detected automatically for the first frame using Haar features, and for the other frames, the library uses the Camshift approach.</p>
</div>
<div class="section" title="The head tracking application"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec34"></a>The head tracking application</h2>
</div>
</div>
</div>
<p>It is relatively easy to use the headtrackr library. It provides a flexible way to create a head tracking <a id="id208" class="indexterm"></a>application. We will discuss the APIs and opportunities it provides:</p>
<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first thing we need to do is to add a headtrackr script. The Haar detector is already included there:<div class="informalexample"><pre class="programlisting">&lt;script src="js/headtrackr.js"&gt;&lt;/script&gt;</pre>
</div></li>
<li class="listitem">Next, we need to define HTML inputs so that we can easily display the content:<div class="informalexample"><pre class="programlisting">&lt;canvas id="buffer" width="320" height="240" style="display:none"&gt;&lt;/canvas&gt;
&lt;video id="inputVideo" autoplay loop width="320" height="240"&gt;&lt;/video&gt;
&lt;canvas id="overlay" width="320" height="240"&gt;&lt;/canvas&gt;
&lt;canvas id="debug" width="320" height="240"&gt;&lt;/canvas&gt;</pre>
</div><p>The first one will hold the data required for the library. The video file will hold the video. The other two tags are optional, the third one provides a canvas to draw the tracking result rectangle on it. The last is used to display the density image.</p></li>
<li class="listitem">If you want, you can add a tag for the headtrackr output text, which will display various messages during the working process, so you can understand the stage at which the tracker is:<div class="informalexample"><pre class="programlisting">&lt;span id='headtrackerMessage'&gt;&lt;/span&gt;</pre>
</div></li>
<li class="listitem">After that, we need to get all the necessary data on the JavaScript side:<div class="informalexample"><pre class="programlisting">var canvasInput = document.getElementById('buffer');
var videoInput = document.getElementById('inputVideo');
var canvasOverlay = document.getElementById('overlay');
var overlayContext = canvasOverlay.getContext('2d');
var debugOverlay = document.getElementById('debug');
canvasOverlay.style.position = "absolute";
canvasOverlay.style.top = '0px';
canvasOverlay.style.zIndex = '100001';
canvasOverlay.style.display = 'block';</pre>
</div><p>The canvas should be above the video, so we set its style to be so.</p></li>
<li class="listitem">Next, you need to initialize tracker parameters:<div class="informalexample"><pre class="programlisting">var htracker = new headtrackr.Tracker({
    altVideo: {webm: "/path/to/your/video.webm"},
    calcAngles: true,
    ui: true,
    debug: debugOverlay
});</pre>
</div><p>There are a lot of parameters, we will focus on those which are useful in this example. By default, the headtrackr library works with a web camera. If you do not have one or your browser does not support it, you can provide a video file using the <code class="literal">altVideo</code> parameter. To calculate the head angle, we use the <code class="literal">calcAngles</code> variable, which is <code class="literal">false</code> by default. The <code class="literal">ui</code> parameter sets debugging messages for a tag with the <code class="literal">headtrackerMessage</code> id. For a density image, we need to set the <code class="literal">debug</code> parameter.</p></li>
<li class="listitem">Next, we init the tracker with a video and canvas inputs. Then, we start the tracker:<div class="informalexample"><pre class="programlisting">htracker.init(videoInput, canvasInput);
htracker.start();</pre>
</div></li>
<li class="listitem">To stop the tracking process, you can use the <code class="literal">stop</code> function. In that case, the library will reinitiate the whole process:<div class="informalexample"><pre class="programlisting">htracker.stop();</pre>
</div></li>
<li class="listitem">To display the result using overlay, we need to add a listener to the <code class="literal">facetrackingEvent</code>. Besides, you can see how we get the rotated version of a rectangle:<div class="informalexample"><pre class="programlisting">document.addEventListener("facetrackingEvent", function (event) {
    overlayContext.clearRect(0, 0, 320, 240);
    if (event.detection == "CS") {
        overlayContext.translate(event.x, event.y);
        overlayContext.rotate(event.angle - (Math.PI / 2));
        overlayContext.strokeStyle = "#00CC00";
        overlayContext.strokeRect((-event.width / 2) &gt;&gt; 0, (-event.height / 2) &gt;&gt; 0, event.width, event.height);
        overlayContext.rotate((Math.PI / 2) - event.angle);
        overlayContext.translate(-event.x, -event.y);
    }
});</pre>
</div></li>
</ol>
<div style="height:10px; width: 1px"></div>
</div>
<p>The result includes a video with the overlay over it and the debug information on the right:</p>
<div class="mediaobject"><img src="../Images/image00128.jpeg" alt="The head tracking application"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can see, there is <a id="id209" class="indexterm"></a>nothing difficult in applying the head tracking with that library. To use the library in a proper manner, you just need to know some parts of the algorithm.</p>
</div>
</div>


<div class="section" title="Summary" id="aid-VF2I1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"></a>Summary</h1>
</div>
</div>
</div>
<p>A face is a really complex object. To detect and track it, you need to use a new level of algorithms. Fortunately, the JavaScript libraries provide such an opportunity through Haar, Brightness Binary features, Meanshift, and Camshift algorithms. All of them have their own area of usage. You can apply these wonderful methods in different programs, for example, people tagging. We discussed them and provided examples which you can start using right away. In addition to face detection, there is a potential to detect other objects such as face particles. Of course, the detection quality may vary significantly and you should be careful when you use other classifiers.</p>
<p>In this chapter, we already touched on the tracking applications a bit and discussed how the tracking can help to create a human interface. In the next chapter, we will learn how to control your browser with motion and how the object tracking can be used in those applications.</p>
</div>
</body>
</html>