- en: Finding Objects via Feature Matching and Perspective Transforms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过特征匹配和透视变换查找对象
- en: In the previous chapter, you learned how to detect and track a simple object
    (the silhouette of a hand) in a very controlled environment. To be more specific,
    we instructed the user of our app to place the hand in the central region of the
    screen and then made assumptions about the size and shape of the object (the hand).
    In this chapter, we want to detect and track objects of arbitrary sizes, possibly
    viewed from several different angles or under partial occlusion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何在非常受控的环境中检测和跟踪一个简单对象（手的轮廓）。具体来说，我们指导我们的应用程序用户将手放置在屏幕中央区域，然后对对象（手）的大小和形状做出了假设。在本章中，我们希望检测和跟踪任意大小的对象，这些对象可能从几个不同的角度或部分遮挡下被观察。
- en: For this, we will make use of feature descriptors, which are a way of capturing
    the important properties of our object of interest. We do this so that the object
    can be located even when it is embedded in a busy visual scene. We will apply
    our algorithm to the live stream of a webcam and do our best to keep the algorithm
    robust yet simple enough to run in real time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用特征描述符，这是一种捕捉我们感兴趣对象重要属性的方法。我们这样做是为了即使对象嵌入在繁忙的视觉场景中，也能定位到该对象。我们将把我们的算法应用于网络摄像头的实时流，并尽力使算法既鲁棒又足够简单，以便实时运行。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Listing the tasks performed by the app
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出应用程序执行的任务
- en: Planning the app
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划应用程序
- en: Setting up the app
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置应用程序
- en: Understanding the process flow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解流程过程
- en: Learning feature extraction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习特征提取
- en: Looking at feature detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察特征检测
- en: Understanding feature descriptors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征描述符
- en: Understanding feature matching
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征匹配
- en: Learning feature tracking
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习特征跟踪
- en: Seeing the algorithm in action
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察算法的实际应用
- en: The goal of this chapter is to develop an app that can detect and track an object
    of interest in the video stream of a webcam—even if the object is viewed from
    different angles or distances or under partial occlusion. Such an object can be
    the cover image of a book, a drawing, or anything else that has a sophisticated
    surface structure.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是开发一个应用程序，可以在网络摄像头的视频流中检测和跟踪感兴趣的对象——即使对象从不同的角度、距离或部分遮挡下观察。这样的对象可以是书的封面图像、一幅画或任何具有复杂表面结构的其他东西。
- en: Once the template image is provided, the app will be able to detect that object,
    estimate its boundaries, and then track it in the video stream.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提供了模板图像，应用程序将能够检测该对象，估计其边界，然后在视频流中跟踪它。
- en: Getting started
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习
- en: This chapter has been tested with **OpenCV 4.1.1**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已在**OpenCV 4.1.1**上进行了测试。
- en: Note that you might have to obtain the so-called extra modules from [https://github.com/Itseez/opencv_contrib](https://github.com/Itseez/opencv_contrib).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可能需要从[https://github.com/Itseez/opencv_contrib](https://github.com/Itseez/opencv_contrib)获取所谓的额外模块。
- en: We install OpenCV with `OPENCV_ENABLE_NONFREE` and the `OPENCV_EXTRA_MODULES_PATH` variable
    set in order to get **Speeded-Up Robust Features** (**SURF**) and the **Fast Library
    for Approximate Nearest Neighbors** (**FLANN**) installed. You can also use the
    Docker files available in the repository, which contain all the required installations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置`OPENCV_ENABLE_NONFREE`和`OPENCV_EXTRA_MODULES_PATH`变量来安装OpenCV，以获取**加速鲁棒特征**（**SURF**）和**快速近似最近邻库**（**FLANN**）。你还可以使用存储库中可用的Docker文件，这些文件包含所有必需的安装。
- en: Additionally, note that you may have to obtain a license to use **SURF** in
    commercial applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，你可能需要获得许可证才能在商业应用程序中使用**SURF**。
- en: The code for this chapter can be found in the GitHub book repository available
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter3](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter3).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在GitHub书籍存储库中找到，该存储库位于[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter3](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter3)。
- en: Listing the tasks performed by the app
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出应用程序执行的任务
- en: 'The app will analyze each captured frame to perform the following tasks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序将分析每个捕获的帧以执行以下任务：
- en: '**Feature extraction**: We will describe an object of interest with **Speeded-Up
    Robust Features** (**SURF**), which is an algorithm used to find distinctive keypoints in
    an image that are both scale-invariant and rotation invariant. These keypoints
    will help us to make sure that we are tracking the right object over multiple
    frames because the appearance of the object might change from time to time. It
    is important to find keypoints that do not depend on the viewing distance or viewing
    angle of the object (hence, the scale and rotation invariance).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：我们将使用**加速鲁棒特征**（**SURF**）来描述感兴趣的物体，这是一种用于在图像中找到既具有尺度不变性又具有旋转不变性的显著关键点的算法。这些关键点将帮助我们确保在多个帧中跟踪正确的物体，因为物体的外观可能会随时间而变化。找到不依赖于物体观看距离或观看角度的关键点非常重要（因此，具有尺度和旋转不变性）。'
- en: '**Feature matching**: We will try to establish a correspondence between keypoints
    using the **Fast Library for Approximate Nearest Neighbors** (**FLANN**) to see
    whether a frame contains keypoints similar to the keypoints from our object of
    interest. If we find a good match, we will mark the object on each frame.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征匹配**：我们将尝试使用**快速近似最近邻库**（**FLANN**）来建立关键点之间的对应关系，以查看帧中是否包含与我们的感兴趣物体中的关键点相似的关键点。如果我们找到一个好的匹配，我们将在每一帧上标记该物体。'
- en: '**Feature tracking**: We will keep track of the located object of interest
    from frame to frame using various forms of **early** **outlier detection** and
    **outlier rejection** to speed up the algorithm.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征跟踪**：我们将使用各种形式的**早期****异常检测**和**异常拒绝**来跟踪从帧到帧的定位感兴趣物体，以加快算法的速度。'
- en: '**Perspective transform**: We will then reverse any translations and rotations
    that the object has undergone by **warping the perspective** so that the object
    appears upright in the center of the screen. This creates a cool effect in which
    the object seems frozen in a position while the entire surrounding scene rotates
    around it.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透视变换**：我们将通过**扭曲透视**来反转物体所经历的任何平移和旋转，使得物体在屏幕中心看起来是垂直的。这会产生一种酷炫的效果，物体似乎被冻结在某个位置，而整个周围场景则围绕它旋转。'
- en: 'An example of the first three steps, namely, the feature extraction, matching,
    and tracking is shown in the following screenshot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前三个步骤的示例，即特征提取、匹配和跟踪：
- en: '![](img/a5306700-6259-401a-8bee-8823fa90ff4c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a5306700-6259-401a-8bee-8823fa90ff4c.png)'
- en: The screenshot contains a template image of our object of interest on the left
    and a handheld printout of the template image on the right. Matching features
    in the two frames are connected with blue lines, and the located object is outlined
    in green on the right.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 截图中包含左边的感兴趣物体的模板图像和右边的手持打印模板图像。两个帧中的匹配特征用蓝色线条连接，右边的定位物体用绿色轮廓标出。
- en: 'The last step is to transform the located object so that it is projected onto
    the frontal plane, as depicted in the following photograph:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将定位的物体转换，使其投影到正面平面上，如图中所示：
- en: '![](img/07ffe9d6-54e5-4355-9322-6ddd5afb49a8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/07ffe9d6-54e5-4355-9322-6ddd5afb49a8.png)'
- en: The image looks roughly like the original template image, appearing close-up,
    while the entire scene seems to warp around it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像看起来大致与原始模板图像相似，呈现近距离视图，而整个场景似乎围绕它扭曲。
- en: Let's first plan the application that we are going to create in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先规划本章将要创建的应用程序。
- en: Planning the app
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划应用
- en: The final app will consist of a Python class for detecting, matching, and tracking
    image features, as well as a script that accesses the webcam and displays each
    processed frame.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用程序将包括一个用于检测、匹配和跟踪图像特征的Python类，以及一个访问摄像头并显示每个处理帧的脚本。
- en: 'The project will contain the following modules and scripts:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目将包含以下模块和脚本：
- en: '`feature_matching`: This module contains an algorithm for feature extraction,
    feature matching, and feature tracking. We separate this algorithm from the rest
    of the application so that it can be used as a standalone module.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_matching`：此模块包含特征提取、特征匹配和特征跟踪的算法。我们将此算法从应用程序的其余部分分离出来，以便它可以作为一个独立的模块使用。'
- en: '`feature_matching.FeatureMatching`: This class implements the entire feature-matching
    process flow. It accepts a **Blue, Green, Red (BGR)** camera frame and tries to
    locate an object of interest in it.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_matching.FeatureMatching`：这个类实现了整个特征匹配流程。它接受一个**蓝、绿、红（BGR）**相机帧，并尝试在其中定位感兴趣的物体。'
- en: '`chapter3`: This is the main script for the chapter.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter3`：这是该章节的主要脚本。'
- en: '`chapter3.main`: This is the main function routine for starting the application,
    accessing the camera, sending each frame for processing to an instance of the `FeatureMatching` class,
    and for displaying results.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter3.main`：这是启动应用程序、访问相机、将每一帧发送到`FeatureMatching`类的实例进行处理的主体函数，以及显示结果的主要函数。'
- en: Let's set up the application before going into the details of the feature-matching
    algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入到特征匹配算法的细节之前，让我们设置应用。
- en: Setting up the app
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置应用
- en: Before we can get down to the nitty-gritty of our feature-matching algorithm,
    we need to make sure that we can access the webcam and display the video stream.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入到特征匹配算法的细节之前，我们需要确保我们可以访问网络摄像头并显示视频流。
- en: Let's learn how to run the application in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中学习如何运行应用程序。
- en: Running the app – the main() function routine
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行应用——`main()`函数主体
- en: 'To run our app, we will need to execute the `main()` function routine. The
    following steps show us the execution of `main()` routine:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行我们的应用，我们需要执行`main()`函数主体。以下步骤显示了`main()`函数的执行：
- en: 'The function first accesses the webcam with the `VideoCapture` method by passing `0` as
    an argument, which is a reference to the default webcam. If it can not access
    the webcam, the app will be terminated:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数首先使用`VideoCapture`方法通过传递`0`作为参数来访问网络摄像头，这是一个默认网络摄像头的引用。如果无法访问网络摄像头，应用将被终止：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, the desired frame size and frame per second of the video stream is set.
    The following snippet shows the code for setting the frame size and frame per
    second of the video:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，设置视频流的帧大小和每秒帧数。以下代码片段显示了设置视频帧大小和帧率的代码：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, an instance of the `FeatureMatching` class is initialized with a path
    to a template (or training) file that depicts the object of interest. The following
    code shows the `FeatureMatching` class:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用指向模板（或训练）文件的路径初始化`FeatureMatching`类的实例，该文件描述了感兴趣的对象。以下代码显示了`FeatureMatching`类：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After that, to process the frames from the camera, we create an iterator from
    the `capture.read` function, which will terminate when the function fails to return
    frame (`(False,None)`). This can be seen in the following code block:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，为了处理来自相机的帧，我们创建了一个从`capture.read`函数的迭代器，该迭代器将在无法返回帧时终止（返回`(False,None)`）。这可以在以下代码块中看到：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the previous code block, the `FeatureMatching.match` method processes the
    **BGR** image (`capture.read` returns `frame` in BGR format). If the object is
    detected in the current frame, the `match` method will report `match_success=True` and
    return the warped image as well as the image that illustrates the matches—`img_flann`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，`FeatureMatching.match`方法处理**BGR**图像（`capture.read`返回的`frame`为BGR格式）。如果当前帧中检测到对象，`match`方法将报告`match_success=True`并返回扭曲的图像以及说明匹配的图像——`img_flann`。
- en: Let's move on and display the results in which our match method will return.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续，并显示我们的匹配方法将返回的结果。
- en: Displaying results
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示结果
- en: 'In fact, we can display the results only if the `match` method returns a result,
    right? This can be seen in the following block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们只能在`match`方法返回结果的情况下显示结果，对吧？这可以在下面的代码块中看到：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Displaying images in OpenCV is straightforward and is done by the `imshow` method,
    which accepts the name of a window and an image. Additionally, loop termination
    criteria on the *Esc *keypress are set.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中显示图像是直接的，通过`imshow`方法完成，该方法接受窗口名称和图像。此外，设置了基于*Esc*按键的循环终止条件。
- en: Now that we have set up our app, let's take a look at the process flow in the
    next section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了我们的应用，让我们看看下一节中的流程图。
- en: Understanding the process flow
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解流程
- en: Features are extracted, matched, and tracked by the `FeatureMatching` class—especially
    by the public `match` method. However, before we can begin analyzing the incoming
    video stream, we have some homework to do. It might not be clear right away what
    some of these things mean (especially for SURF and FLANN), but we will discuss
    these steps in detail in the following sections.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 特征由`FeatureMatching`类提取、匹配和跟踪——特别是通过公共的`match`方法。然而，在我们开始分析传入的视频流之前，我们还有一些作业要做。这些事情的含义可能一开始并不清楚（特别是对于SURF和FLANN），但我们将详细讨论以下章节中的这些步骤。
- en: 'For now, we only have to worry about initialization:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只需要关注初始化：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following steps cover the initialization process:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤涵盖了初始化过程：
- en: 'The following line sets up a SURF detector, which we will use for detecting
    and extracting features from images (see the *Learning feature extraction* section
    for further details), with a Hessian threshold between 300 and 500, that is, `400`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下行设置了一个SURF检测器，我们将使用它来检测和从图像中提取特征（有关更多详细信息，请参阅*学习特征提取*部分），Hessian阈值为300到500，即`400`：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We load a template of our object of interest (`self.img_obj`), or print an
    error if it cannot be found:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载我们感兴趣的对象的模板（`self.img_obj`），或者在找不到时打印错误信息：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Also, we store the shape of the image (`self.sh_train`) for convenience:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们存储图像的形状（`self.sh_train`）以方便使用：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will call the template image the **train image**, as our algorithm will
    be trained to find this image, and every incoming frame a **query image**, as
    we will use these images to query the **train image**. The following photograph
    is the train image:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模板图像称为**训练图像**，因为我们的算法将被训练以找到此图像，而每个输入帧称为**查询图像**，因为我们将使用这些图像来查询**训练图像**。以下照片是训练图像：
- en: '![](img/b9a54a7d-1b81-4d52-9efb-541bd1f28003.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9a54a7d-1b81-4d52-9efb-541bd1f28003.png)'
- en: Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图像版权——Lenna.png由Conor Lawless提供，许可协议为CC BY 2.0
- en: The previous train image has a size of 512 x 512 pixels and will be used to
    train the algorithm.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的训练图像大小为512 x 512像素，将用于训练算法。
- en: 'Next, we apply SURF to the object of interest. This can be done with a convenient
    function call that returns both a list of keypoints and the descriptor (you can
    refer to the *Learning feature extraction* section for further explanation):'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将SURF应用于感兴趣的对象。这可以通过一个方便的函数调用完成，该调用返回关键点和描述符（你可以参阅*学习特征提取*部分以获得进一步解释）：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We will do the same with each incoming frame and then compare lists of features
    across images.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对每个输入帧做同样的事情，然后比较图像之间的特征列表。
- en: 'Now, we set up a FLANN object that will be used to match the features of the
    train and query images (refer to the *Understanding feature matching* section
    for further details). This requires the specification of some additional parameters
    via dictionaries, such as which algorithm to use and how many trees to run in
    parallel:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们设置一个FLANN对象，该对象将用于匹配训练图像和查询图像的特征（有关更多详细信息，请参阅*理解特征匹配*部分）。这需要通过字典指定一些额外的参数，例如使用哪种算法以及并行运行多少棵树：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, initialize some additional bookkeeping variables. These will come
    in handy when we want to make our feature tracking both faster and more accurate.
    For example, we will keep track of the latest computed homography matrix and of
    the number of frames we have spent without locating our object of interest (refer
    to the *Learning feature tracking* section for more details):'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，初始化一些额外的记账变量。当我们想要使我们的特征跟踪既快又准确时，这些变量将很有用。例如，我们将跟踪最新的计算出的单应性矩阵和未定位我们感兴趣的对象的帧数（有关更多详细信息，请参阅*学习特征跟踪*部分）：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, the bulk of the work is done by the **`FeatureMatching.match` **method. This
    method follows the procedure elaborated here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，大部分工作由**`FeatureMatching.match`**方法完成。该方法遵循以下程序：
- en: It extracts interesting image features from each incoming video frame.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从每个输入视频帧中提取有趣的图像特征。
- en: It matches features between the template image and the video frame. This is
    done in `FeatureMatching.match_features`. If no such match is found, it skips
    to the next frame.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在模板图像和视频帧之间匹配特征。这是在`FeatureMatching.match_features`中完成的。如果没有找到此类匹配，它将跳到下一帧。
- en: It finds the corner points of the template image in the video frame. This is
    done in the `detect_corner_points` function. If any of the corners lie (significantly)
    outside the frame, it skips to the next frame.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在视频帧中找到模板图像的角点。这是在`detect_corner_points`函数中完成的。如果任何角点（显著）位于帧外，它将跳到下一帧。
- en: It calculates the area of the quadrilateral that the four corner points span.
    If the area is either too small or too large, it skips to the next frame.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算四个角点所围成的四边形的面积。如果面积要么太小要么太大，它将跳到下一帧。
- en: It outlines the corner points of the template image in the current frame.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在当前帧中勾勒出模板图像的角点。
- en: It finds the perspective transform that is necessary to bring the located object
    from the current frame to the `frontoparallel` plane. If the result is significantly
    different from the result we got recently for an earlier frame, it skips to the
    next frame.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它找到必要的透视变换，将定位的对象从当前帧带到`frontoparallel`平面。如果结果与最近对早期帧的结果显著不同，它将跳到下一帧。
- en: It warps the perspective of the current frame to make the object of interest
    appear centered and upright.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将当前帧的视角进行扭曲，使得感兴趣的对象居中且垂直。
- en: In the following sections, we will discuss the previous steps in detail.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将详细讨论之前的步骤。
- en: Let's first take a look at the feature extraction step in the next section.
    This step is the core of our algorithm. It will find informative areas in the
    image and represent them in a lower dimensionality so that we can use those representations
    afterward to decide whether two images contain similar features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看下一节中的特征提取步骤。这一步是算法的核心。它将在图像中找到信息丰富的区域，并以较低维度表示它们，这样我们就可以在之后使用这些表示来决定两张图像是否包含相似的特征。
- en: Learning feature extraction
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习特征提取
- en: Generally speaking, in machine learning, feature extraction is a process of
    dimensionality reduction of the data that results in an informative description
    of a data element.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，在机器学习中，特征提取是一个数据降维的过程，它导致对数据元素的信息丰富描述。
- en: In computer vision, a feature is usually an *interesting area* of an image.
    It is a measurable property of an image that is very informative about what the
    image represents. Usually, the grayscale value of an individual pixel (that is,
    the *raw data*) does not tell us a lot about the image as a whole. Instead, we
    need to derive a property that is more informative.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，特征通常是指图像的*有趣区域*。它是关于图像所代表内容的非常信息丰富的可测量属性。通常，单个像素的灰度值（即*原始数据*）并不能告诉我们太多关于整个图像的信息。相反，我们需要推导出一个更具信息量的属性。
- en: For example, knowing that there are patches in the image that look like eyes,
    a nose, and a mouth will allow us to reason about how likely it is that the image
    represents a face. In this case, the number of resources required to describe
    the data is drastically reduced. The data refers to, for example, whether we are
    seeing an image of a face. Does the image contain two eyes, a nose, or a mouth?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，知道图像中有看起来像眼睛、鼻子和嘴巴的区域，这将使我们能够推断出图像代表脸的可能性有多大。在这种情况下，描述数据所需资源的数量会大幅减少。数据指的是，例如，我们是否看到的是一张脸的图像。图像是否包含两只眼睛、一个鼻子或一个嘴巴？
- en: More low-level features, such as the presence of edges, corners, blobs, or ridges,
    may be more informative generally. Some features may be better than others, depending
    on the application.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更低层的特征，例如边缘、角点、块或脊的存在，通常更具信息量。某些特征可能比其他特征更好，这取决于应用。
- en: Once we have made up our mind about what is our favorite feature, we first need
    to come up with a way to check whether or not the image contains such features.
    Additionally, we need to find out where it contains them and then create a descriptor
    of the feature. Let's learn how to detect features in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定我们最喜欢的特征是什么，我们首先需要想出一个方法来检查图像是否包含这样的特征。此外，我们还需要找出它们在哪里，然后创建特征的描述符。让我们在下一节学习如何检测特征。
- en: Looking at feature detection
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看看特征检测
- en: In computer vision, the process of finding areas of interest in an image is
    called feature detection. Under the hood, for each point of the image, a feature
    detection algorithm decides whether an image point contains a feature of interest.
    OpenCV provides a whole range of feature detection (and description) algorithms.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，寻找图像中感兴趣区域的过程被称为特征检测。在底层，对于图像的每一个点，特征检测算法会决定该图像点是否包含感兴趣的特征。OpenCV提供了一系列的特征检测（及描述）算法。
- en: 'In OpenCV, the details of the algorithms are encapsulated and all of them have
    similar APIs. Here are some of the algorithms:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，算法的细节被封装起来，并且它们都有相似的API。以下是一些算法：
- en: '**Harris corner detection**: We know that edges are areas with high-intensity
    changes in all directions. Harris and Stephens came up with this algorithm, which
    is a fast way of finding such areas. This algorithm is implemented as `cv2.cornerHarris` in
    OpenCV.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Harris角点检测**：我们知道边缘是所有方向上强度变化都很大的区域。Harris和Stephens提出了这个算法，这是一种快速找到这种区域的方法。这个算法在OpenCV中实现为`cv2.cornerHarris`。'
- en: '**Shi-Tomasi corner detection**: Shi and Tomasi developed a corner detection
    algorithm, and this algorithm is usually better than Harris corner detection by
    finding the *N* strongest corners. This algorithm is implemented as `cv2.goodFeaturesToTrack`
    in OpenCV.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shi-Tomasi角点检测**：Shi和Tomasi开发了一种角点检测算法，这个算法通常比Harris角点检测更好，因为它找到了*N*个最强的角点。这个算法在OpenCV中实现为`cv2.goodFeaturesToTrack`。'
- en: '**Scale-Invariant Feature Transform** (**SIFT**): Corner detection is not sufficient
    when the scale of the image changes. To this end, David Lowe developed a method
    to describe keypoints in an image that are independent of orientation and size
    (hence the term **scale-invariant**). The algorithm is implemented as `cv2.xfeatures2d_SIFT` in
    OpenCV2 but has been moved to the *extra* modules in OpenCV3 since its code is
    proprietary.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺度不变特征变换** (**SIFT**)：当图像的尺度发生变化时，角点检测是不够的。为此，David Lowe开发了一种方法来描述图像中的关键点，这些关键点与方向和大小无关（因此得名
    **尺度不变**）。该算法在OpenCV2中实现为 `cv2.xfeatures2d_SIFT`，但由于其代码是专有的，它已经被移动到OpenCV3的 *extra*
    模块中。'
- en: '**SURF**: SIFT has proven to be really good, but it is not fast enough for
    most applications. This is where SURF comes in, which replaces the expensive Laplacian
    of a Gaussian (function) from SIFT with a box filter. The algorithm is implemented
    as `cv2.xfeatures2d_SURF` in OpenCV2, but, like SIFT, it has been moved to the
    *extra* modules in OpenCV3 since its code is proprietary.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SURF**: SIFT 已经被证明是非常好的，但它的速度对于大多数应用来说还不够快。这就是SURF发挥作用的地方，它用箱式滤波器替换了SIFT中昂贵的高斯拉普拉斯（函数）。该算法在OpenCV2中实现为
    `cv2.xfeatures2d_SURF`，但，就像SIFT一样，由于其代码是专有的，它已经被移动到OpenCV3的 *extra* 模块中。'
- en: OpenCV has support for even more feature descriptors, such as **Features from
    Accelerated Segment Test** (**FAST**), **Binary** **Robust Independent Elementary
    Features** (**BRIEF**), and **Oriented FAST and Rotated BRIEF** (**ORB**), the
    latter being an open source alternative to SIFT or SURF.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV支持更多特征描述符，例如 **加速段测试特征** (**FAST**)、**二进制** **鲁棒独立基本特征** (**BRIEF**) 和
    **方向性FAST和旋转BRIEF** (**ORB**)，后者是SIFT或SURF的开源替代品。
- en: In the next section, we'll learn how to use SURF to detect features in an image.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用SURF在图像中检测特征。
- en: Detecting features in an image with SURF
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SURF在图像中检测特征
- en: In the remainder of this chapter, we will make use of the SURF detector. The
    SURF algorithm can be roughly divided into two distinct steps, which are detecting
    points of interest and formulating a descriptor.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用SURF检测器。SURF算法可以大致分为两个不同的步骤，即检测兴趣点和制定描述符。
- en: SURF relies on the Hessian corner detector for interest point detection, which
    requires the setting of a minimal `minhessianThreshold`. This threshold determines
    how large the output from the Hessian filter must be in order for a point to be
    used as an interesting point.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: SURF依赖于Hessian角点检测器进行兴趣点检测，这需要设置一个最小的 `minhessianThreshold`。此阈值决定了Hessian滤波器的输出必须有多大，才能将一个点用作兴趣点。
- en: When the value is larger, fewer interest points are obtained, but they are theoretically
    more salient and vice versa. Feel free to experiment with different values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当值较大时，获取的兴趣点较少，但理论上它们更明显，反之亦然。请随意尝试不同的值。
- en: 'In this chapter, we will choose a value of `400`, as we did earlier in `FeatureMatching.__init__`,
    where we created a SURF descriptor with the following code snippet:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将选择 `400` 这个值，就像我们在之前的 `FeatureMatching.__init__` 中所做的那样，在那里我们使用以下代码片段创建了一个SURF描述符：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The keypoints in the image can be obtained in a single step, which is given
    as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的关键点可以一步获得，如下所示：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, `key_query` is a list of instances of `cv2.KeyPoint` and has the length
    of the number of detected keypoints. Each `KeyPoint` contains information about
    the location (`KeyPoint.pt`), the size ( `KeyPoint.size `), and other useful information
    about our point of interest.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`key_query` 是 `cv2.KeyPoint` 实例的列表，其长度等于检测到的关键点数量。每个 `KeyPoint` 包含有关位置 (`KeyPoint.pt`)、大小
    (`KeyPoint.size`) 以及关于我们感兴趣点的其他有用信息。
- en: 'We can now easily draw the keypoints using the following function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以很容易地使用以下函数绘制关键点：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Depending on an image, the number of detected keypoints can be very large and
    unclear when visualized; we check it with `len(keyQuery)`. If you care only about
    drawing the keypoints, try setting `min_hessian` to a large value until the number
    of returned keypoints provides a good illustration.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图像的不同，检测到的关键点数量可能非常大且在可视化时不清楚；我们使用 `len(keyQuery)` 来检查它。如果你只关心绘制关键点，尝试将 `min_hessian`
    设置为一个较大的值，直到返回的关键点数量提供了一个良好的说明。
- en: Note that SURF is protected by patent laws. Therefore, if you wish to use SURF
    in a commercial application, you will be required to obtain a license.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，SURF受到专利法的保护。因此，如果你希望在商业应用中使用SURF，你将需要获得许可证。
- en: In order to complete our feature extraction algorithm, we need to obtain descriptors
    for our detected keypoints, which we will do in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的特征提取算法，我们需要为检测到的关键点获取描述符，我们将在下一节中这样做。
- en: Obtaining feature descriptors with SURF
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SURF获取特征描述符
- en: 'The process of extracting features from an image with OpenCV using SURF is
    also a single step. It is done by the `compute` method of our feature extractor.
    The latter accepts an image and the keypoints of the image as arguments:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenCV和SURF从图像中提取特征的过程也是一个单步操作。这是通过特征提取器的`compute`方法完成的。后者接受一个图像和图像的关键点作为参数：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, `desc_query` is a `NumPY` ndarray with shape `(num_keypoints, descriptor_size)`. You
    can see that each descriptor is a vector in an *n*-dimensional space (*n*-length
    array of numbers). Each vector describes the corresponding key point and provides
    some meaningful information about our complete image.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`desc_query`是一个形状为`(num_keypoints, descriptor_size)`的`NumPY` ndarray。你可以看到每个描述符都是一个*n*-维空间中的向量（一个*n*-长度的数字数组）。每个向量描述了相应的关键点，并提供了关于我们完整图像的一些有意义的信息。
- en: Hence, we have completed our feature extraction algorithm that had to provide
    meaningful information about our image in reduced dimensionality. It's up to the
    creator of the algorithm to decide what kind of information is contained in the
    descriptor vector, but at the very least the vectors should be such that they
    are closer to similar keypoints than for keypoints that appear different.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经完成了必须提供关于我们图像在降维后的有意义信息的特征提取算法。算法的创建者决定描述符向量中包含什么类型的信息，但至少这些向量应该使得它们比出现在不同关键点的向量更接近相似的关键点。
- en: 'Our feature extraction algorithm also has a convenient method to combine the
    processes of feature detection and descriptor creation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征提取算法还有一个方便的方法来结合特征检测和描述符创建的过程：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It returns both keypoints and descriptors in a single step and accepts a mask
    of an area of interest, which, in our case, is the complete image.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它在单步中返回关键点和描述符，并接受一个感兴趣区域的掩码，在我们的案例中，是整个图像。
- en: As we have extracted our features, the next step is to query and train images
    that contain similar features, which is accomplished by a feature matching algorithm.
    So, let's learn about feature matching in the next section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提取了特征之后，下一步是查询和训练包含相似特征的图像，这是通过特征匹配算法实现的。所以，让我们在下一节学习特征匹配。
- en: Understanding feature matching
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解特征匹配
- en: Once we have extracted features and their descriptors from two (or more) images,
    we can start asking whether some of these features show up in both (or all) images.
    For example, if we have descriptors for both our object of interest (`self.desc_train`)
    and the current video frame (`desc_query`), we can try to find regions of the
    current frame that look like our object of interest.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从两个（或更多）图像中提取了特征及其描述符，我们就可以开始询问这些特征是否出现在两个（或所有）图像中。例如，如果我们对我们的感兴趣对象（`self.desc_train`）和当前视频帧（`desc_query`）都有描述符，我们可以尝试找到当前帧中看起来像我们的感兴趣对象的部分。
- en: 'This is done by the following method, which makes use of FLANN:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过以下方法实现的，它使用了FLANN：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The process of finding frame-to-frame correspondences can be formulated as the
    search for the nearest neighbor from one set of descriptors for every element
    of another set.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找帧间对应关系的过程可以表述为从另一组描述符集中为每个元素寻找最近邻。
- en: The first set of descriptors is usually called the **train set**, because, in
    machine learning, these descriptors are used to train a model, such as the model
    of the object that we want to detect. In our case, the train set corresponds to
    the descriptor of the template image (our object of interest). Hence, we call
    our template image the **train image** (`self.img_train`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组描述符通常被称为**训练集**，因为在机器学习中，这些描述符被用来训练一个模型，例如我们想要检测的对象模型。在我们的案例中，训练集对应于模板图像（我们感兴趣的对象）的描述符。因此，我们称我们的模板图像为**训练图像**（`self.img_train`）。
- en: The second set is usually called the **query set** because we continually ask
    whether it contains our train image. In our case, the query set corresponds to
    the descriptor of each incoming frame. Hence, we call a frame the **query image**
    (`img_query`).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组通常被称为**查询集**，因为我们不断地询问它是否包含我们的训练图像。在我们的案例中，查询集对应于每个输入帧的描述符。因此，我们称一个帧为**查询图像**（`img_query`）。
- en: Features can be matched in any number of ways, for example, with the help of
    a brute-force matcher (`cv2.BFMatcher`) that looks for each descriptor in the
    first set and the closest descriptor in the second set by trying each one (an
    exhaustive search).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以通过多种方式匹配，例如，使用暴力匹配器（`cv2.BFMatcher`）来查找第一集中的每个描述符，并尝试每个描述符以找到第二集中的最近描述符（一种穷举搜索）。
- en: In the next section, we'll learn how to match features across images with FLANN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用FLANN跨图像匹配特征。
- en: Matching features across images with FLANN
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FLANN跨图像匹配特征
- en: 'The alternative is to use an approximate **k-nearest neighbor** (**kNN**) algorithm
    to find correspondences, which is based on the fast third-party library, FLANN.
    A FLANN match is performed with the following code snippet, where we use kNN with
    `k=2`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用基于快速第三方库FLANN的近似**k最近邻**（**kNN**）算法来查找对应关系。以下代码片段展示了如何使用kNN与`k=2`进行匹配：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The result of `flann.knnMatch` is a list of correspondences between two sets
    of descriptors, both contained in the `matches` variable. These are the train
    set, because it corresponds to the pattern image of our object of interest, and
    the query set, because it corresponds to the image in which we are searching for
    our object of interest.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`flann.knnMatch`的结果是两个描述符集之间的对应关系列表，这两个集都包含在`matches`变量中。这些是训练集，因为它对应于我们感兴趣的对象的模式图像，而查询集，因为它对应于我们正在搜索我们感兴趣的对象的图像。'
- en: Now that we have found the nearest neighbors of our features, let's move ahead
    and find out how we can remove outliers in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了特征的最邻近邻居，让我们继续前进，在下一节中找出我们如何去除异常值。
- en: Testing the ratio for outlier removal
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试用于去除异常值的比率
- en: The more correct matches that are found (which means that more pattern-to-image
    correspondences exist), the higher the chance that the pattern is present in the
    image. However, some matches might be false positives.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 找到的正确匹配越多（这意味着模式到图像的对应关系越多），模式出现在图像中的可能性就越高。然而，一些匹配可能是假阳性。
- en: A well-known technique for removing outliers is called the ratio test. Since
    we performed kNN-matching with `k=2`, the two nearest descriptors are returned
    for each match. The first match is the closest neighbor and the second match is
    the second-closest neighbor. Intuitively, a correct match will have a much closer
    first neighbor than its second-closest neighbor. On the other hand, the two closest
    neighbors will be at a similar distance from an incorrect match.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于去除异常值的有效技术称为比率测试。由于我们执行了`k=2`的kNN匹配，因此每个匹配返回两个最近的描述符。第一个匹配是最接近的邻居，第二个匹配是第二接近的邻居。直观上，正确的匹配将比其第二接近的邻居有更接近的第一个邻居。另一方面，两个最近的邻居将距离错误的匹配相似。
- en: 'Therefore, we can find out how good a match is by looking at the difference
    between the distances. Theratio test says that the match is good only if the distance
    ratio between the first match and the second match is smaller than a given number
    (usually around 0.5). In our case, this number is chosen to be `0.7`. The following
    snippet finds good matches:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过观察距离之间的差异来找出匹配的好坏。比率测试表明，只有当第一匹配和第二匹配之间的距离比率小于一个给定的数字（通常约为0.5）时，匹配才是好的。在我们的例子中，这个数字被选为`0.7`。以下代码片段用于找到好的匹配：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To remove all matches that do not satisfy this requirement, we filter the list
    of matches and store the good matches in the `good_matches` list.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去除所有不满足此要求的匹配，我们过滤匹配列表，并将好的匹配存储在`good_matches`列表中。
- en: 'Then, we pass the matches we found to `FeatureMatching.match` so that they
    can be processed further:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将我们找到的匹配传递给`FeatureMatching.match`，以便它们可以进一步处理：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: However, before elaborating on our algorithm, let's first visualize our matches
    in the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们详细阐述我们的算法之前，让我们首先在下一节中可视化我们的匹配。
- en: Visualizing feature matches
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化特征匹配
- en: 'In OpenCV, we can easily draw matches using `cv2.drawMatches`. Here, we create
    our own function for educational purposes as well as for ease of customization
    of function behavior:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，我们可以轻松地使用`cv2.drawMatches`绘制匹配。在这里，我们创建自己的函数，既是为了教育目的，也是为了便于自定义函数行为：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The function accepts two images, namely, in our case, the image of the object
    of interest and the current video frame. It also accepts keypoints from both images
    as well as the matches. It will draw the images next to each other on a single
    illustration image, illustrate the matches on the image, and return the image.
    The latter is achieved with the following steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接受两个图像，在我们的例子中，是感兴趣物体的图像和当前视频帧。它还接受两个图像的关键点和匹配项。它将在单个插图图像上并排放置图像，在图像上展示匹配项，并返回图像。后者是通过以下步骤实现的：
- en: 'Create a new output image of a size that will fit the two images together;
    make it three-channel in order to draw colored lines on the image:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新输出图像，其大小足以将两个图像放在一起；为了在图像上绘制彩色线条，使其成为三通道：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Place the first image on the left of the new image and the second image on
    the right of the first image:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第一张图像放置在新图像的左侧，第二张图像放置在第一张图像的右侧：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In these expressions, we used the broadcasting rules of the NumPy arrays, which
    are rules for operations on arrays when their shapes do not match but meet certain
    constraints instead. Here, `img[...,None]` assigns one rule to channel (third)
    dimension of the two-dimensional grayscale image (array). Next, once `NumPy` meets
    a dimension that does not match, but instead has the value of one, it broadcasts
    the array. It means the same value is used for all three channels.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些表达式中，我们使用了NumPy数组的广播规则，这是当数组的形状不匹配但满足某些约束时的操作规则。在这里，`img[...,None]`为二维灰度图像（数组）的第三个（通道）维度分配了一个规则。接下来，一旦`NumPy`遇到一个不匹配的维度，但具有值为一的，它就会广播数组。这意味着相同的值用于所有三个通道。
- en: 'For each matching pair of points between both images, we want to draw a small
    blue circle on each image and connect the two circles with a line. For this purpose,
    iterate over the list of matching keypoints with a `for` loop, extract the center
    coordinates from the corresponding keypoints, and shift the coordinate of the
    second center for drawing:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于两个图像之间的每个匹配点对，我们想在每个图像上画一个小蓝色圆圈，并用线条连接两个圆圈。为此，使用`for`循环遍历匹配关键点的列表，从相应的关键点中提取中心坐标，并调整第二个中心坐标以进行绘制：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The keypoints are stored as tuples in Python, with two entries for the *x* and *y* coordinates.
    Each match, `m`, stores the index in the key point lists, where `m.trainIdx` points
    to the index in the first key point list (`kp1`) and `m.queryIdx` points to the
    index in the second key point list (`kp2`).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点以Python中的元组形式存储，包含两个条目用于*x*和*y*坐标。每个匹配项`m`存储在关键点列表中的索引，其中`m.trainIdx`指向第一个关键点列表（`kp1`）中的索引，而`m.queryIdx`指向第二个关键点列表（`kp2`）中的索引。
- en: 'In the same loop, draw circles with a four-pixel radius, the color blue, and
    a one-pixel thickness. Then, connect the circles with a line:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的循环中，用四像素半径、蓝色和一像素粗细的圆圈绘制。然后，用线条连接圆圈：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, `return` the resulting image:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`return`结果图像：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So, now that we have a convenient function, we can illustrate the matches with
    the following code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们有一个方便的功能，我们可以用以下代码来展示匹配：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The blue lines connect the features in the object (left) to the features in
    the scenery (right), as shown here:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色线条将物体（左侧）中的特征与场景（右侧）中的特征连接起来，如图所示：
- en: '![](img/e2f75f72-bfc2-4eb7-86ed-352171b6ded5.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2f75f72-bfc2-4eb7-86ed-352171b6ded5.png)'
- en: This works fine in a simple example such as this, but what happens when there
    are other objects in the scene? Since our object contains some lettering that
    seems highly salient, what happens when there are other words present?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，这工作得很好，但当场景中有其他物体时会发生什么？由于我们的物体包含一些看起来非常突出的文字，当场景中存在其他文字时会发生什么？
- en: 'As it turns out, the algorithm works even under such conditions, as you can
    see in the following screenshot:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，算法在这样的条件下也能正常工作，如下面的截图所示：
- en: '![](img/5df97ad3-c129-4c21-8c2a-af86d856cd7b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5df97ad3-c129-4c21-8c2a-af86d856cd7b.png)'
- en: Interestingly, the algorithm did not confuse the name of the author as seen
    on the left with the black-on-white lettering next to the book in the scene, even
    though they spell out the same name. This is because the algorithm found a description
    of the object that does not rely purely on the grayscale representation. On the
    other hand, an algorithm doing a pixel-wise comparison could have easily gotten
    confused.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，算法没有混淆左侧作者的名字与场景中书本旁边的黑白文字，尽管它们拼写出相同的名字。这是因为算法找到了一个不依赖于纯灰度表示的对象描述。另一方面，一个进行像素级比较的算法可能会轻易混淆。
- en: Now that we have matched our features, let's move ahead and learn how we can
    use these results in order to highlight the object of the interest, which we will
    do with the help of homography estimation in the next section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经匹配了我们的特征，让我们继续学习如何使用这些结果来突出显示感兴趣的对象，我们将通过下一节中的单应性估计来实现这一点。
- en: Mapping homography estimation
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投影单应性估计
- en: Since we are assuming that the object of our interest is planar (that is, an
    image) and rigid, we can find the homography transformation between the feature
    points of the two images.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设我们感兴趣的对象是平面的（即图像）且刚性的，我们可以找到两个图像特征点之间的单应性变换。
- en: 'In the following steps, we will explore how homography can be used to calculate
    the perspective transformation required to bring matched feature points in the
    object image (`self.key_train`) into the same plane as corresponding feature points
    in the current image frame (`key_query`):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将探讨如何使用单应性来计算将匹配的特征点从对象图像（`self.key_train`）转换到与当前图像帧（`key_query`）中对应特征点相同平面的透视变换：
- en: 'First, we store the image coordinates of all the keypoints that are good matches
    in lists for convenience, as shown in the following code snippet:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为了方便起见，我们将所有匹配良好的关键点的图像坐标存储在列表中，如下代码片段所示：
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s encapsulate the logic for corner point detections in a separate
    function:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将角点检测的逻辑封装到一个单独的函数中：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The previous code shows two sequences of points and the shape of the source
    image, the function will return the corners of the points, which is accomplished
    by the following steps:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码显示了两个点的序列和源图像的形状，函数将返回点的角，这是通过以下步骤实现的：
- en: 'Find the perspective transformation (a homography matrix, `H`) for the given
    two sequences of coordinates:'
  id: totrans-189
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为给定的两个坐标序列找到透视变换（单应性矩阵，`H`）：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To find the transformation, the `cv2.findHomography` function will use the **random
    sample consensus** (**RANSAC**) method to probe different subsets of input points.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到变换，`cv2.findHomography`函数将使用**随机样本一致性**（**RANSAC**）方法来探测输入点的不同子集。
- en: 'If the method fails to find the homography matrix, we `raise` an exception,
    which we will catch later in our application:'
  id: totrans-192
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果方法无法找到单应性矩阵，我们将`raise`一个异常，我们将在应用程序中稍后捕获它：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Given the shape of the source image, we store the coordinates of its corners
    in an array:'
  id: totrans-194
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定源图像的形状，我们将其角点的坐标存储在一个数组中：
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A homography matrix can be used to transform any point in the pattern into the
    scenery, such as transforming a corner point in the training image to a corner
    point in the query image. In other words, this means that we can draw the outline
    of the book cover in the query image by transforming the corner points from the
    training image.
  id: totrans-196
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用单应性矩阵将图案中的任何点转换到场景中，例如将训练图像中的角点转换为查询图像中的角点。换句话说，这意味着我们可以通过将角点从训练图像转换到查询图像来在查询图像中绘制书的封面轮廓。
- en: 'In order to do this, the list of corner points of the training image (`src_corners`)
    is taken and projected in the query image by performing a perspective transform:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要从训练图像的角点列表（`src_corners`）中取出并通过对查询图像进行透视变换来投影：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Also, the result is returned immediately, that is, an array of image points
    (two-dimensional `NumPY` ndarray).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结果会立即返回，即一个包含图像点的数组（二维NumPY数组）。
- en: 'Now that we have defined our function, we can call it to detect the corner
    points:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的函数，我们可以调用它来检测角点：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'All that we need to do is draw a line between each point in `dst_corners` and
    the very next one, and we will see an outline in the scenery:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做的只是在每个`dst_corners`中的点之间画线，我们将在场景中看到一个轮廓：
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note, in order to draw the image points, first offset the *x* coordinate of
    the points by the width of the pattern image (because we are showing the two images
    next to each other). Then, we treat the image points as a closed polyline and
    draw it with `cv2.polilines`. We also have to change the data type to an integer
    for drawing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了绘制图像点，首先将点的*x*坐标偏移图案图像的宽度（因为我们是将两个图像并排放置）。然后，我们将图像点视为一个闭合的多边形线，并使用`cv2.polilines`绘制它。我们还需要将数据类型更改为整数以进行绘制。
- en: 'Finally, the outline of the book cover is drawn like this:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，书的封面草图绘制如下：
- en: '![](img/cd7c3b27-d8b2-4d0b-93e0-6acf5f930e3f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cd7c3b27-d8b2-4d0b-93e0-6acf5f930e3f.png)'
- en: 'This works even when the object is only partially visible, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 即使物体只部分可见，这种方法也有效，如下所示：
- en: '![](img/93a5f96d-90f7-4efc-875f-10eb837a2741.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93a5f96d-90f7-4efc-875f-10eb837a2741.png)'
- en: Although the book partially lies outside of the frame, the outline of the book
    is predicted with the boundaries of the outline lying beyond the frame.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管书籍部分在框架之外，但书籍的轮廓是通过超出框架的轮廓边界预测的。
- en: In the next section, let's learn how to warp the image in order to make it look
    closer to the original one.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何扭曲图像，使其看起来更接近原始图像。
- en: Warping the image
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像扭曲
- en: 'We can also do the opposite of homography estimation/transformation by going
    from the probed scenery to the training pattern coordinates. This makes it possible
    for the book cover to be brought onto the frontal plane as if we were looking
    at it directly from above. To achieve this, we can simply take the inverse of
    the homography matrix to get the inverse transformation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过从探测场景到训练模式坐标进行相反的单应性估计/变换。这使得封面可以像直接从上方看它一样被带到前平面。为了实现这一点，我们可以简单地取单应性矩阵的逆来得到逆变换：
- en: '[PRE36]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: However, this would map the top-left corner of the book cover to the origin
    of our new image, which would cut off everything to the left of and above the
    book cover. Instead, we want to roughly center the book cover in the new image.
    Thus, we need to calculate a new homography matrix.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这将把封面左上角映射到新图像的原点，这将切断封面左上方的所有内容。相反，我们希望在大约中心位置放置封面。因此，我们需要计算一个新的单应性矩阵。
- en: 'The book cover should be roughly half of the size of the new image. Hence,
    instead of using the point coordinates of the train image, the following method
    demonstrates how to transform the point coordinates such that they appear in the
    center of the new image:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 封面的大小应该大约是新图像的一半。因此，我们不是使用训练图像的点坐标，而是以下方法演示了如何转换点坐标，使它们出现在新图像的中心：
- en: 'First, find the scaling factor and bias and then, apply the linear scaling
    and transform the coordinates:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，找到缩放因子和偏差，然后应用线性缩放并转换坐标：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As an output, we want an image that has the same shape as the pattern image
    (`sh_query`):'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为输出，我们希望得到一个与模式图像（`sh_query`）形状相同的图像：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we can find the homography matrix between the points in the query image
    and the transformed points of the train image (make sure that the list is converted
    to a `NumPy` array):'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以找到查询图像中的点和训练图像变换后的点之间的单应性矩阵（确保列表被转换为 `NumPy` 数组）：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After that, we can use the homography matrix to transform every pixel in the
    image (this is also called warping the image):'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以使用单应性矩阵来转换图像中的每个像素（这也可以称为图像扭曲）：
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The result looks like this (with the matching on the left and the warped image
    on the right):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来是这样的（左边的匹配和右边的扭曲图像）：
- en: '![](img/b3fd8f98-8217-4cb2-8f2f-f5bfa2bf9709.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3fd8f98-8217-4cb2-8f2f-f5bfa2bf9709.png)'
- en: 'The image that results from the perspective transformation might not be perfectly
    aligned with the `frontoparallel` plane, because, after all, the homography matrix
    just gives an approximation. In most cases, however, our approach works just fine,
    such as in the example shown in the following screenshot:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 由于透视变换后的图像可能不会与 `frontoparallel` 平面完美对齐，因为毕竟单应性矩阵只是给出了一个近似。然而，在大多数情况下，我们的方法仍然工作得很好，例如在以下屏幕截图中的示例所示：
- en: '![](img/f274e9d0-ec49-46f2-9550-c3f20fb3fa13.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f274e9d0-ec49-46f2-9550-c3f20fb3fa13.png)'
- en: Now that we have a pretty good picture about how feature extraction and matching
    is accomplished with a couple of images, let's move on to the completion of our
    app and learn how we can track the features in the next section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对如何使用几幅图像完成特征提取和匹配有了相当好的了解，让我们继续完成我们的应用程序，并学习如何在下一节中跟踪特征。
- en: Learning feature tracking
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习特征跟踪
- en: Now that our algorithm works for single frames, we want to make sure that the
    image found in one frame will also be found in the very next frame.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们的算法适用于单帧，我们想要确保在一帧中找到的图像也会在下一帧中找到。
- en: In `FeatureMatching.__init__`, we created some bookkeeping variables that we
    said we would use for feature tracking. The main idea is to enforce some coherence
    while going from one frame to the next. Since we are capturing roughly 10 frames
    per second, it is reasonable to assume that the changes from one frame to the
    next will not be too radical.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `FeatureMatching.__init__` 中，我们创建了一些用于特征跟踪的记账变量。主要思想是在从一个帧移动到下一个帧的过程中强制一些一致性。由于我们每秒捕获大约10帧，因此可以合理地假设从一个帧到下一个帧的变化不会太剧烈。
- en: Therefore, we can be sure that the result we get in any given frame has to be
    similar to the result we got in the previous frame. Otherwise, we discard the
    result and move on to the next frame.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以确信在任何给定的帧中得到的任何结果都必须与前一帧中得到的结果相似。否则，我们丢弃该结果并继续到下一帧。
- en: However, we have to be careful not to get stuck with a result that we think
    is reasonable but is actually an outlier. To solve this problem, we keep track
    of the number of frames we have spent without finding a suitable result. We use `self.num_frames_no_success` to
    hold the value of the number of frames. If this value is smaller than a certain
    threshold, let's say `self.max_frames_no_success`, we do the comparison between
    the frames.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须小心，不要陷入一个我们认为合理但实际上是异常值的结果。为了解决这个问题，我们跟踪我们没有找到合适结果所花费的帧数。我们使用`self.num_frames_no_success`来保存帧数的值。如果这个值小于某个特定的阈值，比如说`self.max_frames_no_success`，我们就进行帧之间的比较。
- en: If it is greater than the threshold, we assume that too much time has passed
    since the last result was obtained, in which case it would be unreasonable to
    compare the results between the frames. Let's learn about early outlier detection
    and rejection in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它大于阈值，我们假设自上次获得结果以来已经过去了太多时间，在这种情况下，在帧之间比较结果是不合理的。让我们在下一节中了解早期异常值检测和拒绝。
- en: Understanding early outlier detection and rejection
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解早期异常值检测和拒绝
- en: We can extend the idea of outlier rejection to every step in the computation.
    The goal then becomes minimizing the workload while maximizing the likelihood
    that the result we obtain is a good one.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将异常值拒绝的概念扩展到计算的每一步。那么目标就变成了在最大化我们获得的结果是好的可能性的同时，最小化工作量。
- en: 'The resulting procedure for early outlier detection and rejection is embedded
    in the `FeatureMatching.match` method. This method first converts the image to
    grayscale and stores its shape:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 用于早期异常值检测和拒绝的相应过程嵌入在`FeatureMatching.match`方法中。该方法首先将图像转换为灰度并存储其形状：
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, if the outlier is detected during any step of the computation, we raise
    an `Outlier` exception to terminate the computation. The following steps show
    us the matching procedure:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在计算的任何步骤中检测到异常值，我们将引发一个`Outlier`异常来终止计算。以下步骤展示了匹配过程：
- en: 'First, we find good matches between the feature descriptors of the pattern
    and the query image, and then store the corresponding point coordinates from the
    train and query images:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们在模式和查询图像的特征描述符之间找到良好的匹配，然后存储来自训练和查询图像的相应点坐标：
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In order for RANSAC to work in the very next step, we need at least four matches.
    If fewer matches are found, we admit defeat and raise an `Outlier` exception with
    a custom message. We wrap the outlier detection in a `try` block:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使RANSAC在接下来的步骤中工作，我们需要至少四个匹配。如果找到的匹配更少，我们承认失败并引发一个带有自定义信息的`Outlier`异常。我们将异常检测包裹在一个`try`块中：
- en: '[PRE43]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, we find the corner points of the pattern in the query image (`dst_corners`):'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在查询图像中找到模式的角点（`dst_corners`）：
- en: '[PRE44]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If any of these points lie significantly outside the image (by `20` pixels,
    in our case), it means that either we are not looking at our object of interest,
    or the object of interest is not entirely in the image. In both cases, we don''t
    have to proceed, and raise or create an instance of `Outlier`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些点中的任何一个显著地位于图像之外（在我们的例子中是`20`像素），这意味着我们可能没有看到我们感兴趣的对象，或者感兴趣的对象并没有完全在图像中。在两种情况下，我们不需要继续，并引发或创建一个`Outlier`实例：
- en: '[PRE45]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If the four recovered corner points do not span a reasonable quadrilateral
    (a polygon with four sides), it means that we are probably not looking at our
    object of interest. The area of a quadrilateral can be calculated with the following
    code:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果恢复的四个角点不能形成一个合理的四边形（一个有四边的多边形），这意味着我们可能没有看到我们感兴趣的对象。四边形的面积可以用以下代码计算：
- en: '[PRE46]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If the area is either unreasonably small or unreasonably large, we discard
    the frame and raise an exception:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果面积要么不合理地小，要么不合理地大，我们就丢弃该帧并引发异常：
- en: '[PRE47]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we scale the good points of the train image and find the homography matrix
    to bring the object to the frontal panel:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们缩放训练图像中的良好点并找到将对象带到正面面板的透视矩阵：
- en: '[PRE48]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If the recovered homography matrix is too different from the one that we last
    recovered (`self.last_hinv`), it means that we are probably looking at a different
    object. However, we only want to consider `self.last_hinv` if it is fairly recent,
    say, from within the last `self.max_frames_no_success` frames:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果恢复的单应性矩阵与我们上次恢复的矩阵（`self.last_hinv`）差异太大，这意味着我们可能正在观察一个不同的对象。然而，我们只想考虑`self.last_hinv`，如果它是相对较新的，比如说，在最近的`self.max_frames_no_success`帧内：
- en: '[PRE49]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This will help us to keep track of the same object of interest over time. If,
    for any reason, we lose track of the pattern image for more than `self.max_frames_no_success`
    frames, we skip this condition and accept whatever homography matrix was recovered
    up to that point. This ensures that we do not get stuck with a `self.last_hinv`
    matrix, which is actually an outlier.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们跟踪同一感兴趣对象随时间的变化。如果由于任何原因，我们超过`self.max_frames_no_success`帧未能追踪到模式图像，我们将跳过此条件并接受到该点为止恢复的任何单应性矩阵。这确保了我们不会陷入`self.last_hinv`矩阵，这实际上是一个异常值。
- en: 'If we detect an outlier during the outlier detection process, we increase`self.num_frame_no_success`
    and return `False`. We might also want to print a message of the outlier in order
    to see when exactly it appears:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在异常值检测过程中检测到异常值，我们将增加`self.num_frame_no_success`并返回`False`。我们可能还想打印出异常值的信息，以便看到它确切出现的时间：
- en: '[PRE50]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Otherwise, if the outlier was not detected, we can be fairly certain that we
    have successfully located the object of interest in the current frame. In this
    case, we first store the homography matrix and reset the counter:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果没有检测到异常值，我们可以相当肯定，我们已经成功地在当前帧中定位了感兴趣的对象。在这种情况下，我们首先存储单应性矩阵并重置计数器：
- en: '[PRE51]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following lines show the warping of the image for illustration:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行展示了图像扭曲的示例：
- en: '[PRE52]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And finally, we draw good matches and corner points, as we did previously,
    and return the results:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们像之前一样绘制良好的匹配点和角点，并返回结果：
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In the preceding code, as explained previously, we shifted the *x* coordinate
    of the corners by the width of the train image because the query image appears
    next to the train image, and we changed the data type of the corners to integers
    because the `polilines` method accepts integers as coordinates.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，如前所述，我们将角点的*x*坐标移动了训练图像的宽度，因为查询图像出现在训练图像旁边，我们将角点的数据类型更改为整数，因为`polilines`方法接受整数作为坐标。
- en: In the next section, we'll explore how the algorithm works.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨算法的工作原理。
- en: Seeing the algorithm in action
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察算法的实际运行
- en: 'The result of the matching procedure in a live stream from a laptop''s webcam
    looks like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 来自笔记本电脑摄像头的实时流中匹配过程的结果如下所示：
- en: '![](img/45a68c97-f5c4-4006-8e18-c1e053676e9c.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45a68c97-f5c4-4006-8e18-c1e053676e9c.png)'
- en: As you can see, most of the keypoints in the pattern image were matched correctly
    with their counterparts in the query image on the right. The printout of the pattern
    can now be slowly moved around, tilted, and turned. As long as all the corner
    points stay in the current frame, the homography matrix is updated accordingly
    and the outline of the pattern image is drawn correctly.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模式图像中的大多数关键点都正确地与右侧查询图像中的对应点匹配。现在可以缓慢地移动、倾斜和旋转模式图像的打印输出。只要所有角点都保持在当前帧中，单应性矩阵就会相应更新，并正确绘制模式图像的轮廓。
- en: 'This works even if the printout is upside down, as shown here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 即使打印输出是颠倒的，这也适用，如下所示：
- en: '![](img/78314d35-07a7-48d5-a6e5-d6148c2beb1f.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78314d35-07a7-48d5-a6e5-d6148c2beb1f.png)'
- en: 'In all cases, the warped image brings the pattern image to an upright, centered
    position on the `frontoparallel` plane. This creates a cool effect of having the
    pattern image frozen in place in the center of the screen, while the surroundings
    twist and turn around it, like this:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，扭曲的图像将模式图像带到`frontoparallel`平面上方直立的中心位置。这创造了一种效果，即模式图像在屏幕中心被冻结，而周围的环境则围绕它扭曲和转动，就像这样：
- en: '![](img/0c091a5e-985f-4864-9210-cb228a9c144a.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c091a5e-985f-4864-9210-cb228a9c144a.png)'
- en: In most cases, the warped image looks fairly accurate, as shown in the one earlier.
    If for any reason the algorithm accepts a wrong homography matrix that leads to
    an unreasonably warped image, then the algorithm will discard the outlier and
    recover within half a second (that is, within the `self.max_frames_no_success`
    frames), leading to accurate and efficient tracking throughout.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，扭曲后的图像看起来相当准确，如前所述。如果由于任何原因，算法接受了一个导致不合理扭曲图像的错误单应性矩阵，那么算法将丢弃异常值并在半秒内（即`self.max_frames_no_success`帧内）恢复，从而实现准确和高效的跟踪。
- en: Summary
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter looked at a robust feature tracking method that is fast enough
    to run in real time when applied to the live stream of a webcam.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了快速且足够鲁棒的特征跟踪方法，当应用于网络摄像头的实时流时，可以在实时中运行。
- en: First, the algorithm shows you how to extract and detect important features
    in an image, which was independent of perspective and size, be it in a template
    of our object of interest (train image) or a more complex scene in which we expect
    the object of interest to be embedded (query image).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，算法向您展示如何从图像中提取和检测重要特征，这些特征与视角和大小无关，无论是我们感兴趣的对象（列车图像）的模板中，还是我们期望感兴趣的对象嵌入的更复杂的场景中（查询图像）。
- en: A match between feature points in the two images is then found by clustering
    the keypoints using a fast version of the nearest-neighbor algorithm. From there
    on, it is possible to calculate a perspective transformation that maps one set
    of feature points to the other. With this information, we can outline the train
    image as found in the query image and warp the query image so that the object
    of interest appears upright in the center of the screen.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用快速最近邻算法的一个版本对关键点进行聚类，然后在两张图像中的特征点之间找到匹配。从那时起，就可以计算出一种透视变换，将一组特征点映射到另一组。有了这些信息，我们可以概述在查询图像中找到的列车图像，并将查询图像扭曲，使感兴趣的物体垂直地出现在屏幕中央。
- en: With this in hand, we now have a good starting point for designing a cutting-edge
    feature tracking, image stitching, or augmented-reality application.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些，我们现在有一个很好的起点来设计一个前沿的特征跟踪、图像拼接或增强现实应用。
- en: In the next chapter, we will continue studying the geometrical features of a
    scene, but, this time, we will be concentrating on the motion. Specifically, we
    will study how to reconstruct a scene in 3D by inferring its geometrical features
    from camera motion. For this, we will have to combine our knowledge of feature
    matching with the optic flow and structure-from-motion techniques.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续研究场景的几何特征，但这次我们将专注于运动。具体来说，我们将研究如何通过从相机运动中推断其几何特征来重建场景。为此，我们必须将我们对特征匹配的知识与光流和运动结构技术相结合。
- en: Attributions
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归因
- en: '`Lenna.png`—the image of Lenna is available at [http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896) by
    Conor Lawless under the CC 2.0 generic attribution.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`Lenna.png`——Lenna的图像可在[http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896)由Conor
    Lawless提供，根据CC 2.0通用归属权。'
