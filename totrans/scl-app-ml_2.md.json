["```py\ntype Hom[T] = {\n  type Right[X] = Function1[X,T] // Co-vector\n  type Left[X] = Function1[T,X]   // Vector\n }\n```", "```py\ntrait Functor[M[_]] {\n  def map[U,V](m: M[U])(f: U =>V): M[V]\n}\n```", "```py\ntrait ObsFunctor[T] extends Functor[(Hom[T])#Left] { self =>\n  override def map[U,V](vu: Function1[T,U])(f: U =>V): \n    Function1[T,V] = f.compose(vu)\n}\n```", "```py\ntrait CoFunctor[M[_]] {\n  def map[U,V](m: M[U])(f: V =>U): M[V]\n}\n```", "```py\ntrait CoObsFunctor[T] extends CoFunctor[(Hom[T])#Right] {\n  self =>\n    override def map[U,V](vu: Function1[U,T])(f: V =>U): \n       Function1[V,T] = f.andThen(vu)\n}\n```", "```py\ntrait Monad[M[_]] {\n  def unit[T](a: T): M[T]\n  def map[U,V](m: M[U])(f U =>V): M[V]\n  def flatMap[U,V](m: M[U])(f: U =>M[V]): M[V]\n}\n```", "```py\nclass A[T <: Int](param: Param)\nclass B[T <: Double](param: Param)\n```", "```py\nclass A[T <: AnyVal](param: Param)(implicit f: T => Int)\nclass C[T < : AnyVal](param: Param)(implicit f: T => Float)\n```", "```py\n    // …..\n    /* … */\n    ```", "```py\n    require( Math.abs(x) < EPS, \" …\")\n    ```", "```py\n    final protected class SVM { … }\n    private[this] val lsError = …\n    ```", "```py\n    final protected def dot: = …\n    ```", "```py\n    try {\n       correlate …\n    } catch {\n       case e: MathException => ….\n    }\n    Try {    .. } match {\n      case Success(res) =>\n      case Failure(e => ..\n    }\n    ```", "```py\n    private val logger = Logger.getLogger(\"..\")\n    logger.info( … )\n    ```", "```py\n    @inline def main = ….\n    @throw(classOf[IllegalStateException])\n    ```", "```py\ntype DblPair = (Double, Double)\ntype DblArray = Array[Double]\ntype DblMatrix = Array[DblArray]\ntype DblVector = Vector[Double]\ntype XSeries[T] = Vector[T]         // One dimensional vector\ntype XVSeries[T] = Vector[Array[T]] // multi-dimensional vector\n```", "```py\nobject Types {\n  Object ScalaMl {  \n   implicit def double2Array(x: Double): DblArray = \n      Array[Double](x)\n   implicit def dblPair2Vector(x: DblPair): Vector[DblPair] = \n      Vector[DblPair](x._1,x._2)\n   ...\n  }\n}\n```", "```py\ntype Fields = Array[String]\nobject YahooFinancials extends Enumeration {\n   type YahooFinancials = Value\n   val DATE, OPEN, HIGH, LOW, CLOSE, VOLUME, ADJ_CLOSE = Value\n\n   def toDouble(v: Value): Fields => Double =   //1\n   (s: Fields) => s(v.id).toDouble\n   def toDblArray(vs: Array[Value]): Fields => DblArray = //2\n       (s: Fields) => vs.map(v => s(v.id).toDouble)\n  …\n}\n```", "```py\ndef load(fileName: String): Try[Vector[DblPair]] = Try {\n   val src =  Source.fromFile(fileName)  //3\n   val data = extract(src.getLines.map(_.split(\",\")).drop(1)) //4\n   src.close //5\n   data\n }\n```", "```py\ndef extract(cols: Iterator[Array[String]]): XVSeries[Double]= {\n  val features = Array[YahooFinancials](LOW, HIGH, VOLUME) //6\n  val conversion = YahooFinancials.toDblArray(features)  //7\n  cols.map(c => conversion(c)).toVector   \n      .map(x => Array[Double](1.0 - x(0)/x(1), x(2)))  //8\n}\n```", "```py\nval cols = Source.fromFile.getLines.map(_.split(\",\")).toArray.drop(1)\n```", "```py\nval lines = Source.fromFile.getLines\nval fields = lines.map(_.split(\",\")).toArray\nval cols = fields.drop(1)\n```", "```py\n    class MinMax[T <: AnyVal](val values: XSeries[T]) (f : T => Double) { //8\n      val zero = (Double.MaxValue, -Double.MaxValue)\n      val minMax = values./:(zero)((mM, x) => { //9\n        val min = mM._1\n        val max = mM._2\n       (if(x < min) x else min, if(x > max) x else max)\n      })\n      case class ScaleFactors(low:Double ,high:Double, ratio: Double)\n      var scaleFactors: Option[ScaleFactors] = None //10\n\n      def min = minMax._1\n      def max = minMax._2\n      def normalize(low: Double, high: Double): DblVector //11\n      def normalize(value: Double): Double\n    }\n    ```", "```py\ndef normalize(low: Double, high: Double): DblVector = \n  setScaleFactors(low, high).map( scale => { //12\n    values.map(x =>(x - min)*scale.ratio + scale.low) //13\n  }).getOrElse(/* … */)\n\ndef setScaleFactors(l: Double, h: Double): Option[ScaleFactors]={\n    // .. error handling code\n   Some(ScaleFactors(l, h, (h - l)/(max - min))\n}\n```", "```py\ndef normalize(value: Double):Double = setScaleFactors.map(scale => \n   if(value < min) scale.low\n   else if (value > max) scale.high\n   else (value - min)* scale.high + scale.low\n).getOrElse( /* … */)\n```", "```py\nclass MinMaxVector(series: XVSeries[Double]) {\n  val minMaxVector: Vector[MinMax[Double]] = //15\n      series.transpose.map(new MinMax[Double](_))\n  def normalize(low: Double, high: Double): XVSeries[Double]\n}\n```", "```py\nclass ScatterPlot(config: PlotInfo, theme: PlotTheme) { //16\n  def display(xy: Vector[DblPair], width: Int, height) //17\n  def display(xt: XVSeries[Double], width: Int, height)\n  // ….\n}\n```", "```py\nval plot = new ScatterPlot((\"CSCO 2012-2013\", \n   \"Session High - Low\", \"Session Volume\"), new BlackPlotTheme)\nplot.display(volatility_vol, 250, 340)\n```", "```py\nclass LogBinRegression(\n     obsSet: Vector[DblArray], \n     expected: Vector[Int],\n     maxIters: Int, \n     eta: Double, \n     eps: Double) {  //18\n\n   val model: LogBinRegressionModel = train  //19\n   def classify(obs: DblArray): Try[(Int, Double)]   //20\n   def train: LogBinRegressionModel\n   def intercept(weights: DblArray): Double\n   …\n}\n```", "```py\ncase class LogBinRegressionModel(val weights: DblArray)\n```", "```py\ndef intercept(weights: DblArray): Double = {\n  val zeroObs = obsSet.filter(!_.exists( _ > 0.01))\n  if( zeroObs.size > 0)\n    zeroObs.aggregate(0.0)((s,z) => s + dot(z, weights), \n       _ + _ )/zeroObs.size\n  else 0.0\n}\n```", "```py\ndef train: LogBinRegressionModel = {\n  val nWeights = obsSet.head.length + 1  //21\n  val init = Array.fill(nWeights)(Random.nextDouble )  //22\n  val weights = gradientDescent(obsSet.zip(expected),0.0,0,init)\n  new LogBinRegressionModel(weights)   //23\n}\n```", "```py\ntype LabelObs = Vector[(DblArray, Int)]\n\n@tailrec\ndef gradientDescent(\n      obsAndLbl: LabelObs, \n      cost: Double, \n      nIters: Int, \n      weights: DblArray): DblArray = {  //24\n\n  if(nIters >= maxIters) \n       throw new IllegalStateException(\"..\")//25\n  val shuffled = shuffle(obsAndLbl)   //26\n  val errorGrad = shuffled.map{ case(x, y) => {  //27\n      val error = sigmoid(dot(x, weights)) - y\n      (error, x.map( _ * error))  //28\n   }}.unzip\n\n   val scale = 0.5/obsAndLbl.size\n   val newCost = errorGrad._1   //29\n.aggregate(0.0)((s,c) =>s + c*c, _ + _ )*scale\n   val relativeError = cost/newCost - 1.0\n\n   if( Math.abs(relativeError) < eps)  weights  //30\n   else {\n     val derivatives = Vector[Double](1.0) ++ \n                 errorGrad._2.transpose.map(_.sum) //31\n     val newWeights = weights.zip(derivatives)\n                       .map{ case (w, df) => w - eta*df)  //32\n     newWeights.copyToArray(weights)\n     gradientDescent(shuffled, newCost, nIters+1, newWeights)//33\n   }\n}\n```", "```py\nval SPAN = 5\ndef shuffle(labelObs: LabelObs): LabelObs = { \n  shuffle(new ArrayBuffer[Int],0,0).map(labelObs( _ )) //34\n}\n```", "```py\nval maxChunkSize = Random.nextInt(SPAN)+2  //35\n\n@tailrec\ndef shuffle(indices: ArrayBuffer[Int], count: Int, start: Int): \n      Array[Int] = {\n  val end = start + Random.nextInt(maxChunkSize) //36\n  val isOdd = ((count & 0x01) != 0x01)\n  if(end >= sz) \n    indices.toArray ++ slice(isOdd, start, sz) //37\n  else \n    shuffle(indices ++ slice(isOdd, start, end), count+1, end)\n}\n```", "```py\ndef slice(isOdd: Boolean, start: Int, end: Int): Array[Int] = {\n  val r = Range(start, end).toArray\n  (if(isOdd) r else r.reverse)\n}\n```", "```py\ndef classify(obs: DblArray): Try[(Int, Double)] = \n  val linear = dot(obs, model.weights)  //37\n  val prediction = sigmoid(linear)\n  (if(linear > 0.0) 1 else 0, prediction) //38\n})\n```", "```py\ndef dot(obs: DblArray, weights: DblArray): Double =\n   weights.zip(Array[Double](1.0) ++ obs)\n          .aggregate(0.0){case (s, (w,x)) => s + w*x, _ + _ }\n```", "```py\ndef dot(x: DblArray, w: DblArray): Double = \n  x.zip(w.drop(1)).map {case (_x,_w) => _x*_w}.sum + w.head\n```", "```py\nval NITERS = 800; val EPS = 0.02; val ETA = 0.0001\nval path_training = \"resources/data/chap1/CSCO.csv\"\nval path_test = \"resources/data/chap1/CSCO2.csv\"\n```", "```py\nfor {\n  volatilityVol <- load(path_training)    //39\n  minMaxVec <- Try(new MinMaxVector(volatilityVol))    //40\n  normVolatilityVol <- Try(minMaxVec.normalize(0.0,1.0))//41\n  classifier <- logRegr(normVolatilityVol)    //42\n  testValues <- load(path_test)    //43\n  normTestValue0 <- minMaxVec.normalize(testValues(0))  //44\n  class0 <- classifier.classify(normTestValue0)   //45\n  normTestValue1 <- minMaxVec.normalize(testValues(1))    \n  class1 <- classifier.classify(normTestValues1)\n} yield {\n   val modelStr = model.toString\n   …\n}\n```", "```py\ndef load(fileName: String): Try[XVSeries[Double], XSeries[Double]] =  {\n  val src =  Source.fromFile(fileName)\n  val data = extract(src.getLines.map( _.split(\",\")).drop(1)) //46\n  src.close; data //47\n}\n```", "```py\ndef logRegr(obs: XVSeries[Double]): Try[LogBinRegression] = Try {\n    val expected = normalize(labels._2).get  //48\n    new LogBinRegression(obs, expected, NITERS, ETA, EPS)  //49\n}\n```", "```py\nclass F { \n  def f: PartialFunction[Int, Try[Double]] { case n: Int … \n  }\n}\nval pfn = (new F).f\npfn(4)\npfn(10)\n```", "```py\nfor {\n  pfn.isDefinedAt(input)\n  value <- pfn(input)\n} yield { … }\n```", "```py\ndef |> : PartialFunction[U, Try[V]]\n```", "```py\nabstract class ETransform[T](val config: T) { //explicit model\n  type U   // type of input\n  type V   // type of output\n  def |> : PartialFunction[U, Try[V]]  // data transformation\n}\n```", "```py\nclass DataSource(\n  config: DataSourceConfig,   //1\n  srcFilter: Option[Fields => Boolean]= None)\n        extends ETransform[DataSourceConfig](config) { //2\n  type U = List[Fields => Double]   //3\n  type V = List[XSeries[Double]]     //4\n  override def |> : PartialFunction[U, Try[V]] = { //5\n    case u: U if(!u.isEmpty) => … \n  }\n}\n```", "```py\nprivate val eTransformMonad = new _Monad[ETransform] {\n  override def unit[T](t:T) = eTransform(t)   //6\n  override def map[T,U](m: ETransform[T])     //7\n      (f: T => U): ETransform[U] = eTransform( f(m.config) )\n  override def flatMap[T,U](m: ETransform[T])  //8\n      (f: T =>ETransform[U]): ETransform[U] = f(m.config)\n}\n```", "```py\nimplicit class eTransform2Monad[T](fct: ETransform[T]) {\n  def unit(t: T) = eTransformMonad.unit(t)\n  final def map[U](f: T => U): ETransform[U] = \n      eTransformMonad.map(fct)(f)\n  final def flatMap[U](f: T => ETransform[U]): ETransform[U] =\n      eTransformMonad.flatMap(fct)(f)\n}\n```", "```py\nabstract class ITransform[T](val xt: Vector[T]) { //Model input\n   type V   // type of output\n   def |> : PartialFunction[T, Try[V]]  // data transformation\n}\n```", "```py\nclass SVM[T <: AnyVal]( //9  \n    config: SVMConfig, \n    xt: Vector[Array[T]], \n    expected: Vector[Double])(implicit f: T => Double)\n  extends ITransform[Array[T]](xt) {//10\n\n type V = Double  //11\n override def |> : PartialFunction[Array[T], Try[V]] = { //12\n     case x: Array[T] if(x.length == data.size) => ...\n  }\n```", "```py\nprivate val iTransformMonad = new _Monad[ITransform] {\n  override def unit[T](t: T) = iTransform(Vector[T](t))  //13\n\n  override def map[T,U](m: ITransform[T])(f: T => U): \nITransform[U] = iTransform( m.xt.map(f) )   //14\n\n  override def flatMap[T,U](m: ITransform[T])  \n    (f: T=>ITransform[U]): ITransform[U] = \n iTransform(m.xt.flatMap(t => f(t).xt)) //15\n}\n```", "```py\nimplicit class iTransform2Monad[T](fct: ITransform[T]) {\n   def unit(t: T) = iTransformMonad.unit(t)\n\n   final def map[U](f: T => U): ITransform[U] = \n      iTransformMonad.map(fct)(f)\n   final def flatMap[U](f: T => ITransform[U]): ITransform[U] = \n      iTransformMonad.flatMap(fct)(f)\n   def filter(p: T =>Boolean): ITransform[T] =  //16\n      iTransform(fct.xt.filter(p))\n}\n```", "```py\ntype V = Vector[Double]\ntrait F { val f: V => V}\ntrait G { val g: V => Double }\n```", "```py\nclass H {self: G with F => def apply(v:V): Double =g(f(v))}\n```", "```py\nval h = new H with G with F {\n  val f: V => V = (v: V) => v.map(Math.exp(_))\n  val g: V => Double = (v: V) => v.sum\n}\n```", "```py\nval app = new Classification with Validation with PreProcessing { \n   val filter = .. \n}\n```", "```py\nval app = new Clustering with Validation with PreProcessing { \n    val filter = ..  \n}\n```", "```py\ntrait PreProcessiongWithValidation extends PreProcessing {\n   self: Validation => val filter = ..\n}\n```", "```py\nval app = new Classification with PreProcessingWithValidation {\n   val validation: Validation\n}\n```", "```py\ntrait Validator { val g = (n: Int) =>  }trait MyValidator extends Validator { def g(n: Int) = …} //WRONG \n\n```", "```py\ntrait Sampling[T,A,B] { \n  val sampler: ETransform[T] { type U = A; type V = B }\n}\ntrait Normalization[T,A,B] { \n  val normalizer: ETransform[T] { type U = A; type V = B }\n  }\ntrait Aggregation[T,A,B] { \n  val aggregator: ETransform[T] { type U = A; type V = B }\n}\n```", "```py\nclass Workflow[T,U,V,W,Z] {\n  self: Sampling[T,U,V] with \n         Normalization[T,V,W] with \n           Aggregation[T,W,Z] =>\n    def |> (u: U): Try[Z] = for {\n      v <- sampler |> u\n      w <- normalizer |> v\n      z <- aggregator |> w\n    } yield z\n}\n```", "```py\ntype Dbl_F = Function1[Double, Double]\nval samples = 100; val normRatio = 10; val splits = 4\n\nval workflow = new Workflow[Int, Dbl_F, DblVector, DblVector,Int] \n      with Sampling[Int, Dbl_F, DblVector] \n         with Normalization[Int, DblVector, DblVector] \n            with Aggregation[Int, DblVector, Int] {\n    val sampler = new ETransform[Int](samples) { /* .. */} //1\n    val normalizer = new ETransform[Int](normRatio) { /*  .. */}\n    val aggregator = new ETransform[Int](splits) {/*  .. */}\n}\n```", "```py\nval sampler = new ETransform[Int](samples) { //2\n  type U = Dbl_F  //3\n  type V = DblVector  //4\n  override def |> : PartialFunction[U, Try[V]] = { \n    case f: U => \n     Try(Vector.tabulate(samples)(n =>f(1.0*n/samples))) //5\n  }\n}\n```", "```py\nval normalizer = new ETransform[Int](normRatio) {\n  type U = DblVector;  type V = DblVector\n  override def |> : PartialFunction[U, Try[V]] = { case x: U \n    if(x.size >0) => Try((Stats[Double](x)).normalize)\n  }\n}\nval aggregator = new ETransform[Int](splits) {\n  type U = DblVector; type V = Int\n  override def |> : PartialFunction[U, Try[V]] = case x: U \n    if(x.size > 0) => Try(Range(0,x.size).find(x(_)==1.0).get)\n  }\n}\n```", "```py\nval g = (x: Double) => Math.log(x+1.0) + Random.nextDouble\nTry( workflow |> g )  //6\n\n```", "```py\ntrait PreprocessingModule[T] {\n  trait Preprocessor[T] { //7\n    def execute(x: Vector[T]): Try[DblVector] \n  } \n  val preprocessor: Preprocessor[T]//8\n\n  class ExpMovingAverage[T <: AnyVal]( //9\n      p: Int)\n      (implicit num: Numeric[T], f: T =>Double) \n    extends Preprocessor[T] {\n\n    val expMovingAvg = filtering.ExpMovingAverage[T](p) //10\n    val pfn = expMovingAvg |>  //11\n    override def execute(x: Vector[T]): Try[DblVector] = \n      pfn(x).map(_.toVector)\n  }\n\n   class DFTFilter[T <: AnyVal]( \n      fc: Double)\n    (g: (Double,Double) =>Double) \n     (implicit f : T => Double)\n   extends Preprocessor[T] { //12\n\n     val filter = filtering.DFTFir[T](g, fc, 1e-5)\n     val pfn = filter |>\n     override def execute(x: Vector[T]): Try[DblVector]=\n        pfn(x).map(_.toVector)\n   }\n}\n```", "```py\nclass Stats[T < : AnyVal](\n     values: Vector[T])(implicit f ; T => Double)\n  extends MinMax[T](values) {\n\n  val zero = (0.0\\. 0.0)\n  val sums = values./:(zero)((s,x) =>(s._1 +x, s._2 + x*x)) //1\n\n  lazy val mean = sums._1/values.size  //2\n  lazy val variance = \n         (sums._2 - mean*mean*values.size)/(values.size-1)\n  lazy val stdDev = Math.sqrt(variance)\n  …\n}\n```", "```py\ndef gauss(mu: Double, sigma: Double, x: Double): Double = {\n   val y = (x - mu)/sigma\n   INV_SQRT_2PI*Math.exp(-0.5*y*y)/sigma\n}\nval normal = gauss(1.0, 0.0, _: Double)\n```", "```py\ndef zScore: DblVector = values.map(x => (x - mean)/stdDev )\n```", "```py\ntrait Validation { def score: Double }\n```", "```py\nclass BinFValidation[T <: AnyVal](\n     expected: Vector[Int],\n     xt: XVSeries[T])\n     (predict: Array[T] => Int)(implicit f: T => Double) \n  extends Validation { //1\n\n  val counters = {\n    val predicted = xt.map( predict(_))\n    expected.zip(predicted)\n      .aggregate(new Counter[Label])((cnt, ap) => \n         cnt + classify(ap._1, ap._2), _ ++ _) //2\n  }\n\n  override def score: Double = f1   //3\n  lazy val f1 = 2.0*precision*recall/(precision + recall)\n  lazy val precision = compute(FP)  //4\n  lazy val recall = compute(FN) \n\n  def compute(n: Label): Double = {\n    val denom = counters(TP) + counters(n)\n    counters(TP).toDouble/denom\n  }\n  def classify(predicted: Int, expected: Int): Label = //5\n    if(expected == predicted) if(expected == POSITIVE) TP else TN\n    else if(expected == POSITIVE) FN else FP \n}\n```", "```py\nobject Label extends Enumeration {\n  type Label = Value\n  val TP, TN, FP, FN = Value\n}\n```", "```py\nclass MultiFValidation[T <: AnyVal](\n    expected: Vector[Int],\n    xv: XVSeries[T],\n    classes: Int)\n    (predict: Array[T] => Int)(implicit f : T => Double)\n  extends Validation { //7\n\n  val confusionMatrix: Matrix[Int] = //8\n  labeled./:(Matrix[Int](classes)){case (m, (x,n)) => \n    m + (n, predict(x), 1)}  //9\n\n val macroStats: DblPair = { //10\n   val pr= Range(0, classes)./:(0.0,0.0)((s, n) => {\n     val tp = confusionMatrix(n, n)   //11\n     val fn = confusionMatrix.col(n).sum – tp  //12\n     val fp = confusionMatrix.row(n).sum – tp  //13\n     (s._1 + tp.toDouble/(tp + fp), s._2 +tp.toDouble/(tp + fn))\n   })\n   (pr._1/classes, pr._2/classes)\n }\n lazy val precision: Double = macroStats._1\n lazy val recall: Double = macroStats._1\n def score: Double = 2.0*precision*recall/(precision+recall)\n }\n```", "```py\ntype ValidationType[T] = Vector[(Array[T], Int)]\nclass OneFoldXValidation[T <: AnyVal](\n    xt: XVSeries[T],\n    expected: Vector[Int], \n    ratio: Double)(implicit f : T => Double) {  //14\n  val datasSet: (ValidationType[T], ValidationType[T]) //15\n  def trainingSet: ValidationType[T] = datasSet._1\n  def validationSet: ValidationType[T] = datasSet._1\n}\n```", "```py\nval datasSet: (Vector[LabeledData[T]],Vector[LabeledData[T]]) = { \n  val labeledData = xt.drop(1).zip(expected)  //16\n  val trainingSize = (ratio*expected.size).floor.toInt //17\n\n  val valSz = labeledData.size - trainingSize\n  val adjSz = if(valSz < 2) 1 \n          else if(valSz >= labeledData.size)  labeledData.size -1 \n          else valSz  //18\n  val iter = labeledData.grouped(adjSz )  //18\n  val ordLabeledData = labeledData\n      .map( (_, Random.nextDouble) )  //19\n      .sortWith( _._2 < _._2).unzip._1 //20\n\n  (ordlabeledData.takeRight(adjValSz),   \n   ordlabeledData.dropRight(adjValSz))  //21\n}\n```", "```py\ntype Dbl_F = Double => Double \nclass BiasVariance[T](target: Dbl_F,nValues: Int)\n     (implicit f: T => Double) {//22\n  def fit(models: List[Dbl_F]): List[DblPair] = { //23\n    models.map(accumulate(_, models.size)) //24\n  }\n}\n```", "```py\ndef accumulate(f: Dbl_F, y:Double, numModels: Int): DblPair = \n  Range(0, nValues)./:(0.0, 0.0){ case((s,t) x) => { \n    val diff = (f(x) - y)/numModels\n    (s + diff*diff, t + Math.abs(f(x)-target(x))) //25\n  }}\n```", "```py\nval template = (x: Double, n : Int) => \n                        0.2*x*(1.0 + Math.sin(x*0.1)/n) \nval training = (x: Double) => {\n  val r1 = 0.45*(Random.nextDouble-0.5)\n  val r2 = 38.0*(Random.nextDouble - 0.5) + Math.sin(x*0.3)\n  0.2*x*(1.0 + Math.sin(x*0.1 + r1)) + r2\n}\nVal target = (x: Double) => template(x, 1) //26\nval models = List[(Dbl_F, String)] (  //27\n  ((x: Double) => template(x, 4), \"Underfit1\"),  \n  ((x: Double) => template(x, 2), \"Underfit2\"),\n  ((x : Double) => training(x), \"Overfit\")\n  (target, \"target\"),\n)\nval evaluator = new BiasVariance[Double](target, 200)\nevaluator.fit(models.map( _._1)) match { /* … */ }\n```", "```py\nobject XTSeries { \n  def zipWithShift[T](xv: XSeries[T], n: Int): Vector[(T, T)] = \n     xv.drop(n).zip(xv.view.dropRight(n))  //1\n\n  def zipWithShift1[T](xv: XSeries[T]): Vector[(T, T)] = \n     xv.zip(xv.view.drop(n))\n\n  def statistics[T <: AnyVal](xt:XVSeries[T])\n       (implicit f: T =>: Double): Vector[Stats[T]] = \n    xt.transpose.map( Stats[T]( _ ))  //2\n\n  def normalize[T <: AnyVal](  //3\n      xt: XSeries[T], low: Double, high: Double) \n      (implicit ordering: Ordering[T], \n          f: T => Double): Try[DblVector] = \n    Try (Stats[T](xt).normalize(low, high) )\n   ...\n}\n```", "```py\ndef zipToSeries[T: ClassTag](\nx: Vector[T], y: Vector[T]): XVSeries[T]\n```", "```py\ndef splitAt[T](xv: XSeries[T], n: Int): (XSeries[T], XSeries[T])\n```", "```py\ndef zScore[T <: AnyVal](xt: XSeries[T])\n    (implicit f: T => Double): Try[DblVector]\n```", "```py\ndef zScores[T <: AnyVal](xt: XVSeries[T])\n    (implicit f: T => Double): Try[XVSeries[Double]] \n```", "```py\ndef delta(x: DblVector): DblVector\n```", "```py\ndef binaryDelta(x: DblVector): Vector[Int]\n```", "```py\ndef sse[T <: AnyVal](x: Array[T], z: Array[T])\n   (implicit f: T => Double): Double\n```", "```py\ndef mse[T <: AnyVal](x: Array[T], z: Array[T])\n    (implicit f: T => Double): Double\n```", "```py\ndef mse(x: DblVector, z: DblVector): Double\n```", "```py\ndef statistics[T <: AnyVal](xt: XVSeries[T])\n    (implicit f: T => Double): Vector[Stats[T]]\n```", "```py\ndef zipToVector[T](x: XVSeries[T], y: XVSeries[T])\n  (f: (Array[T], Array[T]) =>Double): XSeries[Double] = \n  x.zip(y.view).map{ case (x, y) => f(x,y)}\n```", "```py\nsealed trait Transpose {\n  type Result   //4\n  def apply(): Result  //5\n}\n```", "```py\nimplicit def xvSeries2Matrix[T: ClassTag](from: XVSeries[T]) = \n  new Transpose { type Result = Array[Array[T]]  //6\n    def apply(): Result =  from.toArray.transpose\n}\nimplicit def list2Matrix[T: ClassTag](from: List[Array[T]]) = \n  new Transpose { type Result = Array[Array[T]]  //7\n   def apply(): Result =  from.toArray.transpose\n}\n…\n```", "```py\ndef transpose(tpose: Transpose): tpose.Result = tpose()\n```", "```py\nsealed trait Difference[T] {\n  type Result\n  def apply(f: (Double, Double) => T): Result\n}\n```", "```py\nimplicit def vector2Double[T](x: DblVector) = new Difference[T] {\n  type Result = Vector[T]\n  def apply(f: (Double, Double) => T): Result =  //8\n    zipWithShift(x, 1).collect{case(next,prev) =>f(prev,next)}\n}\n```", "```py\ndef difference[T](\n   diff: Difference[T], \n   f: (Double, Double) => T): diff.Result = diff(f)\n```", "```py\nval diffDouble = (x: Double,y: Double) => y –x //9\nval diffInt = (x: Double,y: Double) => if(y > x) 1 else 0 //10\nval diffBoolean = (x: Double,y: Double) => (y > x) //11\n\n```", "```py\ndef differentialData[T](\n   x: DblVector, \n   y: DblVector, \n   target: DblVector,\n   f: (Double,Double) =>T): Try[(XVSeries[Double],Vector[T])] = \n  Try((zipToSeries(x,y), difference(target, f)))\n```", "```py\nval aggregator = new ETransform[Int](splits) { \n  override def |> : PartialFunction[U, Try[V]] = { \n    case x: U if(!x.isEmpty) => \n      Try( Range(0, x.size).view.find(x(_) == 1.0).get) //12\n   }\n}\n```", "```py\ntrait MovingAverage[T]\n```", "```py\nclass SimpleMovingAverage[T <: AnyVal](period: Int)\n     (implicit num: Numeric[T], f: T => Double) //1\n  extends Etransform[Int](period) with MovingAverage[T] {\n\n  type U = XSeries[T]  //2\n  type V = DblVector   //3\n\n  val zeros = Vector.fill(0.0)(period-1) \n  override def |> : PartialFunction[U, Try[V]] = {\n    case xt: U if( xt.size >= period ) => {\n      val splits = xt.splitAt(period)\n      val slider = xt.take(xt.size - period).zip(splits._2)  //4\n\n      val zero = splits._1.sum/period //5\n      Try( zeros ++ slider.scanLeft(zero) {\n         case (s, (x,y)) => s + (x - y)/period }) //7\n  }\n}\n```", "```py\nclass WeightedMovingAverage[@specialized(Double) T <: AnyVal]( \n    weights: DblArray)\n    (implicit num: Numeric[T], f: T => Double) \n  extends SimpleMovingAverage[T](weights.length) {  //8\n\n  override def |> : PartialFunction[U, Try[V]] = {\n    case xt: U if(xt.size >= weights.length ) => {\n      val smoothed =  (config to xt.size).map( i => \n       xt.slice(i- config, i).zip(weights) //9\n             .map { case(x, w) => x*w).sum  //10\n      )\n      Try(zeros ++ smoothed) //11\n    }\n  }\n}\n```", "```py\nclass ExpMovingAverage[@specialized(Double) T <: AnyVal](  \n    alpha: Double)    //12\n    (implicit f: T => Double) \n  extends ETransform[Double](alpha) with MovingAverage[T]{ //13\n\n  type U = XSeries[T]    //14\n  type V = DblVector    //15\n\n  override def |> : PartialFunction[U, Try[V]] = {\n    case xt: U if( xt.size > 0) => {\n      val alpha_1 = 1-alpha\n      var y: Double = data(0)\n      Try( xt.view.map(x => {\n        val z = x*alpha + y*alpha_1; y = z; z})) //16\n    }\n}\n}\n```", "```py\ndef apply[T <: AnyVal](p: Int)\n     (implicit f: T => Double): ExpMovingAverage[T] = \n  new ExpMovingAverage[T](2/(p + 1))\n```", "```py\nimport YahooFinancials._\nval hp = p >>1\nval w = Array.tabulate(p)(n => \n       if(n == hp) 1.0 else 1.0/(Math.abs(n - hp)+1)) //17\nval sum = w.sum\nval weights = w.map { _ / sum }                          //18\n\nval dataSrc = DataSource(s\"$RESOURCE_PATH$symbol.csv\", false)//19\nval pfnSMvAve = SimpleMovingAverage[Double](p) |>         //20   \nval pfnWMvAve = WeightedMovingAverage[Double](weights) |>  \nval pfnEMvAve = ExpMovingAverage[Double](p) |>\n\nfor {\n   price <- dataSrc.get(adjClose)   //21\n   if(pfnSMvSve.isDefinedAt(price) )\n   sMvOut <- pfnSMvAve(price)         //22\n   if(pfnWMvSve.isDefinedAt(price)\n   eMvOut <- pfnWMvAve(price)\n  if(pfnEMvSve.isDefinedAt(price)\n   wMvOut <- pfnEMvAve(price)\n} yield {\n  val dataSink = DataSink[Double](s\"$OUTPUT_PATH$p.csv\")\n  val results = List[DblSeries](price, sMvOut, eMvOut, wMvOut)\n  dataSink |> results  //23\n}\n```", "```py\n   val pfnMv = SimpleMovingAverage[Double](period) |>\n   val smoothed = transform(xt, pfnMv)\n```", "```py\ntrait DTransform {\n  object Config {\n     final val FORWARD = TransformType.FORWARD\n     final val INVERSE = TransformType.INVERSE\n     final val SINE = DstNormalization.STANDARD_DST_I\n     final val COSINE = DctNormalization.STANDARD_DCT_I\n   }\n   …\n}\n```", "```py\ndef pad(vec: DblVector, \n    even: Boolean = true)(implicit f: T =>Double): DblArray = {\n  val newSize = padSize(vec.size, even)   //1\n  val arr: DblVector = vec.map(_.toDouble)\n  if( newSize > 0) arr ++ Array.fill(newSize)(0.0) else arr //2\n}\n\ndef padSize(xtSz: Int, even: Boolean= true): Int = {\n  val sz = if( even ) xtSz else xtSz-1  //3\n  if( (sz & (sz-1)) == 0) 0\n  else {\n    var bitPos = 0\n    do { bitPos += 1 } while( (sz >> bitPos) > 0) //4\n    (if(even) (1<<bitPos) else (1<<bitPos)+1) - xtSz\n  }\n}\n```", "```py\n val arr: DblVector = vec.map(_.toDouble)\n```", "```py\nclass DFT[@specialized(Double) T <: AnyVal](\n    eps: Double)(implicit f: T => Double)\n    extends ETransform[Double](eps) with DTransform { //5\n  type U = XSeries[T]   //6\n  type V = DblVector\n\n  override def |> : PartialFunction[U, Try[V]] = { //7\n    case xv: U if(xv.size >= 2) => fwrd(xv).map(_._2.toVector) \n  }\n}\n```", "```py\ndef fwrd(xv: U): Try[(RealTransformer, DblArray)] = {\n  val rdt = if(Math.abs(xv.head) < config)  //8\n      new FastSineTransformer(SINE)  //9\n  else  new FastCosineTransformer(COSINE)  //10\n\n  val padded = pad(xv.map(_.toDouble), xv.head == 0.0).toArray\n  Try( (rdt, rdt.transform(padded, FORWARD)) )\n}\n```", "```py\nval F = Array[Double](2.0, 5.0, 15.0)\nval A = Array[Double](2.0, 1.0, 0.33)\n\ndef harmonic(x: Double, n: Int): Double =  \n      A(n)*Math.cos(Math.PI*F(n)*x)\nval h = (x: Double) => \n    Range(0, A.size).aggregate(0.0)((s, i) => \n          s + harmonic(x, i), _ + _)\n```", "```py\nval OUTPUT1 = \"output/chap3/simulated.csv\"\nval OUTPUT2 = \"output/chap3/smoothed.csv\"\nval FREQ_SIZE = 1025; val INV_FREQ = 1.0/FREQ_SIZE\n\nval pfnDFT = DFT[Double] |> //11\nfor {\n  values <- Try(Vector.tabulate(FREQ_SIZE)\n               (n => h(n*INV_FREQ))) //12\n  output1 <- DataSink[Double](OUTPUT1).write(values)\n  spectrum <- pfnDFT(values)\n  output2 <- DataSink[Double](OUTPUT2).write(spectrum) //13\n} yield {\n  val results = format(spectrum.take(DISPLAY_SIZE), \n\"x/1025\", SHORT)\n  show(s\"$DISPLAY_SIZE frequencies: ${results}\")\n}\n```", "```py\nval convol = (n: Int, f: Double, fC: Double) => \n     if( Math.pow(f, n) < fC) 1.0 else 0.0\nval sinc = convol(1, _: Double, _:Double)\nval sinc2 = convol(2, _: Double, _:Double)\nval sinc4 = convol(4, _: Double, _:Double)\n```", "```py\nclass DFTFilter[@specialized(Double) T <: AnyVal]( \n    fC: Double,\n    eps: Double)\n    (g: (Double, Double) =>Double)(implicit f: T => Double)\n  extends DFT[T](eps) { //14\n\n  override def |> : PartialFunction[U, Try[V]] = {\n    case xt: U if( xt.size >= 2 ) => {\n      fwrd(xt).map{ case(trf, freq) => {  //15\n        val cutOff = fC*freq.size\n        val filtered = freq.zipWithIndex\n                     .map{ case(x, n) => x*g(n, cutOff) } //16\n        trf.transform(filtered, INVERSE).toVector }) //17\n    }\n  }\n}\n```", "```py\nimport YahooFinancials._\n\nval inputFile = s\"$RESOURCE_PATH$symbol.csv\"\nval src = DataSource(input, false, true, 1)\nval CUTOFF = 0.005\nval pfnDFTfilter = DFTFilter[Double](CUTOFF)(sinc) |>\nfor {\n  price <- src.get(adjClose)  //18\n  filtered <- pfnDFTfilter(price)  //19\n} \nyield { /* ... */ }\n```", "```py\ndef sinc(f: Double, w: (Double, Double)): Double = \n    if(f > w._1 && f < w._2) 1.0 else 0.0\n```", "```py\nimplicit def double2RealMatrix(x: DblMatrix): RealMatrix = \n    new Array2DRowRealMatrix(x)\nimplicit def double2RealRow(x: DblVector): RealMatrix = \n    new Array2DRowRealMatrix(x)\nimplicit def double2RealVector(x: DblVector): RealVector = \n    new ArrayRealVector(x)\n```", "```py\nval normal = Stats.normal(_)\nclass QRNoise(qr: DblPair,profile: Double=>Double = normal){ //1 \n  def q = profile(qr._1)\n  def r = profile(qr._2)\n  lazy val noiseQ = Array[Double](q, q)   //2\n  lazy val noiseR = Array[Double](r, r)\n}\n```", "```py\ncase class KalmanConfig(A: DblMatrix, B: DblMatrix, \n    H: DblMatrix, P: DblMatrix)\n```", "```py\nclass DKalman(config: KalmanConfig)(implicit qrNoise: QRNoise) \n            extends ETransform[KalmanConfig](config) {\n  type U = Vector[DblPair]  //3\n  type V = Vector[DblPair]  //4\n  type KRState = (KalmanFilter, RealVector)  //5\n  override def |> : PartialFunction[U, Try[V]]\n   ...\n}\n```", "```py\nx = A.operate(x).add(qrNoise.create(0.03, 0.1))\n```", "```py\n  override def |> : PartialFunction[U, Try[V]] = {\n    case xt: U if( !xt.isEmpty) => Try( \n      xt.map { case(current, prev) => {\n        val models = initialize(current, prev) //6\n        val nState = newState(models) //7\n        (nState(0), nState(1))  //8\n      }}\n    ) \n   }\n```", "```py\ndef initialize(current: Double, prev: Double): KRState = {  \n  val pModel = new DefaultProcessModel(config.A, config.B, \n                Q, input, config.P) //9\n  val mModel = new DefaultMeasurementModel(config.H, R) //10\n  val in = Array[Double](current, prev)\n  (new KalmanFilter(pModel,mModel), new ArrayRealVector(in))\n}\n```", "```py\ndef newState(state: KRState): DblArray = {\n  state._1.predict  //11\n  val x = config.A.operate(state._2).add(qrNoise.noisyQ) //12\n  val z = config.H.operate(x).add(qrNoise.noisyR) //13\n  state._1.correct(z)  //14\n  state._1.getStateEstimation  //15\n}\n```", "```py\nImport YahooFinancials._ \nval RESOURCE_DIR = \"resources/data/chap3/\"\nimplicit val qrNoise = new QRNoise((0.7, 0.3)) //16\n\nval H: DblMatrix = ((0.9, 0.0), (0.0, 0.1))    //17\nval P0: DblMatrix = ((0.4, 0.3), (0.5, 0.4))   //18\nval ALPHA1 = 0.5; val ALPHA2 = 0.8\nval src = DataSource(s\"${RESOURCE_DIR}${symbol}.csv\", false)\n\n(src.get(adjClose)).map(zt => {  //19\n   twoStepLagSmoother(zt, ALPHA1)     //20\n   twoStepLagSmoother(zt, ALPHA2)\n})\n```", "```py\ndef twoStepLagSmoother(zSeries: DblVector,alpha: Double): Int = { \n  val A: DblMatrix = ((alpha, 1.0-alpha), (1.0, 0.0))  //21\n  val xt = zipWithShift(1)  //22\n  val pfnKalman = DKalman(A, H, P0) |>    //23\n  pfnKalman(xt).map(filtered =>          //24\n    display(zSeries, filtered.map(_._1), alpha) )\n}\n```", "```py\ndef manhattan[T <: AnyVal, U <: AnyVal](\n    x: Array[T], \n    y: Array[U])(implicit f: T => Double): Double = \n (x,y).zipped.map{case (u,v) => Math.abs(u-v)}.sum\n```", "```py\ndef euclidean[T <: AnyVal, U <: AnyVal](\n    x: Array[T], \n    y: Array[U])(implicit f: T => Double): Double = \n  Math.sqrt((x,y).zipped.map{case (u,v) =>u-v}.map(sqr(_)).sum)\n```", "```py\ndef cosine[T <: AnyVal, U < : AnyVal] (\n     x: Array[T], \n     y: Array[U])(implicit f : T => Double): Double = {\n  val norms = (x,y).zipped\n          .map{ case (u,v) => Array[Double](u*v, u*u, v*v)}\n          ./:(Array.fill(3)(0.0))((s, t) => s ++ t) \n  norms(0)/Math.sqrt(norms(1)*norms(2))\n}\n```", "```py\n def dot(x:Array[Double], y:Array[Double]): Array[Double] = x.zip(y).map{case(x, y) => f(x,y) )\n```", "```py\n def dot(x:Array[Double], y:Array[Double]): Array[Double] = (x, y).zipped map ( _ * _)\n```", "```py\nclass Cluster[T <: AnyVal](val center: DblArray)\n     (implicit f: T => Double) {  //1\n  type DistanceFunc[T] = (DblArray, Array[T])=> Double\n  val members = new ListBuffer[Int]    //2\n  def moveCenter(xt: XVSeries[T]): Cluster[T] \n  ...\n}\n```", "```py\nobject Cluster {\n  def apply[T <: AnyVal](center: DblArray)\n      (implicit f: T => Double): Cluster[T] = \n     new Cluster[T](center)\n}\n```", "```py\ndef moveCenter(xt: XVSeries[T])\n       (implicit m: Manifest[T], num: Numeric[T])\n       : Cluster[T] = {  \n  val sum = transpose(members.map( xt(_)).toList)\n             .map(_.sum)  //3\n  Cluster[T](sum.map( _ / members.size).toArray) //4\n}\n```", "```py\ndef stdDev(xt: XVSeries[T], distance: DistanceFunc): Double = {\n  val ts = members.map(xt( _)).map(distance(center,_)) //5\n  Stats[Double](ts).stdDev  //6\n}\n```", "```py\ndef initialize(xt: U): V = {\n  val stats = statistics(xt)  //7\n  val maxSDevVar = Range(0,stats.size)  //8\n                   .maxBy(stats( _ ).stdDev )\n\n  val rankedObs = xt.zipWithIndex\n                .map{case (x, n) => (x(maxSDevVar), n)}\n                .sortWith( _._1  < _._1)  //9\n\n  val halfSegSize = ((rankedObs.size>>1)/_config.K)\n                  .floor.toInt //10\n  val centroids = rankedObs\n     .filter(isContained( _, halfSegSize, rankedObs.size))\n     .map{ case(x, n) => xt(n)} //11\n  centroids.aggregate(List[Cluster[T]]())((xs, c) => \n        Cluster[T](c) :: xs, _ ::: _)  //12\n}\n```", "```py\ndef isContained(t: (T,Int), hSz: Int, dim: Int): Boolean = \n    (t._2 % hSz == 0) && (t._2 %(hSz<<1) != 0)\n```", "```py\ndef assignToClusters(xt: U, clusters: V, \n     members: Array[Int]): Int =  {\n\n  xt.zipWithIndex.filter{ case(x, n) => {  //13\n    val nearestCluster = getNearestCluster(clusters, x) //14\n    val reassigned = nearestCluster != members(n) \n\n    clusters(nearestCluster) += n //15\n    members(n) = nearestCluster //16\n    reassigned\n  }}.size\n}\n```", "```py\ndef getNearestCluster(clusters: V, x: Array[T]): Int = \n  clusters.zipWithIndex./:((Double.MaxValue, 0)){\n    case (p, (c, n) ) => { \n     val measure = distance(c.center, x) //17\n     if( measure < p._1) (measure, n)  else p\n  }}._2\n```", "```py\ntype KMeansModel[T} = List[Cluster[T]] \ndef stdDev[T](\n    clusters: KMeansModel[T], \n    xt: XVSeries[T], \n    distance: DistanceFunc[T]): DblVector = \n clusters.map( _.stdDev(xt, distance)).toVector\n```", "```py\ncase class KMeansConfig(val K: Int, maxIters: Int)\n```", "```py\nclass KMeans[T <: AnyVal](config: KMeansConfig,  //18\n    distance: DistanceFunc[T],\n    xt: XVSeries[T])\n    (implicit m: Manifest[T], num: Numeric[T], f: T=>Double) //19\n  extends ITransform[Array[T]](xt) with Monitor[T] { \n\n  type V = Cluster[T]     //20\n  val model: Option[KMeansModel[T]] = train\n  def train: Option[KMeansModel[T]]\n  override def |> : PartialFunction[U, Try[V]]\n  ...\n}\n```", "```py\ndef train: Option[KMeansModel[T]] = Try {\n         // STEP 1\n  val clusters =initialize(xt) //21\n  if( clusters.isEmpty)  /* ...  */\n  else  {\n         // STEP 2\n    val members = Array.fill(xt.size)(0)\n    assignToClusters(xt, clusters, members) //22\n    var iters = 0\n\n    // Declaration of the tail recursion def update      \n    if( iters >= _config.maxIters )\n      throw new IllegalStateException( /* .. */)\n         // STEP 3\n    update(clusters, xt, members)  //23\n  } \n} match {\n   case Success(clusters) => Some(clusters)\n   case Failure(e) => /* … */\n}\n```", "```py\n@tailrec\ndef update(clusters: KMeansModel[T], xt: U, \n       members: Array[Int]): KMeansModel[T] = {  //24\n\n  val newClusters = clusters.map( c => {         \n      if( c.size > 0) c.moveCenter(xt) //25\n    else clusters.filter( _.size >0)\n          .maxBy(_.stdDev(xt, distance)) //26\n  }) \n  iters += 1\n  if(iters >= config.maxIters ||       //27\n      assignToClusters(xt, newClusters, members) ==0) \n    newClusters\n  else \n    update(newClusters, xt, membership)   //28\n}\n```", "```py\nval members = Array.fill(xt.size)(0)\nassignToClusters(xt, clusters, members) \nvar newClusters: KMeansModel[T] = List.empty[Cluster[T]]\nRange(0,  maxIters).find( _ => {  //29\n  newClusters = clusters.map( c => {   //30\n    if( c.size > 0)  c.moveCenter(xt) \n    else clusters.filter( _.size > 0)\n           .maxBy(_.stdDev(xt, distance))\n  }) \n  assignToClusters(xt, newClusters, members) > 0  //31\n}).map(_ => newClusters)\n```", "```py\ndef density: Option[DblVector] = \n  model.map( _.map( c => \n    c.getMembers.map(xt(_)).map( distance(c.center, _)).sum)\n```", "```py\noverride def |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] if( x.length == dimension(xt) \n                     && model != None) => \n  Try (model.map( _.minBy(c => distance(c.center,x))).get )\n}\n```", "```py\nval MAX_ITERS = 150\ndef density(K: Int, obs: XVSeries[Double]): DblVector = \n  KMeans[Double](KMeansConfig(K, MAX_ITERS)).density.get //32\n\n```", "```py\nimport YahooFinancials.) \nval START_INDEX = 80; val NUM_SAMPLES = 50  //33\nval PATH = \"resources/data/chap4/\"\n\ntype INPUT = Array[String] => Double\nval extractor = adjClose :: List[INPUT]() //34\nval symbolFiles = DataSource.listSymbolFiles(PATH) //35\n\nfor {\n  prices <- getPrices  //36\n  values <- Try(getPricesRange(prices))  //37\n  stdDev <- Try(ks.map( density(_, values.toVector))) //38\n  pfnKmeans <- Try { \n    KMeans[Double](KMeansConfig(5,MAX_ITERS),values.toVector) |> \n  }   //39\n  predict <- pfnKmeans(values.head)   //40\n} yield {\n  val results = s\"\"\"Daily prices ${prices.size} stocks\")\n     | \\nClusters density ${stdDev.mkString(\", \")}\"\"\"\n  .stripMargin\n  show(results)\n}\n```", "```py\ntype XVSeriesSet = Array[XVSeries[Double]]\ndef getPrices: Try[XVSeriesSet] = Try {\n   symbolFiles.map( DataSource(_, path) |> extractor )\n   .filter( _.isSuccess ).map( _.get)\n}\n```", "```py\ndef getPricesRange(prices: XVSeriesSet) = \n   prices.view.map(_.head.toArray)\n    .map( _.drop(START_INDEX).take(NUM_SAMPLES))\n```", "```py\nclass MultivariateEM[T <: AnyVal](K: Int, \n     xt: XVSeries[T])(implicit f: T => Double)\n  extends ITransform[Array[T]](xt) with Monitor[T] {  //1\n  type V = EMCluster  //2\n  val model: Option[EMModel ] = train //3\n  override def |> : PartialFunction[U, Try[V]]\n}\n```", "```py\ncase class EMCluster(key: Double, val means: DblArray, \n      val density: DblArray)   //4\ntype EMModel = List[EMCluster]\n```", "```py\n  def train: Option[EMModel ] = Try {\n  val data: DblMatrix = xt  //5\n  val multivariateEM = new EM(data)\n  multivariateEM.fit( estimate(data, K) ) //6\n\n  val newMixture = multivariateEM.getFittedModel  //7\n  val components = newMixture.getComponents.toList  //8\n  components.map(p =>EMCluster(p.getKey, p.getValue.getMeans, \n                      p.getValue.getStandardDeviations)) //9\n} match {/* … */}\n```", "```py\nTry {\n     ..\n} match {\n    case Success(results) => …\n    case Failure(exception)  => ...\n}\n```", "```py\noverride def |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] \n    if(isModel && x.length == dimension(xt)) => \n  Try( model.map(_.minBy(c => euclidean(c.means,x))).get)\n}\n```", "```py\nval K = 4; val period = 8\nval smAve = SimpleMovingAverage[Double](period)  //10\nval pfnSmAve = smAve |>    //11\n\nval obs = symbolFiles.map(sym => (\n  for {\n    xs <- DataSource(sym, path, true, 1) |>extractor //12\n    values <- pfnSmAve(xs.head)  //13\n    y <- Try {   \n        values.view.zipWithIndex.drop(period+1).toVector\n          .filter( _._2 % samplingRate == 0)\n          .map( _._1).toArray //14\n    }\n  } yield y).get) \n\nem(K, obs)  //15\n\n```", "```py\ndef em(K: Int, obs: DblMatrix): Int = {\n  val em = MultivariateEM[Double](K, obs.toVector) //16\n  show(s\"${em.toString}\")  //17\n}\n```", "```py\nclass PCA[@specialized(Double) T <: AnyVal](\n    xt: XVSeries[T])(implicit f: T => Double) \n  extends ITransform[Array[T]](xt) with Monitor[T] { //1\n  type V = Double    //2\n\n  val norm = (xv: XVSeries[T]) =>  zScores(xv) //3\n  val model: Option[PCAModel] = train //4\n  override def |> : PartialFunction[U, Try[V]]\n}\n```", "```py\ncase class PCAModel(val covariance: DblMatrix, \n   val eigenvalues: DblArray)\n```", "```py\ndef train: Option[PCAModel] = zScores(xt).map(x => { //5\n  val obs: DblMatrix = x.toArray\n  val cov = new Covariance(obs).getCovarianceMatrix //6\n\n  val transform = new EigenDecomposition(cov) //7\n  val eigenVectors = transform.getV  //8\n    val eigenValues = \n           new ArrayRealVector(transform.getRealEigenvalues)\n\n  val covariance = obs.multiply(eigenVectors).getData  //9\n  PCAModel(covariance, eigenValues.toArray)   //10\n}) match {/* … */}\n```", "```py\noverride def |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] \n     if(isModel && x.length == dimension(xt)) => \n        Try( inner(x, model.get.eigenvalues) )\n}\n```", "```py\nval data = Array[(String, DblVector)] (\n  // Ticker              PE     PS     PB   ROE    OM\n  (\"QCOM\", Array[Double](20.8, 5.32, 3.65, 17.65,29.2)),\n  (\"IBM\",  Array[Double](13, 1.22, 12.2, 88.1,19.9)), \n   …\n)\n```", "```py\nval dim = data.head._2.size\nval input = data.map( _._2.take(dim)) \nval pca = new PCA[Double](input) //11\nshow(s\"PCA model: ${pca.toString}\")  //12\n\n```", "```py\ndata.map( _._2.take(dim))\n```", "```py\nP(GDP=UP|1yTB=UP) = 110/159\nP(1yTB=UP) = num occurrences (1yTB=UP)/total num of occurrences=159/256\np(1yTB=UP|GDP=UP,FDF=UP,CPI=UP) = p(GDP=UP|1yTB=UP) x\n                                  p(FDF=UP|1yTB=UP) x\n                                  p(CPI=UP|1yTB=UP) x\n                                  p(1yTB=UP) = 0.69 x 0.85 x 0.80 x 0.625\n```", "```py\nclass Likelihood[T <: AnyVal](\n     val label: Int, \n     val muSigma: Vector[DblPair], \n     val prior: Double)(implicit f: T => Double)  {  //1\n\n  def score(obs: Array[T], logDensity: LogDensity): Double = //2\n    (obs, muSigma).zipped\n     .map{ case(x, (mu,sig)) => (x, mu, sig)}\n     ./:(0.0)((prob, entry) => {\n         val x = entry._1\n         val mean = entry._2\n         val stdDev = entry._3\n         val logLikelihood = logDensity(mean, stdDev, x) //3\n         val adjLogLikelihood = if(logLikelihood <MINLOGARG)\n                     MINLOGVALUE else logLikelihood)\n         prob + Math.log(adjLogLikelihood) //4\n   }) + Math.log(prior)\n}\n```", "```py\ndef logGauss(mean: Double, stdDev: Double, x: Double): Double ={\n  val y = (x - mean)/stdDev\n  -LOG_2PI - Math.log(stdDev) - 0.5*y*y\n}\nval logNormal = logGauss(0.0, 1.0, _: Double)\n```", "```py\ntype LogDensity = (Double*) => Double\n```", "```py\ntrait NaiveBayesModel[T]  {\n  def classify(x: Array[T], logDensity: LogDensity): Int //5\n}\n```", "```py\nclass BinNaiveBayesModel[T <: AnyVal](\n    pos: Likelihood[T], \n    neg: Likelihood[T])(implicit f: T => Double)\n  extends NaiveBayesModel[T] { //6\n\n  override def classify(x: Array[T], logDensity: logDensity): Int = //7\n   if(pos.score(x,density) > neg.score(x,density)) 1 else 0\n  ...\n}\n```", "```py\nclass MultiNaiveBayesModel[T <: AnyVal(   //8\n   likelihoodSet: Seq[Likelihood[T]])(implicit f: T => Double)\n  extends NaiveBayesModel[T]{\n\n  override def classify(x: Array[T], logDensity: LogDensity): Int = {\n    val <<< = (p1: Likelihood[T], p2: Likelihood[T]) => \n              p1.score(x, density) > p1.score(x, density) //9\n    likelihoodSet.sortWith(<<<).head.label  //10\n  }\n  ...\n}\n```", "```py\nclass NaiveBayes[T <: AnyVal](\n    smoothing: Double, \n    xt: XVSeries[T],\n    expected: Vector[Int],\n    logDensity: LogDensity,\n    classes: Int)(implicit f: T => Double) \n  extends ITransform[Array[T]](xt) \n    with Supervised[T, Array[T]] with Monitor[Double] { //11\n  type V = Int  //12\n  val model: Option[NaiveBayesModel[T]]  //13\n  def train(expected: Int): Likelihood[T]\n  …\n}\n```", "```py\nval model: Option[NaiveBayesModel[T]] = Try {\n  if(classes == 2) \n    BinNaiveBayesModel[T](train(1), train(0))\n  else \n    MultiNaiveBayesModel[T](List.tabulate(classes)( train(_)))\n} match {\n  case Success(_model) => Some(_model)\n  case Failure(e) => /* … */\n}\n```", "```py\ndef train(index: Int): Likelihood[T] = {   //14\n  val xv: XVSeries[Double] = xt\n  val values = xv.zip(expected) //15\n               .filter( _._2 == index).map(_._1) //16\n  if( values.isEmpty )\n     throw new IllegalStateException( /* ... */)\n\n  val dim = dimension(xt)\n  val meanStd = statistics(values).map(stat => \n         (stat.lidstoneMean(smoothing, dim), stat.stdDev)) //17\n  Likelihood(index, meanStd, values.size.toDouble/xv.size) //18\n}\n```", "```py\nobject NaiveBayes {\n  def apply[T <: AnyVal](\n        smoothing: Double, \n        xt: XVSeries[T],\n        expected: Vector[Int],\n        logDensity: LogDensity,\n        classes: Int) (implicit f: T => Double): NaiveBayes[T] = \n    new NaiveBayes[T](smoothing, xt, y, logDensity, classes)\n   …\n}\n```", "```py\noverride def |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] if(x.length >0 && model != None) => \n    Try( model.map(_.classify(x, logDensity)).get)  //19\n}\n```", "```py\ntrait Supervised[T, V] {\n  self: ITransform[V] =>  //20\n    def validate(xt: XVSeries[T], \n      expected: Vector[V]): Try[Double]  //21\n}\n```", "```py\noverride def validate(\n    xt: XVSeries[T], \n    expected: Vector[V]): Try[Double] =  Try {   //22\n  val predict = model.get.classify(_:Array[Int],logDensity) //23\n  MultiFValidation(expected, xt, classes)(predict).score  //24\n}\n```", "```py\nval extractor = toDouble(CLOSE)  // stock closing price\n               :: ratio(HIGH, LOW) //volatility(HIGH-LOW)/HIGH\n               :: toDouble(VOLUME)  // daily stock trading volume\n               :: List[Array[String] =>Double]()\n```", "```py\nval trainRatio = 0.8  //25\nval period = 4\nval symbol =\"IBM\"\nval path = \"resources/chap5\"\nval pfnMv = SimpleMovingAverage[Double](period, false) |> //26\nval pfnSrc = DataSource(symbol, path, true, 1) |>  //27\n\nfor {\n  obs <- pfnSrc(extractor)  //28\n  (x, delta) <- computeDeltas(obs) //29\n  expected <- Try{difference(x.head.toVector, diffInt)}//30\n  features <- Try { transpose(delta) } //31\n  labeled <- //32\n     OneFoldXValidation[Int](features, expected, trainRatio) \n  nb <- NaiveBayes[Int](1.0, labeled.trainingSet) //33\n  f1Score <- nb.validate(labeled.validationSet) //34\n}\nyield {\n  val labels = Array[String](\n    \"price/ave price\", \"volatility/ave. volatility\",\n    \"volume/ave. volume\"\n  )\n  show(s\"\\nModel: ${nb.toString(labels)}\")\n}\n```", "```py\ntype LabeledPairs = (XVSeries[Double], Vector[Array[Int]])\n\ndef computeDeltas(obs: XVSeries[Double]): Try[LabeledPairs] =\nTry{\n  val sm = obs.map(_.toVector).map( pfnMv(_).get.toArray) //35\n  val x = obs.map(_.drop(period-1) )\n  (x, x.zip(sm).map{ case(x,y) => x.zip(y).map(delta(_)) })//36\n}\n```", "```py\nimplicit def intToDouble(n: Int): Double = n.toDouble\n```", "```py\nobject Stats {\n  def bernoulli(mean: Double, p: Int): Double = \n     mean*p + (1-mean)*(1-p)\ndef bernoulli(x: Double*): Double = bernoulli(x(0), x(1).toInt)\n…\n```", "```py\ncase class Document[T <: AnyVal]( //1\ndate: T, title: String, content: String)\n(implicit f: T => Double)   \n```", "```py\ntype TermsRF = Map[String, Double]  \ntype TextParser = String => Array[String] //2\ntype Lexicon = immutable.Map[String, String]  //3\ntype Corpus[T] = Seq[Document[T]]\n\nclass TextAnalyzer[T <: AnyVal](  //4\n     parser: TextParser, \n     lexicon: Lexicon)(implicit f: T => Double)\n  extends ETransform[Lexicon](lexicon) {\n\n  type U = Corpus[T]    //5\n  type V = Seq[TermsRF] //6\n\n  override def |> : PartialFunction[U, Try[V]] = {\n     case docs: U => Try( score(docs) )\n  }\n\n  def score(corpus: Corpus[T]): Seq[TermsRF]  //7\n  def quantize(termsRFSeq: Seq[TermsRF]): //8\n          Try[(Array[String], XVSeries[Double])]\n  def count(term: String): Counter[String] //9\n}\n```", "```py\ndef score(corpus: Corpus[T]): Seq[TermsRF] = {\n  val termsCount = corpus.map(doc =>  //10\n      (doc.date, count(doc.content))) //Seq[(T, Counter[String])]\n\n  val termsCountMap = termsCount.groupBy( _._1).map{ \n     case (t, seq) => (t, seq.aggregate(new Counter[String])\n                         ((s, cnt) => s ++ cnt._2, _ ++ _)) //11\n  }\n  val termsCountPerDate = termsCountMap.toSeq\n         .sortWith( _._1 < _._1).unzip._2  //12\n  val allTermsCounts = termsCountPerDate\n          .aggregate(new Counter[String])((s, cnt) => \n                               s ++ cnt, _ ++ _) //13\n\n  termsCountPerDate.map( _ /allTermsCounts).map(_.toMap) //14\n}\n```", "```py\ndef count(term: String): Counter[String] = \n   parser(term)./:(new Counter[String])((cnt, w) =>   //16\n   if(lexicon.contains(w)) cnt + lexicon(w) else cnt)\n```", "```py\ndef quantize(termsRFSeq: Seq[TermsRF]): \n             Try[(Array[String], XVSeries[Double])] = Try {\n  val keywords = lexicon.values.toArray.distinct //15\n  val relFrequencies = \n      termsRFSeq.map( tf =>  //16\n          keywords.map(key => \n              if(tf.contains(key)) tf.get(key).get else 0.0))\n  (keywords, relFrequencies.toVector) //17\n}\n```", "```py\nval pathLexicon = \"resources/text/lexicon.txt\"\nval LEXICON = loadLexicon  //18\n\ndef parse(content: String): Array[String] = {\n  val regExpr = \"['|,|.|?|!|:|\\\"]\"\n  content.trim.toLowerCase.replace(regExpr,\" \") //19\n  .split(\" \") //20\n  .filter( _.length > 2) //21\n}\n```", "```py\nval pathCorpus = \"resources/text/chap5/\"   //22\nval dateFormat = new SimpleDateFormat(\"MM.dd.yyyy\")\nval pfnDocs = DocumentsSource(dateFormat, pathCorpus) |>  //23\n\nval textAnalyzer = TextAnalyzer[Long](parse, LEXICON)\nval pfnText = textAnalyzer |>   //24\n\nfor {\n  corpus <- pfnDocs(None)  //25\n  termsFreq <- pfnText(corpus)  //26\n  featuresSet <- textAnalyzer.quantize(termsFreq) //27\n  expected <- Try(difference(TSLA_QUOTES, diffInt)) //28\n  nb <- NaiveBayes[Double](1.0, \n             featuresSet._2.zip(expected))//29\n} yield {\n  show(s\"Naive Bayes model${nb.toString(quantized._1)}\")\n   …\n}\n```", "```py\nval TSLA_QUOTES = Array[Double](250.56, 254.84, … )\n```", "```py\nclass SingleLinearRegression[T <: AnyVal](\n    xt: XSeries[T], \n    expected: Vector[T])(implicit f: T => Double)\n  extends ITransform[T](xt) with Monitor[Double] {   //1\n  type V = Double  //2\n\n  val model: Option[DblPair] = train //3\n  def train: Option[DblPair]\n  override def |> : PartialFunction[T, Try[V]]\n  def slope: Option[Double] = model.map(_._1)\n  def intercept: Option[Double] = model.map(_._2)\n}\n```", "```py\ndef train: Option[DblPair] = {\n   val regr = new SimpleRegression(true) //4\n   regr.addData(zipToSeries(xt, expected).toArray)  //5\n   Some((regr.getSlope, regr.getIntercept))  //6\n}\n```", "```py\nval path = \"resources/data/chap6/CU.csv\"\nfor {\n  price <- DataSource(path, false, true, 1) get adjClose //7\n  days <- Try(Vector.tabulate(price.size)(_.toDouble)) //8\n  linRegr <- SingleLinearRegression[Double](days, price) //9\n} yield {\n  if( linRegr.isModel ) {\n    val slope = linRegr.slope.get\n    val intercept = linRegr.intercept.get\n    val error = mse(days, price, slope, intercept)//10\n  }\n  …\n}\n```", "```py\ndef mse(\n    predicted: DblVector, \n    expected: DblVector, \n    slope: Double, \n    intercept: Double): Double = {\n  val predicted = xt.map( slope*_ + intercept)\n  XTSeries.mse(predicted, expected)  //11\n}\n```", "```py\ntrait Regression {\n  val model: Option[RegressionModel] = training //1\n\n  def weights: Option[DblArray] = model.map( _.weights)//2\n  def rss: Option[Double] = model.map(_.rss) //3\n  def isModel: Boolean = model != None\n\n  protected def train: RegressionModel  //4\n  def training: Option[RegressionModel] = Try(train) match {\n    case Success(_model) => Some(_model)\n    case Failure(e) => e match {\n      case err: MatchError => { … }        case _ => { … }\n    }\n  }\n}\n```", "```py\ncase class RegressionModel(  //5\n   val weights: DblArray, val rss: Double) extends Model\n\nobject RegressionModel {\n  def dot[T <: AnyVal](x: Array[T], \n     w: DblArray)(implicit f: T => Double): Double = \n     x.zip(weights.drop(1))  \n      .map{ case(_x, w) => _x*w}.sum + weights.head //6\n}\n```", "```py\nclass MultiLinearRegression[T <: AnyVal](\n     xt: XVSeries[T], \n     expected: DblVector)(implicit f: T => Double)\n  extends ITransform[Array[T]](xt) with Regression \n       with Monitor[Double] { //7\n  type V = Double  //8\n\n  override def train: Option[RegressionModel] //9\n  override def |> : PartialFunction[Array[T], Try[V]] //10\n}\n```", "```py\ndef train: RegressionModel = {\n  val olsMlr = new MultiLinearRAdapter   //11\n  olsMlr.createModel(expected, data)  //12\n  RegressionModel(olsMlr.weights, olsMlr.rss) //13\n}\n```", "```py\nclass MultiLinearRAdapter extends OLSMultipleLinearRegression {\n  def createModel(y: DblVector, x: Vector[DblArray]): Unit = \n     super.newSampleData(y.toArray, x.toArray)\n\n  def weights: DblArray = estimateRegressionParameters\n  def rss: Double = calculateResidualSumOfSquares\n}\n```", "```py\ndef |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] if isModel && \n       x.length == model.get.size-1  \n         => Try( dot(x, model.get) ) //14\n}\n```", "```py\nimport  YahooFinancials._\nval path = \"resources/data/chap6/CU.csv\"  //15\nval src = DataSource(path, true, true, 1)  //16\n\nfor {\n  price <- src.get(adjClose)  //17\n  volatility <- src.get(volatility)  //18\n  volume <- src.get(volume)  //19\n  (features, expected) <- differentialData(volatility, \n                     volume, price, diffDouble)  //20\n  regression <- MultiLinearRegression[Double]( \n                  features, expected)  //21\n} yield {\n  if( regression.isModel ) {\n    val trend = features.map(dot(_,regression.weights.get))\n    display(expected, trend)  //22\n  }\n}\n```", "```py\ndef getRss(\n     xt: Vector[DblArray], \n     expected: DblVector, \n     featureLabels: Array[String]): String = {\n\n  val regression = \n         new MultiLinearRAdapter[Double](xt,expected) //23\n  val modelStr = regression.weights.get.view\n        .zipWithIndex.map{ case( w, n) => {\n    val weights_str = format(w, emptyString, SHORT)\n    if(n == 0 ) s\"${featureLabels(n)} = $weights_str\"\n    else s\"${weights_str}.${featureLabels(n)}\"\n  }}.mkString(\" + \")\n  s\"model: $modelStr\\nRSS =${regression.get.rss}\" //24\n}\n```", "```py\nval SMOOTHING_PERIOD: Int = 16  //25\nval path = \"resources/data/chap6/\"\nval symbols = Array[String](\"CNY\", \"GLD\", \"SPY\", \"TLT\") //26\nval movAvg = SimpleMovingAverage[Double](SMOOTHING_PERIOD) //27\n\nfor {\n  pfnMovAve <- Try(movAvg |>)  //28\n  smoothed <- filter(pfnMovAve)  //29\n  models <- createModels(smoothed)  //30\n  rsses <- Try(getModelsRss(models, smoothed)) //31\n  (mses, tss) <- totalSquaresError(models,smoothed.head) //32\n} yield {\n   s\"\"\"${rsses.mkString(\"\\n\")}\\n${mses.mkString(\"\\n\")}\n      | \\nResidual error= $tss\".stripMargin\n}\n```", "```py\n    Type PFNMOVAVE = PartialFunction[DblVector, Try[DblVector]]\n\n    def filter(pfnMovAve: PFNMOVEAVE): Try[Array[DblVector]] = Try {\n       symbols.map(s => DataSource(s\"$path$s.csv\", true, true, 1))\n          .map( _.get(adjClose) )\n          .map( pfnMovAve(_)).map(_.get)\n    ```", "```py\n    type Models = List[(Array[String], DblMatrix)]\n\n    def createModels(smoothed: Array[DblVector]): Try[Models] = \n    Try {\n      val features = smoothed.drop(1).map(_.toArray)  //33\n      List[(Array[String], DblMatrix)](   //34\n       (Array[String](\"CNY\",\"SPY\",\"GLD\",\"TLT\"), features.transpose),\n       (Array[String](\"CNY\",\"GLD\",\"TLT\"),features.drop(1).transpose),\n       (Array[String](\"CNY\",\"SPY\",\"GLD\"),features.take(2).transpose),\n       (Array[String](\"CNY\",\"SPY\",\"TLT\"), features.zipWithIndex\n                          .filter( _._2 != 1).map( _._1).transpose),\n       (Array[String](\"CNY\",\"GLD\"), features.slice(1,2).transpose)\n       )\n    }\n    ```", "```py\n    def getModelsRss(\n        models: Models, \n        y: Array[DblVector]): List[String] = \n      models.map{ case (labels, m) => \n             s\"${getRss(m.toVector, y.head, labels)}\" }  //35\n\n    ```", "```py\n    def totalSquaresError(\n        models: Models, \n        expected: DblVector): Try[(List[String], Double)] = Try {\n\n      val errors = models.map{case (labels,m) => \n    rssSum(m, expected)._1}//36\n      val mses = models.zip(errors)\n               .map{case(f,e) => s\"MSE: ${f._1.mkString(\" \")} = $e\"}\n      (mses, Math.sqrt(errors.sum)/models.size)  //37\n    }\n    ```", "```py\ndef rssSum(xt: DblMatrix, expected: DblVector): DblPair = {\n  val regression = \n         MultiLinearRegression[Double](xt, expected) //38\n  val pfnRegr = regression |>  //39\n  val results = sse(expected.toArray, xt.map(pfnRegr(_).get))\n  (regression.rss, results) //40\n}\n```", "```py\nclass RidgeRegression[T <: AnyVal](  //1\n     xt: XVSeries[T], \n     expected: DblVector, \n     lambda: Double)(implicit f: T => Double)\n   extends ITransform[Array[T]](xt) with Regression \n       with Monitor[Double] { //2\n\n  type V = Double //3\n  override def train: Option[RegressionModel]  //4\n  override def |> : PartialFunction[Array[T], Try[V]]\n}\n```", "```py\ndef train: RegressionModel = {\n  val mlr = new RidgeRAdapter(lambda, xt.head.size) //5\n  mlr.createModel(data, expected) //6\n  RegressionModel(mlr.getWeights, mlr.getRss)  //7\n}\n```", "```py\nclass RidgeRAdapter(\n    lambda: Double, \n    dim: Int) extends AbstractMultipleLinearRegression {\n  var qr: QRDecomposition = _  //8\n\n  def createModel(x: DblMatrix, y: DblVector): Unit ={ //9\n    this.newXSampleData(x) //10\n    super.newYSampleData(y.toArray)\n  }\n  def getWeights: DblArray = calculateBeta.toArray //11\n  def getRss: Double = rss\n}\n```", "```py\noverride protected def newXSampleData(x: DblMatrix): Unit =  {\n  super.newXSampleData(x)    //12\n  val r: RealMatrix = getX\n  Range(0, dim).foreach(i => \n        r.setEntry(i, i, r.getEntry(i,i) + lambda) ) //13\n  qr = new QRDecomposition(r) //14\n}\n```", "```py\noverride protected def calculateBeta: RealVector =\n   qr.getSolver().solve(getY()) //15\n\n```", "```py\ndef |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] if(isModel && \n      x.length == model.get.size-1) => \n        Try( dot(x, model.get) ) //16\n}\n```", "```py\nval LAMBDA: Double = 0.5\nval src = DataSource(path, true, true, 1)  //17\n\nfor {\n  price <- src.get(adjClose)   //18\n  volatility <- src.get(volatility) //19\n  volume <- src.get(volume)  //20\n  (features, expected) <- differentialData(volatility, \n              volume, price, diffDouble) //21\n  regression <- RidgeRegression[Double](features, \nexpected, LAMBDA)  //22\n} yield {\n  if( regression.isModel ) {\n    val trend = features\n               .map( dot(_, regression.weights.get) )  //23\n\n    val y1 = predict(0.2, expected, volatility, volume) //24\n    val y2 = predict(5.0, expected, volatility, volume)\n    val output = (2 until 10 by 2).map( n => \n          predict(n*0.1, expected, volatility, volume) )\n  }\n}\n```", "```py\ndef predict(\n    lambda: Double, \n    deltaPrice: DblVector, \n    volatility: DblVector, \n    volume: DblVector): DblVector = {\n\n  val observations = zipToSeries(volatility, volume)//25\n  val regression = new RidgeRegression[Double](observations, \n          deltaPrice, lambda)\n  val fnRegr = regression |> //26\n  observations.map( fnRegr(_).get)  //27\n}\n```", "```py\nclass LogisticRegression[T <: AnyVal](\n    xt: XVSeries[T], \n    expected: Vector[Int], \n    optimizer: LogisticRegressionOptimizer)  //1\n    (implicit f: T => Double)\n   extends ITransform[Array[T]](xt) with Regression \n       with Monitor[Double] { //2\n\n  type V = Int //3\n    override def train: RegressionModel  //4\n  def |> : PartialFunction[Array[T], Try[V]]\n}\n```", "```py\ndef train: RegressionModel = {\n  val weights0 = Array.fill(data.head.length +1)(INITIAL_WEIGHT)\n  val lrJacobian = new RegressionJacobian(data, weights0) //5\n  val exitCheck = new RegressionConvergence(optimizer) //6\n\n  def createBuilder: LeastSquaresProblem  //7\n  val optimum = optimizer.optimize(createBuilder) //8\n  RegressionModel(optimum.getPoint.toArray, optimum.getRMS)\n}\n```", "```py\nclass LogisticRegressionOptimizer(\n     maxIters: Int, \n     maxEvals: Int,\n     eps: Double, \n     lsOptimizer: LeastSquaresOptimizer) {  //9\n  def optimize(lsProblem: LeastSquaresProblem): Optimum = \n       lsOptimizer.optimize(lsProblem)\n}\n```", "```py\nclass RegressionJacobian[T <: AnyVal](  //10\n    xv: XVSeries[T], \n    weights0: DblArray)(implicit f: T => Double) \n  extends MultivariateJacobianFunction {\n\n  type GradientJacobian = Pair[RealVector, RealMatrix]\n  override def value(w: RealVector): GradientJacobian = { //11\n    val gradient = xv.map( g => { //12\n      val f = logistic(dot(g, w))//13\n     (f, f*(1.0-f))  //14\n   })\n   xv.zipWithIndex   //15\n    ./:(Array.ofDim[Double](xv.size, weights0.size)) {\n     case (j, (x,i)) => {   \n       val df = gradient(i)._2\n       Range(0, x.size).foreach(n => j(i)(n+1) = x(n)*df)\n       j(i)(0) = 1.0; j //16\n     }\n   }\n   (new ArrayRealVector(gradient.map(_._1).toArray), \n      new Array2DRowRealMatrix(jacobian))  //17\n  }\n}\n```", "```py\nval exitCheck = new ConvergenceChecker[PointVectorValuePair] {\n  override def converged(\n      iters: Int, \n      prev: PointVectorValuePair, \n      current:PointVectorValuePair): Boolean =  \n   sse(prev.getValue, current.geValue) < optimizer.eps \n           && iters >= optimizer.maxIters //18\n}\n```", "```py\ndef createBuilder: LeastSquaresProblem = \n   (new LeastSquaresBuilder).model(lrJacobian)    //19\n   .weight(MatrixUtils.createRealDiagonalMatrix(\n             Array.fill(xt.size)(1.0)))  //20\n   .target(expected.toArray) //21\n   .checkerPair(exitCheck)  //22\n   .maxEvaluations(optimizer.maxEvals)  //23\n   .start(weights0)  //24\n   .maxIterations(optimizer.maxIters) //25\n   .build\n```", "```py\nval optimum = optimizer.optimize(lsp)\n(optimum.getPoint.toArray, optimum.getRMS)\n```", "```py\nimport YahooFinancials._ \nval maxIters = 250\nval maxEvals = 4500\nval eps = 1e-7\n\nval src = DataSource(path, true, true, 1)  //26\nval optimizer = new LevenbergMarquardtOptimizer  //27\n\nfor {\n  price <- src.get(adjClose) //28\n  volatility <- src.get(volatility)  //29\n  volume <- src.get(volume)  //30\n  (features, expected) <- differentialData(volatility, \nvolume, price, diffInt) //31\n  lsOpt <- LogisticRegressionOptimizer(maxIters, maxEvals, \n                        eps, optimizer) //32\n  regr <- LogisticRegression[Double](features, expected, lsOpt)      \n  pfnRegr <- Try(regr |>) //33\n} \nyield {\n   show(s\"${LogisticRegressionEval.toString(regr)}\")\n   val predicted = features.map(pfnRegr(_))\n   val delta = predicted.view.zip(expected.view)\n            .map{case(p, e) => if(p.get == e) 1 else 0}.sum\n   show(s\"Accuracy: ${delta.toDouble/expected.size}\")\n}\n```", "```py\nval HYPERPLANE = - Math.log(1.0/INITIAL_WEIGHT -1)\ndef |> : PartialFunction[Array[T], Try[V]] = {\n  case x: Array[T] if(isModel && \n      model.size-1 == x.length && isModel)  => \n       Try (if(dot(x, model) > HYPERPLANE) 1 else 0 ) //34\n}\n```", "```py\nclass HMMConfig(val numObs: Int, val numStates: Int, \n    val numSymbols: Int, val maxIters: Int, val eps: Double) \n    extends Config\n```", "```py\nobject HMMConfig {\n  def foreach(i: Int, f: Int => Unit): Unit =\n      Range(0, i).foreach(f)  //1\n  def /:(i: Int, f: (Double, Int) => Double, zero: Double) = \n        Range(0, i)./:(zero)(f) //2\n  def maxBy(i: Int, f: Int => Double): Int = \n      Range(0,i).maxBy(f)   //3\n   … \n}\n```", "```py\nclass HMMModel( val A: DMatrix, val B: DMatrix, var pi: DblArray, \n    val numObs: Int) {   //4\n  val numStates = A.nRows\n  val numSymbols = B.nCols\n\n  def setAlpha(obsSeqNum: Vector[Int]): DMatrix\n  def getAlphaVal(a: Double, i: Int, obsId: Int): Double\n  def getBetaVal(b: Double, i: Int, obsId: Int): Double\n  def update(gamma: Gamma, diGamma: DiGamma, \n      obsSeq: Vector[Int])\n  def normalize: Unit\n}\n```", "```py\nclass HMMTreillis(numObs: Int, numStates: Int){ //5\n  var treillis: DMatrix = _   //6\n  val ct = Array.fill(numObs)(0.0) \n\n   def normalize(t: Int): Unit = { //7\n     ct.update(t, /:(numStates, (s, n) => s + treillis(t, n)))\n     treillis /= (t, ct(t))\n   }\n   def getTreillis: DMatrix = treillis\n}\n```", "```py\nclass Alpha(lambda: HMMModel, obsSeq: Vector[Int]) //8\n    extends HMMTreillis(lambda.numObs, lambda.numStates) {\n\n  val alpha: Double = Try { \n    treillis = lambda.setAlpha(obsSeq) //9\n    normalize(0)  //10\n    sumUp  //11\n  }.getOrElse(Double.NaN)\n\n  override def isInitialized: Boolean = alpha != Double.NaN\n\n  val last = lambda.numObs-1\n  def sumUp: Double = {\n    foreach(1, lambda.numObs, t => {\n      updateAlpha(t) //12\n      normalize(t)  //13\n    })\n    /:(lambda.numStates, (s,k) => s + treillis(last, k))\n  }\n\n  def updateAlpha(t: Int): Unit = \n    foreach(lambda.numStates, i => { //14\n      val newAlpha = lambda.getAlphaVal(treillis(t-1, i)\n      treillis += (t, i, newAlpha, i, obsSeq(t))) \n    })\n\n  def logProb: Double = /:(lambda.numObs, (s,t) => //15\n    s + Math.log(ct(t)), Math.log(alpha))\n}\n```", "```py\ndef setAlpha(obsSeq: Array[Int]): DMatrix = \n  Range(0,numStates)./:(DMatrix(numObs, numStates))((m,j) => \n      m += (0, j, pi(j)*B(j, obsSeq.head)))\n}\n```", "```py\nclass Beta(lambda: HMMModel, obsSeq: Vector[Int]) \n     extends HMMTreillis(lambda.numObs, lambda.numStates) {\n\n  val initialized: Boolean  //16\n\n  override def isInitialized: Boolean = initialized\n  def sumUp: Unit =   //17\n    (lambda.numObs-2 to 0 by -1).foreach(t => { //18\n      updateBeta(t)  //19\n      normalize(t) \n    })\n\n   def updateBeta(t: Int): Unit =\n     foreach(lambda.numStates, i => { \n       val newBeta = lambda.getBetaVal(treillis(t+1, i)\n       treillis += (t, i, newBeta, i, obsSeq(t+1))) //20\n     })\n}\n```", "```py\nval initialized: Boolean = Try {\n  treillis = DMatrix(lambda.numObs, lambda.numStates)\n  treillis += (lambda.numObs-1, 1.0) //21\n  normalize(lambda.numObs-1)  //22\n  sumUp  //23\n}._toBoolean(\"Beta initialization failed\")\n```", "```py\nclass BaumWelchEM(config: HMMConfig, obsSeq: Vector[Int]) { //24\n  val lambda = HMMModel(config)\n  val diGamma = new DiGamma(lambda.numObs,lambda.numStates)//25\n  val gamma = new Gamma(lambda.numObs, lambda.numStates) //26\n  val maxLikelihood: Option[Double] //27\n}\n```", "```py\nclass DiGamma(numObs: Int, numStates: Int) {\n  val diGamma = Array.fill(numObs-1)(DMatrix(numStates))\n  def update(alpha: DMatrix, beta: DMatrix, A: DMatrix, \n  B: DMatrix, obsSeq: Array[Int]): Try[Int]\n}\n```", "```py\nclass Gamma(numObs: Int, numStates: Int) {\n  val gamma = DMatrix(numObs, numStates)\n  def update(alpha: DMatrix, beta: DMatrix): Unit\n}\n```", "```py\nval maxLikelihood: Option[Double] = Try {\n\n  @tailrec\n  def getLikelihood(likelihood: Double, index: Int): Double ={\n    lambda.update(gamma, diGamma, obsSeq) //28\n    val _likelihood = frwrdBckwrdLattice   //29\n    val diff = likelihood - _likelihood\n\n    if( diff < config.eps ) _likelihood    //30\n    else if (index >= config.maxIters)  //31\n      throw new IllegalStateException(\" … \")\n    else getLikelihood(_likelihood, index+1) \n  }\n\n  val max = getLikelihood(frwrdBckwrdLattice, 0)\n  lambda.normalize   //32\n  max\n}._toOption(\"BaumWelchEM not initialized\", logger)\n```", "```py\ndef frwrdBckwrdLattice: Double  = {\n  val _alpha = Alpha(lambda, obsSeq) //33\n  val beta = Beta(lambda, obsSeq).getTreillis //34\n  val alphas = _alpha.getTreillis\n  gamma.update(alphas, beta) //35\n  diGamma.update(alphas, beta, lambda.A, lambda.B, obsSeq)\n  _alpha.alpha\n}\n```", "```py\nclass ViterbiPath(lambda: HMMModel, obsSeq: Vector[Int]) {\n  val nObs = lambda.numObs\n  val nStates = lambda.numStates\n  val psi = Array.fill(nObs)(Array.fill(nStates)(0)) //35\n  val qStar = new QStar(nObs, nStates) //36\n\n  val delta = { //37\n    Range(0, nStates)./:(DMatrix(nObs, nStates))((m,n) => {\n     psi(0)(n) = 0\n     m += (0, n, lambda.pi(n) * lambda.B(n,obsSeq.head))\n    })\n  val path = HMMPrediction(viterbi(1), qStar()) //38\n}\n```", "```py\ncase class HMMPrediction(likelihood: Double, states: Array[Int])\n```", "```py\n@tailrec\ndef viterbi(t: Int): Double = {\n  Range(0, numStates).foreach( updateMaxDelta(t, _)) //39\n\n  if( t == obsSeq.size-1) {  //40\n    val idxMaxDelta = Range(0, numStates)\n                .map(i => (i, delta(t, i))).maxBy(_._2) //41\n    qStar.update(t+1, idxMaxDelta._1)  //42\n    idxMaxDelta._2\n  }\n  else viterbi(t+1)  //43\n}\n```", "```py\ndef updateMaxDelta(t: Int, j: Int): Unit = {.\n   val idxDelta = Range(0, nStates)\n        .map(i => (i, delta(t-1, i)*lambda.A(i, j)))\n        .maxBy(_._2)   //44\n   psi(t)(j) = idxDelta._1\n   delta += (t, j, idxDelta._2)  //45\n}\n```", "```py\nclass HMM[@specialized(Double) T <: AnyVal](\n    config: HMMConfig,\n    xt: XVSeries[T], \n    form: HMMForm)\n    (implicit quantize: Array[T] => Int, f: T => Double) \n  extends ITransform[Array[T]](xt) with Monitor[Double] {//46\n\n  type V = HMMPrediction  //47\n  val obsSeq: Vector[Int] = xt.map(quantize(_)) //48\n\n  val model: Option[HMMModel] = train  //49\n  override def |> : PartialFunction[U, Try[V]] //50\n}\n```", "```py\ndef train: Option[HMMModel] = Try {\n  BaumWelchEM(config, obsSeq).lambda }.toOption\n```", "```py\noverride def |> : PartialFunction[U, Try[V]] = {\n  case x: Array[T] if(isModel && x.length > 1) => \n  form match {\n    case _: EVALUATION => \n      evaluation(model.get, Vector[Int](quantize(x))\n    case _: DECODING => \n       decoding(model.get, Vector[Int](quantize(x))\n   }\n}\n```", "```py\ndef evaluation(model: HMMModel, \n    obsSeq: Vector[Int]): Try[HMMPrediction] = Try {\n  HMMPrediction(-Alpha(model,obsSeq).logProb, obsSeq.toArray) \n}\n```", "```py\ndef evaluate[T <: AnyVal]( model: HMMModel, \n    xt: XVSeries[T])(implicit quantize: Array[T] => Int, \n      f: T => Double): Option[HMMPrediction] =  \n  evaluation(model, xt.map(quantize(_))).toOption\n```", "```py\ndef decoding( model: HMMModel, obsSeq: Vector[Int]): \n     Try[HMMPrediction] = Try { \n  ViterbiPath(model, obsSeq).path\n}\n```", "```py\ndef decode[T <: AnyVal](model: HMMModel, \n    xt: XVSeries[T])(implicit quantize: Array[T] => Int,\n    f: T => Double): Option[HMMPrediction] =\n  decoding(model, xt.map(quantize(_))).toOption\n```", "```py\nval OBS_PATH = \"resources/data/chap7/obsprob.csv\"\nval NUM_SYMBOLS = 6\nval NUM_STATES = 5\nval EPS = 1e-4\nval MAX_ITERS = 150\nval observations = Vector[Double](\n   0.01, 0.72, 0.78, 0.56, 0.61, 0.56, 0.45, …  )\n\nval quantize = (x: DblArray) => \n      (x.head* (NUM_STATES+1)).floor.toInt  //51\nval xt = observations.map(Array[Double](_))\n\nval config = HMMConfig(xt.size, NUM_STATES, NUM_SYMBOLS, \n    MAX_ITERS, EPS)\nval hmm = HMM[Array[Int]](config,  xt) //52\nshow(s\"Training):\\n${hmm.model.toString}\")\n```", "```py\nval A0 = Array[Array[Double]](\n  Array[Double](0.21, 0.13, 0.25, 0.06, 0.11, 0.24),\n  Array[Double](0.31, 0.17, 0.18, 0.04, 0.19, 0.11),\n  …. \n) \nval B0 =  Array[Array[Double]](\n  Array[Double](0.61, 0.39),\n  Array[Double](0.54, 0.46),\n  …  \n)\nval PI0 = Array[Double](\n  0.26, 0.04, 0.11, 0.26, 0.19, 0.14)\n\nval xt = Vector[Double](\n  0.0, 1.0, 21.0, 1.0, 30.0, 0.0, 1.0, 0.0, …\n).map(Array[Double](_))\nval max = data.max\nval min = data.min\nimplicit val quantize = (x: DblArray) => \n  ((x.head/(max - min) + min)*(B0.head.length-1)).toInt   //55\nval lambda = HMMModel(\n  DMatrix(A0), DMatrix(B0), PI0, xt.length) //53\nevaluation(lambda, xt).map( _.toString).map(show(_)) //54\n\n```", "```py\nCitigroup // Y(0) = 1 \nupgraded // Y(1) \nMacys // Y(2) \nfrom // Y(3) \nBuy // Y(4) \nto // Y(5)\nStrong Buy //Y(6) = 7\n```", "```py\nclass Crf(nLabels: Int, config: CrfConfig, \n    delims: CrfSeqDelimiter, xt: Vector[String])//1\n  extends ITransform[String](xt) with Monitor[Double]{//2\n\n  type V = Double  //3\n  val tagsGen = new CrfTagger(nLabels) //4\n  val crf = CrfAdapter(nLabels, tagsGen, config.params) //5\n  val model: Option[CrfModel] = train //6\n  weights: Option[DblArray] = model.map( _.weights)\n\n  override def |> : PartialFunction[String, Try[V]] //7\n}\n```", "```py\nclass CrfConfig(w0: Double, maxIters: Int, lambda: Double, \n     eps: Double) extends Config { //8\n  val params = s\"\"\"initValue $w0 maxIters $maxIters\n     | lambda $lambda scale true eps $eps\"\"\".stripMargin\n}\n```", "```py\ncase class CrfSeqDelimiter(obsDelim: String, \n    labelsDelim: String, seqDelim: String)\n```", "```py\nval DEFAULT_SEQ_DELIMITER = \n   new CrfSeqDelimiter(\",\\t/ -():.;'?#`&_\", \"//\", \"\\n\") \n```", "```py\nobject CrfAdapter {\n  import iitb.CRF.CRF\n    def apply(nLabels: Int, tagger: CrfTagger, \n     config: String): CRF = new CRF(nLabels, tagger, config)\n  …\n}\n```", "```py\nclass CrfTagger(nLabels: Int) extends FeatureGenerator\nclass CrfDataSeq(nLabels: Int, tags: Vector[String], delim: String) extends DataSequence\nclass CrfSeqIter(nLabels: Int, input: String, delim: CrfSeqDelimiter) extends DataIter\n```", "```py\ndef train: Option[CrfModel] = Try {\n  val weights = if(xt.size == 1)  //9\n    computeWeights(xt.head) \n  else {\n    val weightsSeries = xt.map( computeWeights(_) )\n    statistics(weightsSeries).map(_.mean).toArray //10\n  }\n  new CrfModel(weights) //11\n}._toOption(\"Crf training failed\", logger)\n```", "```py\nval parXt = xt.par\nval pool = new ForkJoinPool(nTasks)\nv.tasksupport = new ForkJoinTaskSupport(pool)\nparXt.map(computeWeights(_) )\n```", "```py\ndef computeWeights(tagsFile: String): DblArray = {\n  val seqIter = CrfSeqIter(nLabels, tagsFile, delims)\n  tagsGen.train(seqIter)  //12\n  crf.train(seqIter)  //13\n}\n```", "```py\noverride def |> : PartialFunction[String, Try[V]] = {\n   case obs: String if( !obs.isEmpty && isModel) => {\n     val dataSeq = new CrfDataSeq(nLabels,obs,delims.obsDelim)\n     Try (crf.apply(dataSeq)) //14\n   }\n}\n```", "```py\nval LAMBDA = 0.5\nval NLABELS = 9\nval MAX_ITERS = 100\nval W0 = 0.7\nval EPS = 1e-3\nval PATH = \"resources/data/chap7/rating\"\nval OBS_DELIM = \",\\t/ -():.;'?#`&_\"\n\nval config = CrfConfig(W0 , MAX_ITERS, LAMBDA, EPS) //15\nval delims = CrfSeqDelimiter(DELIM,\"//\",\"\\n\") //16\nval crf = Crf(NLABELS, config, delims, PATH) //17\ncrf.weights.map( display(_) )\n```", "```py\ntype F1 = Double => Double\ntype F2 = (Double, Double) => Double\n\ncase class KF[G](val g: G, h: F2) {\n  def metric(v: DblVector, w: DblVector)\n      (implicit gf: G => F1): Double =  //1\n    g(v.zip(w).map{ case(_v, _w) => h(_v, _w)}.sum) //2\n}\n```", "```py\nval kfMonad = new _Monad[KF] {\n  override def map[G,H](kf: KF[G])(f: G =>H): KF[H] = \n     KF[H](f(kf.g), kf.h) //3\n  override def flatMap[G,H](kf: KF[G])(f: G =>KF[H]): KF[H] =\n     KF[H](f(kf.g).g, kf.h)\n}\n```", "```py\nimplicit class kF2Monad[G](kf: KF[G]) {  //4\n  def map[H](f: G =>H): KF[H] = kfMonad.map(kf)(f)\n  def flatMap[H](f: G =>KF[H]): KF[H] =kfMonad.flatMap(kf)(f)\n}\n```", "```py\nclass RBF(s2: Double) extends KF[F1](\n    (x: Double) => Math.exp(-0.5*x*x/s2), \n    (x: Double, y: Double) => x -y)\nclass Polynomial(d: Int) extends KF[F1](\n    (x: Double) => Math.pow(1.0+x, d), \n    (x: Double, y: Double) => x*y)\n```", "```py\nval v = Vector[Double](0.5, 0.2, 0.3)\nval w = Vector[Double](0.1, 0.7, 0.2)\nval composed = for {\n  kf1 <- new RBF(0.6)  //5\n  kf2 <- new Polynomial(3)  //6\n} yield kf2\ncomposed.metric(v, w) //7\n\n```", "```py\ntrait SVMConfigItem { def update(param: svm_parameter): Unit }\n```", "```py\nsealed trait SVMFormulation extends SVMConfigItem {   \n  def update(param: svm_parameter): Unit \n}\n```", "```py\nclass CSVCFormulation (c: Double) extends SVMFormulation {   \n   override def update(param: svm_parameter): Unit = {      \n     param.svm_type = svm_parameter.C_SVC\n     param.C = c\n  }\n}\n```", "```py\nsealed trait SVMKernel extends SVMConfigItem {\n  override def update(param: svm_parameter): Unit \n}\n```", "```py\nclass RbfKernel(gamma: Double) extends SVMKernel {\n  override def update(param: svm_parameter): Unit = {\n    param.kernel_type = svm_parameter.RBF\n    param.gamma = gamma\n}\n```", "```py\n    case svm_parameter.LAPLACE:\n       double sum = 0.0;\n       for(int k = 0; k < x[i].length; k++) { \n         final double diff = x[i][k].value - x[j][k].value; \n         sum += diff*diff;\n        }    \n        return Math.exp(-gamma*Math.sqrt(sum));\n    ```", "```py\nclass SVMExecution(cacheSize: Int, eps: Double, nFolds: Int) \n     extends SVMConfigItem {\n  override def update(param: svm_parameter): Unit = { \n    param.cache_size = cacheSize //1\n    param.eps = eps //2\n  }\n}\n```", "```py\nclass SVMConfig(formula: SVMFormulation, kernel: SVMKernel,\n     exec: SVMExecution) {\n  val param = new svm_parameter\n  formula.update(param) //3\n  kernel.update(param)  //4\n  exec.update(param)  //5\n}\n```", "```py\nobject SVMAdapter {\n  type SVMNodes = Array[Array[svm_node]]\n  class SVMProblem(numObs: Int, expected: DblArray) //6\n\n  def createSVMNode(dim: Int, x: DblArray): Array[svm_node] //7\n  def predictSVM(model: SVMModel, x: DblArray): Double //8\n  def crossValidateSVM(problem: SVMProblem, //9\n     param: svm_parameter, nFolds: Int, expected: DblArray) \n  def trainSVM(problem: SVMProblem,  //10\n     param: svm_parameter): svm_model \n}\n```", "```py\npublic class svm_node implements java.io.Serializable {   \n  public int index;    \n  public double value;\n}\n```", "```py\ncase class SVMModel(val svmmodel: svm_model, \n     val accuracy: Double) extends Model {\n  lazy val residuals: DblArray = svmmodel.sv_coef(0)\n}\n```", "```py\nclass SVM[T <% Double](config: SVMConfig, xt: XVSeries[T], \n   expected: DblVector) extends ITransform[Array[T]](xt) {//11\n\n  type V = Double   //12\n  val normEPS = config.eps*1e-7  //13\n  val model: Option[SVMModel] = train  //14\n\n  def accuracy: Option[Double] = model.map( _.accuracy) //15\n  def mse: Option[Double]  //16\n  def margin: Option[Double]  //17\n}\n```", "```py\ndef train: Option[SVMModel] = Try {\n  val problem = new SVMProblem(xt.size, expected.toArray) //18\n  val dim = dimension(xt)\n\n  xt.zipWithIndex.foreach{ case (_x, n) =>  //19\n      problem.update(n, createSVMNode(dim, _x))\n  }\n  new SVMModel(trainSVM(problem, config.param), accuracy(problem))   //20\n}._toOption(\"SVM training failed\", logger)\n```", "```py\nclass SVMProblem(numObs: Int, expected: DblArray) {\n  val problem = new svm_problem  //21\n  problem.l = numObs\n  problem.y = expected \n  problem.x = new SVMNodes(numObs)\n\n  def update(n: Int, node: Array[svm_node]): Unit = \n    problem.x(n) = node  //22\n}\n```", "```py\ndef createSVMNode(dim: Int, x: DblArray): Array[svm_node] = {\n   val newNode = new Array[svm_node](dim)\n   x.zipWithIndex.foreach{ case (y, j) =>  {\n      val node = new svm_node\n      node.index= j  //23\n      node.value = y  //24\n      newNode(j) = node \n   }}\n   newNode\n```", "```py\ndef trainSVM(problem: SVMProblem, \n     param: svm_parameter): svm_model =\n   svm.svm_train(problem.problem, param) //26\n\n```", "```py\ndef accuracy(problem: SVMProblem): Double = { \n  if( config.isCrossValidation ) {\n    val target = new Array[Double](expected.size)\n    crossValidateSVM(problem, config.param,  //27\n        config.nFolds, target)\n\n    target.zip(expected)\n       .filter{case(x, y) =>Math.abs(x- y) < config.eps}  //28\n       .size.toDouble/expected.size\n  }\n  else 0.0\n}\n```", "```py\ndef crossValidateSVM(problem: SVMProblem, param: svm_parameter, \n    nFolds: Int, expected: DblArray) {\n  svm.svm_cross_validation(problem.problem, param, \n    nFolds, expected)\n}\n```", "```py\noverride def |> : PartialFunction[Array[T], Try[V]] =  {\n   case x: Array[T] if(x.size == dimension(xt) && isModel) =>\n      Try( predictSVM(model.get.svmmodel, x) )  //29\n}\n```", "```py\ndef margin: Option[Double] = \n  if(isModel) {\n    val wNorm = model.get.residuals./:(0.0)((s,r) => s + r*r)\n    if(wNorm < normEPS) None else Some(2.0/Math.sqrt(wNorm))\n  }\n  else None\n```", "```py\nval GAMMA = 0.8\nval CACHE_SIZE = 1<<8\nval NFOLDS = 1\nval EPS = 1e-5\n\ndef evalMargin(features: Vector[DblArray], \n    expected: DblVector, c: Double): Int = {\n  val execEnv = SVMExecution(CACHE_SIZE, EPS, NFOLDS)\n  val config = SVMConfig(new CSVCFormulation(c), \n     new RbfKernel(GAMMA), execEnv)\n  val svc = SVM[Double](config, features, expected)\n  svc.margin.map(_.toString)     //30\n}\n```", "```py\ndef generate: (Vector[DblArray], DblArray) = {\n  val z  = Vector.tabulate(N)(i => {\n    val ri = i*(1.0 + 0.2*Random.nextDouble)\n    Array[Double](i, ri)  //31\n  }) ++\n  Vector.tabulate(N)(i =>Array[Double](i,i*Random.nextDouble))\n  (z, Array.fill(N)(1) ++ Array.fill(N)(-1))  //32\n}\n```", "```py\ngenerate.map(y => \n  (0.1 until 5.0 by 0.1)\n    .flatMap(evalMargin(y._1, y._2, _)).mkString(\"\\n\") \n)\n```", "```py\nclass A { val x = 5;  final val y = 8 } \nclass B extends A { \n  override val x = 9 // OK    \n  override val y = 10 // Error \n}\n```", "```py\nval C = 1.0\ndef evalKernel(xt: Vector[DblArray],  test: Vector[DblArray], \n     labels: DblVector, kF: SVMKernel): Double = { //33\n\n  val config = SVMConfig(new CSVCFormulation(C), kF) //34\n  val svc = SVM[Double](config, xt, labels)\n  val pfnSvc = svc |>  //35\n  test.zip(labels).count{case(x, y) =>pfnSvc(x).get == y}\n    .toDouble/test.size  //36\n}\n```", "```py\ndef genData(variance: Double, mean: Double): Vector[DblArray] = {\n  val rGen = new Random(System.currentTimeMillis)\n  Vector.tabulate(N)( _ => { \n    rGen.setSeed(rGen.nextLong)\n    Array[Double](rGen.nextDouble, rGen.nextDouble)\n      .map(variance*_ - mean)  //37\n  })\n}\n```", "```py\nval trainSet = genData(a, b) ++ genData(a, 1-b)\nval testSet = genData(a, b) ++ genData(a, 1-b)\n```", "```py\nval GAMMA = 0.8; val COEF0 = 0.5; val DEGREE = 2 //38\nval N = 100\n\ndef compareKernel(a: Double, b: Double) {\n  val labels = Vector.fill(N)(0.0) ++ Vector.fill(N)(1.0)\n  evalKernel(trainSet, testSet,labels,new RbfKernel(GAMMA)) \n  evalKernel(trainSet, testSet, labels, \n      new SigmoidKernel(GAMMA)) \n  evalKernel(trainSet, testSet, labels, LinearKernel) \n  evalKernel(trainSet, testSet, labels, \n      new PolynomialKernel(GAMMA, COEF0, DEGREE))\n}\n```", "```py\nval path = \"resources/data/chap8/dividends2.csv\"\nval C = 1.0\nval GAMMA = 0.5\nval EPS = 1e-2\nval NFOLDS = 2\n\nval extractor = relPriceChange :: debtToEquity :: \n    dividendCoverage :: cashPerShareToPrice :: epsTrend :: \n    shortInterest :: dividendTrend :: \n    List[Array[String] =>Double]()  //39\n\nval pfnSrc = DataSource(path, true, false,1) |> //40\nval config = SVMConfig(new CSVCFormulation(C), \n     new RbfKernel(GAMMA), SVMExecution(EPS, NFOLDS))\n\nfor {\n  input <- pfnSrc(extractor) //41\n  obs <- getObservations(input)  //42\n  svc <- SVM[Double](config, obs, input.last.toVector)\n} yield {\n  show(s\"${svc.toString}\\naccuracy ${svc.accuracy.get}\")\n}\n```", "```py\ndef getObservations(input: Vector[DblArray]):\n     Try[Vector[DblArray]] = Try {\n  transpose( input.dropRight(1).map(_.toArray) ).toVector\n}\n```", "```py\nval extractor =  … :: dividendTrend :: …\n```", "```py\nclass OneSVCFormulation(nu: Double) extends SVMFormulation {\n  override def update(param: svm_parameter): Unit = {\n    param.svm_type = svm_parameter.ONE_CLASS\n    param.nu = nu\n  }\n}\n```", "```py\nval NU = 0.2\nval GAMMA = 0.5\nval EPS = 1e-3\nval NFOLDS = 2\n\nval extractor = relPriceChange :: debtToEquity ::\n   dividendCoverage :: cashPerShareToPrice :: epsTrend ::\n   dividendTrend :: List[Array[String] =>Double]()\n\nval filter = (x: Double) => if(x == 0) -1.0 else 1.0  //43\nval pfnSrc = DataSource(path, true, false, 1) |>\nval config = SVMConfig(new OneSVCFormulation(NU),  //44\n    new RbfKernel(GAMMA), SVMExecution(EPS, NFOLDS))\n\nfor {\n  input <- pfnSrc(extractor)\n  obs <- getObservations(input)\n  svc <- SVM[Double](config, obs, \n             input.last.map(filter(_)).toVector)\n} yield {\n  show(s\"${svc.toString}\\naccuracy ${svc.accuracy.get}\")'\n}\n```", "```py\nval path = \"resources/data/chap8/SPY.csv\"\nval C = 12\nval GAMMA = 0.3\nval EPSILON = 2.5\n\nval config = SVMConfig(new SVRFormulation(C, EPSILON), \n    new RbfKernel(GAMMA)) //45\nfor {\n  price <-  DataSource(path, false, true, 1) get close\n  (xt, y) <- getLabeledData(price.size)  //46\n  linRg <- SingleLinearRegression[Double](price, y) //47\n  svr <- SVM[Double](config, xt, price)\n} yield {\n  collect(svr, linRg, price)\n}\n```", "```py\ntype LabeledData = (Vector[DblArray], DblVector)\ndef getLabeledData(numObs: Int): Try[LabeledData ] = Try {\n    val y = Vector.tabulate(numObs)(_.toDouble)\n    val xt = Vector.tabulate(numObs)(Array[Double](_))\n    (xt, y)\n}\n```", "```py\ndef collect(svr: SVM[Double], \n   linr: SingleLinearRegression[Double], price: DblVector){\n\n  val pfSvr = svr |>\n  val pfLinr = linr |>\n  for {\n    if( pfSvr.isDefinedAt(n.toDouble))\n    x <- pfSvr(n.toDouble) \n    if( pfLin.isDefinedAt(n))\n    y <- pfLinr(n)\n  } yield  {  ... }\n}\n```", "```py\ncase class MLPConfig(\n    val alpha: Double,  //1\n    val eta: Double, \n    val numEpochs: Int, \n    val eps: Double, \n    val activation: Double => Double) extends Config {  //1\n}\n```", "```py\nclass MLPNetwork(config: MLPConfig, \n     topology: Array[Int], \n     model: Option[MLPModel] = None)\n     (implicit mode: MLPMode){ //2\n\n  val layers = topology.zipWithIndex.map { case(t, n) => \n    if(topology.size != n+1) \n       MLPLayer(n, t+1, config.activation) \n   else MLPOutLayer(n, t) \n  }  //3\n  val connections = zipWithShift1(layers,1).map{case(src,dst) => \n     new MLPConnection(config, src, dst,  model)} //4\n\n  def trainEpoch(x: DblArray, y: DblArray): Double //5\n  def getModel: MLPModel  //6\n  def predict(x: DblArray): DblArray  //7\n}\n```", "```py\nclass MLPLayer(val id: Int, val numNodes: Int, \n    val activation: Double => Double)  //8\n    (implicit mode: MLPMode){  //9\n\n  val output = Array.fill(numNodes)(1.0)  //10\n\n  def setOutput(xt: DblArray): Unit =\n      xt.copyToArray(output, 1) //11\n  def activate(x: Double): Double = activation(x) .//12\n  def delta(loss: DblArray, srcOut: DblArray, \n      synapses: MLPConnSynapses): Delta //13\n  def setInput(_x: DblArray): Unit  //14\n}\n```", "```py\ndef setInput(x: DblVector): Unit = \n  x.copyToArray(output, output.length -x.length)\n```", "```py\nclass MLPOutLayer(id: Int, numNodes: Int) \n    (implicit mode: MLP.MLPMode)  //15\n  extends MLPLayer(id, numNodes, (x: Double) => x) {\n\n  override def numNonBias: Int = numNodes\n  override def setOutput(xt: DblArray): Unit = \n    obj(xt).copyToArray(output)\n  override def delta(loss: DblArray, srcOut: DblArray, \n     synapses: MLPConnSynapses): Delta \n …\n}\n```", "```py\ntype MLPSynapse = (Double, Double)\n```", "```py\ntype MLPConnSynapses = Array[Array[MLPSynapse]]\n\nclass MLPConnection(config: MLPConfig, \n    src: MLPLayer, \n    dst: MLPLayer,\n    model: Option[MLPModel]) //16\n    (implicit mode: MLP.MLPMode) {\n\n  var synapses: MLPConnSynapses  //17\n  def connectionForwardPropagation: Unit //18\n  def connectionBackpropagation(delta: Delta): Delta  //19\n    …\n}\n```", "```py\nvar synapses: MLPConnSynapses = if(model == None) {\n  val max = BETA/Math.sqrt(src.output.length+1.0) //20\n  Array.fill(dst.numNonBias)(\n    Array.fill(src.numNodes)((Random.nextDouble*max,0.00))\n  )\n} else model.get.synapses(src.id)  //21\n\n```", "```py\ncase class MLPModel(\n   val synapses: Vector[MLPConnSynapses]) extends Model\n```", "```py\ndef trainEpoch(x: DblArray, y: DblArray): Double = {\n  layers.head.setInput(x)  //22\n  connections.foreach( _.connectionForwardPropagation) //23\n\n  val err = mode.error(y, layers.last.output) \n  val bckIterator = connections.reverseIterator \n\n  var delta = Delta(zipToArray(y, layers.last.output)(diff)) //24\n  bckIterator.foreach( iter => \n     delta = iter.connectionBackpropagation(delta))  //25\n  err  //26\n}\n```", "```py\ndef connectionForwardPropagation: Unit = {\n  val _output = synapses.map(x => {\n    val dot = inner(src.output, x.map(_._1) ) //27\n    dst.activate(dot)  //28\n  })\n  dst.setOutput(_output) //29\n}\n```", "```py\ndef crossEntropy(x: Double, y: Double): Double = \n  -(x*Math.log(y) + (1.0 - x)*Math.log(1.0 - y))\n```", "```py\ndef crossEntropy(xt: DblArray, yt: DblArray): Double = \n  yt.zip(xt).aggregate(0.0)({ case (s, (y, x)) => \n    s - y*Math.log(x)}, _ + _)\n```", "```py\ntrait MLPMode { \n  def apply(output: DblArray): DblArray   //30\n  def error(labels: DblArray, output: DblArray): Double = \n    mse(labels, output)  //31\n}\n```", "```py\nclass MLPBinClassifier extends MLPMode {   \n  override def apply(output: DblArray): DblArray = \n    output.map(sigmoid(_))  //32\n  override def error(labels: DblArray,  \n      output: DblArray): Double = \n    crossEntropy(labels.head, output.head)  //33\n}\n```", "```py\nclass MLPRegression extends MLPMode  {\n  override def apply(output: DblArray): DblArray = output\n}\n```", "```py\nclass MLPMultiClassifier extends MLPMode {\n  override def apply(output: DblArray):DblArray = softmax(output)\n}\n```", "```py\ndef softmax(y: DblArray): DblArray = {\n  val softmaxValues = new DblArray(y.size)\n  val expY = y.map( Math.exp(_))  //34\n  val expYSum = expY.sum  //35\n\n  expY.map( _ /expYSum).copyToArray(softmaxValues, 1) //36\n  softmaxValues\n}\n```", "```py\ncase class Delta(val loss: DblArray, \n  val delta: DblMatrix = Array.empty[DblArray],\n  val synapses: MLPConnSynapses = Array.empty[Array[MLPSynapse]] )\n```", "```py\nval diff = (x: Double, y: Double) => x - y\nDelta(zipToArray(y, layers.last.output)(diff))\n```", "```py\ndef delta(error: DblArray, srcOut: DblArray, \n     synapses: MLPConnSynapses): Delta = {\n\n  val deltaMatrix = new ArrayBuffer[DblArray] //34\n  val deltaValues = error./:(deltaMatrix)( (m, l) => {\n    m.append( srcOut.map( _*l) )\n    m\n  })   //35\n  new Delta(error, deltaValues.toArray, synapses)  //36\n}\n```", "```py\ndef delta(oldDelta: DblArray, srcOut: DblArray, \n     synapses: MLPConnSynapses): Delta = {\n\n  val deltaMatrix = new ArrayBuffer[(Double, DblArray)]\n  val weights = synapses.map(_.map(_._1))\n       .transpose.drop(1) //37\n\n  val deltaValues = output.drop(1)\n   .zipWithIndex./:(deltaMatrix){  // 38\n     case (m, (zh, n)) => {\n       val newDelta = inner(oldDelta, weights(n))*zh*(1.0 - zh)\n       m.append((newDelta, srcOut.map( _ * newdelta) )\n       m\n     } \n  }.unzip\n  new Delta(deltaValues._1.toArray, deltaValues._2.toArray)//39\n}\n```", "```py\ndef connectionBackpropagation(delta: Delta): Delta = {  //40\n  val inSynapses =  //41\n    if( delta.synapses.length > 0) delta.synapses \n    else synapses \n\n  val delta = dst.delta(delta.loss, src.output,inSynapses) //42\n  synapse = synapses.zipWithIndex.map{ //43\n    case (synapsesj, j) => synapsesj.zipWithIndex.map{\n      case ((w, dw), i) => { \n        val ndw = config.eta*connectionDelta.delta(j)(i)\n        (w + ndw - config.alpha*dw, ndw)\n      } \n    }\n  }\n  new Delta(connectionDelta.loss, \n       connectionDelta.delta, synapses)\n}\n```", "```py\nclass MLP[T <: AnyVal](config: MLPConfig, \n    hidden: Array[Int] = Array.empty[Int],\n    xt: XVSeries[T], \n    expected: XVSeries[T])\n    (implicit mode: MLPMode, f: T => Double) \n  extends ITransform[Array[T]](xt) with Monitor[Double] {  //44\n\n  type V = DblArray  //45\n\n  lazy val topology = if(hidden.length ==0) \n    Array[Int](xt.head.size, expected.head.size) \n  else  Array[Int](xt.head.size) ++ hidden ++ \n         Array[Int](expected.head.size)  //46\n\n  val model: Option[MLPModel] = train\n  def train: Option[MLPModel]   //47\n  override def |> : PartialFunction[Array[T], Try[V]] \n}\n```", "```py\ndef train: Option[MLPModel] = {\n  val network = new MLPNetwork(config, topology) //48\n  val zi =  xt.toVector.zip(expected.view)   // 49\n\n  Range(0, config.numEpochs).find( n => {  //50\n    val cumulErr = fisherYates(xt.size)\n       .map(zi(_))\n       .map{ case(x, e) => network.trainEpoch(x, e)}\n       .sum/st.size   //51\n     cumulErr  < config.eps  //52\n  }).map(_ => network.getModel)\n}\n```", "```py\ndef fisherYates(n: Int): IndexedSeq[Int] = {\n\n   def fisherYates(seq: Seq[Int]): IndexedSeq[Int] = {\n     Random.setSeed(System.currentTimeMillis)\n    (0 until seq.size).map(i => {\n       var randomIdx: Int = i + Random.nextInt(seq.size-i) //53\n       seq(randomIdx) ^= seq(i)    //54\n       seq(i) = seq(randomIdx) ^ seq(i) \n       seq(randomIdx) ^= (seq(i))\n       seq(i)\n    })\n  }\n\n  if( n <= 0)  Array.empty[Int]\n  else \n    fisherYates(ArrayBuffer.tabulate(n)(n => n)) //55\n}\n```", "```py\noverride def |> : PartialFunction[Array[T],Try[V]] ={\n  case x: Array[T] if(isModel && x.size == dimension(xt)) => \n   Try(MLPNetwork(config, topology, model).predict(x)) //56\n}\n```", "```py\ndef predict(x: DblArray): DblArray = {\n   layers.head.set(x)\n   connections.foreach( _.connectionForwardPropagation)\n   layers.last.output\n}\n```", "```py\ndef fit(threshold: Double): Option[Double] = model.map(m => \n  xt.map( MLPNetwork(config, topology, Some(m)).predict(_) )\n    .zip(expected)\n    .count{case (y, e) =>mse(y, e.map(_.toDouble))< threshold }\n    /xt.size.toDouble\n)\n```", "```py\ndef f1(x: Double): DblArray = Array[Double](  //57\n  0.1+ 0.5*Random.nextDouble, 0.6*Random.nextDouble) \ndef f2(x: Double): DblArray =  Array[Double](  //58\n  0.6 + 0.4*Random.nextDouble, 1.0 - 0.5*Random.nextDouble)\n\nval HALF_TEST_SIZE = (TEST_SIZE>>1)\nval xt = Vector.tabulate(TEST_SIZE)(n =>   //59\n  if( n <HALF_TEST_SIZE) f1(n) else f2(n -HALF_TEST_SIZE))\nval yt = Vector.tabulate(TEST_SIZE)(n => \n  if( n < HALF_TEST_SIZE) Array[Double](0.0) \n  else Array[Double](1.0) )  //60\n\n```", "```py\nval ALPHA = 0.3\nval ETA = 0.03\nval HIDDEN = Array[Int](3)\nval NUM_EPOCHS = 200\nval TEST_SIZE = 12000\nval EPS = 1e-7\n\ndef testEta(eta: Double, \n    xt: XVSeries[Double], \n    yt: XVSeries[Double]): \n    Option[(ArrayBuffer[Double], String)] = {\n\n  implicit val mode = new MLPBinClassifier //61\n  val config = MLPConfig(ALPHA, eta, NUM_EPOCHS, EPS)\n  MLP[Double](config, HIDDEN, xt, yt)  \n    .counters(\"err\").map( (_, s\"eta=$eta\")) //62\n}\n```", "```py\nval etaValues = List[Double](0.01, 0.02, 0.03, 0.1)\nval data = etaValues.flatMap( testEta(_, xt, yt))\n    .map{ case(x, s) => (x.toVector, s) }\n\nval legend = new Legend(\"Err\", \n   \"MLP [2-3-1] training - learning rate\", \"Epochs\", \"Error\")\nLinePlot.display(data, legend, new LightPlotTheme)\n```", "```py\nval path = \"resources/data/chap9/\"\nval ALPHA = 0.8; \nval ETA = 0.01\nval NUM_EPOCHS = 250\nval EPS = 1e-3\nval THRESHOLD = 0.12\nval hiddens = Array[Int](7, 7) //59\n\n```", "```py\nval symbols = Array[String](\n \"FXE\",\"FXA\",\"SPY\",\"GLD\",\"FXB\",\"FXF\",\"FXC\",\"FXY\",\"CYB\"\n)\nval STUDIES = List[Array[String]](   //60\n  Array[String](\"FXY\",\"FXC\",\"GLD\",\"FXA\"),\n  Array[String](\"FXE\",\"FXF\",\"FXB\",\"CYB\"),\n  Array[String](\"FXE\",\"FXC\",\"GLD\",\"FXA\",\"FXY\",\"FXB\"),\n  Array[String](\"FXC\",\"FXY\",\"FXA\"),\n  Array[String](\"CYB\",\"GLD\",\"FXY\"),\n  symbols\n)\n```", "```py\nval prices = symbols.map(s => DataSource(s\"$path$s.csv\"))\n      .flatMap(_.get(close).toOption) //61\n\n```", "```py\nval study = Array[String](\"FXE\",\"FXF\",\"FXB\",\"CYB\")\n```", "```py\nval obs = symbols.flatMap(index.get(_))\n              .map( prices(_).toArray )  //62\nval xv = obs.drop(1).transpose  //63\nval expected = Array[DblArray](obs.head).transpose //64\n\n```", "```py\nimplicit val mode = new MLPBinClassifier  //65\nval classifier = MLP[Double](config, hiddenLayers, xv, expected)\nclassifier.fit(THRESHOLD)\n```", "```py\nIF {Gold price rises to [1316$/ounce]} AND \n   {US$/Yen rate is [104]}).\nTHEN {S&P 500 index is [UP]}\n```", "```py\ntype Pool[T <: Gene] = mutable.ArrayBuffer[Chromosome[T]]\n\nclass Population[T <: Gene](\n    limit: Int, val chromosomes: Pool[T]) {    \n  def select(score: Chromosome[T]=>Unit, cutOff: Double) //1\n  def +- (xOver: Double) //2\n  def ^ (mu: Double) //3\n  …\n}\n```", "```py\nclass Chromosome[T <: Gene](val code: List[T]) {   \n  var cost: Double = Random.nextDouble //4\n  def +- (that: Chromosome[T], idx: GeneticIndices): \n        (Chromosome[T], Chromosome[T]) \n  def ^ (idx: GeneticIndices): Chromosome[T]\n   …\n}\n```", "```py\nclass Gene(val id: String, \n    val target: Double, \n    op: Operator)\n    (implicit quantize: Quantization, encoding: Encoding){//5\n  lazy val bits: BitSet = apply(target, op)\n\n  def apply(value: Double, op: Operator): BitSet //6\n  def unapply(bitSet: BitSet): (Double, Operator) //7\n   …\n  def +- (index: Int, that: Gene): Gene //8\n  def ^ (index: Int): Unit //9\n  …\n}\n```", "```py\ncase class Quantization(toInt: Double => Int,\n     toDouble: Int => Double) {\n   def this(R: Int) =  this((x: Double) => \n         (x*R).floor.toInt, (n: Int) => n/R) \n}\n```", "```py\nclass Encoding(nValueBits: Int, nOpBits: Int) {\n  val rValue = Range(0, nValueBits)  \n  val length = nValueBits + nOpBits\n  val rOp = Range(nValueBits, length)\n}\n```", "```py\ndef apply(value: Double, op: Operator): BitSet = {\n  val bitset = new BitSet(encoding.length)  \n  encoding.rOp foreach(i =>  //10\n    if(((op.id>>i) & 0x01)==0x01) bitset.set(i))\n  encoding.rValue foreach(i => //11\n    if( ((quant.toInt(value)>>i) & 0x01)==0x01) bitset.set(i))\n  bitset\n}\n```", "```py\ndef unapply(bits: BitSet): (Double, Operator) = \n  (quant.toDouble(convert(encoding.rValue, bits)), \n   op(convert(encoding.rOp, bits))) //12\n\n```", "```py\ntrait Operator {\n   def id: Int\n   def apply(id: Int): Operator\n}\n```", "```py\ndef select(score: Chromosome[T]=> Unit, cutOff: Double): Unit = {\n  val cumul = chromosomes.map( _.cost).sum/SCALING_FACTOR //13\n  chromosomes foreach( _ /= cumul) //14\n\n  val _chromosomes = chromosomes.sortWith(_.cost < _.cost)//15\n  val cutOffSize = (cutOff*_chromosomes.size).floor.toInt //16\n  val popSize = if(limit < cutOffSize) limit else cutOffSize\n\n  chromosomes.clear //17\n  chromosomes ++= _chromosomes.take(popSize) //18\n}\n```", "```py\nclass GAConfig(val xover: Double, \n     val mu: Double, \n     val maxCycles: Int, \n     val softLimit: Int => Double) extends Config {\n   val mutation = (cycle : Int) => softLimit(cycle)\n}\n```", "```py\ndef +- (xOver: Double): Unit = \n  if( size > 1) {\n    val mid = size>>1\n    val bottom = chromosomes.slice(mid, size) //19\n    val gIdx = geneticIndices(xOver)  //20\n\n    val offSprings = chromosomes.take(mid).zip(bottom)\n          .map{ case (t, b) => t +- (b, gIdx) }.unzip //21\n    chromosomes ++= offSprings._1 ++ offSprings._2 //22\n  }\n```", "```py\ncase class GeneticIndices(val chOpIdx: Int, //23\n     val geneOpIdx: Int)  //24\n\n```", "```py\ndef geneticIndices(prob: Double): GeneticIndices = {\n  var idx = (prob*chromosomeSize).floor.toInt //25\n  val chIdx = if(idx == chromosomeSize) chromosomeSize-1 \n       else idx //25\n\n  idx = (prob*geneSize).floor.toInt  \n  val gIdx = if(idx == geneSize) geneSize-1 else idx //26\n  GeneticIndices(chIdx, gIdx)\n}\n```", "```py\nval QUANT = 500\nclass Chromosome[T <: Gene](val code: List[T]) {  \n  var cost: Double = QUANT*(1.0 + Random.nextDouble) //27\n\n  def +- (that: Chromosome[T], indices: GeneticIndices): //28\n     (Chromosome[T], Chromosome[T]) \n  def ^ (indices: GeneticIndices): Chromosome[T] //29\n  def /= (normalizeFactor: Double): Unit =   //30\n      cost /= normalizeFactor\n  def decode(implicit d: Gene=>T): List[T] =  //31\n      code.map( d(_)) \n  …\n}\n```", "```py\ndef +- (that: Chromosome[T], indices: GeneticIndices): \n    (Chromosome[T], Chromosome[T]) = {\n  val xoverIdx = indices.chOpIdx //32\n  val xGenes =  spliceGene(indices, that.code(xoverIdx)) //33\n\n  val offSprng1 = code.slice(0, xoverIdx) ::: \n       xGenes._1 :: that.code.drop(xoverIdx+1) //34\n  val offSprng2 = that.code.slice(0, xoverIdx) ::: \n      xGenes._2 :: code.drop(xoverIdx+1)\n  (Chromosome[T](offSprng1), Chromosome[T](offSprng2)) //35\n}\n```", "```py\ndef spliceGene(indices: GeneticIndices, thatCode: T): (T,T) ={\n  ((this.code(indices.chOpIdx) +- (thatCode,indices)), \n   (thatCode +- (code(indices.chOpIdx),indices)) )\n}\n```", "```py\ndef +- (that: Gene, indices: GeneticIndices): Gene = {\n  val clonedBits = cloneBits(bits) //36\n\n  Range(indices.geneOpIdx, bits.size).foreach(n => \n    if( that.bits.get(n) ) clonedBits.set(n) \n    else clonedBits.clear(n) //37\n   )\n   val valOp = decode(clonedBits) //38\n   new Gene(id, valOp._1, valOp._2)\n}\n```", "```py\ndef ^ (prob: Double): Unit = \n  chromosomes ++= chromosomes.map(_ ^ geneticIndices(prob))\n```", "```py\ndef ^ (indices: GeneticIndices): Chromosome[T] = { //39 \n  val mutated = code(indices.chOpIdx) ^ indices \n  val xs = Range(0, code.size).map(i =>\n    if(i== indices.chOpIdx) mutated \n    else code(i)).toList //40\n  Chromosome[T](xs) //41\n}\n```", "```py\ndef ^ (indices: GeneticIndices): Gene = { \n  val idx = indices.geneOpIdx\n  val clonedBits = cloneBits(bits) //42\n\n  clonedBits.flip(idx)  //43\n  val valOp = decode(clonedBits)  //44\n  new Gene(id, valOp._1, valOp._2) //45\n}\n```", "```py\nclass Reproduction[T <: Gene](score: Chromosome[T] => Unit)\n```", "```py\ndef mate(population: Population[T], config: GAConfig, \n    cycle: Int): Boolean = (population.size: @switch) match {\n  case 0 | 1 | 2 => false   //46\n  case _ => {\n    rand.setSeed(rand.nextInt + System.currentTimeMillis)\n    population.select(score, config.softLimit(cycle)) //47\n    population +- rand.nextDouble*config.xover //48\n    population ^ rand.nextDouble*config.mu  //49\n    true\n  }\n}\n```", "```py\nclass GASolver[T <: Gene](config: GAConfig, \n    score: Chromosome[T] => Unit,\n    tracker: Option[Population[T] => Unit] = None) \n     extends ETransform[GAConfig](config) //50\n        with GAMonitor[T] { //51\n\n  type U = Population[T]  //52\n  type V = Population[T]  //53\n\n  val monitor: Option[Population[T] => Unit] = tracker\n  def |>(initialize: => Population[T]): Try[Population[T]] = \n      this.|> (initialize()) //54\n  override def |> : PartialFunction[U, Try[V]] //55\n}\n```", "```py\ntrait GAMonitor[T <: Gene] extends Monitor {\n  self: { \n    def |> :PartialFunction[Population[T],Try[Population[T]]] \n  } => //55\n    val monitor: Option[Population[T] => Unit] //56\n    var state: GAState = GA_NOT_RUNNING //57\n\n    def isReady: Boolean = state == GA_NOT_RUNNING\n    def start: Unit = state = GA_RUNNING\n    def isComplete(population: Population[T], \n        remainingCycles: Int): Boolean  = { … } //58\n}\n```", "```py\nsealed abstract class GAState(description: String)\ncase class GA_FAILED(description: String) \n   extends GAState(description)\nobject GA_RUNNING extends GAState(\"Running\")\n```", "```py\noverride def |> : PartialFunction[U, Try[V]] = {\n  case population: U if(population.size > 1 && isReady) => {\n    start //59\n    val reproduction = Reproduction[T](score)  //60\n\n    @tailrec\n    def reproduce(population: Population[T], \n          n:Int): Population[T] = { //61\n      if( !reproduction.mate(population, config, n) || \n         isComplete(population, config.maxCycles -n) )\n       population\n      else\n       reproduce(population, n+1)\n    }\n    reproduce(population, 0)\n    population.select(score, 1.0) //62\n    Try(population)\n  }\n}\n```", "```py\nclass SOperator(_id: Int) extends Operator {\n  override def id: Int = _id\n  override def apply(idx: Int): SOperator = new SOperator(idx) \n}\n```", "```py\nobject LESS_THAN extends SOperator(1) \nobject GREATER_THAN extends SOperator(2)\nobject EQUAL extends SOperator(3)\n```", "```py\nval operatorFuncMap = Map[Operator, (Double,Double) =>Double](\n  LESS_THAN -> ((x: Double, target: Double) => target - x),\n  … )\n```", "```py\nclass Signal(id: String, target: Double, op: Operator,\n   xt: DblVector, weights: Option[DblVector] = None)\n   (implicit quantize: Quantization, encoding: Encoding) \n  extends Gene(id, target, op) \n```", "```py\noverride def score: Double = \n  if(!operatorFuncMap.contains(op)) Double.MaxValue\n  else {\n    val f = operatorFuncMap.get(op).get\n    if( weights != None ) xt.zip(weights.get)\n        .map{case(x, w) => w*f(x,target)}.sum\n    else xt.map( f(_, target)).sum   \n  }\n```", "```py\nclass StrategyFactory(nSignals: Int)  //63\n    (implicit quantize: Quantization, encoding: Encoding){\n  val signals = new ListBuffer[Signal]   \n  lazy val strategies: Pool[Signal] //64\n  def += (id: String, target: Double, op: SOperator, \n       xt: DblVector, weights: DblVector)\n  …\n }\n```", "```py\nlazy val strategies: Pool[Signal] = {\n  implicit val ordered = Signal.orderedSignals //70\n\n  val xss = new Pool[Signal] //65\n  val treeSet = new TreeSet[Signal] ++= signals.toList //66\n  val subsetsIterator = treeSet.subsets(nSignals) //67\n\n  while( subsetsIterator.hasNext) {\n    val signalList = subsetsIterator.next.toList  //68\n    xss.append(Chromosome[Signal](signalList)) //69\n  } \n  xss\n}\n```", "```py\nImport YahooFinancials._\nval NUM_SIGNALS = 3\n\ndef createStrategies: Try[Pool[Signal]] = {\n  val src = DataSource(path, false, true, 1) //71\n  for {  //72\n    price <- src.get(adjClose)\n    dPrice <- delta(price, -1.0) \n    volume <- src.get(volume)\n    dVolume <- delta(volume, 1.0)\n    volatility <- src.get(volatility)\n    dVolatility <- delta(volatility, 1.0)\n    vPrice = src.get(vPrice)\n  } yield { //72\n    val factory = new StrategyFactory(NUM_SIGNALS) //73\n\n    val weights = dPrice  //74\n    factory += (\"dvolume\", 1.1, GREATER_THAN, dVolume, weights)\n    factory += (\"volatility\", 1.3, GREATER_THAN, \n       volatility.drop(1), weights)\n    factory += (\"vPrice\", 0.8, LESS_THAN, \n       vPrice.drop(1), weights)\n    factory += (\"dVolatility\", 0.9, GREATER_THAN, \n       dVolatility, weights)\n    factory.strategies\n   }\n}\n```", "```py\ndef delta(xt: DblVector, a: Double): Try[DblVector] = Try {\n  zipWithShift(xt, 1).map{case(x, y) => a*(y/x - 1.0)}\n}\n```", "```py\nval avWeights = dPrice.sum/dPrice.size\nval weights = Vector.fill(dPrice.size)(avWeights)\n```", "```py\nval XOVER = 0.8  //Probability(ratio) for cross-over\nval MU = 0.4  //Probability(ratio) for mutation\nval MAX_CYCLES = 400  //Max. number of optimization cycles \n\nval CUTOFF_SLOPE = -0.003  //Slope linear soft limit\nval CUTOFF_INTERCEPT = 1.003  //Intercept linear soft limit\nval softLimit = (n: Int) => CUTOFF_SLOPE*n +CUTOFF_INTERCEPT\n```", "```py\nimplicit val encoding = defaultEncoding //75\nval R = 1024  //Quantization ratio\nimplicit val digitize = new Quantization(R) //76\n\n```", "```py\nval scoring = (chr: Chromosome[Signal]) => {\n  val signals: List[Gene] = chr.code\n  chr.cost = signals.map(_.score).sum //77\n}\n```", "```py\ncreateStrategies.map(strategies => {\n  val limit = strategies.size <<3 //78\n  val initial = Population[Signal](limit, strategies) //79\n\n  val config = GAConfig(XOVER, MU, MAX_CYCLES,softLimit) //80\n  val solver = GASolver[Signal](config,scoring,Some(tracker)) \n                                                //81\n  (solver |> initial)\n    .map(_.fittest.map(_.symbolic).getOrElse(\"NA\")) match {\n      case Success(results) => show(results)\n      case Failure(e) => error(\"GAEval: \", e)\n    } //82\n})\n```", "```py\nclass QLAction(val from: Int, val to: Int)\n```", "```py\nclass QLState[T](val id: Int, \n  val actions: List[QLAction] = List.empty, \n  val instance: T])\n```", "```py\nclass QLSpace[T](states: Seq[QLState[T]], goals: Array[Int]) {\n  val statesMap = states.map(st =>(st.id, st)) //1\n  val goalStates = new HashSet[Int]() ++ goals //2\n\n  def maxQ(state: QLState[T], \n      policy: QLPolicy): Double   //3\n  def init(state0: Int)  //4\n  def nextStates(st: QLState[T]): Seq[QLState[T]]  //5\n   …\n}\n```", "```py\ndef init(state0: Int): QLState[T] = \n  if(state0 < 0) {\n    val seed = System.currentTimeMillis+Random.nextLong\n      states((new Random(seed)).nextInt(states.size-1))\n  }\n  else states(state0)\n```", "```py\ndef apply[T](goals: Array[Int], instances: Seq[T],\n    constraints: Option[Int =>List[Int]): QLSpace[T] ={ //6\n\n  val r = Range(n, instances.size)  \n\n  val states = instances.zipWithIndex.map{ case(x, n) => {\n    val validStates = constraints.map( _(n)).getOrElse(r)\n    val actions = validStates.view\n          .map(new QLAction(n, _)).filter(n != _.to) //7\n    QLState[T](n, actions, x)\n  }}\n  new QLSpace[T](states, goals) \n}\n```", "```py\nclass QLData(val reward: Double, val probability: Double) {\n  var value: Double = 0.0\n  def estimate: Double = value*probability\n}\n```", "```py\nclass QLInput(val from: Int, val to: Int, \n    val reward: Double, val probability: Double = 1.0)\n```", "```py\nclass QLPolicy(input: Seq[QLInput]) { \n  val numStates = Math.sqrt(input.size).toInt  //8\n\n  val qlData = input.map(qlIn => \nnew QLData(qlIn.reward, qlIn.prob))  //9\n\n  def setQ(from: Int, to: Int, value: Double): Unit =\n     qlData(from*numStates + to).value = value //10\n\n  def Q(from: Int, to: Int): Double = \n    qlData(from*numStates + to).value  //11\n  def EQ(from: Int, to: Int): Double = \n    qlData(from*numStates + to).estimate //12\n  def R(from: Int, to: Int): Double = \n    qlData(from*numStates + to).reward //13\n  def P(from: Int, to: Int): Double = \n    qlData(from*numStates + to).probability //14\n}\n```", "```py\nclass QLearning[T](conf: QLConfig, \n    qlSpace: QLSpace[T], qlPolicy: QLPolicy) //15\n    extends ETransform[QLConfig](conf) { //16\n\n  type U = QLState[T] //17\n  type V = QLState[T] //18\n\n  val model: Option[QLModel] = train //19\n  def train: Option[QLModel]\n  def nextState(iSt: QLIndexedState[T]): QLIndexedState[T]\n\n  override def |> : PartialFunction[U, Try[V]]\n  …\n}\n```", "```py\ncase class QLConfig(val alpha: Double, \n  val gamma: Double, \n  val episodeLength: Int, \n  val numEpisodes: Int, \n  val minCoverage: Double) extends Config\n```", "```py\ndef apply[T](config: QLConfig,  //20\n   goals: Array[Int], \n   input: => Seq[QLInput], \n   instances: Seq[T],\n   constraints: Option[Int =>List[Int]] =None): QLearning[T]={\n\n   new QLearning[T](config, \n     QLSpace[T](goals, instances, constraints),\n     new QLPolicy(input))\n}\n```", "```py\ndef apply[T](config: QLConfig, \n   goals: Array[Int],\n   xt: DblVector,  //21\n   reward: (Double, Double) => Double, //22\n   probability: (Double, Double) => Double, //23\n   instances: Seq[T].\n   constraints: Option[Int =>List[Int]] =None): QLearning[T] ={\n\n  val r = Range(0, xt.size)\n  val input = new ArrayBuffer[QLInput] //24\n  r.foreach(i => \n    r.foreach(j => \n     input.append(QLInput(i, j, reward(xt(i), xt(j)), \n          probability(xt(i), xt(j))))\n    )\n  )\n  new QLearning[T](config, \n    QLSpace[T](goals, instances, constraints), \n    new QLPolicy(input))\n}\n```", "```py\nclass QLModel(val bestPolicy: QLPolicy, \n    val coverage: Double) extends Model\n```", "```py\ndef train: Option[QLModel] = Try {\n  val completions = Range(0, config.numEpisodes)\n     .map(epoch => if(train(-1)) 1 else 0).sum  //25\n  completions.toDouble/config.numEpisodes //26\n}\n.map( coverage => {\n  if(coverage > config.minCoverage) \n    Some(new QLModel(qlPolicy, coverage))  //27\n  else None\n}).get\n```", "```py\ncase class QLIndexedState[T](val state: QLState[T], \n  val iter: Int)\n```", "```py\ndef train(state0: Int): Boolean = {\n  @tailrec\n  def search(iSt: QLIndexedState[T]): QLIndexedState[T]\n\n    val finalState = search(\n       QLIndexedState(qlSpace.init(state0), 0)\n    )\n  if( finalState.index == -1) false //28\n  else qlSpace.isGoal(finalState.state) //29\n}\n```", "```py\n@tailrec\ndef search(iSt: QLIndexedState[T]): QLIndexedState[T] = {\n  val states = qlSpace.nextStates(iSt.state) //30 \n\n  if( states.isEmpty || iSt.iter >= config.episodeLength) //31 \n    QLIndexedState(iSt.state, -1)\n\n  else {\n    val state = states.maxBy(s => \n       qlPolicy.R(iSt.state.id, s.id)) //32\n    if( qlSpace.isGoal(state) ) \n       QLIndexedState(state, iSt.iter)  //33\n\n    else {\n      val fromId = iSt.state.id\n      val r = qlPolicy.R(fromId, state.id)   \n      val q = qlPolicy.Q(fromId, state.id) //34\n\n      val nq = q + config.alpha*(r + \n         config.gamma * qlSpace.maxQ(state, qlPolicy)-q) //35\n      qlPolicy.setQ(fromId, state.id,  nq) //36\n      search(QLIndexedState(state, iSt.iter+1))\n    }\n  }\n}\n```", "```py\ndef nextStates(st: QLState[T]): Seq[QLState[T]] = \n  if( st.actions.isEmpty )\n    Seq.empty[QLState[T]]\n  else\n    st.actions.map(ac => statesMap.get(ac.to) )\n```", "```py\ndef maxQ(state: QLState[T], policy: QLPolicy): Double = {\n   val best = states.filter( _ != state)  //37\n      .maxBy(st => policy.EQ(state.id, st.id))  //38\n   policy.EQ(state.id, best.id)\n}\n```", "```py\ndef validateConstraints(numStates: Int, \n    constraints: Int => List[Int]): Boolean = \n  Range(0, numStates).exists( constraints(_).isEmpty )\n```", "```py\noverride def |> : PartialFunction[U, Try[V]] = {\n  case state0: U if(isModel) =>  Try {\n    if(state0.isGoal) state0  //39\n    else nextState(QLIndexedState[T](state0, 0)).state) //40\n  }\n}\n```", "```py\n@tailrec\ndef nextState(iSt: QLIndexedState[T]): QLIndexedState[T] =  {\n  val states = qlSpace.nextStates(iSt.state) //41\n\n  if( states.isEmpty || iSt.iter >=config.episodeLength) \n     iSt  //42\n  else {\n    val fromId = iSt.state.id\n    val qState = states.maxBy(s =>  //43\n       model.map(_.bestPolicy.R(fromId, s.id)).getOrElse(-1.0))\n    nextState(QLIndexedState[T](qState, iSt.iter+1)) //44\n  }\n}\n```", "```py\nclass OptionProperty(timeToExp: Double, volatility: Double, \n     vltyByVol: Double, priceToStrike: Double) { //45\n\n  val toArray = Array[Double](\n     timeToExp, volatility, vltyByVol, priceToStrike\n  )\n}\n```", "```py\nclass OptionModel(symbol: String,  strikePrice: Double, \n   src: DataSource, minExpT: Int, nSteps: Int) {\n\n  val propsList = (for {\n    price <- src.get(adjClose)\n    volatility <- src.get(volatility)\n    nVolatility <- normalize(volatility)\n    vltyByVol <- src.get(volatilityByVol)\n    nVltyByVol <- normalize(vltyByVol)\n    priceToStrike <- normalize(price.map(p => \n       (1.0 - strikePrice/p)))\n  } yield {\n    nVolatility.zipWithIndex./:(List[OptionProperty]()){ //46\n      case(xs, (v,n)) => {\n         val normDecay = (n + minExpT).toDouble/\n            (price.size + minExpT) //47\n         new OptionProperty(normDecay, v, nVltyByVol(n), \n           priceToStrike(n)) :: xs\n      }\n    }.drop(2).reverse  //48\n   })\n  .getOrElse(List.empty[OptionProperty].)\n\n  def quantize(o: DblArray): Map[Array[Int], Double] \n}\n```", "```py\ndef quantize(o: DblArray): Map[Array[Int], Double] = {\n  val mapper = new mutable.HashMap[Int, Array[Int]] //49\n  val _acc = new NumericAccumulator[Int] //50\n\n  val acc = propsList.view.map( _.toArray)\n       .map( toArrayInt( _ )) //51\n       .map(ar => { \n          val enc = encode(ar)  //52\n          mapper.put(enc, ar)\n          enc\n       }).zip(o)\n       ./:(_acc){ \n          case (acc, (t,y)) => { //53\n          acc += (t, y)\n          acc \n       }}\n   acc.map {case (k, (v,w)) => (k, v/w) }  //54\n     .map {case( k,v) => (mapper(k), v) }.toMap\n}\n```", "```py\nval STOCK_PRICES = \"resources/data/chap11/IBM.csv\"\nval OPTION_PRICES = \"resources/data/chap11/IBM_O.csv\"\nval QUANTIZER = 4\nval src = DataSource(STOCK_PRICES, false, false, 1) //55\n\nval model = for {\n  option <- Try(createOptionModel(src)) //56\n  oPrices <- DataSource(OPTION_PRICES, false).extract //57\n   _model <- createModel(option, oPrices) //58\n} yield _model\n\n```", "```py\nval STRIKE_PRICE = 190.0\nval MIN_TIME_EXPIRATION = 6\n\ndef createOptionModel(src: DataSource): OptionModel = \n   new OptionModel(\"IBM\", STRIKE_PRICE, src, \n       MIN_TIME_EXPIRATION, QUANTIZER)\n```", "```py\nval LEARNING_RATE = 0.2\nval DISCOUNT_RATE = 0.7\nval MAX_EPISODE_LEN = 128\nval NUM_EPISODES = 80\n\ndef createModel(option: OptionModel, oPrices: DblArray,\n     alpha: Double, gamma: Double): Try[QLModel] = Try {\n\n  val qPriceMap = option.quantize(oPrices) //59\n  val numStates = qPriceMap.size\n\n  val qPrice = qPriceMap.values.toVector //60\n  val profit= zipWithShift(qPrice,1).map{case(x,y) => y -x}//61\n  val maxProfitIndex = profit.zipWithIndex.maxBy(_._1)._2 //62\n\n  val reward = (x: Double, y: Double) \n            => Math.exp(30.0*(y – x)) //63\n  val probability = (x: Double, y: Double) => \n         if(y < 0.3*x) 0.0 else 1.0  //64\n\n  if( !validateConstraints(profit.size, neighbors)) //65 \n      throw new IllegalStateException(\" ... \")\n\n  val config = QLConfig(alpha, gamma, \n     MAX_EPISODE_LEN, NUM_EPISODES) //66\n  val instances = qPriceMap.keySet.toSeq.drop(1)\n  QLearning[Array[Int]](config, Array[Int](maxProfitIndex), \n     profit, reward, probability, \n     instances, Some(neighbors)).getModel //67\n}\n```", "```py\nclass XcsAction(sensorId: String, target: Double)\n    (implicit quantize: Quantization, encoding: Encoding) //1\n    extends Gene(sensorId, target, EQUAL)\n```", "```py\nval vwoTo80 = new XcsAction(\"VWO\", 80.0)\n```", "```py\ncase class XcsSensor(val sensorId: String, val value: Double) \nval new10ytb = XcsSensor(\"10yTBYield\", 2.76)\n```", "```py\nclass XcsRule(val signal: Signal, val action: XcsAction)\n```", "```py\nval signal = new Signal(\"10ytb\", 2.84, GREATER_THAN) \nval action = new XcsAction(\"vwo\", 240) \nval r1 = new XcsRule(signal, action)\n```", "```py\nval new10ytb = new XcsSensor(\"10ytb\", 2.76) //2\nval s10ytb1 = Signal(\"10ytb\", 2.5, GREATER_THAN)  //3\nval s10ytb2 = Signal(\"10ytb\", 2.2, LESS_THAN) //4\n\n```", "```py\nval r23: XcsRule(s10yTB1, act12) //5\nval r11: XcsRule(s10yTB6, act6) \nval r46: XcsRule(s10yTB7, act12) //6\n\n```", "```py\nval MAX_NUM_ACTIONS = 2048\n\ndef cover(sensor: XcsSensor, actions: List[XcsAction])\n   (implicit quant: Quantization, encoding: Encoding): \n   List[XcsRule] = \n\n  actions./:(List[XcsRule]()) ((xs, act) => {\n    val signal = Signal(sensor.id, sensor.value, \n       new SOperator(Random.nextInt(Signal.numOperators)))\n    new XcsRule(signal, XcsAction(act, Random)) :: xs\n  })\n}\n```", "```py\ndef apply(action: XcsAction, r: Random): XcsAction = \n   (action ^ r.nextInt(XCSACTION_SIZE))\n```", "```py\nclass Xcs(config: XcsConfig, \n    population: Population[Signal], \n    score: Chromosome[Signal]=> Unit, \n    input: Array[QLInput])  //7\n      extends ETransform[XcsConfig](config) { //8\n\n  type U = XcsSensor   //9\n  type V = List[XcsAction]   //10\n\n   val solver = GASolver[Signal](config.gaConfig,score) \n   val features = population.chromosomes.toSeq\n   val qLearner = QLearning[Chromosome[Signal]]( //11\n      config.qlConfig, extractGoals(input), input, features)\n   override def |> : PartialFunction[U, Try[V]]\n   ...\n}\n```", "```py\ncase class DataPoint(x: DblVector, y: Double)\n```", "```py\ntype StreamLike = WeakReference[Stream[DataPoint]] //1\nclass LossFunction(\n    f: (DblVector, DblVector) => Double,\n    weights: DblVector, \n    dataSize: Int) {  //2\n\n  var nElements = 0\n  def compute(stream: () => StreamLike): Double = \n      compute(stream().get, 0.0)  //3\n\n  def _loss(xs: List[DataPoint]): Double = xs.map(\n    dp => dp.y - f(weights, dp.x)).map( sqr(_)).sum //4\n}\n```", "```py\n@tailrec\ndef compute(stream: Stream[DataPoint], loss: Double): Double = {\n  if( nElements >= dataSize)  loss\n  else {\n    val step = if(nElements + STEP > dataSize) \n             dataSize - nElements else STEP\n    nElements += step\n    val newLoss = _loss(stream.take(step).toList) //5\n    compute( stream.drop(STEP), loss + newLoss ) //6\n  }\n }\n```", "```py\nval rand = new ParVector[Float]\nRange(0,MAX).foreach(n => rand.updated(n, n*Random.nextFloat))//1\nrand.tasksupport = new ForkJoinTaskSupport(new ForkJoinPool(16)) \nval randExp = vec.map( Math.exp(_) ) //2\n\n```", "```py\nobject test with Benchmark { def run { /* … /* }\n\n```", "```py\nabstract class ParBenchmark[U](times: Int) { \n  def map(f: U => U)(nTasks: Int): Double  //1\n  def filter(f: U => Boolean)(nTasks: Int): Double //2\n  def timing(g: Int => Unit ): Long\n}\n```", "```py\ndef timing(g: Int => Unit ): Long = {   \n  var startTime = System.currentTimeMillis\n  Range(0, times).foreach(g)\n  System.currentTimeMillis - startTime\n}\n```", "```py\nclass ParArrayBenchmark[U](u: Array[U], //3\n  v: ParArray[U], //4\n  times:Int) extends ParBenchmark[T](times)\n```", "```py\ndef map(f: U => U)(nTasks: Int): Unit = {\n  val pool = new ForkJoinPool(nTasks)\n  v.tasksupport = new ForkJoinTaskSupport(pool)\n  val duration = timing(_ => u.map(f)).toDouble  //5\n  val ratio = timing( _ => v.map(f))/duration  //6\n  show(s\"$numTasks, $ratio\")\n}\n```", "```py\ndef reduce(f: (U,U) => U)(nTasks: Int): Unit = { \n  val pool = new ForkJoinPool(nTasks)\n  v.tasksupport = new ForkJoinTaskSuppor(pool)   \n  val duration = timing(_ => u.reduceLeft(f)).toDouble //7\n  val ratio = timing( _ => v.reduceLeft(f) )/duration  //8\n  show(s\"$numTasks, $ratio\")\n}\n```", "```py\nclass ParMapBenchmark[U](val u: Map[Int, U], \n  val v: ParMap[Int, U], \n  times: Int) extends ParBenchmark[T](times)\n```", "```py\ndef filter(f: U => Boolean)(nTasks: Int): Unit = {\n  val pool = new ForkJoinPool(nTasks)\n  v.tasksupport = new ForkJoinTaskSupport(pool)   \n  val duration = timing(_ => u.filter(e => f(e._2))).toDouble \n  val ratio = timing( _ => v.filter(e => f(e._2)))/duration\n  show(s\"$nTasks, $ratio\")\n}\n```", "```py\nval sz = 1000000; val NTASKS = 16\nval data = Array.fill(sz)(Random.nextDouble) \nval pData = ParArray.fill(sz)(Random.nextDouble) \nval times: Int = 50\n\nval bench = new ParArrayBenchmark[Double](data, pData, times) \nval mapper = (x: Double) => Math.sin(x*0.01) + Math.exp(-x)\nRange(1, NTASKS).foreach(bench.map(mapper)(_)) \nval reducer = (x: Double, y: Double) => x+y \nRange(1, NTASKS).foreach(bench.reduce(reducer)(_)) \n```", "```py\nval sz = 10000000\nval mData = new HashMap[Int, Double]\nRange(0, sz).foreach( mData.put(_, Random.nextDouble)) //9\nval mParData = new ParHashMap[Int, Double]\nRange(0, sz).foreach( mParData.put(_, Random.nextDouble))\n\nval bench = new ParMapBenchmark[Double](mData, mParData, times)\nRange(1, NTASKS).foreach(bench.map(mapper)(_)) //10\nval filterer = (x: Double) => (x > 0.8) \nRange(1, NTASKS).foreach( bench.filter(filterer)(_)) //11\n\n```", "```py\nwhile( true ){\n  receive { case msg1: MsgType => handler } \n}\n```", "```py\nsealed abstract class Message(val i: Int)\ncase class Terminate(i: Int) extends Message(i)\ncase class Start(i: Int =0) extends Message(i)  //1\ncase class Activate(i: Int, x: DblVector) extends Message(i) //2\ncase class Completed(i: Int, x: DblVector) extends Message(i)//3\n\n```", "```py\ntype PfnTransform =  PartialFunction[DblVector, Try[DblVector]]\n\nclass Worker(id: Int, \n     fct: PfnTransform) extends Actor  {  //1\n  override def receive = {\n    case msg: Activate =>  //2\n      sender ! Completed(msg.id+id,  fct(msg.xt).get)\n   }\n}\n```", "```py\nabstract class Controller (\n  val xt: DblVector, \n   val fct: PfnTransform, \n   val nPartitions: Int) extends Actor with Monitor { //3\n\n   def partition: Iterator[DblVector] = { //4\n      val sz = (xt.size.toDouble/nPartitions).ceil.toInt\n      xt.grouped(sz)\n   }\n}\n```", "```py\nabstract class Master(  //5\n    xt: DblVector, \n    fct: PfnTransform, \n    nPartitions: Int) extends Controller(xt, fct, nPartitions) {\n\n  val aggregator = new Aggregator(nPartitions)  //6\n  val workers = List.tabulate(nPartitions)(n => \n        context.actorOf(Props(new Worker(n, fct)), \n               name = s\"worker_$n\"))  //7\n  workers.foreach( context.watch ( _ ) )  //8\n\n  override def preStart: Unit = /* ... */  //9\n  override def postStop: Unit = /* ... */  //10\n  override def receive \n}\n```", "```py\nclass Aggregator(partitions: Int) {\n  val state = new ListBuffer[DblVector]\n\n  def += (x: DblVector): Boolean = {\n    state.append(x)\n    state.size == partitions\n  }\n\n  def clear: Unit = state.clear\n  def completed: Boolean = state.size == partitions\n}\n```", "```py\noverride def receive = {\n  case s: Start => start  //11\n\n  case msg: Completed =>   //12\n    if( aggregator +=  msg.xt) //13\n      workers.foreach( context.stop(_) )   //14\n\n  case Terminated(sender) => //15\n    if( aggregator.completed ) {  \n      context.stop(self)   //16\n      context.system.shutdown\n    }\n}\n```", "```py\ndef start: Unit = workers.zip(partition.toVector)\n             .foreach {case (w, s) => w ! Activate(0,s)} //16\n\n```", "```py\nclass MasterWithRouter(\n    xt: DblVector, \n    fct: PfnTransform, \n    nPartitions: Int) extends Controller(xt, fct, nPartitions)  {\n\n  val aggregator = new Aggregator(nPartitions)\n  val router = {   //17\n    val routerConfig = RoundRobinRouter(nPartitions, //18\n           supervisorStrategy = this.supervisorStrategy)\n    context.actorOf(\n       Props(new Worker(0,fct)).withRouter(routerConfig) )\n   }\n   context.watch(router)\n\n   override def receive\n}\n```", "```py\noverride def receive = {\n  case Start => start\n  case msg: Completed => \n    if( aggregator += msg.xt) context.stop(router)  //19\n   ...\n}\n```", "```py\ndef start: Unit = \n  partition.toVector.foreach {router ! Activate(0, _)}\n```", "```py\ntype Reducer = List[DblVector] => immutable.Seq[Double]\nclass DFTMaster(\n    xt: DblVector, \n    nPartitions: Int, \n    reducer: Reducer)   //20\n      extends Master(xt, DFT[Double].|>, nPartitions)\n```", "```py\ndef fReduce(buf: List[DblVector]): immutable.Seq[Double] = \n   buf.transpose.map( _.sum).toSeq  //21\n\n```", "```py\nval NUM_WORKERS = 4 \nval NUM_DATAPOINTS = 1000000\nval h = (x: Double) =>2.0*Math.cos(Math.PI*0.005*x) + \n    Math.cos(Math.PI*0.05*x) + 0.5*Math.cos(Math.PI*0.2*x) +\n    0.3* Random.nextDouble   //22\n\nval actorSystem = ActorSystem(\"System\")  //23\nval xt = Vector.tabulate(NUM_DATA_POINTS)(h(_))\nval controller = actorSystem.actorOf(\n         Props(new DFTMasterWithRouter(xt, NUM_WORKERS, \n                    fReduce)), \"MasterWithRouter\")  //24\ncontroller ! Start(1) //25\n\n```", "```py\nimport akka.actor.{Actor, ActorSystem, ActorRef, Props} //26\nimport akka.util.Timeout   //27\nimport scala.concurrent.{Await, Future}  //28\n\n```", "```py\nabstract class TransformFutures(\n    xt: DblVector, \n    fct: PfnTransform, \n    nPartitions: Int)\n    (implicit timeout: Timeout) //29\n         extends Controller(xt, fct, nPartitions) {\n  override def receive = {\n    case s: Start => compute(transform) //30\n  }\n}\n```", "```py\ndef transform: Array[Future[DblVector]] = {\n  val futures = new Array[Future[DblVector]](nPartitions) //31\n\n  partition.zipWithIndex.foreach { case (x, n) => { //32\n    futures(n) = Future[DblVector] { fct(x).get } //33\n  }}\n  futures\n}\n```", "```py\ndef compute(futures: Array[Future[DblVector]]): Seq[Double] = \n  reduce(futures.map(Await.result(_, timeout.duration))) //34\n\n```", "```py\ndef reduce(data: Array[DblVector]): Seq[Double] = \n    data.view.map(_.toArray)\n        .transpose.map(_.sum)   //35\n            .take(SPECTRUM_WIDTH).toSeq\n```", "```py\nval f: Future[T] = future { execute task } f onComplete {   \n  case Success(s) => { … }   \n  case Failure(e) => { … }\n}\n```", "```py\nf onFailure { case e: Exception => { … } } \nf onSuccess { case t => { … } }\n```", "```py\ndef compute(futures: Array[Future[DblVector]]): Seq[Double] = {\n  val buffer = new ArrayBuffer[DblVector]\n\n  futures.foreach( f => {\n    f onSuccess {   //36\n      case data: DblVector => buffer.append(data)\n    }\n    f onFailure { case e: Exception =>  /* .. */ } //37\n  })\n   buffer.find( _.isEmpty).map( _ => reduce(buffer)) //38\n}\n```", "```py\nclass DFTTransformFutures(\n    xt: DblVector, \n    partitions: Int)(implicit timeout: Timeout) \n    extends TransformFutures(xt, DFT[Double].|> , partitions)  {\n\n  override def reduce(data: Array[DblVector]): Seq[Double] = \n    data.map(_.toArray).transpose\n        .map(_.sum).take(SPECTRUM_WIDTH).toSeq\n}\n```", "```py\nimport akka.pattern.ask\n\nval duration = Duration(8000, \"millis\")\nimplicit val timeout = new Timeout(duration)\nval master = actorSystem.actorOf(   //39\n       Props(new DFTTransformFutures(xt, NUM_WORKERS)), \n                        \"DFTTransform\")\nval future = master ? Start(0)  //40\nAwait.result(future, timeout.duration)   //41\nactorSystem.shutdown  //42\n\n```", "```py\nrdd.persist(StorageLevel.MEMORY_ONLY_SER).cache\n```", "```py\nclass Pipeline {\n  lazy val x = { println(\"x\"); 1.5}   \n  lazy val m = { println(\"m\"); 3}   \n  val n = { println(\"n\"); 6}   \n  def f = (m <<1)\n  def g(j: Int) = Math.pow(x, j)\n}\nval pipeline = new Pipeline  //1\npipeline.g(pipeline.f)  //2\n\n```", "```py\n        MAVEN_OPTS=\"-Xmx4g -XX:MaxPermSize=512M \n                  -XX:ReservedCodeCacheSize=512m\"\n        mvn [args] –DskipTests clean package\n\n        ```", "```py\n            mvn -Pyarn –Phadoop-2.4 –Dhadoop.version-2.4.0 –DskipTests \n             clean package\n\n            ```", "```py\n            mvn -Pyarn –Phadoop-2.6 –Dhadoop.version-2.6.0 –Dscala-2.11 \n             –DskipTests clean package\n\n            ```", "```py\n            sbt/sbt [args] assembly\n\n            ```", "```py\n             sbt -Pyarn –pHadoop 2.4 assembly\n\n            ```", "```py\n             sbt -Pyarn –pHadoop 2.6 –Dscala-2.11 assembly\n\n            ```", "```py\n    ./bin/spark-submit --class application_class --master local[4] \n       --executor-memory 12G  --jars myApplication.jar \n       –class myApp.class\n    ```", "```py\n    ./bin/spark-submit --class application_class \n       --master spark://162.198.11.201:7077 \n       –-total-executor-cores 80  \n       --executor-memory 12G  \n       --jars myApplication.jar –class myApp.class\n    ```", "```py\ndef convert(\n    xt: immutable.Vector[DblArray], \n    rddConfig: RDDConfig) \n    (implicit sc: SparkContext): RDD[Vector] = {\n\n  val rdd: RDD[Vector] = \n     sc.parallelize(xt.toVector.map(new DenseVector(_))) //3\n  rdd.persist(rddConfig.persist) //4\n  if( rddConfig.cache) rdd.cache  //5\n  rdd\n}\n```", "```py\ncase class RDDConfig(val cache: Boolean, \n    val persist: StorageLevel)\n```", "```py\nclass SparkKMeansConfig(K: Int, maxIters: Int, \n     numRuns: Int = 1) {   \n  val kmeans: KMeans = {      \n    (new KMeans).setK(K) //6\n      .setMaxIterations(maxIters)  //7\n      .setRuns(numRuns) //8\n  }\n}\n```", "```py\nclass SparkKMeans(    //9\n    kMeansConfig: SparkKMeansConfig, \n    rddConfig: RDDConfig, \n    xt: Vector[DblArray])\n   (implicit sc: SparkContext) extends ITransform[DblArray](xt){\n\n  type V = Int   //10\n  val model: Option[KMeansModel] = train  //11\n\n  override def |> : PartialFunction[DblArray, Try[V]] //12\n  def train: Option[KMeansModel] \n}\n```", "```py\noverride def |> : PartialFunction[DblArray, Try[V]] = {\n  case x: DblArray if(x.length > 0 && model != None) => \n     Try[V](model.get.predict(new DenseVector(x)))\n}\n```", "```py\nval K = 8\nval RUNS = 16\nval MAXITERS = 200\nval PATH = \"resources/data/chap12/CSCO.csv\"\nval CACHE = true\n\nval sparkConf = new SparkConf().setMaster(\"local[8]\")\n   .setAppName(\"SparkKMeans\")\n   .set(\"spark.executor.memory\", \"2048m\") //13\nimplicit val sc = new SparkContext(sparkConf) //14\n\nextract.map { case (vty,vol)  => {  //15\n  val vtyVol = zipToSeries(vty, vol)  \n  val conf = SparkKMeansConfig(K,MAXITERS,RUNS) //16\n  val rddConf = RDDConfig(CACHE, \n                    StorageLevel.MEMORY_ONLY) //17\n\n  val pfnSparkKMeans = SparkKMeans(conf,rddConf,vtyVol) |> //18\n  val obs = Array[Double](0.23, 0.67)\n  val clusterId = pfnSparkKMeans(obs)\n}\n```", "```py\ndef extract: Option[(DblVector, DblVector)] = {\n  val extractors = List[Array[String] => Double](\n    YahooFinancials.volatility, YahooFinancials.volume \n  )\n  val pfnSrc = DataSource(PATH, true) |>\n  pfnSrc( extractors ) match {\n    case Success(x) => Some((x(0).toVector, x(1).toVector))\n    case Failure(e) => { error(e.toString); None }\n  }\n}\n```", "```py\n    /**\n    This class is defined as …\n    */\n    // The MathRuntime exception has to be caught here!\n    ```", "```py\n    class BaumWelchEM(val lambda: HMMLambda ...) {\n    require( lambda != null, \"Lambda model is undefined\")\n    ```", "```py\n    final protected class MLP[T <% Double] …\n    ```", "```py\n    final def inputLayer: MLPLayer\n    private def recurse: Unit =\n    ```", "```py\n    class Config extends Serializable { … }\n    ```", "```py\n    val pfn: PartialFunction[U, V]\n    pfn.isDefinedAt(u)\n    ```", "```py\n    assert( p != None, \" … \")\n    ```", "```py\n    try { … }\n    catch { case e: ArrayIndexOutOfBoundsException  => … }\n    if (y < EPS)\n       throw new IllegalStateException( … )\n    ```", "```py\n    Try(process(args)) match {\n       case Success(results) => …\n       case Failure(e) => …\n    }\n    ```", "```py\n    @inline def mean = { … }\n    @implicitNotFound(\"Conversion $T to Array[Int] undefined\")\n    @throws(classOf[IllegalStateException)\n    ```", "```py\n    m_logger.debug( …)\n    Console.println( … )\n    ```", "```py\nprotected class A[T](val x: X, val y: Y,…) { … } \nobject A {\n  def apply[T](x: X, y: Y, ...): A[T] = new A(x, y,…)\n  def apply[T](x: , ..): A[T] = new A(x, y0, …)\n}\n```", "```py\nfinal protected class SVM[T <: AnyVal](\n    config: SVMConfig, \n    xt: XVSeries[T], \n    expected: DblVector)(implicit f: T => Double) \n  extends ITransform[Array[T]](xt) {\n```", "```py\ndef apply[T <: AnyVal](\n    config: SVMConfig, \n    xt: XVSeries[T], \n    expected: DblVector)(implicit f: T => Double): SVM[T] = \n  new SVM[T](config, xt, expected)\n```", "```py\nobject A extends Enumeration {\n  type TA = Value\n  val A, B, C = Value\n}\n\nimport A._\nval counters = Array.fill(A.maxId+1)(0)\nRange(0, 1000).foreach( _ => Random.nextInt(10) match {\n  case 3 => counters(A.id) += 1\n  …\n  case _ => { }\n})\n```", "```py\npackage AA {\n  sealed abstract class A(val level: Int)\n  case class AA extends A(3) { def f =(x:Double) => 23*x}\n  …\n}\n\nimport AA._\ndef compute(a: A, x: Double): Double = a match {\n   case a: A => a.f(x)\n   …\n}\n```", "```py\nfinal protected class SVM[T <: AnyVal](\n    val config: SVMConfig, \n    val xt: XTSeries[Array[T]], \n    val labels: DblVector)(implicit f: T => Double)\n  extends ITransform[Array[T]](xt) with Monitor[Double] {\n\n  type V = \n  val model: Option[SVMModel] = { … }\n  override def |> PartialFunction[Array[T], V]\n  …\n}\n```", "```py\nfinal class SVMConfig(val formulation: SVMFormulation, \n    val kernel: SVMKernel, \n    val svmExec: SVMExecution) extends Config\n\nclass SVMModel(val svmmodel: svm_model) extends Model\n\n```", "```py\ncase class DataSourceConfig(pathName: String, normalize: Boolean, \n     reverseOrder: Boolean, headerLines: Int = 1)\n```", "```py\nfinal class DataSource(config: DataSourceConfig,\n    srcFilter: Option[Fields => Boolean]= None)\n  extends ETransform[DataSourceConfig](config) {\n\n  type Fields = Array[String]\n  type U = List[Fields => Double]\n  type V = XVSeries[Double]\n  override def |> : PartialFunction[U, Try[V]] \n  ...\n}\n```", "```py\noverride def |> : PartialFunction[U, Try[V]] = {\n  case fields: U if(!fields.isEmpty) =>load.map(data =>{ //1\n    val convert = (f: Fields =>Double) => data._2.map(f(_))\n    if( config.normalize)  //2\n      fields.map(t => new MinMax[Double](convert(t)) //3\n           .normalize(0.0, 1.0).toArray ).toVector //4\n    else fields.map(convert(_)).toVector\n  })\n}\n```", "```py\nobject YahooFinancials extends Enumeration {\n   type YahooFinancials = Value\n   val DATE, OPEN, HIGH, LOW, CLOSE, VOLUME, ADJ_CLOSE = Value\n   val adjClose = ((s:Array[String]) =>\n        s(ADJ_CLOSE.id).toDouble)  //5\n   val volume =  (s: Fields) => s(VOLUME.id).toDouble\n   …\n   def toDouble(value: Value): Array[String] => Double = \n       (s: Array[String]) => s(value.id).toDouble\n}\n```", "```py\nval symbols = Array[String](\"CSCO\", ...)  //6\nval prices = symbols\n       .map(s => DataSource(s\"$path$s.csv\",true,true,1))//7\n       .map( _ |> adjClose ) //8\n\n```", "```py\nobject GoogleFinancials extends Enumeration {\n   type GoogleFinancials = Value\n   val DATE, OPEN, HIGH, LOW, CLOSE, VOLUME = Value\n   val close = ((s:Array[String]) =>s(CLOSE.id).toDouble)//5\n   …\n}\n```", "```py\nclass DocumentsSource(dateFormat: SimpleDateFormat,\n    val pathName: String) \n  extends ETransform[SimpleDateFormat](dateFormat) {\n\n type U = Option[Long] //2\n type V = Corpus[Long]  //3\n\n override def |> : PartialFunction[U, Try[V]] = { //4\n    case date: U if (filesList != None) => \n      Try( if(date == None ) getAll else get(date) )\n }\n def get(t: U): V = getAll.filter( _.date == t.get)\n def getAll: V  //5\n ...\n}\n```", "```py\nclass DMatrix(val nRows: Int, val nCols: Int, \n     val data: DblArray) {\n def apply(i: Int, j: Int): Double = data(i*nCols+j)\n def row(iRow: Int): DblArray = { \n   val idx = iRow*nCols\n   data.slice(idx, idx + nCols)\n }\n def col(iCol: Int): IndexedSeq[Double] =\n   (iCol until data.size by nCols).map( data(_) )\n def diagonal: IndexedSeq[Double] = \n    (0 until data.size by nCols+1).map( data(_))\n def trace: Double = diagonal.sum\n  …\n}\n```", "```py\nclass Counter[T] extends mutable.HashMap[T, Int] {\n  def += (t: T): type.Counter = super.put(t, getOrElse(t, 0)+1) \n  def + (t: T): Counter[T] = { \n   super.put(t, getOrElse(t, 0)+1); this \n  }\n  def ++ (cnt: Counter[T]): type.Counter = { \n    cnt./:(this)((c, t) => c + t._1); this\n  }\n  def / (cnt: Counter[T]): mutable.HashMap[T, Double] = map { \n    case(str, n) => (str, if( !cnt.contains(str) ) \n      throw new IllegalStateException(\" ... \")\n        else n.toDouble/cnt.get(str).get )\n  }\n  …\n}\n```", "```py\ntrait Monitor[T] {\n  protected val logger: Logger\n  lazy val _counters = \n      new mutable.HashMap[String, mutable.ArrayBuffer[T]]()\n\n  def counters(key: String): Option[mutable.ArrayBuffer[T]]\n  def count(key: String, value: T): Unit \n  def display(key: String, legend: Legend)\n      (implicit f: T => Double): Boolean\n  def show(msg: String): Int = DisplayUtils.show(msg, logger)\n  def error(msg: String): Int = DisplayUtils.error(msg, logger)\n  ...\n}\n```"]