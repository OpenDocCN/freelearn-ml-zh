["```py\nsetwd(\"~/Desktop/chapter 2\") \nlibrary(rsample) \ndata(attrition) \nstr(attrition) \nmydata<-attrition \n```", "```py\n'data.frame':1470 obs. of  31 variables: \n $ Age                     : int  41 49 37 33 27 32 59 30 38 36 ... \n $ Attrition               : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 1 1 1 .... \n $ BusinessTravel          : Factor w/ 3 levels \"Non-Travel\",\"Travel_Frequently\",..: 3 2 3 2 3 2 3 3 2 3 ... \n $ DailyRate               : int  1102 279 1373 1392 591 1005 1324 1358 216 1299 ... \n $ Department              : Factor w/ 3 levels \"Human_Resources\",..: 3 2 2 2 2 2 2 2 2 2 ... \n $ DistanceFromHome        : int  1 8 2 3 2 2 3 24 23 27 ... \n $ Education               : Ord.factor w/ 5 levels \"Below_College\"<..: 2 1 2 4 1 2 3 1 3 3 ... \n $ EducationField          : Factor w/ 6 levels \"Human_Resources\",..: 2 2 5 2 4 2 4 2 2 4 ... \n $ EnvironmentSatisfaction : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 2 3 4 4 1 4 3 4 4 3 ... \n $ Gender                  : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 2 1 2 2 2 ... \n $ HourlyRate              : int  94 61 92 56 40 79 81 67 44 94 ... \n $ JobInvolvement          : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 3 2 2 3 3 3 4 3 2 3 ... \n $ JobLevel                : int  2 2 1 1 1 1 1 1 3 2 ... \n $ JobRole                 : Factor w/ 9 levels \"Healthcare_Representative\",..: 8 7 3 7 3 3 3 3 5 1 ... \n $ JobSatisfaction         : Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 4 2 3 3 2 4 1 3 3 3 ... \n $ MaritalStatus           : Factor w/ 3 levels \"Divorced\",\"Married\",..: 3 2 3 2 2 3 2 1 3 2 ... \n $ MonthlyIncome           : int  5993 5130 2090 2909 3468 3068 2670 2693 9526 5237 ... \n $ MonthlyRate             : int  19479 24907 2396 23159 16632 11864 9964 13335 8787 16577 ... \n $ NumCompaniesWorked      : int  8 1 6 1 9 0 4 1 0 6 ... \n $ OverTime                : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 1 1 2 1 1 1 ... \n $ PercentSalaryHike       : int  11 23 15 11 12 13 20 22 21 13 ... \n $ PerformanceRating       : Ord.factor w/ 4 levels \"Low\"<\"Good\"<\"Excellent\"<..: 3 4 3 3 3 3 4 4 4 3 ... \n $ RelationshipSatisfaction: Ord.factor w/ 4 levels \"Low\"<\"Medium\"<..: 1 4 2 3 4 3 1 2 2 2 ... \n $ StockOptionLevel        : int  0 1 0 0 1 0 3 1 0 2 ... \n $ TotalWorkingYears       : int  8 10 7 8 6 8 12 1 10 17 ... \n $ TrainingTimesLastYear   : int  0 3 3 3 3 2 3 2 2 3 ... \n $ WorkLifeBalance         : Ord.factor w/ 4 levels \"Bad\"<\"Good\"<\"Better\"<..: 1 3 3 3 3 2 2 3 3 2 ... \n $ YearsAtCompany          : int  6 10 0 8 2 7 1 1 9 7 ... \n $ YearsInCurrentRole      : int  4 7 0 7 2 7 0 0 7 7 ... \n $ YearsSinceLastPromotion : int  0 1 0 3 2 3 0 0 1 7 ... \n $ YearsWithCurrManager    : int  5 7 0 0 2 6 0 0 8 7 ... \n```", "```py\ntable(mydata$Attrition) \n```", "```py\n No   Yes  \n1233  237  \n```", "```py\n# considering only the numeric variables in the dataset \nnumeric_mydata <- mydata[,c(1,4,6,7,10,11,13,14,15,17,19,20,21,24,25,26,28:35)] \n# converting the target variable \"yes\" or \"no\" values into numeric \n# it defaults to 1 and 2 however converting it into 0 and 1 to be consistent \nnumeric_Attrition = as.numeric(mydata$Attrition)- 1 \n# create a new data frame with numeric columns and numeric target  \nnumeric_mydata = cbind(numeric_mydata, numeric_Attrition) \n# loading the required library \nlibrary(corrplot) \n# creating correlation plot \nM <- cor(numeric_mydata) \ncorrplot(M, method=\"circle\") \n```", "```py\n### Overtime vs Attiriton \nl <- ggplot(mydata, aes(OverTime,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$OverTime,mean) \n\nNo Yes\n0.104364326375712 0.305288461538462\n```", "```py\nprint(l) \n```", "```py\n### MaritalStatus vs Attiriton \nl <- ggplot(mydata, aes(MaritalStatus,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$MaritalStatus,mean) \nDivorced 0.100917431192661 \nMarried 0.12481426448737 \nSingle 0.25531914893617 \n```", "```py\nprint(l) \n```", "```py\n###JobRole vs Attrition \nl <- ggplot(mydata, aes(JobRole,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$JobRole,mean) \n\nHealthcare Representative    Human Resources \n               0.06870229    0.23076923 \n    Laboratory Technician    Manager \n               0.23938224    0.04901961 \n   Manufacturing Director    Research Director \n               0.06896552    0.02500000 \n       Research Scientist    Sales Executive \n               0.16095890    0.17484663 \n     Sales Representative \n               0.39759036 \nmean(as.numeric(mydata$Attrition) - 1) \n[1] 0.161224489795918 \n```", "```py\nprint(l)\n```", "```py\n###Gender vs Attrition \nl <- ggplot(mydata, aes(Gender,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$Gender,mean) \n\nFemale 0.147959183673469 \nMale 0.170068027210884 \n```", "```py\nprint(l)\n```", "```py\n###EducationField vs Attrition el <- ggplot(mydata, aes(EducationField,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$EducationField,mean) \n\nHuman Resources    Life Sciences    Marketing \n       0.2592593    0.1468647        0.2201258 \n         Medical   Other Technical  Degree \n       0.1357759    0.1341463        0.2424242\n```", "```py\nprint(l)\n```", "```py\n###Department vs Attrition \nl <- ggplot(mydata, aes(Department,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$Department,mean) \nHuman Resources  Research & Development  Sales \n   0.1904762       0.1383975              0.2062780 \n```", "```py\nprint(l) \n```", "```py\n###BusinessTravel vs Attrition \nl <- ggplot(mydata, aes(BusinessTravel,fill = Attrition)) \nl <- l + geom_histogram(stat=\"count\") \n\ntapply(as.numeric(mydata$Attrition) - 1 ,mydata$BusinessTravel,mean) \n Non-Travel   Travel_Frequently   Travel_Rarely \n  0.0800000    0.2490975           0.1495686\n```", "```py\nprint(l) \n```", "```py\n### x=Overtime, y= Age, z = MaritalStatus , t = Attrition \nggplot(mydata, aes(OverTime, Age)) +   \n  facet_grid(.~MaritalStatus) + \n  geom_jitter(aes(color = Attrition),alpha = 0.4) +   \n  ggtitle(\"x=Overtime, y= Age, z = MaritalStatus , t = Attrition\") +   \n  theme_light() \n```", "```py\n### MonthlyIncome vs. Age, by  color = Attrition \nggplot(mydata, aes(MonthlyIncome, Age, color = Attrition)) +  \n  geom_jitter() + \n  ggtitle(\"MonthlyIncome vs. Age, by  color = Attrition \") + \n  theme_light() \n```", "```py\n# Load the necessary libraries \n# doMC is a library that enables R to use multiple cores available on the sysem thereby supporting multiprocessing.  \nlibrary(doMC) \n# registerDoMC command instructs R to use the specified number of cores to execute the code. In this case, we ask R to use 4 cores available on the system \nregisterDoMC(cores=4) \n# caret library has the ml algorithms and other routines such as cross validation etc.  \nlibrary(caret) \n# Setting the working directory where the dataset is located \nsetwd(\"~/Desktop/chapter 2\") \n# Reading the csv file into R variable called mydata \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n#Removing the non-discriminatory features (as identified during EDA) from the dataset  \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n# setting the seed prior to model building ensures reproducibility of the results obtained \nset.seed(10000) \n# setting the train control parameters specifying gold standard 10 fold cross validation  repeated 10 times \nfitControl = trainControl(method=\"repeatedcv\", number=10,repeats=10) \n###creating a model on the data. Observe that we specified Attrition as the target and that model should learn from rest of the variables. We specified mydata as the dataset to learn. We pass the train control parameters and specify that knn algorithm need to be used to build the model. K can be of any length - we specified 20 as parameter which means the train command will search through 20 different random k values and finally retains the model that produces the best performance measurements. The final model is stored as caretmodel \ncaretmodel = train(Attrition~., data=mydata, trControl=fitControl, method = \"knn\", tuneLength = 20) \n# We output the model object to the console  \ncaretmodel \n```", "```py\nk-Nearest Neighbors  \n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1323, 1323, 1324, 1323, 1324, 1322, ...  \nResampling results across tuning parameters: \n  k   Accuracy   Kappa        \n   5  0.8216447  0.0902934591 \n   7  0.8349033  0.0929511324 \n   9  0.8374198  0.0752842114 \n  11  0.8410920  0.0687849122 \n  13  0.8406861  0.0459679081 \n  15  0.8406875  0.0337742424 \n  17  0.8400748  0.0315670261 \n  19  0.8402770  0.0245499585 \n  21  0.8398721  0.0143638854 \n  23  0.8393945  0.0084393721 \n  25  0.8391891  0.0063246624 \n  27  0.8389174  0.0013913143 \n  29  0.8388503  0.0007113939 \n  31  0.8387818  0.0000000000 \n  33  0.8387818  0.0000000000 \n  35  0.8387818  0.0000000000 \n  37  0.8387818  0.0000000000 \n  39  0.8387818  0.0000000000 \n  41  0.8387818  0.0000000000 \n  43  0.8387818  0.0000000000 \nAccuracy was used to select the optimal model using the largest value. \nThe final value used for the model was k = 11\\. \n```", "```py\ncaretmodel$finalModel \n```", "```py\n11-nearest neighbor model \nTraining set outcome distribution: \n  No  Yes  \n1233  237  \n```", "```py\n # save the model to disk \nsaveRDS(caretmodel, \"production_model.rds\") \n```", "```py\n# Set the working directory to the directory where the saved .rds file is located  \nsetwd(\"~/Desktop/chapter 2\") \n#Load the model  \nloaded_model <- readRDS(\"production_model.rds\") \n```", "```py\n#Using the loaded model to make predictions on unseen data \nfinal_predictions <- predict(loaded_model, unseen_data) \n```", "```py\nlibrary(doMC) \nregisterDoMC(cores = 4)  \nlibrary(caret) \n#setting the random seed for replication \nset.seed(1234) \n# setting the working directory where the data is located \nsetwd(\"~/Desktop/chapter 2\") \n# reading the data \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n#removing the non-discriminatory features identified during EDA \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n#setting up cross-validation \ncvcontrol <- trainControl(method=\"repeatedcv\", repeats=10, number = 10, allowParallel=TRUE) \n# model creation with treebag , observe that the number of bags is set as 10 \ntrain.bagg <- train(Attrition ~ ., data=mydata, method=\"treebag\",B=10, trControl=cvcontrol, importance=TRUE) \ntrain.bagg \n```", "```py\nBagged CART  \n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1324, 1323, 1323, 1322, 1323, 1322, ...  \nResampling results: \n  Accuracy  Kappa     \n  0.854478  0.2971994 \n```", "```py\n# Setting up SVM predict function as the default svmBag$pred function has some code issue \nsvm.predict <- function (object, x) \n{ \n if (is.character(lev(object))) { \n    out <- predict(object, as.matrix(x), type = \"probabilities\") \n    colnames(out) <- lev(object) \n    rownames(out) <- NULL \n  } \n  else out <- predict(object, as.matrix(x))[, 1] \n  out \n} \n# setting up parameters to build svm bagging model \nbagctrl <- bagControl(fit = svmBag$fit, \n                      predict = svm.predict , \n                      aggregate = svmBag$aggregate) \n# fit the bagged svm model \nset.seed(300) \nsvmbag <- train(Attrition ~ ., data = mydata, method=\"bag\",trControl = cvcontrol, bagControl = bagctrl,allowParallel = TRUE) \n# printing the model results \nsvmbag \n```", "```py\nBagged Model  \n\n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \n\nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1324, 1324, 1323, 1323, 1323, 1323, ...  \nResampling results: \n  Accuracy   Kappa     \n  0.8777721  0.4749657 \n\nTuning parameter 'vars' was held constant at a value of 44 \n```", "```py\n# setting up parameters to build svm bagging model \nbagctrl <- bagControl(fit = nbBag$fit, \n                      predict = nbBag$pred , \n                      aggregate = nbBag$aggregate) \n# fit the bagged nb model \nset.seed(300) \nnbbag <- train(Attrition ~ ., data = mydata, method=\"bag\", trControl = cvcontrol, bagControl = bagctrl) \n# printing the model results \nnbbag \n```", "```py\nBagged Model  \n\n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \n\nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1324, 1324, 1323, 1323, 1323, 1323, ...  \nResampling results: \n\n  Accuracy   Kappa      \n  0.8389878  0.00206872 \n\nTuning parameter 'vars' was held constant at a value of 44 \n```", "```py\nbagControl(fit = nnetBag$fit, predict = nnetBag$pred , aggregate = nnetBag$aggregate) \n```", "```py\n# loading required libraries and registering multiple cores to enable parallel processing \nlibrary(doMC) \nlibrary(caret) \nregisterDoMC(cores=4) \n# setting the working directory and reading the dataset \nsetwd(\"~/Desktop/chapter 2\") \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n# removing the non-discriminatory features from the dataset as identified during EDA step \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n# setting the seed for reproducibility \nset.seed(10000) \n# setting the cross validation parameters \nfitControl = trainControl(method=\"repeatedcv\", number=10,repeats=10) \n# creating the caret model with random forest algorithm \ncaretmodel = train(Attrition~., data=mydata, method=\"rf\", trControl=fitControl, verbose=F) \n# printing the model summary \ncaretmodel \n```", "```py\nRandom Forest  \n\n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \n\nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1323, 1323, 1324, 1323, 1324, 1322, ...  \nResampling results across tuning parameters: \n\n  mtry  Accuracy   Kappa     \n   2    0.8485765  0.1014859 \n  23    0.8608271  0.2876406 \n  44    0.8572929  0.2923997 \n\nAccuracy was used to select the optimal model using the largest value. \nThe final value used for the model was mtry = 23\\. \n```", "```py\n# loading the essential libraries and registering the cores for multiprocessing \nlibrary(doMC) \nlibrary(mlbench) \nlibrary(gbm) \nlibrary(caret) \nregisterDoMC(cores=4) \n# setting the working directory and reading the dataset \nsetwd(\"~/Desktop/chapter 2\") \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n# removing the non-discriminatory features as identified by EDA step \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n# converting the target attrition feild to numeric as gbm model expects all numeric feilds in the dataset \nmydata$Attrition = as.numeric(mydata$Attrition) \n# forcing the attrition column values to be 0 and 1 instead of 1 and 2 \nmydata = transform(mydata, Attrition=Attrition-1) \n# running the gbm model with 10 fold cross validation to identify the number of trees to build - hyper parameter tuning \ngbm.model = gbm(Attrition~., data=mydata, shrinkage=0.01, distribution = 'bernoulli', cv.folds=10, n.trees=3000, verbose=F) \n# identifying and printing the value of hyper parameter identified through the tuning above \nbest.iter = gbm.perf(gbm.model, method=\"cv\") \nprint(best.iter) \n# setting the seed for reproducibility \nset.seed(123) \n# creating a copy of the dataset \nmydata1=mydata \n# converting target to a factor \nmydata1$Attrition=as.factor(mydata1$Attrition) \n# setting up cross validation controls \nfitControl = trainControl(method=\"repeatedcv\", number=10,repeats=10) \n# runing the gbm model in tandem with caret  \ncaretmodel = train(Attrition~., data=mydata1, method=\"gbm\", distribution=\"bernoulli\",  trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1)) \n# printing the model summary \nprint(caretmodel) \n```", "```py\n2623 \nStochastic Gradient Boosting  \n\n1470 samples \n  30 predictors \n   2 classes: '0', '1'  \n\nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1323, 1323, 1323, 1322, 1323, 1323, ...  \nResampling results: \n  Accuracy   Kappa     \n  0.8771472  0.4094991 \nTuning parameter 'n.trees' was held constant at a value of 2623 \nTuning parameter 'shrinkage' was held constant at a value of 0.01 \nTuning parameter 'n.minobsinnode' was held constant at a value of 1 \n```", "```py\n# loading the required libraries and registering the cores for multiprocessing \nlibrary(doMC) \nlibrary(xgboost) \nlibrary(caret) \nregisterDoMC(cores=4) \n# setting the working directory and loading the dataset \nsetwd(\"~/Desktop/chapter 2\") \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n# removing the non-discriminatory features from the dataset as identified in EDA step \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n# setting up cross validation parameters \nControlParamteres <- trainControl(method = \"repeatedcv\",number = 10, repeats=10, savePredictions = TRUE, classProbs = TRUE) \n# setting up hyper parameters grid to tune   \nparametersGrid <-  expand.grid(eta = 0.1, colsample_bytree=c(0.5,0.7), max_depth=c(3,6),nrounds=100, gamma=1, min_child_weight=2,subsample=0.5) \n# printing the parameters grid to get an intuition \nprint(parametersGrid) \n# xgboost model building \nmodelxgboost <- train(Attrition~., data = mydata, method = \"xgbTree\", trControl = ControlParamteres, tuneGrid=parametersGrid) \n# printing the model summary \nprint(modelxgboost) \n```", "```py\neXtreme Gradient Boosting  \n1470 samples \n  30 predictors \n   2 classes: 'No', 'Yes'  \n\nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 1323, 1323, 1322, 1323, 1323, 1322, ...  \nResampling results across tuning parameters: \n\n  max_depth  colsample_bytree  Accuracy   Kappa     \n  3          0.5               0.8737458  0.3802840 \n  3          0.7               0.8734728  0.3845053 \n  6          0.5               0.8730674  0.3840938 \n  6          0.7               0.8732589  0.3920721 \n\nTuning parameter 'nrounds' was held constant at a value of 100 \nTuning parameter 'min_child_weight' was held constant at a value of 2 \nTuning parameter 'subsample' was held constant at a value of 0.5 \nAccuracy was used to select the optimal model using the largest value. \nThe final values used for the model were nrounds = 100, max_depth = 3, eta = 0.1, gamma = 1, colsample_bytree = 0.5, min_child_weight = 2 and subsample = 0.5\\. \n```", "```py\n# loading the required libraries and registering the cpu cores for multiprocessing \nlibrary(doMC) \nlibrary(caret) \nlibrary(caretEnsemble) \nregisterDoMC(cores=4) \n# setting the working directory and loading the dataset \nsetwd(\"~/Desktop/chapter 2\") \nmydata <- read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\") \n# removing the non-discriminatory features from the dataset as identified in EDA step \nmydata$EmployeeNumber=mydata$Over18=mydata$EmployeeCount=mydata$StandardHours = NULL \n# setting up control paramaters for cross validation \ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=10, savePredictions=TRUE, classProbs=TRUE) \n# declaring the ML algorithms to use in stacking \nalgorithmList <- c('C5.0', 'nb', 'glm', 'knn', 'svmRadial') \n# setting the seed to ensure reproducibility of the results \nset.seed(10000) \n# creating the stacking model \nmodels <- caretList(Attrition~., data=mydata, trControl=control, methodList=algorithmList) \n# obtaining the stacking model results and printing them \nresults <- resamples(models) \nsummary(results) \n```", "```py\nsummary.resamples(object = results) \n\nModels: C5.0, nb, glm, knn, svmRadial  \nNumber of resamples: 100  \n\nAccuracy  \n               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's \nC5.0      0.8082192 0.8493151 0.8639456 0.8625833 0.8775510 0.9054054    0 \nnb        0.8367347 0.8367347 0.8378378 0.8387821 0.8424658 0.8435374    0 \nglm       0.8299320 0.8639456 0.8775510 0.8790444 0.8911565 0.9387755    0 \nknn       0.8027211 0.8299320 0.8367347 0.8370763 0.8438017 0.8630137    0 \nsvmRadial 0.8287671 0.8648649 0.8775510 0.8790467 0.8911565 0.9319728    0 \n\nKappa  Min.          1st Qu.     Median     Mean   3rd Qu.      Max.  NA's \nC5.0   0.03992485 0.29828006 0.37227344 0.3678459 0.4495049 0.6112590    0 \nnb     0.00000000 0.00000000 0.00000000 0.0000000 0.0000000 0.0000000    0 \nglm    0.26690604 0.39925723 0.47859218 0.4673756 0.5218094 0.7455280    0 \nknn   -0.05965697 0.02599388 0.06782465 0.0756081 0.1320451 0.2431312    0 \nsvmRadial 0.24565 0.38667527 0.44195662 0.4497538 0.5192393 0.7423764    0 \n\n# Identifying the correlation between results \nmodelCor(results) \n```", "```py\n# Setting up the cross validation control parameters for stacking the predictions from individual ML algorithms \nstackControl <- trainControl(method=\"repeatedcv\", number=10, repeats=10, savePredictions=TRUE, classProbs=TRUE) \n# stacking the predictions of individual ML algorithms using generalized linear model \nstack.glm <- caretStack(models, method=\"glm\", trControl=stackControl) \n# printing the stacked final results \nprint(stack.glm) \n```", "```py\nA glm ensemble of 2 base models: C5.0, nb, glm, knn, svmRadial \nEnsemble results: \nGeneralized Linear Model  \n14700 samples \n    5 predictors \n    2 classes: 'No', 'Yes'  \nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 13230, 13230, 13230, 13230, 13230, 13230, ...  \nResampling results: \n  Accuracy   Kappa     \n  0.8844966  0.4869556 \n```", "```py\n# stacking the predictions of individual ML algorithms using random forest \nstack.rf <- caretStack(models, method=\"rf\", trControl=stackControl) \n# printing the summary of rf based stacking \nprint(stack.rf) \n```", "```py\nA rf ensemble of 2 base models: C5.0, nb, glm, knn, svmRadial \nEnsemble results: \nRandom Forest  \n14700 samples \n    5 predictors \n    2 classes: 'No', 'Yes'  \nNo pre-processing \nResampling: Cross-Validated (10 fold, repeated 10 times)  \nSummary of sample sizes: 13230, 13230, 13230, 13230, 13230, 13230, ...  \nResampling results across tuning parameters: \n  mtry  Accuracy   Kappa     \n  2     0.9122041  0.6268108 \n  3     0.9133605  0.6334885 \n  5     0.9132925  0.6342740 \nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 3\\. \n```"]