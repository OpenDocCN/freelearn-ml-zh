["```py\n    >library(rmr2)\n    >LOCAL <- T #to execute rmr2 locally\n    ```", "```py\n    >#map function\n    >map.wc <- function(k,lines){ \n           words.list <- strsplit(lines,'\\\\s+^' )\n            words <- unlist(words.list)         \n            return(keyval(words,1))\n        }\n    ```", "```py\n    >#reduce function\n    >reduce.wc<-function(word,counts){\n              return(keyval(word,sum(counts) ))\n    }\n    ```", "```py\n    >#word count function\n    >wordcount<-function(input,output=NULL){\n                mapreduce(input = input,output = output,input.format = \"text\",map = map.wc,reduce = reduce.wc,combine = T)\n    }\n    >out<-wordcount(hdfs.data,hdfs.out)\n    ```", "```py\n    >results<-from.dfs(out)\n    >results.df<-as.data.frame(results,stringAsFactors=F)\n    >colnames(results.df)<-c('word^' ,^' count^')\n    >head(results.df)\n    ```", "```py\nSys.setenv(SPARK_HOME/.../spark-1.5.0-bin-hadoop2.6\")\n#provide the correct path where spark downloaded folder is kept for SPARK_HOME \n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"),\"\"R\",\"\"lib\"),\".libPaths()))\n```", "```py\n>library(SparkR)\n>sc <- sparkR.init(master=\"local\")\n```", "```py\n>library(SparkR)\n>sc <- sparkR.init(master=\"local\")\n>sqlContext <- sparkRSQL.init(sc)\n\n#Importing data\n>df <- read.csv(\"/Users/harikoduvely/Projects/Book/Data/ENB2012_data.csv\",header = T)\n>#Excluding variable Y2,X6,X8 and removing records from 768 containing mainly null values\n>df <- df[1:768,c(1,2,3,4,5,7,9)]\n>#Converting to a Spark R Dataframe\n>dfsr <- createDataFrame(sqlContext,df) \n>model <- glm(Y1 ~ X1 + X2 + X3 + X4 + X5 + X7,data = dfsr,family = \"gaussian\")\n > summary(model)\n```", "```py\n    deb http://cran.rstudio.com/bin/linux/ubuntu trusty .\n    ```", "```py\n    sudo apt-key adv  --keyserver keyserver.ubuntu.com â€“recv-keys 51716619E084DAB9\n\n    ```", "```py\n    sudo apt-get update\n\n    ```", "```py\n    sudo apt-get install r-base-core\n\n    ```", "```py\n    sudo apt-get install gdebi-core\n\n    ```", "```py\n    wget http://download2.rstudio.org/r-studio-server-0.99.446-amd64.deb\n\n    ```", "```py\n    sudo gdebi r-studio-server-0.99.446-amd64.deb\n\n    ```", "```py\n    ./spark-ec2 -k <keypair> -i <key-file> -s <num-slaves> launch <cluster-name>\n\n    ```", "```py\n    ./spark-ec2 -k <keypair> -i <key-file> login <cluster-name>\n\n    ```", "```py\n>nsquare <- function(n){return(n*n)}\n>range <- c(1:100000)\n>system.time(lapply(range,nsquare))\n```", "```py\n>library(parallel) #included in R core packages, no separate installation required\n>numCores<-detectCores( )  #to find the number of cores in the machine\n>system.time(mclapply(range,nsquare,mc.cores=numCores))\n```", "```py\n>install.packages(Rmpi)#one time\n>library(Rmpi)\n>numNodes<-4 #number of workers nodes\n>cl<-makeCluster(numNodes,type=\"MPI\")\n>system.time(parLapply(cl,range,nsquare))\n>stopCluster(cl)\n>mpi.exit( )\n```", "```py\n>install.packages(foreach)#one time\n>install.packages(doParallel)#one time\n>library(foreach)\n>library(doParallel)\n>system.time(foreach(i=1:100000)   %do%  i^2) #for executing sequentially\n>system.time(foreach(i=1:100000)   %dopar%  i^2) #for executing in parallel\n```", "```py\n>qsort<- function(x) {\n  n <- length(x)\n  if (n == 0) {\n    x\n  } else {\n    p <- sample(n,1)\n    smaller <- foreach(y=x[-p],.combine=c) %:% when(y <= x[p]) %do% y\n    larger  <- foreach(y=x[-p],.combine=c) %:% when(y >  x[p]) %do% y\n    c(qsort(smaller),x[p],qsort(larger))\n  }\n}\nqsort(runif(12))\n```"]