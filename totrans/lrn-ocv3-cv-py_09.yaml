- en: Chapter 9. Neural Networks with OpenCV – an Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a branch of artificial intelligence, one that deals specifically
    with algorithms that enable a machine to recognize patterns and trends in data
    and successfully make predictions and classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the algorithms and techniques used by OpenCV to accomplish some of the
    more advanced tasks in computer vision are directly related to artificial intelligence
    and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to the Machine Learning concepts in OpenCV such
    as artificial neural networks. This is a gentle introduction that barely scratches
    the surface of a vast world, that of Machine Learning, which is continuously evolving.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by defining **Artificial Neural Networks** (**ANN**) with a number
    of logical steps, rather than a classic monolithic sentence using obscure jargon
    with an even more obscure meaning.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, an ANN is a **statistical model**. What is a statistical model?
    A statistical model is a pair of elements, namely the space `S` (a set of observations)
    and the probability `P`, where `P` is a distribution that approximates `S` (in
    other words, a function that would generate a set of observations that is very
    similar to `S`).
  prefs: []
  type: TYPE_NORMAL
- en: 'I like to think of `P` in two ways: as a simplification of a complex scenario,
    and as the function that generated `S` in the first place, or at the very least
    a set of observations very similar to `S`.'
  prefs: []
  type: TYPE_NORMAL
- en: So ANNs are models that take a complex reality, simplify it, and deduce a function
    to (approximately) represent statistical observations one would expect from that
    reality, in mathematical form.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our journey towards comprehending ANNs is to understand how
    an ANN improves on the concept of a simple statistical model.
  prefs: []
  type: TYPE_NORMAL
- en: What if the function that generated the dataset is likely to take a large amount
    of (unknown) inputs?
  prefs: []
  type: TYPE_NORMAL
- en: The approach that ANNs take is to delegate work to a number of **neurons**,
    **nodes**, or **units**, each of which is capable of "approximating" the function
    that created the inputs. Approximation is mathematically the definition of a simpler
    function that approximates a more complex function, which enables us to define
    errors (relative to the application domain). Furthermore, for accuracy's sake,
    a network is generally recognized to be neural if the neurons or units are capable
    of approximating a nonlinear function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons and perceptrons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **perceptron** is a concept that dates back to the 1950s, and (to put it
    simply) a perceptron is a function that takes a number of inputs and produces
    a single value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the inputs has an associated weight that signifies the importance of
    the input in the function. The sigmoid function produces a single value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neurons and perceptrons](img/image00251.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A sigmoid function is a term that indicates that the function produces either
    a 0 or 1 value. The discriminant is a threshold value; if the weighted sum of
    the inputs is greater than a certain threshold, the perceptron produces a binary
    classification of 1, otherwise 0.
  prefs: []
  type: TYPE_NORMAL
- en: How are these weights determined, and what do they represent?
  prefs: []
  type: TYPE_NORMAL
- en: Neurons are interconnected to each other, and each neuron's set of weights (these
    are just numerical parameters) defines the strength of the connection to other
    neurons. These weights are "adaptive", meaning they change in time according to
    a learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of an ANN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s a visual representation of a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The structure of an ANN](img/image00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the figure, there are three distinct layers in a neural
    network: **Input layer**, **Hidden layer** (or middle), and **Output layer**.'
  prefs: []
  type: TYPE_NORMAL
- en: There can be more than one hidden layer; however, one hidden layer would be
    enough to resolve the majority of real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: Network layers by example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do we determine the network's topology, and how many neurons to create for
    each layer? Let's make this determination layer by layer.
  prefs: []
  type: TYPE_NORMAL
- en: The input layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input layer defines the number of inputs into the network. For example,
    let's say you want to create an ANN, which will help you determine what animal
    you're looking at given a description of its attributes. Let's fix these attributes
    to weight, length, and teeth. That's a set of three attributes; our network will
    need to contain three input nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The output layer is equal to the number of classes we identified. Continuing
    with the preceding example of an animal classification network, we will arbitrarily
    set the output layer to `4`, because we know we''re going to deal with the following
    animals: dog, condor, dolphin, and dragon. If we feed in data for an animal that
    is not in one of these categories, the network will return the class most likely
    to resemble this unclassified animal.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hidden layer contains perceptrons. As mentioned, the vast majority of problems
    only require one hidden layer; mathematically speaking, there is no verified reason
    to have more than two hidden layers. We will, therefore, stick to one hidden layer
    and work with that.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of rules of thumb to determine the number of neurons contained
    in the hidden layer, but there is no hard-and-fast rule. The empirical way is
    your friend in this particular circumstance: test your network with different
    settings, and choose the one that fits best.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some of the most common rules used when building an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden neurons should be between the size of the input layer and
    the size of the output layer. If the difference between the input layer size and
    the output layer is large, it is my experience that a hidden layer size much closer
    to the output layer is preferable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For relatively small input layers, the number of hidden neurons is two-thirds
    the size of the input layer, plus the size of the output layer, or less than twice
    the size of the input layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One very important factor to keep in mind is **overfitting**. Overfitting occurs
    when there's such an inordinate amount of information contained in the hidden
    layer (for example, a disproportionate amount of neurons in the layer) compared
    to the information provided by the training data that classification is not very
    meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: The larger the hidden layer, the more training information is required for the
    network to be trained properly. And, needless to say, this is going to lengthen
    the time required by the network to properly train.
  prefs: []
  type: TYPE_NORMAL
- en: So, following the second rules of thumb illustrated earlier, our network will
    have a hidden layer of size 8, just because after a few runs of the network, I
    found it to yield the best results. As a side note, the empirical approach is
    very much encouraged in the world of ANNs. The best network topology is related
    to the type of data fed to the network, so don't refrain from testing ANNs in
    a trial-and-error fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our network has the following sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: `3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden**: `8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of learning algorithms used by ANNs, but we can identify
    three major ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: With this algorithm, we want to obtain a function
    from the ANN, which describes the data we labeled. We know, a priori, the nature
    of this data, and we delegate to the ANN the process of finding a function that
    describes the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: This algorithm differs from supervised learning;
    in this, the data is unlabeled. This implies that we don''t have to select and
    label data, but it also means the ANN has a lot more work to do. The classification
    of the data is usually obtained through techniques such as (but not only) clustering,
    which we explored in [Chapter 7](part0049.xhtml#aid-1ENBI2 "Chapter 7. Detecting
    and Recognizing Objects"), *Detecting and Recognizing Objects*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: Reinforcement learning is a little more complex.
    A system receives an input; a decision-making mechanism determines an action,
    which is performed and scored (success/failure and grades in between); and finally
    the input and the action are paired with their score, so the system learns to
    repeat or change the action to be performed for a certain input or state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a general idea of what ANNs are, let's see how OpenCV implements
    them, and how to put them to good use. Finally, we'll work our way up to a full
    blown application, in which we will attempt to recognize handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsurprisingly, ANNs reside in the `ml` module of OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine a dummy example, as a gentle introduction to ANNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we create an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may wonder about the `MLP` acronym in the function name; it stands for **multilayer
    perceptron**. By now, you should know what a perceptron is.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the network, we need to set its topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The layer sizes are defined by the NumPy array that is passed into the `setLayerSizes`
    method. The first element sets the size of the input layer, the last element sets
    the size of the output layer, and all intermediary elements define the size of
    the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then set the train method to be backpropagation. There are two choices here:
    `BACKPROP` and `RPROP`.'
  prefs: []
  type: TYPE_NORMAL
- en: Both `BACKPROP` and `RPROP` are backpropagation algorithms—in simple terms,
    algorithms that have an effect on weights based on errors in classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two algorithms work in the context of supervised learning, which is what
    we are using in the example. How can we tell this particular detail? Look at the
    next statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should notice a number of details. The method looks extremely similar to
    the `train()` method of support vector machine. The method contains three parameters:
    `samples`, `layout`, and `responses`. Only `samples` is the required parameter;
    the other two are optional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This reveals the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: First, ANN, like SVM, is an OpenCV `StatModel` (**statistical model**); `train`
    and `predict` are the methods inherited from the base `StatModel` class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, a statistical model trained with only `samples` is adopting an unsupervised
    learning algorithm. If we provide `layout` and `responses`, we're in a supervised
    learning context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we're using ANNs, we can specify the type of back propagation algorithm we're
    going to use (`BACKPROP` or `RPROP`), because—as we said—we're in a supervised
    learning environment.
  prefs: []
  type: TYPE_NORMAL
- en: So what is back propagation? Back propagation is a two-phase algorithm that
    calculates the error of predictions and updates in both directions of the network
    (the input and output layers); it then updates the neuron weights accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the ANN; as we specified an input layer of size 9, we need to
    provide 9 inputs, and 9 outputs to reflect the size of the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The structure of the response is simply an array of zeros, with a `1` value
    in the position indicating the class we want to associate the input with. In our
    preceding example, we indicated that the specified input array corresponds to
    class 5 (classes are zero-indexed) of classes 0 to 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we perform classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will yield the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This means that the provided input was classified as belonging to class 5\.
    This is only a dummy example and the classification is pretty meaningless; however,
    the network behaved correctly. In this code, we only provided one training record
    for class 5, so the network classified a new input as belonging to class 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have guessed, the output of a prediction is a tuple, with the first
    value being the class and the second being an array containing the probabilities
    for each class. The predicted class will have the highest value. Let''s move on
    to a slightly more useful example: animal classification.'
  prefs: []
  type: TYPE_NORMAL
- en: ANN-imal classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Picking up from where we left off, let''s illustrate a very simple example
    of an ANN that attempts to classify animals based on their statistics (weight,
    length, and teeth). My intent is to describe a mock real-life scenario to improve
    our understanding of ANNs before we start applying it to computer vision and,
    specifically, OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There are a good few differences between this example and the dummy example,
    so let's examine them in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the usual imports. Then, we import `randint`, just because we want to
    generate some relatively random data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the ANN. This time, we specify the `train` method to be resilient
    back propagation (an improved version of back propagation) and the activation
    function to be a sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we specify the termination criteria similarly to the way we did in the
    CAMShift algorithm in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need some data. We''re not really so much interested in representing
    animals accurately, as requiring a bunch of records to be used as training data.
    So we basically define four sample creation functions and four classification
    functions that will help us train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s proceed with the creation of our fake animal data; we''ll create 5,000
    samples per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we print the results that yield the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'From these results, we deduce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The network got two out of three samples correct, which is not perfect but serves
    as a good example to illustrate the importance of all the elements involved in
    building and training an ANN. The size of the input layer is very important to
    create diversification between the different classes. In our case, we only had
    three statistics and there is a relative overlapping in features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The size of the hidden layer needs to be tested. You will find that increasing
    neurons may improve accuracy to a point, and then it will overfit, unless you
    start compensating with enormous amounts of data: the number of training records.
    Definitely, avoid having too few records or feeding a lot of identical records
    as the ANN won''t learn much from them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important concept in training ANNs is the idea of epochs. A training
    epoch is an iteration through the training data, after which the data is tested
    for classification. Most ANNs train over several epochs; you'll find that some
    of the most common examples of ANNs, classifying handwritten digits, will have
    the training data iterated several hundred times.
  prefs: []
  type: TYPE_NORMAL
- en: I personally suggest you spend a lot of time playing with ANNs and the number
    of epochs, until you reach convergence, which means that further iterations will
    no longer improve (at least not noticeably) the accuracy of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example can be modified as follows to leverage epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, do some tests, starting with the `dog` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat over all classes and output the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we obtain the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Consider the fact that we're only playing with toy/fake data and the size of
    training data/training iterations; this teaches us quite a lot. We can diagnose
    the ANN as overfitting towards certain classes, so it's important to improve the
    quality of the data you feed into the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that said, time for a real-life example: handwritten digit recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digit recognition with ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of Machine Learning is vast and mostly unexplored, and ANNs are but
    one of the many concepts related to Machine Learning, which is one of the many
    subdisciplines of Artificial Intelligence. For the purpose of this chapter, we
    will only be exploring the concept of ANNs in the context of OpenCV. It is by
    no means an exhaustive treatise on the subject of Artificial Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, we're interested in seeing ANNs work in the real world. So let's
    go ahead and make it happen.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST – the handwritten digit database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular resources on the Web for the training of classifiers
    dealing with OCR and handwritten character recognition is the MNIST database,
    publicly available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: This particular database is a freely available resource to kick-start the creation
    of a program that utilizes ANNs to recognize handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Customized training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is always possible to build your own training data. It will take a little
    bit of patience but it's fairly easy; collect a vast number of handwritten digits
    and create images containing a single digit, making sure all the images are the
    same size and in grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: After this, you will have to create a mechanism that keeps a training sample
    in sync with the expected classification.
  prefs: []
  type: TYPE_NORMAL
- en: The initial parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s take a look at the individual layers in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since we''re going to utilize the MNIST database, the input layer will have
    a size of 784 input nodes: that''s because MNIST samples are 28x28 pixel images,
    which means 784 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, there's no hard-and-fast rule for the size of the hidden layer,
    I've found—through several attempts—that 50 to 60 nodes yields the best result
    while not necessitating an inordinate amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: You can increase the size of the hidden layer with the amount of data, but beyond
    a certain point, there will be no advantage to that; you will also have to be
    prepared for your network to take hours to train (the more hidden neurons, the
    longer it takes to train the network).
  prefs: []
  type: TYPE_NORMAL
- en: The output layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output layer will have a size of 10\. This should not be a surprise as we
    want to classify 10 digits (0-9).
  prefs: []
  type: TYPE_NORMAL
- en: Training epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will initially use the entire set of the `train` data from MNIST, which consists
    of over 60,000 handwritten images, half of which were written by US government
    employees, and the other half by high-school students. That's a lot of data, so
    we won't need more than one epoch to achieve an acceptably high accuracy on detection.
  prefs: []
  type: TYPE_NORMAL
- en: From there on, it is up to you to train the network iteratively on the same
    `train` data, and my suggestion is that you use an accuracy test, and find the
    epoch at which the accuracy "peaks". By doing so, you will have a precise measurement
    of the highest possible accuracy achieved by your network given its current configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Other parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a sigmoid activation function, **Resilient Back Propagation** (**RPROP**),
    and extend the termination criteria for each calculation to 20 iterations instead
    of 10, like we did for every other operation in this book that involved `cv2.TermCriteria`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Important notes on train data and ANNs libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Internet for sources, I found an amazing article by Michael Nielsen
    at [http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html),
    which illustrates how to write an ANN library from scratch, and the code for this
    library is freely available on GitHub at [https://github.com/mnielsen/neural-networks-and-deep-learning](https://github.com/mnielsen/neural-networks-and-deep-learning);
    this is the source code for a book, *Neural Networks and Deep Learning*, by Michael
    Nielsen.
  prefs: []
  type: TYPE_NORMAL
- en: In the `data` folder, you will find a `pickle` file, signifying data that has
    been saved to disk through the popular Python library, `cPickle`, which makes
    loading and saving the Python data a trivial task.
  prefs: []
  type: TYPE_NORMAL
- en: This pickle file is a `cPickle` library-serialized version of the MNIST data
    and, as it is so useful and ready to work with, I strongly suggest you use that.
    Nothing stops you from loading the MNIST dataset but the process of deserializing
    the training data is quite tedious and—strictly speaking—outside the remit of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Second, I would like to point out that OpenCV is not the only Python library
    that allows you to use ANNs, not by any stretch of the imagination. The Web is
    full of alternatives that I strongly encourage you to try out, most notably **PyBrain**,
    a library called **Lasagna** (which—as an Italian—I find exceptionally attractive)
    and many custom-written implementations, such as the aforementioned Michael Nielsen's
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Enough introductory details, though. Let's get going.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up an ANN in OpenCV is not difficult, but you will almost definitely
    find yourself training your network countless times, in search of that elusive
    percentage point that boosts the accuracy of your results.
  prefs: []
  type: TYPE_NORMAL
- en: To automate this as much as possible, we will build a mini-library that wraps
    the OpenCV's native implementation of ANNs and lets us rerun and retrain the network
    easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of a wrapper library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let's examine it in order. First, the `load_data`, `wrap_data`, and `vectorized_result`
    functions are included in Michael Nielsen's code for loading the `pickle` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a relatively straightforward loading of a `pickle` file. Most notably,
    though, the loaded data has been split into the `train` and `test` data. Both
    `train` and `test` data are arrays containing two-element tuples: the first one
    is the data itself; the second one is the expected classification. So we can use
    the `train` data to train the ANN and the `test` data to evaluate its accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: The `vectorized_result` function is a very clever function that—given an expected
    classification—creates a 10-element array of zeros, setting a single 1 for the
    expected result. This 10-element array, you may have guessed, will be used as
    a classification for the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first ANN-related function is `create_ANN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This function creates an ANN specifically geared towards handwritten digit recognition
    with MNIST, by specifying layer sizes as illustrated in the *Initial parameters*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need a training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this is quite simple: given a number of samples and training epochs,
    we load the data, and then iterate through the samples an x-number-of-epochs times.'
  prefs: []
  type: TYPE_NORMAL
- en: The important section of this function is the deconstruction of the single training
    record into the `train` data and an expected classification, which is then passed
    into the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we utilize the `numpy` array function, `ravel()`, which takes an
    array of any shape and "flattens" it into a single-row array. So, for example,
    consider this array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding array once "raveled", becomes the following array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is the format that OpenCV's ANN expects data to look like in its `train()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return both the `network` and `test` data. We could have just returned
    the data, but having the `test` data at hand for accuracy checking is quite useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last function we need is a `predict()` function to wrap ANN''s own `predict()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This function takes an ANN and a sample image; it operates a minimum of "sanitization"
    by making sure the shape of the data is as expected and resizing it if it's not,
    and then raveling it for a successful prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The file I created also contains a `test` function to verify that the network
    works and it displays the sample provided for classification.
  prefs: []
  type: TYPE_NORMAL
- en: The main file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This whole chapter has been an introductory journey leading us to this point.
    In fact, many of the techniques we're going to use are from previous chapters,
    so in a way the entire book has led us to this point. So let's put all our knowledge
    to good use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an initial look at the file, and then decompose it for a better
    understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After the initial usual imports, we import the mini-library we created, which
    is stored in `digits_ann.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'I find it good practice to define functions at the top of the file, so let''s
    examine those. The `inside()` function determines whether a rectangle is entirely
    contained in another rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `wrap_digit()` function takes a rectangle that surrounds a digit, turns
    it into a square, and centers it on the digit itself, with 5-point padding to
    make sure the digit is entirely contained in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The point of this function will become clearer later on; let's not dwell on
    it too much at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create the network. We will use 58 hidden nodes, and train over
    20,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is good enough for a preliminary test to keep the training time down to
    a minute or two (depending on the processing power of your machine). The ideal
    is to use the full set of training data (50,000), and iterate through it several
    times, until some convergence is reached (as we discussed earlier, the accuracy
    "peak"). You would do this by calling the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now prepare the data to test. To do that, we''re going to load an image,
    and clean up a little:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a grayscale smoothed image, we can apply a threshold and some
    morphology operations to make sure the numbers are properly standing out from
    the background and relatively cleaned up for irregularities, which might throw
    the prediction operation off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note the threshold flag, which is for an inverse binary threshold: as the samples
    of the MNIST database are white on black (and not black on white), we turn the
    image into a black background with white numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the morphology operation, we need to identify and separate each number
    in the picture. To do this, we first identify the contours in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iterate through the contours, and discard all the rectangles that
    are entirely contained in other rectangles; we only append to the list of good
    rectangles the ones that are not contained in other rectangles and are also not
    as wide as the image itself. In some of the tests, `findContours` yielded the
    entire image as a contour itself, which meant no other rectangle passed the `inside`
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a list of good rectangles, we can iterate through them and
    define a region of interest for each of the rectangles we identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where the `wrap_digit()` function we defined at the beginning of the
    program comes into play: we need to pass a square region of interest to the predictor
    function; if we simply resized a rectangle into a square, we''d ruin our test
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder why think of the number one. A rectangle surrounding the number
    one would be very narrow, especially if it has been drawn without too much of
    a lean to either side. If you simply resized it to a square, you would "fatten"
    the number one in such a way that nearly the entire square would turn black, rendering
    the prediction impossible. Instead, we want to create a square around the identified
    number, which is exactly what `wrap_digit()` does.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is quick-and-dirty; it allows us to draw a square around a number
    and simultaneously pass that square as a region of interest for the prediction.
    A purer approach would be to take the original rectangle and "center" it into
    a square `numpy` array with rows and columns equal to the larger of the two dimensions
    of the original rectangle. The reason for this is you will notice that some of
    the square will include tiny bits of adjacent numbers, which can throw the prediction
    off. With a square created from a `np.zeros()` function, no impurities will be
    accidentally dragged in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the prediction for the square region is complete, we draw it on the original
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it! The final result will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The main file](img/image00253.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Possible improvements and potential applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have illustrated how to build an ANN, feed it training data, and use it for
    classification. There are a number of aspects we can improve, depending on the
    task at hand, and a number of potential applications of our new-found knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of improvements that can be applied to this approach, some
    of which we have already discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could enlarge your dataset and iterate more times, until a
    performance peak is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could also experiment with the several activation functions (`cv2.ml.ANN_MLP_SIGMOID_SYM`
    is not the only one; there is also `cv2.ml.ANN_MLP_IDENTITY` and `cv2.ml.ANN_MLP_GAUSSIAN`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could utilize different training flags (`cv2.ml.ANN_MLP_UPDATE_WEIGHTS`,
    `cv2.ml.ANN_MLP_NO_INPUT_SCALE`, `cv2.ml.ANN_MLP_NO_OUTPUT_SCALE`), and training
    methods (back propagation or resilient back propagation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aside from that, bear in mind one of the mantras of software development: there
    is no single best technology, there is only the best tool for the job at hand.
    So, careful analysis of the application requirements will lead you to the best
    choices of parameters. For example, not everyone draws digits the same way. In
    fact, you will even find that some countries draw numbers in a slightly different
    way.'
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database was compiled in the US, in which the number seven is drawn
    like the character 7\. But you will find that the number 7 in Europe is often
    drawn with a small horizontal line half way through the diagonal portion of the
    number, which was introduced to distinguish it from the number 1.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a more detailed overview of regional handwriting variations, check the Wikipedia
    article on the subject, which is a good introduction, available at [https://en.wikipedia.org/wiki/Regional_handwriting_variation](https://en.wikipedia.org/wiki/Regional_handwriting_variation).
  prefs: []
  type: TYPE_NORMAL
- en: This means the MNIST database has limited accuracy when applied to European
    handwriting; some numbers will be classified more accurately than others. So you
    may end up creating your own dataset. In almost all circumstances, it is preferable
    to utilize the `train` data that's relevant and belongs to the current application
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, remember that once you're happy with the accuracy of your network,
    you can always save it and reload it later, so it can be utilized in third-party
    applications without having to train the ANN every time.
  prefs: []
  type: TYPE_NORMAL
- en: Potential applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding program is only the foundation of a handwriting recognition application.
    Straightaway, you can quickly extend the earlier approach to videos and detect
    handwritten digits in real-time, or you could train your ANN to recognize the
    entire alphabet for a full-blown OCR system.
  prefs: []
  type: TYPE_NORMAL
- en: Car registration plate detection seems like an obvious extension of the lessons
    learned to this point, and it should be an even easier domain to work with, as
    registration plates use consistent characters.
  prefs: []
  type: TYPE_NORMAL
- en: Also, for your own edification or business purposes, you may try to build a
    classifier with ANNs and plain SVMs (with feature detectors such as SIFT) and
    see how they benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we scratched the surface of the vast and fascinating world
    of ANNs, focusing on OpenCV's implementation of it. We learned about the structure
    of ANNs, and how to design a network topology based on application requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we utilized various concepts that we explored in the previous chapters
    to build a handwritten digit recognition application.
  prefs: []
  type: TYPE_NORMAL
- en: To boldly go…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope you enjoyed the journey through the Python bindings for OpenCV 3\. Although
    covering OpenCV 3 would take a series of books, we explored very fascinating and
    futuristic concepts, and I encourage you to get in touch and let me and the OpenCV
    community know what your next groundbreaking computer-vision-based project is!
  prefs: []
  type: TYPE_NORMAL
