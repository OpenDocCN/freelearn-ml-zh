- en: Chapter 9. Neural Networks with OpenCV – an Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a branch of artificial intelligence, one that deals specifically
    with algorithms that enable a machine to recognize patterns and trends in data
    and successfully make predictions and classifications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Many of the algorithms and techniques used by OpenCV to accomplish some of the
    more advanced tasks in computer vision are directly related to artificial intelligence
    and machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to the Machine Learning concepts in OpenCV such
    as artificial neural networks. This is a gentle introduction that barely scratches
    the surface of a vast world, that of Machine Learning, which is continuously evolving.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by defining **Artificial Neural Networks** (**ANN**) with a number
    of logical steps, rather than a classic monolithic sentence using obscure jargon
    with an even more obscure meaning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: First of all, an ANN is a **statistical model**. What is a statistical model?
    A statistical model is a pair of elements, namely the space `S` (a set of observations)
    and the probability `P`, where `P` is a distribution that approximates `S` (in
    other words, a function that would generate a set of observations that is very
    similar to `S`).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'I like to think of `P` in two ways: as a simplification of a complex scenario,
    and as the function that generated `S` in the first place, or at the very least
    a set of observations very similar to `S`.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: So ANNs are models that take a complex reality, simplify it, and deduce a function
    to (approximately) represent statistical observations one would expect from that
    reality, in mathematical form.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The next step in our journey towards comprehending ANNs is to understand how
    an ANN improves on the concept of a simple statistical model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: What if the function that generated the dataset is likely to take a large amount
    of (unknown) inputs?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The approach that ANNs take is to delegate work to a number of **neurons**,
    **nodes**, or **units**, each of which is capable of "approximating" the function
    that created the inputs. Approximation is mathematically the definition of a simpler
    function that approximates a more complex function, which enables us to define
    errors (relative to the application domain). Furthermore, for accuracy's sake,
    a network is generally recognized to be neural if the neurons or units are capable
    of approximating a nonlinear function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at neurons.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Neurons and perceptrons
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **perceptron** is a concept that dates back to the 1950s, and (to put it
    simply) a perceptron is a function that takes a number of inputs and produces
    a single value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the inputs has an associated weight that signifies the importance of
    the input in the function. The sigmoid function produces a single value:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Neurons and perceptrons](img/image00251.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: A sigmoid function is a term that indicates that the function produces either
    a 0 or 1 value. The discriminant is a threshold value; if the weighted sum of
    the inputs is greater than a certain threshold, the perceptron produces a binary
    classification of 1, otherwise 0.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个术语，表示该函数产生0或1的值。判别值是一个阈值；如果输入的加权和大于某个特定的阈值，感知器产生二进制分类1，否则为0。
- en: How are these weights determined, and what do they represent?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重是如何确定的，它们代表了什么？
- en: Neurons are interconnected to each other, and each neuron's set of weights (these
    are just numerical parameters) defines the strength of the connection to other
    neurons. These weights are "adaptive", meaning they change in time according to
    a learning algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元相互连接，每个神经元的权重集合（这些只是数值参数）定义了与其他神经元连接的强度。这些权重是“自适应的”，意味着它们会根据学习算法随时间变化。
- en: The structure of an ANN
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN的结构
- en: 'Here''s a visual representation of a neural network:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是神经网络的一个视觉表示：
- en: '![The structure of an ANN](img/image00252.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![ANN的结构](img/image00252.jpeg)'
- en: 'As you can see from the figure, there are three distinct layers in a neural
    network: **Input layer**, **Hidden layer** (or middle), and **Output layer**.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，神经网络中有三个不同的层：**输入层**、**隐藏层**（或中间层）和**输出层**。
- en: There can be more than one hidden layer; however, one hidden layer would be
    enough to resolve the majority of real-life problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可以有多个隐藏层；然而，一个隐藏层就足以解决大多数现实生活中的问题。
- en: Network layers by example
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过例子看网络层
- en: How do we determine the network's topology, and how many neurons to create for
    each layer? Let's make this determination layer by layer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定网络的拓扑结构，以及每个层应该创建多少个神经元？让我们一层一层地做出这个决定。
- en: The input layer
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入层
- en: The input layer defines the number of inputs into the network. For example,
    let's say you want to create an ANN, which will help you determine what animal
    you're looking at given a description of its attributes. Let's fix these attributes
    to weight, length, and teeth. That's a set of three attributes; our network will
    need to contain three input nodes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层定义了网络中的输入数量。例如，假设你想创建一个ANN，它将帮助你根据动物属性的描述确定你所看到的动物。让我们将这些属性固定为重量、长度和牙齿。这是一组三个属性；我们的网络需要包含三个输入节点。
- en: The output layer
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出层
- en: 'The output layer is equal to the number of classes we identified. Continuing
    with the preceding example of an animal classification network, we will arbitrarily
    set the output layer to `4`, because we know we''re going to deal with the following
    animals: dog, condor, dolphin, and dragon. If we feed in data for an animal that
    is not in one of these categories, the network will return the class most likely
    to resemble this unclassified animal.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的数量等于我们识别的类别数。继续使用前面提到的动物分类网络的例子，我们将任意设置输出层为`4`，因为我们知道我们将处理以下动物：狗、秃鹫、海豚和龙。如果我们输入不属于这些类别的动物的数据，网络将返回最可能类似于这个未分类动物的分类。
- en: The hidden layer
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层
- en: The hidden layer contains perceptrons. As mentioned, the vast majority of problems
    only require one hidden layer; mathematically speaking, there is no verified reason
    to have more than two hidden layers. We will, therefore, stick to one hidden layer
    and work with that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层包含感知器。如前所述，绝大多数问题只需要一个隐藏层；从数学上讲，没有经过验证的理由说需要超过两个隐藏层。因此，我们将坚持使用一个隐藏层并与之工作。
- en: 'There are a number of rules of thumb to determine the number of neurons contained
    in the hidden layer, but there is no hard-and-fast rule. The empirical way is
    your friend in this particular circumstance: test your network with different
    settings, and choose the one that fits best.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多经验法则可以用来确定隐藏层中包含的神经元数量，但没有一条是固定不变的。在这种情况下，经验方法是你的朋友：用不同的设置测试你的网络，并选择最适合的那个。
- en: 'These are some of the most common rules used when building an ANN:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在构建ANN时最常用的规则之一：
- en: The number of hidden neurons should be between the size of the input layer and
    the size of the output layer. If the difference between the input layer size and
    the output layer is large, it is my experience that a hidden layer size much closer
    to the output layer is preferable.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏神经元的数量应该在输入层和输出层的大小之间。如果输入层和输出层的大小差异很大，根据我的经验，隐藏层的大小更接近输出层是更可取的。
- en: For relatively small input layers, the number of hidden neurons is two-thirds
    the size of the input layer, plus the size of the output layer, or less than twice
    the size of the input layer.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于相对较小的输入层，隐藏神经元的数量是输入层大小的三分之二，加上输出层的大小，或者小于输入层大小的两倍。
- en: One very important factor to keep in mind is **overfitting**. Overfitting occurs
    when there's such an inordinate amount of information contained in the hidden
    layer (for example, a disproportionate amount of neurons in the layer) compared
    to the information provided by the training data that classification is not very
    meaningful.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 需要牢记的一个重要因素是**过拟合**。当隐藏层中包含的信息量与训练数据提供的信息量不成比例（例如，层中神经元的数量过多）时，就会发生过拟合，这种分类并不很有意义。
- en: The larger the hidden layer, the more training information is required for the
    network to be trained properly. And, needless to say, this is going to lengthen
    the time required by the network to properly train.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层越大，网络训练所需的训练信息就越多。不用说，这将延长网络正确训练所需的时间。
- en: So, following the second rules of thumb illustrated earlier, our network will
    have a hidden layer of size 8, just because after a few runs of the network, I
    found it to yield the best results. As a side note, the empirical approach is
    very much encouraged in the world of ANNs. The best network topology is related
    to the type of data fed to the network, so don't refrain from testing ANNs in
    a trial-and-error fashion.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据前面提到的第二个经验法则，我们的网络将有一个大小为8的隐藏层，因为经过几次网络的运行，我发现它能够产生最佳结果。顺便提一下，在ANNs的世界中，经验方法非常受鼓励。最佳的网络拓扑结构取决于网络接收到的数据类型，因此不要犹豫，尝试以试错的方式测试ANNs。
- en: 'In summary, our network has the following sizes:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的网络具有以下大小：
- en: '**Input**: `3`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**: `3`'
- en: '**Hidden**: `8`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**: `8`'
- en: '**Output**: `4`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**: `4`'
- en: The learning algorithms
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习算法
- en: 'There are a number of learning algorithms used by ANNs, but we can identify
    three major ones:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs使用了多种学习算法，但我们可以识别出三种主要的学习算法：
- en: '**Supervised learning**: With this algorithm, we want to obtain a function
    from the ANN, which describes the data we labeled. We know, a priori, the nature
    of this data, and we delegate to the ANN the process of finding a function that
    describes the data.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**: 使用此算法，我们希望从ANN中获得一个函数，该函数描述了我们标记的数据。我们知道，事先我们知道数据的性质，并将找到描述数据的函数的过程委托给ANN。'
- en: '**Unsupervised learning**: This algorithm differs from supervised learning;
    in this, the data is unlabeled. This implies that we don''t have to select and
    label data, but it also means the ANN has a lot more work to do. The classification
    of the data is usually obtained through techniques such as (but not only) clustering,
    which we explored in [Chapter 7](part0049.xhtml#aid-1ENBI2 "Chapter 7. Detecting
    and Recognizing Objects"), *Detecting and Recognizing Objects*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**: 此算法与监督学习不同；在这种情况下，数据是无标签的。这意味着我们不必选择和标记数据，但也意味着ANN有更多的工作要做。数据的分类通常通过（但不限于）聚类等技术获得，我们在[第7章](part0049.xhtml#aid-1ENBI2
    "第7章。检测和识别对象")中探讨了这些技术，即*检测和识别对象*。'
- en: '**Reinforcement learning**: Reinforcement learning is a little more complex.
    A system receives an input; a decision-making mechanism determines an action,
    which is performed and scored (success/failure and grades in between); and finally
    the input and the action are paired with their score, so the system learns to
    repeat or change the action to be performed for a certain input or state.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**: 强化学习稍微复杂一些。系统接收一个输入；决策机制确定一个动作，执行并评分（成功/失败以及介于两者之间的等级）；最后，输入和动作与它们的评分配对，因此系统学会重复或改变执行特定输入或状态的行动。'
- en: Now that we have a general idea of what ANNs are, let's see how OpenCV implements
    them, and how to put them to good use. Finally, we'll work our way up to a full
    blown application, in which we will attempt to recognize handwritten digits.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对ANNs有一个大致的了解，让我们看看OpenCV如何实现它们，以及如何有效地使用它们。最后，我们将逐步过渡到一个完整的应用程序，我们将尝试识别手写数字。
- en: ANNs in OpenCV
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenCV中的ANNs
- en: Unsurprisingly, ANNs reside in the `ml` module of OpenCV.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，ANNs位于OpenCV的`ml`模块中。
- en: 'Let''s examine a dummy example, as a gentle introduction to ANNs:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来考察ANNs，作为一个温和的介绍：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we create an ANN:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个ANN：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may wonder about the `MLP` acronym in the function name; it stands for **multilayer
    perceptron**. By now, you should know what a perceptron is.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对函数名中的`MLP`这个缩写感到好奇；它代表**多层感知器**。到现在为止，你应该知道感知器是什么了。
- en: 'After creating the network, we need to set its topology:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建网络之后，我们需要设置其拓扑：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The layer sizes are defined by the NumPy array that is passed into the `setLayerSizes`
    method. The first element sets the size of the input layer, the last element sets
    the size of the output layer, and all intermediary elements define the size of
    the hidden layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 层的大小由传递给`setLayerSizes`方法的NumPy数组定义。第一个元素设置输入层的大小，最后一个元素设置输出层的大小，所有中间元素定义隐藏层的大小。
- en: 'We then set the train method to be backpropagation. There are two choices here:
    `BACKPROP` and `RPROP`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将训练方法设置为反向传播。这里有两个选择：`BACKPROP`和`RPROP`。
- en: Both `BACKPROP` and `RPROP` are backpropagation algorithms—in simple terms,
    algorithms that have an effect on weights based on errors in classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`BACKPROP`和`RPROP`都是反向传播算法——简单来说，这些算法基于分类中的误差对权重产生影响。'
- en: 'These two algorithms work in the context of supervised learning, which is what
    we are using in the example. How can we tell this particular detail? Look at the
    next statement:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个算法在监督学习环境中工作，这正是我们在示例中使用的内容。我们如何知道这个特定细节？看看下一个语句：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should notice a number of details. The method looks extremely similar to
    the `train()` method of support vector machine. The method contains three parameters:
    `samples`, `layout`, and `responses`. Only `samples` is the required parameter;
    the other two are optional.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到许多细节。这个方法看起来与支持向量机的`train()`方法极其相似。该方法包含三个参数：`samples`、`layout`和`responses`。只有`samples`是必需参数；其他两个是可选的。
- en: 'This reveals the following information:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这揭示了以下信息：
- en: First, ANN, like SVM, is an OpenCV `StatModel` (**statistical model**); `train`
    and `predict` are the methods inherited from the base `StatModel` class.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，ANN，像SVM一样，是一个OpenCV `StatModel`（**统计模型**）；`train`和`predict`是从基类`StatModel`继承的方法。
- en: Second, a statistical model trained with only `samples` is adopting an unsupervised
    learning algorithm. If we provide `layout` and `responses`, we're in a supervised
    learning context.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，仅使用`samples`训练的统计模型采用无监督学习算法。如果我们提供`layout`和`responses`，我们处于监督学习环境中。
- en: As we're using ANNs, we can specify the type of back propagation algorithm we're
    going to use (`BACKPROP` or `RPROP`), because—as we said—we're in a supervised
    learning environment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是人工神经网络（ANNs），我们可以指定将要使用的反向传播算法类型（`BACKPROP`或`RPROP`），因为——正如我们所说的——我们处于一个监督学习环境中。
- en: So what is back propagation? Back propagation is a two-phase algorithm that
    calculates the error of predictions and updates in both directions of the network
    (the input and output layers); it then updates the neuron weights accordingly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是反向传播？反向传播是一个两阶段算法，它计算预测和更新的误差，并更新网络两个方向（输入层和输出层）的权重；然后相应地更新神经元权重。
- en: 'Let''s train the ANN; as we specified an input layer of size 9, we need to
    provide 9 inputs, and 9 outputs to reflect the size of the output layer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练ANN；由于我们指定了大小为9的输入层，我们需要提供9个输入，以及9个输出以反映输出层的大小：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The structure of the response is simply an array of zeros, with a `1` value
    in the position indicating the class we want to associate the input with. In our
    preceding example, we indicated that the specified input array corresponds to
    class 5 (classes are zero-indexed) of classes 0 to 8.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的结构只是一个全零的数组，在表示我们想要将输入与之关联的类别的位置上有一个`1`值。在我们的前一个例子中，我们指出指定的输入数组对应于类别5（类别从0开始索引）的0到8类别。
- en: 'Lastly, we perform classification:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进行分类：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will yield the following result:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This means that the provided input was classified as belonging to class 5\.
    This is only a dummy example and the classification is pretty meaningless; however,
    the network behaved correctly. In this code, we only provided one training record
    for class 5, so the network classified a new input as belonging to class 5.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着提供的输入被分类为属于类别5。这只是一个示例，分类几乎没有意义；然而，网络的行为是正确的。在这段代码中，我们只为类别5提供了一个训练记录，因此网络将新的输入分类为属于类别5。
- en: 'As you may have guessed, the output of a prediction is a tuple, with the first
    value being the class and the second being an array containing the probabilities
    for each class. The predicted class will have the highest value. Let''s move on
    to a slightly more useful example: animal classification.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，预测的输出是一个元组，第一个值是类别，第二个值是一个包含每个类别概率的数组。预测的类别将具有最高值。让我们继续到一个更有用的例子：动物分类。
- en: ANN-imal classification
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN-imal分类
- en: 'Picking up from where we left off, let''s illustrate a very simple example
    of an ANN that attempts to classify animals based on their statistics (weight,
    length, and teeth). My intent is to describe a mock real-life scenario to improve
    our understanding of ANNs before we start applying it to computer vision and,
    specifically, OpenCV:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们上次停止的地方继续，让我们展示一个ANN的非常简单的例子，该ANN试图根据动物的统计数据（重量、长度和牙齿）进行分类。我的意图是描述一个模拟的真实场景，以便在我们开始将其应用于计算机视觉和，特别是OpenCV之前，提高我们对ANN的理解：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There are a good few differences between this example and the dummy example,
    so let's examine them in order.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个例子和虚拟例子之间有很多不同之处，所以让我们按顺序检查它们。
- en: 'First, the usual imports. Then, we import `randint`, just because we want to
    generate some relatively random data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入一些常用的库。然后，我们导入`randint`，因为我们想生成一些相对随机的数据：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we create the ANN. This time, we specify the `train` method to be resilient
    back propagation (an improved version of back propagation) and the activation
    function to be a sigmoid function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建ANN。这次，我们指定`train`方法为鲁棒的逆向传播（逆向传播的改进版本）和激活函数为Sigmoid函数：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, we specify the termination criteria similarly to the way we did in the
    CAMShift algorithm in the previous chapter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们指定终止条件与上一章中我们在CAMShift算法中所做的方式类似：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we need some data. We''re not really so much interested in representing
    animals accurately, as requiring a bunch of records to be used as training data.
    So we basically define four sample creation functions and four classification
    functions that will help us train the network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一些数据。我们并不真的那么关心准确表示动物，而是需要大量记录作为训练数据。因此，我们基本上定义了四个样本创建函数和四个分类函数，这些函数将帮助我们训练网络：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s proceed with the creation of our fake animal data; we''ll create 5,000
    samples per class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续创建我们的假动物数据；我们将为每个类别创建5,000个样本：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the end, we print the results that yield the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出以下代码的结果：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'From these results, we deduce the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中，我们得出以下结论：
- en: The network got two out of three samples correct, which is not perfect but serves
    as a good example to illustrate the importance of all the elements involved in
    building and training an ANN. The size of the input layer is very important to
    create diversification between the different classes. In our case, we only had
    three statistics and there is a relative overlapping in features.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络正确分类了三个样本中的两个，这并不完美，但作为一个很好的例子来说明构建和训练ANN所涉及的所有元素的重要性。输入层的大小对于在各个类别之间创建多样性非常重要。在我们的例子中，我们只有三个统计数据，并且特征之间存在相对重叠。
- en: 'The size of the hidden layer needs to be tested. You will find that increasing
    neurons may improve accuracy to a point, and then it will overfit, unless you
    start compensating with enormous amounts of data: the number of training records.
    Definitely, avoid having too few records or feeding a lot of identical records
    as the ANN won''t learn much from them.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的大小需要测试。你会发现增加神经元可能会在一定点提高准确性，然后它会过拟合，除非你开始用大量的数据来补偿：训练记录的数量。肯定要避免记录太少或提供大量相同的记录，因为ANN不会从它们中学到很多东西。
- en: Training epochs
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练轮次
- en: Another important concept in training ANNs is the idea of epochs. A training
    epoch is an iteration through the training data, after which the data is tested
    for classification. Most ANNs train over several epochs; you'll find that some
    of the most common examples of ANNs, classifying handwritten digits, will have
    the training data iterated several hundred times.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练ANN时，另一个重要的概念是轮次的概念。训练轮次是对训练数据的迭代，之后数据将用于分类测试。大多数ANN会在多个轮次中进行训练；你会发现一些最常用的ANN例子，如手写数字分类，其训练数据会迭代数百次。
- en: I personally suggest you spend a lot of time playing with ANNs and the number
    of epochs, until you reach convergence, which means that further iterations will
    no longer improve (at least not noticeably) the accuracy of the results.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人建议你花很多时间玩ANN和时代，直到你达到收敛，这意味着进一步的迭代将不再提高（至少不是明显地）结果的准确性。
- en: 'The preceding example can be modified as follows to leverage epochs:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例可以通过以下方式修改以利用时代：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, do some tests, starting with the `dog` class:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从`狗`类别开始做一些测试：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Repeat over all classes and output the results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有类别重复操作并输出结果：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we obtain the following results:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到以下结果：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Consider the fact that we're only playing with toy/fake data and the size of
    training data/training iterations; this teaches us quite a lot. We can diagnose
    the ANN as overfitting towards certain classes, so it's important to improve the
    quality of the data you feed into the training process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们只是在玩玩具/假数据，以及训练数据/训练迭代的规模；这教会了我们很多。我们可以诊断ANN对某些类别的过度拟合，因此提高你输入训练过程的数据质量非常重要。
- en: 'All that said, time for a real-life example: handwritten digit recognition.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，现在是时候举一个现实生活中的例子了：手写数字识别。
- en: Handwritten digit recognition with ANNs
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ANN进行手写数字识别
- en: The world of Machine Learning is vast and mostly unexplored, and ANNs are but
    one of the many concepts related to Machine Learning, which is one of the many
    subdisciplines of Artificial Intelligence. For the purpose of this chapter, we
    will only be exploring the concept of ANNs in the context of OpenCV. It is by
    no means an exhaustive treatise on the subject of Artificial Intelligence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的世界广阔而大部分未被探索，ANN只是与机器学习相关联的许多概念之一，而机器学习是人工智能的许多子学科之一。为了本章的目的，我们只将在OpenCV的背景下探索ANN的概念。这绝对不是关于人工智能主题的详尽论述。
- en: Ultimately, we're interested in seeing ANNs work in the real world. So let's
    go ahead and make it happen.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们感兴趣的是看到ANN在现实世界中的工作情况。所以，让我们继续前进，让它发生。
- en: MNIST – the handwritten digit database
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST – 手写数字数据库
- en: One of the most popular resources on the Web for the training of classifiers
    dealing with OCR and handwritten character recognition is the MNIST database,
    publicly available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在Web上，用于训练处理OCR和手写字符识别的分类器的最流行资源之一是MNIST数据库，可在[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)公开获取。
- en: This particular database is a freely available resource to kick-start the creation
    of a program that utilizes ANNs to recognize handwritten digits.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的数据库是一个免费资源，可以启动创建一个利用ANN识别手写数字的程序。
- en: Customized training data
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定制化训练数据
- en: It is always possible to build your own training data. It will take a little
    bit of patience but it's fairly easy; collect a vast number of handwritten digits
    and create images containing a single digit, making sure all the images are the
    same size and in grayscale.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总是可能构建自己的训练数据。这需要一点耐心，但相当容易；收集大量手写数字，并创建包含单个数字的图像，确保所有图像大小相同且为灰度图。
- en: After this, you will have to create a mechanism that keeps a training sample
    in sync with the expected classification.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你需要创建一个机制，以确保训练样本与预期的分类保持同步。
- en: The initial parameters
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始参数
- en: 'Let''s take a look at the individual layers in the network:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看网络中的各个单独层：
- en: Input layer
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: The input layer
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入层
- en: 'Since we''re going to utilize the MNIST database, the input layer will have
    a size of 784 input nodes: that''s because MNIST samples are 28x28 pixel images,
    which means 784 pixels.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将利用MNIST数据库，输入层将有784个输入节点：这是因为MNIST样本是28x28像素的图像，这意味着784个像素。
- en: The hidden layer
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层
- en: As we have seen, there's no hard-and-fast rule for the size of the hidden layer,
    I've found—through several attempts—that 50 to 60 nodes yields the best result
    while not necessitating an inordinate amount of training data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，隐藏层的大小并没有一个固定的规则，我发现——通过多次尝试——50到60个节点可以得到最佳结果，同时不需要大量的训练数据。
- en: You can increase the size of the hidden layer with the amount of data, but beyond
    a certain point, there will be no advantage to that; you will also have to be
    prepared for your network to take hours to train (the more hidden neurons, the
    longer it takes to train the network).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The output layer
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output layer will have a size of 10\. This should not be a surprise as we
    want to classify 10 digits (0-9).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Training epochs
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will initially use the entire set of the `train` data from MNIST, which consists
    of over 60,000 handwritten images, half of which were written by US government
    employees, and the other half by high-school students. That's a lot of data, so
    we won't need more than one epoch to achieve an acceptably high accuracy on detection.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: From there on, it is up to you to train the network iteratively on the same
    `train` data, and my suggestion is that you use an accuracy test, and find the
    epoch at which the accuracy "peaks". By doing so, you will have a precise measurement
    of the highest possible accuracy achieved by your network given its current configuration.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Other parameters
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a sigmoid activation function, **Resilient Back Propagation** (**RPROP**),
    and extend the termination criteria for each calculation to 20 iterations instead
    of 10, like we did for every other operation in this book that involved `cv2.TermCriteria`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Important notes on train data and ANNs libraries**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Internet for sources, I found an amazing article by Michael Nielsen
    at [http://neuralnetworksanddeeplearning.com/chap1.html](http://neuralnetworksanddeeplearning.com/chap1.html),
    which illustrates how to write an ANN library from scratch, and the code for this
    library is freely available on GitHub at [https://github.com/mnielsen/neural-networks-and-deep-learning](https://github.com/mnielsen/neural-networks-and-deep-learning);
    this is the source code for a book, *Neural Networks and Deep Learning*, by Michael
    Nielsen.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: In the `data` folder, you will find a `pickle` file, signifying data that has
    been saved to disk through the popular Python library, `cPickle`, which makes
    loading and saving the Python data a trivial task.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This pickle file is a `cPickle` library-serialized version of the MNIST data
    and, as it is so useful and ready to work with, I strongly suggest you use that.
    Nothing stops you from loading the MNIST dataset but the process of deserializing
    the training data is quite tedious and—strictly speaking—outside the remit of
    this book.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Second, I would like to point out that OpenCV is not the only Python library
    that allows you to use ANNs, not by any stretch of the imagination. The Web is
    full of alternatives that I strongly encourage you to try out, most notably **PyBrain**,
    a library called **Lasagna** (which—as an Italian—I find exceptionally attractive)
    and many custom-written implementations, such as the aforementioned Michael Nielsen's
    implementation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Enough introductory details, though. Let's get going.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Mini-libraries
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up an ANN in OpenCV is not difficult, but you will almost definitely
    find yourself training your network countless times, in search of that elusive
    percentage point that boosts the accuracy of your results.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中设置ANN并不困难，但你几乎肯定会发现自己无数次地训练网络，以寻找那个难以捉摸的百分比点，以提升你结果的准确性。
- en: To automate this as much as possible, we will build a mini-library that wraps
    the OpenCV's native implementation of ANNs and lets us rerun and retrain the network
    easily.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能自动化这个过程，我们将构建一个迷你库，它封装了OpenCV的ANN原生实现，并允许我们轻松地重新运行和重新训练网络。
- en: 'Here''s an example of a wrapper library:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个包装库的示例：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let's examine it in order. First, the `load_data`, `wrap_data`, and `vectorized_result`
    functions are included in Michael Nielsen's code for loading the `pickle` file.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按顺序检查它。首先，`load_data`、`wrap_data`和`vectorized_result`函数包含在Michael Nielsen的加载`pickle`文件的代码中。
- en: 'It''s a relatively straightforward loading of a `pickle` file. Most notably,
    though, the loaded data has been split into the `train` and `test` data. Both
    `train` and `test` data are arrays containing two-element tuples: the first one
    is the data itself; the second one is the expected classification. So we can use
    the `train` data to train the ANN and the `test` data to evaluate its accuracy.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对简单的`pickle`文件加载过程。不过，值得注意的是，加载的数据已经被分割成`train`和`test`数据。`train`和`test`数据都是包含两个元素的元组数组：第一个是数据本身；第二个是预期的分类。因此，我们可以使用`train`数据来训练ANN，使用`test`数据来评估其准确性。
- en: The `vectorized_result` function is a very clever function that—given an expected
    classification—creates a 10-element array of zeros, setting a single 1 for the
    expected result. This 10-element array, you may have guessed, will be used as
    a classification for the output layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`vectorized_result`函数是一个非常巧妙的函数，它给定一个预期的分类，创建一个包含10个零的数组，并将预期的结果设置为单个1。你可能已经猜到了，这个10个元素的数组将被用作输出层的分类。'
- en: 'The first ANN-related function is `create_ANN`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个与ANN相关的函数是`create_ANN`：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This function creates an ANN specifically geared towards handwritten digit recognition
    with MNIST, by specifying layer sizes as illustrated in the *Initial parameters*
    section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数通过指定如*初始参数*部分所示的大小，创建了一个专门针对MNIST手写数字识别的ANN。
- en: 'We now need a training function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要一个训练函数：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Again, this is quite simple: given a number of samples and training epochs,
    we load the data, and then iterate through the samples an x-number-of-epochs times.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这很简单：给定样本数量和训练轮数，我们加载数据，然后迭代样本x次轮数。
- en: The important section of this function is the deconstruction of the single training
    record into the `train` data and an expected classification, which is then passed
    into the ANN.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的重要部分是将单个训练记录分解成`train`数据和预期的分类，然后将其传递给ANN。
- en: 'To do so, we utilize the `numpy` array function, `ravel()`, which takes an
    array of any shape and "flattens" it into a single-row array. So, for example,
    consider this array:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们利用`numpy`数组的`ravel()`函数，它可以将任何形状的数组“展平”成一个单行数组。例如，考虑以下数组：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding array once "raveled", becomes the following array:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数组一旦“展平”，就变成了以下数组：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is the format that OpenCV's ANN expects data to look like in its `train()`
    method.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是OpenCV的ANN在`train()`方法中期望的数据格式。
- en: Finally, we return both the `network` and `test` data. We could have just returned
    the data, but having the `test` data at hand for accuracy checking is quite useful.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回了`network`和`test`数据。我们本来可以直接返回数据，但手头有`test`数据用于准确性检查是非常有用的。
- en: 'The last function we need is a `predict()` function to wrap ANN''s own `predict()`
    method:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的最后一个函数是`predict()`函数，用于封装ANN的`predict()`方法：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function takes an ANN and a sample image; it operates a minimum of "sanitization"
    by making sure the shape of the data is as expected and resizing it if it's not,
    and then raveling it for a successful prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个ANN和一个样本图像；它通过确保数据的形状符合预期并在必要时调整大小，然后展平它以进行成功的预测，进行最少的“净化”操作。
- en: The file I created also contains a `test` function to verify that the network
    works and it displays the sample provided for classification.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建的文件还包含一个`test`函数来验证网络是否工作，并显示用于分类的样本。
- en: The main file
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要文件
- en: This whole chapter has been an introductory journey leading us to this point.
    In fact, many of the techniques we're going to use are from previous chapters,
    so in a way the entire book has led us to this point. So let's put all our knowledge
    to good use.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 整个这一章都是一个引导性的旅程，引导我们到达这个点。实际上，我们将要使用的大多数技术都来自前面的章节，所以从某种意义上说，整本书都引导我们到达这个点。因此，让我们充分利用我们的知识。
- en: 'Let''s take an initial look at the file, and then decompose it for a better
    understanding:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看一下文件，然后对其进行分解以便更好地理解：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After the initial usual imports, we import the mini-library we created, which
    is stored in `digits_ann.py`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行初始的常规导入之后，我们导入我们创建的小型库，该库存储在`digits_ann.py`中。
- en: 'I find it good practice to define functions at the top of the file, so let''s
    examine those. The `inside()` function determines whether a rectangle is entirely
    contained in another rectangle:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现将函数定义在文件顶部是一个好的实践，让我们来检查一下这些函数。`inside()`函数用于判断一个矩形是否完全包含在另一个矩形内：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `wrap_digit()` function takes a rectangle that surrounds a digit, turns
    it into a square, and centers it on the digit itself, with 5-point padding to
    make sure the digit is entirely contained in it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`wrap_digit()`函数接受一个围绕数字的矩形，将其转换为方形，并将其中心对准数字本身，同时添加5点填充以确保数字完全包含在内：'
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The point of this function will become clearer later on; let's not dwell on
    it too much at the moment.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的目的将在稍后变得清晰；现在我们不要过多地关注它。
- en: 'Now, let''s create the network. We will use 58 hidden nodes, and train over
    20,000 samples:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建网络。我们将使用58个隐藏节点，并在20,000个样本上进行训练：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is good enough for a preliminary test to keep the training time down to
    a minute or two (depending on the processing power of your machine). The ideal
    is to use the full set of training data (50,000), and iterate through it several
    times, until some convergence is reached (as we discussed earlier, the accuracy
    "peak"). You would do this by calling the following function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于初步测试来说已经足够好了，可以将训练时间缩短到一分钟或两分钟（取决于你机器的处理能力）。理想的情况是使用完整的训练数据集（50,000个），并多次迭代，直到达到某种收敛（正如我们之前讨论的，准确性的“峰值”）。你可以通过调用以下函数来完成这项工作：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can now prepare the data to test. To do that, we''re going to load an image,
    and clean up a little:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以准备数据以进行测试。为此，我们将加载一张图片，并进行一些清理：
- en: '[PRE29]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now that we have a grayscale smoothed image, we can apply a threshold and some
    morphology operations to make sure the numbers are properly standing out from
    the background and relatively cleaned up for irregularities, which might throw
    the prediction operation off:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个灰度平滑的图像，我们可以应用阈值和一些形态学操作，以确保数字能够从背景中正确突出，并且相对干净，这样就不会因为不规则性而影响预测操作：
- en: '[PRE30]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note the threshold flag, which is for an inverse binary threshold: as the samples
    of the MNIST database are white on black (and not black on white), we turn the
    image into a black background with white numbers.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意阈值标志，这是用于逆二值阈值的：由于MNIST数据库的样本是白色背景上的黑色数字（而不是白色背景上的黑色），我们将图像转换为黑色背景上的白色数字。
- en: 'After the morphology operation, we need to identify and separate each number
    in the picture. To do this, we first identify the contours in the image:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在形态学操作之后，我们需要识别并分离图片中的每个数字。为此，我们首先识别图像中的轮廓：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we iterate through the contours, and discard all the rectangles that
    are entirely contained in other rectangles; we only append to the list of good
    rectangles the ones that are not contained in other rectangles and are also not
    as wide as the image itself. In some of the tests, `findContours` yielded the
    entire image as a contour itself, which meant no other rectangle passed the `inside`
    test:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历轮廓，并丢弃所有完全包含在其他矩形内的矩形；我们只将那些既不包含在其他矩形内，也不像图像本身那么宽的矩形添加到良好的矩形列表中。在一些测试中，`findContours`返回整个图像作为一个轮廓本身，这意味着没有其他矩形通过了`inside`测试：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that we have a list of good rectangles, we can iterate through them and
    define a region of interest for each of the rectangles we identified:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了良好的矩形列表，我们可以遍历它们，并为每个我们识别的矩形定义一个感兴趣的区域：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is where the `wrap_digit()` function we defined at the beginning of the
    program comes into play: we need to pass a square region of interest to the predictor
    function; if we simply resized a rectangle into a square, we''d ruin our test
    data.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在程序开头定义的`wrap_digit()`函数发挥作用的地方：我们需要将一个感兴趣的区域传递给预测函数；如果我们简单地将一个矩形调整成方形，就会破坏我们的测试数据。
- en: You may wonder why think of the number one. A rectangle surrounding the number
    one would be very narrow, especially if it has been drawn without too much of
    a lean to either side. If you simply resized it to a square, you would "fatten"
    the number one in such a way that nearly the entire square would turn black, rendering
    the prediction impossible. Instead, we want to create a square around the identified
    number, which is exactly what `wrap_digit()` does.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is quick-and-dirty; it allows us to draw a square around a number
    and simultaneously pass that square as a region of interest for the prediction.
    A purer approach would be to take the original rectangle and "center" it into
    a square `numpy` array with rows and columns equal to the larger of the two dimensions
    of the original rectangle. The reason for this is you will notice that some of
    the square will include tiny bits of adjacent numbers, which can throw the prediction
    off. With a square created from a `np.zeros()` function, no impurities will be
    accidentally dragged in:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once the prediction for the square region is complete, we draw it on the original
    image:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And that''s it! The final result will look similar to this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![The main file](img/image00253.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Possible improvements and potential applications
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have illustrated how to build an ANN, feed it training data, and use it for
    classification. There are a number of aspects we can improve, depending on the
    task at hand, and a number of potential applications of our new-found knowledge.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Improvements
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of improvements that can be applied to this approach, some
    of which we have already discussed:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could enlarge your dataset and iterate more times, until a
    performance peak is reached
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could also experiment with the several activation functions (`cv2.ml.ANN_MLP_SIGMOID_SYM`
    is not the only one; there is also `cv2.ml.ANN_MLP_IDENTITY` and `cv2.ml.ANN_MLP_GAUSSIAN`)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could utilize different training flags (`cv2.ml.ANN_MLP_UPDATE_WEIGHTS`,
    `cv2.ml.ANN_MLP_NO_INPUT_SCALE`, `cv2.ml.ANN_MLP_NO_OUTPUT_SCALE`), and training
    methods (back propagation or resilient back propagation)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aside from that, bear in mind one of the mantras of software development: there
    is no single best technology, there is only the best tool for the job at hand.
    So, careful analysis of the application requirements will lead you to the best
    choices of parameters. For example, not everyone draws digits the same way. In
    fact, you will even find that some countries draw numbers in a slightly different
    way.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database was compiled in the US, in which the number seven is drawn
    like the character 7\. But you will find that the number 7 in Europe is often
    drawn with a small horizontal line half way through the diagonal portion of the
    number, which was introduced to distinguish it from the number 1.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a more detailed overview of regional handwriting variations, check the Wikipedia
    article on the subject, which is a good introduction, available at [https://en.wikipedia.org/wiki/Regional_handwriting_variation](https://en.wikipedia.org/wiki/Regional_handwriting_variation).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要更详细地了解区域手写变体，请查看维基百科上的相关文章，它是一个很好的介绍，可在[https://en.wikipedia.org/wiki/Regional_handwriting_variation](https://en.wikipedia.org/wiki/Regional_handwriting_variation)找到。
- en: This means the MNIST database has limited accuracy when applied to European
    handwriting; some numbers will be classified more accurately than others. So you
    may end up creating your own dataset. In almost all circumstances, it is preferable
    to utilize the `train` data that's relevant and belongs to the current application
    domain.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着MNIST数据库在应用于欧洲手写时准确性有限；一些数字的分类会比其他数字更准确。因此，你可能需要创建自己的数据集。在几乎所有情况下，利用与当前应用领域相关且属于该领域的`train`数据都是更可取的。
- en: Finally, remember that once you're happy with the accuracy of your network,
    you can always save it and reload it later, so it can be utilized in third-party
    applications without having to train the ANN every time.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，记住一旦你对网络的准确性满意，你总是可以保存它并在以后重新加载，这样它就可以在不每次都训练神经网络的情况下用于第三方应用。
- en: Potential applications
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在应用
- en: The preceding program is only the foundation of a handwriting recognition application.
    Straightaway, you can quickly extend the earlier approach to videos and detect
    handwritten digits in real-time, or you could train your ANN to recognize the
    entire alphabet for a full-blown OCR system.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 上述程序只是一个手写识别应用的基石。立即，你可以快速扩展之前的方法到视频，并实时检测手写数字，或者你可以训练你的神经网络来识别整个字母表，以构建一个完整的OCR系统。
- en: Car registration plate detection seems like an obvious extension of the lessons
    learned to this point, and it should be an even easier domain to work with, as
    registration plates use consistent characters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 车牌识别似乎是将所学知识扩展到这一点的明显延伸，并且它应该是一个更容易处理的领域，因为车牌使用的是一致的字符。
- en: Also, for your own edification or business purposes, you may try to build a
    classifier with ANNs and plain SVMs (with feature detectors such as SIFT) and
    see how they benchmark.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了您自己的教育或商业目的，您可以尝试使用神经网络和普通的SVMs（带有特征检测器如SIFT）构建一个分类器，并看看它们的基准如何。
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we scratched the surface of the vast and fascinating world
    of ANNs, focusing on OpenCV's implementation of it. We learned about the structure
    of ANNs, and how to design a network topology based on application requirements.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仅仅触及了神经网络（ANNs）这个广阔而迷人的世界的表面，重点关注了OpenCV对其的实现。我们学习了神经网络的结构，以及如何根据应用需求设计网络拓扑。
- en: Finally, we utilized various concepts that we explored in the previous chapters
    to build a handwritten digit recognition application.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们利用了在前几章中探讨的各种概念来构建一个手写数字识别应用。
- en: To boldly go…
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 勇敢地前行…
- en: I hope you enjoyed the journey through the Python bindings for OpenCV 3\. Although
    covering OpenCV 3 would take a series of books, we explored very fascinating and
    futuristic concepts, and I encourage you to get in touch and let me and the OpenCV
    community know what your next groundbreaking computer-vision-based project is!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您喜欢通过OpenCV 3的Python绑定进行旅行的过程。尽管涵盖OpenCV 3需要一系列书籍，但我们探索了非常有趣和未来派的概念，并鼓励您联系我，让OpenCV社区了解您下一个基于计算机视觉的突破性项目是什么！
