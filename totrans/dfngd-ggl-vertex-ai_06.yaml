- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Low-Code Options for Building ML Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BigQuery Machine Learning**, often abbreviated as **BQML**, is a tool offered
    as part of Google Cloud that seamlessly merges the worlds of data warehousing
    and ML. Designed to bridge the gap between data analysts and ML models, BQML empowers
    individuals to build, evaluate, and predict with ML models directly within the
    confines of BigQuery, without the need to move data or master a new toolset.'
  prefs: []
  type: TYPE_NORMAL
- en: This integration not only simplifies the process of model creation but also
    presents an intuitive transition for those familiar with SQL. With just a few
    statements, you can go from data analysis to predictive insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go over the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is BQML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using BQML for feature transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building ML models with BQML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing inference with BQML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is BQML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BQML is a powerful, built-in ML service provided by Google Cloud that allows
    users to create, train, and deploy ML models using familiar SQL queries. BQML
    is designed to simplify the process of building and deploying ML models for those
    who may not have a strong background in data science or programming. In this chapter,
    we will explore the key features and capabilities of BQML and how you can use
    it to leverage the power of Google Cloud AI for your projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'BQML provides a seamless way to integrate ML into your data analytics workflows
    without requiring a deep understanding of ML concepts or programming languages.
    With BQML, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create and train ML models using SQL queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make predictions using trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of your models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform feature transformation and hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand model explanations and weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export and import models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Utilizing BQML offers numerous advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: BQML eliminates the need to load data into local memory, thereby addressing
    the constraint posed by sizable datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BQML streamlines the ML process by handling standard tasks such as dividing
    data into train and test sets, selecting and adjusting learning rates, and choosing
    an optimization approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With automatic versioning of BQML models, tracking alterations and reverting
    to earlier versions when needed becomes effortless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When serving predictions, BQML models can be seamlessly integrated with Vertex
    AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also a few limitations to using BQML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited model types**: BQML supports a restricted set of ML models, such
    as linear regression, logistic regression, k-means clustering, matrix factorization,
    and others. It may not meet the requirements of projects that necessitate advanced
    or specialized models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizability**: BQML’s automated approach to ML means that there is limited
    scope for customization. Users might not be able to fine-tune models or experiment
    with different model architectures as they could with other ML frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Although BQML is designed for handling large datasets, it
    may not scale as effectively as other distributed ML frameworks when working with
    extremely large datasets or complex models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.PREDICT` function to make predictions based on images. BQML now also supports
    adding **remote models** as API endpoints, which opens up the possibility of adding
    any model hosted on Vertex AI endpoints or adding other cloud-based ML services
    such as Vision API to support additional use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: BQML might not be the best option for extensive feature
    engineering as it focuses more on simplifying the ML process. Users may need to
    perform feature engineering outside of BQML for advanced feature engineering tasks.
    We will discuss the limitations in more detail in the *Feature engineering* section
    of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External data sources**: BQML works primarily with Google BigQuery data,
    limiting its flexibility in terms of data sources. If you want to use data from
    different sources or formats, you might need to import it into BigQuery first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model portability**: BQML models are tightly integrated with Google Cloud.
    Exporting models for use outside of the Google ecosystem may be challenging and
    might require additional work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at how you can start using BigQuery for ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with BigQuery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google BigQuery is a serverless, fully managed data warehouse that enables
    super-fast SQL queries using the processing power of Google’s infrastructure.
    Since BigQuery is not part of Vertex AI, we won’t be covering the features of
    the tool in depth in this book, but here’s a quick guide on getting started. This
    should be enough information to help you follow along with the exercises later
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set up a Google Cloud project**: Before you can use BigQuery, you’ll need
    to set up a **Google Cloud Platform** (**GCP**) project. Head over to the Google
    Cloud console and create a new project. If you’ve never used GCP before, you might
    need to create an account and set up billing information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enable the BigQuery API**: Within your GCP project, navigate to the **API
    & Services** section and enable the BigQuery API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Access the BigQuery console**: Once the API has been enabled, you can access
    the BigQuery console either via the GCP dashboard or directly through the BigQuery
    console link ([https://console.cloud.google.com/bigquery](https://console.cloud.google.com/bigquery)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a dataset**: Datasets are containers for tables, views, and other
    data objects in BigQuery. To create one, click on the vertical ellipsis next to
    your GCP project name in the BigQuery console, select **Create Dataset**, and
    fill in the dataset’s name. Then, click **Create Dataset**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Load data**: BigQuery supports various data formats, including CSV, JSON,
    and others. You can load data into BigQuery from Google Cloud Storage, send data
    directly with an API request, or manually upload files. To load data, navigate
    to your dataset in the BigQuery console on the left, click **Create Table**, and
    follow the prompts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run SQL queries**: With data loaded into BigQuery, you can then run SQL queries.
    Use the query editor in the BigQuery console to start analyzing your data using
    SQL.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s look at how you can use BigQuery’s native functions to do large-scale
    feature/data transformations to prepare training data for the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Using BQML for feature transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two types of feature preprocessing are supported by BQML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic preprocessing**: During training, BQML carries out automatic preprocessing.
    For further details, please carries out automatic preprocessing like missing data
    imputation, one-hot encoding, and timestamp transformation and encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRANSFORM` clause provided by BQML to define customized preprocessing using
    manual preprocessing functions. These functions can also be utilized outside the
    `TRANSFORM` clause.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While BQML does support some feature engineering tasks, it has certain limitations
    compared to more flexible and feature-rich ML frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited preprocessing functions**: BQML provides a basic set of SQL functions
    for data preprocessing, such as scaling and encoding. However, it may lack some
    advanced preprocessing techniques or specialized functions available in other
    ML libraries such as **scikit-learn** or **TensorFlow**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No automated feature selection**: BQML does not offer automated feature selection
    methods to identify the most important variables in your dataset. You must manually
    select and engineer features based on your domain knowledge and intuition, or
    use external tools for feature selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex feature transformations**: BQML’s SQL-based approach may not be well
    suited for certain complex feature transformations that involve non-linear combinations,
    rolling windows, or sequential patterns in the data. In such cases, you may need
    to preprocess your data using other tools or programming languages before using
    BQML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom feature generation**: BQML lacks the flexibility to create custom
    features, such as domain-specific functions or transformations, as easily as you
    can with more versatile ML libraries. You might need to implement these custom
    features outside of BQML, which could be cumbersome and less efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering pipelines**: BQML does not provide a built-in mechanism
    to create and manage reusable feature engineering pipelines. In contrast, other
    ML frameworks offer functionality to build modular and maintainable pipelines,
    streamlining the process of applying the same transformations to training and
    validation datasets or during model deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While BQML simplifies the ML process, it may not be the best choice for projects
    that require extensive or advanced feature engineering. In such cases, you may
    need to preprocess your data using external tools or libraries and then import
    the transformed data into BigQuery for further analysis with BQML.
  prefs: []
  type: TYPE_NORMAL
- en: Manual preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BQML provides a variety of manual preprocessing functions that can be utilized
    with the `CREATE MODEL` syntax to preprocess your data before training. These
    functions can also be used outside the `TRANSFORM` clause. These preprocessing
    functions can be scalar, operating on a single row, or analytic, operating on
    all rows and outputting results based on statistics collected across all rows.
    When ML analytic functions are used inside the `TRANSFORM` clause during training,
    the same statistics are automatically applied to the input in prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists all the supported data preprocessing functions in
    BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.BUCKETIZE` | Bucketizes a numerical expression into user-defined categories
    based on provided split points |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.POLYNOMIAL_EXPAND` | Generates polynomial combinations of a given set
    of numerical features up to a specified degree |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.FEATURE_CROSS` | Generates feature crosses of categorical features up
    to a specified degree |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.NGRAMS` | Extracts n-grams from an array of tokens, based on a given
    range of *n* values |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.QUANTILE_BUCKETIZE` | Bucketizes a numerical expression into quantile-based
    categories based on several buckets |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.HASH_BUCKETIZE` | Bucketizes a string expression into a fixed number
    of hash-based buckets |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.MIN_MAX_SCALER` | Scales a numerical expression to the range [0, 1] capped
    with `MIN` and `MAX` across all rows |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.STANDARD_SCALER` | Standardizes a numerical expression |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.MAX_ABS_SCALER` | Scales a numerical expression to the range [-1, 1]
    by dividing through the largest maximum absolute value |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.ROBUST_SCALER` | Scales a numerical expression using statistics that
    are robust to outliers |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.NORMALIZER` | Normalizes an array expression to have a unit norm using
    the given p-norm |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.IMPUTER` | Replaces `NULL` in an expression using a specified value (for
    example, mean, median, or most frequent) |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.ONE_HOT_ENCODER` | Encodes a string expression using a one-hot encoding
    scheme |'
  prefs: []
  type: TYPE_TB
- en: '| `ML.LABEL_ENCODER` | Encodes a string expression to an `INT64` in `[``0,
    n_categories]` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Data transformation functions
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of all the functions with their inputs and outputs, as well
    as an example for each:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ML.BUCKETIZE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucketizes a numerical expression into user-defined categories based on provided
    split points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`numerical_expression`: Numerical expression to bucketize.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`array_split_points`: Sorted numerical array with split points.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_boundaries` (optional): If `TRUE`, the two boundaries are removed
    from `array_split_points`. The default value is `FALSE`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`STRING` is the name of the buckets into which the `numerical_expression` field
    is split.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Submitting this query in BigQuery should generate the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.FEATURE_CROSS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This generates feature crosses of categorical features up to a specified degree:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Submitting this query in BigQuery should generate the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **animal** | **color** | **animal_color** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| dog | brown | dog_brown |'
  prefs: []
  type: TYPE_TB
- en: '| cat | black | cat_black |'
  prefs: []
  type: TYPE_TB
- en: '| bird | yellow | bird_yellow |'
  prefs: []
  type: TYPE_TB
- en: '| fish | orange | fish_orange |'
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – BigQuery Query Output
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `ML.FEATURE_CROSS` function can be used to create a cross of multiple
    columns if you include more columns in the `ARRAY` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '`ML.NGRAMS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This extracts n-grams from an array of tokens, based on a given range of *n*
    values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`array_input`: `ARRAY` of `STRING`. The strings are the tokens to be merged.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`range`: `ARRAY` of two `INT64` elements or a single `INT64`. These two sorted
    `INT64` elements in the `ARRAY` input are the range of n-gram sizes to return.
    A single `INT64` is equivalent to the range of [x, x].'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`separator`: Optional `STRING`. `separator` connects two adjacent tokens in
    the output. The default value is whitespace.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: `ARRAY` of `STRING`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Submitting this query in BigQuery should generate the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **fruit** | **fruit2** | **fruit3** | **fruit_ngrams** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| apple | cherry | pear | [apple cherry, cherry pear] |'
  prefs: []
  type: TYPE_TB
- en: '| banana | banana | melon | [banana banana, banana melon] |'
  prefs: []
  type: TYPE_TB
- en: '| cherry | cherry | pineapple | [cherry cherry, cherry pineapple] |'
  prefs: []
  type: TYPE_TB
- en: Table 6.3 – Output from BigQuery
  prefs: []
  type: TYPE_NORMAL
- en: '`ML.QUANTILE_BUCKETIZE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This bucketizes a numerical expression into quantile-based categories based
    on several buckets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`numerical_expression`: Numerical expression to bucketize'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_buckets`: `INT64`. The number of buckets to split `numerical_expression`
    into'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: `STRING`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with five rows of age data.
    Then, we use the `ML.QUANTILE_BUCKETIZE` function to bucketize the `age` column
    into four quantile buckets. The resulting `age_bucket` column shows which quantile
    bucket each row of the dataset belongs to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.HASH_BUCKETIZE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This bucketizes a string expression into a fixed number of hash-based buckets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`string_expression`: `STRING`. The string expression to bucketize.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hash_bucket_size`: `INT64`. The number of buckets. Expected `hash_bucket_size
    >= 0`. If `hash_bucket_size = 0`, the function only hashes the string without
    bucketizing the hashed value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: `INT64`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with four rows of data. Then,
    we use the `ML.HASH_BUCKETIZE` function to hash the `animal` column into two buckets.
    The resulting `animal_bucket` column shows which hash bucket each row of the dataset
    belongs to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the `ML.HASH_BUCKETIZE` function can be used to hash the values of
    a column into a different number of buckets by specifying a different value for
    the second argument.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with four rows of data. Then,
    we use the `ML.HASH_BUCKETIZE` function to hash the animal column into two buckets.
    The resulting `animal_bucket` column shows which hash bucket each row of the dataset
    belongs to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`ML.MIN_MAX_SCALER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales a numerical expression to the range [0, 1] capped with `MIN` and `MAX`
    across all rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input: `numerical_expression`. Numerical expression to scale.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: `DOUBLE`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with five rows of age data.
    Then, we use the `ML.MIN_MAX_SCALER` function to scale the `age` column to a range
    of 0 to 1\. Note that the `ML.MIN_MAX_SCALER` function can be used to scale the
    values of a column to a different range by specifying different values for the
    `MIN` and `MAX` arguments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.STANDARD_SCALER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function standardizes a numerical expression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input: `numerical_expression`. Numerical expression to scale.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: `DOUBLE`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with five rows of age data.
    Then, we use the `ML.STANDARD_SCALER` function to standardize the `age` column
    to have a mean of 0 and a standard deviation of 1\. The resulting `scaled_age`
    column shows the standardized values of the `age` column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.MAX_ABS_SCALER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function scales a numerical expression to the range [-1, 1] by dividing
    through the largest maximum absolute value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input: `numerical_expression`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: `DOUBLE`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, we create a virtual table dataset with five rows of age data.
    Then, we use the `ML.MAX_ABS_SCALER` function to scale the `age` column so that
    the absolute values of the largest magnitude element in the column are scaled
    to 1\. The resulting `scaled_age` column shows the scaled values of the `age`
    column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.NORMALIZER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function normalizes `array_expression` to have a unit norm using the given
    p-norm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input: `array_expression, p`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: `ARRAY<DOUBLE>`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.IMPUTER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function replaces `NULL` in an expression using a specified value (for
    example, mean, median, or most frequent).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Input: `expression, strategy`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: `DOUBLE` for numerical expression. `STRING` for `STRING` expression.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ML.ONE_HOT_ENCODER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function encodes `string_expression` using a one-hot encoding scheme.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`string_expression`: The `STRING` expression to be encoded.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop` (optional): This determines which category to drop during encoding.
    The default value is `none`, which means all categories are retained.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (optional): `INT64`. This limits the encoding vocabulary to the `top_k`
    frequent categories. The default value is 32,000, and the max supported value
    is 1 million to avoid suffering from high dimensionality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_threshold` (optional): `INT64`. It limits the encoding vocabulary
    to categories whose frequency is `>= frequency_threshold`. The default value is
    `5`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: It is an array of `STRUCT` that contains the encoded values, where
    `index` is the index of the encoded value and `value` is the value of the encoded
    value.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding query results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **color** | **encoding.index** | **encoding.value** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| green | 0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| red | 0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| purple | 0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| blue | 0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| green | 0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.4: Output from the previous query'
  prefs: []
  type: TYPE_NORMAL
- en: '`ML.LABEL_ENCODER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This function converts string values into `INT64` numbers within a designated
    range. The function organizes the encoding terms in alphabetical order, and any
    category not found in this vocabulary will be represented as `0`. When utilized
    in the `TRANSFORM` clause, the vocabulary and categories omitted during the training
    process are seamlessly applied during prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`string_expression`: The `STRING` expression to be encoded.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k`: Optional `INT64`. This limits the encoding vocabulary to the `top_k`
    frequent categories. The default value is 32,000, and the max supported value
    is 1 million.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_threshold`: Optional `INT64`. This limits the encoding vocabulary
    to categories whose frequency is `>= frequency_threshold`. The default value is
    `5`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output: `INT64`. This is the encoded value of the string expression in the
    specified range.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example SQL statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding query results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **fruit** | **encoded_fruit** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| orange | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| pear | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| banana | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| apple | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| kiwi | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| apple | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| banana | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.5: Output from the previous query'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the different type of ML models you can build with BQML.
  prefs: []
  type: TYPE_NORMAL
- en: Building ML models with BQML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BQML supports model training for several different use cases. The key model
    categories that are currently supported are supervised learning models, unsupervised
    learning models, time series models, imported models, and remote models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table showcases some of the key ML model types that are supported
    within BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model Type** | **Model Types** | **Manually Defined** **Feature Preprocessing**
    | **Hyperparameter Tuning** **in BQML** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Linear and logistic regression | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Deep neural networks | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Wide-and-deep | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Boosted trees | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | Random forest | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised | AutoML tables | Not supported | Automated |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | k-means | Supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | Matrix factorization | Not supported | Supported |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | PCA | Supported | Not supported |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised | Autoencoder | Supported | Not supported |'
  prefs: []
  type: TYPE_TB
- en: '| Time series | `ARIMA_PLUS` | Only automatic preprocessing | Supported`(``auto.ARIMA4)*`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Time series | `ARIMA_PLUS_XREG` | Only automatic preprocessing | Supported`(``auto.ARIMA4)*`
    |'
  prefs: []
  type: TYPE_TB
- en: Table 6.6 – Supported capabilities for key ML models
  prefs: []
  type: TYPE_NORMAL
- en: There are two other important model creation options available in BigQuery to
    help you utilize ML models built outside BigQuery – imported models and remote
    models.
  prefs: []
  type: TYPE_NORMAL
- en: BQML allows you to import models that have been trained outside BigQuery so
    that they can be used for inference within BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following model frameworks are supported for importing:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Lite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BQML allows you to register existing Vertex AI endpoints as a remote model.
    Once registered in BigQuery, you can send a prediction request to the Vertex AI
    endpoint from within BigQuery for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Creating BQML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The BigQuery function that’s used to initiate model creation is aptly called
    `CREATE`. In this section, we’ll look at the options available to a user when
    they’re creating different types of BQML models using the `CREATE` function. You
    don’t necessarily need to read through the details of every single model at the
    moment. This should be used more as a reference, as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Linear or logistic regression models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is the syntax for creating regression models, along with the
    different required and optional arguments you need to provide as part of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The key options that can be specified in the `CREATE MODEL` statement are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: Specifies the required model type (for example, linear or logistic
    regression).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INPUT_LABEL_COLS`: Defines the label column names in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OPTIMIZE_STRATEGY`: Selects the approach for training linear regression models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO_STRATEGY`: Chooses the training approach based on several conditions:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `batch_gradient_descent` strategy is employed if either `l1_reg` or `warm_start`
    is specified
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_gradient_descent` is also used if the overall cardinality of training
    features surpasses 10,000'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When overfitting may be an issue, specifically when the number of training samples
    is less than 10 times the total cardinality, `batch_gradient_descent` is chosen
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For all other scenarios, the `NORMAL_EQUATION` strategy is implemented
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BATCH_GRADIENT_DESCENT`: Engages the batch gradient descent method for model
    training, optimizing the loss function through the use of the gradient function.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NORMAL_EQUATION`: Derives the least square solution for the linear regression
    issue using an analytical formula. The use of the `NORMAL_EQUATION` strategy is
    not permissible in the following situations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_reg` is defined'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warm_start` is defined'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The total cardinality of training features exceeds 10,000
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L1_REG`: Sets the amount of L1 regularization applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L2_REG`: Sets the amount of L2 regularization applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_ITERATIONS`: Determines the maximum number of training iterations or steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE_STRATEGY`: Selects the strategy for specifying the learning rate
    during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE`: Defines the learning rate for gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EARLY_STOP`: Indicates whether training should stop after the first iteration
    with minimal relative loss improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_REL_PROGRESS`: Sets the minimum relative loss improvement to continue
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_METHOD`: Chooses the method for splitting input data into training
    and evaluation sets. The options here are `''AUTO_SPLIT''`, `''RANDOM''`, `''CUSTOM''`,
    `''SEQ''`, and `''NO_SPLIT''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_EVAL_FRACTION`: Specifies the fraction of data used for evaluation
    with `''RANDOM''` and `''``SEQ''` splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_COL`: Identifies the column used to split the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LS_INIT_LEARN_RATE`: Sets the initial learning rate for the `''``LINE_SEARCH''`
    strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARM_START`: Retrains a model with new training data, new model options, or
    both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO_CLASS_WEIGHTS`: Balances class labels using weights for each class in
    inverse proportion to the frequency of that class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLASS_WEIGHTS`: Defines the weights to use for each class label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENABLE_GLOBAL_EXPLAIN`: Computes global explanations using Explainable AI
    for global feature importance evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CALCULATE_P_VALUES`: Computes p-values and standard errors during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FIT_INTERCEPT`: Fits an intercept to the model during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CATEGORY_ENCODING_METHOD`: Specifies the encoding method to use on non-numeric
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating deep neural network models and wide-and-deep models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the syntax for creating deep learning models, along with the different
    required and optional arguments you need to provide as part of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following options can be specified as part of the model creation request:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_name`: The name of the BQML model you’re creating or replacing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_type`: Specifies the type of model, either `''DNN_CLASSIFIER''` or `''DNN_REGRESSOR''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_fn`: For DNN model types, this specifies the activation function
    of the neural network. The options are `''RELU''`, `''RELU6''`, `''CRELU''`, `''ELU''`,
    `''SELU''`, `''SIGMOID''`, and `''TANH''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auto_class_weights`: Specifies whether to balance class labels using weights
    for each class in inverse proportion to the frequency of that class. Use only
    with the `DNN_CLASSIFIER` model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: For DNN model types, this specifies the mini-batch size of samples
    that are fed to the neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_weights`: The weights to use for each class label. This option cannot
    be specified if `AUTO_CLASS_WEIGHTS` is `TRUE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_split_method`: The method to split input data into training and evaluation
    sets. The options are `''AUTO_SPLIT''`, `''RANDOM''`, `''CUSTOM''`, `''SEQ''`,
    and `''NO_SPLIT''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_split_eval_fraction`: Used with `''RANDOM''` and `''SEQ''` splits. It
    specifies the fraction of the data used for evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_split_col`: Identifies the column used to split the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout`: For DNN model types, this specifies the dropout rate of units in
    the neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stop`: Whether training should stop after the first iteration in which
    the relative loss improvement is less than the value specified for `MIN_REL_PROGRESS`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_global_explain`: Specifies whether to compute global explanations using
    Explainable AI to evaluate global feature importance to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_units`: For DNN model types, this specifies the hidden layers of the
    neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_label_cols`: The label column name(s) in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integrated_gradients_num_steps`: Specifies the number of steps to sample between
    the example being explained and its baseline for approximating the integral in
    integrated gradients attribution methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_reg`: The L1 regularization strength of the optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`: The L2 regularization strength of the optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn_rate`: The initial learning rate for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iterations`: The maximum number of training iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`: For DNN model types, this specifies the optimizer for training
    the model. The options are `''ADAGRAD''`, `''ADAM''`, `''FTRL''`, `''RMSPROP''`,
    and `''SGD''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`warm_start`: Whether to retrain a model with new training data, new model
    options, or both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf_version`: Specifies the TensorFlow version for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating boosted tree and random forest models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the syntax for creating boosted tree and random forest models, along
    with different required and optional arguments you need to provide as part of
    the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the options that can be specified as part of the model creation request:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: Specifies whether the model is a boosted tree classifier, boosted
    tree regressor, random forest classifier, or random forest regressor. The options
    are `''BOOSTED_TREE_CLASSIFIER''`, `''BOOSTED_TREE_REGRESSOR''`, `''RANDOM_FOREST_CLASSIFIER''`,
    and `''RANDOM_FOREST_REGRESSOR''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BOOSTER_TYPE` (applicable only for `Boosted_Tree_Models`): Specifies the type
    of booster used for the boosted tree model. **GBTREE** stands for **Gradient Boosting
    Tree** and **DART** stands for **Dropouts meet** **Multiple Additive** **Regression
    Trees**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_PARALLEL_TREE`: Specifies the number of parallel trees to grow. Larger
    numbers can lead to improved performance but can also increase training time and
    memory usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DART_NORMALIZE_TYPE` (applicable only for `Boosted_Tree_Models`): Specifies
    the normalization method used for the `''TREE''` means normalization by the number
    of dropped trees in the boosting process and `''FOREST''` means normalization
    by the total number of trees in the forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TREE_METHOD`: Specifies the method used to construct each decision tree in
    the ensemble. `''AUTO''` means that the algorithm will choose the best method
    based on the data, `''EXACT''` means exact greedy algorithm, `''APPROX''` means
    approximate greedy algorithm, and `''HIST''` means histogram-based algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_TREE_CHILD_WEIGHT`: Specifies the minimum sum of instance weights required
    in a child node of a tree. If the sum is below this value, the node will not be
    split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COLSAMPLE_BYTREE`: Specifies the fraction of columns to be randomly sampled
    for each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COLSAMPLE_BYLEVEL`: Specifies the fraction of columns to be randomly sampled
    for each level of a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COLSAMPLE_BYNODE`: Specifies the fraction of columns to be randomly sampled
    for each split node of a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_SPLIT_LOSS`: Specifies the minimum loss reduction required to split a
    node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_TREE_DEPTH`: Specifies the maximum depth of each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SUBSAMPLE`: Specifies the fraction of training instances to be randomly sampled
    for each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO_CLASS_WEIGHTS`: If set to `TRUE`, the algorithm will automatically determine
    the weights to be assigned to each class based on the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLASS_WEIGHTS`: Specifies the weight to be assigned to each class. This can
    be used to balance the data if the classes are imbalanced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INSTANCE_WEIGHT_COL`: Specifies the name of the column containing the instance
    weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L1_REG`: Specifies the L1 regularization parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`L2_REG`: Specifies the L2 regularization parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EARLY_STOP`: If set to `TRUE`, the training process will stop early if the
    performance improvement falls below a certain threshold. The options are `TRUE`
    and `FALSE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEARN_RATE` (applicable only for `Boosted_Tree_Models`): Specifies the learning
    rate, which controls the step size at each iteration of the boosting process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INPUT_LABEL_COLS`: Specifies the names of the columns containing the input
    features and the label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_ITERATIONS` (applicable only for `Boosted_Tree_Models`): Specifies the
    maximum number of boosting iterations to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_REL_PROGRESS`: Specifies the minimum relative progress required to continue
    the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_METHOD`: Specifies the method used to split the data into training
    and validation sets. `''AUTO_SPLIT''` means that the algorithm will automatically
    split the data, `''RANDOM''` means random splitting, `''CUSTOM''` means user-defined
    splitting, `''SEQ''` means sequential splitting, and `''NO_SPLIT''` means no splitting
    (use all data for training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_EVAL_FRACTION`: Specifies the fraction of the data to be used for
    validation when splitting the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DATA_SPLIT_COL`: Specifies the name of the column used to split the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENABLE_GLOBAL_EXPLAIN`: If set to `TRUE`, the algorithm will compute global
    feature importance scores. The options are `TRUE` and `FALSE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XGBOOST_VERSION`: Specifies the version of XGBoost to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BQML also allows you to import deep learning models trained outside BigQuery.
    This is an extremely useful feature because it gives you the flexibility to train
    models using a more custom setup outside BigQuery and yet be able to use BigQuery’s
    compute infrastructure for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you can use the import feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the available options as part of the import feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: Specifies whether the model is TensorFlow, TensorFlow Lite, or
    an ONNX model. The options are `''TENSORFLOW''`, `''ONNX''`, and `''TENSORFLOW_LITE''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_PATH`: Provides the Cloud Storage URI of the model to import into BQML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the syntax for creating k-means models, along with different required
    and optional arguments you need to provide as part of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the options that can be specified as part of the model creation
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MODEL_TYPE`: Specifies the type of model. This option is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_CLUSTERS` (optional): For a k-means model, this specifies the number of
    clusters to identify in the input data. The default value is `log10(n)`, where
    `n` is the number of training examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KMEANS_INIT_METHOD` (optional): For a k-means model, this specifies the method
    of initializing the clusters. The default value is `''RANDOM''`. The options are
    `''RANDOM''`, `''KMEANS++''`, and `''CUSTOM''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KMEANS_INIT_COL` (optional): For a k-means model, this identifies the column
    that will be used to initialize the centroids. This option can only be specified
    when `KMEANS_INIT_METHOD` has a value of `CUSTOM`. The corresponding column must
    be of the `BOOL` type, and the `NUM_CLUSTERS` model option must be present in
    the query and its value must equal the total number of `TRUE` rows in this column.
    BQML cannot use this column as a feature and excludes it from features automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DISTANCE_TYPE` (optional): For a k-means model, this specifies the type of
    metric to compute the distance between two points. The default value is `''EUCLIDEAN''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STANDARDIZE_FEATURES` (optional): For a k-means model, this specifies whether
    to standardize numerical features. The default value is `TRUE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_ITERATIONS` (optional): The maximum number of training iterations or steps.
    The default value is `20`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EARLY_STOP` (optional): Whether training should stop after the first iteration
    in which the relative loss improvement is less than the value specified for `MIN_REL_PROGRESS`.
    The default value is `TRUE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_REL_PROGRESS` (optional): The minimum relative loss improvement that is
    necessary to continue training when `EARLY_STOP` is set to `TRUE`. For example,
    a value of 0.01 specifies that each iteration must reduce the loss by 1% for training
    to continue. The default value is `0.01`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARM_START` (optional): Whether to retrain a model with new training data,
    new model options, or both. Unless explicitly overridden, the initial options
    used to train the model are used for the warm start run. The value of `MODEL_TYPE`
    and the training data schema must remain constant in a warm start model’s retraining.
    The default value is `FALSE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at the support BQML offers for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning with BQML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BQML allows you to fine-tune hyperparameters when building ML models through
    the use of `CREATE MODEL` statements. This process, known as hyperparameter tuning,
    is a commonly employed method for enhancing model accuracy by finding the ideal
    set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example BigQuery SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the options that can be specified as part of the model creation
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NUM_TRIALS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Description: This determines the maximum number of submodels to train. Tuning
    will cease after training `num_trials` submodels or upon search space exhaustion.
    The maximum value is 100.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arguments: `int64_value` must be an `INT64` value ranging from 1 to 100.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is suggested to use at least (`num_hyperparameters` * 10) trials for model
    tuning.3
  prefs: []
  type: TYPE_NORMAL
- en: '`MAX_PARALLEL_TRIALS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Description: This represents the maximum number of trials to run concurrently.
    The default value is 1, while the maximum value is 5.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arguments: `int64_value` must be an `INT64` value ranging from 1 to 5.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A larger `max_parallel_trials` value can speed up hyperparameter tuning, but
    it may compromise the final model’s quality for the `VIZIER_DEFAULT` tuning algorithm
    as parallel trials cannot benefit from concurrent training results.
  prefs: []
  type: TYPE_NORMAL
- en: '`HPARAM_TUNING_ALGORITHM`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Description: This determines the algorithm for hyperparameter tuning and supports
    the following values:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VIZIER_DEFAULT` (default and recommended): Uses the default Vertex AI Vizier
    algorithm, which combines advanced search algorithms such as Bayesian optimization
    with Gaussian processes and employs transfer learning to utilize previously tuned
    models.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RANDOM_SEARCH`: Employs random search to explore the search space.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GRID_SEARCH`: Utilizes grid search to explore the search space. This is only
    available when every hyperparameter’s search space is discrete.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYPERPARAMETER`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Syntax: `hyperparameter={HPARAM_RANGE(min, max) |` `HPARAM_CANDIDATES([candidates])
    }...`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This parameter configures a hyperparameter’s search space. Refer to the hyperparameters
    and objectives for each model type to find out which tunable hyperparameters are
    supported:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`HPARAM_RANGE(min, max)`: Specifies the continuous search space for a hyperparameter
    – for example, `learn_rate` = `HPARAM_RANGE``(0.0001, 1.0)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HPARAM_CANDIDATES([candidates])`: Specifies a hyperparameter with discrete
    values – for example, `OPTIMIZER=HPARAM_CANDIDATES(`[‘`adagrad`’, ‘`sgd`’, ‘`ftrl`’]`)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HPARAM_TUNING_OBJECTIVES`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This parameter specifies objective metrics for the model. The candidates are
    a subset of model evaluation metrics. Only one objective is supported currently.
    Refer to *Table 6.7*, which shows each model type, to see the supported hyperparameters
    and tuning objectives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **Model Type** | **Hyperparameter Objectives** | **Hyperparameter** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `LINEAR_REG` |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`l1_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `LOGISTIC_REG` |'
  prefs: []
  type: TYPE_TB
- en: '`precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`l1_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `KMEANS` |'
  prefs: []
  type: TYPE_TB
- en: '`davies_bouldin_index`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `num_clusters` |'
  prefs: []
  type: TYPE_TB
- en: '| `MATRIX_``FACTORIZATION (``implicit/explicit)` |'
  prefs: []
  type: TYPE_TB
- en: '`mean_average_precision (``explicit model)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error (implicit/explicit)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalized_discounted_cumulative_gain (``explicit model)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`average_rank (``explicit model)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_factors`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wals_alpha(implicit` `model only)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `DNN_CLASSIFIER` |'
  prefs: []
  type: TYPE_TB
- en: '`precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_units`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn_rate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_fn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `DNN_REGRESSOR` |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `BOOSTED_TREE_``CLASSIFIER` |'
  prefs: []
  type: TYPE_TB
- en: '`precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`learn_rate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tree_depth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_split_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_parallel_tree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tree_child_weight`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bylevel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bynode`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`booster_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dart_normalize_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tree_method`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `BOOSTED_TREE_``REGRESSOR` |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `RANDOM_FOREST_``CLASSIFIER` |'
  prefs: []
  type: TYPE_TB
- en: '`precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`l1_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2_reg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tree_depth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_split_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_parallel_tree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tree_child_weight`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bylevel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bynode`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tree_method`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `RANDOM_FOREST_``REGRESSOR` |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score (default)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.7 – Supported hyperparameter objectives by model type
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning)'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at BQML features that you can use when trying to evaluate ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the BQML model has been trained, you will want to evaluate the key performance
    statistics, depending on the type of model. You can do so by using the `ML.EVALUATE`
    function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the options you can specify as part of the evaluation query:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_name`: The name of the model being evaluated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_name` (optional): The name of the table containing the evaluation data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_statement` (optional): The query used to generate the evaluation data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold` (optional): A custom threshold value for binary-class classification
    models that’s used during evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perform_aggregation` (optional): A Boolean value that identifies the level
    of evaluation for forecasting accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horizon` (optional): The number of forecasted time points against which evaluation
    metrics are computed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`confidence_level` (optional): The percentage of future values that fall within
    the prediction interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the `ML.Evaluate` function depends on the type of model being
    evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model Type** | **Returned Fields** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Regression models |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classification models |'
  prefs: []
  type: TYPE_TB
- en: '`precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`f1_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_loss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| k-means model |'
  prefs: []
  type: TYPE_TB
- en: '`Davies-Bouldin index`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mean` `squared distance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Matrix factorization model with implicit feedback |'
  prefs: []
  type: TYPE_TB
- en: '`mean_average_precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalized_discounted_cumulative_gain`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`average_rank`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Matrix factorization model with explicit feedback |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2_score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explained_variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PCA model |'
  prefs: []
  type: TYPE_TB
- en: '`total_explained_variance_ratio`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Time series `ARIMA_PLUS` or `ARIMA_PLUS_XREG` model with input data and perform_aggregation
    = false |'
  prefs: []
  type: TYPE_TB
- en: '`time_series_id_col` `or time_series_id_cols`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_series_timestamp_col`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_series_data_col`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forecasted_time_series_data_col`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lower_bound`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upper_bound`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`absolute_percentage_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Time series `ARIMA_PLUS` or `ARIMA_PLUS_XREG` model with input data and perform_aggregation
    = true |'
  prefs: []
  type: TYPE_TB
- en: '`time_series_id_col` `or time_series_id_cols`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root_mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_absolute_percentage_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`symmetric_mean_absolute_percentage_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Time series `ARIMA_PLUS` model without input data |'
  prefs: []
  type: TYPE_TB
- en: '`time_series_id_col` `or time_series_id_cols`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`non_seasonal_p`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`non_seasonal_d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`non_seasonal_q`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_drift`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_likelihood`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AIC`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variance`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seasonal_periods`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_holiday_effect`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_spikes_and_dips`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_step_change`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Autoencoder model |'
  prefs: []
  type: TYPE_TB
- en: '`mean_absolute_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_squared_log_error`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Remote model |'
  prefs: []
  type: TYPE_TB
- en: '`remote_eval_metrics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.8 – ML.Evaluate Output
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we look at how you can use your BQML models for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Doing inference with BQML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised ML, the ultimate goal is to use a trained model to make predictions
    on new data. BQML provides the `ML.PREDICT` function for this purpose. Using this
    function, you can easily predict outcomes by supplying new data to a trained model.
    The `ML.PREDICT` function can be used during model creation, after model creation,
    or after a failure, so long as at least one iteration has been completed. The
    function returns a table with the same number of rows as the input table, and
    it includes all columns from the input table and all output columns from the model,
    with the output column names prefixed with `predicted_`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output fields that are included in the response of the `ML.PREDICT` function
    depends on the type of model being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model Type** | **Output Columns** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Linear regressionBoosted tree regressorRandom forest regressorDNN regressor
    | `predicted_<label_column_name>` |'
  prefs: []
  type: TYPE_TB
- en: '| Binary logistic regressionBoosted tree classifierRandom forest classifierDNN
    classifierMulticlass logistic regression | `predicted_<label_column_name>,` `predicted_<label_column_name>_probs`
    |'
  prefs: []
  type: TYPE_TB
- en: '| k-means | `centroid_id, nearest_centroids_distance` |'
  prefs: []
  type: TYPE_TB
- en: '| PCA | `principal_component_<index>,`input columns (if keep_original_columns
    is set to true) |'
  prefs: []
  type: TYPE_TB
- en: '| Autoencoder | `latent_col_<index>,`input columns |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow Lite | The output of the TensorFlow Lite model’s predict method
    |'
  prefs: []
  type: TYPE_TB
- en: '| Remote models | The output columns containing all Vertex AI endpoint output
    fields, and a remote_model_status field containing status messages from the Vertex
    AI endpoint |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX models | The output of the ONNX model’s predict method |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost models | The output of the XGBoost model’s predict method |'
  prefs: []
  type: TYPE_TB
- en: Table 6.9 – ML.Predict Output
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s work through a hands-on exercise where we use BQML to train an ML
    model and use it for generating predictions.
  prefs: []
  type: TYPE_NORMAL
- en: User exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the notebook in [*Chapter 6*](B17792_06.xhtml#_idTextAnchor079), *Low-Code
    Options for Building ML Models*, in this book’s GitHub repository for a hands-on
    exercise around training a BQML model. In this exercise, you will use one of the
    public datasets available in BigQuery to train a model to predict the likelihood
    of a customer defaulting on their loan next month.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BQML is a powerful tool for data scientists and analysts who want to train ML
    models with ease while using a low-code option to build and deploy models in GCP.
    With BQML, users can leverage the power of BigQuery to quickly and easily create
    models without needing to write complex code.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored the features and benefits of BQML. We saw how it
    provides a simple and intuitive interface for training models using SQL queries.
    We also explored some of the key features of BQML, including the ability to perform
    data preprocessing and feature engineering directly within BigQuery, as well as
    the ability to evaluate model performance through native evaluation functions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of BQML is its integration with BigQuery, which makes
    it easy to scale and manage large datasets. This makes it a great option for companies
    and organizations that are dealing with massive amounts of data and need to quickly
    build and deploy models.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of BQML is its support for a wide range of ML models, including
    linear regression, logistic regression, k-means clustering, and more. This makes
    it a versatile tool that can be used for a variety of use cases, from predicting
    customer churn to clustering data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed some of the limitations of BQML. For example, while it provides
    a low-code option for building and deploying models, it may not be suitable for
    more complex use cases that require custom models or extensive feature engineering.
    Additionally, while BQML provides a range of metrics for evaluating model performance,
    users may need to do additional analysis to fully understand the effectiveness
    of their models.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, BQML is a powerful tool for data scientists and analysts
    who want to quickly and easily build and deploy ML models. Its integration with
    BigQuery and other GCP services makes it a great option for companies and organizations
    that need to work with large amounts of data, while its support for a wide range
    of models and metrics makes it a versatile tool for a variety of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, BQML is a valuable addition to the suite of ML tools available in GCP.
    Its low-code interface, integration with BigQuery, and support for a wide range
    of models make it a great option for data scientists and analysts who want to
    focus on their data and insights, rather than complex code and infrastructure.
    With BQML, users can quickly and easily build and deploy models, enabling them
    to extract valuable insights from their data and make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to train fully custom TensorFlow deep
    learning models on Vertex AI using its serverless training features. This chapter
    will also do a deep dive into building the model using TensorFlow, packaging it
    for submission to Vertex AI, monitoring the training progress, and evaluating
    the trained model.
  prefs: []
  type: TYPE_NORMAL
