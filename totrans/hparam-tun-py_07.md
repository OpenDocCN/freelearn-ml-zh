# *第6章*: 探索多保真优化

**多保真优化**（**MFO**）是四组超参数调整方法中的第四组。这一组的主要特点是，这一组中的所有方法都利用了整个超参数调整管道的廉价近似，因此我们可以以更低的计算成本和更快的实验时间获得相似的性能结果。当您有一个非常大的模型或一个非常大的样本数量时，例如，当您正在开发基于神经网络的模型时，这一组是合适的。

在本章中，我们将讨论MFO组中的几种方法，包括从粗到细的搜索、连续减半、超带宽和**贝叶斯优化与超带宽**（**BOHB**）。与[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)*探索启发式搜索*一样，我们将讨论每种方法的定义、它们之间的差异、它们的工作原理以及每种方法的优缺点。

到本章结束时，您将能够自信地解释MFO及其变体，以及它们在高级和技术上是如何工作的。您还将能够区分它们之间的差异，以及每种方法的优缺点。您还将体验到理解每种方法在实际操作中的关键好处：能够配置方法以匹配您自己的问题，并在方法出现错误或意外输出时知道该怎么做。

在本章中，我们将涵盖以下主要主题：

+   介绍MFO

+   理解从粗到细的搜索

+   理解连续减半

+   理解超带宽

+   理解BOHB

# 介绍MFO

MFO是一组超参数调整方法，通过创建整个超参数调整管道的廉价近似，我们可以以更低的计算成本和更快的实验时间获得相似的性能结果。有许多方法可以创建廉价近似。例如，我们可以在前几个步骤中仅对完整数据的子集进行操作，而不是直接对完整数据进行操作，或者我们也可以在用完整周期训练模型之前尝试使用较少的周期来训练基于神经网络的模型。换句话说，MFO方法通过*结合廉价低保真和昂贵高保真*评估来工作，其中通常廉价评估的比例远大于昂贵评估，这样我们就可以实现更低的计算成本，从而更快地完成实验。然而，MFO方法也可以归类为**信息搜索**类别的一部分，因为它们利用先前迭代的知识来获得（希望）更好的搜索空间。

我们在前几章中学到的所有方法都可以归类为**黑盒优化**方法。所有黑盒优化方法都试图在不利用ML模型内部发生的情况或模型使用的数据的任何信息的情况下进行超参数调整。黑盒优化器将只关注从定义的超参数空间中搜索最佳的超参数集，并将*其他因素视为黑盒*（见*图6.1*）。这种特性有其自身的优点和缺点。它使我们能够利用黑盒优化器，这对于各种类型的模型或数据来说更加灵活，但它也让我们付出了代价，因为我们没有考虑可能加快进程的其他因素。

![图6.1 – 黑盒优化器的示意图

![图片](img/B18753_06_001.jpg)

图6.1 – 黑盒优化器的示意图

黑盒优化方法的成本意味着当我们处理需要非常长的训练迭代时间才能完成的一个非常大的模型或大数据时，我们无法利用它们。这就是超参数调整方法的MFO组出现的地方！通过考虑黑盒优化器视为黑盒的其他因素，我们可以在牺牲一点黑盒优化器所具有的通用性的情况下，拥有一个更快的进程。

通用性

**通用性**意味着模型能够在许多未见过的案例上执行。

此外，本组分类的大多数方法都可以很好地利用并行计算资源，这可以进一步加快超参数调整过程的速度。然而，MFO方法提供的更快过程的好处是有代价的。我们可能会有*更差的调整结果*，因为我们有可能在低保真度评估步骤中排除了更好的子空间。然而，*速度提升可以说是更重要的*，特别是当我们处理一个非常大的模型和/或大数据时。

重要提示

超参数调整方法的MFO组与包括穷举搜索、贝叶斯优化和启发式搜索在内的黑盒优化方法相比，并不是一个完全不同的组。事实上，我们也可以将多保真优化方法中使用的类似程序应用到黑盒优化器中。换句话说，*我们可以结合黑盒和多保真模型，从而获得两者的最佳效果*。

例如，我们可以使用**贝叶斯优化**（**BO**）方法之一（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)，*探索贝叶斯优化*）并在其上应用连续减半法（见*理解连续减半*部分）。这样，我们将确保我们只在重要的子空间上执行BO，而不是让BO自行探索整个超参数空间。通过这样做，我们可以拥有更快的实验时间，同时降低计算成本。

现在您已经了解了 MFO 是什么，它与黑盒优化方法有何不同，以及它在高层次上是如何工作的，我们将在以下几节中深入探讨几个 MFO 方法。

# 理解粗细搜索

**粗细搜索**（**CFS**）是网格搜索和随机搜索超参数调整方法的组合（参见[*第 3 章*](B18753_03_ePub.xhtml#_idTextAnchor031)*，探索穷举搜索）。与被归类在**无信息搜索**方法组的网格搜索和随机搜索不同，CFS 利用先前迭代的知识来拥有（希望）更好的搜索空间。换句话说，CFS 是一种 *结合了顺序和并行* 超参数调整方法。实际上，这是一个非常简单的方法，因为它基本上是 *两种其他简单方法的组合：网格搜索和随机搜索*。

CFS 可以有效地作为超参数调整方法，当你使用中等规模模型时，例如浅层神经网络（其他类型的模型也可以工作）以及适量的训练数据。

CFS 的主要思想只是从整个超参数空间开始进行 *粗略* 随机搜索，然后逐渐 *细化* 搜索的细节，无论是使用随机搜索还是网格搜索。以下图总结了 CFS 作为超参数调整方法的工作方式。

![图 6.2 – CFS 的说明

![img/B18753_06_002.jpg]

图 6.2 – CFS 的说明

如 *图 6.2* 所示，CFS 首先在整个预定义的超参数空间中进行随机搜索。然后，它根据第一次粗略随机搜索评估结果寻找一个有希望的子空间。有希望的子空间的定义可能不同，可以根据您的偏好进行调整。以下列表显示了您可以采用的几个有希望的子空间定义：

+   仅根据前一次试验中进行的评估，获取最佳超参数集的 *最高 N 分位数*。

+   对前一次试验中的不良超参数集设置一个 *硬阈值* 以过滤掉。

+   进行 *单变量分析* 以获取每个超参数的最佳值范围。

无论您使用什么定义来定义有希望的子空间，我们总会为每个超参数得到一个值列表。然后，我们可以根据每个超参数值列表中的最小值和最大值创建一个新的超参数空间。

在得到有希望的子空间后，我们可以在较小的区域内通过执行网格搜索或另一个随机搜索来继续该过程。请注意，您还可以设置条件，以确定何时继续使用随机搜索以及何时开始使用网格搜索。再次强调，选择适当的条件取决于您。然而，进行随机搜索比进行网格搜索更好，这样我们就可以与昂贵的保真度高的方法相比，有更多的基于低成本低保真度方法的评估。我们重复执行此过程，直到达到停止标准。

以下过程更详细地解释了CFS作为超参数调整方法的工作原理：

1.  将原始完整数据集分为训练集和测试集。（参见[*第 1 章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，*评估机器学习模型*。）

1.  根据训练集和停止标准，定义超参数空间`H`及其伴随的分布，目标函数`f`。

1.  定义创建网格搜索超参数空间`grid_size`的网格大小和随机搜索迭代次数`random_iters`。

1.  通过利用目标函数`f`定义有希望的子空间的标准。

1.  定义何时开始使用网格搜索的标准。

1.  将初始最佳超参数集`best_set`设置为`None`。

1.  在当前的超参数空间`H`上，进行`random_iters`次随机搜索。

1.  根据在*步骤 4*中定义的标准选择有希望的子空间：

    1.  如果当前表现最佳的超参数集比之前的`best_set`差，将`best_set`添加到有希望的子空间中。

    1.  如果当前表现最佳的超参数集比之前的`best_set`好，更新`best_set`。

1.  如果满足`步骤 5`中的标准，请执行以下操作：

    1.  使用每个超参数的独特`grid_size`值，将当前超参数空间`H`更新为在*步骤 8*中选定的有希望的子空间。

    1.  在更新的超参数空间`H`上执行网格搜索。

1.  如果未满足*步骤 5*中的标准，请执行以下操作：

    1.  使用每个超参数的最小值和最大值，将当前超参数空间`H`更新为在*步骤 8*中选定的有希望的子空间。

    1.  在更新的超参数空间`H`上，进行`random_iters`次随机搜索。

1.  重复*步骤 8 – 10*，直到满足停止标准。

1.  使用最佳超参数组合在完整训练集上进行训练。

1.  在测试集上评估最终训练的模型。

在CFS中，多保真特性既不是基于数据量也不是基于训练轮数，而是基于在每个试验中搜索空间中执行的*搜索粒度*。换句话说，我们将*继续使用所有数据*和*所有训练轮数*，在每个试验中使用*精细的超参数空间*。

让我们看看CFS作为超参数调整方法在由`make_classification`生成的虚拟数据上的工作情况，以创建具有几个可定制配置的虚拟分类数据。在这个例子中，我们使用以下配置来生成虚拟数据：

+   *类别数量*。我们将数据中的目标类别数量设置为2，通过设置`n_classes=2`。

+   *样本数量*。我们将样本数量设置为500，通过设置`n_samples=500`。

+   *特征数量*。我们将特征数量或数据中的依赖变量数量设置为25，通过设置`n_features=25`。

+   *信息特征数量*。我们将具有高重要性以区分所有目标类的特征数量设置为18，通过设置`n_informative=18`。

+   *冗余特征数量*。我们将基本上只是其他特征加权求和的特征数量设置为5，通过设置`n_redundant=5`。

+   *随机种子*。为了确保可重复性，我们设置`random_state=0`。

我们使用一个`12`作为停止标准。我们将每个随机搜索试验的迭代次数设置为`20`。最后，我们利用*顶部N百分位数*方案来定义每个试验中的具有希望的子空间，其中`N=50`。我们定义超参数空间如下：

+   隐藏层中的神经元数量：`hidden_layer_sizes=range(1,51)`

+   初始学习率：`learning_rate_init=np.linspace(0.001,0.1,50)`

以下图显示了CFS在每个迭代或试验中的工作方式。*紫色点*表示当前试验中测试的超参数值，而*红色矩形*表示下一个试验中要搜索的具有希望的子空间。

![图6.3 – CFS过程的说明

](img/B18753_06_003.jpg)

图6.3 – CFS过程的说明

在*图6.3*中，我们可以清楚地看到CFS是如何从整个超参数空间开始工作，然后逐渐在更小的子空间中搜索的。也值得注意，尽管在这个例子中我们只使用了随机搜索，但我们仍然可以看到CFS在试验次数增加时仍然提高了其保真度，直到在最后一个试验中得到最终的超参数集。我们还可以在以下图中看到每个试验的性能。

![图6.4 – 收敛图

](img/B18753_06_004.jpg)

图6.4 – 收敛图

*图6.4*中的蓝色线反映了所有测试超参数在每个试验中的平均交叉验证分数（参见*图6.3*中的紫色点）。红色线反映了每个试验中表现最佳的超参数集的交叉验证分数。我们可以看到红色线具有很好的*非递减单调*特性。这是因为我们总是将所有先前试验中最佳的超参数集添加回具有希望的子空间定义中，正如在先前程序中的*步骤8*所定义的。我们将在*第7章，通过Scikit进行超参数调整*中学习如何实现CFS。

下表总结了利用CFS作为超参数调整方法的优缺点。

![图6.5 – CFS的优缺点

](img/B18753_06_005.jpg)

图6.5 – CFS的优缺点

在本节中，我们讨论了CFS，探讨了它是什么，如何工作，以及优缺点。我们将在下一节讨论另一个有趣的MFO方法。

# 理解连续减半

**连续减半**（**SH**）是一种MFO方法，它不仅能够专注于更有希望的超参数子空间，而且还能在每个试验中**明智地分配计算成本**。与CFS不同，CFS在每个试验中利用所有数据，而SH可以为不太有希望的子空间利用较少的数据，而为更有希望的子空间利用更多的数据。可以说，SH是CFS的一个变种，具有更清晰的算法定义，并且在计算成本的分配上更为明智。将SH作为超参数调整方法的最有效方式是在处理大型模型（例如，深度神经网络）和/或处理大量数据时。

与CFS类似，SH也**使用网格搜索或随机搜索**来搜索最佳的超参数集。在第一次迭代中，SH将在整个超参数空间上执行带有少量**预算**或资源的网格或随机搜索，然后它将逐渐增加预算，同时在每次迭代中移除最差的半数超参数候选者。换句话说，SH在更大的搜索空间上以较低的预算进行超参数调整，在更有希望的较小子空间上以较高的预算进行超参数调整。SH也可以被视为超参数候选者之间的**锦标赛**，只有最佳候选者才能在试验结束时幸存。

SH中的预算定义

在默认的超参数调整设置中，预算定义为数据中的样本数量。然而，也可以以其他方式定义预算。例如，我们也可以将预算定义为最大训练时间、XGBoost训练步骤中的迭代次数、随机森林中的估计器数量，或者训练神经网络模型时的epoch数量。

在我们讨论如何在正式程序中工作之前，为了更好地理解SH，让我们先看看以下示例。我们使用与*理解CFS*部分中使用的相同模型和相同的超参数空间定义。我们还使用类似的程序生成一个大小大一百倍的虚拟分类数据集，这意味着我们有`50000`个样本，而不是像CFS示例中那样只有`500`个样本。

在这个例子中，我们使用随机搜索而不是网格搜索来在每个试验中采样超参数候选者。以下图显示了超参数候选者在试验中的准确度分数。每条线代表每个超参数候选者的目标函数分数的趋势，在这种情况下是*七折交叉验证准确度分数*，相对于试验次数。基于从SH调整过程中选择的最佳超参数集，最终的目标函数分数是`0.984`。我们将在[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)*，通过Scikit进行超参数调整*和[*第9章*](B18753_09_ePub.xhtml#_idTextAnchor082)*，通过Optuna进行超参数调整*中学习如何实现SH。

![图6.6 – SH过程的示意图

](img/B18753_06_006.jpg)

图6.6 – SH过程的示意图

在*图6.6*中，我们可以清楚地看到SH如何只从每个试验中选取顶部的超参数候选者（见橙色椭圆）以在下一个试验中进行进一步评估。在第一次迭代中，进行了`240`次随机搜索，只有`50000`个样本中的`600`个可用。这意味着我们在第一次迭代中有`240`个超参数候选者，`n_candidates`。在这些超参数候选者中，SF只选取了前`80`个候选者，在第二次迭代中使用更多的样本进行评估，即`1800`个样本。对于第三次迭代，SF再次只选取了前`27`个候选者，并在`5400`个样本上评估它们。

这个过程会一直持续到我们*不能使用更多的样本数量*，因为这将超过最初定义的最大资源，`max_resources`。在这个例子中，最大资源定义为数据中的样本数量。然而，它也可以根据预算或资源的定义来定义为总轮数或训练步骤。

在这个例子中，我们在第4次迭代时停止，需要基于`48600`个样本评估`3`个候选者。最终选择的超参数候选者是那些在`48600`个样本上评估出的七折交叉验证准确度分数最高的一个。

如您所注意到的，每次试验中样本数量的逐渐增加和候选人数量的逐渐减少遵循相同的乘数因子，即`factor`，在这个例子中是`3`。这就是为什么我们必须在第4次迭代时停止，因为如果我们继续到第5次迭代，我们需要`48600*3=145800`个样本，而我们只有`50000`个样本在数据中。请注意，在运行SH调整过程之前，我们必须自己设置乘数因子的值。换句话说，这个乘数因子是SH的超参数。

SH中的乘数因子

SH 中的 halving term 指的是将乘数因子值设为二。换句话说，每个试验中只有最好的半数超参数候选者会传递到下一个试验。然而，我们也可以用另一个值来改变这一点。例如，当我们把乘数因子设为三时，这意味着我们只取每个试验中排名前三分之一的超参数候选者。在实践中，将乘数因子设为三通常比设为二效果更好。

除了乘数因子和最大资源外，SH 还具有其他超参数，例如第一次迭代中使用的最小资源量，`min_resources`，以及第一次迭代中要评估的候选者初始数量，`n_candidates`。如果在 SH 调整过程中使用了网格搜索，`n_candidates` 将等于搜索空间中超参数所有组合的数量。如果使用了随机搜索，那么我们必须自己设置 `n_candidates` 的值。在我们的例子中，由于使用了随机搜索，我们设置了 `min_resources=600` 和 `n_candidates=240`。

虽然 `factor` 设为三是一种常见做法，但 `min_resources` 和 `n_candidates` 并非如此。在选择 `min_resources` 和 `n_candidates` 超参数的正确值之前，需要考虑许多因素。换句话说，它们之间存在权衡，如下所述：

+   当好坏超参数可以通过较少的样本（较小的 `min_resources` 值）轻松区分时，为 `n_candidates` 选择一个较大的值是有用的。

+   当我们需要更多的样本（更大的 `min_resources` 值）来区分好坏超参数时，为 `n_candidates` 选择一个较小的值是有用的。

SH 还有一个超参数，即最小早期停止率，`min_early_stopping`。这个整型超参数的默认值为零。如果将其设置为大于零的值，它将减少迭代次数，同时增加第一次迭代中使用的资源数量。在我们之前的例子中，我们设置了 `min_early_stopping=0`。

总结来说，SH 作为一种超参数调整方法，其工作原理如下：

1.  将原始数据集分为训练集和测试集。

1.  定义基于训练集的超参数空间，`H`，以及伴随的分布和目标函数，`f`。

1.  定义预算/资源。通常，这被定义为样本数量或训练轮数。

1.  定义最大资源量，`max_resources`。通常，这被定义为数据中的总样本数或总轮数。

1.  定义乘数因子，`factor`，第一次迭代中使用的最小资源量，`min_resources`，以及最小早期停止率，`min_early_stopping`。

1.  定义第一次迭代要评估的超参数候选者初始数量，`n_candidates`。如果使用网格搜索，这将从搜索空间中总超参数组合数自动解析。

1.  使用以下公式计算最大迭代次数*n*iter：

![公式](img/Formula_B18753_06_001.png)

1.  断言`n_candidates` ≥ ![公式](img/Formula_B18753_06_002.png)以确保在最后迭代中至少有一个候选者。

1.  预热第一次迭代：

    1.  从超参数空间中采样`n_candidates`个超参数集。如果使用网格搜索，只需返回空间中的所有超参数组合。这组候选者被称为*candidates1*。

    1.  根据目标函数*f*，使用`min_resources`评估所有*candidates1*超参数集。

    1.  计算用于选择下一迭代顶级候选者的*topK*值：

![公式](img/Formula_B18753_06_003.png)

1.  对于每个迭代*i*，从第二次迭代开始直到*n*iter迭代，按以下步骤进行：

    1.  通过从*candidatesi-1*中选择*topK*个最优化目标函数得分的候选者来更新当前候选者集*candidatesi*。

    1.  根据以下公式更新当前分配的资源*resourcesi*：

![公式](img/Formula_B18753_06_004.png)

1.  根据目标函数*f*，使用*resourcesi*评估所有*candidatesi*超参数集。

1.  根据以下公式更新*topK*值：

![公式](img/Formula_B18753_06_005.png)

1.  返回最佳超参数候选者：

    1.  使用分配的资源数量和目标函数`f`评估最后迭代中的所有候选者。请注意，最后迭代中分配的资源可能少于`max_resources`。

    1.  选择具有最佳目标函数得分的候选者。

1.  使用第11步中最佳的超参数集在完整训练集上训练。

1.  在测试集上评估最终训练的模型。

根据前面的示例和所述程序，我们可以看到SH在前几次迭代中通过使用少量资源进行低成本的、低保真度的评估，并在最后几次迭代中通过使用大量资源开始进行更昂贵的、高保真度的评估。

与其他黑盒方法的集成

SH除了网格搜索和随机搜索之外，还可以与其他黑盒超参数调优方法结合使用。例如，在**Optuna**（见[*第9章*](B18753_09_ePub.xhtml#_idTextAnchor082)*，通过Optuna进行超参数调优*）包中，我们可以将TPE（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）与SH结合，其中SH充当**修剪器**。请注意，在Optuna中，预算/资源定义为训练步数或epoch数，而不是样本数。

以下是一个列表，列出了SH作为超参数调整方法的优缺点：

![图6.7 – SH的优缺点

](img/B18753_06_007.jpg)

图6.7 – SH的优缺点

在实践中，大多数时候，我们不知道如何平衡资源量和候选者数量之间的权衡，因为没有明确的定义来区分坏的和好的超参数。可以帮助我们在这种权衡中找到甜点的一点是利用之前的类似实验配置，或者基于之前类似实验的可用的元数据执行**元学习**。

现在你已经了解了SH，它的运作方式，何时使用它，以及它的优缺点，在下一节中，我们将学习这个方法的扩展，它试图克服SH的缺点。

# 理解超带宽

**超带宽**（**HB**）是SH的一个扩展，专门设计用来克服SH固有的问题（参见*图6.7*）。尽管我们可以执行元学习来帮助我们平衡权衡，但在实践中我们大多数时候并没有所需的元数据。此外，SH在前几次迭代中可能会移除更好的超参数集，这也是令人担忧的，仅仅通过找到权衡的甜点是无法解决的。HB通过迭代调用SH多次来尝试解决这些问题。

由于HB只是SH的一个扩展，建议你在处理大型模型（例如，深度神经网络）和/或处理大量数据时，像SH一样使用HB作为你的超参数调整方法。此外，当你没有时间或元数据来帮助你配置资源量和候选者数量之间的权衡时，使用HB甚至比SH更好，这种情况在大多数时候都会发生。

HB和SH之间的主要区别在于它们的超参数。HB具有与SH相同的超参数（参见*理解SH*部分），除了`n_candidates`。在HB中，我们不需要选择`n_candidates`的最佳值，因为它在HB算法中自动计算。

基本上，HB通过迭代运行SH，每次迭代中`n_candidates`和`min_resources`都有变化，以及每个`n_candidates`和`min_resources`可能的最小值，并逐渐降低`n_candidates`的可能值，同时提高*资源*可能的最大值（参见*图6.8*）。这就像是一种暴力方法，尝试*几乎*所有可能的`n_candidates`和`min_resources`的组合。

![图6.8 – HB过程的示意图。这里，nj和rj分别指代bracket-j的n_candidates和min_resources

](img/B18753_06_008.jpg)

图6.8 – HB过程的示意图。这里，nj和rj分别指代bracket-j的n_candidates和min_resources

如*图6.8*所示，假设我们设置`factor=3`，`min_resources=1`，`max_resources=27`，和`min_early_stopping=0`。正如你所见，HB在第一个括号中分配了最小数量的资源，同时分配了最大数量的候选者，而在最后一个括号中分配了最大数量的资源，同时分配了最小数量的候选者。再次强调，每个括号都指的是每次SH运行，这意味着在这个示例中我们运行了四次SH，其中最后一个括号基本上等同于在小的超参数空间上进行随机搜索或网格搜索。

通过测试几乎所有的`n_candidates`和`min_resources`的可能组合，HB能够在SH中消除权衡，同时减少在第一次迭代中排除更好的超参数的可能性。然而，HB的这项开创性特性*并不能保证它总是比SH更好*。为什么？因为HB实际上并没有尝试所有可能的组合。我们可能通过执行一次SH就能找到比HB尝试的所有可能组合更好的`n_candidates`和`min_resources`值的组合。然而，这需要时间和运气，因为我们必须手动选择`n_candidates`和`min_resources`值。

集成其他黑盒方法

在HB的原始论文中，作者为每次SH运行使用了随机搜索。然而，与SH一样，我们也可以将HB与其他黑盒方法集成。

下面的过程进一步说明了HB作为超参数调整方法的正式工作原理：

1.  将完整原始数据集分为训练集和测试集。

1.  根据训练集定义超参数空间，`H`及其伴随的分布，以及目标函数，`f`。

1.  定义`budget`资源。这通常定义为样本数或训练epoch数。

1.  定义最大资源，`max_resources`。这通常定义为数据中的总样本数或总epoch数。

1.  定义乘数因子，`factor`，最小早期停止率，`min_early_stopping`，以及所有括号的最小资源数量，`min_resources`。通常，如果预算定义为样本数，则`min_resources`设置为1。

1.  创建一个字典，`top_candidates`，它将被用来存储每次SH运行中表现最好的超参数集。

1.  使用以下公式计算括号的数量，*nbrackets*：

![](img/Formula_B18753_06_016.png)

1.  对于每个括号-*j*，从`j=1`开始，直到`j=nbrackets`，执行以下操作：

    1.  使用以下公式计算SH第一次迭代中用于括号-*j*的最小资源数量，![](img/Formula_B18753_06_017.png)：

![](img/Formula_B18753_06_018.png)

1.  使用以下公式计算SH第一次迭代中用于括号-*j*的初始超参数候选数量，![](img/Formula_B18753_06_019.png)：

![](img/Formula_B18753_06_020.png)

1.  根据*理解SH*部分给出的SH过程，利用 ![](img/Formula_B18753_06_021.png) 作为当前SH运行的`min_resources`，![](img/Formula_B18753_06_022.png) 作为`n_candidates`执行*步骤 7 – 11*。

1.  将当前SH运行输出的最佳超参数集以及目标函数分数存储在`top_candidates`字典中。

1.  从`top_candidates`字典中选择具有最佳目标函数分数的最佳候选者。

1.  使用*步骤 9*中的最佳超参数集在完整训练集上训练。

1.  在测试集上评估最终训练好的模型。

以下表格总结了利用HB作为超参数调整方法的优缺点：

![图 6.9 – HB 的优缺点](img/B18753_06_009.jpg)

](img/B18753_06_009.jpg)

图 6.9 – HB 的优缺点

值得注意的是，尽管HB可以帮助我们处理SH的权衡，但它有更高的计算成本，因为我们必须迭代地运行几个SH轮次。当我们面临一个坏的和好的超参数难以通过小预算值区分的情况时，成本甚至更高。为什么？HB的前几个使用小预算的括号会导致噪声估计，因为较小预算上的SH迭代中的相对排名并不反映较高预算上的实际相对排名。在最极端的情况下，最佳的超参数集将来自最后一个括号（随机搜索）。如果这种情况发生，那么HB将比随机搜索慢 *n**括号* 倍。

在本节中，我们讨论了HB，它是什么，它是如何工作的，以及它的优缺点。我们将在下一节讨论另一个有趣的MFO方法。

# 理解BOHB

**贝叶斯优化和超带**（**BOHB**）是HB的一个扩展，它在理解超参数候选者与目标函数之间的关系方面优于CFS、SH和HB。如果CFS、SH和HB都是基于随机搜索的有信息搜索组，那么BOHB是基于BO方法的有信息搜索组。这意味着BOHB能够根据以往的经验而不是运气来决定需要搜索哪个子空间。

如其名所示，BOHB是BO和HB方法的组合。虽然SH和HB也可以与其他黑盒方法一起使用（参见*理解SH*和*理解HB*部分），但BOHB专门设计用来以支持HB的方式利用BO方法。此外，BOHB中的BO方法还跟踪所有预算上的所有先前评估，以便它可以作为未来评估的基础。请注意，BOHB中使用的BO方法是**多变量TPE**，它能够考虑到超参数之间的相互依赖性（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）。

BOHB的主要卖点是其能够实现强大的初始性能和最终性能。这可以从原始BOHB论文的*图6.10*中轻松看出（以下注释中提供详细信息）。如果没有进行元学习，BO（BO）将优于随机搜索，如果我们有更多时间让它从以往的经验中学习。如果没有时间，BO将提供与随机搜索相似甚至更差的性能。另一方面，当时间有限时，HB的表现远优于随机搜索，但如果允许随机搜索有更多时间探索超参数空间，HB的表现将与随机搜索相似。通过结合两者的优点，BOHB不仅能够在有限的时间内优于随机搜索，而且当给随机搜索足够的时间时也能做到。

![图6.10 – 随机搜索、BO、HB和BOHB的比较

](img/B18753_06_010.jpg)

图6.10 – 随机搜索、BO、HB和BOHB的比较

原始BOHB论文

*《大规模鲁棒且高效的超参数优化：BOHB》由Stefan Falkner、Aaron Klein和Frank Hutter著，第35届国际机器学习会议论文集，PMLR 80:1437-1446，2018 ([http://proceedings.mlr.press/v80/falkner18a.html](http://proceedings.mlr.press/v80/falkner18a.html))。

以下过程进一步说明了BOHB作为超参数调整方法在形式上是如何工作的。请注意，BOHB和HB非常相似，除了HB中的随机搜索被多变量TPE和随机搜索的组合所取代。由于HB只是多次迭代地执行SH，实际的替换实际上是在HB的每个SH运行（每个括号）中执行的。

让我们从之前的说明重新开始。

*6.（前六个步骤与理解HB部分中的相同。)*

7. 定义仅执行随机搜索而不是拟合多变量TPE的概率，*random_prob*。

8. 定义多变量TPE拟合过程中良好超参数集的百分比，*top_n_percent*。（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*。）

9. 定义一个字典*candidates_dict*，该字典存储特定SH迭代中使用的预算/资源，以及超参数候选对和目标函数得分的键值对。

10. 定义在开始拟合多变量TPE之前随机采样的超参数集的最小数量`n_min`。默认情况下，我们将`n_min`设置为空间中超参数数量加一。

11. 对于每个bracket-*j*，从`j=1`开始，直到`j=nbrackets`，执行以下操作：

1.  使用以下公式计算SH算法第一次迭代中用于bracket-*j*的最低资源量！[公式](img/Formula_B18753_06_023.png)：

![公式图](img/Formula_B18753_06_024.png)

1.  使用以下公式计算SH算法第一次迭代中用于bracket-j的初始超参数候选数量！[公式](img/Formula_B18753_06_025.png)：

![公式](img/Formula_B18753_06_026.png)

1.  通过利用![公式](img/Formula_B18753_06_027.png)作为min_resources和![公式](img/Formula_B18753_06_028.png)作为n_candidates，按照理解SH部分中所述的SH过程执行步骤7 – 11，其中步骤9\. I.被以下程序替换：

1.  从均匀分布中生成一个介于零和一之间的随机数，rnd。

1.  如果`rnd<random_prod`或*models_dict*为空，则执行随机搜索以采样初始超参数候选。

1.  计算在`candidates_dict[`![公式](img/Formula_B18753_06_029.png)`]`中采样的超参数数量，并将其存储为`num_curr_candidates`。

1.  如果`num_curr_candidates < n_min`，则执行随机搜索以采样初始超参数候选。

1.  或者，利用多变量TPE（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化）来采样初始超参数候选。请注意，我们总是在`candidates_dict`中使用的最大预算上利用多变量TPE。对于好组和坏组，超参数集的数量基于以下公式定义：

![公式](img/Formula_B18753_06_030.png)

![公式图](img/Formula_B18753_06_031.png)

1.  将采样的初始超参数候选以及目标函数得分（来自步骤ii、iv或v）存储在`candidates_dict[`![公式](img/Formula_B18753_06_032.png)``]`中。

1.  将当前SH运行输出的最佳超参数集以及目标函数得分存储在`top_candidates`字典中。

1.  从`top_candidates`字典中选择具有最佳目标函数得分的最佳候选者。

1.  使用第14步中找到的最佳超参数集在完整训练集上进行训练。

1.  在测试集上评估最终训练好的模型。

注意，为了确保BOHB跟踪所有预算上的所有评估，我们还需要在每个HB分组的每个SH迭代中，将超参数候选者及其目标函数得分存储在`candidates_dict[budget]`中。在这里，每个SH迭代中的超参数候选者指的是*candidatesi*，而预算指的是*step 10*部分中的*resourcesi*，这也可以在以下图中看到：

![图6.11 – BOHB跟踪所有预算上的评估

](img/B18753_06_011.jpg)

图6.11 – BOHB跟踪所有预算上的评估

你可能会想知道BOHB是否可以利用并行资源，因为它使用的是一种因无法利用并行计算资源而臭名昭著的BO方法。答案是*是的，这是可能的*！你可以利用并行资源，因为在每个BOHB迭代中，特别是在HB迭代中，我们可以利用多个工作者并行评估多组超参数。

那么，BOHB中使用的多变量TPE的顺序性质如何呢？是的，TPE模型内部可能需要执行一些顺序过程。然而，BOHB限制了提供给多变量TPE的超参数组数，所以可能不会花费太多时间。此外，对超参数组数的限制实际上是BOHB的作者们专门设计的。以下是从BOHB原始论文中的直接引用：

TPE中的并行性是通过限制优化EI的样本数量来实现的，故意不全面优化以获得多样性。这确保了模型连续的建议在并行评估时足够多样化，从而产生接近线性的速度提升。

还值得注意的是，我们始终在最大的预算上使用多变量TPE，以确保它有足够的预算（高保真度）来拟合，以最小化噪声估计的机会。因此，结合对传递给TPE的超参数组数的限制，我们试图确保多变量TPE被拟合在正确的超参数组数上。

下表总结了利用BOHB作为超参数调整方法的优缺点：

![图6.12 – BOHB的优缺点

](img/B18753_06_012.jpg)

图6.12 – BOHB的优缺点

就像HB在面临无法用小预算值轻松区分好坏超参数的情况下，与随机搜索相比可能慢* nbrackets*倍一样，BOHB在面临相同条件时，与传统的BO相比也会慢* nbrackets*倍。

在本节中，我们详细介绍了BOHB，包括它是什么，如何工作，以及它的优缺点。

# 摘要

在本章中，我们讨论了四种超参数调整方法中的第四组，称为MFO组。我们一般性地讨论了MFO，以及它区别于黑盒优化方法的特点，还讨论了包括CFS、SH、HB和BOHB在内的几个变体。我们看到了它们之间的差异以及每种方法的优缺点。从现在开始，当有人向你询问MFO时，你应该能够自信地解释它。你也应该能够调试并设置最适合你特定问题定义的所选方法的最佳配置。

在下一章中，我们将开始使用scikit-learn包实现我们迄今为止所学的各种超参数调整方法。我们将熟悉scikit-learn包，并学习如何在各种超参数调整方法中利用它。
