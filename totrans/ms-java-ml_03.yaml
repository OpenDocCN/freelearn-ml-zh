- en: Chapter 3. Unsupervised Machine Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we focused on supervised learning, that is, learning from
    a training dataset that was labeled. In the real world, obtaining data with labels
    is often difficult. In many domains, it is virtually impossible to label data
    either due to the cost of labeling or difficulty in labeling due to the sheer
    volume or velocity at which data is generated. In those situations, unsupervised
    learning, in its various forms, offers the right approaches to explore, visualize,
    and perform descriptive and predictive modeling. In many applications, unsupervised
    learning is often coupled with supervised learning as a first step to isolate
    interesting data elements for labeling.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on various methodologies, techniques, and algorithms
    that are practical and well-suited for unsupervised learning. We begin by noting
    the issues that are common between supervised and unsupervised learning when it
    comes to handling data and transformations. We will then briefly introduce the
    particular challenges faced in unsupervised learning owing to the lack of "ground
    truth" and the nature of learning under those conditions.
  prefs: []
  type: TYPE_NORMAL
- en: We will then discuss the techniques of feature analysis and dimensionality reduction
    applied to unlabeled datasets. This is followed by an introduction to the broad
    spectrum of clustering methods and discussions on the various algorithms in practical
    use, just as we did with supervised learning in [Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning"), *Practical Approach to Real-World
    Supervised Learning*, showing how each algorithm works, when to use it, and its
    advantages and limitations. We will conclude the section on clustering by presenting
    the different cluster evaluation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Following the treatment of clustering, we will approach the subject of outlier
    detection. We will contrast various techniques and algorithms that illustrate
    what makes some objects outliers—also called anomalies—within a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will conclude with clustering and outlier detection experiments,
    conducted with a real-world dataset and an analysis of the results obtained. In
    this case study, we will be using ELKI and SMILE Java libraries for the machine
    learning tasks and will present code and results from the experiments. We hope
    that this will provide the reader with a sense of the power and ease of use of
    these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Issues in common with supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many of the issues that we discussed related to supervised learning are also
    common with unsupervised learning. Some of them are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Types of features handled by the algorithm**: Most clustering and outlier
    algorithms need numeric representation to work effectively. Transforming categorical
    or ordinal data has to be done carefully'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Curse of dimensionality**: Having a large number of features results in sparse
    spaces and affects the performance of clustering algorithms. Some option must
    be chosen to suitably reduce dimensionality—either feature selection where only
    a subset of the most relevant features are retained, or feature extraction, which
    transforms the feature space into a new set of principal variables of a lower
    dimensional space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability in memory and training time**: Many unsupervised learning algorithms
    cannot scale up to more than a few thousands of instances either due to memory
    or training time constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers and noise in data**: Many algorithms are affected by noise in the
    features, the presence of anomalous data, or missing values. They need to be transformed
    and handled appropriately'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues specific to unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some issues that pertain to unsupervised learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter setting**: Deciding on number of features, usefulness of features,
    number of clusters, shapes of clusters, and so on, pose enormous challenges to
    certain unsupervised methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation methods**: Since unsupervised learning methods are ill-posed due
    to lack of ground-truth, evaluation of algorithms becomes very subjective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard or soft labeling**: Many unsupervised learning problems require giving
    labels to the data in an exclusive or probabilistic manner. This poses a problem
    for many algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability of results and models**: Unlike supervised learning, the
    lack of ground truth and the nature of some algorithms make interpreting the results
    from both model and labeling even more difficult'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature analysis and dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the first tools to master are the different feature analysis and dimensionality
    reduction techniques. As in supervised learning, the need for reducing dimensionality
    arises from numerous reasons similar to those discussed earlier for feature selection
    and reduction.
  prefs: []
  type: TYPE_NORMAL
- en: A smaller number of discriminating dimensions makes visualization of data and
    clusters much easier. In many applications, unsupervised dimensionality reduction
    techniques are used for compression, which can then be used for transmission or
    storage of data. This is particularly useful when the larger data has an overhead.
    Moreover, applying dimensionality reduction techniques can improve the scalability
    in terms of memory and computation speeds of many algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use similar notation to what was used in the chapter on supervised
    learning. The examples are in *d* dimensions and are represented as vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** = *(x**[1]**,x**[2]**,…x**[d]* *)**^T*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire dataset containing *n* examples can be represented as an observation
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notation](img/B05137_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The idea of dimensionality reduction is to find k ≤ *d* features either by transformation
    of the input features, projecting or combining them such that the lower dimension
    *k* captures or preserves interesting properties of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Linear methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear dimensionality methods are some of the oldest statistical techniques
    to reduce features or transform the data into lower dimensions, preserving interesting
    discriminating properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, with linear methods we are performing a transformation, such
    that a new data element is created using a linear transformation of the original
    data element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear methods](img/B05137_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**s = Wx**'
  prefs: []
  type: TYPE_NORMAL
- en: Here, **W**[k × d] is the linear transformation matrix. The variables **s**
    are also referred to as latent or hidden variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we will discuss the two most practical and often-used methodologies.
    We will list some variants of these techniques so that the reader can use the
    tools to experiment with them. The main assumption here—which often forms the
    limitation—is the linear relationships between the transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PCA is a widely-used technique for dimensionality reduction(*References* [1]).
    The original coordinate system is rotated to a new coordinate system that exploits
    the directions of maximum variance in the data, resulting in uncorrelated variables
    in a lower-dimensional subspace that were correlated in the original feature space.
    PCA is sensitive to the scaling of the features.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA is generally effective on numeric datasets. Many tools provide the categorical-to-continuous
    transformations for the nominal features, but this affects the performance. The
    number of principal components, or *k*, is also an input provided by the user.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA, in its most basic form, tries to find projections of data onto new axes,
    which are known as **principal components**. Principal components are projections
    that capture maximum variance directions from the original space. In simple words,
    PCA finds the first principal component through rotation of the original axes
    of the data in the direction of maximum variance. The technique finds the next
    principal component by again determining the next best axis, orthogonal to the
    first axis, by seeking the second highest variance and so on until most variances
    are captured. Generally, most tools give either a choice of number of principal
    components or the option to keep finding components until some percentage, for
    example, 99%, of variance in the original dataset is captured.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, the objective of finding maximum variance can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_013.jpg)![How does it work?](img/B05137_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*λ* **v** = **Cv** is the eigendecomposition'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, **W** is the principal components and **S** is the new transformation
    of the input data. Generally, eigenvalue decomposition or singular value decomposition
    is used in the computation part.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Principal Component Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the advantages of PCA is that it is optimal in that it minimizes the
    reconstruction error of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA assumes normal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computation of variance-covariance matrix can become intensive for large
    datasets with high-dimensions. Alternatively, **Singular Value Decomposition**
    (**SVD**) can be used as it works iteratively and there is no need for an explicit
    covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA has issues when there is noise in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA fails when the data lies in the complex manifold, a topic that we will discuss
    in the non-linear dimensionality reduction section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA assumes a correlation between the features and in the absence of those correlations,
    it is unable to do any transformations; instead, it simply ranks them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By transforming the original feature space into a new set of variables, PCA
    causes a loss in interpretability of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other variants of PCA that are popular and overcome some of the
    biases and assumptions of PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**) assumes that there are mixtures
    of non-Gaussians from the source and, using the generative technique, tries to
    find the decompositions of original data in the smaller mixtures or components
    (*References* [2]). The key difference between PCA and ICA is that PCA creates
    components that are uncorrelated, while ICA creates components that are independent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it assumes ![Advantages and limitations](img/B05137_03_022.jpg)
    as a mixture of independent sources ∈ ![Advantages and limitations](img/B05137_03_024.jpg),
    such that each data element *y* = [*y* *¹* *,y* *²* *,….y* *^k* ]*^T* and independence
    is implied by ![Advantages and limitations](img/B05137_03_026.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probabilistic Principal Component Analysis** (**PPCA**) is based on finding
    the components using mixture models and maximum likelihood formulations using
    **Expectation Maximization** (**EM**) (*References* [3]). It overcomes the issues
    of missing data and outlier impacts that PCA faces.'
  prefs: []
  type: TYPE_NORMAL
- en: Random projections (RP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When data is separable by a large margin—even if it is high-dimensional data—one
    can randomly project the data down to a low-dimensional space without impacting
    separability and achieve good generalization with a relatively small amount of
    data. Random Projections use this technique and the details are described here
    (*References* [4]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Random projections work with both numeric and categorical features, but categorical
    features are transformed into binary. Outputs are lower dimensional representations
    of the input data elements. The number of dimensions to project, *k*, is part
    of user-defined input.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This technique uses random projection matrices to project the input data into
    a lower dimensional space. The original data ![How does it work?](img/B05137_03_027.jpg)
    is transformed to the lower dimension space ![How does it work?](img/B05137_03_028.jpg)
    where *k << p* using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here columns in the *k* x *d* matrix **R** are i.i.d zero mean normal variables
    and are scaled to unit length. There are variants of how the random matrix **R**
    is constructed using probabilistic sampling. Computational complexity of RP is
    *O(knd)*, which scales much better than PCA. In many practical datasets, it has
    been shown that RP gives results comparable to PCA and can scale to large dimensions
    and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It scales to very large values of dataset size and dimensionalities. In text
    and image learning problems, with large dimensions, this technique has been successfully
    used as the preprocessing technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes a large information loss can occur while using RP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multidimensional Scaling (MDS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many forms of MDS—classical, metric, and non-metric. The main idea
    of MDS is to preserve the pairwise similarity/distance values. It generally involves
    transforming the high dimensional data into two or three dimensions (*References*
    [5]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MDS can work with both numeric and categorical data based on the user-selected
    distance function. The number of dimensions to transform to, *k*, is a user-defined
    input.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given *n* data elements, an *n* x *n* affinity or distance matrix is computed.
    There are choices of using distances such as Euclidean, Mahalanobis, or similarity
    concepts such as cosine similarity, Jaccard coefficients, and so on. MDS in its
    very basic form tries to find a mapping of the distance matrix in a lower dimensional
    space where the Euclidean distance between the transformed points is similar to
    the affinity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![How does it work?](img/B05137_03_037.jpg) input space and ![How does
    it work?](img/B05137_03_038.jpg) mapped space.
  prefs: []
  type: TYPE_NORMAL
- en: If the input affinity space is transformed using kernels then the MDS becomes
    a non-linear method for dimensionality reduction. Classical MDS is equivalent
    to PCA when the distances between the points in input space is Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key disadvantage is the subjective choice of the lower dimension needed
    to interpret the high dimensional data, normally restricted to two or three for
    humans. Some data may not map effectively in this lower dimensional space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantage is you can perform linear and non-linear mapping to the lowest
    dimensions using the framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonlinear methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, nonlinear dimensionality reduction involves either performing nonlinear
    transformations to the computations in linear methods such as KPCA or finding
    nonlinear relationships in the lower dimension as in manifold learning. In some
    domains and datasets, the structure of the data in lower dimensions is nonlinear—and
    that is where techniques such as KPCA are effective—while in some domains the
    data does not unfold in lower dimensions and you need manifold learning.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Principal Component Analysis (KPCA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernel PCA uses the Kernel trick described in [Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning"), *Practical Approach to Real-World
    Supervised Learning*, with the PCA algorithm for transforming the data in a high-dimensional
    space to find effective mapping (*References* [6]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to PCA with addition of choice of kernel and kernel parameters. For
    example, if **Radial Basis Function** (**RBF**) or Gaussian Kernel is chosen,
    then the kernel, along with the gamma parameter, becomes user-selected values.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the same way as **Support Vector Machines** (**SVM**) was discussed in the
    previous chapter, KPCA transforms the input space to high dimensional feature
    space using the "kernel trick". The entire PCA machinery of finding maximum variance
    is then carried out in the transformed space.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_014.jpg)![How does it work?](img/B05137_03_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Instead of linear covariance matrix, a nonlinear transformation is applied to
    the input space using kernel methods by constructing the *N* x *N* matrix, in
    place of doing the actual transformations using *ϕ* *(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: '*k(x,y) = ((* *ϕ* *(x),* *ϕ* *(y)) =* *ϕ* *(x)* *^T* *ϕ* *(y)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the kernel transformation doesn''t actually transform the features into
    explicit feature space, the principal components found can be interpreted as projections
    of data onto the components. In the following figure, a binary nonlinear dataset,
    generated using the scikit-learn example on circles (*References* [27]), demonstrates
    the linear separation after KPCA using the RBF kernel and returning to almost
    similar input space by the inverse transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: KPCA on Circle Dataset and Inverse Transform.'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: KPCA overcomes the nonlinear mapping presented by PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KPCA has similar issues with outlier, noisy, and missing values to standard
    PCA. There are robust methods and variations to overcome this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KPCA has scalability issues in space due to an increase in the kernel matrix,
    which can become a bottleneck in large datasets with high dimensions. SVD can
    be used in these situations, as an alternative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manifold learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When high dimensional data is embedded in lower dimensions that are nonlinear,
    but have complex structure, manifold learning is very effective.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Manifold learning algorithms require two user-provided parameters: *k*, representing
    the number of neighbors for the initial search, and *n*, the number of manifold
    coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As seen in the following figure, the three-dimensional S-Curve, plotted using
    the scikit-learn utility (*References* [27]), is represented in 2D PCA and in
    2D manifold using LLE. It is interesting to observe how the blue, green, and red
    dots are mixed up in the PCA representation while the manifold learning representation
    using LLE cleanly separates the colors. It can also be observed that the rank
    ordering of Euclidean distances is not maintained in the manifold representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Data representation after PCA and manifold learning'
  prefs: []
  type: TYPE_NORMAL
- en: To preserve the structure, the geodesic distance is preserved instead of the
    Euclidean distance. The general approach is to build a graph structure such as
    an adjacency matrix, and then compute geodesic distance using different assumptions.
    In the Isomap Algorithm, the global pairwise distances are preserved (*References*
    [7]). In the **Local Linear Embedding** (**LLE**) Algorithm, the mapping is done
    to take care of local neighborhood, that is, nearby points map to nearby points
    in the transformation (*References* [9]). Laplacian Eigenmaps is similar to LLE,
    except it tries to maintain the "locality" instead of "local linearity" in LLE
    by using graph Laplacian (*References* [8]).
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Isomap is non-parametric; it preserves the global structure, and has no local
    optimum, but is hampered by speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLE and Laplacian Eigenmaps are non-parametric, have no local optima, are fast,
    but don't preserve global structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering algorithms can be categorized in different ways based on the techniques,
    the outputs, the process, and other considerations. In this topic, we will present
    some of the most widely used clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a rich set of clustering techniques in use today for a wide variety
    of applications. This section presents some of them, explaining how they work,
    what kind of data they can be used with, and what their advantages and drawbacks
    are. These include algorithms that are prototype-based, density-based, probabilistic
    partition-based, hierarchy-based, graph-theory-based, and those based on neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: k-means is a centroid- or prototype-based iterative algorithm that employs partitioning
    and relocation methods (*References* [10]). k-means finds clusters of spherical
    shape depending on the distance metric used, as in the case of Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: k-means can handle mostly numeric features. Many tools provide categorical to
    numeric transformations, but having a large number of categoricals in the computation
    can lead to non-optimal clusters. User-defined *k*, the number of clusters to
    be found, and the distance metric to use for computing closeness are two basic
    inputs. k-means generates clusters, association of data to each cluster, and centroids
    of clusters as the output.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most common variant known as Lloyd's algorithm initializes *k* centroids
    for the given dataset by picking data elements randomly from the set. It assigns
    each data element to the centroid it is closest to, using some distance metric
    such as Euclidean distance. It then computes the mean of the data points for each
    cluster to form the new centroid and the process is repeated until either the
    maximum number of iterations is reached or there is no change in the centroids.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, each step of the clustering can be seen as an optimization
    step where the equation to optimize is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_053.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ci is all points belong to cluster *i*. The problem of minimizing is classified
    as NP-hard and hence k-Means has a tendency to get stuck in local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The choice of the number of clusters, *k*, is difficult to pick, but normally
    search techniques such as varying *k* for different values and measuring metrics
    such as sum of square errors can be used to find a good threshold. For smaller
    datasets, hierarchical k-Means can be tried.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means can converge faster than most algorithms for smaller values of *k* and
    can find effective global clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means convergence can be affected by initialization of the centroids and hence
    there are many variants to perform random restarts with different seeds and so
    on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means can perform badly when there are outliers and noisy data points. Using
    robust techniques such as medians instead of means, k-Medoids, overcomes this
    to a certain extent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means does not find effective clusters when they are of arbitrary shapes or
    have different densities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Density-based spatial clustering of applications with noise (DBSCAN) is a density-based
    partitioning algorithm. It separates dense region in the space from sparse regions
    (*References* [14]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Only numeric features are used in DBSCAN. The user-defined parameters are *MinPts*
    and the neighborhood factor given by *ϵ*.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The algorithm first finds the ϵ-neighborhood of every point *p*, given by ![How
    does it work?](img/B05137_03_058.jpg). A *high density* region is identified as
    a region where the number of points in a ϵ-neighborhood is greater than or equal
    to the given *MinPts*; the point such a ϵ-neighborhood is defined around is called
    a *core points*. Points within the ϵ-neighborhood of a *core point* are said to
    be *directly reachable*. All *core points* that can in effect be reached by hopping
    from one directly reachable core point to another point *directly reachable* from
    the second point, and so on, are considered to be in the same cluster. Further,
    any point that has fewer than *MinPts* in its *ϵ*-neighborhood, but is directly
    reachable from a core point, belongs to the same cluster as the core point. These
    points at the edge of a cluster are called *border points*. A *noise point* is
    any point that is neither a core point nor a border point.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DBSCAN algorithm does not require the number of clusters to be specified
    and can find it automatically from the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN can find clusters of various shapes and sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN has in-built robustness to noise and can find outliers from the datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN is not completely deterministic in its identification of the points and
    its categorization into border or core depends on the order of data processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metrics selected such as Euclidean distance can often affect performance
    due to the curse of dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there are clusters with large variations in the densities, the static choice
    of *{MinPts,* *ϵ**}* can pose a big limitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean shift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mean shift is a very effective clustering algorithm in many image, video, and
    motion detection based datasets (*References* [11]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Only numeric features are accepted as data input in the mean shift algorithm.
    The choice of kernel and the bandwidth of the kernel are user-driven choices that
    affect the performance. Mean shift generates modes of data points and clusters
    data around the modes.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mean shift is based on the statistical concept of **kernel density estimation**
    (**KDE**), which is a probabilistic method to estimate the underlying data distribution
    from the sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'A kernel density estimate for kernel *K* (**x**) of given bandwidth *h* is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_063.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For *n* points with dimensionality *d*. The mean shift algorithm works by moving
    each data point in the direction of local increasing density. To estimate this
    direction, gradient is applied to the KDE and the gradient takes the form of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_065.jpg)![How does it work?](img/B05137_03_066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here g(**x**)= –K'(**x**) is the derivative of the kernel. The vector, m(**x**),
    is called the mean shift vector and it is used to move the points in the direction
  prefs: []
  type: TYPE_NORMAL
- en: '**x**^((t+1)) = **x**^t + m(**x**)'
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is guaranteed to converge when the gradient of the density function
    is zero. Points that end up in a similar location are marked as clusters belonging
    to the same region.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mean shift is non-parametric and makes no underlying assumption on the data
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can find non-complex clusters of varying shapes and sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to explicitly give the number of clusters; the choice of the
    bandwidth parameter, which is used in estimation, implicitly controls the clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean shift has no local optima for a given bandwidth parameter and hence it
    is deterministic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean shift is robust to outliers and noisy points because of KDE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean shift algorithm is computationally slow and does not scale well with
    large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bandwidth selection should be done judiciously; otherwise it can result in merged
    modes, or the appearance of extra, shallow modes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expectation maximization (EM) or Gaussian mixture modeling (GMM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GMM or EM is a probabilistic partition-based method that partitions data into
    *k* clusters using probability distribution-based techniques (*References* [13]).
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Only numeric features are allowed in EM/GMM. The model parameter is the number
    of mixture components, given by *k*.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GMM is a generative method that assumes that there are *k* Gaussian components,
    each Gaussian component has a mean *µ*[i] and covariance Ʃ[i]. The following expression
    represents the probability of the dataset given the *k* Gaussian components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_072.jpg)![How does it work?](img/B05137_03_073.jpg)![How
    does it work?](img/B05137_03_074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The two-step task of finding the means {**µ**[1], **µ**[2], …**µ**[k]} for each
    of the *k* Gaussian components such that the data points assigned to each maximizes
    the probability of that component is done using the **Expectation Maximization**
    (**EM**) process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterative process can be defined into an E-step, that computes the *expected*
    cluster for all data points for the cluster, in an iteration *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_077.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The M-step maximizes to compute *µ*t+1 given the data points belonging to the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_079.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The EM process can result in GMM convergence into local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Works very well with any features; for categorical data, discrete probability
    is calculated, while for numeric a continuous probability function is estimated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has computational scalability problems. It can result in local optimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of *k* Gaussians has to be given *apriori*, similar to k-Means.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hierarchical clustering is a connectivity-based method of clustering that is
    widely used to analyze and explore the data more than it is used as a clustering
    technique (*References* [12]). The idea is to iteratively build binary trees either
    from top or bottom, such that similar points are grouped together. Each level
    of the tree provides interesting summarization of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hierarchical clustering generally works on similarity-based transformations
    and so both categorical and continuous data are accepted. Hierarchical clustering
    only needs the similarity or distance metric to compute similarity and does not
    need the number of clusters like in k-means or GMM.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are many variants of hierarchical clustering, but we will discuss agglomerative
    clustering. Agglomerative clustering works by first putting all the data elements
    in their own groups. It then iteratively merges the groups based on the similarity
    metric used until there is a single group. Each level of the tree or groupings
    provides unique segmentation of the data and it is up to the analyst to choose
    the right level that fits the problem domain. Agglomerative clustering is normally
    visualized using a dendrogram plot, which shows merging of data points at similarity.
    The popular choices of similarity methods used are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single linkage**: Similarity is the minimum distance between the groups of
    points:![How does it work?](img/B05137_03_080.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete linkage**: Similarity is the maximum distance between the groups
    of points:![How does it work?](img/B05137_03_081.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average linkage**: Average similarity between the groups of points:![How
    does it work?](img/B05137_03_082.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hierarchical clustering imposes a hierarchical structure on the data even when
    there may not be such a structure present.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of similarity metrics can result in a vastly different set of merges
    and dendrogram plots, so it has a large dependency on user input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering suffers from scalability with increased data points.
    Based on the distance metrics used, it can be sensitive to noise and outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-organizing maps (SOM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SOM is a neural network based method that can be viewed as dimensionality reduction,
    manifold learning, or clustering technique (*References* [17]). Neurobiological
    studies show that our brains map different functions to different areas, known
    as topographic maps, which form the basis of this technique.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Only numeric features are used in SOM. Model parameters consists of distance
    function, (generally Euclidean distance is used) and the lattice parameters in
    terms of width and height or number of cells in the lattice.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SOM, also known as Kohonen networks, can be thought of as a two-layer neural
    network where each output layer is a two-dimensional lattice, arranged in rows
    and columns and each neuron is fully connected to the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like neural networks, the weights are initially generated using random values.
    The process has three distinct training phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Competitive phase**: Neurons in this phase compete based on the discriminant
    values, generally based on distance between neuron weight and input vector; such
    that the minimal distance between the two decides which neuron the input gets
    assigned to. Using Euclidean distance, the distance between an input *x*i and
    neuron in the lattice position *(j, i)* is given by *w*[ji]:![How does it work?](img/B05137_03_086.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cooperation phase**: In this phase, the winning neurons find the best spatial
    location in the topological neighborhood. The topological neighborhood for the
    winning neuron *I*(**x**) for a given neuron *(j, i)*, at a distance *S*[ij],
    neighborhood of size σ, is defined by:![How does it work?](img/B05137_03_091.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The neighborhood size is defined in the way that it decreases with time using
    some well-known decay functions such as an exponential, function defined as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_092.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Adaptive phase**: In this phase, the weights of the winning neuron and its
    neighborhood neurons are updated. The update to weights is generally done using:![How
    does it work?](img/B05137_03_093.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the learning rate *n(t)* is again defined as exponential decay like the
    neighborhood size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SOM Visualization using Unified Distance Matrix (U-Matrix) creates a single
    metric of average distance between the weights of the neuron and its neighbors,
    which then can be visualized in different color intensities. This helps to identify
    *similar* neurons in the neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The biggest advantage of SOM is that it is easy to understand and clustering
    of the data in two dimensions with U-matrix visualization enables understanding
    the patterns very effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choice of similarity/distance function makes vast difference in clusters and
    must be carefully chosen by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SOM's computational complexity makes it impossible to use on datasets greater
    than few thousands in size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering is a partition-based clustering technique using graph theory
    as its basis (*References* [15]). It converts the dataset into a connected graph
    and does graph partitioning to find the clusters. This is a popular method in
    image processing, motion detection, and some unstructured data-based domains.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Only numeric features are used in spectral clustering. Model parameters such
    as the choice of kernel, the kernel parameters, the number of eigenvalues to select,
    and partitioning algorithms such as k-Means must be correctly defined for optimum
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following steps describe how the technique is used in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the data points, an affinity (or adjacency) matrix is computed using a
    smooth kernel function such as the Gaussian kernel:![How does it work?](img/B05137_03_095.jpg)For
    the points that are closer, ![How does it work?](img/B05137_03_096.jpg) and for
    points further away, ![How does it work?](img/B05137_03_097.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to compute the graph Laplacian matrix using various methods
    of normalizations. All Laplacian matrix methods use the diagonal degree matrix
    *D*, which measures degree at each node in the graph:![How does it work?](img/B05137_03_099.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A simple Laplacian matrix is *L = D (degree matrix) – A(affinity matrix)*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the first *k* eigenvalues from the eigenvalue problem or the generalized
    eigenvalue problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a partition algorithm such as k-Means to further separate clusters in the
    k-dimensional subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spectral clustering works very well when the cluster shape or size is irregular
    and non-convex. Spectral clustering has too many parameter choices and tuning
    to get good results is quite an involved task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering has been shown, theoretically, to be more stable in the
    presence of noisy data. Spectral clustering has good performance when the clusters
    are not well separated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Affinity propagation can be viewed as an extension of K-medoids method for its
    similarity with picking exemplars from the data (*References* [16]). Affinity
    propagation uses graphs with distance or the similarity matrix and picks all examples
    in the training data as exemplars. Iterative message passing as *affinities* between
    data points automatically detects clusters, the exemplars, and even the number
    of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, other than maximum number of iterations, which is common to most
    algorithms, no input parameters are required.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two kinds of messages are exchanged between the data points that we will explain
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Responsibility *r(i,k)*: This is a message from the data point to the candidate
    exemplar. This gives a metric of how well the exemplar is suited for that data
    point compared to other exemplars. The rules for updating the responsibility are
    as follows: ![How does it work?](img/B05137_03_102.jpg) where *s(i, k)* = similarity
    between two data points *i* and *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a(i, k)* = availability of exemplar *k* for *i*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Availability *a(i,k)*: This is a message from the candidate exemplar to a data
    point. This gives a metric indicating how good of a support the exemplar can be
    to the data point, considering other data points in the calculations. This can
    be viewed as soft cluster assignment. The rule for updating the availability is
    as follows:![How does it work?](img/B05137_03_109.jpg)![How does it work?](img/B05137_03_110.jpg)![How
    does it work?](img/B05137_03_111.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 4: Message types used in Affinity Propagation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The algorithm can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![How does it work?](img/B05137_03_216.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all increments *i* to *n*:![How does it work?](img/B05137_03_217.jpg)![How
    does it work?](img/B05137_03_218.jpg)![How does it work?](img/B05137_03_219.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: End.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all **x**[i] such that *(r(i,i) + a(i,i) > 0)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**x**[i] is exemplar.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: All non-exemplars **x**[j] are assigned to the closest exemplar using the similarity
    measure *s(i, j)*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: End.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Affinity propagation is a deterministic algorithm. Both k-means or K-medoids
    are sensitive to the selection of initial points, which is overcome by considering
    every point as an exemplar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters doesn't have to be specified and is automatically determined
    through the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It works in non-metric spaces and doesn''t require distances/similarity to
    even have constraining properties such as triangle inequality or symmetry. This
    makes the algorithm usable on a wide variety of datasets with categorical and
    text data and so on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm can be parallelized easily due to its update methods and it has
    fast training time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering validation and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering validation and evaluation is one of the most important mechanisms
    to determine the usefulness of the algorithms (*References* [18]). These topics
    can be broadly classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal evaluation measures**: In this the measures uses some form of clustering
    quality from the data themselves, without any access to the ground truth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External evaluation measures**: In this the measures use some external information
    such as known ground truth or class labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal evaluation measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Internal evaluation uses only the clusters and data information to gather metrics
    about how good the clustering results are. The applications may have some influence
    over the choice of the measures. Some algorithms are biased towards particular
    evaluation metrics. So care must be taken in choosing the right metrics, algorithms,
    and parameters based on these considerations. Internal evaluation measures are
    based on different qualities, as mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compactness**: Variance in the clusters measured using different strategies
    is used to give compactness values; the lower the variance, the more compact the
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Separation**: How well are the clusters separated from each other?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here''s a compact explanation of the notation used in what follows: dataset
    with all data elements =*D*, number of data elements =*n*, dimensions or features
    of each data element=*d*, center of entire data *D = c*, number of clusters =
    *NC*, *i*^(th) cluster = *C*[i], number of data in the *i*^(th) cluster =*n*[i],
    center of *i*^(th) cluster = *c*[i], variance in the *i*^(th) cluster = σ(*C*[i]),
    distance between two points *x* and *y* = *d (x,y)*.'
  prefs: []
  type: TYPE_NORMAL
- en: R-Squared
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal is to measure the degree of difference between clusters using the
    ratio of the sum of squares between clusters to the total sum of squares on the
    whole data. The formula is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![R-Squared](img/B05137_03_129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dunn's Indices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal is to identify dense and well-separated clusters. The measure is given
    by maximal values obtained from the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dunn''s Indices](img/B05137_03_130.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Davies-Bouldin index
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal is to identify clusters with low intra-cluster distances and high
    inter-cluster distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Davies-Bouldin index](img/B05137_03_131.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Silhouette's index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The goal is to measure the pairwise difference of between-cluster and within-cluster
    distances. It is also used to find optimal cluster number by maximizing the index.
    The formula is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette''s index](img/B05137_03_132.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Silhouette's index](img/B05137_03_133.jpg) and ![Silhouette's index](img/B05137_03_134.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: External evaluation measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The external evaluation measures of clustering have similarity to classification
    metrics using elements from the confusion matrix or using information theoretic
    metrics from the data and labels. Some of the most commonly used measures are
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Rand index
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rand index measures the correct decisions made by the clustering algorithm
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rand index](img/B05137_03_135.jpg)'
  prefs: []
  type: TYPE_IMG
- en: F-Measure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'F-Measure combines the precision and recall measures applied to clustering
    as given in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![F-Measure](img/B05137_03_136-New.jpg)![F-Measure](img/B05137_03_138.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n*[ij] is the number of data elements of class *i* in the cluster *j*,
    *n*[j] is the number of data in the cluster *j* and *n*[i] is the number of data
    in the class *i*. The higher the F-Measure, the better the clustering quality.
  prefs: []
  type: TYPE_NORMAL
- en: Normalized mutual information index
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NMI is one of the many entropy-based measures applied to clustering. The entropy
    associated with a clustering *C* is a measure of the uncertainty about a cluster
    picking a data element randomly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalized mutual information index](img/B05137_03_143.jpg) where ![Normalized
    mutual information index](img/B05137_03_144.jpg) is the probability of the element
    getting picked in cluster *C*i.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mutual information between two clusters is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalized mutual information index](img/B05137_03_146.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here ![Normalized mutual information index](img/B05137_03_147.jpg), which is
    the probability of the element being picked by both clusters *C* and *C^'*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalized mutual information** (**NMI**) has many forms; one is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalized mutual information index](img/B05137_03_150.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Outlier or anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grubbs, in 1969, offers the definition, "An outlying observation, or outlier,
    is one that appears to deviate markedly from other members of the sample in which
    it occurs".
  prefs: []
  type: TYPE_NORMAL
- en: Hawkins, in 1980, defined outliers or anomaly as "an observation which deviates
    so much from other observations as to arouse suspicions that it was generated
    by a different mechanism".
  prefs: []
  type: TYPE_NORMAL
- en: Barnett and Lewis, 1994, defined it as "an observation (or subset of observations)
    which appears to be inconsistent with the remainder of that set of data".
  prefs: []
  type: TYPE_NORMAL
- en: Outlier algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Outlier detection techniques are classified based on different approaches to
    what it means to be an outlier. Each approach defines outliers in terms of some
    property that sets apart some objects from others in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical-based**: This is improbable according to a chosen distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distance-based**: This is isolated from neighbors according to chosen distance
    measure and fraction of neighbors within threshold distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-based**: This is more isolated from its neighbors than they are in
    turn from their neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering-based**: This is in isolated clusters relative to other clusters
    or is not a member of any cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-dimension-based**: This is an outlier by usual techniques after data
    is projected to lower dimensions, or by choosing an appropriate metric for high
    dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Statistical-based techniques that use parametric methods for outlier detection
    assume some knowledge of the distribution of the data (*References* [19]). From
    the observations, the model parameters are estimated. Data points that have probabilities
    lower than a threshold value in the model are considered outliers. When the distribution
    is not known or none is suitable to assume, non-parametric methods are used.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Statistical methods for outlier detection work with real-valued datasets. The
    choice of distance metric may be a user-selected input in the case of parametric
    methods assuming multivariate distributions. In the case of non-parametric methods
    using frequency-based histograms, a user-defined threshold frequency is used.
    Selection of kernel method and bandwidth are also user-determined in Kernel Density
    Estimation techniques. The output from statistical-based methods is a score indicating
    outlierness.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most of the statistical-based outlier detections either assume a distribution
    or fit a distribution to the data to detect probabilistically the least likely
    data generated from the distribution. These methods have two distinct steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training step**: Here, an estimate of the model to fit the data is performed'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Testing step**: On each instance a goodness of fit is performed based on
    the model and the particular instance, yielding a score and the outlierness'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parametric-based methods assume a distribution model such as multivariate Gaussians
    and the training normally involves estimating the means and variance using techniques
    such as **Maximum Likelihood Estimates** (**MLE**). The testing typically includes
    techniques such as mean-variance or box-plot tests, accompanied by assumptions
    such as "if outside three standard deviations, then outlier".
  prefs: []
  type: TYPE_NORMAL
- en: 'A normal multivariate distribution can be estimated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_151.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with the mean **µ** and covariance Ʃ.
  prefs: []
  type: TYPE_NORMAL
- en: The Mahalanobis distance can be the estimate of the data point from the distribution
    given by the equation ![How does it work?](img/B05137_03_154.jpg). Some variants
    such as **Minimum Covariant Determinant** (**MCD**) are also used when Mahalanobis
    distance is affected by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A non-parametric method involves techniques such as constructing histograms
    for every feature using frequency or width-based methods. When the ratio of the
    data in a bin to that of the average over the histogram is below a user defined
    threshold, such a bin is termed sparse. A lower probability of feature results
    in a higher outlier score. The total outlier score can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_155.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w*[f] is the weight given to feature *f*, *p*[f] is the probability of
    the value of the feature in the test data point, and *F* is the sum of weights
    of the feature set. Kernel Density Estimations are also used in non-parametric
    methods using user-defined kernels and bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the model fits or distribution of the data is known, these methods are
    very efficient as you don't have to store entire data, just the key statistics
    for doing tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of distribution, however, can pose a big issue in parametric methods.
    Most non-parametric methods using kernel density estimates don't scale well with
    large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distance-based algorithms work under the general assumption that normal data
    has other data points closer to it while anomalous data is well isolated from
    its neighbors (*References* [20]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Distance-based techniques require natively numeric or categorical features to
    be transformed to numeric values. Inputs to distance-based methods are the distance
    metric used, the distance threshold ϵ, and π, the threshold fraction, which together
    determine if a point is an outlier. For KNN methods, the choice *k* is an input.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are many variants of distance-based outliers and we will discuss how
    each of them works at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'DB (*ϵ**, π*) Algorithms: Given a radius of *ϵ* and threshold of π, a data
    point is considered as an outlier if π percentage of points have distance to the
    point less than *ϵ*. There are further variants using nested loop structures,
    grid-based structures, and index-based structures on how the computation is done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*KNN*-based methods are also very common where the outlier score is computed
    either by taking the *KNN* distance to the point or the average distance to point
    from *{1NN,2NN,3NN…KNN}*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main advantage of distance-based algorithms is that they are non-parametric
    and make no assumptions on distributions and how to fit models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance calculations are straightforward and computed in parallel, helping
    the algorithms to scale on large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major issues with distance-based methods is the curse of dimensionality
    discussed in the first chapter; for large dimensional data, sparsity can lead
    to noisy outlierness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Density-based methods extend the distance-based methods by not only measuring
    the local density of the given point, but also the local densities of its neighborhood
    points. Thus, the relative factor added gives it the edge in finding more complex
    outliers that are local or global in nature, but at the added cost of computation.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Density-based algorithm must be supplied the minimum number of points *MinPts*
    in a neighborhood of input radius *ϵ* centered on an object that determines it
    is a core object in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will first discuss the **Loca** **Outlier Factor** (**LOF**) method and then
    discuss some variants of LOF [21].
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the *MinPts* as the parameter, LOF of a data point is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_168.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here |*N* *[MinPts]* *(p)*| is the number of data points in the neighborhood
    of point *p*, and *lrd* *[MinPts]* is the local reachability density of the point
    and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_171.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here ![How does it work?](img/B05137_03_172.jpg) is the reachability of the
    point and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_173.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the disadvantages of LOF is that it may miss outliers whose neighborhood
    density is close to that of its neighborhood. **Connectivity-based outliers**
    (**COF**) using set-based nearest path and set-based nearest trail originating
    from the data point are used to improve on LOF. COF treats the low-density region
    differently to the isolated region and overcomes the disadvantage of LOF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_175.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another disadvantage of LOF is that when clusters are in varying densities
    and not separated, LOF will generate counter-intuitive scores. One way to overcome
    this is to use the **influence space** (**IS**) of the points using KNNs and its
    reverse KNNs or RNNs. RNNs have the given point as one of their K nearest neighbors.
    Outlierness of the point is known as Influenced Outliers or INFLO and is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_176.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *den*(*p*) is the local density of *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_179.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Density-based outlier detection methods are particularly suited for
    finding local as well as global outliers'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been shown that density-based methods are more effective than distance-based
    methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based outlier detection has high computational cost and, often, poor
    interpretability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some believe that clustering techniques, where the goal is to find groups of
    data points located together, are in some sense antithetical to the problem of
    anomaly or outlier detection. However, as an advanced unsupervised learning technique,
    clustering analysis offers several methods to find interesting groups of clusters
    that are either located far off from other clusters or do not lie in any clusters
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As seen before, clustering techniques work well with real-valued data, although
    some categorical values translated to numeric values are tolerated. In the case
    of k-Means and k-Medoids, input values include the number of clusters *k* and
    the distance metric. Variants may require a threshold score to identify outlier
    groups. For Gaussian Mixture Models using EM, the number of mixture components
    must be supplied by the user. When using CBLOF, two user-defined parameters are
    expected: the size of small clusters and the size of large clusters. Depending
    on the algorithm used, individual or groups of objects are output as outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in the section on clustering, there are various types of clustering
    methods and we will give a few examples of how clustering algorithms have been
    extended for outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means or k-Medoids and their variants generally cluster data elements together
    and are affected by outliers or noise. Instead of preprocessing these data points
    by removal or transformation, such points that weaken the "tightness" of the clusters
    are considered outliers. Typically, outliers are revealed by a two-step process
    of first running clustering algorithms and then evaluating some form of outlier
    score that measures distance from point to centroid. Also, many variants treat
    clusters of size smaller than a threshold as an outlier group.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian mixture modeling** (**GMM**) using **Expectation maximization**
    (**EM**) is another well-known clustering-based outlier detection technique, where
    a data point that has low probability of belonging to a cluster becomes an outlier
    and the outlier score becomes the inverse of the EM probabilistic output score.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster-based Local Outlier Factor** (**CBLOF**) uses a two-stage process
    to find outliers. First, a clustering algorithm performs partitioning of data
    into clusters of various sizes. Using two user-defined parameters, size of large
    clusters, and size of small clusters, two sets of clusters are formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_215.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given that clustering-based techniques are well-understood, results are more
    interpretable and there are more tools available for these techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many clustering algorithms only detect clusters, and are less effective in unsupervised
    techniques compared to outlier algorithms that give scores or ranks or otherwise
    identify outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-dimensional-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key issues with distance-, density-, or even clustering-based methods,
    is the curse of dimensionality. As dimensions increase, the contrast between distances
    diminishes and the concept of neighborhood becomes less meaningful. The normal
    points in this case look like outliers and false positives increase by large volume.
    We will discuss some of the latest approaches taken in addressing this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algorithms that project data to lower-dimensional subspaces can handle missing
    data well. In these techniques, such as SOD, *ϕ*, the number of ranges in each
    dimension becomes an input (*References* [25]). When using an evolutionary algorithm,
    the number of cells with the lowest sparsity coefficients is another input parameter
    to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The broad idea to solve the high dimensional outlier issue is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Either have a robust distance metric coupled with all of the previous techniques,
    so that outliers can be identified in full dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or project data on to smaller subspaces and find outliers in the smaller subspaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Angle-based Outlier Degree** (**ABOD**) method uses the basic assumption
    that if a data point in high dimension is an outlier, all the vectors originating
    from it towards data points nearest to it will be in more or less the same direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_184.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The ABOD method of distinguishing outliers from inliers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a point *p*, and any two points *x* and *y*, the angle between the two
    points and *p* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_187.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Measure of variance used as the ABOD score is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_188.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The smaller the ABOD value, the smaller the measure of variance in the angle
    spectrum, and the larger the chance of the point being the outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method that has been very useful in high dimensional data is using
    the **Subspace Outlier Detection** (**SOD**) approach (*References* [23]). The
    idea is to partition the high dimensional space such that there are an equal number
    of ranges, say *ϕ*, in each of the *d* dimensions. Then the Sparsity Coefficient
    for a cell *C* formed by picking a range in each of the *d* dimensions is measured
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_192.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the total number of data points and *N(C)* is the number of data
    points in cell *C*. Generally, the data points lying in cells with negative sparsity
    coefficient are considered outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ABOD method is *O(n*³*)* with the number of data points and becomes impractical
    with larger datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sparsity coefficient method in subspaces requires efficient search in lower
    dimension and the problem becomes NP-Hard and some form of evolutionary or heuristic
    based search is employed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sparsity coefficient methods being NP-Hard can result in local optima.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-class SVM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many domains there is a particular class or category of interest and the
    "rest" do not matter. Finding a boundary around this class of interest is the
    basic idea behind one-class SVM (*References* [26]). The basic assumption is that
    all the points of the positive class (class of interest) cluster together while
    the other class elements are spread around and we can find a tight hyper-sphere
    around the clustered instances. SVM, which has great theoretical foundations and
    applications in binary classifications is reformulated to solve one-class SVM.
    The following figure illustrates how a nonlinear boundary is simplified by using
    one-class SVM with slack so as to not overfit complex functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![One-class SVM](img/B05137_03_197.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: One-Class SVM for nonlinear boundaries'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data inputs are generally numeric features. Many SVMs can take nominal features
    and apply binary transformations to them. Also needed are: marking the class of
    interest, SVM hyper-parameters such as kernel choice, kernel parameters and cost
    parameter, among others. Output is a SVM model that can predict whether instances
    belong to the class of interest or not. This is different from scoring models,
    which we have seen previously.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input is training instances {**x**[1],**x**[2]…**x**[n]} with certain instances
    marked to be in class +1 and rest in -1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to SVM also needs a kernel that does the transformation *ϕ* from
    input space to features space as ![How does it work?](img/B05137_03_199.jpg) using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_200.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a hyper-sphere that bounds the classes using SVM reformulated equation
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_03_201.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Such that ![How does it work?](img/B05137_03_202.jpg)+![How does it work?](img/B05137_03_203.jpg),
    ![How does it work?](img/B05137_03_204.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: '*R* is the radius of the hyper-sphere with center **c** and *ν* ∈ (0,1] represents
    an upper bound on the fraction of the data that are outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: As in normal SVM, we perform optimization using quadratic programming is done
    to obtain the solution as the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key advantage to using one-class SVM—as is true of binary SVM—is the many
    theoretical guarantees in error and generalization bounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-dimensional data can be easily mapped in one-class SVM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear SVM with kernels can even find non-spherical shapes to bound the
    clusters of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training cost in space and memory increases as the size of the data increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter tuning, especially the kernel parameters and the cost parameter tuning
    with unlabeled data is a big challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlier evaluation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measuring outliers in terms of labels, ranks, and scores is an area of active
    research. When the labels or the ground truth is known, the idea of evaluation
    becomes much easier as the outlier class is known and standard metrics can be
    employed. But when the ground truth is not known, the evaluation and validation
    methods are very subjective and there is no well-defined, rigorous statistical
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In cases where the ground truth is known, the evaluation of outlier algorithms
    is basically the task of finding the best thresholds for outlier scores (scoring-based
    outliers).
  prefs: []
  type: TYPE_NORMAL
- en: The balance between reducing the false positives and improving true positives
    is the key concept and Precision-Recall curves (described in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*) are used to find the best optimum
    threshold. Confidence score, predictions, and actual labels are used in supervised
    learning to plot PRCurves, and instead of confidence scores, outlier scores are
    ranked and used here. ROC curves and area under curves are also used in many applications
    to evaluate thresholds. Comparing two or more algorithms and selection of the
    best can also be done using area under curve metrics when the ground truth is
    known.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most real-world cases, knowing the ground truth is very difficult, at least
    during the modeling task. Hawkins describes the evaluation method in this case
    at a very high level as "a sample containing outliers would show up such characteristics
    as large gaps between 'outlying' and 'inlying' observations and the deviation
    between outliers and the group of inliers, as measured on some suitably standardized
    scale".
  prefs: []
  type: TYPE_NORMAL
- en: 'The general technique used in evaluating outliers when the ground truth is
    not known is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Histogram of outlier scores**: A visualization-based method, where outlier
    scores are grouped into predefined bins and users can select thresholds based
    on outlier counts, scores, and thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Score normalization and distance functions**: In this technique, some form
    of normalization is done to make sure all outlier algorithms that produce scores
    have the same ranges. Some form of distance or similarity or correlation based
    method is used to find commonality of outliers across different algorithms. The
    general intuition here is: the more the algorithms that weigh the data point as
    outlier, the higher the probability of that point actually being an outlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we present a case study that illustrates how to apply clustering and outlier
    techniques described in this chapter in the real world, using open-source Java
    frameworks and a well-known image dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tools and software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now introduce two new tools that were used in the experiments for this
    chapter: SMILE and Elki. SMILE features a Java API that was used to illustrate
    feature reduction using PCA, Random Projection, and IsoMap. Subsequently, the
    graphical interface of Elki was used to perform unsupervised learning—specifically,
    clustering and outlier detection. Elki comes with a rich set of algorithms for
    cluster analysis and outlier detection including a large number of model evaluators
    to choose from.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Find out more about SMILE at: [http://haifengl.github.io/smile/](http://haifengl.github.io/smile/)
    and to learn more about Elki, visit [http://elki.dbs.ifi.lmu.de/](http://elki.dbs.ifi.lmu.de/).'
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Character-recognition is a problem that occurs in many business areas, for example,
    the translation of medical reports and hospital charts, postal code recognition
    in the postal service, check deposit service in retail banking, and others. Human
    handwriting can vary widely among individuals. Here, we are looking exclusively
    at handwritten digits, 0 to 9\. The problem is made interesting due to the verisimilitude
    within certain sets of digits, such as 1/2/7 and 6/9/0\. In our experiments in
    this chapter we use clustering and outlier analysis using several different algorithms
    to illustrate the relative strengths and weaknesses of the methods. Given the
    widespread use of these techniques in data mining applications, our main focus
    is to gain insights into the data and the algorithms and evaluation measures;
    we do not apply the models for prediction on test data.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As suggested by the title of the chapter, our experiments aim to demonstrate
    Unsupervised Learning by ignoring the labels identifying the digits in the dataset.
    Having learned from the dataset, clustering and outlier analyses can yield invaluable
    information for describing patterns in the data, and are often used to explore
    these patterns and inter-relationships in the data, and not just to predict the
    class of unseen data. In the experiments described here, we are concerned with
    description and exploration rather than prediction. Labels are used when available
    by external evaluation measures, as they are in these experiments as well.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is already done for us. For details on how the data was collected, see:
    The MNIST database: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each feature in a data point is the greyscale value of one of 784 pixels. Consequently,
    the type of all features is numeric; there are no categorical types except for
    the class attribute, which is a numeral in the range 0-9\. Moreover, there are
    no missing data elements in the dataset. Here is a table with some basic statistics
    for a few pixels. The images are pre-centred in the 28 x 28 box so in most examples,
    the data along the borders of the box are zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Average | Std Dev | Min | Max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pixel300 | 94.25883 | 109.117 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel301 | 72.778 | 103.0266 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel302 | 49.06167 | 90.68359 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel303 | 28.0685 | 70.38963 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel304 | 12.84683 | 49.01016 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel305 | 4.0885 | 27.21033 | 0 | 255 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel306 | 1.147 | 14.44462 | 0 | 254 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel307 | 0.201667 | 6.225763 | 0 | 254 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel308 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel309 | 0.009167 | 0.710047 | 0 | 55 |'
  prefs: []
  type: TYPE_TB
- en: '| pixel310 | 0.102667 | 4.060198 | 0 | 237 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 1: Summary of features from the original dataset before pre-processing*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **Mixed National Institute of Standards and Technology** (**MNIST**) dataset
    is a widely used dataset for evaluating unsupervised learning methods. The MNIST
    dataset is mainly chosen because the clusters in high dimensional data are not
    well separated.
  prefs: []
  type: TYPE_NORMAL
- en: The original MNIST dataset had black and white images from NIST. They were normalized
    to fit in a 20 x 20 pixel box while maintaining the aspect ratio. The images were
    centered in a 28 x 28 image by computing the center of mass and translating it
    to position it at the center of the 28 x 28 dimension grid.
  prefs: []
  type: TYPE_NORMAL
- en: Each pixel is in a range from 0 to 255 based on the intensity. The 784 pixel
    values are flattened out and become a high dimensional feature set for each image.
    The following figure depicts a sample digit 3 from the data, with mapping to the
    grid where each pixel has an integer value from 0 to 255.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments described in this section are intended to show the application
    of unsupervised learning techniques to a well-known dataset. As was done in [Chapter
    2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised Learning"),
    *Practical Approach to Real-World Supervised Learning* with supervised learning
    techniques, multiple experiments were carried out using several clustering and
    outlier methods. Results from experiments with and without feature reduction are
    presented for each of the selected methods followed by an analysis of the results.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since our focus is on exploring the dataset using various unsupervised techniques
    and not on the predictive aspect, we are not concerned with train, validation,
    and test samples here. Instead, we use the entire dataset to train the models
    to perform clustering analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of outlier detection, we create a reduced sample of only two classes
    of data, namely, 1 and 7\. The choice of a dataset with two similarly shaped digits
    was made in order to set up a problem space in which the discriminating power
    of the various anomaly detection techniques would stand out in greater relief.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis and dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We demonstrate different feature analysis and dimensionality reduction methods—PCA,
    Random Projection, and IsoMap—using the Java API of the SMILE machine learning
    toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature analysis and dimensionality reduction](img/B05137_03_209.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Showing digit 3 with pixel values distributed in a 28 by 28 matrix
    ranging from 0 to 254.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for loading the dataset and reading the values is given here along
    with inline comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following snippet illustrates dimensionality reduction achieved using the
    API for PCA support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA](img/B05137_03_210.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: PCA on MNIST – On the left, we see that over 90 percent of variance
    in data is accounted for by fewer than half the original number of features; on
    the right, a representation of the data using the first two principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of set of 11 random features after PCA'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PCA computation reduces the number of features to 274\. In the following
    table you can see basic statistics for a randomly selected set of features. Feature
    data has been normalized as part of the PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Features | Average | Std Dev | Min | Max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 2.982922 | -35.0821 | 19.73339 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 2.415088 | -32.6218 | 31.63361 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 2.165878 | -21.4073 | 16.50271 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 1.78834 | -27.537 | 31.52653 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0 | 1.652688 | -21.4661 | 22.62837 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0 | 1.231167 | -15.157 | 10.19708 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0 | 0.861705 | -6.04737 | 7.220233 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0 | 0.631403 | -6.80167 | 3.633182 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0 | 0.606252 | -5.46206 | 4.118598 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0 | 0.578355 | -4.21456 | 3.621186 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0 | 0.528816 | -3.48564 | 3.896156 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 2: Summary of set of 11 random features after PCA*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Random projections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we illustrate the straightforward usage of the API for performing data
    transformation using random projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Random projections](img/B05137_03_211.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: PCA and Random projection - representations in two dimensions using
    Smile API'
  prefs: []
  type: TYPE_NORMAL
- en: ISOMAP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This code snippet illustrates use of the API for Isomap transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![ISOMAP](img/B05137_03_212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: IsoMap – representation in two dimensions with k = 10 using Smile
    API'
  prefs: []
  type: TYPE_NORMAL
- en: Observations on feature analysis and dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can make the following observations from the results shown in the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: The PCA variance and number of dimensions plot clearly shows that around 100
    linearly combined features have a similar representation or variance in the data
    (> 95%) as that of the 784 original features. This is the key first step in any
    unsupervised feature reduction analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even PCA with two dimensions and not 100 as described previously shows some
    really good insights in the scatterplot visualization. Clearly, digits 2, 8, and
    4 are very well separated from each other and that makes sense as they are written
    quite distinctly from each other. Digits such as {1,7}, {3,0,5}, and {1,9} in
    the low dimensional space are either overlapping or tightly clustered. This shows
    that with just two features it is not possible to discriminate effectively. It
    also shows that there is overlap in the characteristics or features amongst these
    classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next plot comparing PCA with Random Projections, both done in lower dimension
    of 2, shows that there is much in common between the outputs. Both have similar
    separation for distinct classes as described in PCA previously. It is interesting
    to note that PCA does much better in separating digits {8,9,4}, for example, than
    Random Projections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isomap, the next plot, shows good discrimination, similar to PCA. Subjectively,
    it seems to be separating the data better than Random Projections. Visually, for
    instance, {3,0,5} is better separated out in Isomap than PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two sets of experiments were conducted using the MNIST-6000 dataset. The dataset
    consists of 6,000 examples, each of which represents a hand-written digit as greyscale
    values of a 28 x 28 square of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: First, we run some clustering techniques to identify the 10 clusters of digits.
    For the experiments in this part of the case study, we use the software Elki.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first set of experiments, there is no feature-reduction involved. All
    28x28 pixels are used. Clustering techniques including k-Means, EM (Diagonal Gaussian
    Model Factory), DBSCAN, Hierarchical (HDBSCAN Hierarchy Extraction), as well as
    Affinity Propagation were used. In each case, we use metrics from two internal
    evaluators: Davies Bouldin and Silhouette, and several external evaluators: Precision,
    Recall, F1 measure, and Rand Index.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering models, results, and evaluation](img/B05137_03_213.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: K-Means – using Sum of Squared Errors (SSE) to find optimal *k*,
    the number of clusters. An elbow in the curve, which is typically used to pick
    the optimal k value, is not particularly detectable in the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of k-Means, we did several runs using a range of k values. The plot
    shows that the Sum of Squared Errors (SSE) metric decreases with k.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table shows results for *k=10* and ranks for each are in parentheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Silhouette | Davies-Bouldin Index | Precision | Recall | F1 |
    Rand |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| K-Means Lloyd | +-0.09 0.0737 (1) | 2.8489 (3) | 0.4463 (3) | 0.47843 (3)
    | 0.4618 (1) | 0.8881 (3) |'
  prefs: []
  type: TYPE_TB
- en: '| EM (Diagonal Gaussian Model Factory) | NaN | 0 (1) | 0.1002 (6) | 1 (1) |
    0.1822 (4) | 0.1003 (5) |'
  prefs: []
  type: TYPE_TB
- en: '| DBSCAN | 0 (4) | 0 (1) | 0.1003 (5) | 1 (1) | 0.1823 (3) | 0.1003 (5) |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical (HDBSCAN Hierarchy Extraction) | +-0.05 0.0435 (3) | 2.7294
    | 0.1632 (4) | 0.9151 (2) | 0.2770 (2) | 0.5211 (4) |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical (Simplified Hierarchy Extraction) | NaN | 0 (1) | 1 (1) | 0.0017
    (5) | 0.0033 (6) | 0.8999 (2) |'
  prefs: []
  type: TYPE_TB
- en: '| Affinity Propagation | +-0.07 0.04690 (2) | 1.7872 (2) | 0.8279 (2) | 0.0281
    (4) | 0.0543 (5) | 0.9019 (1) |'
  prefs: []
  type: TYPE_TB
- en: '*Table 3\. Evaluation of clustering algorithms for MNIST data*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the second clustering experiment, the dataset was first pre-processed using
    PCA, and the resulting data with 273 features per example was used with the same
    algorithms as in the first experiment. The results are shown in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Silhouette | Davies-Bouldin Index | Precision | Recall | F1 |
    Rand |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| K-Means Lloyd | +-0.14 0.0119 | 3.1830 | 0.3456 | 0.4418 | 0.3878 (1) | 0.8601
    |'
  prefs: []
  type: TYPE_TB
- en: '| EM (Diagonal Gaussian Model Factory) | +-0.16 -0.0402 | 3.5429 | 0.1808 |
    0.3670 | 0.2422 | 0.7697 |'
  prefs: []
  type: TYPE_TB
- en: '| DBSCAN | +-0.13 -0.0351 | 1.3236 | 0.1078 | 0.9395 (1) | 0.1934 | 0.2143
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical (HDBSCAN Hierarchy Extraction) | +-0.05 0.7920 (1) | 0.0968
    | 0.1003 | 0.9996 | 0.1823 | 0.1005 |'
  prefs: []
  type: TYPE_TB
- en: '| Affinity Propagation | +-0.09 0.0575 | 1.6296 | 0.6130 (1) | 0.0311 | 0.0592
    | 0.9009 (1) |'
  prefs: []
  type: TYPE_TB
- en: '| Subspace (DOC) | +-0.00 0.0 | 0 (1) | 0.1003 | 1 | 0.1823 | 0.1003 |'
  prefs: []
  type: TYPE_TB
- en: '*Table 4\. Evaluation of clustering algorithms for MNIST data after PCA*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Observations and clustering analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in tables 2.1 and 2.2, different algorithms discussed in the sections
    on clustering are compared using different evaluation measures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, comparing different internal and external measures based on technical,
    domain and business requirements is very important. When labels or outcomes are
    available in the dataset, using external measures becomes an easier choice. When
    labeled data is not available, the norm is to use internal measures with some
    ranking for each and looking at comparative ranking across all measures. The important
    and often interesting observations are made at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of k-Means with varying *k*, (shown in the figure)
    using a measure such as Sum of Squared Errors, is the basic step to see "optimality"
    of number of clusters. The figure clearly shows that as *k* increases the score
    improves as cluster separation improves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we analyze Table 2.1 where all 784 features were used and all evaluation
    measures for the different algorithms are shown, some key things stand out:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-Means and Affinity Propagation both show a large overlap in the Silhouette
    index in terms of standard deviation and average, respectively (k-Means +-0.09
    0.0737; Affinity Propagation +-0.07 0.04690). Hence it is difficult to analyze
    them on this metric.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the measures such as DB Index (minimal is good), Rand Index (closer to 1
    is good), we see that Affinity Propagation and Hierarchical Clustering show very
    good results.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the measures where the labels are taken into account, Hierarchical Clustering,
    DBSCAN, and EM has either high Precision or high Recall and consequently, the
    F1 measure is low. k-Means gives the highest F1 measure when precision and recall
    are taken into consideration.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Table 2.2 where the dataset with 273 features—reduced using PCA with 95%
    variance retained—is run through the same algorithms and evaluated by the same
    measures, we make the following interesting observations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By reducing the features there is a negative impact on every measure for certain
    algorithms; for example, all the measures of k-Means degrade. An algorithm such
    as Affinity Propagation has a very low impact and in some cases even a positive
    impact when using reduced features. When compared to the results where all the
    features were used, AP shows similar Rand Index and F1, better Recall, DB Index
    and Silhouette measures, and small changes in Precision, demonstrating clear robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering shows similar results as before in terms of better DB
    index and Rand Index, and scores close to AP in Rand Index.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the outlier detection techniques, we used a subset of the original dataset
    containing all examples of digit 1 and an under-sampled subset of digit 7 examples.
    The idea is that the similarity in shape of the two digits would cause the digit
    7 examples to be found to be outliers.
  prefs: []
  type: TYPE_NORMAL
- en: The models used were selected from Angular, Distance-based, clustering, LOF,
    and One-Class SVM.
  prefs: []
  type: TYPE_NORMAL
- en: The outlier metrics used in the evaluation were ROC AUC, Average Precision,
    R-Precision, and Maximum F1 measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the results obtained, with ranks in parentheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | ROC AUC | Avg. Precision | R-Precision | Maximum F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Angular (ABOD) | 0.9515 (3) | 0.1908 (4) | 0.24 (4) | 0.3298 (4) |'
  prefs: []
  type: TYPE_TB
- en: '| Distance-based (KNN Outlier) | 0.9863 (1) | 0.4312 (3) | 0.4533 (3) | 0.4545
    (3) |'
  prefs: []
  type: TYPE_TB
- en: '| Distance Based (Local Isolation Coefficient) | 0.9863 (1) | 0.4312 (3) |
    0.4533 (3) | 0.4545 (3) |'
  prefs: []
  type: TYPE_TB
- en: '| Clustering (EM Outlier) | 0.5 (5) | 0.97823827 (1) | 0.989 (1) | 0.9945 (1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LOF | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
  prefs: []
  type: TYPE_TB
- en: '| LOF (ALOKI) | 0.5 (5) | 0.0110 (7) | 0.0110 (7) | 0.0218 (7) |'
  prefs: []
  type: TYPE_TB
- en: '| LOF (COF) | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
  prefs: []
  type: TYPE_TB
- en: '| One-Class SVM (RBF) | 0.9820 (2) | 0.5637 (2) | 0.5333 (2) | 0.5697 (2) |'
  prefs: []
  type: TYPE_TB
- en: '| One-Class SVM (Linear) | 0.8298 (4) | 0.1137 (5) | 0.16 (5) | 0.1770 (5)
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 5 Evaluation measures of Outlier analysis algorithms*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Observations and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the same way as we evaluated different clustering methods, we used several
    observations to compare a number of outlier algorithms. Once again, the right
    methodology is to judge an algorithm based on ranking across all the metrics and
    then getting a sense of how it does across the board as compared to other algorithms.
    The outlier metrics used here are all standard external measures used to compare
    outlier algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to see with the right parameters, that is, *k=2*, EM can find
    the right distribution and find outliers more efficiently than most. It ranks
    very high and is first among the important metrics that include Maximum F1, R-Precision,
    and Avg. Precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1-Class SVM with non-linear RBF Kernel does consistently well across most measures,
    that is, ranks second best in ROC area, R-Precision and Avg. Precision, and Maximum
    F1\. The difference between Linear SVM, which ranks about fifth in most rankings
    and 1-Class SVM, which ranks second shows that the problem is indeed nonlinear
    in nature. Generally, when the dimensions are high (784), and outliers are nonlinear
    and rare, 1-Class SVM with kernels do really well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local outlier-based techniques (LOF and its variants) are consistently ranked
    lower in almost all the measures. This gives the insight that the outlier problem
    may not be local, but rather global. Distance-based algorithms (KNN and Local
    Isolation) perform the best in ROC area under the curve and better than local
    outlier-based, even though using distance-based metrics gives the insight that
    the problem is indeed global and suited for distance-based measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Table 1: Summary of features from the original dataset before pre-processing'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both supervised and unsupervised learning methods share common concerns with
    respect to noisy data, high dimensionality, and demands on memory and time as
    the size of data grows. Other issues peculiar to unsupervised learning, due to
    the lack of ground truth, are questions relating to subjectivity in the evaluation
    of models and their interpretability, effect of cluster boundaries, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Feature reduction is an important preprocessing step that mitigates the scalability
    problem, in addition to presenting other advantages. Linear methods such as PCA,
    Random Projection, and MDS, each have specific benefits and limitations, and we
    must be aware of the assumptions inherent in each. Nonlinear feature reduction
    methods include KPCA and Manifold learning.
  prefs: []
  type: TYPE_NORMAL
- en: Among clustering algorithms, k-Means is a centroid-based technique initialized
    by selecting the number of clusters and it is sensitive to the initial choice
    of centroids. DBSCAN is one of the density-based algorithms that does not need
    initializing with number of clusters and is robust against noise and outliers.
    Among the probabilistic-based techniques are Mean Shift, which is deterministic
    and robust to noise, and EM/GMM, which performs well with all types of features.
    Both Mean Shift and EM/GMM tend to have scalability problems.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering is a powerful method involving building binary trees
    that iteratively groups data points until a similarity threshold is reached. Tolerance
    to noise depends on the similarity metric used. SOM is a two-layer neural network,
    allowing visualization of clusters in a 2-D grid. Spectral clustering treats the
    dataset as a connected graph and identifies clusters by graph partitioning. Affinity
    propagation, another graph-based technique, uses message passing between data
    points as affinities to detect clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The validity and usefulness of clustering algorithms is demonstrated using various
    validation and evaluation measures. Internal measures have no access to ground
    truth; when labels are available, external measures can be used. Examples of internal
    measures are Silhouette index and Davies-Bouldin index. Rand index and F-measure
    are external evaluation measures.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier and anomaly detection is an important area of unsupervised learning.
    Techniques are categorized as Statistical-based, Distance-based, Density-based,
    Clustering-based, High-dimensional-based, and One Class SVM. Outlier evaluation
    techniques include supervised evaluation, where ground truth is known, and unsupervised
    evaluation, when ground truth is not known.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments using the SMILE Java API and Elki toolkit illustrate the use of
    the various clustering and outlier detection techniques on the MNIST6000 handwritten
    digits dataset. Results from different evaluation techniques are presented and
    compared.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K. Pearson (1901). *On lines and planes of closest fit to systems of points
    in space*. Philosophical Magazine, 2:559–572.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. D. Back (1997). "*A first application of independent component analysis to
    extracting structure from stock returns*," Neural Systems, vol. 8, no. 4, pp.
    473–484.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tipping ME, Bishop CM (1999). *Probabilistic principal component analysis*.
    Journal of the Royal Statistical Society, Series B, 61(3):611–622\. 10.1111/1467-9868.00196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sanjoy Dasgupta (2000). *Experiments with random projection*. In Proceedings
    of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00),
    Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc.,
    San Francisco, CA, USA, 143-151.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Cox and M. Cox (2001). *Multidimensional Scaling*. Chapman Hall, Boca Raton,
    2nd edition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller (1999). *Kernel
    principal component analysis*. In Advances in kernel methods, MIT Press, Cambridge,
    MA, USA 327-352.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tenenbaum, J.B.; De Silva, V.; & Langford, J.C (2000).*A global geometric framework
    for nonlinear dimensionality reduction*. Science. Vol. 290, Issue 5500, pp. 2319-2323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M. Belkin and P. Niyogi (2003). *Laplacian eigenmaps for dimensionality reduction
    and data representation*. Neural Computation, 15(6):1373–1396.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: S. Roweis and L. Saul (2000). *Nonlinear dimensionality reduction by locally
    linear embedding*. Science, 290:2323–2326.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hartigan, J. and Wong, M (1979). *Algorithm AS136: A k-means clustering algorithm*.
    Applied Statistics, 28, 100-108.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dorin Comaniciu and Peter Meer (2002). *Mean Shift: A robust approach toward
    feature space analysis*. IEEE Transactions on Pattern Analysis and Machine Intelligence
    pp. 603-619.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical Clustering Jain, A. and Dubes, R (1988). *Algorithms for Clustering
    Data*. Prentice-Hall, Englewood Cliffs, NJ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mclachlan, G. and Basford, K (1988). *Mixture Models: Inference and Applications
    to Clustering*. Marcel Dekker, New York, NY'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ester, M., Kriegel, H-P., Sander, J. and Xu, X (1996). *A density-based algorithm
    for discovering clusters in large spatial databases with noise*. In Proceedings
    of the 2^(nd) ACM SIGKDD, 226-231, Portland, Oregon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Y. Ng, M. I. Jordan, and Y. Weiss (2001). *On spectral clustering: Analysis
    and an algorithm*, in Advances in Neural Information Processing Systems. MIT Press,
    pp. 849–856.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delbert Dueck and Brendan J. Frey (2007). *Non-metric affinity propagation for
    unsupervised image categorization*. In IEEE Int. Conf. Computer Vision (ICCV),
    pages 1–8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Teuvo Kohonen (2001). *Self-Organizing Map*. Springer, Berlin, Heidelberg. 1995.Third,
    Extended Edition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M. Halkidi, Y. Batistakis, and M. Vazirgiannis (2001). *On clustering validation
    techniques*, J. Intell. Inf. Syst., vol. 17, pp. 107–145.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Markou, S. Singh (2003). *Novelty detection: a review – part 1: statistical
    approaches*, Signal Process. 83 (12) 2481–2497'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Byers, S. D. AND Raftery, A. E (1998). *Nearest neighbor clutter removal for
    estimating features in spatial point processes*. J. Amer. Statis. Assoc. 93, 577–584.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Breunig, M. M., Kriegel, H.-P., Ng, R. T., AND Sander, J (1999). *Optics-of:
    Identifying local outliers*. In Proceedings of the 3rd European Conference on
    Principles of Data Mining and Knowledge Discovery. Springer-Verlag, 262–270.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Brito, M. R., Chavez, E. L., Quiroz, A. J., AND yukich, J. E (1997). *Connectivity
    of the mutual k-nearest neighbor graph in clustering and outlier detection*. Statis.
    Prob. Lett. 35, 1, 33–42.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggarwal C and Yu P S (2000). *Outlier detection for high dimensional data*.
    In Proc ACM SIGMOD International Conference on Management of Data (SIGMOD), Dallas,
    TX.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ghoting, A., Parthasarathy, S., and Otey, M (2006). *Fast mining of distance-based
    outliers in high dimensional spaces* In Proceedings SIAM Int Conf on Data Mining
    (SDM) Bethesda ML dimensional spaces. In Proc. SIAM Int. Conf. on Data Mining
    (SDM), Bethesda, ML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kriegel, H.-P., Schubert, M., and Zimek, A (2008). *Angle-based outlier detection*,
    In Proceedings ACM SIGKDD Int. Conf on Knowledge Discovery and Data Mining (SIGKDD)
    Las Vegas NV Conf. on Knowledge Discovery and Data Mining (SIGKDD), Las Vegas,
    NV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schoelkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., AND Williamson,
    R. C (2001). *Estimating the support of a high-dimensional distribution*. Neural
    Comput. 13, 7, 1443–1471.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'F Pedregosa, et al. *Scikit-learn: Machine learning in Python*. Journal of
    Machine Learning Research, 2825-2830.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
