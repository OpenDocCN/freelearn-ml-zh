<html><head></head><body>
  <div id="_idContainer105" class="Basic-Text-Frame">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-57" class="chapterTitle">Interpretation Challenges</h1>
    <p class="normal">In this chapter, we will discuss the traditional methods used for machine learning interpretation for both regression and classification. This includes model performance evaluation methods such as RMSE, R-squared, AUC, ROC curves, and the many metrics derived from confusion matrices. We will then examine the limitations of these performance metrics and explain what exactly makes “white-box” models intrinsically interpretable and why we cannot always use white-box models. To answer these questions, we’ll consider the trade-off between prediction performance and model interpretability. Finally, we will discover some new “glass-box” models such as <strong class="keyWord">Explainable Boosting Machines</strong> (<strong class="keyWord">EBMs</strong>) and GAMI-Net that attempt to not compromise on this trade-off between predictive performance and interpretability.</p>
    <p class="normal">The following are the main topics that will be covered in this chapter:</p>
    <ul>
      <li class="bulletList">Reviewing traditional model interpretation methods</li>
      <li class="bulletList">Understanding the limitations of traditional model interpretation methods</li>
      <li class="bulletList">Studying intrinsically interpretable (white-box) models</li>
      <li class="bulletList">Recognizing the trade-off between performance and interpretability</li>
      <li class="bulletList">Discovering newer interpretable (glass-box) models</li>
    </ul>
    <h1 id="_idParaDest-58" class="heading-1">Technical requirements</h1>
    <p class="normal">From <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, onward, we are using a custom <code class="inlineCode">mldatasets</code> library to load our datasets. Instructions on how to install this library can be found in the <em class="italic">Preface</em>. In addition to <code class="inlineCode">mldatasets</code>, this chapter’s examples also use the <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">rulefit</code>, <code class="inlineCode">interpret</code>, <code class="inlineCode">statsmodels</code>, <code class="inlineCode">matplotlib</code>, and <code class="inlineCode">gaminet</code> libraries. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="http://packt.link/swCyB"><span class="url">packt.link/swCyB</span></a>.</p>
    </div>
    <h1 id="_idParaDest-59" class="heading-1">The mission</h1>
    <p class="normal">Picture <a id="_idIndexMarker158"/>yourself, a data science consultant, in a conference room in Fort Worth, Texas, during early January 2019. In this conference room, executives for one of the world’s largest airlines, <strong class="keyWord">American Airlines</strong> (<strong class="keyWord">AA</strong>), are<a id="_idIndexMarker159"/> briefing you on their <strong class="keyWord">On-Time Performance</strong> (<strong class="keyWord">OTP</strong>). OTP is<a id="_idIndexMarker160"/> a widely accepted <strong class="keyWord">Key Performance Indicator</strong> (<strong class="keyWord">KPI</strong>) for<a id="_idIndexMarker161"/> flight punctuality. It is measured as the percentage of flights that arrived within 15 minutes of the scheduled arrival. It turns out that AA has achieved an OTP of just over 80% for 3 years in a row, which is acceptable, and a significant improvement, but they are still ninth in the world and fifth in North America. To brag about it next year in their advertising, they aspire to achieve, at least, number one in North America for 2019, besting their biggest rivals.</p>
    <p class="normal">On the financial front, it is estimated that delays cost the airline close to $2 billion, so reducing this by 25–35% to be on parity with their competitors could produce sizable savings. And it is estimated that it costs passengers just as much due to tens of millions of lost hours. A reduction in delays would result in happier customers, which could lead to an increase in ticket sales.</p>
    <p class="normal">Your task is to create models that can accurately predict delays for domestic flights only. What they hope to gain from the models is the following:</p>
    <ul>
      <li class="bulletList">To understand what factors impacted domestic arrival delays the most in 2018</li>
      <li class="bulletList">To anticipate a delay caused by the airline in midair with enough accuracy to mitigate some of these factors in 2019</li>
    </ul>
    <p class="normal">But not <a id="_idIndexMarker162"/>all delays are made equal. The <strong class="keyWord">International Air Transport Association</strong> (<strong class="keyWord">IATA</strong>) has over 80 delay codes ranging from 14 (<em class="italic">oversales booking errors</em>) to 75 (<em class="italic">de-icing of aircraft, removal of ice/snow, frost prevention</em>). Some are preventable, and others unavoidable.</p>
    <p class="normal">The airline executives told you that the airline is not, for now, interested in predicting delays caused by events out of their control, such as extreme weather, security events, and air traffic control issues. They are also not interested in delays caused by late arrivals from previous flights using the same aircraft because this was not the root cause. Nevertheless, they would like to know the effect of a busy hub on avoidable delays even if <a id="_idIndexMarker163"/>this has to do with congestion because, after all, perhaps there’s something they can do with flight scheduling or flight speed, or even gate selection. And while they understand that international flights occasionally impact domestic flights, they hope to tackle the sizeable local market first.</p>
    <p class="normal">Executives have provided you with a dataset from the United States Department of Transportation <em class="italic">Bureau of Transportation Statistics</em> with all 2018 AA domestic flights.</p>
    <h1 id="_idParaDest-60" class="heading-1">The approach</h1>
    <p class="normal">Upon <a id="_idIndexMarker164"/>careful consideration, you have decided to approach this both as a regression problem and a classification problem. Therefore, you will produce models that predict minutes delayed as well as models that classify whether flights were delayed by more than 15 minutes. For interpretation, using both will enable you to use a wider variety of methods and expand your interpretation accordingly. So we will approach this example by taking the following steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Predicting minutes delayed with various regression methods</li>
      <li class="numberedList">Classifying flights as delayed or not delayed with various classification methods</li>
    </ol>
    <p class="normal">These steps in the <em class="italic">Reviewing traditional model interpretation methods</em> section are followed by conclusions spread out in the rest of the sections of this chapter.</p>
    <h1 id="_idParaDest-61" class="heading-1">The preparations</h1>
    <p class="normal">You will <a id="_idIndexMarker165"/>find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb</span></a>.</p>
    <h1 id="_idParaDest-62" class="heading-1">Loading the libraries</h1>
    <p class="normal">To run this example, you<a id="_idIndexMarker166"/> need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn), <code class="inlineCode">rulefit</code>, <code class="inlineCode">statsmodels</code>, <code class="inlineCode">interpret</code>, <code class="inlineCode">tf</code>, and <code class="inlineCode">gaminet</code> to fit models and calculate performance metrics</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code> to create visualizations</li>
    </ul>
    <p class="normal">Load these libraries as seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures, StandardScaler,\
                                  MinMaxScaler
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics, linear_model, tree, naive_bayes,\
                    neighbors, ensemble, neural_network, svm
<span class="hljs-keyword">from</span> rulefit <span class="hljs-keyword">import</span> RuleFit
<span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
<span class="hljs-keyword">from</span> interpret.glassbox <span class="hljs-keyword">import</span> ExplainableBoostingClassifier
<span class="hljs-keyword">from</span> interpret <span class="hljs-keyword">import</span> show
<span class="hljs-keyword">from</span> interpret.perf <span class="hljs-keyword">import</span> ROC
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> gaminet <span class="hljs-keyword">import</span> GAMINet
<span class="hljs-keyword">from</span> gaminet.utils <span class="hljs-keyword">import</span> plot_trajectory, plot_regularization,\
                    local_visualize, global_visualize_density,\
                    feature_importance_visualize
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
    <h2 id="_idParaDest-63" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We then <a id="_idIndexMarker167"/>load the data as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df = mldatasets.load(<span class="hljs-string">"aa-domestic-delays-2018"</span>)
</code></pre>
    <p class="normal">There should be nearly 900,000 records and 23 columns. We can take a peek at what was loaded like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(aad18_df.info())
</code></pre>
    <p class="normal">The <a id="_idIndexMarker168"/>following is the output:</p>
    <pre class="programlisting con"><code class="hljs-con">RangeIndex: 899527 entries, 0 to 899526<br/>Data columns (total 23 columns):
FL_NUM                  899527 non-null int64
ORIGIN                  899527 non-null object
DEST                    899527 non-null object
PLANNED_DEP_DATETIME    899527 non-null object
CRS_DEP_TIME            899527 non-null int64
DEP_TIME                899527 non-null float64
DEP_DELAY               899527 non-null float64
DEP_AFPH                899527 non-null float64
DEP_RFPH                899527 non-null float64
TAXI_OUT                899527 non-null float64
WHEELS_OFF              899527 non-null float64
    :          :  :    :
WEATHER_DELAY           899527 non-null float64
NAS_DELAY               899527 non-null float64
SECURITY_DELAY          899527 non-null float64
LATE_AIRCRAFT_DELAY     899527 non-null float64
dtypes: float64(17), int64(3), object(3)
</code></pre>
    <p class="normal">Everything seems to be in order because all columns are there and there are no <code class="inlineCode">null</code> values.</p>
    <h3 id="_idParaDest-64" class="heading-3">The data dictionary</h3>
    <p class="normal">Let’s <a id="_idIndexMarker169"/>examine the data dictionary.</p>
    <p class="normal">General features are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">FL_NUM</code>: Flight number.</li>
      <li class="bulletList"><code class="inlineCode">ORIGIN</code>: Starting airport code (IATA).</li>
      <li class="bulletList"><code class="inlineCode">DEST</code>: Destination airport code (IATA).</li>
    </ul>
    <p class="normal">Departure features are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">PLANNED_DEP_DATETIME</code>: The planned date and time of the flight.</li>
      <li class="bulletList"><code class="inlineCode">CRS_DEP_TIME</code>: The planned departure time.</li>
      <li class="bulletList"><code class="inlineCode">DEP_TIME</code>: The actual departure time.</li>
      <li class="bulletList"><code class="inlineCode">DEP_AFPH</code>: The number of actual flights per hour occurring during the interval in between the planned and actual departure from the origin airport (factoring in 30 minutes of padding). The feature tells you how busy the origin airport was during takeoff.</li>
      <li class="bulletList"><code class="inlineCode">DEP_RFPH</code>: The departure relative flights per hour is the ratio of actual flights per hour over the median number of flights per hour that occur at the origin airport at that time of day, day of the week, and month of the year. The feature tells you how <em class="italic">relatively</em> busy the origin airport was during takeoff.</li>
      <li class="bulletList"><code class="inlineCode">TAXI_OUT</code>: The time duration elapsed between the departure from the origin airport gate and wheels off.</li>
      <li class="bulletList"><code class="inlineCode">WHEELS_OFF</code>: The point in time that the aircraft’s wheels leave the ground.</li>
    </ul>
    <p class="normal">In-flight features<a id="_idIndexMarker170"/> are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">CRS_ELAPSED_TIME</code>: The planned amount of time needed for the flight trip.</li>
      <li class="bulletList"><code class="inlineCode">PCT_ELAPSED_TIME</code>: The ratio of actual flight time over planned flight time to gauge the plane’s relative speed.</li>
      <li class="bulletList"><code class="inlineCode">DISTANCE</code>: The distance between two airports.</li>
    </ul>
    <p class="normal">Arrival features are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">CRS_ARR_TIME</code>: The planned arrival time.</li>
      <li class="bulletList"><code class="inlineCode">ARR_AFPH</code>: The number of actual flights per hour occurring during the interval between the planned and actual arrival time at the destination airport (factoring in 30 minutes of padding). The feature tells you how busy the destination airport was during landing.</li>
      <li class="bulletList"><code class="inlineCode">ARR_RFPH</code>: The arrival relative flights per hour is the ratio of actual flights per hour over the median number of flights per hour that occur at the destination airport at that time of day, day of the week, and month of the year. The feature tells you how <em class="italic">relatively</em> busy the destination airport was during landing.</li>
    </ul>
    <p class="normal">Delay features are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">DEP_DELAY</code>: The total delay on departure in minutes.</li>
      <li class="bulletList"><code class="inlineCode">ARR_DELAY</code>: The total delay on arrival in minutes can be subdivided into any or all of the following:<ol class="alphabeticList" style="list-style-type: lower-alpha;">
          <li class="alphabeticList" value="1"><code class="inlineCode">CARRIER_DELAY</code>: The delay in minutes caused by circumstances within the airline’s control (for example, maintenance or crew problems, aircraft cleaning, baggage loading, fueling, and so on).</li>
          <li class="alphabeticList"><code class="inlineCode">WEATHER_DELAY</code>: The delay in minutes caused by significant meteorological conditions (actual or forecasted).</li>
          <li class="alphabeticList"><code class="inlineCode">NAS_DELAY</code>: The delay in minutes mandated by a national aviation system such as non-extreme weather conditions, airport operations, heavy traffic volume, and air traffic control.</li>
          <li class="alphabeticList"><code class="inlineCode">SECURITY_DELAY</code>: The delay in minutes caused by the evacuation of a terminal or concourse, re-boarding of an aircraft because of a security breach, faulty screening equipment, or long lines above 29 minutes in screening areas.</li>
          <li class="alphabeticList"><code class="inlineCode">LATE_AIRCRAFT_DELAY</code>: The<a id="_idIndexMarker171"/> delay in minutes caused by a previous flight with the same aircraft that arrived late.</li>
        </ol>
      </li>
    </ul>
    <h3 id="_idParaDest-65" class="heading-3">Data preparation</h3>
    <p class="normal">For <a id="_idIndexMarker172"/>starters, <code class="inlineCode">PLANNED_DEP_DATETIME</code> must be a datetime data type:</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df[<span class="hljs-string">'PLANNED_DEP_DATETIME'</span>] =\
pd.to_datetime(aad18_df[<span class="hljs-string">'PLANNED_DEP_DATETIME'</span>])
</code></pre>
    <p class="normal">The exact day and time of a flight don’t matter, but maybe the month and day of the week do because of weather and seasonal patterns that can only be appreciated at this level of granularity. Also, the executives mentioned weekends and winters being especially bad for delays. Therefore, we will create features for the month and day of the week:</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df[<span class="hljs-string">'DEP_MONTH'</span>] = aad18_df[<span class="hljs-string">'PLANNED_DEP_DATETIME'</span>].dt.month
aad18_df[<span class="hljs-string">'DEP_DOW'</span>] = aad18_df[<span class="hljs-string">'PLANNED_DEP_DATETIME'</span>].dt.dayofweek
</code></pre>
    <p class="normal">We don’t need the <code class="inlineCode">PLANNED_DEP_DATETIME</code> column so let’s drop it like this:</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df = aad18_df.<span class="code-highlight"><strong class="hljs-slc">drop</strong></span>([<span class="hljs-string">'PLANNED_DEP_DATETIME'</span>], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">It is essential to record whether the arrival or destination airport is a hub. AA, in 2019, had 10 hubs: Charlotte, Chicago–O’Hare, Dallas/Fort Worth, Los Angeles, Miami, New York–JFK, New York–LaGuardia, Philadelphia, Phoenix–Sky Harbor, and Washington–National. Therefore, we can encode which <code class="inlineCode">ORIGIN</code> and <code class="inlineCode">DEST</code> airports are AA hubs using their IATA codes, and <a id="_idIndexMarker173"/>get rid of columns with codes since they are too specific (<code class="inlineCode">FL_NUM</code>, <code class="inlineCode">ORIGIN</code>, and <code class="inlineCode">DEST</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Create list with 10 hubs (with their IATA codes)</span>
hubs = [<span class="hljs-string">'CLT'</span>, <span class="hljs-string">'ORD'</span>, <span class="hljs-string">'DFW'</span>, <span class="hljs-string">'LAX'</span>, <span class="hljs-string">'MIA'</span>, <span class="hljs-string">'JFK'</span>, <span class="hljs-string">'LGA'</span>, <span class="hljs-string">'PHL'</span>,
        <span class="hljs-string">'PHX'</span>, <span class="hljs-string">'DCA'</span>]
<span class="hljs-comment">#Boolean series for if ORIGIN or DEST are hubsis_origin_hub =</span>
aad18_df[<span class="hljs-string">'ORIGIN'</span>].isin(hubs)
is_dest_hub = aad18_df[<span class="hljs-string">'DEST'</span>].isin(hubs)
<span class="hljs-comment">#Use boolean series to set ORIGIN_HUB and DEST_HUB</span>
aad18_df[<span class="hljs-string">'ORIGIN_HUB'</span>] = <span class="hljs-number">0</span>
aad18_df.loc[is_origin_hub, <span class="hljs-string">'ORIGIN_HUB'</span>] = <span class="hljs-number">1</span>
aad18_df[<span class="hljs-string">'DEST_HUB'</span>] = <span class="hljs-number">0</span>
aad18_df.loc[is_dest_hub, <span class="hljs-string">'DEST_HUB'</span>] = <span class="hljs-number">1</span>
<span class="hljs-comment">#Drop columns with codes</span>
aad18_df = aad18_df.drop([<span class="hljs-string">'FL_NUM'</span>, <span class="hljs-string">'ORIGIN'</span>, <span class="hljs-string">'DEST'</span>], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">After all these operations, we have a fair number of useful features, but we are yet to determine the target feature. There are two columns that could serve this purpose. We have <code class="inlineCode">ARR_DELAY</code>, which is the total number of minutes delayed regardless of the reason, and then there’s <code class="inlineCode">CARRIER_DELAY</code>, which is just the total number of those minutes that can be attributed to the airline. For instance, look at the following sample of flights delayed over 15 minutes (which is considered late according to the airline’s definition):</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df.loc[aad18_df[<span class="hljs-string">'ARR_DELAY'</span>] &gt; <span class="hljs-number">15</span>,\
[<span class="hljs-string">'ARR_DELAY'</span>,<span class="hljs-string">'CARRIER_DELAY'</span>]].head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">The preceding code outputs <em class="italic">Figure 3.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_01.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.1: Sample observations with arrival delays over 15 minutes</p>
    <p class="normal">Of all the <a id="_idIndexMarker174"/>delays in <em class="italic">Figure 3.1</em>, one of them (#26) wasn’t at all the responsibility of the airline because only 0 minutes could be attributed to the airline. Four of them were partially the responsibility of the airline (#8, #16, #33, and #40), two of which were over 15 minutes late due to the airline (#8 and #40). The rest of them were entirely the airline’s fault. We can tell that although the total delay is useful information, the airline executives were only interested in delays caused by the airline so <code class="inlineCode">ARR_DELAY</code> can be discarded. Furthermore, there’s another more important reason it should be discarded, and it’s that if the task at hand is to predict a delay, we cannot use pretty much the very same delay (minus the portions not due to the airline) to predict it. For this very same reason, it is best to remove <code class="inlineCode">ARR_DELAY</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">aad18_df = aad18_df.drop([<span class="hljs-string">'ARR_DELAY'</span>], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Finally, we can put the target feature alone as <code class="inlineCode">y</code> and all the rest as <code class="inlineCode">X</code>. After this, we split <code class="inlineCode">y</code> and <code class="inlineCode">X</code> into train and test datasets. Please note that the target feature (<code class="inlineCode">y</code>) stays the same for regression, so we split it into <code class="inlineCode">y_train_reg</code> and <code class="inlineCode">y_test_reg</code>. However, for classification, we must make binary versions of these labels denoting whether it’s more than 15 minutes late or not, called <code class="inlineCode">y_train_class</code> and <code class="inlineCode">y_test_class</code>. Please note that we are <a id="_idIndexMarker175"/>setting a fixed <code class="inlineCode">random_state</code> for reproducibility:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span> 
np.random.seed(rand)
y = aad18_df[<span class="hljs-string">'CARRIER_DELAY'</span>]
X = aad18_df.drop([<span class="hljs-string">'CARRIER_DELAY'</span>], axis=<span class="hljs-number">1</span>).copy()
X_train, X_test, y_train_reg, y_test_reg = <span class="code-highlight"><strong class="hljs-slc">train_test_split</strong></span>(
    X, y, test_size=<span class="hljs-number">0.15</span>, <span class="code-highlight"><strong class="hljs-slc">random_state</strong></span>=rand
)
y_train_class = y_train_reg.apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
y_test_class = y_test_reg.apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">15</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">To examine how linearly correlated the features are to the target <code class="inlineCode">CARRIER_DELAY</code>, we can compute Pearson’s correlation coefficient, turn coefficients to absolute values (because we aren’t interested in whether they are positively or negatively correlated), and sort them in descending order:</p>
    <pre class="programlisting code"><code class="hljs-code">corr = aad18_df.<span class="code-highlight"><strong class="hljs-slc">corr</strong></span>()
<span class="hljs-built_in">abs</span>(corr[<span class="hljs-string">'CARRIER_DELAY'</span>]).<span class="code-highlight"><strong class="hljs-slc">sort_values</strong></span>(ascending=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">As you can tell from the output, only one feature (<code class="inlineCode">DEP_DELAY</code>) is highly correlated. The others aren’t:</p>
    <pre class="programlisting con"><code class="hljs-con">CARRIER_DELAY         1.000000
DEP_DELAY             0.703935
ARR_RFPH              0.101742
LATE_AIRCRAFT_DELAY   0.083166
DEP_RFPH              0.058659
ARR_AFPH              0.035135
DEP_TIME              0.030941
NAS_DELAY             0.026792
:          :
WEATHER_DELAY         0.003002
SECURITY_DELAY        0.000460
</code></pre>
    <p class="normal">However, this is only <em class="italic">linearly</em> correlated and on a one-by-one basis. It doesn’t mean that they don’t have a non-linear relationship, or that several features interacting together wouldn’t impact the target. In the next section, we will discuss this further.</p>
    <h1 id="_idParaDest-66" class="heading-1">Reviewing traditional model interpretation methods</h1>
    <p class="normal">To explore as <a id="_idIndexMarker176"/>many model classes and interpretation methods as possible, we will fit the data into regression and classification models.</p>
    <h2 id="_idParaDest-67" class="heading-2">Predicting minutes delayed with various regression methods</h2>
    <p class="normal">To <a id="_idIndexMarker177"/>compare and contrast regression methods, we will first create a dictionary named <code class="inlineCode">reg_models</code>. Each model is its own dictionary and the function that creates it is the <code class="inlineCode">model</code> attribute. This structure will be used later to neatly store the fitted model and its metrics. Model classes in this dictionary have been chosen to represent several model families and to illustrate important concepts that we will discuss later:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_models = {
    <span class="hljs-comment">#Generalized Linear Models (GLMs)</span>
    <span class="hljs-string">'linear'</span>:{<span class="hljs-string">'model'</span>: linear_model.LinearRegression()}, 
    <span class="hljs-string">'linear_poly'</span>:{
        <span class="hljs-string">'model'</span>:make_pipeline(
            PolynomialFeatures(degree=<span class="hljs-number">2</span>),
            linear_model.LinearRegression(fit_intercept=<span class="hljs-literal">False</span>)
        )
    },
    <span class="hljs-string">'linear_interact'</span>:{
        <span class="hljs-string">'model'</span>:make_pipeline(
            PolynomialFeatures(interaction_only=<span class="hljs-literal">True</span>),
            linear_model.LinearRegression(fit_intercept=<span class="hljs-literal">False</span>)
        )
    },
    <span class="hljs-string">'ridge'</span>:{
        <span class="hljs-string">'model'</span>: linear_model.RidgeCV(
            alphas=[<span class="hljs-number">1e-3</span>, <span class="hljs-number">1e-2</span>, <span class="hljs-number">1e-1</span>, <span class="hljs-number">1</span>])
    },
    <span class="hljs-comment">#Trees  </span>
    <span class="hljs-string">'decision_tree'</span>:{
        <span class="hljs-string">'</span><span class="hljs-string">model'</span>: tree.DecisionTreeRegressor(
             max_depth=<span class="hljs-number">7</span>, random_state=rand
        )
    },
    <span class="hljs-comment">#RuleFit</span>
    <span class="hljs-string">'rulefit'</span>:{
        <span class="hljs-string">'model'</span>: RuleFit(
             max_rules=<span class="hljs-number">150</span>,
             rfmode=<span class="hljs-string">'regress'</span>,
             random_state=rand
        )
    },
    <span class="hljs-comment">#Nearest Neighbors</span>
    <span class="hljs-string">'knn'</span>:{<span class="hljs-string">'model'</span>: neighbors.KNeighborsRegressor(n_neighbors=<span class="hljs-number">7</span>)},
    <span class="hljs-comment">#Ensemble Methods</span>
    <span class="hljs-string">'random_forest'</span>:{
        <span class="hljs-string">'model'</span>:ensemble.RandomForestRegressor(
            max_depth=<span class="hljs-number">7</span>, random_state=rand)
    },
    <span class="hljs-comment">#Neural Networks</span>
    <span class="hljs-string">'mlp'</span>:{
        <span class="hljs-string">'model'</span>:neural_network.MLPRegressor(
            hidden_layer_sizes=(<span class="hljs-number">21</span>,),
            max_iter=<span class="hljs-number">500</span>, 
            early_stopping=<span class="hljs-literal">True</span>,
            random_state=rand
        )
    }
}
</code></pre>
    <p class="normal">Before we start <a id="_idIndexMarker178"/>fitting the data to these models, we will briefly explain them one by one:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">linear</code>: <strong class="keyWord">Linear regression</strong> was<a id="_idIndexMarker179"/> the first model class we discussed. For better or for worse, it makes several assumptions about the data. Chief among them is the assumption that the prediction must be a linear combination of <em class="italic">X</em> features. This, naturally, limits the capacity to discover non-linear relationships and interactions among the features.</li>
      <li class="bulletList"><code class="inlineCode">linear_poly</code>: <strong class="keyWord">Polynomial regression</strong> extends<a id="_idIndexMarker180"/> linear regression by adding polynomial features. In this case, as indicated by <code class="inlineCode">degree=2</code>, the polynomial degree is two, so it’s quadratic. This means, in addition to having all features in their monomial form (for example, <code class="inlineCode">DEP_FPH</code>), it also has them in a quadratic form (for example, <code class="inlineCode">DEP_FPH²</code>), plus the many interaction terms for all of the 21 features. In other words, for <code class="inlineCode">DEP_FPH</code>, there would be interaction terms such as <code class="inlineCode">DEP_FPH</code> <img src="../Images/B18406_03_001.png" alt="" role="presentation"/> <code class="inlineCode">DISTANCE</code>, <code class="inlineCode">DEP_FPH</code> <img src="../Images/B18406_03_001.png" alt="" role="presentation"/> <code class="inlineCode">DELAY</code>, and so on for the rest of the features.</li>
      <li class="bulletList"><code class="inlineCode">linear_interact</code>: This is just like<a id="_idIndexMarker181"/> the <strong class="keyWord">polynomial regression</strong> model but without the quadratic terms – in other words, only the interactions, as <code class="inlineCode">interaction_only=True</code> would suggest. It’s useful because there is no reason to believe any of our features have a relationship that is better fitted with <a id="_idIndexMarker182"/>quadratic terms. Still, perhaps it’s the interaction with other features that makes an impact.</li>
      <li class="bulletList"><code class="inlineCode">ridge</code>: <strong class="keyWord">Ridge regression</strong> is a<a id="_idIndexMarker183"/> variation of linear regression. However, even though the method behind linear regression, called <strong class="keyWord">ordinary least squares</strong> (<strong class="keyWord">OLS</strong>), does <a id="_idIndexMarker184"/>a pretty good job of reducing the error and fitting the model to the features, it does it without <a id="_idIndexMarker185"/>considering <strong class="keyWord">overfitting</strong>. The problem here is that OLS treats all features equally, so the model becomes more complex as each variable is added. As the word <em class="italic">overfitting</em> suggests, the resulting model fits the training data too well, resulting in the lowest bias but the highest variance. There’s a sweet spot in this <strong class="keyWord">trade-off between bias and variance</strong>, and one way of getting to this spot is by reducing the complexity added by the introduction of too many features. Linear regression is not equipped to do so on its own. </li>
    </ul>
    <p class="normal">This is where ridge regression comes along, with our<a id="_idIndexMarker186"/> friend <strong class="keyWord">regularization</strong>. It does this by shrinking coefficients that don’t contribute to the outcome with a penalty term called<a id="_idIndexMarker187"/> the <strong class="keyWord">L2 norm</strong>. It penalizes complexity, thus constraining the algorithm from overfitting. In this example, we use a cross-validated version of <code class="inlineCode">ridge</code> (<code class="inlineCode">RidgeCV</code>) that tests several regularization strengths (<code class="inlineCode">alphas</code>).</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">decision_tree</code>: A <strong class="keyWord">decision tree</strong> is <a id="_idIndexMarker188"/>precisely as the name suggests. Imagine a tree-like structure where at every point that branches subdivide to form more branches, there is a “test” performed on a feature, partitioning the datasets into each branch. When branches stop subdividing, they become leaves, and at every leaf, there’s <em class="italic">a decision</em>, be it to assign a <em class="italic">class</em> for classification or a fixed value for regression. We are limiting this tree to <code class="inlineCode">max_depth=7</code> to prevent overfitting because the larger the tree, the better it will fit our training data, and the less likely the tree will generalize to non-training data.</li>
      <li class="bulletList"><code class="inlineCode">rule_fit</code>: <strong class="keyWord">RuleFit</strong> is a<a id="_idIndexMarker189"/> regularized linear regression expanded to include feature interactions in the form of rules. The rules are formed by traversing a decision tree, except it discards the leaves and keeps the feature interactions found traversing the branches toward these leaves. It uses <strong class="keyWord">LASSO Regression</strong>, which,<a id="_idIndexMarker190"/> like ridge, uses regularization, but instead of using<a id="_idIndexMarker191"/> the <strong class="keyWord">L2 norm</strong>, it uses the <strong class="keyWord">L1 norm</strong>. The<a id="_idIndexMarker192"/> result is that useless features end up with a coefficient of zero and do not just converge to zero, as they do with L2, which makes it easy for the algorithm to filter them out. We are limiting the rules to 150 (<code class="inlineCode">max_rules=150</code>) and the attribute <code class="inlineCode">rfmode='regress'</code> tells RuleFit that this is a regression problem, since it can also be used for classification. Unlike all other models used here, this isn’t a scikit-learn one but was created by Christoph Molnar adapting a paper called <em class="italic">Predictive learning via rule ensembles</em>.</li>
      <li class="bulletList"><code class="inlineCode">knn</code>: <strong class="keyWord">k-Nearest Neighbors</strong> (<strong class="keyWord">k-NN</strong>) is a <a id="_idIndexMarker193"/>simple method based on the <em class="italic">locality</em> assumption, which is that data points that are close to each other are similar. In <a id="_idIndexMarker194"/>other words, they must have similar predicted values, and, in practice, this isn’t a bad guess, so it takes data points nearest to the point you want to predict and derives a prediction based on that. In this case, <code class="inlineCode">n_neighbors=7</code> so k = 7. It’s <a id="_idIndexMarker195"/>an <strong class="keyWord">instance-based machine learning model</strong>, also known<a id="_idIndexMarker196"/> as a <strong class="keyWord">lazy learner</strong> because it simply stores the training data. During inference, it employs training data to calculate the similarity with points and generate a prediction based on that. This is opposed to what model-based machine learning <a id="_idIndexMarker197"/>techniques, or <strong class="keyWord">eager learners</strong>, do, which is to use training data to learn formulas, parameters, coefficients, or bias/weights, which they then leverages to make a prediction during inference.</li>
      <li class="bulletList"><code class="inlineCode">random_forest</code>: Imagine not one but hundreds of decision trees trained on random combinations of the features and random samples of data. <strong class="keyWord">Random forest</strong> takes<a id="_idIndexMarker198"/> an average of these randomly generated decision trees to create the best tree. This concept of training less effective models in parallel and combining them using an averaging process <a id="_idIndexMarker199"/>is called <strong class="keyWord">bagging</strong>. It is <a id="_idIndexMarker200"/>an <strong class="keyWord">ensemble</strong> method because it combines more than one model (usually called <strong class="keyWord">weak learners</strong>) into<a id="_idIndexMarker201"/> a <strong class="keyWord">strong learner</strong>. In <a id="_idIndexMarker202"/>addition to <em class="italic">bagging</em>, there are two other ensemble techniques, called <strong class="keyWord">boosting</strong> and <strong class="keyWord">stacking</strong>. For <a id="_idIndexMarker203"/>bagging <a id="_idIndexMarker204"/>deeper, trees are better because they reduce variance, so this is why we are using <code class="inlineCode">max_depth=7</code>.</li>
      <li class="bulletList"><code class="inlineCode">mlp</code>: <strong class="keyWord">A multi-layer perceptron</strong> is a “vanilla” feedforward (sequential) neural network, so it uses <a id="_idIndexMarker205"/>non-linear activation functions <code class="inlineCode">(MLPRegressor</code> uses <em class="italic">ReLU</em> by default), stochastic gradient descent, and backpropagation. In this case, we are using 21 neurons in the first and only hidden layer, hence <code class="inlineCode">hidden_layer_sizes=(21,)</code>, running training for 500 epochs (<code class="inlineCode">max_iter=500</code>), and terminating training when the validation score is not improving (<code class="inlineCode">early_stopping=True</code>).</li>
    </ul>
    <p class="normal">If you are<a id="_idIndexMarker206"/> unfamiliar with some of these models, don’t fret! We will cover them in more detail later in this chapter and the book. Also, please note that some of these models have a random process somewhere. To ensure reproducibility, we have set <code class="inlineCode">random_state</code>. It is best to always set this; otherwise, it will randomly set it every single time, which will make your results hard to reproduce.</p>
    <p class="normal">Now, let’s iterate over our dictionary of models (<code class="inlineCode">reg_models</code>), fit them to the training data, and predict and compute two metrics based on the quality of these predictions. We’ll then save the fitted model, test predictions, and metrics in the dictionary for later use. Note that <code class="inlineCode">rulefit</code> only accepts <code class="inlineCode">numpy</code> arrays, so we can’t <code class="inlineCode">fit</code> it in the same way. Also, note that <code class="inlineCode">rulefit</code> and <code class="inlineCode">mlp</code> take longer than the rest to train, so this can take a few minutes to run:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> model_name <span class="hljs-keyword">in</span> reg_models.keys():
    <span class="hljs-keyword">if</span> model_name != <span class="hljs-string">'rulefit'</span>:
        fitted_model = reg_models[model_name]\
        [<span class="hljs-string">'model'</span>].fit(X_train, y_train_reg)
    <span class="hljs-keyword">else</span> :
        fitted_model = reg_models[model_name][<span class="hljs-string">'model'</span>].\
        fit(X_train.values, y_train_reg.values, X_test.columns
        )
        y_train_pred = fitted_model.predict(X_train.values)
        y_test_pred = fitted_model.predict(X_test.values)
    reg_models[model_name][<span class="hljs-string">'fitted'</span>] = fitted_model
    reg_models[model_name][<span class="hljs-string">'preds'</span>] = y_test_pred
    reg_models[model_name][<span class="hljs-string">'RMSE_train'</span>] =\
    math.sqrt(
        metrics.mean_squared_error(y_train_reg, y_train_pred)
    )
    reg_models[model_name][<span class="hljs-string">'RMSE_test'</span>] =\
    math.sqrt(metrics.mean_squared_error(y_test_reg, y_test_pred)
    )
    reg_models[model_name][<span class="hljs-string">'R2_test'</span>] =\
    metrics.r2_score(y_test_reg, y_test_pred)
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker207"/>now convert the dictionary to a <code class="inlineCode">DataFrame</code> and display the metrics in a sorted and color-coded fashion:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_metrics = pd.<span class="code-highlight"><strong class="hljs-slc">DataFrame.from_dict</strong></span>(
    reg_models, <span class="hljs-string">'index'</span>)[[<span class="hljs-string">'RMSE_train'</span>, <span class="hljs-string">'RMSE_test'</span>, <span class="hljs-string">'R2_test'</span>]
]
reg_metrics.<span class="code-highlight"><strong class="hljs-slc">sort_values</strong></span>(by=<span class="code-highlight"><strong class="hljs-string-slc">'RMSE_test'</strong></span>).style.<span class="hljs-built_in">format</span>(
    {<span class="hljs-string">'RMSE_train'</span>: <span class="hljs-string">'{:.2f}'</span>, <span class="hljs-string">'RMSE_test'</span>: <span class="hljs-string">'{:.2f}'</span>, 
     <span class="hljs-string">'R2_test'</span>: <span class="hljs-string">'{:.3f}'</span>}
).background_gradient(
    cmap=<span class="hljs-string">'viridis_r'</span>, low=<span class="hljs-number">0.1</span>, high=<span class="hljs-number">1</span>,
    subset=[<span class="hljs-string">'RMSE_train'</span>, <span class="hljs-string">'RMSE_test'</span>]
).background_gradient(
    cmap=<span class="hljs-string">'plasma'</span>, low=<span class="hljs-number">0.3</span>, high=<span class="hljs-number">1</span>, subset=[<span class="hljs-string">'R2_test'</span>]
)
</code></pre>
    <p class="normal">The preceding code outputs <em class="italic">Figure 3.2</em>. Please note that color-coding doesn’t work in all Jupyter Notebook implementations:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_02.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.2: Regression metrics for our models</p>
    <p class="normal">To interpret the metrics in <em class="italic">Figure 3.2</em>, we ought to first understand what they mean, both in general<a id="_idIndexMarker208"/> and in the context of this regression exercise:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">RMSE</strong>: <strong class="keyWord">Root Mean Square Error</strong> is defined <a id="_idIndexMarker209"/>as the standard deviation of the residuals. It’s the square root of the squared residuals divided by the number of observations – in this case, flights. It tells you, on average, how far apart the predictions are from the actuals, and as you can probably tell from the color-coding, less is better because you want your predictions to be as close as possible to the actuals in the <em class="italic">test</em> (<strong class="keyWord">hold-out</strong>) dataset. We<a id="_idIndexMarker210"/> have also included this metric for the <strong class="keyWord">train</strong> dataset to see how well it’s generalizing. You expect the test error to be higher than the training error, but not by much. If it is, like it is for <code class="inlineCode">random_forest</code>, you need to tune some of the parameters to reduce overfitting. In this case, reducing the trees’ maximum depth, increasing the number of<a id="_idIndexMarker211"/> trees (also called <strong class="keyWord">estimators</strong>), and reducing the maximum number of features to use should do the trick. On the other hand, with <code class="inlineCode">knn</code>, you can adjust the number of neighbors, but it is expected, because <a id="_idIndexMarker212"/>of its <strong class="keyWord">lazy learner</strong> nature, to overperform on the training data.
    <p class="normal">In any case, these numbers are pretty good because even our worst performing model is below a test RMSE of 10 minutes, and about half of them have a test RMSE of less than 7.5, quite possibly predicting a delay effectively, on average, since the threshold for a delay is 15 minutes.</p>
    <p class="normal">Note <a id="_idIndexMarker213"/>that <code class="inlineCode">linear_poly</code> is the second and <code class="inlineCode">linear_interact</code> is the fourth most performant model, significantly ahead of <code class="inlineCode">linear</code>, suggesting that non-linearity and interactivity are important factors to produce better predictive performance.</p></li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">R</strong><sup class="superscript-bold" style="font-weight: bold;">2</sup>: <strong class="keyWord">R-squared</strong> is also <a id="_idIndexMarker214"/>known as <a id="_idIndexMarker215"/>the <strong class="keyWord">coefficient of determination</strong>. It’s defined as the proportion of the variance in the <em class="italic">y</em> (outcome) target that can be explained by the <em class="italic">X</em> (predictors) features in the model. It answers the question of what proportion of the model variability is explainable? And as you can probably tell from the color-coding, more is better. And our models appear to include significant <em class="italic">X</em> features, as evidenced by our <em class="italic">Pearson’s correlation coefficients</em>. So if this <em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">2</sup> value was low, perhaps adding additional features would help, such as flight logs, terminal conditions, and even those things airline executives said they weren’t interested in exploring right now, such as <em class="italic">knock-off</em> effects and international flights. These<a id="_idIndexMarker216"/> could fill in the gaps in the unexplained variance.</li>
    </ul>
    <p class="normal">Let’s see if we can get good metrics with classification.</p>
    <h2 id="_idParaDest-68" class="heading-2">Classifying flights as delayed or not delayed with various classification methods</h2>
    <p class="normal">Just as <a id="_idIndexMarker217"/>we did with regression, to compare and contrast classification methods, we will first create a dictionary for them named <code class="inlineCode">class_models</code>. Each model is its own dictionary and the function that creates it is the <code class="inlineCode">model</code> attribute. This structure will be used later to store the fitted model and its metrics. Model classes in this dictionary have been chosen to represent several model families and to illustrate important concepts that we will discuss later. Some of these will look familiar because they are the same methods used in regression but applied to classification:</p>
    <pre class="programlisting code"><code class="hljs-code">class_models = {
    <span class="hljs-comment">#Generalized Linear Models (GLMs)</span>
    <span class="hljs-string">'logistic'</span>:{<span class="hljs-string">'model'</span>: linear_model.LogisticRegression()},
    <span class="hljs-string">'ridge'</span>:{
        <span class="hljs-string">'model'</span>: linear_model.RidgeClassifierCV(
            cv=<span class="hljs-number">5</span>, alphas=[<span class="hljs-number">1e-3</span>, <span class="hljs-number">1e-2</span>, <span class="hljs-number">1e-1</span>, <span class="hljs-number">1</span>],
            class_weight=<span class="hljs-string">'balanced'</span>
        )
    },
    <span class="hljs-comment">#Tree</span>
    <span class="hljs-string">'decision_tree'</span>:{
        <span class="hljs-string">'model'</span>: tree. DecisionTreeClassifier(max_depth=<span class="hljs-number">7</span>,
                                              random_state=rand)
    },
    <span class="hljs-comment">#Nearest Neighbors</span>
    <span class="hljs-string">'knn'</span>:{<span class="hljs-string">'model'</span>: neighbors.KNeighborsClassifier(n_neighbors=<span class="hljs-number">7</span>)},
    <span class="hljs-comment">#Naive Bayes</span>
    <span class="hljs-string">'naive_bayes'</span>:{<span class="hljs-string">'model'</span>: naive_bayes.GaussianNB()},
    <span class="hljs-comment">#Ensemble Methods</span>
    <span class="hljs-string">'gradient_boosting'</span>:{
        <span class="hljs-string">'model'</span>:ensemble.
        GradientBoostingClassifier(n_estimators=<span class="hljs-number">210</span>)
    },
    <span class="hljs-string">'random_forest'</span>:{
        <span class="hljs-string">'model'</span>:ensemble.RandomForestClassifier(
            max_depth=<span class="hljs-number">11</span>,class_weight=<span class="hljs-string">'balanced'</span>, random_state=rand
        )
    },
    <span class="hljs-comment">#Neural Networks</span>
    <span class="hljs-string">'mlp'</span>:{
        <span class="hljs-string">'model'</span>: make_pipeline(
            StandardScaler(),
            neural_network.MLPClassifier(
                hidden_layer_sizes=(<span class="hljs-number">7</span>,),
                max_iter=<span class="hljs-number">500</span>,
                early_stopping=<span class="hljs-literal">True</span>,
                random_state=rand
            )
        )
    }
}
</code></pre>
    <p class="normal">Before we start<a id="_idIndexMarker218"/> fitting the data to these models, we will briefly explain them one by one:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">logistic</code>: <strong class="keyWord">logistic regression</strong> was introduced in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>. It has many <a id="_idIndexMarker219"/>of the same pros and cons<a id="_idIndexMarker220"/> as <strong class="keyWord">linear regression</strong>. For instance, feature interactions must be added manually. Like other classification models, it returns a probability between 0 and 1, which, when closer to 1, denotes a probable match to <a id="_idIndexMarker221"/>a <strong class="keyWord">positive class</strong> while, when closer to 0, it denotes an improbable match to the <strong class="keyWord">positive class</strong>, and therefore a probable match to <a id="_idIndexMarker222"/>the <strong class="keyWord">negative class</strong>. Naturally, 0.5 is the threshold used to decide between classes, but it doesn’t have to be. As we will examine later in the book, there are interpretation and performance reasons to adjust the threshold. Note that this is a binary classification problem, so we are only choosing between delayed (positive) and not delayed (negative), but this method could be extended to multi-class classification. It would then be <a id="_idIndexMarker223"/>called <strong class="keyWord">multinomial classification</strong>.</li>
      <li class="bulletList"><code class="inlineCode">ridge</code>: <strong class="keyWord">Ridge classification</strong> leverages<a id="_idIndexMarker224"/> the same regularization technique used in <strong class="keyWord">ridge regression</strong> but<a id="_idIndexMarker225"/> applied to classification. It does this by converting the target values to -1 (for a negative class) and keeping 1 for a positive class and then performing ridge regression. At its heart, its regression in disguise will predict values between -1 and 1, and then convert them back to a 0–1 scale. Like with <code class="inlineCode">RidgeCV</code> for regression, <code class="inlineCode">RidgeClassifierCV</code> uses leave-one-out cross-validation, which means it first splits the data into different equal-size sets – in this case, we are using five sets (<code class="inlineCode">cv=5</code>) – and then removes features one at a time to see how well the model performs without them, on average in all the five sets. Those features that don’t make much of a difference are penalized by testing several regularization strengths (<code class="inlineCode">alphas</code>) to find the optimal strength. As with all <em class="italic">regularization</em> techniques, the point is to discourage learning from unnecessary complexity, minimizing the impact of less salient features.</li>
      <li class="bulletList"><code class="inlineCode">decision_tree</code>: A standard <strong class="keyWord">decision tree</strong>, such<a id="_idIndexMarker226"/> as this one, is also known <a id="_idIndexMarker227"/>as a <strong class="keyWord">CART</strong> (<strong class="keyWord">classification and regression tree</strong>) because it can be used for regression or classification tasks. It has the same algorithm for both tasks but functions slightly differently, like the algorithm used to decide where to “split” a branch. In this case, we are only allowing our trees to have a depth of 7.</li>
      <li class="bulletList"><code class="inlineCode">knn</code>: <strong class="keyWord">k-NN</strong> can also <a id="_idIndexMarker228"/>be applied to classification tasks, except instead of averaging <a id="_idIndexMarker229"/>what the nearest neighbors’ target features (or labels) are, it chooses the most frequent one (also known as the <strong class="keyWord">mode</strong>). We are also using a k-value of 7 for classification (<code class="inlineCode">n_neighbors</code>).</li>
      <li class="bulletList"><code class="inlineCode">naive_bayes</code>: <strong class="keyWord">Gaussian Naïve Bayes</strong> is <a id="_idIndexMarker230"/>part of the family of <em class="italic">Naïve Bayes</em> classifiers, which are called naïve because they make the assumption that the features are independent of each other, which is usually not the case. This dramatically impedes its capacity to predict unless the assumption is correct. It’s called <em class="italic">Bayes</em> because it’s based on <strong class="keyWord">Bayes’ theorem of conditional probabilities</strong>, which is that the conditional<a id="_idIndexMarker231"/> probability of a class is the class probability times the feature probability given the class. <em class="italic">Gaussian Naïve Bayes</em> makes an additional assumption, which is that continuous values have a normal distribution, also<a id="_idIndexMarker232"/> known as a <strong class="keyWord">Gaussian distribution</strong>.</li>
      <li class="bulletList"><code class="inlineCode">gradient_boosting</code>: Like <strong class="keyWord">random forest</strong>, <strong class="keyWord">gradient-boosted trees</strong> are also an ensemble method, but <a id="_idIndexMarker233"/>that leverages <strong class="keyWord">boosting</strong> instead<a id="_idIndexMarker234"/> of <strong class="keyWord">bagging</strong>. <strong class="keyWord">Boosting</strong> doesn’t work in parallel but in sequence, iteratively<a id="_idIndexMarker235"/> training weak learners and incorporating their strengths into a stronger learner, while adapting another weak learner to tackle their weaknesses. Although ensembles and boosting, in particular, can be done with a model class, this method uses decision trees. We have limited the number of trees to 210 (<code class="inlineCode">n_estimators=210</code>).</li>
      <li class="bulletList"><code class="inlineCode">random_forest</code>: The same <strong class="keyWord">random forest</strong> as <a id="_idIndexMarker236"/>with regression except it generates classification decision trees and not regression trees.</li>
      <li class="bulletList"><code class="inlineCode">mlp</code>: The <a id="_idIndexMarker237"/>same <strong class="keyWord">multi-layer perceptron</strong> as with regression, but the output layer, by default, uses<a id="_idIndexMarker238"/> a <strong class="keyWord">logistic</strong> function in the output layer to yield probabilities, which it then converts to 1 or 0, based on the 0.5 threshold. Another difference is that we are using seven neurons in the first and only hidden layer (<code class="inlineCode">hidden_layer_sizes=(7,)</code>) because binary classification tends to require fewer of them to achieve an optimal result.</li>
    </ul>
    <p class="normal">Please<a id="_idIndexMarker239"/> note that some of these models use balanced weights for the classes (<code class="inlineCode">class_weight='balanced'</code>), which is very important because this happens to be an <strong class="keyWord">imbalanced classification</strong> task. By<a id="_idIndexMarker240"/> that, we mean that negative classes vastly outnumber positive classes. We can find out what this looks like for our training data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(y_train_class[y_train_class==<span class="hljs-number">1</span>].shape[<span class="hljs-number">0</span>] y_train_class.shape[<span class="hljs-number">0</span>])
</code></pre>
    <p class="normal">As you can see, the output in our training data’s positive classes represents only 6% of the total. Models that account for this will achieve <em class="italic">more balanced</em> results. There are different ways of accounting for <em class="italic">class imbalance</em>, which we will discuss in further detail in <em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>, but <code class="inlineCode">class_weight='balanced'</code> applies a weight inversely proportional to class frequencies, giving the outnumbered <em class="italic">positive</em> class a leg up.</p>
    <h3 id="_idParaDest-69" class="heading-3">Training and evaluating the classification models</h3>
    <p class="normal">Now, let’s iterate <a id="_idIndexMarker241"/>over our dictionary of models (<code class="inlineCode">class_models</code>), fit them to the training data, and predict both probabilities and the class except for <code class="inlineCode">ridge</code>, which doesn’t output probabilities. We’ll then<a id="_idIndexMarker242"/> compute five metrics based on the quality of these predictions. Lastly, we’ll save the fitted model, test predictions, and metrics in the dictionary for later use. You can go get a coffee while you run the next snippet of code because <code class="inlineCode">gradient_boosting</code> of <code class="inlineCode">sklearn</code> takes longer than the rest to train, so this can take a few minutes to run:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> model_name <span class="hljs-keyword">in</span> class_models.keys():
    fitted_model = class_models[model_name]
    [<span class="hljs-string">'model'</span>].fit(X_train,y_train_class)
    y_train_pred = fitted_model.predict(X_train.values)
    <span class="hljs-keyword">if</span> model_name == <span class="hljs-string">'ridge'</span>:
        y_test_pred = fitted_model.predict(X_test.values)
    <span class="hljs-keyword">else</span>:
        y_test_prob = fitted_model.predict_proba(X_test.values)[:,<span class="hljs-number">1</span>]
        y_test_pred = np.where(y_test_prob &gt; <span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
     class_models[model_name][<span class="hljs-string">'fitted'</span>] = fitted_model
    class_models[model_name][<span class="hljs-string">'probs'</span>] = y_test_prob
    class_models[model_name][<span class="hljs-string">'preds'</span>] = y_test_pred
    class_models[model_name][<span class="hljs-string">'Accuracy_train'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">accuracy_score</strong></span>(y_train_class, y_train_pred
    )
    class_models[model_name][<span class="hljs-string">'Accuracy_test'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">accuracy_score</strong></span>(y_test_class, y_test_pred
    )
    class_models[model_name][<span class="hljs-string">'</span><span class="hljs-string">Recall_train'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">recall_score</strong></span>(y_train_class, y_train_pred
    )
    class_models[model_name][<span class="hljs-string">'Recall_test'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">recall_score</strong></span>(y_test_class, y_test_pred
    )
    <span class="hljs-keyword">if</span> model_name != <span class="hljs-string">'ridge'</span>:
        class_models[model_name][<span class="hljs-string">'ROC_AUC_test'</span>] =\
        metrics.<span class="code-highlight"><strong class="hljs-slc">roc_auc_score</strong></span>(y_test_class, y_test_prob)
    <span class="hljs-keyword">else</span>:
        class_models[model_name][<span class="hljs-string">'ROC_AUC_test'</span>] = np.nan
    class_models[model_name][<span class="hljs-string">'F1_test'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">f1_score</strong></span>(y_test_class, y_test_pred
    )
    class_models[model_name][<span class="hljs-string">'MCC_test'</span>] =\
    metrics.<span class="code-highlight"><strong class="hljs-slc">matthews_corrcoef</strong></span>(y_test_class, y_test_pred
    )
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker243"/>now <a id="_idIndexMarker244"/>convert the dictionary to a <code class="inlineCode">DataFrame</code> and display the metrics in a sorted and color-coded fashion:</p>
    <pre class="programlisting code"><code class="hljs-code">class_metrics = pd.<span class="code-highlight"><strong class="hljs-slc">DataFrame.from_dict</strong></span>(
    class_models,<span class="hljs-string">'index'</span>)[[<span class="hljs-string">'Accuracy_train'</span>, <span class="hljs-string">'Accuracy_test'</span>,
                           <span class="hljs-string">'Recall_train'</span>, <span class="hljs-string">'Recall_test'</span>,
                           <span class="hljs-string">'ROC_AUC_test'</span>, <span class="hljs-string">'F1_test'</span>, <span class="hljs-string">'MCC_test'</span>]
]
class_metrics.<span class="code-highlight"><strong class="hljs-slc">sort_values</strong></span>(
    by=<span class="code-highlight"><strong class="hljs-string-slc">'ROC_AUC_test'</strong></span>, ascending=<span class="hljs-literal">False</span>).
    style.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(class_metrics.columns, [<span class="hljs-string">'{:.3f}'</span>]*<span class="hljs-number">7</span>))
).background_gradient(
cmap=<span class="hljs-string">'plasma'</span>, low=<span class="hljs-number">1</span>, high=<span class="hljs-number">0.1</span>, subset=[<span class="hljs-string">'Accuracy_train'</span>,
                                        <span class="hljs-string">'</span><span class="hljs-string">Accuracy_test'</span>]
).background_gradient(
    cmap=<span class="hljs-string">'viridis'</span>,
    low=<span class="hljs-number">1</span>,
    high=<span class="hljs-number">0.1</span>,
    subset=[<span class="hljs-string">'Recall_train'</span>, <span class="hljs-string">'Recall_test'</span>,
            <span class="hljs-string">'ROC_AUC_test'</span>, <span class="hljs-string">'F1_test'</span>, <span class="hljs-string">'MCC_test'</span>]
)
</code></pre>
    <p class="normal">The preceding code outputs <em class="italic">Figure 3.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_03.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.3: Classification metrics for our models</p>
    <p class="normal">To<a id="_idIndexMarker245"/> interpret <a id="_idIndexMarker246"/>the metrics in <em class="italic">Figure 3.3</em>, we ought to first understand what they mean, both in general and in the context of this classification exercise:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Accuracy</strong>: Accuracy<a id="_idIndexMarker247"/> is the simplest way to measure the effectiveness of a classification task, and it’s the percentage of correct predictions over all predictions. In other words, in a binary classification task, you can calculate this by adding the number of <strong class="keyWord">True Positives</strong> (<strong class="keyWord">TPs</strong>)<strong class="keyWord"> </strong>and <strong class="keyWord">True Negatives</strong> (<strong class="keyWord">TNs</strong>) and dividing them by a tally of all predictions made. As with regression metrics, you can measure accuracy for both train and test to gauge overfitting.</li>
      <li class="bulletList"><strong class="keyWord">Recall</strong>: Even<a id="_idIndexMarker248"/> though accuracy sounds like a great metric, recall is much better in this case and the reason is you could have an accuracy of 94%, which sounds pretty good, but it turns out you are always predicting no delay! In other words, even if you get high accuracy, it is meaningless unless you are predicting accurately for the least represented class, delays. We can find this number with<a id="_idIndexMarker249"/> recall (also known <a id="_idIndexMarker250"/>as <strong class="keyWord">sensitivity</strong> or <strong class="keyWord">true positive rate</strong>), which is <img src="../Images/B18406_03_003.png" alt="" role="presentation"/> , and it can be interpreted as how much of the relevant results were returned – in other words, in this case, what percentage of the actual delays were predicted.
    <p class="normal">Another good measure involving true<a id="_idIndexMarker251"/> positives is <strong class="keyWord">precision</strong>, which is how much our predicted samples are relevant, which is <img src="../Images/B18406_03_004.png" alt="" role="presentation"/>. In this case, that would be what percentage of predicted delays were actual delays. For imbalanced classes, it is recommended to use both, but depending on your preference for <em class="italic">FN</em> over <em class="italic">FP</em>, you will prefer recall over precision or vice versa.</p></li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">ROC-AUC</strong>: <strong class="keyWord">ROC</strong> is an <a id="_idIndexMarker252"/>acronym for <strong class="keyWord">Receiver Operating Characteristic</strong> and was designed to separate signal from noise. What it does is plot the proportion of <strong class="keyWord">true positive rate</strong> (<strong class="keyWord">recall</strong>) on the <em class="italic">x</em> axis and the false positive rate on the <em class="italic">y</em> axis. <strong class="keyWord">AUC</strong> stands for <strong class="keyWord">area under the curve</strong>, which <a id="_idIndexMarker253"/>is a number <a id="_idIndexMarker254"/>between 0 and 1 that assesses the prediction ability of the classifier 1 being perfect, 0.5 being as good as a random coin toss, and anything lower meaning that if we inverted the results of our prediction, we would have a better prediction. To illustrate this, let’s generate a ROC curve for our worst-performing model, Naïve Bayes, according to the AUC metric:
        <pre class="programlisting code"><code class="hljs-code">plt.tick_params(axis = <span class="hljs-string">'</span><span class="hljs-string">both'</span>, which = <span class="hljs-string">'major'</span>)
fpr, tpr, _ = metrics.<span class="code-highlight"><strong class="hljs-slc">roc_curve</strong></span>(
  y_test_class, class_models[<span class="hljs-string">'naive_bayes'</span>][<span class="hljs-string">'probs'</span>])
plt.plot(
    fpr,
    tpr,
    label=<span class="hljs-string">'ROC curve (area = %0.2f)'</span>
    % class_models[<span class="hljs-string">'naive_bayes'</span>][<span class="hljs-string">'ROC_AUC_test'</span>]
)
plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">'k–'</span>) <span class="hljs-comment">#random coin toss line</span>
plt.xlabel(<span class="hljs-string">'False Positive Rate'</span>)
plt.ylabel(<span class="hljs-string">'True Positive Rate'</span>)
plt.xlim([<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>])
plt.ylim([<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>])
plt.legend(loc=<span class="hljs-string">"lower right"</span>)
</code></pre>
      
    <p class="normal">The preceding code outputs <em class="italic">Figure 3.4</em>. Note that the diagonal line signifies half the area. In other words, the point where it has a coin-toss-like prediction quality:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_04.png" alt="A picture containing polygon  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.4: ROC curve for Naïve Bayes</p></li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">F1</strong>: The <strong class="keyWord">F1-score</strong> is also <a id="_idIndexMarker255"/>called the harmonic average of precision and recall because it’s calculated like this: <img src="../Images/B18406_03_005.png" alt="" role="presentation"/>. Since it includes both precision and recall metrics, which pertain to the proportion of true positives, it’s a good metric choice to use when your dataset is imbalanced, and you don’t prefer either precision or recall.</li>
      <li class="bulletList"><strong class="keyWord">MCC</strong>: The <strong class="keyWord">Matthews correlation coefficient</strong> is a<a id="_idIndexMarker256"/> metric drawn from biostatistics. It’s gaining popularity in the broader data science community because it has the ability to produce high scores considering <em class="italic">TP</em>, <em class="italic">FN</em>, <em class="italic">TN</em>, and <em class="italic">FP</em> fairly, because it takes into account the proportions <a id="_idIndexMarker257"/>of classes. This makes it optimal for imbalanced classification tasks. Unlike all <a id="_idIndexMarker258"/>other metrics used so far, it doesn’t range from 0 to 1 but from -1, complete disagreement, to 1, a total agreement between predictions and actuals. The mid-point, 0, is equivalent to a random prediction:</li>
    </ul>
    <p class="center"><img src="../Images/B18406_03_006.png" alt="" role="presentation"/></p>
    <p class="normal">Our classification metrics are mostly very good, exceeding 96% accuracy and 75% recall. However, even recall isn’t everything. For instance, <code class="inlineCode">RandomForest</code>, due to its class balancing with weights, got the highest recall but did poorly in F1 and MCC, which suggests that precision is not very good.</p>
    <p class="normal">Ridge classification also had the same setting and had such a poor F1 score that the precision must have been dismal. This doesn’t mean this weighting technique is inherently wrong, but it often requires more control. This book will cover techniques to achieve the right balance between fairness and accuracy, accuracy and reliability, reliability and validity, and so on. This is a balancing act that requires many metrics and visualizations. A key takeaway from this exercise should be that a <strong class="keyWord">single metric will not tell you the whole story</strong>, and interpretation is about <strong class="keyWord">telling the most relevant and sufficiently complete story</strong>.</p>
    <h1 id="_idParaDest-70" class="heading-1">Understanding limitations of traditional model interpretation methods</h1>
    <p class="normal">In a nutshell, traditional interpretation methods <em class="italic">only cover high-level questions about your models</em> such as <a id="_idIndexMarker259"/>the following:</p>
    <ul>
      <li class="bulletList">In aggregate, do they perform well?</li>
      <li class="bulletList"><em class="italic">What</em> changes in hyperparameters may impact predictive performance?</li>
      <li class="bulletList"><em class="italic">What</em> latent patterns can you find between the features and their predictive performance?</li>
    </ul>
    <p class="normal">These questions are very limiting if you are trying to understand not only whether your model works but <em class="italic">why</em> and <em class="italic">how</em>?</p>
    <p class="normal">This gap in understanding can lead to unexpected issues with your model that won’t necessarily be immediately apparent. Let’s consider that models, once deployed, are not static but dynamic. They face different challenges than they did in the “lab” when you were training them. They may face not only performance issues but issues with bias, such as imbalance with underrepresented classes, or security vulnerabilities with adversarial attacks. Realizing that the features have changed in the real-world environment, we might have to add new features instead of merely retraining with the same feature set. And if there are some troubling assumptions made by your model, you might have to re-examine the whole pipeline. But how do you recognize that these problems exist in the first place? That’s when you will need a whole new set of interpretation tools that can help you dig deeper and answer more specific questions about your model. These tools provide interpretations that can truly account for <strong class="keyWord">Fairness, Accountability, and Transparency</strong> (<strong class="keyWord">FAT</strong>), which we discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em></p>
    <h1 id="_idParaDest-71" class="heading-1">Studying intrinsically interpretable (white-box) models</h1>
    <p class="normal">So far, in this <a id="_idIndexMarker260"/>chapter, we have already fitted our training data to model classes representing each of these “white-box” model families. The purpose of this section is to show you exactly why they are <em class="italic">intrinsically interpretable</em>. We’ll do so by employing the models that were previously fitted.</p>
    <h2 id="_idParaDest-72" class="heading-2">Generalized Linear Models (GLMs)</h2>
    <p class="normal">GLMs are <a id="_idIndexMarker261"/>a large<a id="_idIndexMarker262"/> family of model classes that have a model for every statistical distribution. Just like <strong class="keyWord">linear regression</strong> assumes your target<a id="_idIndexMarker263"/> feature and residuals have a normal distribution, <strong class="keyWord">logistic regression</strong> assumes<a id="_idIndexMarker264"/> the Bernoulli distribution. There are GLMs for every distribution, such as <strong class="keyWord">Poisson regression</strong> for <a id="_idIndexMarker265"/>Poisson distribution <a id="_idIndexMarker266"/>and <strong class="keyWord">multinomial response</strong> for multinomial distribution. You choose which GLM to use based on the distribution of your target variable and whether your data meets the other assumptions of the GLM (they vary). In addition to an underlying distribution, what ties GLMs together into a single family is the fact that they all have a linear predictor. In other words, the <img src="../Images/B18406_03_007.png" alt="" role="presentation"/> target variable (or predictor) can be expressed mathematically as a weighted sum of <em class="italic">X</em> features, where weights are called <em class="italic">b</em> coefficients. This is the simple formula, the linear predictor function, that all GLMs share:</p>
    <p class="center"><img src="../Images/B18406_03_008.png" alt="" role="presentation"/></p>
    <p class="normal">However, although they share this same formula, they each have a different link function, which provides a link between the linear predictor function and the mean of the statistical distribution of the GLM. This can add some non-linearity to the resulting model formula while retaining the linear combination between the <em class="italic">b</em> coefficients and the <em class="italic">X</em> input data, which can be a source of confusion. Still, it’s linear because of the linear combination.</p>
    <p class="normal">There are also many variations for specific GLMs. For instance, <strong class="keyWord">Polynomial regression</strong> is <em class="italic">linear regression</em> with <a id="_idIndexMarker267"/>polynomials of its<a id="_idIndexMarker268"/> features, and <strong class="keyWord">ridge regression</strong> is <em class="italic">linear regression</em> with L2 regularization. We won’t cover all GLMs in this section because they aren’t needed for the example in this chapter, but all have plausible use cases.</p>
    <p class="normal">Incidentally, there’s also a similar concept called <strong class="keyWord">Generalized Additive Models</strong> (<strong class="keyWord">GAMs</strong>), which are GLMs that <a id="_idIndexMarker269"/>don’t require linear combinations of features and coefficients and instead retain the addition part, but of arbitrary functions applied to the features. GAMs are also interpretable, but they are not as common, and are usually tailored to specific use cases <em class="italic">ad hoc</em>.</p>
    <h3 id="_idParaDest-73" class="heading-3">Linear regression</h3>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability, and Why Does It All Matter?</em>, we <a id="_idIndexMarker270"/>covered the formula of <a id="_idIndexMarker271"/>simple linear regression, which only has a single <em class="italic">X</em> feature. Multiple linear regression extends this to have any number of features, so instead of being:</p>
    <p class="center"><img src="../Images/B18406_03_009.png" alt="" role="presentation"/></p>
    <p class="normal">it can be:</p>
    <p class="center"><img src="../Images/B18406_03_010.png" alt="" role="presentation"/></p>
    <p class="normal">with <em class="italic">n</em> features, and where <img src="../Images/B18406_03_011.png" alt="" role="presentation"/> is the intercept, and thanks to linear algebra this can be a simple matrix multiplication:</p>
    <p class="center"><img src="../Images/B18406_03_008.png" alt="" role="presentation"/></p>
    <p class="normal">The method used to arrive at the optimal <em class="italic">b</em> coefficients, <strong class="keyWord">OLS</strong>, is well-studied and understood. Also, in addition to the coefficients, you can extract confidence intervals for each. The model’s correctness<a id="_idIndexMarker272"/> depends on whether the input data meets the assumptions: <strong class="keyWord">linearity</strong>, normality, independence, a lack of multicollinearity, and homoscedasticity. We’ve discussed linearity, so far, quite a bit so we will briefly explain the rest:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Normality</strong> is the <a id="_idIndexMarker273"/>property that each feature is normally distributed. This can be tested <a id="_idIndexMarker274"/>with a <strong class="keyWord">Q-Q plot</strong>, histogram, or <strong class="keyWord">Kolmogorov-Smirnov</strong> test, and<a id="_idIndexMarker275"/> non-normality can be corrected with non-linear transformations. If a feature isn’t normally distributed, it will make its coefficient confidence intervals invalid.</li>
      <li class="bulletList"><strong class="keyWord">Independence</strong> is<a id="_idIndexMarker276"/> when your <em class="italic">observations</em> (the rows in your dataset) are independent of each other, like different and unrelated events. If your <em class="italic">observations</em> aren’t independent, it could affect your interpretation of the results. In this chapter’s example, if you had multiple rows about the same flight, that could violate this assumption and make results hard to understand. This can be tested by looking for duplicate flight numbers.</li>
      <li class="bulletList">Multicollinearity<a id="_idIndexMarker277"/> occurs when the features are highly correlated with each other. <strong class="keyWord">Lack of multicollinearity</strong> is desirable because otherwise, you’d have inaccurate coefficients. This can be<a id="_idIndexMarker278"/> tested <a id="_idIndexMarker279"/>with a <strong class="keyWord">correlation matrix</strong>, <strong class="keyWord">tolerance measure</strong>, or <strong class="keyWord">Variance Inflation Factor</strong> (<strong class="keyWord">VIF</strong>), and<a id="_idIndexMarker280"/> it can be fixed by removing one of each highly correlated feature.</li>
      <li class="bulletList"><strong class="keyWord">Homoscedasticity</strong> was<a id="_idIndexMarker281"/> briefly discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em> and it’s when the residuals (the errors) are more or less equal across the regression line. This can be tested with the <strong class="keyWord">Goldfeld–Quandt test</strong>, and<a id="_idIndexMarker282"/> heteroscedasticity (the lack of homoscedasticity) can be corrected with non-linear transformations. This assumption is often violated in practice.</li>
    </ul>
    <p class="normal">Even<a id="_idIndexMarker283"/> though <a id="_idIndexMarker284"/>we haven’t done it for this chapter’s example, if you are going to rely on linear regression heavily, it’s always good to test these assumptions before you even begin to fit your data to a linear regression model. This book won’t detail how this is done because it’s more about model-agnostic and deep-learning interpretation methods than delving into how to meet the assumptions of a specific class of models, such as <strong class="keyWord">normality</strong> and <strong class="keyWord">homoscedasticity</strong>. However, we covered the characteristics that trump interpretation the most in <em class="chapterRef">Chapter 2,</em> <em class="italic">Key Concepts of Interpretability</em>, and we will continue to look for these characteristics: <strong class="keyWord">non-linearity</strong>, <strong class="keyWord">non-monotonicity</strong>, and <strong class="keyWord">interactivity</strong>. We will do this mainly because the linearity and correlation of and between features are still relevant, regardless of the modeling class used to make predictions. And these are characteristics that can be easily tested in the methods used for linear regression.</p>
    <h4 class="heading-4">Interpretation</h4>
    <p class="normal">So how <a id="_idIndexMarker285"/>do we interpret a linear regression model? Easy! Just get the coefficients and the intercept. Our scikit-learn models have these attributes embedded in the fitted model:</p>
    <pre class="programlisting code"><code class="hljs-code">coefs_lm = reg_models[<span class="hljs-string">'linear'</span>][<span class="hljs-string">'fitted'</span>].coef_
intercept_lm = reg_models[<span class="hljs-string">'linear'</span>][<span class="hljs-string">'fitted'</span>].intercept_
<span class="hljs-built_in">print</span>(<span class="hljs-string">'coefficients:%s'</span> % coefs_lm)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'intercept:%s'</span> % intercept_lm)
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">coefficients:   [ 0.0045 -0.0053 0.8941 0.0152 ...]
intercept: -37.86
</code></pre>
    <p class="normal">So now you know the formula, which looks something like this:</p>
    <p class="center"><img src="../Images/B18406_03_013.png" alt="" role="presentation"/></p>
    <p class="normal">This formula should provide some intuition on how the model can be interpreted globally. Interpreting each coefficient in the model can be done for multiple linear regression, just as we did with the simple linear regression example in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>. The coefficients act as weights, but they also tell a story that varies depending on the kind of feature. To make interpretation more manageable, let’s put our coefficients in a <code class="inlineCode">DataFrame</code> alongside the names of each feature:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.DataFrame({<span class="hljs-string">'feature'</span>: X_train.columns.tolist(),\
              <span class="hljs-string">'coef'</span>: coefs_lm.tolist()})
</code></pre>
    <p class="normal">The preceding code produces the DataFrame in <em class="italic">Figure 3.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_05.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.5: Coefficients of linear regression features</p>
    <p class="normal">Here’s<a id="_idIndexMarker286"/> how<a id="_idIndexMarker287"/> to interpret a feature using the coefficients in <em class="italic">Figure 3.5</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Continuous</strong>: Like <code class="inlineCode">ARR_RFPH</code>, you know that for every one-unit increase (relative flights per hour), it increases the predicted delay by 0.373844 minutes, if all other features stay the same.</li>
      <li class="bulletList"><strong class="keyWord">Binary</strong>: Like <code class="inlineCode">ORIGIN_HUB</code>, you know the difference between the origin airport being a hub or not is expressed by the coefficient -1.029088. In other words, since it’s a negative number, the origin airport is a hub. It reduces the delay by just over 1 minute if all other features stay the same.</li>
      <li class="bulletList"><strong class="keyWord">Categorical</strong>: We don’t have categorical features, but we have ordinal features that could have been, and <strong class="keyWord">actually should have been</strong>, categorical features. For instance, <code class="inlineCode">DEP_MONTH</code> and <code class="inlineCode">DEP_DOW</code> are integers from 1–12 and 0–6, respectively. If they are treated as ordinals, we are assuming because of the linear nature of linear regression that an increase or decrease in months has an impact on the outcome. It’s the same with the day of the week. But the impact is tiny. Had we treated them as dummy or one-hot encoded features, we could measure whether Fridays are more prone to carrier delays than Saturdays and Wednesdays, or Julys more than Octobers and Junes. This couldn’t possibly be modeled with them in order, because they have no relation to this order (yep – it’s non-linear!). So, say we had a feature called <code class="inlineCode">DEP_FRIDAY</code> and another called <code class="inlineCode">DEP_JULY</code>. They are treated like binary features and can tell you precisely what effect a departure being on a Friday or in <a id="_idIndexMarker288"/>July has on the model. Some features were kept as ordinal or continuous on purpose, despite being good candidates for being categorical, to demonstrate how not making the right adjustments to your features can impact the <strong class="keyWord">expressive power</strong> of model interpretation. It would have been good to tell airline executives more about how the day and time of a departure impacted delays. Also, in some cases – not in this one – an oversight like this can grossly affect a linear regression model’s performance.</li>
    </ul>
    <p class="normal">The <a id="_idIndexMarker289"/>intercept (-37.86) is not a feature, but it does have a meaning, which is, if all features were at 0, what would the prediction be? In practice, this doesn’t happen unless your features happen to all have a plausible reason to be 0. Just as in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em> you wouldn’t have expected anyone to have a height of 0, in this example, you wouldn’t expect a flight to have a distance of 0. However, if you standardized the features so that they had a mean of 0, then you would change the interpretation of the intercept to be the prediction you expect if all features are their mean value.</p>
    <h4 class="heading-4">Feature importance</h4>
    <p class="normal">The <a id="_idIndexMarker290"/>coefficients can also be leveraged to calculate feature importance. Unfortunately, scikit-learn’s linear regressor is ill-equipped to do this because it doesn’t output the standard error of the coefficients. According to <a id="_idIndexMarker291"/>their importance, all it takes to rank features is to divide the <img src="../Images/B18406_03_014.png" alt="" role="presentation"/>s by their corresponding standard errors. This result is something called <a id="_idIndexMarker292"/>the <strong class="keyWord">t-statistic</strong>:</p>
    <p class="center"><img src="../Images/B18406_03_015.png" alt="" role="presentation"/></p>
    <p class="normal">And then you take an absolute value of this and sort them from high to low. It’s easy enough to calculate, but you need the standard error. You could reverse-engineer the linear algebra involved to retrieve it using the intercept, and the coefficients returned by scikit-learn. However, it’s probably a lot easier to fit the linear regression model again, but this time using the <code class="inlineCode">statsmodels</code> library, which has a summary with all the statistics included! By the way, <code class="inlineCode">statsmodels</code> names its linear regressor <code class="inlineCode">OLS</code>, which makes sense because <code class="inlineCode">OLS</code> is the name of the mathematical method that fits the data:</p>
    <pre class="programlisting code"><code class="hljs-code">linreg_mdl = sm.<span class="code-highlight"><strong class="hljs-slc">OLS</strong></span>(y_train_reg, sm.add_constant(X_train))
linreg_mdl = linreg_mdl.fit()
<span class="hljs-built_in">print</span>(linreg_mdl.summary())
</code></pre>
    <p class="normal">There’s quite a bit to unpack in the regression summary. This book won’t address everything except that the t-statistic can tell you how important features are in relation to each other. There’s another more pertinent statistical interpretation, which is that if you were to hypothesize that the <em class="italic">b</em> coefficient is 0 – in other words, that the feature has no impact on the model – the distance of the t-statistic from 0 helps reject that null hypothesis. This is what the <strong class="keyWord">p-value</strong> to the right of the t-statistic does. It’s no coincidence that the closest <em class="italic">t</em> to 0 (for <code class="inlineCode">ARR_AFPH</code>) has the only p-value above 0.05. This puts this feature at a level of insignificance since everything below 0.05 is statistically significant according to this method of hypothesis testing.</p>
    <p class="normal">So to rank our features, let’s extract the DataFrame from the <code class="inlineCode">statsmodels</code> summary. Then, we drop the <code class="inlineCode">const</code> (the intercept) because this is not a feature. Then, we make a new column with the absolute value of the t-statistic and sort it accordingly. To demonstrate how the absolute value of the t-statistic and p-value are inversely related, we are also color-coding these columns:</p>
    <pre class="programlisting code"><code class="hljs-code">summary_df = linreg_mdl.<span class="code-highlight"><strong class="hljs-slc">summary2</strong></span>().tables[<span class="hljs-number">1</span>]
summary_df = summary_df.<span class="code-highlight"><strong class="hljs-slc">drop</strong></span>(
    [<span class="hljs-string">'const'</span>]).reset_index().rename(columns={<span class="hljs-string">'index'</span>:<span class="hljs-string">'feature'</span>}
)
summary_df[<span class="hljs-string">'t_abs'</span>] = <span class="hljs-built_in">abs</span>(summary_df[<span class="hljs-string">'t'</span>])
summary_df.<span class="code-highlight"><strong class="hljs-slc">sort_values</strong></span>(by=<span class="code-highlight"><strong class="hljs-string-slc">'t_abs'</strong></span>, ascending=<span class="hljs-literal">False</span>).
    style.<span class="hljs-built_in">format</span>(
        <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(summary_df.columns[<span class="hljs-number">1</span>:], [<span class="hljs-string">'{:.4f}'</span>]*<span class="hljs-number">7</span>)
    )
).background_gradient(cmap=<span class="hljs-string">'</span><span class="hljs-string">plasma_r'</span>, low=<span class="hljs-number">0</span>, high=<span class="hljs-number">0.1</span>,\
                      subset=[<span class="hljs-string">'P&gt;|t|'</span>, <span class="hljs-string">'t_abs'</span>])
</code></pre>
    <p class="normal">The<a id="_idIndexMarker293"/> preceding code outputs <em class="italic">Figure 3.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_06.png" alt="Table, Excel  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.6: Linear regression summary table sorted by the absolute value of the t-statistic</p>
    <p class="normal">Something<a id="_idIndexMarker294"/> particularly interesting about the feature importance in <em class="italic">Figure 3.6</em> is that different kinds of delays occupy five out of the top six positions. Of course, this could be because linear regression is confounding the different non-linear effects these have, or perhaps there’s something here we should look further into – especially since the <code class="inlineCode">statsmodels</code> summary in the “<strong class="keyWord">Warnings</strong>” section cautions:</p>
    <pre class="programlisting con"><code class="hljs-con">[2] The condition number is large, 5.69e+04. This might indicate that there are strong multicollinearity or other numerical problems.
</code></pre>
    <p class="normal">This is odd. Hold<a id="_idIndexMarker295"/> that thought. We will examine this further later.</p>
    <h3 id="_idParaDest-74" class="heading-3">Ridge regression</h3>
    <p class="normal">Ridge regression<a id="_idIndexMarker296"/> is part of a sub-family<a id="_idIndexMarker297"/> of <strong class="keyWord">penalized</strong> or <strong class="keyWord">regularized</strong> regression<a id="_idIndexMarker298"/> along with the likes of LASSO and ElasticNet <a id="_idIndexMarker299"/>because, as explained earlier in this chapter, it penalizes using the <em class="italic">L2 norm</em>. This sub-family is also <a id="_idIndexMarker300"/>called <strong class="keyWord">sparse linear models</strong> because, thanks to the regularization, it cuts out some of the noise by making irrelevant features less relevant. <strong class="keyWord">Sparsity</strong> in <a id="_idIndexMarker301"/>this context means less is more because reduced complexity will lead to lower variance and improved generalization.</p>
    <p class="normal">To illustrate this concept, look at the feature importance table (<em class="italic">Figure 3.6</em>) we output for linear regression. Something that should be immediately apparent is how the <code class="inlineCode">t_abs</code> column starts with every row a different color, and then a whole bunch of them are the same shade of yellow. Because of the variation in confidence intervals, the absolute t-value is not something you can take proportionally and say that your top feature is hundreds of times more relevant than every one of your bottom 10 features. However, it should indicate that there are significantly more important features than others to the point of irrelevance, and possibly confoundment, hence creating noise. There’s ample research on how there’s a tendency for a small subset of features to have the most substantial effects on the outcome of the model. This is<a id="_idIndexMarker302"/> called the <strong class="keyWord">bet on sparsity principle</strong>. Whether it’s true or not for your data, it’s always good to test the theory by applying regularization, especially in cases where data is very wide (many features) or exhibits multicollinearity. These regularized regression techniques can be incorporated into feature selection processes or to inform your understanding of what features are essential.</p>
    <p class="normal">There is a technique to adapt ridge regression to classification problems. It was briefly discussed before. It converts the labels to a -1 to 1 scale for training to predict values between -1 and 1, and then turns them back to a 0–1 scale. However, it uses regularized linear regression to fit the data and can be interpreted in the same way.</p>
    <h4 class="heading-4">Interpretation</h4>
    <p class="normal">Ridge regression<a id="_idIndexMarker303"/> can be interpreted in the same way as linear regression, both globally and locally, because once the model has been fitted, there’s no difference. The formula is the same:</p>
    <p class="center"><img src="../Images/B18406_03_016.png" alt="" role="presentation"/></p>
    <p class="normal">Except <img src="../Images/B18406_03_017.png" alt="" role="presentation"/> coefficients are different because they were penalized with a <img src="../Images/B18406_03_018.png" alt="" role="presentation"/> parameter, which controls how much shrinkage to apply.</p>
    <p class="normal">We can quickly compare coefficients by extracting the ridge coefficients from their fitted model and placing them side by side in a <code class="inlineCode">DataFrame</code> with the coefficients of the linear regression:</p>
    <pre class="programlisting code"><code class="hljs-code">coefs_ridge = reg_models[<span class="hljs-string">'ridge'</span>][<span class="hljs-string">'fitted'</span>].coef_
coef_ridge_df =pd.DataFrame(
    {
        <span class="hljs-string">'feature'</span>:X_train.columns.values.tolist(),
        <span class="hljs-string">'coef_linear'</span>: coefs_lm,\
        <span class="hljs-string">'coef_ridge'</span>: coefs_ridge
    }
)
coef_ridge_df[<span class="hljs-string">'coef_regularization'</span>] =\
    coef_ridge_df[<span class="hljs-string">'coef_linear'</span>] - coef_ridge_df[<span class="hljs-string">'coef_ridge'</span>]
coef_ridge_df.style.background_gradient(
    cmap=<span class="hljs-string">'plasma_r'</span>, low=<span class="hljs-number">0</span>, high=<span class="hljs-number">0.1</span> , subset=[<span class="hljs-string">'coef_regularization'</span>]
)
</code></pre>
    <p class="normal">As you can tell in the <em class="italic">Figure 3.7</em> output of the preceding code, the coefficients are always slightly different, but sometimes they are lower and sometimes higher:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_07.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.7: Linear regression coefficients compared to ridge regression coefficients</p>
    <p class="normal">We didn’t <a id="_idIndexMarker304"/>save the <img src="../Images/B18406_03_018.png" alt="" role="presentation"/> parameter (which scikit-learn calls <em class="italic">alpha</em>) that the ridge regression cross-validation deemed optimal. However, we can run a little experiment of our own to figure out which parameter was the best. We do this by iterating through 100 possible alphas values between 100 (1) and 1013 (100,000,000,000,000), fitting the data to the ridge model, and then appending the coefficients to an array. We exclude the eight coefficient in the array because it’s so much larger<a id="_idIndexMarker305"/> than the rest, and it will make it harder to visualize the effects of shrinkage:</p>
    <pre class="programlisting code"><code class="hljs-code">num_alphas = <span class="hljs-number">100</span>
alphas = np.<span class="code-highlight"><strong class="hljs-slc">logspace</strong></span>(<span class="hljs-number">0</span>, <span class="hljs-number">13</span>, num_alphas)
alphas_coefs = []
<span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:
    ridge = linear_model.<span class="code-highlight"><strong class="hljs-slc">Ridge</strong></span>(alpha=alpha).fit(
      X_train, y_train_reg)
    alphas_coefs.<span class="code-highlight"><strong class="hljs-slc">append</strong></span>(np.concatenate((ridge.coef_[:<span class="hljs-number">8</span>],
                                        ridge.coef_[<span class="hljs-number">9</span>:])))
</code></pre>
    <p class="normal">Now that we have an array of coefficients, we can plot the progression of coefficients:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.gca().invert_xaxis()
plt.tick_params(axis = <span class="hljs-string">'both'</span>, which = <span class="hljs-string">'major'</span>)
plt.plot(alphas, alphas_coefs)
plt.xscale(<span class="hljs-string">"</span><span class="hljs-string">log"</span>)
plt.xlabel(<span class="hljs-string">'Alpha'</span>)
plt.ylabel(<span class="hljs-string">'Ridge coefficients'</span>)
plt.grid()
plt.show()
</code></pre>
    <p class="normal">The preceding code generates <em class="italic">Figure 3.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_08.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.8: Value of alpha hyperparameters versus the value of ridge regression coefficients</p>
    <p class="normal">Something<a id="_idIndexMarker306"/> to note in <em class="italic">Figure 3.8</em> is that the higher the alpha, the higher the regularization. This is why when alpha is 1012, all coefficients have converged to 0, and as the alpha becomes smaller, they get to a point where they have all diverged and more or less stabilized. In this case, this point is reached at about 102. Another way of seeing it is when all coefficients are around 0, it means that the regularization is so strong that all features are irrelevant. When they have sufficiently diverged and stabilized, the regularization makes them all relevant, which defeats the purpose. </p>
    <p class="normal">Now on that note, if we go back to our code, we will find that this is what we chose for alphas in our <code class="inlineCode">RidgeCV</code>: <code class="inlineCode">alphas=[1e-3, 1e-2, 1e-1, 1]</code>. As you can tell from the preceding plot, by the time the alphas have reached <code class="inlineCode">1</code> and below, the coefficients have already stabilized even though they are still fluctuating slightly. This can explain why our ridge was not better performing than linear regression. Usually, you would expect a regularized model to perform better than one that isn’t – unless your hyperparameters are not right.</p>
    <div class="note">
      <p class="normal">Interpretation and hyperparameters</p>
      <p class="normal">Well-tuned regularization can help cut out the noise and thus increase interpretability, but the alphas chosen for <code class="inlineCode">RidgeCV</code> were selected on purpose to be able to convey this point: <em class="italic">regularization can only work if you chose hyperparameters correctly</em>, or, when regularization hyperparameter tuning is automatic, the method must be optimal for your dataset.</p>
    </div>
    <h4 class="heading-4">Feature importance</h4>
    <p class="normal">This is<a id="_idIndexMarker307"/> precisely the same as with linear regression, but again we need the standard error of the coefficients, which is something that cannot be extracted from the scikit-learn model. You can use the <code class="inlineCode">statsmodels fit_regularized</code> method to this effect.</p>
    <h3 id="_idParaDest-75" class="heading-3">Polynomial regression</h3>
    <p class="normal">Polynomial regression<a id="_idIndexMarker308"/> is a special case of <a id="_idIndexMarker309"/>linear or logistic regression where the features have been expanded to have higher degree terms. We have only performed polynomial linear regression in this chapter’s exercise, so we will only discuss this variation. However, it is applied similarly.</p>
    <p class="normal">A two-feature multiple linear regression would look like this:</p>
    <p class="center"><img src="../Images/B18406_03_020.png" alt="" role="presentation"/></p>
    <p class="normal">However, in polynomial regression, every feature is expanded to have higher degree terms and interactions between all the features. So, if this two-feature example was expanded to a second-degree polynomial, the linear regression formula would look like this:</p>
    <p class="center"><img src="../Images/B18406_03_021.png" alt="" role="presentation"/></p>
    <p class="normal">It’s still linear regression in every way except it has extra features, higher-degree terms, and interactions. While you can limit polynomial expansion to only one or a few features, we used <code class="inlineCode">PolynomialFeatures</code>, which does this to all features. Therefore, 21 features were likely multiplied many times over. We can extract the coefficients from our fitted model and, using the <code class="inlineCode">shape</code> property of the <code class="inlineCode">numpy</code> array, return how many coefficients were generated. This amount corresponds to the number of features generated:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_models[<span class="hljs-string">'linear_poly'</span>][<span class="hljs-string">'fitted'</span>].\
get_params()[<span class="hljs-string">'linearregression'</span>].coef_.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">It outputs <code class="inlineCode">253</code>. We<a id="_idIndexMarker310"/> can do the same with the version of polynomial regression, which was with interaction terms only:</p>
    <pre class="programlisting code"><code class="hljs-code">reg_models[<span class="hljs-string">'linear_interact'</span>][<span class="hljs-string">'fitted'</span>].\
get_params()[<span class="hljs-string">'linearregression'</span>].coef_.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">The above <a id="_idIndexMarker311"/>code outputs <code class="inlineCode">232</code>. The reality is that most terms in a polynomial generated like this are interactions between all the features.</p>
    <h4 class="heading-4">Interpretation and feature importance</h4>
    <p class="normal">Polynomial regression<a id="_idIndexMarker312"/> can be interpreted, both <a id="_idIndexMarker313"/>globally and locally, in precisely the same way as linear regression. In this case, it’s not practical to understand a formula with 253 linearly combined terms, so it loses what we defined in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, as <strong class="keyWord">global holistic interpretation</strong>. However, it still can be interpreted <a id="_idIndexMarker314"/>in all other scopes and retains many of the properties of linear regression. For instance, since the model is additive, it is easy to separate the effects of the features. You can also use the same many peer-reviewed tried and tested statistical methods that are used for linear regression. For instance, you can use the t-statistic, p-value, confidence bounds, R-squared, as well as the many tests used to assess goodness of fit, residual analysis, linear correlation, and analysis of variance. This wealth of statistically proven methods to test and interpret models isn’t something most model classes can count on. Unfortunately, many of them are model-specific to linear regression and its special cases.</p>
    <p class="normal">Also, we won’t do it here because there are so many terms. Still, you could undoubtedly rank features for polynomial regression in the same way we have for linear regression using the <code class="inlineCode">statsmodels</code> library. The challenge is figuring out the order of the features generated by <code class="inlineCode">PolynomialFeatures</code> to name them accordingly in the feature name column. Once this is done, you can tell if some second-degree terms or interactions are important. This could tell you if these features have a non-linear nature or highly depend on other features.</p>
    <h3 id="_idParaDest-76" class="heading-3">Logistic regression</h3>
    <p class="normal">We<a id="_idIndexMarker315"/> discussed<a id="_idIndexMarker316"/> logistic regression as well as its interpretation and feature importance in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>. We will only expand on that a bit here in the context of this chapter’s classification exercise and to underpin why exactly it is interpretable. The fitted logistic regression model has coefficients and intercepts just as the linear regression model does:</p>
    <pre class="programlisting code"><code class="hljs-code">coefs_log = class_models[<span class="hljs-string">'logistic'</span>][<span class="hljs-string">'fitted'</span>].<span class="code-highlight"><strong class="hljs-slc">coef_</strong></span>
intercept_log = class_models[<span class="hljs-string">'logistic'</span>][<span class="hljs-string">'fitted'</span>].<span class="code-highlight"><strong class="hljs-slc">intercept_</strong></span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">'coefficients:%s'</span> % coefs_log)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'intercept:%s'</span> % intercept_log)
</code></pre>
    <p class="normal">The preceding code outputs this:</p>
    <pre class="programlisting con"><code class="hljs-con">coefficients: [[-6.31114061e-04 -1.48979793e-04  2.01484473e-01  1.32897749e-01 1.31740116e-05 -3.83761619e-04 -7.60281290e-02  ..]]
intercept: [-0.20139626]
</code></pre>
    <p class="normal">However, the way these coefficients appear in the formula for a specific prediction <img src="../Images/B18406_03_022.png" alt="" role="presentation"/>is entirely different:</p>
    <p class="center"><img src="../Images/B18406_03_023.png" alt="" role="presentation"/></p>
    <p class="normal">In other words, the probability that <img src="../Images/B18406_03_024.png" alt="" role="presentation"/> (is a positive case) is expressed by a <strong class="keyWord">logistic function</strong> that <a id="_idIndexMarker317"/>involves exponentials of the linear combination of <img src="../Images/B18406_03_014.png" alt="" role="presentation"/> coefficients and the <em class="italic">x</em> features. The<a id="_idIndexMarker318"/> presence of the exponentials<a id="_idIndexMarker319"/> explains why the coefficients extracted from the model are log odds because to isolate the coefficients, you should apply a logarithm to both sides of the equation.</p>
    <h4 class="heading-4">Interpretation</h4>
    <p class="normal">To <a id="_idIndexMarker320"/>interpret each coefficient, you do it in precisely the same way as with linear regression, except with each unit increase in the features, you increase the odds of getting the positive case by a factor expressed by the exponential of the coefficient – all things being equal (remember the <strong class="keyWord">ceteris paribus</strong> assumption discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>). An exponential (<img src="../Images/B18406_03_026.png" alt="" role="presentation"/>) has to be applied to each coefficient because they express an increase in log odds and not odds. Besides incorporating the log odds into the interpretation, the same that was said about continuous, binary, and categorical in linear regression interpretation applies to logistic regression.</p>
    <h4 class="heading-4">Feature importance</h4>
    <p class="normal">Frustrating as<a id="_idIndexMarker321"/> it is, there isn’t consensus yet from the statistical community on how to best get feature importance for logistic regression. There’s a standardize-all-features-first method, a pseudo R<sup class="superscript">2</sup> method, a <em class="italic">one feature at a time</em> ROC AUC method, a partial chi-squared statistic method, and then the simplest one, which is multiplying the standard deviations of each feature times the coefficients. We won’t cover all these methods, but it has to be noted that computing feature importance consistently and reliably is a problem for most model classes, even white-box ones. We will dig deeper into this in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>. For logistic regression, perhaps the most popular method is achieved by standardizing all the features before training – that is, making sure they are centered at zero and divided by their standard deviation. But we didn’t do this because<a id="_idIndexMarker322"/> although it has other benefits, it makes the interpretation of coefficients more difficult, so here we are using the rather crude method leveraged in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, which is to multiply the standard deviations of each feature times the coefficients:</p>
    <pre class="programlisting code"><code class="hljs-code">stdv = np.std(X_train, <span class="hljs-number">0</span>)
<span class="hljs-built_in">abs</span>(coefs_log.reshape(<span class="hljs-number">21</span>,) * stdv).sort_values(ascending=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The preceding code yields the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">DEP_DELAY              8.92
CRS_ELAPSED_TIME       6.03
DISTANCE               5.31
LATE_AIRCRAFT_DELAY    4.99
NAS_DELAY              2.39
WEATHER_DELAY          2.16
TAXI_OUT               1.31
SECURITY_DELAY         0.38
ARR_AFPH               0.32
WHEELS_OFF        0.01
PCT_ELAPSED_TIME    0.003
</code></pre>
    <p class="normal">It can still approximate the importance of features quite well. And just like with linear regression, you can tell that delay features are ranking quite high. All five of them are among the top eight features. Indeed, it’s something we should look into. We will discuss more on that as we discuss some other white-box methods.</p>
    <h2 id="_idParaDest-77" class="heading-2">Decision trees</h2>
    <p class="normal">Decision trees <a id="_idIndexMarker323"/>have been used for<a id="_idIndexMarker324"/> the longest time, even before they were turned into algorithms. They hardly require any mathematical abilities to understand them, and this low barrier to comprehensibility makes them extremely interpretable in their simplest representations. However, in practice, there are many types of decision tree learning, and most of them are not very interpretable because they<a id="_idIndexMarker325"/> use <strong class="keyWord">ensemble methods</strong> (boosting, bagging, and stacking), or even leverage PCA or some other embedder. Even non-ensembled decision trees can get extremely complicated as they become deeper. Regardless of the complexity of a decision tree, they can always be mined for important insights about your data and expected predictions, and they can be fitted to both regression and classification tasks.</p>
    <h3 id="_idParaDest-78" class="heading-3">CART decision trees</h3>
    <p class="normal">The <strong class="keyWord">Classification and Regression Trees</strong> (<strong class="keyWord">CART</strong>) algorithm is the “vanilla” no-frills decision<a id="_idIndexMarker326"/> tree<a id="_idIndexMarker327"/> of choice in most use cases. And as noted, most decision trees aren’t white-box models, but this one is because it is expressed as a mathematical formula, visualized, and printed as a set of rules that subdivides the tree into branches and eventually the leaves.</p>
    <p class="normal">The mathematical formula:</p>
    <p class="center"><img src="../Images/B18406_03_027.png" alt="" role="presentation"/></p>
    <p class="normal">And what this means is that if according to the identity function <em class="italic">I</em>, <em class="italic">x</em> is in the subset <em class="italic">R</em><sub class="subscript-italic" style="font-style: italic;">m</sub>, then it returns a 1, otherwise a 0. This binary term is multiplicated by the averages of all elements in the subset <em class="italic">R</em><sub class="subscript-italic" style="font-style: italic;">m</sub> denoted as <img src="../Images/B18406_03_028.png" alt="" role="presentation"/>. So if x<sub class="subscript">i</sub> is in the subset belonging to the leaf node <em class="italic">R</em><sub class="subscript-italic" style="font-style: italic;">k</sub> then the prediction <img src="../Images/B18406_03_029.png" alt="" role="presentation"/>. In other words, the prediction is the average of all elements in the subset<em class="italic">R</em><sub class="subscript-italic" style="font-style: italic;">k</sub>. This is what happens to regression tasks, and in binary classification, there is simply no <img src="../Images/B18406_03_028.png" alt="" role="presentation"/> to multiply the <em class="italic">I</em> identify function.</p>
    <p class="normal">At the heart of every decision tree algorithm, there’s a method to generate the <em class="italic">R</em><sub class="subscript-italic" style="font-style: italic;">m</sub> subsets. For CART, this is achieved using something called <a id="_idIndexMarker328"/>the <strong class="keyWord">Gini index</strong>, recursively splitting on where the two branches are as different as possible. This concept will be explained in greater detail in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>.</p>
    <h4 class="heading-4">Interpretation</h4>
    <p class="normal">A decision tree<a id="_idIndexMarker329"/> can be globally and locally interpreted visually. Here, we have established a maximum depth of 2 (<code class="inlineCode">max_depth=2</code>) because we could generate all 7 layers, but the text would be too small to appreciate. One of the limitations of this method is that it can get complicated to visualize with depths above 3 or 4. However, you can always programmatically traverse the branches of the tree and visualize only some branches at a time:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, axes = plt.subplots(
    nrows = <span class="hljs-number">1</span>, ncols = <span class="hljs-number">1</span>, figsize = (<span class="hljs-number">16</span>,<span class="hljs-number">8</span>), dpi=<span class="hljs-number">600</span>)
tree.plot_tree(
    class_models[<span class="hljs-string">'</span><span class="hljs-string">decision_tree'</span>][<span class="hljs-string">'fitted'</span>],
    feature_names=X_train.columns.values.tolist(),
    filled = <span class="hljs-literal">True</span>, max_depth=<span class="hljs-number">2</span>
)
fig.show()
</code></pre>
    <p class="normal">The preceding code prints out the tree in <em class="italic">Figure 3.9</em>. From the tree, you can tell that the very first branch splits the decision tree based on the value of <code class="inlineCode">DEP_DELAY</code> being equal to or smaller than 20.5. It tells you the Gini index that informed that decision and the number of <code class="inlineCode">samples</code> (just another way of saying observations, data points, or rows) present. You can traverse these branches till they reach a leaf. There is one leaf node in this tree, and it is on the far left. This is a classification tree, so you can tell by the value =[629167, 0] that all 629,167 samples left in this node have been classified as a 0 (not delayed):</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_09.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 3.9: Our models’ plotted decision tree</p>
    <p class="normal">Another way<a id="_idIndexMarker330"/> the tree can be better visualized but with fewer details, such as the Gini index and sample size, is by printing out the decisions made in every branch and the class in every node:</p>
    <pre class="programlisting code"><code class="hljs-code">text_tree = tree.export_text(
    class_models[<span class="hljs-string">'</span><span class="hljs-string">decision_tree'</span>][<span class="hljs-string">'fitted'</span>],
    feature_names=X_train.columns.values.tolist()
)
<span class="hljs-built_in">print</span>(text_tree)
</code></pre>
    <p class="normal">And the preceding code outputs the following:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_10.png" alt="A close-up of a document  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 3.10: Our decision tree’s structure</p>
    <p class="normal">There’s a lot <a id="_idIndexMarker331"/>more that can be done with a decision tree, and scikit-learn provides an API to explore the tree.</p>
    <h4 class="heading-4">Feature importance</h4>
    <p class="normal">Calculating feature importance<a id="_idIndexMarker332"/> in a CART decision tree is reasonably straightforward. As you can appreciate from the visualizations, some features appear more often in the decisions, but their appearances are weighted by how much they contributed to the overall reduction in the Gini index compared to the previous node. All the sum of the relative decrease in the Gini index throughout the<a id="_idIndexMarker333"/> tree is tallied, and the contribution of each feature is a percentage of this reduction:</p>
    <pre class="programlisting code"><code class="hljs-code">dt_imp_df = pd.DataFrame(
    {
        <span class="hljs-string">'feature'</span>:X_train.columns.values.tolist(),
        <span class="hljs-string">'importance'</span>: class_models[<span class="hljs-string">'decision_tree'</span>][<span class="hljs-string">'fitted'</span>].\
                      <span class="code-highlight"><strong class="hljs-slc">feature_importances_</strong></span>
    }
).sort_values(by=<span class="hljs-string">'importance'</span>, ascending=<span class="hljs-literal">False</span>)
dt_imp_df
</code></pre>
    <p class="normal">The <code class="inlineCode">dt_imp_df</code> DataFrame output by the preceding code can be appreciated in <em class="italic">Figure 3.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_11.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.11: Our decision tree’s feature importance</p>
    <p class="normal">This last feature importance table, <em class="italic">Figure 3.11</em>, increases suspicions about the delay features. They<a id="_idIndexMarker334"/> occupy, yet again, five of the top six positions. Is it possible that all five of them have such an outsized effect on the model?</p>
    <p class="normal"><strong class="keyWord">Interpretation and domain expertise</strong></p>
    <p class="normal">The <a id="_idIndexMarker335"/>target <code class="inlineCode">CARRIER_DELAY</code> is also called a dependent variable because it’s dependent on all the other features, the independent variables. Even though a statistical relationship doesn’t imply causation, we want to inform our feature selection based on our understanding of what independent variables could plausibly affect a dependent one. </p>
    <p class="normal">It makes sense that a departure delay (<code class="inlineCode">DEPARTURE_DELAY</code>) affects the arrival delay (which we removed), and therefore, <code class="inlineCode">CARRIER_DELAY</code>. Similarly, <code class="inlineCode">LATE_AIRCRAFT_DELAY</code> makes sense as a predictor because it is known before the flight takes off if a previous aircraft was several minutes late, causing this flight to be at risk of arriving late, but not as a cause of the current flight (ruling this option out). However, even though the Bureau of Transportation Statistics website defines delays in such a way that they appear to be discrete categories, some may be determined well after a flight has departed. For instance, in predicting a delay mid-flight, could we use <code class="inlineCode">WEATHER_DELAY</code> if the bad weather hasn’t yet happened? And could we use <code class="inlineCode">SECURITY_DELAY</code> if the security breach hasn’t yet occurred? The answers to these questions are that we probably shouldn’t because the rationale for including them is they could serve to rule out <code class="inlineCode">CARRIER_DELAY</code>, but this only works if they are discrete categories that pre-date the dependent variable! If they don’t they would be producing what is known as data leakage. Before coming to further conclusions, what you would need to do is talk to the airline executives to determine the timeline on which each delay category gets consistently set and (hypothetically) is accessible from the cockpit or the airline’s command center. Even if you are forced to remove them from the models, maybe other data can fill the void in a meaningful way, such as the first 30 minutes of flight logs and/or historical weather patterns.</p>
    <p class="normal">Interpretation is not always directly inferred from the data and the machine learning models, but by working closely with domain experts. But sometimes domain experts can mislead you too. In fact, another insight is with all the time-based metrics and categorical features we engineered at the beginning of the chapter (<code class="inlineCode">DEP_DOW</code>, <code class="inlineCode">DEST_HUB</code>, <code class="inlineCode">ORIGIN_HUB</code>, and so on). It turns out they have consistently had little to no effect on the models. Despite the airline executives hinting at the importance of days of the week, hubs, and congestion, we should have explored the data further, looking for correlations before engineering the data. But even if we do engineer some useless features, it also helps to use a <a id="_idIndexMarker336"/>white-box model to assess their impact, as we have. In data science, practitioners often will learn the same way that the most performant machine learning models do – by trial and error!</p>
    <h2 id="_idParaDest-79" class="heading-2">RuleFit</h2>
    <p class="normal"><strong class="keyWord">RuleFit</strong> is a<a id="_idIndexMarker337"/> model-class family <a id="_idIndexMarker338"/>that is a hybrid between a LASSO linear regression to get regularized coefficients for every feature and decision rules, which also uses LASSO to regularize. These <strong class="keyWord">decision</strong> <strong class="keyWord">rules</strong> are<a id="_idIndexMarker339"/> extracted by traversing a decision tree, finding interaction effects between features, and assigning coefficients to them based on their impact on the model. The implementation used in this chapter uses gradient-boosted decision trees to perform this task.</p>
    <p class="normal">We haven’t covered decision rules explicitly in this chapter, but they are yet another family of <strong class="keyWord">intrinsically interpretable models</strong>. They weren’t included because, at the time of writing, the only Python library that supports decision rules, called <strong class="keyWord">Bayesian Rule List</strong> (<strong class="keyWord">BRL</strong>) by Skater, is <a id="_idIndexMarker340"/>still at an experimental stage. In any case, the concept behind decision rules is very similar. They extract the feature interactions from a decision tree but don’t discard the leaf node, and instead of assigning coefficients, they use the predictions in the leaf node to construct the rules. The last rule is a catch-all, like an <em class="italic">ELSE</em> statement. Unlike RuleFit, it can only be understood sequentially because it’s so similar to any <em class="italic">IF-THEN-ELSE</em> statement, but that’s its main advantage.</p>
    <h3 id="_idParaDest-80" class="heading-3">Interpretation and feature importance</h3>
    <p class="normal">You can put<a id="_idIndexMarker341"/> everything <a id="_idIndexMarker342"/>you need to know about RuleFit into a single DataFrame (<code class="inlineCode">rulefit_df</code>). Then you remove the rules that have a coefficient of <code class="inlineCode">0</code>. It has these because in LASSO, unlike ridge, coefficient estimates converge to zero. You can sort the DataFrame by importance in a descending manner to see what features or feature interactions (in the form of rules) are most important:</p>
    <pre class="programlisting code"><code class="hljs-code">rulefit_df = reg_models[<span class="hljs-string">'rulefit'</span>][<span class="hljs-string">'fitted'</span>].get_rules()
rulefit_df = rulefit_df[rulefit_df.coef !=<span class="hljs-number">0</span>].\
sort_values(
    by=<span class="hljs-string">"importance"</span>, ascending=<span class="hljs-literal">False</span>
)
rulefit_df
</code></pre>
    <p class="normal">The rules in the <code class="inlineCode">rulefit_df</code> DataFrame can be seen in <em class="italic">Figure 3.12</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_03_12.png" alt="Table  Description automatically generated"/></p>
    <p class="packt_figref">Figure 3.12: RuleFit’s rules</p>
    <p class="normal">There’s a <code class="inlineCode">type</code> for <a id="_idIndexMarker343"/>every RuleFit feature in <em class="italic">Figure 3.12</em>. Those that are <code class="inlineCode">linear</code> are interpreted as you would any linear regression coefficient. Those<a id="_idIndexMarker344"/> that are <code class="inlineCode">type=rule</code> are also to be treated like binary features in a linear regression model. For instance, if the rule <code class="inlineCode">LATE_AIRCRAFT_DELAY &lt;= 333.5 &amp; DEP_DELAY &gt; 477.5</code> is true, then the coefficient <code class="inlineCode">172.103034</code> is applied to the prediction. The rules capture the interaction effects, so you don’t have to add interaction terms to the model manually or use some non-linear method to find them. Furthermore, it does this in an easy-to-understand manner. You can use RuleFit to guide your understanding of feature interactions even if you choose to productionize other models.</p>
    <h2 id="_idParaDest-81" class="heading-2">Nearest neighbors</h2>
    <p class="normal">Nearest neighbors <a id="_idIndexMarker345"/>is a<a id="_idIndexMarker346"/> family of models that even includes unsupervised methods. All of them use the closeness between data points to inform their predictions. Of all these methods, only the supervised k-NN and its cousin Radius Nearest Neighbors are somewhat interpretable.</p>
    <h3 id="_idParaDest-82" class="heading-3">k-Nearest Neighbors</h3>
    <p class="normal">The idea <a id="_idIndexMarker347"/>behind <strong class="keyWord">k-NN</strong> is straightforward. It takes the <em class="italic">k</em> closest <a id="_idIndexMarker348"/>points to a data point in the training data and uses their labels (<code class="inlineCode">y_train</code>) to inform the predictions. If it’s a classification task, it’s the <strong class="keyWord">mode</strong> of all the labels, and if it’s a regression task, it’s the <strong class="keyWord">mean</strong>. It’s a <strong class="keyWord">lazy learner</strong> because the “fitted model” is not much more than the training data and the parameters, such as <em class="italic">k</em> and the list of classes (if it’s a classification). It doesn’t do much till inference. That’s when it leverages the training data, tapping into it directly rather than extracting parameters, weights/biases, or coefficients learned by the model as <strong class="keyWord">eager learners</strong> do.</p>
    <h4 class="heading-4">Interpretation</h4>
    <p class="normal"><strong class="keyWord">k-NN</strong> only has<a id="_idIndexMarker349"/> local interpretability because since there’s no fitted model, you don’t have global modular or global holistic interpretability. For classification tasks, you could attempt to get a sense of this using the decision boundaries and regions we studied in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>. Still, it’s always based on local instances.</p>
    <p class="normal">To interpret a local point from our test dataset, we query the <code class="inlineCode">pandas</code> DataFrame using its index. We will be using flight #721043:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(X_test.loc[<span class="hljs-number">721043</span>,:])
</code></pre>
    <p class="normal">The preceding code outputs the following <code class="inlineCode">pandas</code> series:</p>
    <pre class="programlisting con"><code class="hljs-con">CRS_DEP_TIME         655.00
DEP_TIME            1055.00
DEP_DELAY            240.00
TAXI_OUT            35.00
WHEELS_OFF            1130.00
CRS_ARR_TIME        914.00
CRS_ELAPSED_TIME        259.00
DISTANCE            1660.00WEATHER_DELAY        0.00
NAS_DELAY            22.00
SECURITY_DELAY        0.00
LATE_AIRCRAFT_DELAY    221.00
DEP_AFPH             90.80
ARR_AFPH             40.43
DEP_MONTH            10.00
DEP_DOW            4.00
DEP_RFPH            0.89
ARR_RFPH            1.06
ORIGIN_HUB            1.0
DEST_HUB            0.00
PCT_ELAPSED_TIME        1.084942
Name: 721043, dtype: float64
</code></pre>
    <p class="normal">In the <code class="inlineCode">y_test_class</code> labels for flight #721043, we can tell that it was delayed because this code outputs 1:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(y_test_class[<span class="hljs-number">721043</span>])
</code></pre>
    <p class="normal">However, our k-NN model predicted that it was not because this code outputs 0:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'knn'</span>][<span class="hljs-string">'preds'</span>][X_test.index.get_loc(<span class="hljs-number">721043</span>)])
</code></pre>
    <p class="normal">Please <a id="_idIndexMarker350"/>note that the predictions are output as a <code class="inlineCode">numpy</code> array, so we can’t access the prediction for flight #721043 using its <code class="inlineCode">pandas</code> index (721043). We have to use the sequential location of this index in the test dataset using <code class="inlineCode">get_loc</code> to retrieve it.</p>
    <p class="normal">To find out why this was the case, we can use <code class="inlineCode">kneighbors</code> on our model to find the seven nearest neighbors of this point. To this end, we have to <code class="inlineCode">reshape</code> our data because <code class="inlineCode">kneighbors</code> will only accept it in the same shape found in the training set, which is (n, 21) where n is the number of observations (rows). In this case, <code class="inlineCode">n=1</code> because we only want the nearest neighbors for a single data point. And as you can tell from what was output by <code class="inlineCode">X_test.loc[721043,:]</code>, the <code class="inlineCode">pandas</code> series has a shape of (21,1), so we have to reverse this shape:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'knn'</span>][<span class="hljs-string">'fitted'</span>].\
      <span class="code-highlight"><strong class="hljs-slc">kneighbors</strong></span>(X_test.loc[<span class="hljs-number">721043</span>,:].values.reshape(<span class="hljs-number">1</span>,<span class="hljs-number">21</span>), <span class="hljs-number">7</span>))
</code></pre>
    <p class="normal"><code class="inlineCode">kneighbors</code> outputs two arrays:</p>
    <pre class="programlisting con"><code class="hljs-con">(array([[143.3160128 , 173.90740076, 192.66705727, 211.57109221,
         243.57211853, 259.61593993, 259.77507391]]),
array([[105172, 571912,  73409,  89450,  77474, 705972, 706911]]))
</code></pre>
    <p class="normal">The first is <a id="_idIndexMarker351"/>the distance of each of the seven closest training points to our test data point. And the second is the location of these data points in the training data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(y_train_class.iloc[[<span class="hljs-number">105172</span>, <span class="hljs-number">571912</span>, <span class="hljs-number">73409</span>, <span class="hljs-number">89450</span>, <span class="hljs-number">77474</span>,\
                          <span class="hljs-number">705972</span>, <span class="hljs-number">706911</span>]])
</code></pre>
    <p class="normal">The preceding code outputs the following <code class="inlineCode">pandas</code> series:</p>
    <pre class="programlisting con"><code class="hljs-con">3813      0
229062    1
283316    0
385831    0
581905    1
726784    1
179364    0
Name: CARRIER_DELAY, dtype: int64
</code></pre>
    <p class="normal">We can tell that the prediction reflects the <strong class="keyWord">mode</strong> because the most common class in the seven nearest points was 0 (not delayed). You can increase or decrease the <em class="italic">k</em> to see if this holds. Incidentally, when using binary classification, it’s recommended to choose an odd-numbered <em class="italic">k</em> so that there are no ties. Another important aspect is the distance metric that was used to select the closest data points. You can easily find out which one it is using:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'knn'</span>][<span class="hljs-string">'fitted'</span>].<span class="code-highlight"><strong class="hljs-slc">effective_metric_</strong></span>)
</code></pre>
    <p class="normal">The output is Euclidean, which makes sense for this example. After all, Euclidean is optimal for a <strong class="keyWord">real-valued vector space</strong> because<a id="_idIndexMarker352"/> most features are continuous. You could also test alternative distance metrics such as <code class="inlineCode">minkowski</code>, <code class="inlineCode">seuclidean</code>, or <code class="inlineCode">mahalanobis</code>. When most of your features are binary and categorical, you<a id="_idIndexMarker353"/> have an <strong class="keyWord">integer-valued</strong> <strong class="keyWord">vector space</strong>. So your distances ought to be calculated with algorithms suited for this space such as <code class="inlineCode">hamming</code> or <code class="inlineCode">canberra</code>.</p>
    <h4 class="heading-4">Feature importance</h4>
    <p class="normal">Feature<a id="_idIndexMarker354"/> importance is, after all, a global model interpretation method and k-NN has a hyper-local nature, so there’s no way of deriving feature importance from a k-NN model.</p>
    <h2 id="_idParaDest-83" class="heading-2">Naïve Bayes</h2>
    <p class="normal">Like GLMs, Naïve Bayes<a id="_idIndexMarker355"/> is a family of <a id="_idIndexMarker356"/>model classes with a model tailored to different statistical distributions. However, unlike GLMs’ assumption that the target <em class="italic">y</em> feature has the chosen distribution, all Naïve Bayes models assume that your <em class="italic">X</em> features have this distribution. More importantly, they were based on Bayes’ theorem of conditional probability, so they output a probability and are, therefore, exclusively classifiers. But they treat the probability of each feature impacting the model independently, which is a strong assumption. This<a id="_idIndexMarker357"/> is why they are called naïve. There’s one for Bernoulli called Bernoulli Naïve Bayes, one for multinomial <a id="_idIndexMarker358"/>called <strong class="keyWord">Multinomial Naïve Bayes</strong>, and, of course, one for Gaussian, which is the most common.</p>
    <h3 id="_idParaDest-84" class="heading-3">Gaussian Naïve Bayes</h3>
    <p class="normal">Bayes’ theorem<a id="_idIndexMarker359"/> is defined by this formula: <img src="../Images/B18406_03_031.png" alt="" role="presentation"/></p>
    <p class="normal">In other words, to <a id="_idIndexMarker360"/>find the probability of <em class="italic">A</em> happening given that <em class="italic">B</em> is true, you take the conditional probability of <em class="italic">B</em> given <em class="italic">A</em> is true times the probability of <em class="italic">A</em> occurring, divided by the probability of <em class="italic">B</em>. In the context of a machine learning classifier, this formula can be rewritten as follows:</p>
    <p class="center"><img src="../Images/B18406_03_032.png" alt="" role="presentation"/></p>
    <p class="normal">This is because what we want is the probability of <em class="italic">y</em> given <em class="italic">X</em> is true. But our <em class="italic">X</em> has more than one feature, so this can be expanded like this:</p>
    <p class="center"><img src="../Images/B18406_03_033.png" alt="" role="presentation"/></p>
    <p class="normal">To compute <img src="../Images/B18406_03_007.png" alt="" role="presentation"/> predictions, we have to consider that we have to calculate and compare probabilities for each <em class="italic">C</em><sub class="subscript-italic" style="font-style: italic;">k</sub> class (the probability of a delay versus the probability of no delay) and choose the class with the highest probability:</p>
    <p class="center"><img src="../Images/B18406_03_035.png" alt="" role="presentation"/></p>
    <p class="normal">Calculating <a id="_idIndexMarker361"/>the probability <a id="_idIndexMarker362"/>of each class <img src="../Images/B18406_03_036.png" alt="" role="presentation"/> (also known as the class prior) is <a id="_idIndexMarker363"/>relatively trivial. In fact, the fitted model has stored this in an attribute called <code class="inlineCode">class_prior_</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'naive_bayes'</span>][<span class="hljs-string">'fitted'</span>].class_prior_)
</code></pre>
    <p class="normal">This outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">array([0.93871674, 0.06128326])
</code></pre>
    <p class="normal">Naturally, since delays caused by the carrier only occur 6% of the time, there is a marginal probability of this occurring.</p>
    <p class="normal">Then the formula has a product <img src="../Images/B18406_03_037.png" alt="" role="presentation"/> of conditional probabilities that each feature belongs to a class <img src="../Images/B18406_03_038.png" alt="" role="presentation"/>. Since this is binary there’s no need to calculate the probabilities of multiple classes because they are inversely proportional. Therefore, we can drop <em class="italic">C</em><sub class="subscript-italic" style="font-style: italic;">k</sub> and replace it with a 1 like this:</p>
    <p class="center"><img src="../Images/B18406_03_039.png" alt="" role="presentation"/></p>
    <p class="normal">This is because what we are trying to predict is the probability of a delay. Also, <img src="../Images/B18406_03_040.png" alt="" role="presentation"/> is its own formula, which differs according to the assumed distribution of the model – in this case, Gaussian:</p>
    <p class="center"><img src="../Images/B18406_03_041.png" alt="" role="presentation"/></p>
    <p class="normal">This <a id="_idIndexMarker364"/>formula <a id="_idIndexMarker365"/>is called the probability density of the Gaussian distribution.</p>
    <h4 class="heading-4">Interpretation and feature importance</h4>
    <p class="normal">So what <a id="_idIndexMarker366"/>are<a id="_idIndexMarker367"/> these <strong class="keyWord">sigmas</strong> (<img src="../Images/B18406_03_042.png" alt="" role="presentation"/>) and <strong class="keyWord">thetas</strong> (<img src="../Images/B18406_03_043.png" alt="" role="presentation"/>) in<a id="_idIndexMarker368"/> the formula? They are, respectively, the variance and mean of the <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> feature when <img src="../Images/B18406_03_044.png" alt="" role="presentation"/>. The concept behind this is that features have a different variance and mean in one class versus another, which can inform the classification. This is a binary classification task, but you could calculate <img src="../Images/B18406_03_045.png" alt="" role="presentation"/>and <img src="../Images/B18406_03_043.png" alt="" role="presentation"/> for both classes. Fortunately, the fitted model has this stored:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'naive_bayes'</span>][<span class="hljs-string">'fitted'</span>].<span class="code-highlight"><strong class="hljs-slc">sigma_</strong></span>)
</code></pre>
    <p class="normal">There are<a id="_idIndexMarker369"/> two arrays output, the first one corresponding <a id="_idIndexMarker370"/>to the negative class and the second to the positive. The arrays contain the sigma (variance) for each of the 21 features given the class:</p>
    <pre class="programlisting con"><code class="hljs-con">array([[2.50123026e+05, 2.61324730e+05, ..., 1.13475535e-02],
       [2.60629652e+05, 2.96009867e+05, ..., 1.38936741e-02]])
</code></pre>
    <p class="normal">You can also extract the thetas (means) from the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(class_models[<span class="hljs-string">'naive_bayes'</span>][<span class="hljs-string">'fitted'</span>].<span class="code-highlight"><strong class="hljs-slc">theta_</strong></span>)
</code></pre>
    <p class="normal">The preceding code also outputs two arrays, one for each class:</p>
    <pre class="programlisting con"><code class="hljs-con">array([[1.30740577e+03, 1.31006271e+03, ..., 9.71131781e-01],
       [1.41305545e+03, 1.48087887e+03, ..., 9.83974416e-01]])
</code></pre>
    <p class="normal">These two arrays are all you need to debug and interpret Naïve Bayes results because you can use them to compute the conditional probability that the <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> feature is given a positive class <img src="../Images/B18406_03_047.png" alt="" role="presentation"/>. You could use this probability to rank the features by importance on a global level, or interpret a specific prediction on a local level.</p>
    <p class="normal"><em class="italic">Naïve Bayes</em> is a fast algorithm with some good use cases, such as spam filtering and recommendation systems, but the independence assumption hinders its performance in most situations. Speaking of performance, let’s discuss this topic in the context of interpretability.</p>
    <h1 id="_idParaDest-85" class="heading-1">Recognizing the trade-off between performance and interpretability</h1>
    <p class="normal">We have <a id="_idIndexMarker371"/>briefly touched on this topic before, but high performance often requires complexity, and complexity inhibits interpretability. As studied in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, this complexity comes from primarily three sources: non-linearity, non-monotonicity, and<a id="_idIndexMarker372"/> interactivity. If the model adds any complexity, it is <strong class="keyWord">compounded by the number and nature of features</strong> in your dataset, which by itself is a source of complexity.</p>
    <h2 id="_idParaDest-86" class="heading-2">Special model properties</h2>
    <p class="normal">These <a id="_idIndexMarker373"/>special properties can help make a model more interpretable.</p>
    <h3 id="_idParaDest-87" class="heading-3">The key property: explainability</h3>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, we<a id="_idIndexMarker374"/> discussed why being able to look under the hood of the model and intuitively understand how all its moving parts derive its predictions in a consistent manner is, mostly, what separates <em class="italic">explainability</em> from <em class="italic">interpretability</em>. This property is also<a id="_idIndexMarker375"/> called <strong class="keyWord">transparency</strong> or <strong class="keyWord">translucency</strong>. A <a id="_idIndexMarker376"/>model can be interpretable without this, but in the same way as interpreting a person’s decisions because we can’t understand what is going on “under the hood.” This is often<a id="_idIndexMarker377"/> called <strong class="keyWord">post-hoc interpretability</strong> and this is the kind of interpretability this book primarily focuses on, with a few exceptions. That being said, we ought to recognize that if a model is understood by leveraging its mathematical formula (grounded in statistical and probability theory), as we’ve done with linear regression and Naïve Bayes, or by visualizing a human-interpretable structure, as with decision trees, or a set of rules as with RuleFit, it is much more interpretable than machine learning model classes where none of this is practically possible.</p>
    <p class="normal">White-box models will always have the upper hand in this regard, and as listed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, there are many use cases in which a white-box model is a must-have. But even if you don’t productionize white-box models, they can always serve a purpose in assisting with interpretation, if data dimensionality allows. Transparency is a key property because it wouldn’t matter if it didn’t comply with the other properties as long as it had explainability; it would still be more interpretable than those without it.</p>
    <h3 id="_idParaDest-88" class="heading-3">The remedial property: regularization</h3>
    <p class="normal">In this chapter, we’ve <a id="_idIndexMarker378"/>learned that <em class="italic">regularization</em> limits the complexity added by the introduction of too many features, and this can make the model more interpretable, not to mention more performant. Some models incorporate regularization into the training algorithm, such as RuleFit and gradient-boosted trees; others have the ability to integrate it, such as multi-layer perceptron, or linear regression, and some cannot include it, such as k-NN. Regularization comes in many forms. Decision trees have a method called pruning, which can help reduce complexity by removing non-significant branches. Neural networks have a technique called dropout, which randomly drops neural network nodes from layers during training. Regularization is a remedial property because it can help even the least interpretable models lessen complexity and thus improve interpretability.</p>
    <h2 id="_idParaDest-89" class="heading-2">Assessing performance</h2>
    <p class="normal">By<a id="_idIndexMarker379"/> now, in this chapter, you have already assessed performance on all of the white-box models reviewed in the last section as well as a few black-box models. Maybe you’ve already noticed that black-box models have topped most metrics, and for most use cases, this is generally the case.</p>
    <p class="normal">Figuring out which model classes are more interpretable is not an exact science, but the following table (<em class="italic">Figure 3.17</em>) is sorted by those models with the most desirable properties – that is, they don’t introduce non-linearity, non-monotonicity, and interactivity. Of course, explainability on its own is a property that is a game-changer, regardless, and regularization can help. There are also cases in which it’s hard to assess properties. For instance, polynomial (linear) regression implements a linear model, but it fits non-linear relationships, which is why it is color-coded differently. As you will learn in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>, some libraries support adding monotonic constraints to gradient-boosted trees and neural networks, which means it’s possible to make these monotonic. However, the black-box methods we used in this chapter do not support monotonic constraints.</p>
    <p class="normal">The task columns tell you whether they can be used for regression or classification. And the <strong class="keyWord">Performance Rank</strong> columns<a id="_idIndexMarker380"/> show you how well these models ranked in RMSE (for regression) and ROC AUC (for classification), where lower ranks are better. Please note that even though we have used only one metric to assess performance for this chart for simplicity’s sake, the discussion about performance should be more nuanced than that. Another thing to note is that ridge regression did poorly, but this is because we used the wrong hyperparameters, as explained in the previous section:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_13.png" alt="A picture containing chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.13: A table assessing the interpretability and performance of several white-hat and black-box models we have explored in this chapter</p>
    <p class="normal">Because it’s <a id="_idIndexMarker381"/>compliant with all five properties, it’s easy to tell why <strong class="keyWord">linear regression is the gold standard for interpretability</strong>. Also, while recognizing that this is anecdotal evidence, it should be immediately apparent that most of the best ranks are with black-box models. This is no accident! The math behind neural networks and gradient-boosted trees is brutally efficient in achieving the best metrics. Still, as the red dots suggest, they have all the properties that make a model less interpretable, making their biggest strength (complexity) a potential weakness.</p>
    <p class="normal">This is precisely why black-box models are our primary interest in this book, although many of the methods you will learn to apply to white-box models. In <em class="italic">Part 2</em>, which comprises <em class="chapterRef">Chapters 4</em> to <em class="chapterRef">9</em>, we will learn model-agnostic and deep-learning-specific methods that assist with interpretation. And in <em class="italic">Part 3</em>, which includes <em class="chapterRef">Chapters 10</em> to <em class="chapterRef">14</em>, we will learn how to tune models and datasets to increase interpretability:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_14.png" alt="Chart, funnel chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.14: A table comparing white-box, black-box, and glass-box models, or at least what is known so far about them</p>
    <h1 id="_idParaDest-90" class="heading-1">Discovering newer interpretable (glass-box) models</h1>
    <p class="normal">In the<a id="_idIndexMarker382"/> last decade, there have been significant efforts in both industry and in academia to create new models that can have enough complexity to find the sweet spot between underfitting and overfitting, known as the <strong class="keyWord">bias-variance trade-off</strong>, but<a id="_idIndexMarker383"/> retain an adequate level of explainability.</p>
    <p class="normal">Many models fit this description, but most of them are meant for specific use cases, haven’t been properly tested yet, or have released a library or open-sourced code. However, two general-purpose ones are already gaining traction, which we will look at now.</p>
    <h2 id="_idParaDest-91" class="heading-2">Explainable Boosting Machine (EBM)</h2>
    <p class="normal"><strong class="keyWord">EBM</strong> is part of<a id="_idIndexMarker384"/> Microsoft’s InterpretML framework, which includes many of the model-agnostic methods <a id="_idIndexMarker385"/>we will use later in the book.</p>
    <p class="normal">EBM leverages the <strong class="keyWord">GAMs</strong> we mentioned earlier, which are like linear models but look like this:</p>
    <p class="center"><img src="../Images/B18406_03_048.png" alt="" role="presentation"/></p>
    <p class="normal">Individual functions <em class="italic">f</em><sub class="subscript-italic" style="font-style: italic;">1</sub> through <em class="italic">f</em><sub class="subscript-italic" style="font-style: italic;">p</sub> are fitted to each feature using spline functions. Then a link function <em class="italic">g</em> adapts the GAM to perform different tasks such as classification or regression, or adjust predictions to different statistical distributions. GAMs are white-box models, so what makes EBM a glass-box model? It incorporates bagging and gradient boosting, which tend to make models more performant. The boosting is done one feature at a time using a low learning rate so as not to confound them. It also finds practical interaction terms automatically, which improves performance while maintaining interpretability:</p>
    <p class="center"><img src="../Images/B18406_03_049.png" alt="" role="presentation"/></p>
    <p class="normal">Once fitted, this formula is made up of complicated non-linear formulas, so a global holistic interpretation isn’t likely feasible. However, since the effects of each feature or pairwise interaction terms are additive, they are easily separable, and global modular interpretation is entirely possible. Local interpretation is equally easy, given that a mathematical formula can assist in debugging any prediction.</p>
    <p class="normal">One <a id="_idIndexMarker386"/>drawback is that EBM can be much slower than gradient-boosted trees and neural networks because of the <em class="italic">one feature at a time</em> approach, a low learning rate not impacting the feature order, and spline fitting methods. However, it is parallelizable, so in environments with ample resources and multiple cores or machines, it will be much quicker. To avoid waiting for results for an hour or two, it is best to create abbreviated versions of <code class="inlineCode">X_train</code> and <code class="inlineCode">X_test</code> – that is, with fewer columns representing only the eight features white-box models found to be most important: <code class="inlineCode">DEP_DELAY</code>, <code class="inlineCode">LATE_AIRCRAFT_DELAY</code>, <code class="inlineCode">PCT_ELAPSED_TIME</code>, <code class="inlineCode">WEATHER_DELAY, NAS_DELAY</code>, <code class="inlineCode">SECURITY_DELAY</code>, <code class="inlineCode">DISTANCE</code>, <code class="inlineCode">CRS_ELAPSED_TIME</code>, and <code class="inlineCode">TAXI_OUT</code>. These are placed in a <code class="inlineCode">feature_samp</code> array, and then the <code class="inlineCode">X_train</code> and <code class="inlineCode">X_test</code> DataFrames are subset to only include this feature. We are setting the <code class="inlineCode">sample2_size</code> to 10%, but if you feel you have enough resources to handle it, adjust accordingly:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Make new abbreviated versions of datasets</span>
feature_samp = [
    <span class="hljs-string">'DEP_DELAY'</span>,
    <span class="hljs-string">'LATE_AIRCRAFT_DELAY'</span>,
    <span class="hljs-string">'PCT_ELAPSED_TIME'</span>,
    <span class="hljs-string">'DISTANCE'</span>,
    <span class="hljs-string">'WEATHER_DELAY'</span>,
    <span class="hljs-string">'NAS_DELAY'</span>,
    <span class="hljs-string">'SECURITY_DELAY'</span>,
    <span class="hljs-string">'CRS_ELAPSED_TIME'</span>
]
X_train_abbrev2 = X_train[feature_samp]
X_test_abbrev2 = X_test[feature_samp]
<span class="hljs-comment">#For sampling among observations</span>
np.random.seed(rand)
sample2_size = <span class="hljs-number">0.1</span>
sample2_idx = np.random.choice(
    X_train.shape[<span class="hljs-number">0</span>], math.ceil(
      X_train.shape[<span class="hljs-number">0</span>]*sample2_size), replace=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">To train <a id="_idIndexMarker387"/>your EBM, all you have to do is instantiate an <code class="inlineCode">ExplainableBoostingClassifier()</code> and then fit your model to your training data. Note that we are using <code class="inlineCode">sample_idx</code> to sample a portion of the data so that it takes less time:</p>
    <pre class="programlisting code"><code class="hljs-code">ebm_mdl = <span class="code-highlight"><strong class="hljs-slc">ExplainableBoostingClassifier</strong></span>()
ebm_mdl.fit(X_train_abbrev2.iloc[sample2_idx], y_train_class.iloc[sample2_idx])
</code></pre>
    <h3 id="_idParaDest-92" class="heading-3">Global interpretation</h3>
    <p class="normal">Global interpretation<a id="_idIndexMarker388"/> is dead simple. It comes with an <code class="inlineCode">explain_global</code> dashboard you can explore. It loads with the feature importance plot first, and you can select individual features to graph what was learned from each one:</p>
    <pre class="programlisting code"><code class="hljs-code">show(ebm_mdl.<span class="code-highlight"><strong class="hljs-slc">explain_global</strong></span>())
</code></pre>
    <p class="normal">The preceding code generates a dashboard that looks like <em class="italic">Figure 3.15</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_03_15.png" alt="Chart, bar chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 3.15: EBM’s global interpretation dashboard</p>
    <h3 id="_idParaDest-93" class="heading-3">Local interpretation</h3>
    <p class="normal">Local interpretation<a id="_idIndexMarker389"/> uses a dashboard like global does, except you choose specific predictions to interpret with <code class="inlineCode">explain_local</code>. In this case, we are selecting #76, which, as you can tell, was incorrectly predicted. But the LIME-like plot we will study in <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>, helps us make sense of it:</p>
    <pre class="programlisting code"><code class="hljs-code">ebm_lcl = ebm_mdl.<span class="code-highlight"><strong class="hljs-slc">explain_local</strong></span>(
    X_test_abbrev2.iloc[<span class="hljs-number">76</span>:<span class="hljs-number">77</span>], y_test_class[<span class="hljs-number">76</span>:<span class="hljs-number">77</span>], name=<span class="hljs-string">'EBM'</span>
)
show(ebm_lcl)
</code></pre>
    <p class="normal">Similar to the global dashboard, the preceding code generates another one, depicted in <em class="italic">Figure 3.16</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_16.png" alt="Chart, bar chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.16: EBM’s local interpretation dashboard</p>
    <h3 id="_idParaDest-94" class="heading-3">Performance</h3>
    <p class="normal">EBM <a id="_idIndexMarker390"/>performance, at least measured with the ROC AUC, is not far from what was achieved by the top two classification models, and we can only expect it to get better with 10 times more training and testing data!</p>
    <pre class="programlisting code"><code class="hljs-code">ebm_perf = <span class="code-highlight"><strong class="hljs-slc">ROC</strong></span>(ebm_mdl.predict_proba).<span class="code-highlight"><strong class="hljs-slc">explain_perf</strong></span>(
    X_test_abbrev2.iloc[sample_idx],y_test_class.iloc[sample_idx],
    name=<span class="hljs-string">'EBM'</span>
)
show(ebm_perf)
</code></pre>
    <p class="normal">You can appreciate the performance dashboard produced by the preceding code in <em class="italic">Figure 3.17</em>.</p>
    <p class="packt_figref"><img src="../Images/B18406_03_17.png" alt="Chart, line chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 3.17: One of EBM’s performance dashboards</p>
    <p class="normal">The performance dashboard can also compare several models at a time since its explainers are model-agnostic. And there’s even a fourth dashboard that can be used for data exploration. Next, we will cover another GAM-based model.</p>
    <h2 id="_idParaDest-95" class="heading-2">GAMI-Net</h2>
    <p class="normal">There’s <a id="_idIndexMarker391"/>also a newer GAM-based method<a id="_idIndexMarker392"/> with similar properties to EBM but trained with neural networks. At the time of writing, this method has yet to get commercial traction but yields good interpretability and performance.</p>
    <p class="normal">As we have previously discussed, interpretability is decreased by each additional feature, especially those that don’t significantly impact model performance. In addition to too many features, it’s also trumped by the added complexity of non-linearities, non-monotonicity, and interactions. GAMI-Net tackles all these problems by fitting non-linear subnetworks for each feature in the main effects network first. Then, fitting a pairwise interaction network with subnetworks for each combination of features. The user provides a maximum number of interactions to keep, which are then fitted to the residuals of the main effects network. See <em class="italic">Figure 3.18</em> for a diagram.</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_18.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 3.18: Diagram of the GAMI-Net model</p>
    <p class="normal">GAMI-Net has three <a id="_idIndexMarker393"/>interpretability constraints built in:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sparsity</strong>: Only the top features and interactions are kept.</li>
      <li class="bulletList"><strong class="keyWord">Heredity</strong>: A pairwise interaction can be included if at least one of its parent features is included.</li>
      <li class="bulletList"><strong class="keyWord">Marginal clarity</strong>: Non-orthogonality in interactions is penalized to approximate better marginal clarity.</li>
    </ul>
    <p class="normal">The GAMI-Net implementation <a id="_idIndexMarker394"/>can also enforce monotonic constraints, which we will cover in more detail in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>.</p>
    <p class="normal">Before<a id="_idIndexMarker395"/> we <a id="_idIndexMarker396"/>start, we must create a dictionary called <code class="inlineCode">meta_info</code> with details about each feature and target, such as the type (continuous, categorical, and target) and the scaler used to scale each feature — since the library expects each feature to be scaled independently. All the features in the abbreviated dataset are continuous so we can leverage dictionary comprehension to this easily:</p>
    <pre class="programlisting code"><code class="hljs-code">meta_info = {col:{<span class="hljs-string">"type"</span>:<span class="hljs-string">"continuous"</span>} <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span>\
                                         X_train_abbrev.columns}
</code></pre>
    <p class="normal">Next, we will create a copy of <code class="inlineCode">X_train_abbrev</code> and <code class="inlineCode">X_train_abbrev</code> and then scale them and store the scalers in the dictionary. Then, we will append information about the target variable to the dictionary. And lastly, we will convert all the data to <code class="inlineCode">numpy</code> format:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train_abbrev2 = X_train_abbrev.copy()
X_test_abbrev2 = X_test_abbrev.copy()
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> meta_info.keys():
    scaler = MinMaxScaler()
    X_train_abbrev2[[key]] =scaler.fit_transform(
      X_train_abbrev2[[key]])
    X_test_abbrev2[[key]] = scaler.transform(X_test_abbrev2[[key]])
    meta_info[key][<span class="hljs-string">"scaler"</span>] = scaler
meta_info[<span class="hljs-string">"CARRIER_DELAY"</span>] = {<span class="hljs-string">"type"</span>:<span class="hljs-string">"target"</span>, <span class="hljs-string">"values"</span>:[<span class="hljs-string">"no"</span>, <span class="hljs-string">"yes"</span>]}
X_train_abbrev2 = X_train_abbrev2.to_numpy().astype(np.float32)
X_test_abbrev2 = X_test_abbrev2.to_numpy().astype(np.float32)
y_train_class2 = y_train_class.to_numpy().reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
y_test_class2 = y_test_class.to_numpy().reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now that we have a <code class="inlineCode">meta_info</code> dictionary and the dataset is ready, we can initialize and fit GAMI-Net to the training data. In addition to <code class="inlineCode">meta_info</code>, it has a lot of parameters: <code class="inlineCode">interact_num</code> defines how many top interactions it should consider, and <code class="inlineCode">task_type</code> defines whether it’s a classification or regression task. Note that GAMI-Net trains three neural networks, so there are three epoch parameters to fill in (<code class="inlineCode">main_effect_epochs</code>, <code class="inlineCode">interaction_epochs</code>, and <code class="inlineCode">tuning_epochs</code>). The learning rate (<code class="inlineCode">lr_bp</code>) and early stopping thresholds (<code class="inlineCode">early_stop_thres</code>) are entered as a list for each of the epoch parameters. You will also find lists for the architecture of the networks, where each item corresponds <a id="_idIndexMarker397"/>to a number of nodes per layer (<code class="inlineCode">interact_arch</code> and <code class="inlineCode">subnet_arch</code>). Furthermore, there are additional parameters for <a id="_idIndexMarker398"/>batch size, activation function, whether to enforce heredity constraint, a loss threshold for early stopping, and what percentage of the training data to use for validation (<code class="inlineCode">val_ratio</code>). Finally, there are two optional parameters for monotonic constraints (<code class="inlineCode">mono_increasing_list</code>, <code class="inlineCode">mono_decreasing_list</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">gami_mdl = GAMINet(
    meta_info=meta_info,
    interact_num=<span class="hljs-number">8</span>,
    task_type=<span class="hljs-string">"Classification"</span>,
    main_effect_epochs=<span class="hljs-number">80</span>,
    interaction_epochs=<span class="hljs-number">60</span>,
    tuning_epochs=<span class="hljs-number">40</span>,
    lr_bp=[<span class="hljs-number">0.0001</span>] * <span class="hljs-number">3</span>,
    early_stop_thres=[<span class="hljs-number">10</span>] * <span class="hljs-number">3</span>,
    interact_arch=[<span class="hljs-number">20</span>] * <span class="hljs-number">5</span>,
    subnet_arch=[<span class="hljs-number">20</span>] * <span class="hljs-number">5</span>,
    batch_size=<span class="hljs-number">200</span>,
    activation_func=tf.nn.relu,
    heredity=<span class="hljs-literal">True</span>, 
    loss_threshold=<span class="hljs-number">0.01</span>,
    val_ratio=<span class="hljs-number">0.2</span>,
    verbose=<span class="hljs-literal">True</span>,
    reg_clarity=<span class="hljs-number">1</span>,
    random_state=rand
)
gami_mdl.fit(X_train_abbrev2, y_train_class2)
</code></pre>
    <p class="normal">We can plot the training loss for each epoch across all three trainings with <code class="inlineCode">plot_trajectory</code>. Then, with <code class="inlineCode">plot_regularization</code>, we can plot the outcome for the regularization of both the main effects and interaction networks. Both plotting functions can save the image in a folder but will do so in a folder called <code class="inlineCode">results</code> by default, unless you change the path with the <code class="inlineCode">folder</code> parameter:</p>
    <pre class="programlisting code"><code class="hljs-code">data_dict_logs = gami_mdl.summary_logs(save_dict=<span class="hljs-literal">False</span>)
plot_trajectory(data_dict_logs)
plot_regularization(data_dict_logs)
</code></pre>
    <p class="normal">The preceding snippet generates the plots in <em class="italic">Figure 3.19</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_19.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <figure class="mediaobject">Figure 3.19: The trajectory and regularization plots for the GAMI-net training process</figure>
    <p class="normal"><em class="italic">Figure 3.19</em> tells<a id="_idIndexMarker399"/> the story of how the three stages sequentially<a id="_idIndexMarker400"/> reduce loss while regularizing to only keep the fewest features and interactions as possible.</p>
    <h3 id="_idParaDest-96" class="heading-3">Global interpretation</h3>
    <p class="normal">Global explanations <a id="_idIndexMarker401"/>can be extracted in a dictionary with the <code class="inlineCode">global_explain</code> function and then turned into a feature importance plot with <code class="inlineCode">feature_importance_visualize</code>, like in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">data_dict_global = gami_mdl.global_explain(save_dict=<span class="hljs-literal">True</span>)
feature_importance_visualize(data_dict_global)
plt.show()
</code></pre>
    <p class="normal">The preceding snippet outputs the following plot:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_20.png" alt="Histogram  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 3.20: A global explanation plot</p>
    <p class="normal">As you can<a id="_idIndexMarker402"/> tell by <em class="italic">Figure 3.20</em>, the most important feature is, by far, <code class="inlineCode">DEP_DELAY</code> and one interaction is among the top six features in the plot. We can also use the <code class="inlineCode">global_visualize_density</code> plot to output partial dependence plots, which we will cover in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>.</p>
    <h3 id="_idParaDest-97" class="heading-3">Local interpretation</h3>
    <p class="normal">Let’s examine an<a id="_idIndexMarker403"/> explanation for a single prediction using <code class="inlineCode">local_explain</code>, followed by <code class="inlineCode">local_visualize</code>. We are selecting test case #73:</p>
    <pre class="programlisting code"><code class="hljs-code">data_dict_local = gami_mdl.local_explain(
    X_test_abbrev2[[<span class="hljs-number">73</span>]], y_test_class2[[<span class="hljs-number">73</span>]], save_dict=<span class="hljs-literal">False</span>
)
local_visualize(data_dict_local[<span class="hljs-number">0</span>])
plt.tight_layout()
plt.show()
</code></pre>
    <p class="normal">The preceding code generates the plot in <em class="italic">Figure 3.21</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_03_21.png" alt="Graphical user interface, application, Teams  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.21: A local explanation plot for test case #73</p>
    <p class="normal"><em class="italic">Figure 3.21</em> tells the story of how each feature weighs in the outcome. Note that <code class="inlineCode">DEP_DELAY</code> is over 50 but that there’s an intercept that almost cancels it out. The intercept is a counterbalance – after all, the dataset is unbalanced toward it being less likely to be a <code class="inlineCode">CARRIER_DELAY</code>. But all the subsequent features after the intercept are not enough to push the outcome positively.</p>
    <h3 id="_idParaDest-98" class="heading-3">Performance</h3>
    <p class="normal">To determine<a id="_idIndexMarker404"/> the predictive performance of the GAMI-Net model, all we need to do is get the scores (<code class="inlineCode">y_test_prob</code>) and predictions (<code class="inlineCode">y_test_pred</code>) for the test dataset and then use scikit-learn’s <code class="inlineCode">metrics</code> functions to compute them:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_prob = gami_mdl.predict(X_test_abbrev2)
y_test_pred = np.where(y_test_prob &gt; <span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">'accuracy: %.3g, recall: %.3g, roc auc: %.3g, f1: %.3g,</span>
<span class="hljs-string">      mcc: %.3g'</span>
    % (
        metrics.accuracy_score(y_test_class2, y_test_pred),
        metrics.recall_score(y_test_class2, y_test_pred),
        metrics.roc_auc_score(y_test_class2, y_test_prob),
        metrics.f1_score(y_test_class2, y_test_pred),
        metrics.matthews_corrcoef(y_test_class2, y_test_pred)
    )
)
</code></pre>
    <p class="normal">The preceding code yields the following metrics:</p>
    <pre class="programlisting con"><code class="hljs-con">accuracy: 0.991, recall: 0.934, roc auc: 0.998,
f1: 0.924, mcc: 0.919 
</code></pre>
    <p class="normal">The performance was not bad considering it was trained on 10% of the training data and evaluated on only 10% of the test data – especially the recall score, which was among the top three places.</p>
    <h1 id="_idParaDest-99" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission was to train models that could predict preventable delays with enough accuracy to be useful, and then, to understand the factors that impacted these delays, according to these models, to improve OTP. The resulting regression models all predicted delays, on average, well below the 15-minute threshold according to the RMSE. And most of the classification models achieved an F1 score well above 50% – one of them reached 98.8%! We also managed to find factors that impacted delays for all white-box models, some of which performed reasonably well. So, it seems like it was a resounding success!</p>
    <p class="normal">Don’t celebrate just yet! Despite the high metrics, this mission was a failure. Through interpretation methods, we realized that the models were accurate mostly for the wrong reasons. This realization helps underpin the mission-critical lesson that a model can easily be right for the wrong reasons, so <em class="italic">the question “why?” is not a question to be asked only when it performs poorly but always</em>. And using interpretation methods is how we ask that question.</p>
    <p class="normal">But if the mission failed, why is this section called <em class="italic">Mission accomplished?</em> Good question!</p>
    <p class="normal">It turns out there was a secret mission. Hint: it’s the title of this chapter. The point of it was to learn about common interpretation challenges through the failure of the overt mission. In case you missed them, here are the interpretation challenges we stumbled upon:</p>
    <ul>
      <li class="bulletList">Traditional model interpretation methods only cover surface-level questions about your models. Note that we had to resort to model-specific global interpretation methods to discover that the models were right for the wrong reasons.</li>
      <li class="bulletList">Assumptions can derail any machine learning project since this is information that you suppose without evidence. Note that it is crucial to work closely with domain experts to inform decisions throughout the machine learning workflow, but sometimes they can also mislead you. Ensure you check for inconsistencies between the data and what you assume to be the truth about that data. Finding and correcting these problems is at the heart of what interpretability is about.</li>
      <li class="bulletList">Many model classes, even white-box models, have issues with computing feature importance consistently and reliably.</li>
      <li class="bulletList">Incorrect model tuning can lead to a model that performs well enough but is less interpretable. Note that a regularized model overfits less but is also more interpretable. We will cover methods to address this challenge in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>. Feature selection and engineering can also have the same effect, which you can read about in <em class="chapterRef">Chapter 10</em>, <em class="italic">Feature Selection and Engineering for Interpretability</em>.</li>
      <li class="bulletList">There’s a trade-off between predictive performance and interpretability. And this trade-off extends to execution speed. For these reasons, this book primarily focuses on black-box models, which have the predictive performance we want and a reasonable execution speed but could use some help on the interpretability side.</li>
    </ul>
    <p class="normal">If you learned about these challenges, then congratulations! Mission accomplished!</p>
    <h1 id="_idParaDest-100" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, we covered some traditional methods for interpretability and what their limitations are. We learned about <strong class="keyWord">intrinsically interpretable models</strong> and how to both use them and interpret them, for both regression and classification. We also studied the <strong class="keyWord">performance versus interpretability trade-off</strong> and some models that attempt not to compromise in this trade-off. We also discovered many practical interpretation challenges involving the roles of feature selection and engineering, hyperparameters, domain experts, and execution speed.</p>
    <p class="normal">In the next chapter, we will learn more about different interpretation methods to measure the effect of a feature on a model.</p>
    <h1 id="_idParaDest-101" class="heading-1">Dataset sources</h1>
    <p class="normal">United States Department of Transportation Bureau of Transportation Statistics. (2018). Airline On-Time Performance Data. Originally retrieved from <a href="https://www.transtats.bts.gov"><span class="url">https://www.transtats.bts.gov</span></a>.</p>
    <h1 id="_idParaDest-102" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Friedman, J. and Popescu, B, 2008, <em class="italic">Predictive Learning via Rule Ensembles</em>. The Annals of Applied Statistics, 2(3), 916-954. <a href="http://doi.org/10.1214/07-AOAS148"><span class="url">http://doi.org/10.1214/07-AOAS148</span></a></li>
      <li class="bulletList">Hastie, T., Tibshirani, R., and Wainwright, M., 2015, <em class="italic">Statistical Learning with Sparsity: The Lasso and Generalizations</em>. Chapman &amp; Hall/Crc Monographs on Statistics &amp; Applied Probability, Taylor &amp; Francis</li>
      <li class="bulletList">Thomas, D.R., Hughes, E., and Zumbo, B.D., 1998, <em class="italic">On Variable Importance in Linear Regression.</em> Social Indicators Research 45, 253–275: <a href="https://doi.org/10.1023/A:1006954016433"><span class="url">https://doi.org/10.1023/A:1006954016433</span></a></li>
      <li class="bulletList">Nori, H., Jenkins, S., Koch, P., and Caruana, R., 2019, <em class="italic">InterpretML: A Unified Framework for Machine Learning Interpretability</em>. arXiv preprint: <a href="https://arxiv.org/pdf/1909.09223.pdf"><span class="url">https://arxiv.org/pdf/1909.09223.pdf</span></a></li>
      <li class="bulletList">Hastie, T., and Tibshirani, R., 1987, <em class="italic">Generalized Additive Models: Some Applications.</em> Journal of the American Statistical Association, 82(398):371–386: <a href="http://doi.org/10.2307%2F2289439"><span class="url">http://doi.org/10.2307%2F2289439</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_3.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>