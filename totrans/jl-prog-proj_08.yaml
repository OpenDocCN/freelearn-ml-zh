- en: Leveraging Unsupervised Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our supervised machine learning project was a success and we're well on our
    way to becoming experts in recommender systems. It's now time to leave behind
    the safety of our neatly tagged data and venture into the unknown. Yes, I'm talking
    about unsupervised machine learning. In this chapter, we'll train a model that
    will help us find hidden patterns in a mountain of data. And since we've come
    so far on our journey of learning Julia, it's time to take off the training wheels
    and take on our first client.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Just kidding—for now, we'll play pretend, but we'll indeed tackle a machine
    learning problem that could very well be one of the first tasks of a junior data
    scientist. We'll help our imaginary customer discover key insights for supporting
    their advertising strategy, a very important component of beginning their operations
    in San Francisco.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In the process, we''ll learn about the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: What unsupervised machine learning is and when and how to use it
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of clustering, one of the most important unsupervised learning tasks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform efficient data munging with the help of query
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaprogramming in Julia
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and running unsupervised machine learning models with clustering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Julia package ecosystem is under continuous development and new package
    versions are released on a daily basis. Most of the times this is great news,
    as new releases bring new features and bug fixes. However, since many of the packages
    are still in beta (version 0.x), any new release can introduce breaking changes.
    As a result, the code presented in the book can stop working. In order to ensure
    that your code will produce the same results as described in the book, it is recommended
    to use the same package versions. Here are the external packages used in this
    chapter and their specific versions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to install a specific version of a package you need to run:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively you can install all the used packages by downloading the `Project.toml`
    file provided with the chapter and using `pkg>` instantiate as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Unsupervised machine learning
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](a3fe07c4-b551-4573-ba72-edba84b1041a.xhtml), *Machine Learning
    For Recommender Systems*, we learned about supervised machine learning. We used
    various features in the data (such as the user's ratings) to perform classification
    tasks. In supervised machine learning, we act a bit like a teacher—we provide
    a multitude of examples to our algorithm, which, once it gets enough data (and
    so its training is complete), is able to make generalizations about new items
    and infer their category or class.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: But not all of the data lends itself to these kinds of tasks. Sometimes our
    data isn't labeled in any way. Imagine items as diverse as a website's traffic
    logs or the appointments made by customers at a dental clinic. These are just
    raw observations that aren't categorized in any way and don't contain any meaning.
    In such cases, data analysts employ unsupervised machine learning algorithms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非所有数据都适合这些任务。有时我们的数据根本没有任何标签。想象一下像网站流量日志或牙科诊所客户预约这样多样化的项目。这些只是未经分类的原始观察结果，没有任何含义。在这种情况下，数据分析师会使用无监督机器学习算法。
- en: Unsupervised machine learning is used to discover hidden structures and patterns
    in otherwise unlabeled data. It is a very powerful machine learning task, successfully
    employed in a variety of fields, such as marketing (to identify groups of customers
    who share similar purchase preferences), medicine (used to spot tumours), IT security
    (by flagging abnormal user behaviour or web traffic), tax collection (alerting
    of possible tax evasion), and many, many more.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习用于在未标记的数据中发现隐藏的结构和模式。这是一个非常强大的机器学习任务，在各个领域都得到了成功应用，例如市场营销（用于识别具有相似购买偏好的客户群体）、医学（用于发现肿瘤）、IT安全（通过标记异常用户行为或网络流量）、税收征收（警告可能的逃税行为），以及许多其他领域。
- en: Any supervised machine learning task can be treated as unsupervised if we simply
    ignore the features that provide data classification. For example, we could use
    the famous Iris flower dataset to perform unsupervised learning if we didn't want
    to take into account the Species column. This would leave us with unlabeled sepal
    and petal lengths and widths, which could form interesting clusters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们简单地忽略提供数据分类的特征，任何监督机器学习任务都可以被视为无监督的。例如，如果我们不想考虑物种列，我们可以使用著名的鸢尾花数据集进行无监督学习。这将使我们只剩下未标记的花瓣长度和宽度，它们可以形成有趣的簇。
- en: 'As we''ve seen in [Chapter 1](90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml), *Getting
    Started with Julia Programming, ***setosa** can be reliably identified, as it
    consistently has lower petal length and width. But **versicolor** and **virginica**?
    Not so much. You can view this in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第1章](90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml)，“Julia编程入门”，中看到的，**setosa**可以可靠地识别，因为它花瓣长度和宽度始终较低。但是**versicolor**和**virginica**呢？就不那么明显了。你可以在下面的图中查看：
- en: '![](img/c427b97a-884f-4983-8c4a-267791e9bd88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c427b97a-884f-4983-8c4a-267791e9bd88.png)'
- en: The diagram shows how **setosa** forms distinct clusters in almost all of the
    plots—but **versicolor** and **virginica** don't. This is unsupervised learning.
    Easy, right?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了**setosa**在几乎所有图表中形成独特的簇——但是**versicolor**和**virginica**没有。这是无监督学习。简单，对吧？
- en: 'Not quite—it gets trickier than that. In our Iris flowers example, we cheat
    a bit as we color code the plots by species. So, the data is not really unlabeled.
    In a real unsupervised learning scenario, the plots would look like this, with
    all of the species information removed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不完全是——这比那更复杂。在我们的鸢尾花例子中，我们在用颜色编码图表时有点作弊，因为我们按物种给图表着色。所以，数据并不是真正未标记的。在一个真正的无监督学习场景中，图表将看起来像这样，所有物种信息都被移除：
- en: '![](img/6ddba8d4-463a-497a-ae55-93090c61419d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6ddba8d4-463a-497a-ae55-93090c61419d.png)'
- en: Even without the colors and the labels, since the distribution of the points
    is the same, we can still easily identify the **setosa** cluster. Except that,
    obviously, without the species labels we'd have absolutely no idea what it represents.
    And this is a very important point—*the algorithm cannot label the clusters by
    itself*. It can identify a degree of similarity between the various data points,
    but it can't tell what that *means* (it won't know it's **setosa**). A corollary
    of this is that there isn't a correct way of defining the clusters. They're the
    result of exploratory data mining techniques—and just like the exploration of
    an unknown territory, taking slightly different paths (looking at data from a
    different perspective) will lead to different results. To paraphrase the famous
    saying, the clusters are in the eye of the beholder.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有颜色和标签，由于点的分布相同，我们仍然可以轻松地识别出**setosa**簇。但是，显然，如果没有物种标签，我们根本不知道它代表什么。这是一个非常重要的观点——*算法不能自己标记簇*。它可以在各种数据点之间识别出相似度，但它无法解释那*意味着什么*（它不会知道它是**setosa**）。这个观点的一个推论是，没有定义簇的正确方法。它们是探索性数据挖掘技术的结果——就像探索未知领土一样，走不同的路径（从不同的角度观察数据）会导致不同的结果。用一句名言来概括，簇在观察者的眼中。
- en: 'The most common tasks of unsupervised machine learning are defined as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习最常见的任务定义如下：
- en: '**Clustering (or cluster analysis)**: Clustering is used to identify and group
    objects that are more similar to each other when compared to items in other potential
    groups or clusters. The comparison is done by using some metric present in the
    features of the data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类（或聚类分析）**：聚类用于识别和分组那些与其他潜在分组或聚类中的项目相比更相似的对象。这种比较是通过使用数据特征中的一些度量来完成的。'
- en: '**Anomaly detection**: It is used to flag entities that do not fall within
    an expected pattern, as defined by the other items in the dataset. They are important
    as, in general, anomalies represent some kind of a problem, such as bank or tax
    fraud, a software bug, or a medical condition.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：它用于标记那些不符合由数据集中其他项目定义的预期模式的项目。它们很重要，因为通常，异常代表某种问题，如银行或税务欺诈、软件错误或医疗状况。'
- en: In the remainder of this chapter, we'll focus exclusively on clustering—a very
    useful and valuable unsupervised machine learning task. We'll take a closer look
    at the theory behind clustering and then we'll implement an unsupervised machine
    learning project using the San Francisco business data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将专注于聚类——这是一个非常有用且宝贵的无监督机器学习任务。我们将更深入地研究聚类背后的理论，然后我们将使用旧金山商业数据实现一个无监督机器学习项目。
- en: Clustering
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: As you've probably come to realize by now, when it comes to data science, there
    are almost always multiple avenues to attack a problem. At the algorithmic level,
    depending on the particularities of the data and the specific problem we're trying
    to solve, we'll usually have more than one option. A wealth of choices is usually
    good news as some algorithms can produce better results than others, depending
    on the specifics. Clustering is no exception—a few well-known algorithms are available,
    but we must understand their strengths and their limitations in order to avoid
    ending up with irrelevant clusters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如您现在可能已经意识到的，在数据科学领域，几乎总是有多个途径来解决问题。在算法层面，根据数据的特定性和我们试图解决的特定问题，我们通常会有不止一个选择。丰富的选择通常是好消息，因为一些算法可能比其他算法产生更好的结果，具体取决于具体情况。聚类也不例外——有一些众所周知的算法可用，但我们必须了解它们的优点和局限性，以避免得到无关的聚类。
- en: 'Scikit-learn, the famous Python machine learning library, drives the point
    home by using a few toy datasets. The datasets produce easily recognizable plots,
    making it easy for a human to identify the clusters. However, applying unsupervised
    learning algorithms will lead to strikingly different results—some of them in
    clear contradiction of what our human pattern recognition abilities would tell
    us:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以Scikit-learn这个著名的Python机器学习库为例，通过使用一些玩具数据集来说明这一点。这些数据集产生易于识别的图表，使得人类能够轻松识别聚类。然而，应用无监督学习算法将导致截然不同的结果——其中一些与我们的模式识别能力所告诉我们的明显相矛盾：
- en: '![](img/5bf78baa-39c0-473a-b027-2d00f62f0e18.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bf78baa-39c0-473a-b027-2d00f62f0e18.png)'
- en: 'The preceding four plots illustrate the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的四个图示说明了以下内容：
- en: Two concentrical circular clusters
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个同心圆状聚类
- en: Two curves
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个曲线
- en: Three blobs
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三个团块
- en: A square made of uniformly distributed values resulting in a single cluster
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由均匀分布的值构成的单个聚类的正方形
- en: 'Color coding the clusters would result in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对聚类进行颜色编码将产生以下图表：
- en: '![](img/7124b5d9-59bd-4e93-b76b-11f90c8b79c3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7124b5d9-59bd-4e93-b76b-11f90c8b79c3.png)'
- en: Using our innate pattern recognition abilities, we can easily see the well-defined
    clusters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们天生的模式识别能力，我们可以轻松地看到定义良好的聚类。
- en: 'If the clusters are obvious enough for you, you might be surprised to discover
    that, when it comes to machine learning, things are not that clear-cut. Here is
    how some of the most common algorithms interpret the data (the following diagram
    and all of the details of the tests are available on the Scikit-learn website
    at [http://scikit-learn.org](http://scikit-learn.org)):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于您来说聚类非常明显，您可能会惊讶地发现，在机器学习领域，事情并不那么清晰。以下是一些最常见算法如何解释数据（以下图表和测试的所有细节都可以在Scikit-learn网站上找到：[http://scikit-learn.org](http://scikit-learn.org))：
- en: '![](img/5a6a4e9f-6779-4967-80c9-abfefe822dcd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a6a4e9f-6779-4967-80c9-abfefe822dcd.png)'
- en: The diagram shows the color-coded clusters together with the computing times
    for eight well-known algorithms—MiniBatchKMeans, Affinity Propagation, Mean Shift,
    Spectral Clustering, Ward, Agglomerative Clustering, DBSCAN, and Birch.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了颜色编码的聚类以及八个知名算法的计算时间——MiniBatchKMeans、Affinity Propagation、Mean Shift、Spectral
    Clustering、Ward、Agglomerative Clustering、DBSCAN和Birch。
- en: Data analysis of the San Francisco business
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 旧金山商业数据分析
- en: As our learning project for this chapter, let's imagine that we have been hired
    by a new client, the famous ACME Recruiting. They are a major HR company and want
    to open a new office in San Francisco. They are working on a very ambitious marketing
    strategy to accompany their launch. ACME wants to run outdoor campaigns via billboards;
    employ transit advertising with posters on buses, taxis, and bikes; and use direct
    mail by sending leaflets via snail mail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的学习项目，让我们假设我们被一家新客户，著名的ACME招聘公司雇佣。他们是一家主要的人力资源公司，并希望在旧金山开设一家新办公室。他们正在制定一项非常雄心勃勃的营销策略来配合他们的开业。ACME希望通过广告牌进行户外宣传活动；在公交车、出租车和自行车上张贴海报进行交通广告；并通过邮寄传单进行直接邮件营销。
- en: 'They''re targeting business clients and came to us to help them with two things:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们针对的是商业客户，并来找我们帮助他们做两件事：
- en: They want to know the best areas to run their campaign where to place the billboards,
    on what bus lines to place the ads and to what mail addresses to send the leaflets.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们想知道最佳的竞选区域，在哪里放置广告牌，在哪些公交线路上投放广告，以及向哪些邮寄地址发送传单。
- en: They would like to understand the market's recruiting needs so that they can
    get in touch with professionals with the required qualifications.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们希望了解市场的招聘需求，以便他们可以联系到具有所需资格的专业人士。
- en: Our plan is to use a database with information about the companies registered
    in San Francisco and employ unsupervised learning, in the form of clustering,
    to detect the areas with the highest density of companies. That's where ACME should
    spend their advertising dollars. Once we identify the companies they'll target,
    we'll be able to see what domain of activity they have. Our client will use this
    information to assess their recruiting needs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计划是使用有关旧金山注册公司的数据库，并使用无监督学习，即聚类，来检测公司密度最高的区域。这就是ACME应该花费他们的广告费用的地方。一旦我们确定了他们将要针对的公司，我们就能看到他们从事的活动领域。我们的客户将使用这些信息来评估他们的招聘需求。
- en: Data-wise, we're off to a good start, as the city of San Francisco makes a lot
    of interesting data openly available at [https://data.sfgov.org](https://data.sfgov.org).
    Browsing through the website, we can find a database of registered tax-paying
    businesses. It provides a wealth of information including the name, address, opening
    and closing dates (if the business is closed), location geo-coordinates (and sometimes
    the name of the neighborhood), and more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据方面，我们有一个良好的开端，因为旧金山市公开提供了大量有趣的数据，可以在[https://data.sfgov.org](https://data.sfgov.org)网站上找到。浏览该网站，我们可以找到一个注册纳税企业的数据库。它提供了丰富的信息，包括名称、地址、开业和停业日期（如果企业已关闭）、地理位置坐标（有时还有街区名称）等。
- en: 'You can download the file at [https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u](https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u)
    by clicking the Export button in the toolbar, or use the direct download URL:
    [https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD](https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击工具栏中的导出按钮，从[https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u](https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u)下载文件，或者使用直接下载URL：[https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD](https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD)。
- en: However, I strongly suggest using the file provided in this chapter's support
    files, just to make sure that we use exactly the same data and get the same results
    if you follow through. Please download it from [https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我强烈建议使用本章支持文件中提供的文件，以确保我们使用确切相同的数据，并在你继续操作时得到相同的结果。请从[https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip)下载。
- en: For each entry, we also get the **North American Industry Classification System** (**NAICS**)
    Code, which is (the standard used by Federal statistical agencies in classifying
    business establishments for the purpose of collecting, analyzing, and publishing
    statistical data related to the U.S. business economy). This is important as we'll
    use it to identify the top most common types of businesses, which will help our
    client attract relevant candidates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个条目，我们还得到了 **北美行业分类系统**（**NAICS**）代码，该代码是（联邦统计机构用于分类商业机构以收集、分析和发布与美国商业经济相关的统计数据的标准）。这很重要，因为我们将使用它来识别最常见的商业类型，这将帮助我们的客户吸引相关候选人。
- en: In our dataset, the NAICS code is indicated as a range, for example, 4400–4599\.
    Fortunately, we also get the name of the corresponding sector of activity. In
    this example, 4400–4599 stands for *retail trade*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，NAICS代码表示为一个范围，例如，4400–4599。幸运的是，我们还得到了相应活动部门的名称。在这个例子中，4400–4599 代表
    *零售业*。
- en: 'It''s time to load the data and slice and dice! By now, I''m sure you know
    the drill:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候加载数据并进行切片和切块了！到现在为止，我相信你已经知道了该怎么做：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using `describe(df)` gives us a treasure trove of information about each column.
    I''m including just `nunique` and `nmissing` in the next screenshot, for the sake
    of brevity, but feel free to check the data in more detail as an exercise:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe(df)` 可以给我们提供关于每一列的宝贵信息。为了简洁起见，我在下一张截图里只包括了 `nunique` 和 `nmissing`，但你可以自由地作为练习更详细地检查数据：
- en: '![](img/5377007e-7718-48d2-8736-afc2d4947f2c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5377007e-7718-48d2-8736-afc2d4947f2c.png)'
- en: 'Check the number and percentage of missing values (under the `nmissing` column)
    and the number of unique values (as `nunique`). We can see that, for the `Location
    Id` column, we get `222871` unique values and zero missing entries. The number
    of unique location IDs is equal to the number of rows in the dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 检查缺失值的数量和百分比（在 `nmissing` 列下）以及唯一值的数量（作为 `nunique`）。我们可以看到，对于 `Location Id`
    列，我们得到了 `222871` 个唯一值和零缺失条目。唯一的位置ID数量等于数据集的行数：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Moving on, `Ownership Name` stands for the entity that registered the business
    (either a person or another company) while `DBA Name` represents the name of the
    business itself. For both of these, we can see that `Number Unique` is smaller
    than `Location Id`, meaning that some companies are owned by the same entities—and
    that some companies will have the same name. Looking further at `Street Address`,
    it turns out that a large number of companies share the location with other businesses
    (156,658 unique addresses for `222871` companies). Finally, we can see that we
    have `City`, `State`, and `Zipcode` information for almost all of our records.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`Ownership Name` 代表注册商业的实体（无论是个人还是另一家公司），而 `DBA Name` 则代表商业本身的名称。对于这两者，我们可以看到
    `Number Unique` 小于 `Location Id`，这意味着一些公司由相同的实体拥有——并且一些公司会有相同的名称。进一步查看 `Street
    Address`，我们发现大量公司与其他商业共享位置（对于 `222871` 家公司有 156,658 个唯一的地址）。最后，我们可以看到我们几乎所有的记录都有
    `City`、`State` 和 `Zipcode` 信息。
- en: The dataset also provides information about the date when a business was registered
    (`Business Start Date`), closed (`Business End Date`), and when it started and
    finished operating at that location (`Location Start Date` and `Location End Date`
    respectively). There are more details, but they are mostly irrelevant for our
    analysis, such as `Parking Tax`, `Transient Occupancy Tax`, and `LIC Code` data
    (missing for over 95% of the records) and the `Supervisor District`, `Neighborhoods
    Analysis Boundaries`, and `Business Corridor` information (the business corridor
    data is missing in 99.87% of cases though).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还提供了关于商业注册日期（`Business Start Date`）、关闭日期（`Business End Date`）以及在该位置开始和结束运营的日期（分别对应
    `Location Start Date` 和 `Location End Date`）的信息。还有更多细节，但它们对于我们分析大多不相关，例如 `Parking
    Tax`、`Transient Occupancy Tax` 和 `LIC Code` 数据（超过95%的记录缺失）以及 `Supervisor District`、`Neighborhoods
    Analysis Boundaries` 和 `Business Corridor` 信息（尽管商业走廊数据在99.87%的情况下缺失）。
- en: It's time to clean up our data and get it ready for analysis!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候清理我们的数据，为分析做好准备！
- en: Data wrangling with query
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用查询进行数据处理
- en: 'So far, we''ve seen how to manipulate `DataFrame` instances using the `DataFrames`
    API. I''m sure that you''re by now aware that we can remove uninteresting columns
    by using `delete!(df::DataFrame, column_name::Symbol)`, for instance. You may
    remember from the previous chapter that you can filter a `DataFrame` instance
    using the square brackets notation in combination with the *dot *element-wise
    operations, such as  in the following example:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用`DataFrames` API来操作`DataFrame`实例。我相信你现在已经知道，我们可以通过使用`delete!(df::DataFrame,
    column_name::Symbol)`等方法来删除不感兴趣的列。你可能还记得，在前一章中，你可以通过结合方括号表示法和点元素操作符来过滤`DataFrame`实例，例如以下示例：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/63b2cb5f-2e06-496a-b829-998e916afde3.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/63b2cb5f-2e06-496a-b829-998e916afde3.png)'
- en: Now, if you're thinking that Julia spoils us with beautiful syntax and great
    readability and that the preceding has neither—well, you'd be right! The previous
    syntax, although usable, can definitely be improved. And I bet you won't be too
    surprised to hear that Julia's package ecosystem already provides better ways
    of wrangling `DataFrames`. Enter `Query`!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你认为Julia以其美丽的语法和出色的可读性而宠爱我们，而前面的代码既不美丽也不易读——那么，你是对的！虽然前面的语法是可用的，但肯定可以改进。而且，我相信你不会对听到Julia的包生态系统已经提供了更好的方法来处理`DataFrames`而感到惊讶。现在，让我们来看看`Query`！
- en: Query is a package for querying Julia data. It works with a multitude of data
    sources, including Array, DataFrame, CSV, SQLite, ODBC, and others. It provides
    filter, project, join, and group functionality, and it's heavily inspired by Microsoft's
    **Language Integrated Query** (**LINQ**). If this doesn't mean a lot to you, don't
    worry; you'll see it in action right away.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Query是一个用于查询Julia数据的包。它与多种数据源一起工作，包括Array、DataFrame、CSV、SQLite、ODBC等。它提供了过滤、投影、连接和分组功能，并且深受微软的**语言集成查询**（**LINQ**）的启发。如果你对此不太了解，不要担心；你很快就会看到它的实际应用。
- en: 'Here is how the preceding operation would be refactored to use query in order
    to filter out the businesses that pay parking tax:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何重构前面的操作，使用查询来过滤出支付停车税的企业：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you're familiar with SQL, you can easily recognize the familiar language
    query constructs, `from`, `where`, and `select`. That's powerful!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉SQL，你很容易就能识别出熟悉的查询语言结构，如`from`、`where`和`select`。这非常强大！
- en: 'However, having to use this verbose syntax to convert column names such as `Parking
    Tax` into symbols in order to access our data is inconvenient. Before we begin,
    we''d be better off renaming the columns to be more symbol-friendly and replacing
    the spaces with underscores. We''ll use the `DataFrames.rename!` function in combination
    with a comprehension:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了将诸如`Parking Tax`之类的列名转换为符号以便访问我们的数据，不得不使用这种冗长的语法，这确实不方便。在我们开始之前，最好将列名重命名为更符合符号的格式，并将空格替换为下划线。我们将使用`DataFrames.rename!`函数结合列表推导来完成这项工作：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `rename!` function accepts a `DataFrame` and an `Array{Pair}` in the form
    `:current_column_name => :new_current_name`. We use the comprehension to build
    the array, and we do this by iterating over each current column name (returned
    by `names(df)`), converting the resulting symbol into a string, replacing `" "`
    with `"_"` and then converting the string back to a symbol.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`rename!`函数接受一个`DataFrame`和一个形式为`:current_column_name => :new_current_name`的`Array{Pair}`。我们使用列表推导来构建数组，通过遍历每个当前列名（由`names(df)`返回），将结果符号转换为字符串，将`"
    "`替换为`"_"`，然后将字符串转换回符号。'
- en: 'Now we can use the more succinct dot notation with query, so the preceding
    snippet will look like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用更简洁的点符号表示法与查询结合，所以前面的代码片段将看起来像这样：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Metaprogramming in Julia
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Julia中的元编程
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you haven't heard about it before, it basically means that a program has
    the ability to read, analyse, and transform, and even modify itself while running.
    Some languages, called **homoiconic**, come with very powerful metaprogramming
    facilities. In homoiconic languages, the program itself is internally represented
    as a data structure that's available to the program and can be manipulated. Lisp
    is the prototypical homoiconic programming language, and for this reason, this
    kind of metaprogramming is done through Lisp-style macros. They work with the
    code's representation and are different from preprocessor macros of C and C++
    fame, where the text files containing the code are manipulated before parsing
    and evaluation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有听说过它，它基本上意味着一个程序具有在运行时读取、分析、转换，甚至修改自己的能力。一些被称为 **同构** 的语言，提供了非常强大的元编程功能。在同构语言中，程序本身在内部被表示为程序可以访问并操作的数据结构。Lisp
    是典型的同构编程语言，因此这种元编程是通过 Lisp 风格的宏来实现的。它们与 C 和 C++ 中的预处理宏不同，预处理宏在解析和评估之前会操作包含代码的文本文件。
- en: Julia, inspired to a certain degree by Lisp, is also a homoiconic language.
    Hence, for metaprogramming in Julia, we need to understand two key aspects—the
    representation of the code by means of expressions and symbols and the manipulation
    of the code using macros. If we see the execution of a Julia program as a sequence
    of steps, metaprogramming kicks in and modifies the code after the parsing step,
    but before the code is evaluated by the compiler.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 语言在一定程度上受到 Lisp 语言的影响，也是一种同构语言。因此，在 Julia 中的元编程，我们需要了解两个关键方面——通过表达式和符号表示代码，以及使用宏来操作代码。如果我们把
    Julia 程序的执行看作是一系列步骤，元编程就会在解析步骤之后、代码被编译器评估之前介入并修改代码。
- en: Learning about symbols and expressions in metaprogramming
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习元编程中的符号和表达式
- en: Understanding metaprogramming is not easy, so don't panic if it doesn't come
    naturally from the start. One of the reasons for this, I think, is that it takes
    place at a level of abstraction higher than what we're used to with regular programming.
    I'm hoping that opening the discussion with symbols will make the introduction
    less abstract. We've used symbols extensively throughout this book, especially
    as arguments for the various functions. They look like this—`:x` or `:scientific`
    or `:Celsius`. As you may have noticed, a symbol represents an identifier and
    we use it very much like a constant. However, it's more than that. It represents
    a piece of code that, instead of being evaluated as the variable, is used to refer
    to a variable itself.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 理解元编程并不容易，所以如果你一开始觉得它不太自然，请不要慌张。我认为其中一个原因是它发生在比我们习惯的常规编程更高的抽象层次上。我希望通过从符号开始讨论，可以使介绍更加具体。我们在整本书中广泛使用了符号，特别是作为各种函数的参数。它们看起来是这样的——`:x`
    或 `:scientific` 或 `:Celsius`。正如你可能已经注意到的，符号代表一个标识符，我们非常像使用常量一样使用它。然而，它不仅仅是这样。它代表了一段代码，这段代码不是作为变量来评估，而是用来引用变量本身。
- en: A good analogy for understanding the relationship between a symbol and a variable
    has to do with the words in a phrase. Take for example the sentence: *Richard
    is tall*. Here, we understand that *Richard *is the name of a person, most likely
    a man. And Richard, the person, is tall. However, in the sentence: *Richard has
    seven letters*,it is obvious that now we aren't talking about Richard the person.
    It wouldn't make too much sense to assume that Richard the person has seven letters.
    We are talking about the word *Richard *itself.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解符号和变量之间的关系，一个很好的类比是短语中的单词。以句子为例：*理查德很高*。在这里，我们理解到 *理查德* 是一个人的名字，很可能是男性。理查德这个人很高。然而，在句子：*理查德有七个字母*中，很明显我们现在谈论的不是理查德这个人。假设理查德这个人有七个字母，这并没有太多意义。我们谈论的是单词
    *理查德* 本身。
- en: 'The equivalent, in Julia, of the first sentence (*Richard is tall*) would be
    `julia> x`. Here, `x` is immediately evaluated in order to produce its value.
    If it hasn''t been defined, it will result in an error, shown as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Julia 中，与第一句话（*理查德很高*）等价的表达方式是 `julia> x`。在这里，`x` 被立即评估以产生其值。如果它没有被定义，将会导致错误，如下所示：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Julia''s symbols mimic the second sentence, where we talk about the word itself.
    In English, we wrap the word in single quotes, ''Richard'', to indicate that we''re
    not referring to a person but to the word itself. In the same way, in Julia, we
    prefix the variable name with a column, `:x`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Julia的符号模仿了第二句话，其中我们谈论的是单词本身。在英语中，我们用单引号将单词括起来，写成`'Richard'`，以表明我们不是指人，而是指单词本身。同样，在Julia中，我们在变量名前加一个冒号，`:x`：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Hence, the column `:` prefix is an operator that stops the evaluation. An unevaluated
    expression can be evaluated on demand by using the `eval()` function or the `@eval`
    macro, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，冒号`:`前缀是一个运算符，它停止评估。未评估的表达式可以通过使用`eval()`函数或`@eval`宏按需评估，如下所示：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But we can go beyond symbols. We can write more complex symbol-like statements,
    for example,`:(x = 2)`. This works a lot like a symbol but it is, in fact, an
    `Expr` type, which stands for expression. The expression, like any other type,
    can be referenced through variable names and, like symbols, they can be evaluated:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以超越符号。我们可以编写更复杂的类似符号的语句，例如，`:(x = 2)`。这工作方式与符号非常相似，但实际上它是一个`Expr`类型，代表表达式。表达式，像任何其他类型一样，可以通过变量名引用，并且像符号一样，它们可以被评估：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Even more powerful, since `Expr` is a type, it has properties that expose its
    internal structure:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 更加强大的是，由于`Expr`是一个类型，它具有暴露其内部结构的属性：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Every `Expr` object has two fields—`head` representing its kind and `args`
    standing for the arguments. We can view the internals of `Expr` by using the `dump()`
    function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Expr`对象都有两个字段——`head`代表其类型，`args`代表参数。我们可以通过使用`dump()`函数查看`Expr`的内部结构：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This leads us to even more important discoveries. First, it means that we can
    programmatically manipulate `Expr` through its properties:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了更加重要的发现。首先，这意味着我们可以通过其属性程序化地操作`Expr`：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Our expression is no longer `:(x = 2)`; it's now `:(x = 3)`. By manipulating
    the `args` of the `assign` expression, the value of `x` is now `3`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的表达式不再是`:(x = 2)`；现在它是`:(x = 3)`。通过操作`assign`表达式的`args`，`x`的值现在是`3`。
- en: 'Second, we can programmatically create new instances of `Expr` using the type''s
    constructor:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以使用类型的构造函数程序化地创建`Expr`的新实例：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please notice here that we wrapped the equals sign (`=`) in parenthesis to designate
    an expression, as Julia gets confused otherwise, thinking we want to perform an
    assignment right there.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里我们用括号将等号（`=`）括起来，以指定一个表达式，因为否则Julia会困惑，认为我们想要在那里立即执行赋值操作。
- en: Quoting expressions
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用表达式
- en: 'The previous procedure, in which we wrap an expression within `:(...)` in order
    to create `Expr` objects, is called **quoting**. It can also be done using quote
    blocks. Quote blocks make quoting easier as we can pass *regular-looking *code
    into them (as opposed to translating everything in to symbols), and supports quoting
    multiple lines of code in order to build randomly complex expressions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的过程，即我们在`:(...)`中包装一个表达式以创建`Expr`对象，被称为**引用**。它也可以使用quote块来完成。quote块使引用变得更加容易，因为我们可以将看起来像常规代码的内容传递给它们（而不是将所有内容转换为符号），并且支持引用多行代码以构建随机复杂的表达式：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Interpolating strings
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串插值
- en: 'Just like with string interpolation, we can reference variables within the
    expressions:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就像字符串插值一样，我们可以在表达式中引用变量：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Macros
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 宏
- en: Now, we finally have the knowledge to understand macros. They are language constructs,
    which are executed after the code is parsed, but before it is evaluated. It can
    optionally accept a tuple of arguments and must return an `Expr`. The resulting
    `Expression` is directly compiled, so we don't need to call `eval()` on it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于有了理解宏的知识。它们是语言结构，在代码解析之后、评估之前执行。它可以可选地接受一个参数元组，并且必须返回一个`Expr`。生成的`Expression`将被直接编译，因此我们不需要对它调用`eval()`。
- en: 'For example, we can implement a configurable version of the previous `greet`
    expression as a macro:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将之前的`greet`表达式作为一个宏实现为可配置版本：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Macros are very powerful language constructs that allow parts of the code to
    be customized before the full program is run. The official Julia documentation
    has a great example to illustrate this behavior:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 宏是功能强大的语言结构，允许在运行完整程序之前对代码的部分进行自定义。官方Julia文档有一个很好的例子来说明这种行为：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We define a macro called `twostep`, which has a body that calls the `println` function
    to output text to the console. It returns an expression which, when evaluated,
    will also output a piece of text via the same `println` function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为 `twostep` 的宏，其主体调用 `println` 函数将文本输出到控制台。它返回一个表达式，当评估时，也会通过相同的 `println`
    函数输出一段文本。
- en: 'Now we can see it in action:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到它的实际应用：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `I execute at runtime` message is outputted, but not the `I execute at parse
    time` message. This is a very powerful thing. Imagine that output instead of a
    simple text output if we'd had some very computationally intensive or time-consuming
    operations. In a simple function, we'd have to run this code every time, but with
    a macro, this is done only once, at parse time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出了 “I execute at runtime” 消息，但没有输出 “I execute at parse time” 消息。这是一件非常强大的事情。想象一下，如果我们有一些计算密集型或耗时操作，而不是简单的文本输出。在一个简单的函数中，我们每次都必须运行这段代码，但使用宏，这只需在解析时运行一次。
- en: Closing words about macros
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于宏的结束语
- en: 'Besides they''re very powerful, macros are also very convenient. They can provide
    a lot of functionality with minimal overhead and can simplify the invocation of
    functions that take expressions as arguments. For example, `@time` is a very useful
    macro that executes an `Expression` while measuring the execution time. And the
    great thing is that we can pass the argument expression as *regular* code, instead
    of building the `Expr` by hand:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了它们非常强大之外，宏也非常方便。它们可以用最小的开销提供很多功能，并且可以简化接受表达式作为参数的函数的调用。例如，`@time` 是一个非常有用的宏，它在测量执行时间的同时执行一个
    `Expression`。而且，最棒的是，我们可以将参数表达式作为 *常规* 代码传递，而不是手动构建 `Expr`：
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Macros—and metaprogramming in general—are powerful concepts that require whole
    books to discuss at length. We must stop here in order to get back to our machine
    learning project. ACME Recruiting is eagerly waiting for our findings. I recommend
    going over the official documentation at [https://docs.julialang.org/en/stable/manual/metaprogramming/](https://docs.julialang.org/en/stable/manual/metaprogramming/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 宏——以及一般意义上的元编程——是强大的概念，需要整本书来详细讨论。我们必须在这里停下来，以便回到我们的机器学习项目。ACME 招聘公司热切地等待我们的发现。我建议查看官方文档
    [https://docs.julialang.org/en/stable/manual/metaprogramming/](https://docs.julialang.org/en/stable/manual/metaprogramming/)。
- en: Beginning with Query.jl basics
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Query.jl 基础开始
- en: The `Query` package can be added in the standard way—`pkg> add Query`. Once
    you bring it into scope using `Query`, it makes a rich API available for querying
    Julia data sources, `DataFrames` being the most common source. A query is initiated
    using the `@from` macro.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用标准方式添加 `Query` 包——`pkg> add Query`。一旦使用 `Query` 将其引入作用域，它就提供了一个丰富的 API 用于查询
    Julia 数据源，其中 `DataFrames` 是最常见的数据源。使用 `@from` 宏来启动查询。
- en: '@from'
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '@from'
- en: 'The general structure of a query is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的一般结构如下：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Within the `begin...end` block, `var` represents a row in the `data_source`.
    The query statements are given one per line and can include any combination of
    available query commands, such as `@select`, `@orderby`, `@join`, `@group`, and
    `@collect`. Let's take a closer look at the most important ones.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `begin...end` 块中，`var` 代表 `data_source` 中的行。查询语句每行给出一个，可以包括任何可用的查询命令的组合，例如
    `@select`、`@orderby`、`@join`、`@group` 和 `@collect`。让我们更详细地看看其中最重要的几个。
- en: '@select'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '@select'
- en: 'The `@select` query command, similar to its `SQL SELECT` counterpart, indicates
    which values are to be returned. Its general syntax is `@select condition`, where
    `condition` can be any `Julia` expression. Most commonly, we''ll want to return
    the whole row and, in this case, we''ll just pass `var` itself. For instance,
    let''s create a new `DataFrame` to hold a shopping list:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`@select` 查询命令，类似于其 `SQL SELECT` 对应命令，表示要返回哪些值。其一般语法是 `@select condition`，其中
    `condition` 可以是任何 `Julia` 表达式。最常见的情况是我们希望返回整行，在这种情况下，我们只需传递 `var` 本身。例如，让我们创建一个新的
    `DataFrame` 来保存购物清单：'
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/9aa23832-92c7-4f21-9940-cf16ff6b12ba.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9aa23832-92c7-4f21-9940-cf16ff6b12ba.png)'
- en: A cool (geeky!) and handy shopping list.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个酷（极客！）且实用的购物清单。
- en: 'We can `@select` the whole row with the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下方式 `@select` 整行：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It''s not very useful, as this basically returns the whole `DataFrame`, but
    we can also reference a column using `dot` notation, for example, `p.produce`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是很实用，因为这基本上返回整个 `DataFrame`，但我们也可以使用点符号引用列，例如，`p.produce`：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Given that `@select` accepts any random `Julia` expression, we''re free to
    manipulate the data as we see fit:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `@select` 接受任何随机的 `Julia` 表达式，我们可以自由地按需操作数据：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'However, a better approach is to return `NamedTuple`, using the special query
    curly brackets syntax:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52c58edd-0b81-4afd-90c0-5778a62abd0a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Here, we pass both the keys and the values for `NamedTuple`, but they''re not
    mandatory. They are, however, useful if we want properly named columns (and who
    doesn''t, right?):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/073cc81e-d228-4770-9373-e322e9a9153c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Without the explicit labels, `query` will assign column names such as `__1__`,
    `__2__`, and so on. It's not very readable!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '@collect'
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might''ve noticed in the previous screenshots that the type of the returned
    value was `query result`. A query will return an iterator that can be further
    used to loop over the individual elements of the result set. But we can use the
    `@collect` statement to materialize the result into a specific data structure,
    most commonly `Array` or `DataFrame`. This is shown as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We get the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e2749c8-5626-4d7d-b967-496d2a38f8c8.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'By default, `@collect` will produce an `Array` of `NamedTuple` elements. But
    we can pass it an extra argument for the data type we desire:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output looks like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7252e5e9-f2be-40bd-9438-bf942ad5bca9.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Our result is now a `DataFrame`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '@where'
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most useful commands is `@where`, which allows us to filter a data
    source so that only the elements that satisfy the condition are returned. Similar
    to `@select`, the condition can be any arbitrary `Julia` expression:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We get the following output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4393176-520b-46ea-91e0-e895d15310f9.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Only bread has a `qty` smaller than `2`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Filtering can be made even more powerful by means of range variables. These
    act like new variables belonging to the `query` expression and can be introduced
    using the `@let` macro:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/163d72fb-c86d-4a01-96a5-e90290ed56a7.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Here, you can see how, within the `begin...end` block, we defined a local variable
    called `weekly_qty` with a value equal to `7 * p.qty`. We used the `@let` macro
    to introduce new variables. In the next line, we used it to filter out the rows
    that have a `weekly_qty` smaller than `10`. And then finally, we selected it and
    collected it into a `DataFrame`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '@join'
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s make things even more interesting:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee9d6f9d-db29-44dd-8eb5-46c6756025e6.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'We instantiate a new `DataFrame`, called `products_info`, which contains important
    information about items in our shopping list—their prices and whether or not they
    can be considered allergenic. We could use `DataFrames.hcat!` to append some columns
    from `products_info` to `shopping_list`, but again, the syntax is not so nice
    and the approach is not that flexible. We''ve been spoiled by Julia and we like
    it that way! Fortunately, Query provides a `@join` macro:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/ba3ba9b7-2257-4995-84d9-d30da441fb1c.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'The general syntax of a `@join` command is as follow:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Query provides two other variants of `@join`: group join and left outer join.
    If you would like to read about them, please check the official documentation
    at [http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1](http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 查询提供了 `@join` 的两种其他变体：分组连接和左外连接。如果您想了解更多信息，请查看官方文档[http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1](http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1)。
- en: '@group'
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '@group'
- en: 'The `@group` statement groups elements by some attribute:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`@group` 语句按某些属性对元素进行分组：'
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Not bad, but what we'd really like is to summarize the data. Query provides
    this under the name `split-apply-combine` (also known as, `dplyr`). This requires
    an aggregation function that will be used to collapse the dataset based on the
    `Grouping` variable. If that's too abstract, an example will surely clear things
    up.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错，但我们真正想要的是总结数据。Query 在此提供了名为 `split-apply-combine`（也称为 `dplyr`）的功能。这需要一个聚合函数，该函数将根据
    `Grouping` 变量来折叠数据集。如果这太抽象了，一个例子肯定会澄清一切。
- en: 'Say we want to get a count of allergenic items together with a comma-separated
    list of their names, so we know what to stay away from:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要获取过敏物品的数量以及它们名称的逗号分隔列表，这样我们就可以知道要避免什么：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The result will be a two-row `DataFrame`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个两行的 `DataFrame`：
- en: '![](img/2b3e656a-eb92-41c2-8e8b-691f6cabab33.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b3e656a-eb92-41c2-8e8b-691f6cabab33.png)'
- en: '@orderby'
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '@orderby'
- en: Query also provides a sorting macro named `@orderby`. It takes a list of attributes
    upon which to apply the sorting. Similar to SQL, the order is ascending by default,
    but we can change that by using the `descending` function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查询还提供了一个名为 `@orderby` 的排序宏。它接受一个属性列表，用于应用排序。类似于 SQL，默认情况下是升序排序，但我们可以通过使用 `descending`
    函数来更改它。
- en: 'Given our previously defined `products_info` `DataFrame`, we can easily sort
    it as needed, for example, with the most expensive products first and then by
    product name:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们之前定义的 `products_info` `DataFrame`，我们可以轻松地按需对其进行排序，例如，首先按价格最高的产品排序，然后按产品名称排序：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/a3ed77a4-067a-4959-b3b2-8931b7d40832.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a3ed77a4-067a-4959-b3b2-8931b7d40832.png)'
- en: All right, that was quite a detour! But now that we have knowledge of the great
    `Query` package, we're ready to efficiently slice and dice our data. Let's go!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这确实是一个相当大的绕路！但现在我们已经了解了伟大的 `Query` 包，我们准备好高效地切割和剖析我们的数据了。让我们开始吧！
- en: Preparing our data
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备我们的数据
- en: Our data cleaning plan is to only keep the businesses registered in San Francisco,
    CA, for which we have the address, zip code, NAICS code, and business location
    and which have not been closed (so they don't have a business end date) and have
    not moved away (don't have a location end date).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据清理计划是只保留在旧金山、加利福尼亚注册的企业，我们有它们的地址、邮编、NAICS代码和业务位置，并且它们尚未关闭（因此没有业务结束日期）且未搬迁（没有位置结束日期）。
- en: 'Using the `DataFrame` API to apply the filters would be tedious. But with Query,
    it''s a walk in the park:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DataFrame` API 应用过滤器将是繁琐的。但有了 Query，这就像散步一样简单：
- en: '[PRE44]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can see how `@where` filters are applied, requiring that `lowercase(b.City)`
    equals `"san francisco"` and that `b.State` equals `"CA"`. Then, we use `! isna`
    to make sure we only keep the rows where `b.Street_Address`, `b.Source_Zipcode`,
    `b.NAICS_Code`, `b.NAICS_Code_Description`, and `b.Business_Location` are not
    missing. The `isna` function is provided by the `DataValues` package (which is
    used by Query itself) and that's why we're adding and using it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `@where` 过滤器是如何应用的，要求 `lowercase(b.City)` 等于 `"san francisco"`，并且 `b.State`
    等于 `"CA"`。然后，我们使用 `! isna` 确保只保留 `b.Street_Address`、`b.Source_Zipcode`、`b.NAICS_Code`、`b.NAICS_Code_Description`
    和 `b.Business_Location` 不为空的行。`isna` 函数由 `DataValues` 包提供（该包由 Query 本身使用），这就是为什么我们要添加并使用它的原因。
- en: We also make sure that `b.Business_Location` matches a certain format that corresponds
    to geolocation coordinates. Finally, we make sure that, on the contrary, `b.Business_End_Date`
    and `b.Location_End_Date` are in fact missing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还确保 `b.Business_Location` 与对应于地理坐标的特定格式匹配。最后，我们确保 `b.Business_End_Date` 和
    `b.Location_End_Date` 实际上是缺失的。
- en: Executing the query produces a new `DataFrame` with almost 57,000 rows.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 执行查询会产生一个包含近57,000行的新的 `DataFrame`。
- en: 'The next step is to take our `clean_df` data and extract the geo-coordinates
    out of the `Business_Location` column. Again, Query comes to the rescue:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 `clean_df` 数据中提取 `Business_Location` 列中的地理坐标。同样，Query 来帮忙：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We make good use of the range variables feature (defined by `@let`) to introduce
    a `geo` variable, which uses `match` to extract the latitude and longitude pairs
    from the `Business_Location` data. Next, inside the `@select` block, the two values
    in the geo array are converted in to proper float values and added to the resulting
    `DataFrame`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们充分利用了范围变量特性（由 `@let` 定义）来引入一个 `geo` 变量，它使用 `match` 从 `Business_Location` 数据中提取经纬度对。接下来，在
    `@select` 块内部，geo 数组中的两个值被转换为适当的浮点值并添加到结果 `DataFrame` 中：
- en: '![](img/be644856-1437-46d8-a631-490fadafaa8a.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be644856-1437-46d8-a631-490fadafaa8a.png)'
- en: We're done! Our data is now neatly represented in our `clean_df_geo` `DataFrame`,
    containing the name of the business, zip code, NAICS code, NAICS code description,
    latitude, and longitude.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了！我们的数据现在已整洁地表示在 `clean_df_geo` `DataFrame` 中，包含企业的名称、邮政编码、NAICS 代码、NAICS
    代码描述、纬度和经度。
- en: 'If we run `describe(clean_df_geo)`, we''ll see that we have 56,549 businesses
    with 53,285 unique names with only 18 NAICS code descriptions. We don''t know
    how many zip codes the companies are spread across, but it''s easy to find out:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行 `describe(clean_df_geo)`，我们会看到我们有 56,549 家企业，有 53,285 个独特的名称，只有 18 个
    NAICS 代码描述。我们不知道公司分布在多少个邮政编码中，但很容易找到：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Our businesses are registered within `79` zip codes in the city of San Francisco.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的业务在旧金山市的 `79` 个邮政编码内注册。
- en: Unsupervised machine learning with clustering
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于聚类的无监督机器学习
- en: Julia's package ecosystem provides a dedicated library for clustering. Unsurprisingly,
    it's called **Clustering**. We can simply execute `pkg> add Clustering` to install
    it. The `Clustering` package implements a few common clustering algorithms—k-means,
    affinity propagation, DBSCAN, and kmedoids.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 的包生态系统提供了一个专门的库用于聚类。不出所料，它被称为 **聚类**。我们可以简单地执行 `pkg> add Clustering` 来安装它。`Clustering`
    包实现了一些常见的聚类算法——k-means、亲和传播、DBSCAN 和 kmedoids。
- en: The k-means algorithm
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 算法
- en: The k-means algorithm is one of the most popular ones, providing a balanced
    combination of good results and good performance in a wide range of applications.
    However, one complication is that we're required to give it the number of clusters
    beforehand. More exactly, this number, called **k** (hence the first letter of
    the name of the algorithm), represents the number of centroids. A **centroid**
    is a point that is representative of each cluster.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 算法是最受欢迎的算法之一，在广泛的应用中提供了良好的结果和良好的性能的平衡组合。然而，一个复杂的问题是，我们事先必须给它指定簇的数量。更确切地说，这个数字称为
    **k**（因此算法名称的首字母），代表质心的数量。**质心**是代表每个簇的点。
- en: The k-means algorithm applies an iterative approach—it places the centroids
    using the algorithm defined by the seeding procedure, then it assigns each point
    to its corresponding centroid, the mean to which is closest. The algorithm stops
    on convergence, that is, when the point assignment doesn't change with a new iteration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 算法采用迭代方法——它使用由种子过程定义的算法放置质心，然后它将每个点分配到其对应的质心，即最近的均值。算法在收敛时停止，也就是说，当点分配在新一轮迭代中不改变时。
- en: Algorithm seeding
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法种子
- en: There are a few ways to pick the centroids. Clustering provides three, one of
    which is random (labeled as the `:rand` option in clustering), which randomly
    selects a subset of points as seeds (so all centroids are random). This is the
    default seeding strategy in classical k-means. There's also k-means++, a better
    variation proposed in 2007 by David Arthur and Sergei Vassilvitskii (labeled as
    `:kmpp`), which picks one cluster center randomly, but then searches for the other
    centers in relation to the first one. The last available approach is centrality
    seeding (`:kmcen`), which picks the samples with the highest centrality.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以选择质心。聚类提供了三种，其中一种是随机（在聚类中标记为 `:rand` 选项），它随机选择一个点的子集作为种子（因此所有质心都是随机的）。这是经典
    k-means 的默认种子策略。还有 k-means++，这是一种在 2007 年由 David Arthur 和 Sergei Vassilvitskii
    提出的更好变体（标记为 `:kmpp`），它随机选择一个簇中心，然后根据第一个中心搜索其他中心。最后一种可用方法是中心性种子（`:kmcen`），它选择具有最高中心性的样本。
- en: Finding the areas with the most businesses
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找拥有最多企业的区域
- en: 'In the previous section, we successfully cleaned our data, now neatly accessible
    in `clean_df_geo` `DataFrame`. If you run into any problems with the data cleaning
    process, you can just go ahead and load the dataset from scratch by using the
    `clean_df_geo.tsv` file provided in this chapter''s support files ([https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/clean_df_geo.tsv.zip](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/clean_df_geo.tsv.zip)).
    In order to load it, all you have to do is run the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: So we want to identify the areas with the highest density of businesses. One
    approach is to use unsupervised machine learning to identify the areas by zip
    code and the number of businesses registered.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll train our model using the data in the `:zipcode` column plus the number
    of businesses registered in the area. We''ll need the number of businesses per
    zip code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We execute a query against `clean_df_geo` `DataFrame`, grouping it by `:Source_Zipcode`
    into `g`. We store the number of businesses from the current zip code in the `bcount`
    range variable, as returned by `length(g)`, but not before converting the number
    into a `Float64`. The reason we''re doing this is that, as we''ll see right away,
    clustering expects the input to be `Float64`, so this will save us another processing
    step later. Back to our query. We also apply sorting by `bcount` to allow us,
    the humans, to better understand the data (not needed for training the model).
    Finally, we instantiate a new `DataFrame`, with two columns, a zip code, and `businesses_count`,
    without forgetting to convert the zip code into `Float64` too, for the same reason
    as before. When converting `key(g)`, please also note that we''re first calling
    the `get` function. This is because, within a query block, the computed values
    are represented as `DataValues` and to access the wrapped value we need to invoke
    `get`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06081711-3cd3-4ee0-81d6-e1a9240bcef2.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Our training data is made of `79` zip codes and their corresponding businesses
    count. The top 22 areas have over 1,000 businesses each, and the number drops
    sharply for the rest:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output is as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dca65879-db99-4b9e-a076-ee713ba143ec.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'You probably remember `Gadfly`, the Julia plotting library we used in [Chapter
    1](90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml), *Getting Started with Julia Programming*,
    to visualize the Iris flowers dataset. Let''s use it to quickly get a glimpse
    of our data:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will render the following histogram:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da643e25-42cc-4547-8bd3-ab887ba9c316.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'We can easily see that most of the areas only have one registered business,
    followed by a few others, which only have a few. We can safely remove these from
    our training dataset as they won''t be useful to our client. The only thing we
    need to do is to add the `@where bcount > 10` filter in the query for computing
    `model_data`, between the `@let` and the `@orderby` statements:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Once we remove all of the areas that host less than `10` companies, we're left
    with only `28` zip codes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Training our model
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Only one small step and we''re ready to train our model. We need to convert
    the `DataFrame` into an array and to permute the dimensions of the array so that
    the `DataFrame` columns become rows. In the new structure, each column (zip code
    and count pair) is considered a training sample. Let''s do it:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Our training data is ready! It''s time to put it to good use:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We're using the k-means algorithm by invoking the function with the same name.
    As arguments, we provide the `training_data` array and give it three clusters.
    We want to split the areas into three tiers—low, medium, and high density. The
    training shouldn't take more than a few seconds. And since we gave it the `display=:iter`
    argument, we get progressive debug info at each iteration. For the seeding algorithm,
    we have used k-means++ (`:kmpp`).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the results
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can take a look at how the points were assigned:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Each element in the array corresponds to the element at the same index in the
    `model_data`. Let''s combine the data so it''s easier to follow:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now let''s see what we end up with:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56e2d71e-dee6-4766-ba2e-4315548961e3.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the first three zip codes have been assigned to cluster `3`,
    the last eight to cluster `2`, and the rest to cluster `1`. You''ve probably noticed
    that the IDs of the clusters don''t follow the actual count values, which is normal
    since the data is unlabeled. It is us who must interpret the meaning of the clusters.
    And our algorithm has decided that the areas with the highest density of businesses
    will stay in cluster `3`, the low densities in cluster `2`, and the average ones
    in cluster `1`. Plotting the data with `Gadfly` will confirm our findings:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'It produces this plot:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/960b6dca-9559-473d-8984-8607b0b20df6.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Excellent! We can now inform our client that the best areas to target are in
    the zip codes 94110, 94103, and 94109, allowing them to reach 11,965 businesses
    in these dense parts of the city. They would also like to know which are these companies,
    so let''s prepare a list:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We use the zip codes we extracted in the clustering step to filter the `clean_df_geo`
    dataset:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a19e81a0-f09a-42c8-9144-b0afca3d4207.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'We end up with 11,965 companies concentrated in three area codes. Let''s plot
    the points using the `geo` coordinates:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2ab408-24e1-45b5-87aa-59c304bf229d.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the locations are in close proximity, but there is one outlier
    whose coordinates are way off. Maybe there''s an error in our data. Using Query,
    we can easily remove the culprit:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'With our cleaned-up list, we can now explore the domain of activity for these
    companies. This will help our client reach out to candidates that fit the market''s
    demand, as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'That was easy:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2afb42f-a3dc-4711-a19d-7359d3a19d18.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: 'All the companies in the targeted area are active in just `18` domains, out
    of which real estate is the most common one. Surely, our client''s executives
    would appreciate a chart:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This is what we get:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58c3bb0d-073c-449a-a7cd-dd1b376307bb.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Yes, the chart clearly shows that real estate is the activity in which most
    of the businesses are involved, with tech and retail coming in next.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Refining our findings
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Great progress so far, but a list of almost 12,000 companies is still hard
    to handle. We can help our client by breaking it down into clusters of businesses
    located in close proximity. It''s the same workflow as before. First, we extract
    our training data:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5459fe82-6cca-4a38-ac6f-1147121f6bf2.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: 'Now we permute the dimensions to set the data in the format expected by clustering
    (just like we did before):'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Our training array is ready!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the same k-means algorithm with k-means++ seeding.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that k-means is generally not the best choice for clustering
    geolocation data. DBSCAN is usually better suited and I recommend that you look
    into it for production applications. The k-means algorithm will fail, for example,
    when dealing with close points that wrap over 180 degrees. For our example project
    and for the data we're handling, k-means works fine, but keep this limitation
    in mind.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Training works in the same way. We''ll go with `12` clusters, in order to get
    roughly 1,000 companies per group:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This time it takes `24` iterations to reach convergence. Let''s see what we''ve
    got:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Most of the data is evenly spread, but we can spot a few clusters which don''t
    get that many businesses. Plotting the numbers gives us a clear picture:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here is the plot:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28e9133d-5275-495d-84cf-066d876c91c9.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Now we can *paste* the cluster assignments onto the `companies_in_top_areas`
    `DataFrame`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Visualizing our clusters on the map
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get a better understanding of our data, in terms of points density and location
    proximity, we can render a plot with `Gadfly`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a58eb08a-a6dd-4c12-b4c9-d9b153b7a5e2.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: We can see a pretty good cluster distribution, so our approach worked!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: However, it would be even better if we could display the clusters on a map.
    Unfortunately, at the moment, there's no easy way to do this in Julia, so we'll
    use a third-party tool.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: PlotlyJS ([https://github.com/sglyon/PlotlyJS.jl](https://github.com/sglyon/PlotlyJS.jl))
    provides related functionality, but my tests didn't produce good results given
    that the coordinates are tightly packed in the San Francisco area.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Using BatchGeo to quickly build maps of our data
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BatchGeo ([https://batchgeo.com](https://batchgeo.com)) is a popular web app
    for creating map-based data visualizations. It uses high-definition maps from
    Google and it provides a no-login, albeit limited, free version, which we can
    try out right away.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'BatchGeo expects a CSV file with a series of columns, so our first job is to
    set that up. It couldn''t be any simpler with Query:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e29ce70-793a-42eb-bf6e-f1759dae3f86.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: The structured data is available in a new `DataFrame` called `export_data`.
    Unfortunately, BatchGeo has added a 250-row limit for free accounts, so we'll
    have to limit our export to just the top 250 rows.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we can export it:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Success! The only thing left to do is to open [https://batchgeo.com](https://batchgeo.com)
    in your favorite web browser and drag and drop the `business.csv` file to the
    designated place:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by performing the steps, as shown in the following screenshot:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f6f4b281-3bca-470f-a1ee-d7e714f537fd.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Click Validate & Set Options. You''ll see that the columns were picked correctly
    and the defaults are good:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7926f0f1-20b1-4f06-a165-8c751748f4bb.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on Make Map will render our clusters on top of the map of San Francisco:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9794ae2a-642c-4479-a4eb-676c170b1f0e.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: Victory—a beautiful rendering of our data!
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also disable the clustering so that each individual business will be
    plotted:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41672512-690b-4238-85fd-70555ca207e5.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: Finally, we can save our map, follow the instructions, and get a unique URL
    for our visualization. Mine can be found at [https://batchgeo.com/map/7475bf3c362eb66f37ab8ddbbb718b87](https://batchgeo.com/map/7475bf3c362eb66f37ab8ddbbb718b87)
    .
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Excellent, just in time for the meeting with our client!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the optimal number of clusters for k-means (and other algorithms)
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the nature of the data and the problem you'll be looking to solve,
    the number of clusters can come as a business requirement, or it may be an obvious
    choice (as in our case, where we wanted to identify low, middle, and high business
    density zones and so ended up with three clusters). However, in some cases, the
    answer might not be that obvious. In such situations, we'll need to apply a different
    algorithm to evaluate the optimal number of clusters.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common is the Elbow method. It is an iterative approach where
    we run the clustering algorithm with different values for k, for example between
    1 and 10\. The goal is to compare the total intra-cluster variation by plotting
    the sum of squared errors between each point and the mean of its cluster as a
    function of k. Using the visualization, we identify the *elbow-like *point of
    inflection, like this:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80d348c1-5ba4-4e46-b73f-ae1945d4e09f.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: This is the elbow.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about this at [http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)
    (with examples in R).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Clustering validation
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides picking the optimum number of clusters, another aspect is cluster validation,
    that is, to determine how well the items fit the assigned clusters. This can be
    used to confirm that patterns do indeed exist and to compare competing clustering
    algorithms.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Clustering provides a small API for clustering validation with three techniques,
    including Silhouettes, one of the most common. You can find the documentation
    at [http://clusteringjl.readthedocs.io/en/latest/validate.html](http://clusteringjl.readthedocs.io/en/latest/validate.html)
    and you can read more about validation theory at [http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/](http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked into unsupervised machine learning techniques with
    Julia. We focused on clustering, one of the most widely used applications of unsupervised
    learning. Starting with a dataset about businesses registered in San Francisco,
    we performed complex—but not complicated, thanks to Query—data cleansing. In the
    process, we also learned about metaprogramming, a very powerful coding technique
    and one of Julia's most powerful and defining features.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Once our data was in top shape and after mastering the basics of clustering
    theory, using the k-means algorithm, we got down to business. We performed clustering
    to identify the areas with the highest density of companies to help our imaginary
    customer, ACME Recruiting, to target the best areas for advertising. After identifying
    the parts of the city that would give ACME the best reach, we performed data analysis
    to get the top domains of activity required by our customer so they could build
    a database of relevant candidates.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we performed clustering on the geolocation data of the businesses in
    the targeted areas and then we rendered these on top of a map. Our client was
    thrilled with our findings and their marketers now having all the necessary information
    to start planning their campaigns. Congratulations!
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll leave the fascinating world of machine learning in
    order to discover yet another key concept in data science—time series. We'll learn
    how to handle dates and time in Julia, how to work with time series data, and
    how to make forecasts. Exciting, isn't it?
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
