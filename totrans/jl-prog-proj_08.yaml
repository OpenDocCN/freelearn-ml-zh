- en: Leveraging Unsupervised Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our supervised machine learning project was a success and we're well on our
    way to becoming experts in recommender systems. It's now time to leave behind
    the safety of our neatly tagged data and venture into the unknown. Yes, I'm talking
    about unsupervised machine learning. In this chapter, we'll train a model that
    will help us find hidden patterns in a mountain of data. And since we've come
    so far on our journey of learning Julia, it's time to take off the training wheels
    and take on our first client.
  prefs: []
  type: TYPE_NORMAL
- en: Just kidding—for now, we'll play pretend, but we'll indeed tackle a machine
    learning problem that could very well be one of the first tasks of a junior data
    scientist. We'll help our imaginary customer discover key insights for supporting
    their advertising strategy, a very important component of beginning their operations
    in San Francisco.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the process, we''ll learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What unsupervised machine learning is and when and how to use it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of clustering, one of the most important unsupervised learning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform efficient data munging with the help of query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaprogramming in Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and running unsupervised machine learning models with clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Julia package ecosystem is under continuous development and new package
    versions are released on a daily basis. Most of the times this is great news,
    as new releases bring new features and bug fixes. However, since many of the packages
    are still in beta (version 0.x), any new release can introduce breaking changes.
    As a result, the code presented in the book can stop working. In order to ensure
    that your code will produce the same results as described in the book, it is recommended
    to use the same package versions. Here are the external packages used in this
    chapter and their specific versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to install a specific version of a package you need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively you can install all the used packages by downloading the `Project.toml`
    file provided with the chapter and using `pkg>` instantiate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](a3fe07c4-b551-4573-ba72-edba84b1041a.xhtml), *Machine Learning
    For Recommender Systems*, we learned about supervised machine learning. We used
    various features in the data (such as the user's ratings) to perform classification
    tasks. In supervised machine learning, we act a bit like a teacher—we provide
    a multitude of examples to our algorithm, which, once it gets enough data (and
    so its training is complete), is able to make generalizations about new items
    and infer their category or class.
  prefs: []
  type: TYPE_NORMAL
- en: But not all of the data lends itself to these kinds of tasks. Sometimes our
    data isn't labeled in any way. Imagine items as diverse as a website's traffic
    logs or the appointments made by customers at a dental clinic. These are just
    raw observations that aren't categorized in any way and don't contain any meaning.
    In such cases, data analysts employ unsupervised machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning is used to discover hidden structures and patterns
    in otherwise unlabeled data. It is a very powerful machine learning task, successfully
    employed in a variety of fields, such as marketing (to identify groups of customers
    who share similar purchase preferences), medicine (used to spot tumours), IT security
    (by flagging abnormal user behaviour or web traffic), tax collection (alerting
    of possible tax evasion), and many, many more.
  prefs: []
  type: TYPE_NORMAL
- en: Any supervised machine learning task can be treated as unsupervised if we simply
    ignore the features that provide data classification. For example, we could use
    the famous Iris flower dataset to perform unsupervised learning if we didn't want
    to take into account the Species column. This would leave us with unlabeled sepal
    and petal lengths and widths, which could form interesting clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen in [Chapter 1](90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml), *Getting
    Started with Julia Programming, ***setosa** can be reliably identified, as it
    consistently has lower petal length and width. But **versicolor** and **virginica**?
    Not so much. You can view this in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c427b97a-884f-4983-8c4a-267791e9bd88.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram shows how **setosa** forms distinct clusters in almost all of the
    plots—but **versicolor** and **virginica** don't. This is unsupervised learning.
    Easy, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'Not quite—it gets trickier than that. In our Iris flowers example, we cheat
    a bit as we color code the plots by species. So, the data is not really unlabeled.
    In a real unsupervised learning scenario, the plots would look like this, with
    all of the species information removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ddba8d4-463a-497a-ae55-93090c61419d.png)'
  prefs: []
  type: TYPE_IMG
- en: Even without the colors and the labels, since the distribution of the points
    is the same, we can still easily identify the **setosa** cluster. Except that,
    obviously, without the species labels we'd have absolutely no idea what it represents.
    And this is a very important point—*the algorithm cannot label the clusters by
    itself*. It can identify a degree of similarity between the various data points,
    but it can't tell what that *means* (it won't know it's **setosa**). A corollary
    of this is that there isn't a correct way of defining the clusters. They're the
    result of exploratory data mining techniques—and just like the exploration of
    an unknown territory, taking slightly different paths (looking at data from a
    different perspective) will lead to different results. To paraphrase the famous
    saying, the clusters are in the eye of the beholder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common tasks of unsupervised machine learning are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering (or cluster analysis)**: Clustering is used to identify and group
    objects that are more similar to each other when compared to items in other potential
    groups or clusters. The comparison is done by using some metric present in the
    features of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: It is used to flag entities that do not fall within
    an expected pattern, as defined by the other items in the dataset. They are important
    as, in general, anomalies represent some kind of a problem, such as bank or tax
    fraud, a software bug, or a medical condition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we'll focus exclusively on clustering—a very
    useful and valuable unsupervised machine learning task. We'll take a closer look
    at the theory behind clustering and then we'll implement an unsupervised machine
    learning project using the San Francisco business data.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you've probably come to realize by now, when it comes to data science, there
    are almost always multiple avenues to attack a problem. At the algorithmic level,
    depending on the particularities of the data and the specific problem we're trying
    to solve, we'll usually have more than one option. A wealth of choices is usually
    good news as some algorithms can produce better results than others, depending
    on the specifics. Clustering is no exception—a few well-known algorithms are available,
    but we must understand their strengths and their limitations in order to avoid
    ending up with irrelevant clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn, the famous Python machine learning library, drives the point
    home by using a few toy datasets. The datasets produce easily recognizable plots,
    making it easy for a human to identify the clusters. However, applying unsupervised
    learning algorithms will lead to strikingly different results—some of them in
    clear contradiction of what our human pattern recognition abilities would tell
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bf78baa-39c0-473a-b027-2d00f62f0e18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding four plots illustrate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two concentrical circular clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two curves
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Three blobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A square made of uniformly distributed values resulting in a single cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Color coding the clusters would result in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7124b5d9-59bd-4e93-b76b-11f90c8b79c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Using our innate pattern recognition abilities, we can easily see the well-defined
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the clusters are obvious enough for you, you might be surprised to discover
    that, when it comes to machine learning, things are not that clear-cut. Here is
    how some of the most common algorithms interpret the data (the following diagram
    and all of the details of the tests are available on the Scikit-learn website
    at [http://scikit-learn.org](http://scikit-learn.org)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a6a4e9f-6779-4967-80c9-abfefe822dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram shows the color-coded clusters together with the computing times
    for eight well-known algorithms—MiniBatchKMeans, Affinity Propagation, Mean Shift,
    Spectral Clustering, Ward, Agglomerative Clustering, DBSCAN, and Birch.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis of the San Francisco business
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our learning project for this chapter, let's imagine that we have been hired
    by a new client, the famous ACME Recruiting. They are a major HR company and want
    to open a new office in San Francisco. They are working on a very ambitious marketing
    strategy to accompany their launch. ACME wants to run outdoor campaigns via billboards;
    employ transit advertising with posters on buses, taxis, and bikes; and use direct
    mail by sending leaflets via snail mail.
  prefs: []
  type: TYPE_NORMAL
- en: 'They''re targeting business clients and came to us to help them with two things:'
  prefs: []
  type: TYPE_NORMAL
- en: They want to know the best areas to run their campaign where to place the billboards,
    on what bus lines to place the ads and to what mail addresses to send the leaflets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They would like to understand the market's recruiting needs so that they can
    get in touch with professionals with the required qualifications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our plan is to use a database with information about the companies registered
    in San Francisco and employ unsupervised learning, in the form of clustering,
    to detect the areas with the highest density of companies. That's where ACME should
    spend their advertising dollars. Once we identify the companies they'll target,
    we'll be able to see what domain of activity they have. Our client will use this
    information to assess their recruiting needs.
  prefs: []
  type: TYPE_NORMAL
- en: Data-wise, we're off to a good start, as the city of San Francisco makes a lot
    of interesting data openly available at [https://data.sfgov.org](https://data.sfgov.org).
    Browsing through the website, we can find a database of registered tax-paying
    businesses. It provides a wealth of information including the name, address, opening
    and closing dates (if the business is closed), location geo-coordinates (and sometimes
    the name of the neighborhood), and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the file at [https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u](https://data.sfgov.org/Economy-and-Community/Map-of-Registered-Business-Locations/ednt-jx6u)
    by clicking the Export button in the toolbar, or use the direct download URL:
    [https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD](https://data.sfgov.org/api/views/ednt-jx6u/rows.csv?accessType=DOWNLOAD).'
  prefs: []
  type: TYPE_NORMAL
- en: However, I strongly suggest using the file provided in this chapter's support
    files, just to make sure that we use exactly the same data and get the same results
    if you follow through. Please download it from [https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/Map_of_Registered_Business_Locations.csv.zip).
  prefs: []
  type: TYPE_NORMAL
- en: For each entry, we also get the **North American Industry Classification System** (**NAICS**)
    Code, which is (the standard used by Federal statistical agencies in classifying
    business establishments for the purpose of collecting, analyzing, and publishing
    statistical data related to the U.S. business economy). This is important as we'll
    use it to identify the top most common types of businesses, which will help our
    client attract relevant candidates.
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, the NAICS code is indicated as a range, for example, 4400–4599\.
    Fortunately, we also get the name of the corresponding sector of activity. In
    this example, 4400–4599 stands for *retail trade*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time to load the data and slice and dice! By now, I''m sure you know
    the drill:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `describe(df)` gives us a treasure trove of information about each column.
    I''m including just `nunique` and `nmissing` in the next screenshot, for the sake
    of brevity, but feel free to check the data in more detail as an exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5377007e-7718-48d2-8736-afc2d4947f2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the number and percentage of missing values (under the `nmissing` column)
    and the number of unique values (as `nunique`). We can see that, for the `Location
    Id` column, we get `222871` unique values and zero missing entries. The number
    of unique location IDs is equal to the number of rows in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Moving on, `Ownership Name` stands for the entity that registered the business
    (either a person or another company) while `DBA Name` represents the name of the
    business itself. For both of these, we can see that `Number Unique` is smaller
    than `Location Id`, meaning that some companies are owned by the same entities—and
    that some companies will have the same name. Looking further at `Street Address`,
    it turns out that a large number of companies share the location with other businesses
    (156,658 unique addresses for `222871` companies). Finally, we can see that we
    have `City`, `State`, and `Zipcode` information for almost all of our records.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset also provides information about the date when a business was registered
    (`Business Start Date`), closed (`Business End Date`), and when it started and
    finished operating at that location (`Location Start Date` and `Location End Date`
    respectively). There are more details, but they are mostly irrelevant for our
    analysis, such as `Parking Tax`, `Transient Occupancy Tax`, and `LIC Code` data
    (missing for over 95% of the records) and the `Supervisor District`, `Neighborhoods
    Analysis Boundaries`, and `Business Corridor` information (the business corridor
    data is missing in 99.87% of cases though).
  prefs: []
  type: TYPE_NORMAL
- en: It's time to clean up our data and get it ready for analysis!
  prefs: []
  type: TYPE_NORMAL
- en: Data wrangling with query
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve seen how to manipulate `DataFrame` instances using the `DataFrames`
    API. I''m sure that you''re by now aware that we can remove uninteresting columns
    by using `delete!(df::DataFrame, column_name::Symbol)`, for instance. You may
    remember from the previous chapter that you can filter a `DataFrame` instance
    using the square brackets notation in combination with the *dot *element-wise
    operations, such as  in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/63b2cb5f-2e06-496a-b829-998e916afde3.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if you're thinking that Julia spoils us with beautiful syntax and great
    readability and that the preceding has neither—well, you'd be right! The previous
    syntax, although usable, can definitely be improved. And I bet you won't be too
    surprised to hear that Julia's package ecosystem already provides better ways
    of wrangling `DataFrames`. Enter `Query`!
  prefs: []
  type: TYPE_NORMAL
- en: Query is a package for querying Julia data. It works with a multitude of data
    sources, including Array, DataFrame, CSV, SQLite, ODBC, and others. It provides
    filter, project, join, and group functionality, and it's heavily inspired by Microsoft's
    **Language Integrated Query** (**LINQ**). If this doesn't mean a lot to you, don't
    worry; you'll see it in action right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how the preceding operation would be refactored to use query in order
    to filter out the businesses that pay parking tax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you're familiar with SQL, you can easily recognize the familiar language
    query constructs, `from`, `where`, and `select`. That's powerful!
  prefs: []
  type: TYPE_NORMAL
- en: 'However, having to use this verbose syntax to convert column names such as `Parking
    Tax` into symbols in order to access our data is inconvenient. Before we begin,
    we''d be better off renaming the columns to be more symbol-friendly and replacing
    the spaces with underscores. We''ll use the `DataFrames.rename!` function in combination
    with a comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `rename!` function accepts a `DataFrame` and an `Array{Pair}` in the form
    `:current_column_name => :new_current_name`. We use the comprehension to build
    the array, and we do this by iterating over each current column name (returned
    by `names(df)`), converting the resulting symbol into a string, replacing `" "`
    with `"_"` and then converting the string back to a symbol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the more succinct dot notation with query, so the preceding
    snippet will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Metaprogramming in Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you haven't heard about it before, it basically means that a program has
    the ability to read, analyse, and transform, and even modify itself while running.
    Some languages, called **homoiconic**, come with very powerful metaprogramming
    facilities. In homoiconic languages, the program itself is internally represented
    as a data structure that's available to the program and can be manipulated. Lisp
    is the prototypical homoiconic programming language, and for this reason, this
    kind of metaprogramming is done through Lisp-style macros. They work with the
    code's representation and are different from preprocessor macros of C and C++
    fame, where the text files containing the code are manipulated before parsing
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Julia, inspired to a certain degree by Lisp, is also a homoiconic language.
    Hence, for metaprogramming in Julia, we need to understand two key aspects—the
    representation of the code by means of expressions and symbols and the manipulation
    of the code using macros. If we see the execution of a Julia program as a sequence
    of steps, metaprogramming kicks in and modifies the code after the parsing step,
    but before the code is evaluated by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about symbols and expressions in metaprogramming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding metaprogramming is not easy, so don't panic if it doesn't come
    naturally from the start. One of the reasons for this, I think, is that it takes
    place at a level of abstraction higher than what we're used to with regular programming.
    I'm hoping that opening the discussion with symbols will make the introduction
    less abstract. We've used symbols extensively throughout this book, especially
    as arguments for the various functions. They look like this—`:x` or `:scientific`
    or `:Celsius`. As you may have noticed, a symbol represents an identifier and
    we use it very much like a constant. However, it's more than that. It represents
    a piece of code that, instead of being evaluated as the variable, is used to refer
    to a variable itself.
  prefs: []
  type: TYPE_NORMAL
- en: A good analogy for understanding the relationship between a symbol and a variable
    has to do with the words in a phrase. Take for example the sentence: *Richard
    is tall*. Here, we understand that *Richard *is the name of a person, most likely
    a man. And Richard, the person, is tall. However, in the sentence: *Richard has
    seven letters*,it is obvious that now we aren't talking about Richard the person.
    It wouldn't make too much sense to assume that Richard the person has seven letters.
    We are talking about the word *Richard *itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equivalent, in Julia, of the first sentence (*Richard is tall*) would be
    `julia> x`. Here, `x` is immediately evaluated in order to produce its value.
    If it hasn''t been defined, it will result in an error, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Julia''s symbols mimic the second sentence, where we talk about the word itself.
    In English, we wrap the word in single quotes, ''Richard'', to indicate that we''re
    not referring to a person but to the word itself. In the same way, in Julia, we
    prefix the variable name with a column, `:x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, the column `:` prefix is an operator that stops the evaluation. An unevaluated
    expression can be evaluated on demand by using the `eval()` function or the `@eval`
    macro, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'But we can go beyond symbols. We can write more complex symbol-like statements,
    for example,`:(x = 2)`. This works a lot like a symbol but it is, in fact, an
    `Expr` type, which stands for expression. The expression, like any other type,
    can be referenced through variable names and, like symbols, they can be evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Even more powerful, since `Expr` is a type, it has properties that expose its
    internal structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Every `Expr` object has two fields—`head` representing its kind and `args`
    standing for the arguments. We can view the internals of `Expr` by using the `dump()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads us to even more important discoveries. First, it means that we can
    programmatically manipulate `Expr` through its properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Our expression is no longer `:(x = 2)`; it's now `:(x = 3)`. By manipulating
    the `args` of the `assign` expression, the value of `x` is now `3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we can programmatically create new instances of `Expr` using the type''s
    constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Please notice here that we wrapped the equals sign (`=`) in parenthesis to designate
    an expression, as Julia gets confused otherwise, thinking we want to perform an
    assignment right there.
  prefs: []
  type: TYPE_NORMAL
- en: Quoting expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous procedure, in which we wrap an expression within `:(...)` in order
    to create `Expr` objects, is called **quoting**. It can also be done using quote
    blocks. Quote blocks make quoting easier as we can pass *regular-looking *code
    into them (as opposed to translating everything in to symbols), and supports quoting
    multiple lines of code in order to build randomly complex expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Interpolating strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like with string interpolation, we can reference variables within the
    expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Macros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we finally have the knowledge to understand macros. They are language constructs,
    which are executed after the code is parsed, but before it is evaluated. It can
    optionally accept a tuple of arguments and must return an `Expr`. The resulting
    `Expression` is directly compiled, so we don't need to call `eval()` on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can implement a configurable version of the previous `greet`
    expression as a macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Macros are very powerful language constructs that allow parts of the code to
    be customized before the full program is run. The official Julia documentation
    has a great example to illustrate this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We define a macro called `twostep`, which has a body that calls the `println` function
    to output text to the console. It returns an expression which, when evaluated,
    will also output a piece of text via the same `println` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `I execute at runtime` message is outputted, but not the `I execute at parse
    time` message. This is a very powerful thing. Imagine that output instead of a
    simple text output if we'd had some very computationally intensive or time-consuming
    operations. In a simple function, we'd have to run this code every time, but with
    a macro, this is done only once, at parse time.
  prefs: []
  type: TYPE_NORMAL
- en: Closing words about macros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides they''re very powerful, macros are also very convenient. They can provide
    a lot of functionality with minimal overhead and can simplify the invocation of
    functions that take expressions as arguments. For example, `@time` is a very useful
    macro that executes an `Expression` while measuring the execution time. And the
    great thing is that we can pass the argument expression as *regular* code, instead
    of building the `Expr` by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Macros—and metaprogramming in general—are powerful concepts that require whole
    books to discuss at length. We must stop here in order to get back to our machine
    learning project. ACME Recruiting is eagerly waiting for our findings. I recommend
    going over the official documentation at [https://docs.julialang.org/en/stable/manual/metaprogramming/](https://docs.julialang.org/en/stable/manual/metaprogramming/).
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with Query.jl basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Query` package can be added in the standard way—`pkg> add Query`. Once
    you bring it into scope using `Query`, it makes a rich API available for querying
    Julia data sources, `DataFrames` being the most common source. A query is initiated
    using the `@from` macro.
  prefs: []
  type: TYPE_NORMAL
- en: '@from'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The general structure of a query is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Within the `begin...end` block, `var` represents a row in the `data_source`.
    The query statements are given one per line and can include any combination of
    available query commands, such as `@select`, `@orderby`, `@join`, `@group`, and
    `@collect`. Let's take a closer look at the most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: '@select'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `@select` query command, similar to its `SQL SELECT` counterpart, indicates
    which values are to be returned. Its general syntax is `@select condition`, where
    `condition` can be any `Julia` expression. Most commonly, we''ll want to return
    the whole row and, in this case, we''ll just pass `var` itself. For instance,
    let''s create a new `DataFrame` to hold a shopping list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9aa23832-92c7-4f21-9940-cf16ff6b12ba.png)'
  prefs: []
  type: TYPE_IMG
- en: A cool (geeky!) and handy shopping list.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can `@select` the whole row with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s not very useful, as this basically returns the whole `DataFrame`, but
    we can also reference a column using `dot` notation, for example, `p.produce`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that `@select` accepts any random `Julia` expression, we''re free to
    manipulate the data as we see fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'However, a better approach is to return `NamedTuple`, using the special query
    curly brackets syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52c58edd-0b81-4afd-90c0-5778a62abd0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we pass both the keys and the values for `NamedTuple`, but they''re not
    mandatory. They are, however, useful if we want properly named columns (and who
    doesn''t, right?):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/073cc81e-d228-4770-9373-e322e9a9153c.png)'
  prefs: []
  type: TYPE_IMG
- en: Without the explicit labels, `query` will assign column names such as `__1__`,
    `__2__`, and so on. It's not very readable!
  prefs: []
  type: TYPE_NORMAL
- en: '@collect'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might''ve noticed in the previous screenshots that the type of the returned
    value was `query result`. A query will return an iterator that can be further
    used to loop over the individual elements of the result set. But we can use the
    `@collect` statement to materialize the result into a specific data structure,
    most commonly `Array` or `DataFrame`. This is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e2749c8-5626-4d7d-b967-496d2a38f8c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, `@collect` will produce an `Array` of `NamedTuple` elements. But
    we can pass it an extra argument for the data type we desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7252e5e9-f2be-40bd-9438-bf942ad5bca9.png)'
  prefs: []
  type: TYPE_IMG
- en: Our result is now a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: '@where'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most useful commands is `@where`, which allows us to filter a data
    source so that only the elements that satisfy the condition are returned. Similar
    to `@select`, the condition can be any arbitrary `Julia` expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4393176-520b-46ea-91e0-e895d15310f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Only bread has a `qty` smaller than `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Filtering can be made even more powerful by means of range variables. These
    act like new variables belonging to the `query` expression and can be introduced
    using the `@let` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/163d72fb-c86d-4a01-96a5-e90290ed56a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can see how, within the `begin...end` block, we defined a local variable
    called `weekly_qty` with a value equal to `7 * p.qty`. We used the `@let` macro
    to introduce new variables. In the next line, we used it to filter out the rows
    that have a `weekly_qty` smaller than `10`. And then finally, we selected it and
    collected it into a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: '@join'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s make things even more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee9d6f9d-db29-44dd-8eb5-46c6756025e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We instantiate a new `DataFrame`, called `products_info`, which contains important
    information about items in our shopping list—their prices and whether or not they
    can be considered allergenic. We could use `DataFrames.hcat!` to append some columns
    from `products_info` to `shopping_list`, but again, the syntax is not so nice
    and the approach is not that flexible. We''ve been spoiled by Julia and we like
    it that way! Fortunately, Query provides a `@join` macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ba3ba9b7-2257-4995-84d9-d30da441fb1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The general syntax of a `@join` command is as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Query provides two other variants of `@join`: group join and left outer join.
    If you would like to read about them, please check the official documentation
    at [http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1](http://www.queryverse.org/Query.jl/stable/querycommands.html#Joining-1).'
  prefs: []
  type: TYPE_NORMAL
- en: '@group'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `@group` statement groups elements by some attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Not bad, but what we'd really like is to summarize the data. Query provides
    this under the name `split-apply-combine` (also known as, `dplyr`). This requires
    an aggregation function that will be used to collapse the dataset based on the
    `Grouping` variable. If that's too abstract, an example will surely clear things
    up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to get a count of allergenic items together with a comma-separated
    list of their names, so we know what to stay away from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be a two-row `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b3e656a-eb92-41c2-8e8b-691f6cabab33.png)'
  prefs: []
  type: TYPE_IMG
- en: '@orderby'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Query also provides a sorting macro named `@orderby`. It takes a list of attributes
    upon which to apply the sorting. Similar to SQL, the order is ascending by default,
    but we can change that by using the `descending` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our previously defined `products_info` `DataFrame`, we can easily sort
    it as needed, for example, with the most expensive products first and then by
    product name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a3ed77a4-067a-4959-b3b2-8931b7d40832.png)'
  prefs: []
  type: TYPE_IMG
- en: All right, that was quite a detour! But now that we have knowledge of the great
    `Query` package, we're ready to efficiently slice and dice our data. Let's go!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our data cleaning plan is to only keep the businesses registered in San Francisco,
    CA, for which we have the address, zip code, NAICS code, and business location
    and which have not been closed (so they don't have a business end date) and have
    not moved away (don't have a location end date).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `DataFrame` API to apply the filters would be tedious. But with Query,
    it''s a walk in the park:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see how `@where` filters are applied, requiring that `lowercase(b.City)`
    equals `"san francisco"` and that `b.State` equals `"CA"`. Then, we use `! isna`
    to make sure we only keep the rows where `b.Street_Address`, `b.Source_Zipcode`,
    `b.NAICS_Code`, `b.NAICS_Code_Description`, and `b.Business_Location` are not
    missing. The `isna` function is provided by the `DataValues` package (which is
    used by Query itself) and that's why we're adding and using it.
  prefs: []
  type: TYPE_NORMAL
- en: We also make sure that `b.Business_Location` matches a certain format that corresponds
    to geolocation coordinates. Finally, we make sure that, on the contrary, `b.Business_End_Date`
    and `b.Location_End_Date` are in fact missing.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the query produces a new `DataFrame` with almost 57,000 rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to take our `clean_df` data and extract the geo-coordinates
    out of the `Business_Location` column. Again, Query comes to the rescue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We make good use of the range variables feature (defined by `@let`) to introduce
    a `geo` variable, which uses `match` to extract the latitude and longitude pairs
    from the `Business_Location` data. Next, inside the `@select` block, the two values
    in the geo array are converted in to proper float values and added to the resulting
    `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be644856-1437-46d8-a631-490fadafaa8a.png)'
  prefs: []
  type: TYPE_IMG
- en: We're done! Our data is now neatly represented in our `clean_df_geo` `DataFrame`,
    containing the name of the business, zip code, NAICS code, NAICS code description,
    latitude, and longitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run `describe(clean_df_geo)`, we''ll see that we have 56,549 businesses
    with 53,285 unique names with only 18 NAICS code descriptions. We don''t know
    how many zip codes the companies are spread across, but it''s easy to find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Our businesses are registered within `79` zip codes in the city of San Francisco.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning with clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Julia's package ecosystem provides a dedicated library for clustering. Unsurprisingly,
    it's called **Clustering**. We can simply execute `pkg> add Clustering` to install
    it. The `Clustering` package implements a few common clustering algorithms—k-means,
    affinity propagation, DBSCAN, and kmedoids.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm is one of the most popular ones, providing a balanced
    combination of good results and good performance in a wide range of applications.
    However, one complication is that we're required to give it the number of clusters
    beforehand. More exactly, this number, called **k** (hence the first letter of
    the name of the algorithm), represents the number of centroids. A **centroid**
    is a point that is representative of each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm applies an iterative approach—it places the centroids
    using the algorithm defined by the seeding procedure, then it assigns each point
    to its corresponding centroid, the mean to which is closest. The algorithm stops
    on convergence, that is, when the point assignment doesn't change with a new iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm seeding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few ways to pick the centroids. Clustering provides three, one of
    which is random (labeled as the `:rand` option in clustering), which randomly
    selects a subset of points as seeds (so all centroids are random). This is the
    default seeding strategy in classical k-means. There's also k-means++, a better
    variation proposed in 2007 by David Arthur and Sergei Vassilvitskii (labeled as
    `:kmpp`), which picks one cluster center randomly, but then searches for the other
    centers in relation to the first one. The last available approach is centrality
    seeding (`:kmcen`), which picks the samples with the highest centrality.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the areas with the most businesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we successfully cleaned our data, now neatly accessible
    in `clean_df_geo` `DataFrame`. If you run into any problems with the data cleaning
    process, you can just go ahead and load the dataset from scratch by using the
    `clean_df_geo.tsv` file provided in this chapter''s support files ([https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/clean_df_geo.tsv.zip](https://github.com/PacktPublishing/Julia-Programming-Projects/blob/master/Chapter08/data/clean_df_geo.tsv.zip)).
    In order to load it, all you have to do is run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: So we want to identify the areas with the highest density of businesses. One
    approach is to use unsupervised machine learning to identify the areas by zip
    code and the number of businesses registered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll train our model using the data in the `:zipcode` column plus the number
    of businesses registered in the area. We''ll need the number of businesses per
    zip code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute a query against `clean_df_geo` `DataFrame`, grouping it by `:Source_Zipcode`
    into `g`. We store the number of businesses from the current zip code in the `bcount`
    range variable, as returned by `length(g)`, but not before converting the number
    into a `Float64`. The reason we''re doing this is that, as we''ll see right away,
    clustering expects the input to be `Float64`, so this will save us another processing
    step later. Back to our query. We also apply sorting by `bcount` to allow us,
    the humans, to better understand the data (not needed for training the model).
    Finally, we instantiate a new `DataFrame`, with two columns, a zip code, and `businesses_count`,
    without forgetting to convert the zip code into `Float64` too, for the same reason
    as before. When converting `key(g)`, please also note that we''re first calling
    the `get` function. This is because, within a query block, the computed values
    are represented as `DataValues` and to access the wrapped value we need to invoke
    `get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06081711-3cd3-4ee0-81d6-e1a9240bcef2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our training data is made of `79` zip codes and their corresponding businesses
    count. The top 22 areas have over 1,000 businesses each, and the number drops
    sharply for the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dca65879-db99-4b9e-a076-ee713ba143ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You probably remember `Gadfly`, the Julia plotting library we used in [Chapter
    1](90a7f09d-d63b-45d7-baf5-576470d0910f.xhtml), *Getting Started with Julia Programming*,
    to visualize the Iris flowers dataset. Let''s use it to quickly get a glimpse
    of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This will render the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da643e25-42cc-4547-8bd3-ab887ba9c316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can easily see that most of the areas only have one registered business,
    followed by a few others, which only have a few. We can safely remove these from
    our training dataset as they won''t be useful to our client. The only thing we
    need to do is to add the `@where bcount > 10` filter in the query for computing
    `model_data`, between the `@let` and the `@orderby` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Once we remove all of the areas that host less than `10` companies, we're left
    with only `28` zip codes.
  prefs: []
  type: TYPE_NORMAL
- en: Training our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Only one small step and we''re ready to train our model. We need to convert
    the `DataFrame` into an array and to permute the dimensions of the array so that
    the `DataFrame` columns become rows. In the new structure, each column (zip code
    and count pair) is considered a training sample. Let''s do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Our training data is ready! It''s time to put it to good use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We're using the k-means algorithm by invoking the function with the same name.
    As arguments, we provide the `training_data` array and give it three clusters.
    We want to split the areas into three tiers—low, medium, and high density. The
    training shouldn't take more than a few seconds. And since we gave it the `display=:iter`
    argument, we get progressive debug info at each iteration. For the seeding algorithm,
    we have used k-means++ (`:kmpp`).
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can take a look at how the points were assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Each element in the array corresponds to the element at the same index in the
    `model_data`. Let''s combine the data so it''s easier to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see what we end up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56e2d71e-dee6-4766-ba2e-4315548961e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the first three zip codes have been assigned to cluster `3`,
    the last eight to cluster `2`, and the rest to cluster `1`. You''ve probably noticed
    that the IDs of the clusters don''t follow the actual count values, which is normal
    since the data is unlabeled. It is us who must interpret the meaning of the clusters.
    And our algorithm has decided that the areas with the highest density of businesses
    will stay in cluster `3`, the low densities in cluster `2`, and the average ones
    in cluster `1`. Plotting the data with `Gadfly` will confirm our findings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces this plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/960b6dca-9559-473d-8984-8607b0b20df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Excellent! We can now inform our client that the best areas to target are in
    the zip codes 94110, 94103, and 94109, allowing them to reach 11,965 businesses
    in these dense parts of the city. They would also like to know which are these companies,
    so let''s prepare a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the zip codes we extracted in the clustering step to filter the `clean_df_geo`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a19e81a0-f09a-42c8-9144-b0afca3d4207.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We end up with 11,965 companies concentrated in three area codes. Let''s plot
    the points using the `geo` coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2ab408-24e1-45b5-87aa-59c304bf229d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the locations are in close proximity, but there is one outlier
    whose coordinates are way off. Maybe there''s an error in our data. Using Query,
    we can easily remove the culprit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'With our cleaned-up list, we can now explore the domain of activity for these
    companies. This will help our client reach out to candidates that fit the market''s
    demand, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'That was easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2afb42f-a3dc-4711-a19d-7359d3a19d18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All the companies in the targeted area are active in just `18` domains, out
    of which real estate is the most common one. Surely, our client''s executives
    would appreciate a chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58c3bb0d-073c-449a-a7cd-dd1b376307bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, the chart clearly shows that real estate is the activity in which most
    of the businesses are involved, with tech and retail coming in next.
  prefs: []
  type: TYPE_NORMAL
- en: Refining our findings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Great progress so far, but a list of almost 12,000 companies is still hard
    to handle. We can help our client by breaking it down into clusters of businesses
    located in close proximity. It''s the same workflow as before. First, we extract
    our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5459fe82-6cca-4a38-ac6f-1147121f6bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we permute the dimensions to set the data in the format expected by clustering
    (just like we did before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Our training array is ready!
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the same k-means algorithm with k-means++ seeding.
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that k-means is generally not the best choice for clustering
    geolocation data. DBSCAN is usually better suited and I recommend that you look
    into it for production applications. The k-means algorithm will fail, for example,
    when dealing with close points that wrap over 180 degrees. For our example project
    and for the data we're handling, k-means works fine, but keep this limitation
    in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training works in the same way. We''ll go with `12` clusters, in order to get
    roughly 1,000 companies per group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This time it takes `24` iterations to reach convergence. Let''s see what we''ve
    got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the data is evenly spread, but we can spot a few clusters which don''t
    get that many businesses. Plotting the numbers gives us a clear picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28e9133d-5275-495d-84cf-066d876c91c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can *paste* the cluster assignments onto the `companies_in_top_areas`
    `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing our clusters on the map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get a better understanding of our data, in terms of points density and location
    proximity, we can render a plot with `Gadfly`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a58eb08a-a6dd-4c12-b4c9-d9b153b7a5e2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a pretty good cluster distribution, so our approach worked!
  prefs: []
  type: TYPE_NORMAL
- en: However, it would be even better if we could display the clusters on a map.
    Unfortunately, at the moment, there's no easy way to do this in Julia, so we'll
    use a third-party tool.
  prefs: []
  type: TYPE_NORMAL
- en: PlotlyJS ([https://github.com/sglyon/PlotlyJS.jl](https://github.com/sglyon/PlotlyJS.jl))
    provides related functionality, but my tests didn't produce good results given
    that the coordinates are tightly packed in the San Francisco area.
  prefs: []
  type: TYPE_NORMAL
- en: Using BatchGeo to quickly build maps of our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BatchGeo ([https://batchgeo.com](https://batchgeo.com)) is a popular web app
    for creating map-based data visualizations. It uses high-definition maps from
    Google and it provides a no-login, albeit limited, free version, which we can
    try out right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'BatchGeo expects a CSV file with a series of columns, so our first job is to
    set that up. It couldn''t be any simpler with Query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e29ce70-793a-42eb-bf6e-f1759dae3f86.png)'
  prefs: []
  type: TYPE_IMG
- en: The structured data is available in a new `DataFrame` called `export_data`.
    Unfortunately, BatchGeo has added a 250-row limit for free accounts, so we'll
    have to limit our export to just the top 250 rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we can export it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Success! The only thing left to do is to open [https://batchgeo.com](https://batchgeo.com)
    in your favorite web browser and drag and drop the `business.csv` file to the
    designated place:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by performing the steps, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f6f4b281-3bca-470f-a1ee-d7e714f537fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click Validate & Set Options. You''ll see that the columns were picked correctly
    and the defaults are good:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7926f0f1-20b1-4f06-a165-8c751748f4bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on Make Map will render our clusters on top of the map of San Francisco:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9794ae2a-642c-4479-a4eb-676c170b1f0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Victory—a beautiful rendering of our data!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also disable the clustering so that each individual business will be
    plotted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41672512-690b-4238-85fd-70555ca207e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we can save our map, follow the instructions, and get a unique URL
    for our visualization. Mine can be found at [https://batchgeo.com/map/7475bf3c362eb66f37ab8ddbbb718b87](https://batchgeo.com/map/7475bf3c362eb66f37ab8ddbbb718b87)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Excellent, just in time for the meeting with our client!
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the optimal number of clusters for k-means (and other algorithms)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the nature of the data and the problem you'll be looking to solve,
    the number of clusters can come as a business requirement, or it may be an obvious
    choice (as in our case, where we wanted to identify low, middle, and high business
    density zones and so ended up with three clusters). However, in some cases, the
    answer might not be that obvious. In such situations, we'll need to apply a different
    algorithm to evaluate the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common is the Elbow method. It is an iterative approach where
    we run the clustering algorithm with different values for k, for example between
    1 and 10\. The goal is to compare the total intra-cluster variation by plotting
    the sum of squared errors between each point and the mean of its cluster as a
    function of k. Using the visualization, we identify the *elbow-like *point of
    inflection, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80d348c1-5ba4-4e46-b73f-ae1945d4e09f.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the elbow.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about this at [http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/](http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/)
    (with examples in R).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides picking the optimum number of clusters, another aspect is cluster validation,
    that is, to determine how well the items fit the assigned clusters. This can be
    used to confirm that patterns do indeed exist and to compare competing clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering provides a small API for clustering validation with three techniques,
    including Silhouettes, one of the most common. You can find the documentation
    at [http://clusteringjl.readthedocs.io/en/latest/validate.html](http://clusteringjl.readthedocs.io/en/latest/validate.html)
    and you can read more about validation theory at [http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/](http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked into unsupervised machine learning techniques with
    Julia. We focused on clustering, one of the most widely used applications of unsupervised
    learning. Starting with a dataset about businesses registered in San Francisco,
    we performed complex—but not complicated, thanks to Query—data cleansing. In the
    process, we also learned about metaprogramming, a very powerful coding technique
    and one of Julia's most powerful and defining features.
  prefs: []
  type: TYPE_NORMAL
- en: Once our data was in top shape and after mastering the basics of clustering
    theory, using the k-means algorithm, we got down to business. We performed clustering
    to identify the areas with the highest density of companies to help our imaginary
    customer, ACME Recruiting, to target the best areas for advertising. After identifying
    the parts of the city that would give ACME the best reach, we performed data analysis
    to get the top domains of activity required by our customer so they could build
    a database of relevant candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we performed clustering on the geolocation data of the businesses in
    the targeted areas and then we rendered these on top of a map. Our client was
    thrilled with our findings and their marketers now having all the necessary information
    to start planning their campaigns. Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll leave the fascinating world of machine learning in
    order to discover yet another key concept in data science—time series. We'll learn
    how to handle dates and time in Julia, how to work with time series data, and
    how to make forecasts. Exciting, isn't it?
  prefs: []
  type: TYPE_NORMAL
