- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Genomics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因组学
- en: '**Genomics** is the study of an organism’s genome or genetic material. In humans,
    genetic materials are stored in the form of **Deoxyribonucleic Acid** (**DNA**).
    These are the instructions that make up a human being, and 99.9% of the genomes
    in humans are identical and only 0.1% is different, which accounts for the differences
    in physical characteristics, such as eye color. Most of these variations are harmless,
    but some variants can cause health conditions, such as sickle cell anemia. Therefore,
    the analysis of such information can be used to predict or prevent a disease or
    provide personalized treatment, also known as **precision medicine**. There are
    four chemical bases present in DNA, namely **adenine** (**A**), **thymine** (**T**),
    **cytosine** (**C**), and **guanine** (**G**). They always bond in a particular
    manner; for example, adenine will always bond with thymine, and cytosine with
    guanine. The combination of these chemical bases is what makes up a DNA sequence.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**基因组学**是研究生物体基因组或遗传物质的科学。在人类中，遗传物质以**脱氧核糖核酸**（**DNA**）的形式储存。这些是构成人类个体的指令，人类基因组中有99.9%是相同的，只有0.1%是不同的，这导致了诸如眼色等身体特征的差异。大多数这些变异是无害的，但一些变异可能导致健康问题，例如镰状细胞性贫血。因此，分析此类信息可以用于预测或预防疾病，或提供个性化治疗，也称为**精准医疗**。DNA
    中存在四种化学碱基，即**腺嘌呤**（**A**）、**胸腺嘧啶**（**T**）、**胞嘧啶**（**C**）和**鸟嘌呤**（**G**）。它们总是以特定的方式结合；例如，腺嘌呤总是与胸腺嘧啶结合，而胞嘧啶与鸟嘌呤结合。这些化学碱基的组合构成了
    DNA 序列。'
- en: '**Sequencing** is at the heart of genomics. To understand what it means, the
    **Human Genome Project** ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf))
    was started in 1989 with the objective of sequencing one human genome within 15
    years. This was completed in 12 years in 2001 and involved thousands of scientists.
    With the development of next-generation sequencing technology, the whole human
    genome can now be generated in about a day. A single human genome is around 3
    billion base pairs long; a similar size has been seen for other organisms such
    as a mouse or a cow.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**测序**是基因组学的核心。为了理解它的含义，**人类基因组计划**（[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf)）于
    1989 年启动，目标是 15 年内测序一个人类基因组。这项工作在 2001 年的 12 年内完成，涉及数千名科学家。随着下一代测序技术的发展，现在可以在大约一天内生成整个人类基因组。单个人类基因组大约有
    30 亿个碱基对长；其他生物体，如老鼠或牛，也有类似的大小。'
- en: 'Since the time and cost of generating the genome sequence have significantly
    dropped, it has led to the generation of an enormous amount of data. So, in order
    to analyze this magnitude of data, we need powerful machines and a large amount
    of cost-effective storage. The good news is that DNA sequencing data is publicly
    available, and one of the largest repositories is the **National Center for Biotechnology
    Information** (**NCBI**). We can use statistical and **machine learning** (**ML**)
    models to gain insights from the genome data, which can be compute intensive.
    This poses two major challenges: big data and a massive ML model are required
    to make predictions, such as the prediction of promoters or predicting the masked
    DNA sequence.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成基因组序列的时间和成本显著降低，这导致了大量数据的生成。因此，为了分析如此大量的数据，我们需要强大的机器和大量的经济高效的存储。好消息是 DNA
    测序数据是公开可用的，其中最大的存储库之一是**国家生物技术信息中心**（**NCBI**）。我们可以使用统计和**机器学习**（**ML**）模型从基因组数据中获得见解，这可能需要大量的计算。这提出了两个主要挑战：需要大数据和大规模的
    ML 模型来进行预测，例如预测启动子或预测掩蔽的 DNA 序列。
- en: 'Therefore, this chapter will help navigate these challenges by covering the
    following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将通过涵盖以下主题来帮助您应对这些挑战：
- en: Managing large genomics data on AWS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 上管理大型基因组数据
- en: Designing architecture for genomics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计基因组学架构
- en: Applying ML to genomics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将机器学习应用于基因组学
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，您应该具备以下先决条件：
- en: A web browser (for the best experience, it is recommended that you use a Chrome
    or Firefox browser)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络浏览器（为了获得最佳体验，建议您使用 Chrome 或 Firefox 浏览器）
- en: Access to the AWS account that you used in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问您在 [*第 5 章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析* 中使用的 AWS 账户
- en: Access to the SageMaker Studio development environment that we created in [*Chapter
    5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问我们在**第5章**（[B18493_05.xhtml#_idTextAnchor095]）*数据分析*中创建的SageMaker Studio开发环境
- en: Example Jupyter notebooks for this chapter are provided in the companion GitHub
    repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的示例Jupyter笔记本可在配套的GitHub仓库中找到（[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06)）
- en: Managing large genomics data on AWS
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS上管理大型基因组数据
- en: 'Apart from the large size of the genomics dataset, other challenges for managing
    it include discoverability, accessibility, availability, and storing it in a storage
    system that allows for scalable data processing while keeping the critical data
    safe. *The responsible and secure sharing of genomic and health data is key to
    accelerating research and improving human health*, is a stated objective of the
    **Global Alliance for Genomics and Health** (**GA4GH**). This approach requires
    two important things: one is a deep technical understanding of the domain, and
    the second is access to compute and storage resources. You can also find many
    genomics datasets hosted by AWS on the **Registry of Open Data on** **AWS** ([https://registry.opendata.aws/](https://registry.opendata.aws/)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基因组数据集的大规模之外，管理它所面临的挑战还包括可发现性、可访问性、可用性，以及将其存储在允许可扩展数据处理同时保持关键数据安全的存储系统中。*负责和安全地共享基因组与健康数据是加速研究和改善人类健康的关键目标*，这是**全球基因组与健康联盟**（**GA4GH**）的一个明确目标。这种方法需要两个重要的事情：一是对领域有深入的技术理解，二是访问计算和存储资源。你还可以在**AWS开放数据注册表**（[https://registry.opendata.aws/](https://registry.opendata.aws/)）上找到由AWS托管的许多基因组数据集。
- en: Before you can begin any processing on the genomics dataset using cloud services,
    you need to make sure that it’s transferred and stored on the AWS cloud. For storing
    data, we recommend using **Amazon Simple Storage Services** (**Amazon S3**), as
    genomics data produced by next-generation sequencers are persisted in files, and
    a lot of genomic data analysis tools also take files as inputs and write the output
    back as files. For example, using an ML model for data analysis might involve
    taking large DNA sequence files as input and storing the inference or prediction
    results in a file, for which Amazon S3 makes a natural fit.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以使用云服务对基因组数据集进行任何处理之前，您需要确保它已传输并存储在AWS云上。对于存储数据，我们建议使用**Amazon Simple Storage
    Services**（**Amazon S3**），因为下一代测序仪产生的基因组数据以文件形式持久化，许多基因组数据分析工具也以文件作为输入，并将输出写回文件。例如，使用机器学习模型进行数据分析可能涉及以大型DNA序列文件作为输入，并将推理或预测结果存储在文件中，对于这种情况，Amazon
    S3是一个自然的选择。
- en: You can store genomics data securely by enabling server-side encryption with
    either **Amazon S3-managed encryption keys** (**SSE-S3**) or **AWS Key Management
    Service** (**AWS KMS**) keys. Moreover, Amazon S3 also allows you to enable the
    data life cycle by storing the infrequently accessed data in the **Amazon S3 Standard-Infrequent
    Access** (**S3 Standard-IA**) class tier or archiving the data to a low-cost storage
    option, such as **Amazon S3 Glacier Deep Archive** when the data is not in use
    to significantly reduce the cost. This pattern is discussed in detail in the *Tiered
    storage for cost optimization* section of [*Chapter 4*](B18493_04.xhtml#_idTextAnchor074),
    *Data Storage.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过启用服务器端加密来安全地存储基因组数据，无论是使用**Amazon S3管理的加密密钥**（**SSE-S3**）还是**AWS密钥管理服务**（**AWS
    KMS**）密钥。此外，Amazon S3还允许您通过将不常访问的数据存储在**Amazon S3标准-不经常访问**（**S3 Standard-IA**）层或存档到低成本存储选项，例如当数据未使用时存档到**Amazon
    S3 Glacier Deep Archive**，来启用数据生命周期，从而显著降低成本。这种模式在**第4章**（[B18493_04.xhtml#_idTextAnchor074]）*数据存储*的*分层存储以优化成本*部分有详细讨论。
- en: For transferring genomics data to Amazon S3, you can use the AWS DataSync service,
    as discussed in [*Chapter 2*](B18493_02.xhtml#_idTextAnchor035), *Data Management*
    *and Transfer.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将基因组数据传输到Amazon S3，您可以使用在**第2章**（[B18493_02.xhtml#_idTextAnchor035]）*数据管理和传输*中讨论的AWS
    DataSync服务。
- en: Let’s take a closer look at the detailed architecture for applying ML models
    to the genomics dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看将机器学习模型应用于基因组数据集的详细架构。
- en: Designing architecture for genomics
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计基因组学架构
- en: 'In this section, we will describe a sample reference architecture for transferring,
    storing, processing, and gaining insights on genomics datasets on the AWS cloud,
    in a secure and cost-effective manner. *Figure 12**.1* shows a sample genomics
    data processing workflow:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述一个示例参考架构，用于在AWS云中以安全和经济的方式传输、存储、处理和获取基因组数据集的见解。*图12*.*1*显示了示例基因组数据处理流程：
- en: '![Figure 12.1 – Genomics data processing workflow](img/B18493_12_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – 基因组数据处理流程](img/B18493_12_001.jpg)'
- en: Figure 12.1 – Genomics data processing workflow
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 基因组数据处理流程
- en: '*Figure 12**.1* shows the following workflow:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12*.*1*显示了以下工作流程：'
- en: A scientist or a lab technician will collect sample genomic data, for example,
    skin cells, prepare it in a lab, and then load it into a sequencer.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 科学家或实验室技术人员将收集样本基因组数据，例如皮肤细胞，在实验室中准备它，然后将其加载到测序仪中。
- en: The sequencer will then generate a sequence, which might be short DNA fragments.
    These are usually called **reads** because you are reading DNA.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，测序仪将生成一个序列，这可能是短的DNA片段。这些通常被称为**reads**，因为您正在读取DNA。
- en: The DNA sequence is stored in an on-premises data storage system.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNA序列存储在本地数据存储系统中。
- en: The AWS DataSync service will then transfer the genomic data securely to the
    cloud; for further details, refer to [*Chapter 2*](B18493_02.xhtml#_idTextAnchor035),
    *Data Management* *and Transfer*.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，AWS DataSync服务将基因组数据安全地传输到云端；有关更多详细信息，请参阅[*第2章*](B18493_02.xhtml#_idTextAnchor035)，*数据管理*
    *和传输*。
- en: The raw genomic data is then stored on Amazon S3\. You can use AWS Analytics
    tools for data processing.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将原始基因组数据存储在Amazon S3上。您可以使用AWS Analytics工具进行数据处理。
- en: 'Amazon Genomics CLI is a purpose-built open source tool for processing raw
    genomics data in the cloud at a petabyte scale. For details, please refer to this
    link: [https://aws.amazon.com/genomics-cli/](https://aws.amazon.com/genomics-cli/).'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amazon Genomics CLI是一个专为在云中处理PB级原始基因组数据而构建的开源工具。有关详细信息，请参阅此链接：[https://aws.amazon.com/genomics-cli/](https://aws.amazon.com/genomics-cli/)。
- en: Optionally, we recommend storing the processed genomics data on **Amazon Feature
    Store**, which is a fully managed service for storing, sharing, versioning, and
    managing ML features for training and inference to enable the reuse of features
    across ML applications.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，我们建议将处理过的基因组数据存储在**Amazon Feature Store**上，这是一个用于存储、共享、版本控制和管理的完全托管服务，用于存储、共享、版本控制和管理的机器学习特征，以便在机器学习应用之间重用特征。
- en: 'You can add granular access control policies on genomics data stored on Amazon
    S3 or Amazon Feature Store by using the **AWS Lake Formation** service, based
    on your business requirements. For details on AWS Lake Formation, refer to this
    link: [https://aws.amazon.com/lake-formation](https://aws.amazon.com/lake-formation).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用**AWS Lake Formation**服务在Amazon S3或Amazon Feature Store上存储的基因组数据上添加细粒度的访问控制策略，根据您的业务需求。有关AWS
    Lake Formation的详细信息，请参阅此链接：[https://aws.amazon.com/lake-formation](https://aws.amazon.com/lake-formation)。
- en: Once the data is processed and stored on either Amazon Feature Store or Amazon
    S3, you can run ML models such as **DNABERT** ([https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1](https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1))
    using **Amazon SageMaker** to gain further insights or to predict the masked DNA
    sequence. The ML model can take a batch of genomic data, make inferences, and
    store the results back in Amazon S3.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据被处理并存储在Amazon Feature Store或Amazon S3上，您可以使用**Amazon SageMaker**运行如**DNABERT**（[https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1](https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1)）等机器学习模型，以获得更深入的见解或预测掩码DNA序列。该机器学习模型可以处理一批基因组数据，进行推理，并将结果存储回Amazon
    S3。
- en: Additionally, you can archive the unused data to Amazon S3 Glacier Deep Archive
    to have significant cost savings on data storage.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您可以将未使用的数据存档到Amazon S3 Glacier Deep Archive，以在数据存储上实现显著的成本节约。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed discussion on Amazon Genomics CLI, AWS Lake Formation, and Amazon
    Feature Store is out of scope for this chapter; however, we will use the DNABERT
    model in the *Applying ML to genomics* section of this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不涉及对Amazon Genomics CLI、AWS Lake Formation和Amazon Feature Store的详细讨论；然而，我们将在本章的*将机器学习应用于基因组*部分使用DNABERT模型。
- en: Let’s learn how to apply the ML model to genomics applications and predict masked
    sequences in a DNA sequence using a pretrained ML model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何将机器学习模型应用于基因组应用，并使用预训练的机器学习模型预测DNA序列中的掩码序列。
- en: Applying ML to genomics
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习应用于基因组
- en: Before we dive into ML model details, let’s first understand the genomic data,
    which is stored as DNA in every organism. There are four chemical bases present
    in DNA, namely `ACTCCACAGTACCTCCGAGA`. A single complete sequence of the human
    genome is around 3 billion **base pairs** (**bp**) long and takes about 200 GB
    of data storage ([https://www.science.org/doi/10.1126/science.abj6987](https://www.science.org/doi/10.1126/science.abj6987)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨机器学习模型细节之前，让我们首先了解基因组数据，它以DNA的形式存储在每一个生物体中。DNA中存在四种化学碱基，即`ACTCCACAGTACCTCCGAGA`。人类基因组的一个完整序列大约有30亿个**碱基对**（**bp**）长，大约需要200
    GB的数据存储空间([https://www.science.org/doi/10.1126/science.abj6987](https://www.science.org/doi/10.1126/science.abj6987))。
- en: However, for analyzing the DNA sequence, we don’t need the complete human genome
    sequence. Usually, we analyze a part of the human DNA; for example, to determine
    hair growth or skin growth, a lab technician will take a small section of human
    skin and prepare it to run through the next-generation sequencer, which will then
    read the DNA and generate the DNA sequence, which are short fragments of DNA.
    ML models can be used for various tasks, such as DNA classification, **promoter
    recognition**, interpretation of structural variation in human genomes, precision
    medicine, cancer research, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于分析DNA序列，我们不需要完整的基因组序列。通常，我们只分析人类DNA的一部分；例如，为了确定毛发生长或皮肤生长，实验室技术人员会取一小部分人类皮肤并准备它通过下一代测序仪，然后读取DNA并生成DNA序列，这些是DNA的短片段。机器学习模型可用于各种任务，例如DNA分类、**启动子识别**、解释人类基因组中的结构变异、精准医疗、癌症研究等等。
- en: 'In this section, we will showcase how to fine-tune the DNABERT model using
    Amazon SageMaker for the proximal promoter recognition task. DNABERT is based
    on the BERT model fine-tuned on DNA sequences, as outlined in the research paper
    *Supervised promoter recognition: a benchmark framework* ([https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf](https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf)).
    Therefore, let’s take an example of deploying a pretrained DNABERT model for promoter
    recognition using DNA sequence data on Amazon SageMaker service.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何使用Amazon SageMaker对DNABERT模型进行微调，以执行近端启动子识别任务。DNABERT基于在DNA序列上微调的BERT模型，如研究论文*监督启动子识别：一个基准框架*（[https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf](https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf)）中概述的。因此，让我们以在Amazon
    SageMaker服务上使用DNA序列数据部署预训练的DNABERT模型进行启动子识别为例。
- en: Amazon SageMaker is a fully managed service by AWS to assist ML practitioners
    in building, training, and deploying ML models. Although it provides features
    and an integrated development environment for each phase of ML, it is highly modular
    in nature, meaning if you already have a trained model, you can use the **SageMaker
    hosting** features to deploy the model for performing inference/predictions on
    your model. For details on the various types of deployment options provided by
    SageMaker, refer to [*Chapter 7*](B18493_07.xhtml#_idTextAnchor128), *Deploying
    Machine Learning Models* *at Scale*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 是 AWS 提供的一项完全托管服务，旨在协助机器学习从业者构建、训练和部署机器学习模型。尽管它为机器学习的每个阶段提供了功能和集成开发环境，但其本质上是高度模块化的，这意味着如果您已经有一个训练好的模型，您可以使用
    **SageMaker托管** 功能来部署模型，以便在您的模型上执行推理/预测。有关SageMaker提供的各种部署选项的详细信息，请参阅[*第7章*](B18493_07.xhtml#_idTextAnchor128)，*大规模部署机器学习模型*。
- en: 'We will deploy a version of the DNABERT pretrained model provided by **Hugging
    Face**, an AI community with a library of 65K+ pretrained transformer models.
    Amazon SageMaker provides first-party deep learning containers for Hugging Face
    for both training and inference. These containers include Hugging Face pretrained
    transformer models, tokenizers, and dataset libraries. For a list of all the available
    containers, you can refer to this link: [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    These containers are regularly maintained and updated with security patches and
    remove undifferentiated heavy lifting for ML practitioners.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署由**Hugging Face**提供的DNABERT预训练模型版本，Hugging Face是一个拥有65K+预训练transformer模型的AI社区。Amazon
    SageMaker为Hugging Face提供了一方深度学习容器，用于训练和推理。这些容器包括Hugging Face预训练transformer模型、分词器和数据集库。有关所有可用容器的列表，您可以参考此链接：[https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)。这些容器定期维护和更新，包括安全补丁，并减轻了机器学习实践者的繁重工作。
- en: 'With a few lines of configuration code, you can deploy a pretrained model in
    the Hugging Face library on Amazon SageMaker, and start making predictions using
    your model. Amazon SageMaker provides a lot of features for deploying ML models.
    For example, in the case of **real-time inference**, when you choose to deploy
    the model as an API, set up an **autoscaling policy** to scale up and down the
    number of instances on which the model is deployed based on the number of invocations
    on the model. Additionally, you can do **blue/green deployment**, add **security
    guardrails**, **auto-rollback,** and so on, using Amazon SageMaker hosting features.
    For details on deploying models for inference, refer to this link: [https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).
    Now that we have understood the benefits of using Amazon SageMaker for deploying
    models and Hugging Face integration, let’s see how we can deploy a pretrained
    DNABERT model for promoter recognition.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几行配置代码，您可以在Amazon SageMaker上部署Hugging Face库中的预训练模型，并开始使用您的模型进行预测。Amazon SageMaker为部署ML模型提供了许多功能。例如，在**实时推理**的情况下，当您选择将模型作为API部署时，设置**自动扩展策略**以根据模型调用的数量调整部署的实例数量。此外，您可以使用Amazon
    SageMaker托管功能进行**蓝绿部署**、添加**安全护栏**、**自动回滚**等操作。有关部署推理模型的详细信息，请参阅此链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)。现在我们已经了解了使用Amazon
    SageMaker部署模型和Hugging Face集成的优势，让我们看看我们如何部署用于促进识别的预训练DNABERT模型。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The full code for deploying the model is available on GitHub: [https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型的完整代码可在GitHub上找到：[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb)。
- en: 'We need to follow three steps for deploying pretrained transformer models for
    real-time inference provided by the Hugging Face library on Amazon SageMaker:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遵循三个步骤来部署由Hugging Face库在Amazon SageMaker上提供的预训练transformer模型进行实时推理：
- en: Provide model hub configuration where we supply the Hugging Face model ID and
    the task – in our case, text classification.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供模型中心配置，其中我们提供Hugging Face模型ID和任务——在我们的案例中，是文本分类。
- en: Create a `HuggingFaceModel` class provided by the SageMaker API, where we provide
    parameters such as the transformer version, PyTorch version, Python version, hub
    configuration, and role.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个由SageMaker API提供的`HuggingFaceModel`类，其中我们提供参数，如transformer版本、PyTorch版本、Python版本、中心配置和角色。
- en: Finally, we use the `deploy()` API, where we supply the number of instances
    and the type of instance on which to deploy the model.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`deploy()` API，其中我们提供要部署的实例数量和实例类型。
- en: 'The following code snippet showcases the three steps that we just outlined:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了我们刚刚概述的三个步骤：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this code snippet, we basically tell SageMaker to deploy the Hugging Face
    model provided in `'HF_MODEL_ID'` for the task mentioned in `'HF_TASK'`; in our
    case, `text classification`, as we want to classify promoter regions by providing
    a DNA sequence. The `HuggingFaceModel` class defines the container on which the
    model will be deployed. Finally, the `deploy()` API launches the Hugging Face
    container defined by the `HuggingFaceModel` class and loads the model provided
    in the hub configuration to the initial number of instances and type of instances
    provided by the ML practitioner.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '使用此代码片段，我们基本上告诉SageMaker部署在`''HF_MODEL_ID''`中提供的Hugging Face模型，用于`''HF_TASK''`中提到的任务；在我们的案例中，是`text
    classification`，因为我们希望通过提供DNA序列来对启动子区域进行分类。`HuggingFaceModel`类定义了模型将要部署的容器。最后，`deploy()`
    API启动由`HuggingFaceModel`类定义的Hugging Face容器，并将存储在hub配置中的模型加载到由ML实践者提供的初始实例数量和实例类型。 '
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The number of instances on which the model is deployed as an API can be updated
    even after the model is deployed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型已部署，也可以更新模型作为API部署的实例数量。
- en: 'Once the model is deployed, you can then use the `predict()` API provided by
    SageMaker to make inferences or predictions on the model, as shown in the following
    code snippet:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署后，您可以使用SageMaker提供的`predict()` API对模型进行推理或预测，如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output will be the label with the highest probability. In our case, it’s
    either `LABEL_0` or `LABEL_1`, denoting the absence or presence of a promoter
    region in a DNA sequence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是具有最高概率的标签。在我们的案例中，是`LABEL_0`或`LABEL_1`，表示DNA序列中是否存在启动子区域。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding code deploys the model as an API on a long-running instance, so
    if you are not using the endpoint, please make sure to delete it; otherwise, you
    will be charged for it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将模型作为API部署在长时间运行的实例上，因此如果您不使用端点，请务必将其删除；否则，您将为此付费。
- en: 'You can also see the endpoint details on SageMaker Studio by clicking on an
    orange triangle icon (**SageMaker resources**) in the left navigation panel and
    selecting **Endpoints**, as shown in the following screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在左侧导航面板中点击橙色三角形图标（**SageMaker资源**）并选择**端点**，在SageMaker Studio上查看端点详情，如图所示：
- en: '![Figure 12.2 – Accessing Endpoints on SageMaker Studio](img/B18493_12_002.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 在SageMaker Studio上访问端点](img/B18493_12_002.jpg)'
- en: Figure 12.2 – Accessing Endpoints on SageMaker Studio
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 在SageMaker Studio上访问端点
- en: 'This will show all the SageMaker endpoints (models deployed for real-time inference
    as an API). Double-clicking on the endpoint name will show you the details by
    calling the `DescribeEndpoint()` API behind the scenes. The SageMaker Studio UI
    shows you a lot of options, such as **Test inference**, **Data quality**, **Model
    quality**, **Model explainability**, **Model bias**, **Monitoring job history**,
    and **AWS settings**. The data is populated on these tabs based on the features
    you have enabled; for example, to understand the data and model quality, you need
    to enable the model monitoring feature of SageMaker, which monitors the models
    deployed as real-time endpoints on a schedule set by you, compares it with baseline
    statistics, and stores the report in S3\. For details on model monitoring, refer
    to this link: [https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示所有SageMaker端点（作为API部署用于实时推理的模型）。双击端点名称将显示通过调用后台的`DescribeEndpoint()` API的详细信息。SageMaker
    Studio UI向您展示了大量选项，例如**测试推理**、**数据质量**、**模型质量**、**模型可解释性**、**模型偏差**、**监控作业历史记录**和**AWS设置**。这些选项卡上的数据基于您启用的功能；例如，要了解数据和模型质量，您需要启用SageMaker的模型监控功能，该功能根据您设置的日程安排监控作为实时端点部署的模型，将其与基线统计数据进行比较，并将报告存储在S3中。有关模型监控的详细信息，请参阅此链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)。
- en: 'The **AWS settings** tab, on the other hand, will always be populated with
    model endpoint metadata, such as the endpoint name, type, status, creation time,
    last updated time, **Amazon Resource Name** (**ARN**), endpoint runtime settings,
    endpoint configuration settings, production variants (in case you have more than
    one variant of the same model), instance details (type and number of instances),
    model name, and lineage, as applicable. *Figure 12**.3* shows some of the metadata
    associated with the endpoint:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**AWS 设置**选项卡将始终包含模型端点元数据，例如端点名称、类型、状态、创建时间、最后更新时间、**Amazon Resource Name**（**ARN**）、端点运行时设置、端点配置设置、生产变体（如果你有同一模型的多个变体）、实例详细信息（类型和实例数量）、模型名称和血统，根据适用情况。*图
    12.3* 展示了一些与端点相关的元数据：
- en: '![Figure 12.3 – SageMaker endpoint details](img/B18493_12_003.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – SageMaker 端点详细信息](img/B18493_12_003.jpg)'
- en: Figure 12.3 – SageMaker endpoint details
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – SageMaker 端点详细信息
- en: 'Also, if you quickly want to test your model from the SageMaker Studio UI,
    you can click on **Test inference**, provide a payload (input request) in JSON
    format in **JSON editor**, as shown in *Figure 12**.4*, and quickly see the response
    provided by the model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想从 SageMaker Studio UI 快速测试你的模型，你可以点击 **测试推理**，在 **JSON 编辑器**中提供 JSON
    格式的有效载荷（输入请求），如图 *图 12.4* 所示，并快速查看模型提供的响应：
- en: '![Figure 12.4 – Test inference from the SageMaker Studio UI](img/B18493_12_004.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 从 SageMaker Studio UI 进行测试推理](img/B18493_12_004.jpg)'
- en: Figure 12.4 – Test inference from the SageMaker Studio UI
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 从 SageMaker Studio UI 进行测试推理
- en: 'Now that we understand how pretrained models on the Hugging Face model library
    can be deployed and tested on Amazon SageMaker, let’s take another example of
    how to fine-tune the pretrained model in the Hugging Face library, and deploy
    the fine-tuned model. For this section, we will use a BERT-based model trained
    on protein sequences known as **ProtBERT**, published in this research paper:
    [https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在亚马逊 SageMaker 上部署和测试 Hugging Face 模型库中的预训练模型，让我们再举一个例子，看看如何微调 Hugging
    Face 库中的预训练模型，并部署微调后的模型。对于本节，我们将使用一个基于 BERT 的、在蛋白质序列上训练的模型，称为 **ProtBERT**，该模型发表在以下研究论文中：[https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3)。
- en: The study of protein structure, functions, and interactions is called **proteomics**,
    which takes the help of genomic studies as proteins are the functional product
    of the genome. Both proteomics and genomics are used to prevent diseases and are
    an active part of drug discovery. Although there are a lot of tasks that contribute
    to drug discovery, protein classification, and secondary structure identification
    play a vital role. In the following section, we will understand how to fine-tune
    a large protein model (the Hugging Face library) to predict the secondary structure
    of a protein using the distributed training features of Amazon SageMaker.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质结构、功能和相互作用的研究被称为 **蛋白质组学**，它借助基因组学研究，因为蛋白质是基因组的功能性产物。蛋白质组学和基因组学都用于预防疾病，并且是药物发现活动的一部分。尽管有很多任务有助于药物发现、蛋白质分类和二级结构识别，但它们发挥着至关重要的作用。在下一节中，我们将了解如何使用亚马逊
    SageMaker 的分布式训练功能微调大型蛋白质模型（Hugging Face 库），以预测蛋白质的二级结构。
- en: Protein secondary structure prediction for protein sequences
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质序列的二级结构预测
- en: Protein sequences are made up of 20 essential **amino acids**, each represented
    by a capital letter. They combine to form a protein sequence, which you can use
    to do protein classification or predict the secondary structure of the protein,
    among other tasks. Protein sequences assume a particular 3D structure based on
    constraints, which are optimized for undertaking a particular function. You can
    think of these constraints as rules of grammar or meaning in natural language,
    which allows us to use **natural language processing** (**NLP**) techniques to
    protein sequences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质序列由 20 种必需的 **氨基酸** 组成，每种氨基酸由一个大写字母表示。它们组合形成一个蛋白质序列，你可以用它来进行蛋白质分类或预测蛋白质的二级结构，以及其他任务。蛋白质序列基于约束条件形成特定的
    3D 结构，这些约束条件是为了执行特定功能而优化的。你可以将这些约束条件视为自然语言中的语法规则或意义，这使我们能够使用 **自然语言处理**（**NLP**）技术来处理蛋白质序列。
- en: 'In this section, we will focus on fine-tuning the `prot_t5_xl_uniref50` model,
    which has around *11 billion parameters*, and you can use the same training script
    to fine-tune a smaller `prot_bert_bfd` model, which has close to *420 million
    parameters*, with different configuration to accommodate the size of the model.
    The code for fine-tuning the `prot_t5_xl_uniref50` model is provided in the GitHub
    repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点调整`prot_t5_xl_uniref50`模型，该模型大约有*110亿个参数*，您可以使用相同的训练脚本以不同的配置调整较小的`prot_bert_bfd`模型，该模型大约有*4.2亿个参数*，以适应模型的大小。调整`prot_t5_xl_uniref50`模型的代码在GitHub仓库中提供：[https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12)。
- en: 'To create a SageMaker training job using the model in the Hugging Face library,
    we will need a Hugging Face estimator from SageMaker SDK. The estimator class
    will handle all the training tasks based on the configuration that we provide.
    For example, to train a Hugging Face model with the basic configuration, we will
    need to provide the following parameters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Hugging Face库中的模型创建SageMaker训练作业，我们需要一个来自SageMaker SDK的Hugging Face估计器。估计器类将根据我们提供的配置处理所有训练任务。例如，要使用基本配置训练Hugging
    Face模型，我们需要提供以下参数：
- en: '`entry_point`: This is where we will specify the training script for training
    the model.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entry_point`: 这是我们将指定用于训练模型的训练脚本的位置。'
- en: '`source_dir`: This is the name of the folder where the training script or other
    helper files will reside.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source_dir`: 这是训练脚本或其他辅助文件所在文件夹的名称。'
- en: '`instance_type`: This is the type of machine in the cloud where the training
    script will run.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_type`: 这是训练脚本将在云中运行的机器类型。'
- en: '`instance_count`: This is the number of machines in the cloud that will be
    launched for running the training job. If the count is greater than `1`, then
    it will automatically launch a cluster.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_count`: 这是将用于运行训练作业的云中机器的数量。如果计数大于`1`，则将自动启动一个集群。'
- en: '`transfomer_version`, `pytorch_version`, `py_version`: These determine the
    version of transformers, PyTorch, and Python that will be present in the container.
    Based on the value of these parameters, SageMaker will fetch the appropriate container,
    which will be provisioned onto the instances (machines) in the cloud.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transfomer_version`, `pytorch_version`, `py_version`: 这些参数决定了容器中将存在的transformers、PyTorch和Python的版本。根据这些参数的值，SageMaker将获取适当的容器，并将其部署到云中的实例（机器）上。'
- en: '`hyperparameters`: This defines the named arguments that will be passed to
    the training script.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hyperparameters`: 这定义了将传递给训练脚本的命名参数。'
- en: 'The following code snippet formalizes these parameters into a SageMaker training
    job:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将这些参数正式化为SageMaker训练作业：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the estimator is defined, you can then provide the S3 location (path)
    of your data to start the training job. SageMaker provides some very useful environment
    variables for a training job, which include the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了估计器，您就可以提供您数据的S3位置（路径）以启动训练作业。SageMaker为训练作业提供了一些非常有用的环境变量，包括以下内容：
- en: '`SM_MODEL_DIR`: This provides the path where the training job will store the
    model artifacts, and once the job finishes, the stored model in this folder is
    directly uploaded to the S3 output location.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_MODEL_DIR`: 这提供了训练作业将存储模型工件的位置，一旦作业完成，该文件夹中存储的模型将直接上传到S3输出位置。'
- en: '`SM_NUM_GPUS`: This represents the number of GPUs available to the host.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_NUM_GPUS`: 这表示主机可用的GPU数量。'
- en: '`SM_CHANNEL_XXXX`: This represents the input data path for the specified channel.
    For example, `data`, in our case, will correspond to the `SM_CHANNEL_DATA` environment
    variable, which is used by the training script, as shown in the following code
    snippet:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SM_CHANNEL_XXXX`: 这表示指定通道的输入数据路径。例如，在我们的案例中，`data`将对应于`SM_CHANNEL_DATA`环境变量，该变量被训练脚本使用，如下面的代码片段所示：'
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When we call the `fit()` method on the Hugging Face estimator, SageMaker will
    first provision the ephemeral compute environment, and copy the training script
    and data to the compute environment. It will then kick off the training, save
    the trained model to the S3 output location provided in the estimator class, and
    finally, tear down all the resources so that users only pay for the amount of
    time the training job was running for.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the SageMaker Hugging Face estimator, as well as how
    to leverage the SageMaker SDK to instantiate the estimator, please see the AWS
    documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)),
    and the SageMaker SDK documentation ([https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We will extend this basic configuration to fine-tune `prot_t5_xl_uniref50` with
    *11 billion parameters* using the distributed training features of SageMaker.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do a deeper dive into the code, let’s first understand some of the
    concepts that we will leverage for training the model. Since it’s a big model,
    the first step is to get an idea of the size of the model, which will help us
    determine whether it will fit in a single GPU memory or not. The SageMaker model
    parallel documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html))
    gives a good idea of estimating the size of the model. For a training job that
    uses **automatic mixed precision** (**AMP**) with a **floating-point 16-bit**
    (**FP16**) size, using Adam optimizers, we can calculate the memory required per
    parameter using the following breakdown, which comes to about 20 bytes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: An FP16 parameter ~ 16 bits or 2 bytes
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An FP16 gradient ~ 16 bits or 2 bytes
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An FP32 optimizer state ~ 64 bits or 8 bytes based on the Adam optimizers
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An FP32 copy of parameter ~ 32 bits or 4 bytes (needed for the **optimizer apply**
    (**OA**) operation)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An FP32 copy of gradient ~ 32 bits or 4 bytes (needed for the OA operation)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, our model with 11 billion parameters will require more than 220 GB
    of memory, which is larger than the typical GPU memory currently available on
    a single GPU. Even if we are able to get such a machine with GPU memory greater
    than 220 GB, it will not be cost-effective, and we won’t be able to scale our
    training job. Another constraint to understand here is the batch size, as we will
    need at least one batch of data in the memory to begin training. Using a smaller
    batch size will drive down the GPU utilization and degrade the training efficiency,
    as the model might not be able to converge.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we will have to partition our model into multiple GPUs, and in order
    to increase the batch size, we will also need to shard the data. So, we will use
    a hybrid approach that will utilize both data parallel and model parallel approaches.
    This approach has been explained in detail in [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116),
    *Distributed Training of Machine* *Learning Models*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须将我们的模型分割成多个GPU，为了增加批量大小，我们还需要对数据进行分片。所以，我们将采用一种混合方法，该方法将利用数据并行和模型并行的两种方法。这种方法在[*第6章*](B18493_06.xhtml#_idTextAnchor116)《机器学习模型的分布式训练》中已有详细解释。
- en: 'Since our model size is 220 GB, we will have to use a mechanism in the training
    job, which will optimize it for memory in order to avoid **out of memory** (**OOM**)
    errors. For memory optimization, the SageMaker model parallel library provides
    two types of model parallelism, namely, pipeline parallelism and tensor parallelism,
    along with memory-saving techniques, such as activation checkpointing, activation
    offloading, and optimizer state sharding. Let’s understand each of these terms:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型大小为220 GB，我们将在训练作业中使用一种机制，以优化内存以避免**内存不足**（**OOM**）错误。为了内存优化，SageMaker模型并行库提供了两种类型的模型并行性，即流水线并行性和张量并行性，以及内存节省技术，如激活检查点、激活卸载和优化器状态分片。让我们了解每个术语：
- en: '`partitions` parameter value provided in the configuration is `2`, then the
    model will be divided into 2 partitions across 4 GPUs, meaning it will have 2
    model replicas. Since, in this case, the number of GPUs is greater than the number
    of partitions, we will have to enable `ddp` parameter to `true`. Otherwise, the
    training job will give an error. Pipeline parallelism with `ddp` enabled is illustrated
    in *Figure 12**.5*:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果配置中提供的`partitions`参数值为`2`，则模型将在4个GPU上分成2个分区，这意味着它将有2个模型副本。由于在这种情况下，GPU的数量大于分区的数量，我们必须将`ddp`参数设置为`true`。否则，训练作业将给出错误。启用`ddp`的流水线并行性在*图12.5*中得到了说明：
- en: '![Figure 12.5 – Showcasing the hybrid approach with model and data parallelism](img/B18493_12_005.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – 展示了模型和数据并行的混合方法](img/B18493_12_005.jpg)'
- en: Figure 12.5 – Showcasing the hybrid approach with model and data parallelism
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 展示了模型和数据并行的混合方法
- en: '**Tensor Parallelism**: This applies the same concept as pipeline parallelism
    and goes a step further to divide the largest layer of our model and places it
    on different nodes. This concept is illustrated in *Figure 12**.6*:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量并行性**：这种方法与流水线并行性具有相同的概念，并且更进一步，将我们模型的最大层分割并放置在不同的节点上。这一概念在*图12.6*中得到了说明：'
- en: '![Figure 12.6 – Tensor parallelism shards tensors (layers) of the model across
    GPUs](img/B18493_12_006.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – 张量并行性将模型的张量（层）分割到多个GPU上](img/B18493_12_006.jpg)'
- en: Figure 12.6 – Tensor parallelism shards tensors (layers) of the model across
    GPUs
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – 张量并行性将模型的张量（层）分割到多个GPU上
- en: '**Activation checkpointing**: This helps in reducing memory usage by clearing
    out the activations of layers and computing it again during the backward pass.
    For any deep learning model, the data is first passed through the intermediary
    layers in the forward pass, which compute the outputs, also known as activations.
    These activations need to be stored as they are used for computing gradients during
    the backward pass. Now, storing activations for a large model in memory can increase
    memory usage significantly and can cause bottlenecks. In order to overcome this
    issue, an activation checkpointing or gradient checkpointing technique comes in
    handy, which clears out the activations of intermediary layers.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活检查点**：通过清除层的激活并在反向传播期间重新计算，这有助于减少内存使用。对于任何深度学习模型，数据首先在正向传播中通过中间层，这些层计算输出，也称为激活。这些激活需要被存储，因为它们在反向传播期间用于计算梯度。现在，在内存中存储大型模型的激活可以显著增加内存使用，并可能导致瓶颈。为了克服这个问题，激活检查点或梯度检查点技术就派上用场了，它可以清除中间层的激活。'
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Activation checkpointing results in extra computation time in order to reduce
    memory usage.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 激活检查点会导致额外的计算时间，以减少内存使用。
- en: '**Activation offloading**: This uses activation checkpointing, where it only
    keeps a few activations in the GPU memory during the model training. It offloads
    the checkpointed activations to CPU memory during the forward pass, which are
    loaded back to the GPU during the backward pass of a particular micro-batch of
    data.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活卸载**：这使用激活检查点，在模型训练期间仅在GPU内存中保留少量激活。在正向传递期间，将检查点激活卸载到CPU内存，在特定微批数据的反向传递期间将它们加载回GPU。'
- en: '**Optimizer state sharding**: This is another useful memory-saving technique
    that partitions the state of the optimizers described by the set of weights across
    the data parallel device groups. It can be used only when we are using a stateful
    optimizer, such as Adam or FP16.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器状态分片**：这是另一种有用的内存节省技术，它将描述由权重集定义的优化器状态分割到数据并行设备组中。它只能在使用状态优化器的情况下使用，例如Adam或FP16。'
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since optimizer state sharding partitions the optimizer state across the data
    parallel device groups, it will only become useful if the data parallel degree
    is greater than one.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于优化器状态分片将优化器状态分割到数据并行设备组中，它只有在数据并行度大于一的情况下才会变得有用。
- en: Another important concept to understand with model parallelism is the **micro-batch**,
    which is a smaller subset of the batch of data. During training, you pass a certain
    number of records of data forward and backward through the layers, known as a
    batch or sometimes a mini-batch. A full pass through your dataset is called an
    **epoch**. SageMaker model parallelism shards the batch into smaller subsets,
    which are called micro-batches. These micro-batches are then passed through the
    **pipeline scheduler** to increase GPU utilization. The pipeline scheduler is
    explained in detail in [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116), *Distributed
    Training of Machine* *Learning Models*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型并行中理解的一个重要概念是**微批**，它是数据批次的一个较小子集。在训练过程中，你将一定数量的数据记录正向和反向通过层，称为批次或有时称为小批量。完整遍历你的数据集称为**一个epoch**。SageMaker模型并行将批次分割成更小的子集，这些子集称为微批。然后，这些微批通过**管道调度器**传递以提高GPU利用率。管道调度器在[*第6章*](B18493_06.xhtml#_idTextAnchor116)中详细解释，*机器学习模型的分布式训练*。
- en: 'So, now that we understand the memory-saving techniques used by the SageMaker
    model parallel library let’s see how we can use them in the code. The following
    code snippet wraps all the memory-saving techniques together in a simple manner:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们了解了SageMaker模型并行库使用的内存节省技术，让我们看看如何在代码中使用它们。以下代码片段以简单的方式将所有内存节省技术组合在一起：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: All the memory-saving techniques are highlighted in the code snippet, where
    you first have to make sure to set the `optimize` parameter to `memory`. This
    instructs the SageMaker model splitting algorithm to optimize for memory consumption.
    Once this is set, then you can simply enable other memory-saving features by setting
    their value to `true`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有节省内存的技术都在代码片段中突出显示，你首先必须确保将`optimize`参数设置为`memory`。这指示SageMaker模型拆分算法优化内存消耗。一旦设置完成，你就可以通过将它们的值设置为`true`来简单地启用其他内存节省功能。
- en: 'You will then supply the `distribution` configuration to the `HuggingFace`
    estimator class, as shown in the following code snippet:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将`distribution`配置提供给`HuggingFace`估计器类，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, we also provide the `train.py` training script as `entry_point`
    to the estimator. In [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116), *Distributed
    Training of Machine Learning Models*, we understood that when we are using model
    parallel, we have to update our training script with SageMaker model parallel
    constructs. In this example, since we are using the `HuggingFace` estimator and
    the `Trainer` API for model training, it has built-in support for SageMaker model
    parallel. So, we simply import the Hugging Face `Trainer` API and provide configuration
    related to model training, and based on the model parallel configuration provided
    in the `HuggingFace` estimator, it will invoke the SageMaker model parallel constructs
    during model training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们还提供了`train.py`训练脚本作为估计器的`entry_point`。在[*第6章*](B18493_06.xhtml#_idTextAnchor116)中，*机器学习模型的分布式训练*，我们了解到当我们使用模型并行时，我们必须使用SageMaker模型并行结构更新我们的训练脚本。在这个例子中，因为我们使用`HuggingFace`估计器和`Trainer`
    API进行模型训练，它内置了对SageMaker模型并行的支持。因此，我们只需导入Hugging Face的`Trainer` API并提供与模型训练相关的配置，根据`HuggingFace`估计器中提供的模型并行配置，它将在模型训练期间调用SageMaker模型并行结构。
- en: 'In our `train.py` script, first, we need to import the `Trainer` module, as
    shown in the following code snippet:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since we are training a T5-based BERT model (`prot_t5_xl_uniref50`) we will
    also need to import the `T5Tokenizer` and `T5ForConditionalGeneration` modules
    from the `transformers` library:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to transform the protein sequences and load them into a PyTorch
    DataLoader. After we have the data in `DataLoader`, we will provide `TrainingArguments`,
    as shown in the following code snippet, which will be used by the `Trainer` API:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see, `TrainingArguments` contains a list of hyperparameters, such
    as the number of epochs, learning rate, weight decay, evaluation strategy, and
    so on, which will be used for model training. For details on the different hyperparameters
    for the `TrainingArguments` API, you can refer to this URL: [https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments).
    Make sure that when you are providing `TrainingArguments`, the value of `dataloader_drop_last`
    is set to `true`. This will make sure that the batch size is divisible by the
    micro-batches, and setting `fp16` to `true` will use the automatic mixed precision
    for model training, which also helps reduce the memory footprint, as floating-point
    16 takes 2 bytes to store a parameter.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will define the `Trainer` API, which takes `TrainingArguments` and
    the training and validation datasets as input:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once the `Trainer` API has been defined, the model training is kicked off with
    the `train()` method, and once the training is complete, we will use the `save_model()`
    method to save the trained model to the specified model directory:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `save_model()` API takes the model path as a parameter, coming from the
    `SM_MODEL_DIR` SageMaker container environment variable and parsed as a `model_dir`
    variable. The model artifacts stored in this path will then be copied to the S3
    path specified in the `HuggingFace` estimator, and all the resources, such as
    the training instance, will then be torn down so that users only pay for the duration
    of the training job.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: We are training a very large model, `prot_t5_xl_uniref50`, with 11 billion parameters
    on a very big instance, `ml.g5.48xlarge`, which is an NVIDIA A10G tensor core
    machine with 8 GPUs and 768 GB of GPU memory. Although we are using model parallel,
    the training of the model will take more than 10 hours, and you will incur a cost
    for it. Alternatively, you can use a smaller model, such as `prot_bert_bfd`, which
    has approximately 420 million parameters and is pretrained on protein sequences.
    Since it’s a relatively smaller model that can fit into a single GPU memory, you
    can only use the SageMaker distributed data parallel library, as described in
    [*Chapter 6*](B18493_06.xhtml#_idTextAnchor116)*,* *Distributed Training of Machine*
    *Learning Models*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, you can deploy the model using the SageMaker `HuggingFaceModel`
    class, as explained in the *Applying ML to genomics* main section of this chapter,
    or simply use the `huggingface_estimator.deploy()` API, as shown in the following
    code snippet:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您可以使用SageMaker的`HuggingFaceModel`类来部署模型，如本章“将机器学习应用于基因组学”主部分中所述，或者简单地使用`huggingface_estimator.deploy()`
    API，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the model is deployed, you can use the `predictor` variable to make predictions:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署后，您可以使用`predictor`变量进行预测：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both the discussed deployment options will deploy the model for real-time inference
    as an API on a long-running instance. So, if you are not using the endpoint, make
    sure to delete it; otherwise, you will incur a cost for it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所讨论的部署选项都将模型作为API部署到长期运行的实例上进行实时推理。因此，如果您不使用端点，请确保将其删除；否则，您将为此承担费用。
- en: Now that we understand the applications of ML to genomics, let’s recap what
    we have learned so far in the next section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了机器学习在基因组学中的应用，接下来让我们回顾一下到目前为止所学的知识。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started with understanding the concepts of genomics and
    how you can store and manage large genomics data on AWS. We also discussed the
    end-to-end architecture design for transferring, storing, analyzing, and applying
    ML to genomics data using AWS services. We then focused on how you can deploy
    large state-of-the-art models for genomics, such as DNABERT, for promoter recognition
    tasks using Amazon SageMaker with a few lines of code and how you can test your
    endpoint using code and the SageMaker Studio UI.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先了解了基因组学的概念以及如何在AWS上存储和管理大量基因组数据。我们还讨论了使用AWS服务进行数据传输、存储、分析和将机器学习应用于基因组数据的端到端架构设计。然后，我们关注了如何使用Amazon
    SageMaker和几行代码部署用于基因组学的大规模最先进模型，例如DNABERT，用于启动子识别任务，以及如何使用代码和SageMaker Studio
    UI测试您的端点。
- en: We then moved on to understanding proteomics, which is the study of protein
    sequences, structure, and their functions. We walked through an example of predicting
    protein secondary structure for protein sequences using a Hugging Face pretrained
    model with 11 billion parameters. Since it is a large model with memory requirements
    greater than 220 GB, we explored various memory-saving techniques, such as activation
    checkpointing, activation offloading, optimizer state sharding, and tensor parallelism,
    provided by the SageMaker model parallel library. We then used these techniques
    to train our model for predicting protein structure. We also understood how SageMaker
    provides integration with Hugging Face and makes it simple to use state-of-the-art
    models, which otherwise need a lot of heavy lifting in order to train.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们转向了解蛋白质组学，这是研究蛋白质序列、结构和它们的功能的学科。我们通过一个使用具有110亿参数的Hugging Face预训练模型预测蛋白质二级结构的示例来了解蛋白质组学。由于这是一个内存需求超过220
    GB的大模型，我们探索了SageMaker模型并行库提供的各种内存节省技术，如激活检查点、激活卸载、优化器状态分片和张量并行。然后，我们使用这些技术来训练我们的模型以预测蛋白质结构。我们还了解了SageMaker如何与Hugging
    Face集成，并使其能够轻松使用最先进模型，否则在训练过程中需要做大量的繁重工作。
- en: In the next chapter, we will review another domain, autonomous vehicles, and
    understand how high-performance computing capabilities provided by AWS can be
    used for training and deploying ML models at scale.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾另一个领域，即自动驾驶汽车，并了解AWS提供的高性能计算能力如何用于大规模训练和部署机器学习模型。
