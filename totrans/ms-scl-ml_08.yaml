- en: Chapter 8. Integrating Scala with R and Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章。将 Scala 与 R 和 Python 集成
- en: While Spark provides MLlib as a library for machine learning, in many practical
    situations, R or Python present a more familiar and time-tested interface for
    statistical computations. In particular, R's extensive statistical library includes
    very popular ANOVA/MANOVA methods of analyzing variance and variable dependencies/independencies,
    sets of statistical tests, and random number generators that are not currently
    present in MLlib. The interface from R to Spark is available under SparkR project.
    Finally, data analysts know Python's NumPy and SciPy linear algebra implementations
    for their efficiency as well as other time-series, optimization, and signal processing
    packages. With R/Python integration, all these familiar functionalities can be
    exposed to Scala/Spark users until the Spark/MLlib interfaces stabilize and the
    libraries make their way into the new framework while benefiting the users with
    Spark's ability to execute workflows in a distributed way across multiple machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark 提供了 MLlib 作为机器学习库，但在许多实际情况下，R 或 Python 提供了更熟悉且经过时间考验的统计计算接口。特别是，R 的广泛统计库包括非常流行的
    ANOVA/MANOVA 方法，用于分析方差和变量依赖/独立性，一系列统计测试，以及目前尚未出现在 MLlib 中的随机数生成器。从 R 到 Spark 的接口可在
    SparkR 项目下获得。最后，数据分析师知道 Python 的 NumPy 和 SciPy 线性代数实现，因为它们的效率以及其他时间序列、优化和信号处理包。通过
    R/Python 集成，所有这些熟悉的功能都可以暴露给 Scala/Spark 用户，直到 Spark/MLlib 接口稳定，并且库进入新框架，同时让用户受益于
    Spark 在多台机器上以分布式方式执行工作流的能力。
- en: When people program in R or Python, or with any statistical or linear algebra
    packages for this matter, they are usually not specifically focusing on the functional
    programming aspects. As I mentioned in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, Scala should be treated as a high-level
    language and this is where it shines. Integration with highly efficient C and
    Fortran implementations, for example, of the freely available **Basic Linear Algebra
    Subprograms** (**BLAS**), **Linear Algebra** **Package** (**LAPACK**), and **Arnoldi
    Package** (**ARPACK**), is known to find its way into Java and thus Scala ([http://www.netlib.org](http://www.netlib.org),
    [https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java)).
    I would like to leave Scala at what it's doing best. In this chapter, however,
    I will focus on how to use these languages with Scala/Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们在 R 或 Python 中编程，或者使用任何此类统计或线性代数包时，他们通常不会特别关注函数式编程方面。正如我在[第 1 章](ch01.xhtml
    "第 1 章. 探索性数据分析")中提到的，*探索性数据分析*，Scala 应该被视为一种高级语言，这正是它发光的地方。与高度高效的 C 和 Fortran
    实现集成，例如，免费可用的 **基本线性代数子程序** （**BLAS**），**线性代数** **包** （**LAPACK**），和 **Arnoldi
    包** （**ARPACK**），已知其方式进入 Java 和 Scala ([http://www.netlib.org](http://www.netlib.org)，[https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java))。我希望
    Scala 能够专注于它做得最好的事情。然而，在本章中，我将重点介绍如何使用这些语言与 Scala/Spark 一起使用。
- en: I will use the publicly available United States Department of Transportation
    flights dataset for this chapter ([http://www.transtats.bts.gov](http://www.transtats.bts.gov)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用公开可用的美国交通部航班数据集来完成本章 ([http://www.transtats.bts.gov](http://www.transtats.bts.gov))。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing R and configuring SparkR if you haven't done so yet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有安装，请安装 R 和配置 SparkR
- en: Learning about R (and Spark) DataFrames
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 R（和 Spark）DataFrame
- en: Performing linear regression and ANOVA analysis with R
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 R 进行线性回归和方差分析
- en: Performing **Generalized Linear Model** (**GLM**) modeling with SparkR
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SparkR 进行 **广义线性模型** （**GLM**）建模
- en: Installing Python if you haven't done so yet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有安装，请安装 Python
- en: Learning how to use PySpark and call Python from Scala
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用 PySpark 并从 Scala 调用 Python
- en: Integrating with R
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 R 集成
- en: As with many advanced and carefully designed technologies, people usually either
    love or hate R as a language. One of the reason being that R was one of the first
    language implementations that tries to manipulate complex objects, even though
    most of them turn out to be just a list as opposed to struct or map as in more
    mature modern implementations. R was originally created at the University of Auckland
    by Ross Ihaka and Robert Gentleman around 1993, and had its roots in the S language
    developed at Bell Labs around 1976, when most of the commercial programming was
    still done in Fortran. While R incorporates some functional features such as passing
    functions as a parameter and map/apply, it conspicuously misses some others such
    as lazy evaluation and list comprehensions. With all this said, R has a very good
    help system, and if someone says that they never had to go back to the `help(…)`
    command to figure out how to run a certain data transformation or model better,
    they are either lying or just starting in R.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多高级且精心设计的科技一样，人们对 R 语言通常要么爱得要命，要么恨之入骨。其中一个原因在于，R 是最早尝试操作复杂对象的编程语言实现之一，尽管其中大多数最终只是列表，而不是像更成熟的现代实现中的
    struct 或 map。R 最初由 Ross Ihaka 和 Robert Gentleman 在 1993 年左右在奥克兰大学创建，其根源可以追溯到 1976
    年左右在贝尔实验室开发的 S 语言，当时大多数商业编程仍在 Fortran 中进行。虽然 R 包含了一些功能特性，如将函数作为参数传递和 map/apply，但它明显缺少一些其他特性，如惰性评估和列表推导。尽管如此，R
    有一个非常好的帮助系统，如果有人说他们从未需要回到 `help(…)` 命令来了解如何更好地运行某个数据转换或模型，那么他们要么在撒谎，要么是刚开始使用 R。
- en: Setting up R and SparkR
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 R 和 SparkR
- en: To run SparkR, you'll need R version 3.0 or later. Follow the given instructions
    for the installation, depending on you operating system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 SparkR，您需要 R 版本 3.0 或更高版本。根据您的操作系统，遵循给定的安装说明。
- en: Linux
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux
- en: 'On a Linux system, detailed installation documentation is available at [https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux).
    However, for example, on a Debian system, one installs it by running the following
    command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 系统上，详细的安装文档可在 [https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux)
    找到。然而，例如，在 Debian 系统上，可以通过运行以下命令来安装它：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To list installed/available packages on the Linux repository site, perform
    the following command:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出 Linux 仓库站点上已安装/可用的软件包，请执行以下命令：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'R packages, which are a part of `r-base` and `r-recommended`, are installed
    into the `/usr/lib/R/library` directory. These can be updated using the usual
    package maintenance tools such as `apt-get` or aptitude. The other R packages
    available as precompiled Debian packages, `r-cran-*` and `r-bioc-*`, are installed
    into `/usr/lib/R/site-library`. The following command shows all packages that
    depend on `r-base-core`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: R 软件包，作为 `r-base` 和 `r-recommended` 的一部分，被安装到 `/usr/lib/R/library` 目录。这些可以通过常用的软件包维护工具如
    `apt-get` 或 aptitude 进行更新。其他作为预编译的 Debian 软件包提供的 R 软件包，如 `r-cran-*` 和 `r-bioc-*`，被安装到
    `/usr/lib/R/site-library`。以下命令显示了所有依赖于 `r-base-core` 的软件包：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This comprises of a large number of contributed packages from CRAN and other
    repositories. If you want to install R packages that are not provided as package,
    or if you want to use newer versions, you need to build them from source that
    requires the `r-base-dev` development package that can be installed by the following
    command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括来自 CRAN 和其他仓库的大量贡献软件包。如果您想安装未作为软件包提供的 R 软件包，或者如果您想使用较新版本，则需要从源代码构建它们，这需要安装
    `r-base-dev` 开发软件包，可以通过以下命令安装：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This pulls in the basic requirements to compile R packages, such as the development
    tools group install. R packages may then be installed by the local user/admin
    from the CRAN source packages, typically from inside R using the `R> install.packages()`
    function or `R CMD INSTALL`. For example, to install the R `ggplot2` package,
    run the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引入编译 R 软件包的基本需求，例如开发工具组的安装。然后，本地用户/管理员可以从 CRAN 源软件包安装 R 软件包，通常在 R 内部使用 `R>
    install.packages()` 函数或 `R CMD INSTALL`。例如，要安装 R 的 `ggplot2` 软件包，请运行以下命令：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will download and optionally compile the package and its dependencies
    from one of the available sites. Sometime R is confused about the repositories;
    in this case, I recommend creating a `~/.Rprofile` file in the home directory
    pointing to the closest CRAN repository:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从可用的站点之一下载并可选地编译软件包及其依赖项。有时 R 会混淆仓库；在这种情况下，我建议在主目录中创建一个 `~/.Rprofile` 文件，指向最近的
    CRAN 仓库：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`~/.Rprofile` contains commands to customize your sessions. One of the commands
    I recommend to put in there is `options (prompt="R> ")` to be able to distinguish
    the shell you are working in by the prompt, following the tradition of most tools
    in this book. The list of known mirrors is available at [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`~/.Rprofile` 包含用于自定义会话的命令。我推荐放入那里的一个命令是 `options (prompt="R> ")`，以便能够通过提示符区分您正在使用的
    shell，遵循本书中大多数工具的传统。已知镜像列表可在 [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html)
    查找。'
- en: 'Also, it is good practice to specify the directory to install `system/site/user`
    packages via the following command, unless your OS setup does it already by putting
    these commands into `~/.bashrc` or system `/etc/profile`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，指定安装 `system/site/user` 软件包的目录也是一个好的做法，除非您的操作系统设置已经通过将以下命令放入 `~/.bashrc`
    或系统 `/etc/profile` 来完成：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Mac OS
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mac OS
- en: 'R for Mac OS can be downloaded, for example, from [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx).
    The latest version at the time of the writing is 3.2.3\. Always check the consistency
    of the downloaded package. To do so, run the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: R for Mac OS 可以从 [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx)
    下载。写作时的最新版本是 3.2.3。始终检查下载的软件包的一致性。为此，请运行以下命令：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The environment settings in the preceding subsection also apply to the Mac OS
    setup.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节中描述的环境设置也适用于 Mac OS 设置。
- en: Windows
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Windows
- en: R for Windows can be downloaded from [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    as an exe installer. Run this executable as an administrator to install R.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: R for Windows 可以从 [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    下载为 exe 安装程序。以管理员身份运行此可执行文件来安装 R。
- en: One can usually edit the environment setting for **System/User** by following
    the **Control Panel** | **System and Security** | **System** | **Advanced system
    settings** | **Environment Variables** path from the Windows menu.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过遵循 Windows 菜单中的 **控制面板** | **系统和安全** | **系统** | **高级系统设置** | **环境变量** 路径来编辑
    **系统/用户** 的环境设置。
- en: Running SparkR via scripts
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过脚本运行 SparkR
- en: 'To run SparkR, one needs to run install the `R/install-dev.sh` script that
    comes with the Spark git tree. In fact, one only needs the shell script and the
    content of the `R/pkg` directory, which is not always included with the compiled
    Spark distributions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 SparkR，需要运行 Spark git 树中包含的 `R/install-dev.sh` 脚本。实际上，只需要 shell 脚本和 `R/pkg`
    目录的内容，这些内容并不总是包含在编译好的 Spark 发行版中：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running Spark via R's command line
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 R 的命令行运行 Spark
- en: 'Alternatively, we can also initialize Spark from the R command line directly
    (or from RStudio at [http://rstudio.org/](http://rstudio.org/)) using the following
    commands:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以直接从 R 命令行（或从 RStudio [http://rstudio.org/](http://rstudio.org/)）使用以下命令初始化
    Spark：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As described previously in [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib*, the `SPARK_HOME` environment variable
    needs to point to your local Spark installation directory and `SPARK_MASTER` and
    `YARN_CONF_DIR` to the desired cluster manager (local, standalone, mesos, and
    YARN) and YARN configuration directory if one is using Spark with the YARN cluster
    manager.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在 [第 3 章](ch03.xhtml "第 3 章。使用 Spark 和 MLlib") 中，*使用 Spark 和 MLlib*，`SPARK_HOME`
    环境变量需要指向您的本地 Spark 安装目录，`SPARK_MASTER` 和 `YARN_CONF_DIR` 指向所需的集群管理器（本地、独立、Mesos
    和 YARN）以及 YARN 配置目录，如果使用 Spark 与 YARN 集群管理器一起使用的话。
- en: Although most all of the distributions come with a UI, in the tradition of this
    book and for the purpose of this chapter I'll use the command line.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数发行版都附带了一个用户界面，但根据本书的传统和本章的目的，我将使用命令行。
- en: DataFrames
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrames
- en: The DataFrames originally came from R and Python, so it is natural to see them
    in SparkR.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 最初来自 R 和 Python，因此在 SparkR 中看到它们是自然的。
- en: Note
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that the implementation of DataFrames in SparkR is on top of RDDs,
    so they work differently than the R DataFrames.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SparkR 中 DataFrames 的实现是在 RDDs 之上，因此它们的工作方式与 R DataFrames 不同。
- en: The question of when and where to store and apply the schema and other metadata
    like types has been a topic of active debate recently. On one hand, providing
    the schema early with the data enables thorough data validation and potentially
    optimization. On the other hand, it may be too restrictive for the original data
    ingest, whose goal is just to capture as much data as possible and perform data
    formatting/cleansing later on, the approach often referred as schema on read.
    The latter approach recently won more ground with the tools to work with evolving
    schemas such as Avro and automatic schema discovery tools, but for the purpose
    of this chapter, I'll assume that we have done the schema discovery part and can
    start working with a DataFrames.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，关于何时何地存储和应用模式以及其他元数据（如类型）的问题一直是活跃讨论的主题。一方面，在数据早期提供模式可以实现对数据的彻底验证和潜在的优化。另一方面，对于原始数据摄取来说可能过于限制，其目标只是尽可能多地捕获数据，并在之后进行数据格式化/清洗，这种方法通常被称为“读取时模式”。后者最近在处理如Avro等演变模式的工具和自动模式发现工具的支持下赢得了更多支持，但为了本章的目的，我将假设我们已经完成了模式发现部分，并可以开始使用DataFrames。
- en: 'Let''s first download and extract a flight delay dataset from the United States
    Department of Transportation, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从美国运输部下载并提取一个航班延误数据集，如下所示：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you have Spark running on the cluster, you want to copy the file in HDFS:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个集群上运行Spark，你想要将文件复制到HDFS：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `flights/readme.html` files gives you detailed metadata information, as
    shown in the following image:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`flights/readme.html`文件提供了详细的元数据信息，如下面的图像所示：'
- en: '![DataFrames](img/B04935_08_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![DataFrames](img/B04935_08_01.jpg)'
- en: 'Figure 08-1: Metadata provided with the On-Time Performance dataset released
    by the US Department of Transportation (for demo purposes only)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图08-1：美国运输部发布的准时性能数据集提供的元数据（仅用于演示目的）
- en: 'Now, I want you to analyze the delays of `SFO` returning flights and possibly
    find the factors contributing to the delay. Let''s start with the R `data.frame`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想让你分析`SFO`返回航班的延误，并可能找出导致延误的因素。让我们从R的`data.frame`开始：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you were flying from Salt Lake City on Sunday with Alaska Airlines in July
    2015, consider yourself unlucky (we have only done simple analysis so far, so
    one shouldn't attach too much significance to this result). There may be multiple
    other random factors contributing to the delay.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是在2015年7月的星期日乘坐阿拉斯加航空从盐湖城起飞，那么你觉得自己很不幸（我们到目前为止只进行了简单的分析，因此不应该过分重视这个结果）。可能还有其他多个随机因素导致了延误。
- en: 'Even though we ran the example in SparkR, we still used the R `data.frame`.
    If we want to analyze data across multiple months, we will need to distribute
    the load across multiple nodes. This is where the SparkR distributed DataFrame
    comes into play, as it can be distributed across multiple threads even on a single
    node. There is a direct way to convert the R DataFrame to SparkR DataFrame (and
    thus to RDD):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在SparkR中运行了示例，但我们仍然使用了R的`data.frame`。如果我们想要分析跨多个月份的数据，我们需要在多个节点上分配负载。这就是SparkR分布式DataFrame发挥作用的地方，因为它甚至可以在单个节点上跨多个线程进行分布。有一个直接的方法可以将R
    DataFrame转换为SparkR DataFrame（以及RDD）：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If I run it on a laptop, I will run out of memory. The overhead is large due
    to the fact that I need to transfer the data between multiple threads/nodes, we
    want to filter it as soon as possible:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在笔记本电脑上运行它，我会耗尽内存。由于我需要在多个线程/节点之间传输数据，开销很大，我们希望尽快过滤它：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will run even on my laptop. There is, of course, a reverse conversion
    from Spark''s DataFrame to R''s `data.frame`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这甚至可以在我的笔记本电脑上运行。当然，从Spark的DataFrame到R的`data.frame`有一个反向转换：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Alternatively, I can use the `spark-csv` package to read it from the `.csv`
    file, which, if the original `.csv` file is in a distributed filesystem such as
    HDFS, will avoid shuffling the data over network in a cluster setting. The only
    drawback, at least currently, is that Spark cannot read from the `.zip` files
    directly:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我可以使用`spark-csv`包从`.csv`文件中读取它，如果原始`.csv`文件位于HDFS等分布式文件系统上，这将避免在集群设置中通过网络在集群中重新洗牌数据。但至少目前，Spark不能直接从`.zip`文件中读取：
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that we loaded the additional `com.databricks:spark-csv_2.10:1.3.0` package
    by supplying the `--package` flag on the command line; we can easily go distributed
    by using a Spark instance over a cluster of nodes or even analyze a larger dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通过在命令行上提供`--package`标志来加载了额外的`com.databricks:spark-csv_2.10:1.3.0`包；我们可以通过在节点集群上使用Spark实例轻松地实现分布式，甚至可以分析更大的数据集：
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will download and put the on-time performance data in the flight''s directory
    (remember, as we discussed in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory Data
    Analysis"), *Exploratory Data Analysis*, we would like to treat directories as
    big data datasets). We can now run the same analysis over the whole period of
    2015 (for the available data):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载并将准点性能数据放在航班的目录中（记住，正如我们在[第1章](ch01.xhtml "第1章. 探索性数据分析")中讨论的，*探索性数据分析*，我们希望将目录视为大数据数据集）。现在我们可以对2015年整个期间的相同分析进行操作（对于可用的数据）：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that we used a `cache()` call to pin the dataset to the memory as we will
    use it again later. This time it''s Minneapolis/United on Saturday! However, you
    probably already know why: there is only one record for this combination of `DayOfWeek`,
    `Origin`, and `UniqueCarrier`; it''s most likely an outlier. The average over
    about `30` flights for the previous outlier was reduced to `30` minutes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了`cache()`调用将数据集固定在内存中，因为我们稍后会再次使用它。这次是周六的明尼阿波利斯/联合！然而，你可能已经知道原因：对于这种`DayOfWeek`、`Origin`和`UniqueCarrier`组合，只有一个记录；这很可能是异常值。关于前一个异常值的平均大约`30`个航班现在减少到`30`分钟：
- en: '[PRE19]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Sunday still remains a problem in terms of delays. The limit to the amount of
    data we can analyze now is only the number of cores on the laptop and nodes in
    the cluster. Let's look at more complex machine learning models now.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 周日仍然是延误问题。我们现在可以分析的数据量的限制现在只是笔记本电脑上的核心数和集群中的节点数。现在让我们看看更复杂的机器学习模型。
- en: Linear models
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'Linear methods play an important role in statistical modeling. As the name
    suggests, linear model assumes that the dependent variable is a weighted combination
    of independent variables. In R, the `lm` function performs a linear regression
    and reports the coefficients, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 线性方法在统计建模中扮演着重要的角色。正如其名所示，线性模型假设因变量是独立变量的加权组合。在R中，`lm`函数执行线性回归并报告系数，如下所示：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `summary` function provides even more information:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary`函数提供了更多信息：'
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: While we considered generalized linear models in [Chapter 3](ch03.xhtml "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, and we will also consider
    the `glm` implementation in R and SparkR shortly, linear models provide more information
    in general and are an excellent tool for working with noisy data and selecting
    the relevant attribute for further analysis.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[第3章](ch03.xhtml "第3章. 使用Spark和MLlib")中考虑了广义线性模型，*使用Spark和MLlib*，我们也将很快考虑R和SparkR中的`glm`实现，但线性模型通常提供更多信息，并且是处理噪声数据和选择相关属性进行进一步分析的优秀工具。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Data analysis life cycle**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析生命周期**'
- en: While most of the statistical books focus on the analysis and best use of available
    data, the results of statistical analysis in general should also affect the search
    for the new sources of information. In the complete data life cycle, discussed
    at the end of [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark and MLlib"),
    *Working with Spark and MLlib*, a data scientist should always transform the latest
    variable importance results into the theories of how to collect data. For example,
    if the ink usage analysis for home printers points to an increase in ink usage
    for photos, one could potentially collect more information about the format of
    the pictures, sources of digital images, and paper the user prefers to use. This
    approach turned out to be very productive in a real business situation even though
    not fully automated.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数统计书籍都侧重于分析和最佳使用现有数据，但统计分析的一般结果也应该影响对新信息来源的搜索。在完整的[第3章](ch03.xhtml "第3章.
    使用Spark和MLlib")中讨论的数据生命周期结束时，*使用Spark和MLlib*，数据科学家应始终将最新的变量重要性结果转化为如何收集数据的理论。例如，如果家用打印机的墨水使用分析表明照片墨水使用量增加，那么可以收集更多关于图片格式、数字图像来源以及用户偏好的纸张类型的信息。这种方法在实际商业环境中证明非常有效，尽管并非完全自动化。
- en: 'Specifically, here is a short description of the output that linear models
    provide:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，以下是线性模型提供的输出简要描述：
- en: '**Residuals**: These are statistics for the difference between the actual and
    predicted values. A lot of techniques exist to detect the problems with the models
    on patterns of the residual distribution, but this is out of scope of this book.
    A detailed residual table can be obtained with the `resid(model)` function.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差**：这些是实际值与预测值之间差异的统计数据。存在许多技术可以检测残差分布模式中的模型问题，但这超出了本书的范围。可以使用`resid(model)`函数获得详细的残差表。'
- en: '**Coefficients**: These are the actual linear combination coefficients; the
    t-value represents the ratio of the value of the coefficient to the estimate of
    the standard error: higher values mean a higher likelihood that this coefficient
    has a non-trivial effect on the dependent variable. The coefficients can also
    be obtained with `coef(model)` functions.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系数**: 这些是实际的线性组合系数；t值表示系数值与标准误差估计的比率：高值意味着这个系数对因变量的非平凡影响的可能性更高。系数也可以通过`coef(model)`函数获得。'
- en: '**Residual standard error**: This reports the standard mean square error, the
    metric that is the target of optimization in a straightforward linear regression.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差标准误差**: 这报告了标准均方误差，这是简单线性回归中优化的目标。'
- en: '**Multiple R-squared**: This is the fraction of the dependent variable variance
    that is explained by the model. The adjusted value accounts for the number of
    parameters in your model and is considered to be a better metric to avoid overfitting
    if the number of observations does not justify the complexity of the models, which
    happens even for big data problems.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重R平方**: 这表示模型解释的因变量方差的比例。调整后的值考虑了模型中的参数数量，被认为是避免过拟合的更好指标，尤其是在观测数量不足以证明模型复杂性的情况下，即使在大数据问题中也是如此。'
- en: '**F-statistic**: The measure of model quality. In plain terms, it measures
    how all the parameters in the model explain the dependent variable. The p-value
    provides the probability that the model explains the dependent variable just due
    to random chance. The values under 0.05 (or 5%) are, in general, considered satisfactory.
    While in general, a high value probably means that the model is probably not statistically
    valid and "nothing else matters, the low F-statistic does not always mean that
    the model will work well in practice, so it cannot be directly applied as a model
    acceptance criterion.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量**: 模型质量的衡量标准。简单来说，它衡量模型中所有参数解释因变量的程度。p值提供了模型仅因随机机会解释因变量的概率。一般来说，低于0.05（或5%）的值被认为是令人满意的。虽然一般来说，高值可能意味着模型可能没有统计上的有效性，“其他因素都不重要”，低F统计量并不总是意味着模型在实际中会表现良好，因此不能直接作为模型接受标准。'
- en: Once the linear models are applied, usually more complex `glm` or recursive
    models, such as decision trees and the `rpart` function, are applied to find interesting
    variable interactions. Linear models are good for establishing baseline on the
    other models that can improve.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用线性模型之后，通常还会应用更复杂的`glm`或递归模型，如决策树和`rpart`函数，以寻找有趣的变量交互。线性模型适用于为其他可以改进的模型建立基线。
- en: 'Finally, ANOVA is a standard technique to study the variance if the independent
    variables are discrete:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果自变量是离散的，方差分析（ANOVA）是研究方差的标准技术：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The measure of the model quality is F-statistics. While one can always run R
    algorithms with RDD using the pipe mechanism with `Rscript`, I will partially
    cover this functionality with respect to **Java Specification Request** (**JSR**)
    223 Python integration later. In this section, I would like to explore specifically
    a generalized linear regression `glm` function that is implemented both in R and
    SparkR natively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量的衡量标准是F统计量。虽然可以使用`Rscript`的管道机制通过RDD运行R算法，但我将在稍后部分部分介绍与**Java规范请求**（**JSR**）223
    Python集成相关的功能。在本节中，我想特别探讨在R和SparkR中本地实现的广义线性回归`glm`函数。
- en: Generalized linear model
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'Once again, you can run either R `glm` or SparkR `glm`. The list of possible
    link and optimization functions for R implementation is provided in the following
    table:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以运行R `glm`或SparkR `glm`。以下表格提供了R实现可能的链接和优化函数列表：
- en: 'The following list shows possible options for R `glm` implementation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了R `glm`实现的可能选项：
- en: '| Family | Variance | Link |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Family | Variance | Link |'
- en: '| --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| gaussian | gaussian | identity |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| gaussian | gaussian | identity |'
- en: '| binomial | binomial | logit, probit or cloglog |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| binomial | binomial | logit, probit 或 cloglog |'
- en: '| poisson | poisson | log, identity or sqrt |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| poisson | poisson | log, identity 或 sqrt |'
- en: '| Gamma | Gamma | inverse, identity or log |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Gamma | Gamma | inverse, identity 或 log |'
- en: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
- en: '| quasi | user-defined | user-defined |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| quasi | 用户定义 | 用户定义 |'
- en: 'I will use a binary target, `ArrDel15`, which indicates whether the plane was
    more than 15 minutes late for the arrival. The independent variables will be `DepDel15`,
    `DayOfWeek`, `Month`, `UniqueCarrier`, `Origin`, and `Dest`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用二进制目标 `ArrDel15`，表示飞机是否晚于 15 分钟到达。自变量将是 `DepDel15`、`DayOfWeek`、`Month`、`UniqueCarrier`、`Origin`
    和 `Dest`：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'While you wait for the results, open another shell and run `glm` in the `SparkR`
    mode on the full seven months of data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当你等待结果时，打开另一个 shell 并在 `SparkR` 模式下对完整七个月的数据运行 `glm`：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here we try to build a model explaining delays as an effect of carrier, day
    of week, and origin on destination airports, which is captured by the formular
    construct `ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试构建一个模型，解释延误作为承运人、星期几和出发机场到目的机场的影响，这由公式构造 `ArrDel15 ~ UniqueCarrier + DayOfWeek
    + Origin + Dest` 捕获。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Nulls, big data, and Scala**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**空值、大数据和 Scala**'
- en: Note that in the SparkR case of `glm`, I had to explicitly filter out the non-cancelled
    flights and removed the NA—or nulls in the C/Java lingo. While R does this for
    you by default, NAs in big data are very common as the datasets are typically
    sparse and shouldn't be treated lightly. The fact that we have to deal with nulls
    explicitly in MLlib warns us about some additional information in the dataset
    and is definitely a welcome feature. The presence of an NA can carry information
    about the way the data was collected. Ideally, each NA should be accompanied by
    a small `get_na_info` method as to why this particular value was not available
    or collected, which leads us to the `Either` type in Scala.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 SparkR 的 `glm` 情况中，我必须明确过滤掉未取消的航班并移除 NA 或者在 C/Java 术语中的空值。虽然 R 默认会为你做这件事，但大数据中的
    NA 非常常见，因为数据集通常是稀疏的，不应轻视。我们必须在 MLlib 中显式处理空值的事实提醒我们数据集中的一些附加信息，这绝对是一个受欢迎的功能。NA
    的存在可以携带有关数据收集方式的信息。理想情况下，每个 NA 都应该有一个小的 `get_na_info` 方法来说明为什么这个特定的值不可用或未收集，这使我们想到了
    Scala 中的 `Either` 类型。
- en: Even though nulls are inherited from Java and a part of Scala, the `Option`
    and `Either` types are new and more robust mechanism to deal with special cases
    where nulls were traditionally used. Specifically, `Either` can provide a value
    or exception message as to why it was not computed; while `Option` can either
    provide a value or be `None`, which can be readily captured by the Scala pattern-matching
    framework.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 即使空值是从 Java 继承并成为 Scala 的一部分，但 `Option` 和 `Either` 类型是新的、更健壮的机制，用于处理传统上使用空值时的特殊情况。具体来说，`Either`
    可以提供一个值或异常信息，说明为什么没有进行计算；而 `Option` 可以提供一个值或为 `None`，这可以很容易地被 Scala 的模式匹配框架捕获。
- en: 'One thing you will notice is that SparkR will run multiple threads, and even
    on a single node, it will consume CPU time from multiple cores and returns much
    faster even with a larger size of data. In my experiment on a 32-core machine,
    it was able to finish in under a minute (as opposed to 35 minutes for R `glm`).
    To get the results, as in the R model case, we need to run the `summary()` method:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 SparkR 会运行多个线程，甚至在单个节点上，它也会从多个核心消耗 CPU 时间，并且即使数据量较大，返回速度也很快。在我的 32 核机器实验中，它能在不到一分钟内完成（相比之下，R
    的 `glm` 需要 35 分钟）。为了得到结果，就像 R 模型的情况一样，我们需要运行 `summary()` 方法：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The worst performer is `NK` (Spirit Airlines). Internally, SparkR uses limited-memory
    BFGS, which is a limited-memory quasi-Newton optimization method that is similar
    to the results obtained with R `glm` on the July data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最差的是 `NK`（精神航空）。内部，SparkR 使用有限的内存 BFGS，这是一种类似于 R `glm` 在 7 月数据上获得结果的有限内存拟牛顿优化方法：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Other parameters of SparkR `glm` implementation are provided in the following
    table:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR `glm` 实现的其他参数如下表所示：
- en: 'The following table shows a list of parameters for SparkR `glm` implementation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了 SparkR `glm` 实现的参数列表：
- en: '| Parameter | Possible Values | Comments |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 可能的值 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `formula` | A symbolic description like in R | Currently only a subset of
    formula operators are supported: ''`~`'', ''`.`'', ''`:`'', ''`+`'', and ''`-`''
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `formula` | 类似于 R 的符号描述 | 目前仅支持公式运算符的子集："`~`"，"`.`"，"`:`"，"`+`"，和 "`-`" |'
- en: '| `family` | gaussian or binomial | Needs to be in quotes: gaussian -> linear
    regression, binomial -> logistic regression |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `family` | 高斯或二项式 | 需要加引号：高斯 -> 线性回归，二项式 -> 逻辑回归 |'
- en: '| `data` | DataFrame | Needs to be SparkR DataFrame, not `data.frame` |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `data` | DataFrame | 需要是 SparkR DataFrame，而不是 `data.frame` |'
- en: '| `lambda` | positive | Regularization coefficient |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `lambda` | 正数 | 正则化系数 |'
- en: '| `alpha` | positive | Elastic-net mixing parameter (refer to glmnet''s documentation
    for details) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `alpha` | 正数 | 弹性网络混合参数（有关详细信息，请参阅glmnet的文档） |'
- en: '| `standardize` | TRUE or FALSE | User-defined |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `standardize` | TRUE或FALSE | 用户定义 |'
- en: '| `solver` | l-bfgs, normal or auto | auto will choose the algorithm automatically,
    l-bfgs means limited-memory BFGS, normal means using normal equation as an analytical
    solution to the linear regression problem |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `solver` | l-bfgs、正常或自动 | 自动将自动选择算法，l-bfgs表示有限内存BFGS，正常表示使用正规方程作为线性回归问题的解析解
    |'
- en: Reading JSON files in SparkR
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SparkR中读取JSON文件
- en: 'Schema on Read is one of the convenient features of big data. The DataFrame
    class has the ability to figure out the schema of a text file containing a JSON
    record per line:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 读取时模式是大数据的一个便利特性。DataFrame类能够确定每行包含一个JSON记录的文本文件的模式：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Writing Parquet files in SparkR
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SparkR中写入Parquet文件
- en: 'As we mentioned in the previous chapter, the Parquet format is an efficient
    storage format, particularly for low cardinality columns. Parquet files can be
    read/written directly from R:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中提到的，Parquet格式是一种高效的存储格式，尤其是对于低基数列。Parquet文件可以直接从R中读取/写入：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can see that the new Parquet file is 66 times smaller that the original
    zip file downloaded from the DoT:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，新的Parquet文件比从DoT下载的原始zip文件小66倍：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Invoking Scala from R
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从R调用Scala
- en: Let's assume that one has an exceptional implementation of a numeric method
    in Scala that we want to call from R. One way of doing this would be to use the
    R `system()` function that invokes `/bin/sh` on Unix-like systems. However, the
    `rscala` package is a more efficient way that starts a Scala interpreter and maintains
    communication over TCP/IP network connection.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个在Scala中实现的数值方法的卓越实现，我们希望从R中调用它。一种方法是通过R的`system()`函数调用Unix-like系统上的`/bin/sh`。然而，`rscala`包是一种更有效的方法，它启动一个Scala解释器，并通过TCP/IP网络连接维护通信。
- en: 'Here, the Scala interpreter maintains the state (memoization) between the calls.
    Similarly, one can define functions, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Scala解释器在调用之间维护状态（记忆化）。同样，可以定义函数，如下所示：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'R from Scala can be invoked using the `!` or `!!` Scala operators and `Rscript`
    command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`!`或`!!` Scala运算符和`Rscript`命令可以从Scala调用R：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using Rserve
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Rserve
- en: 'A more efficient way is to use the similar TCP/IP binary transport protocol
    to communicate with R with `Rsclient/Rserve` ([http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)).
    To start `Rserve` on a node that has R installed, perform the following action:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更有效的方法是使用类似的TCP/IP二进制传输协议，通过`Rsclient/Rserve`与R进行通信（[http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)）。要在已安装R的节点上启动`Rserve`，请执行以下操作：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: By default, `Rserv` opens a connection on `localhost:6311`. The advantage of
    the binary network protocol is that it is platform-independent and multiple clients
    can communicate with the server. The clients can connect to `Rserve`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Rserv`在`localhost:6311`上打开一个连接。二进制网络协议的优点是它是平台无关的，多个客户端可以与服务器通信。客户端可以连接到`Rserve`。
- en: Note that, while passing the results as a binary object has its advantages,
    you have to be careful with the type mappings between R and Scala. `Rserve` supports
    other clients, including Python, but I will also cover JSR 223-compliant scripting
    at the end of this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然将结果作为二进制对象传递有其优点，但你必须小心R和Scala之间的类型映射。`Rserve`支持其他客户端，包括Python，但我还会在本章末尾介绍JSR
    223兼容的脚本。
- en: Integrating with Python
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成Python
- en: Python has slowly established ground as a de-facto tool for data science. It
    has a command-line interface and decent visualization via matplotlib and ggplot,
    which is based on R's ggplot2\. Recently, Wes McKinney, the creator of Pandas,
    the time series data-analysis package, has joined Cloudera to pave way for Python
    in big data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Python已经逐渐成为数据科学的事实上的工具。它有一个命令行界面，以及通过matplotlib和ggplot（基于R的ggplot2）的不错的可视化。最近，Pandas时间序列数据分析包的创造者Wes
    McKinney加入了Cloudera，为Python在大数据中的应用铺平道路。
- en: Setting up Python
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Python
- en: Python is usually part of the default installation. Spark requires version 2.7.0+.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Python通常是默认安装的一部分。Spark需要版本2.7.0+。
- en: 'If you don''t have Python on Mac OS, I recommend installing the Homebrew package
    manager from [http://brew.sh](http://brew.sh):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在Mac OS上安装Python，我建议安装Homebrew包管理器，请访问[http://brew.sh](http://brew.sh)：
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Otherwise, on a Unix-like system, Python can be compiled from the source distribution:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，在Unix-like系统上，Python可以从源分布编译：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It is good practice to place it in a directory different from the default Python
    installation. It is normal to have multiple versions of Python on a single system,
    which usually does not lead to problems as Python separates the installation directories.
    For the purpose of this chapter, as for many machine learning takes, I''ll also
    need a few packages. The packages and specific versions may differ across installations:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好的做法是将它放在与默认Python安装不同的目录中。在单个系统上拥有多个Python版本是正常的，通常不会导致问题，因为Python会分离安装目录。为了本章的目的，就像许多机器学习任务一样，我还需要一些包。包和特定版本可能因安装而异：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If everything compiles—SciPy uses a Fortran compiler and libraries for linear
    algebra—we are ready to use Python 2.7.11!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切编译成功——SciPy使用Fortran编译器和库进行线性代数——我们就可以使用Python 2.7.11了！
- en: Note
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that if one wants to use Python with the `pipe` command in a distributed
    environment, Python needs to be installed on every node in the network.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果想在分布式环境中使用Python的`pipe`命令，Python需要安装在网络中的每个节点上。
- en: PySpark
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark
- en: 'As `bin/sparkR` launches R with preloaded Spark context, `bin/pyspark` launches
    Python shell with preloaded Spark context and Spark driver running. The `PYSPARK_PYTHON`
    environment variable can be used to point to a specific Python version:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`bin/sparkR`启动带有预加载Spark上下文的R，`bin/pyspark`启动带有预加载Spark上下文和Spark驱动程序的Python
    shell。可以使用`PYSPARK_PYTHON`环境变量指向特定的Python版本：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'PySpark directly supports most of MLlib functionality on Spark RDDs ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)),
    but it is known to lag a few releases behind the Scala API ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)).
    As of the 1.6.0+ release, it also supports DataFrames ([http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark直接支持Spark RDDs上大多数MLlib功能([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python))，但它已知落后Scala
    API([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python))几个版本。截至1.6.0+版本，它还支持DataFrames([http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html))：
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Calling Python from Java/Scala
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Java/Scala调用Python
- en: As this is really a book about Scala, we should also mention that one can call
    Python code and its interpreter directly from Scala (or Java). There are a few
    options available that will be discussed in this chapter.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书实际上是关于Scala的，我们也应该提到，可以直接从Scala（或Java）调用Python代码及其解释器。本章将讨论一些可用的选项。
- en: Using sys.process._
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用sys.process._
- en: 'Scala, as well as Java, can call OS processes via spawning a separate thread,
    which we already used for interactive analysis in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*: the `.!` method will start the process
    and return the exit code, while `.!!` will return the string that contains the
    output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Scala和Java一样，可以通过启动一个单独的线程来调用操作系统进程，这在[第1章](ch01.xhtml "第1章. 探索性数据分析")中我们已经用于交互式分析，*探索性数据分析*：`.!`方法将启动进程并返回退出代码，而`.!!`将返回包含输出的字符串：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s try a more complex SVD computation (similar to the one we used in SVD++
    recommendation engine, but this time, it invokes BLAS C-libraries at the backend).
    I created a Python executable that takes a string representing a matrix and the
    required rank as an input and outputs an SVD approximation with the provided rank:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更复杂的SVD计算（类似于我们在SVD++推荐引擎中使用的，但这次，它在后端调用BLAS C库）。我创建了一个Python可执行文件，它接受一个表示矩阵的字符串和所需的秩作为输入，并输出具有提供秩的SVD近似：
- en: '[PRE39]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s call it `svd.py` and put in in the current directory. Given a matrix
    and rank as an input, it produces an approximation of a given rank:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其命名为`svd.py`并将其放在当前目录中。给定一个矩阵和秩作为输入，它产生给定秩的近似：
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To call it from Scala, let''s define the following `#<<<` method in our DSL:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Scala调用它，让我们在我们的DSL中定义以下`#<<<`方法：
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we can use the `#<<<` operator to call Python''s SVD method:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`#<<<`运算符来调用Python的SVD方法：
- en: '[PRE42]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Note that as we requested the resulting matrix rank to be one, all rows and
    columns are linearly dependent. We can even pass several lines of input at a time,
    as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们要求结果的矩阵秩为1，所有行和列都是线性相关的。我们甚至可以一次传递多行输入，如下所示：
- en: '[PRE43]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Spark pipe
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark pipe
- en: 'SVD decomposition is usually a pretty heavy operation, so the relative overhead
    of calling Python in this case is small. We can avoid this overhead if we keep
    the process running and supply several lines at a time, like we did in the last
    example. Both Hadoop MR and Spark implement this approach. For example, in Spark,
    the whole computation will take only one line, as shown in the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 分解通常是一个相当耗时的操作，因此在这种情况下调用 Python 的相对开销很小。如果我们保持进程运行并一次提供几行，就可以避免这种开销，就像我们在上一个例子中所做的那样。Hadoop
    MR 和 Spark 都实现了这种方法。例如，在 Spark 中，整个计算只需要一行，如下所示：
- en: '[PRE44]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The whole pipeline is ready to be distributed across a cluster of multicore
    workstations! I think you will be in love with Scala/Spark already.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道已准备好在多核工作站集群中分发！我想你已经爱上 Scala/Spark 了。
- en: Note that debugging the pipelined executions might be tricky as the data is
    passed from one process to another using OS pipes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调试管道执行可能会很棘手，因为数据是通过 OS 管道从一个进程传递到另一个进程的。
- en: Jython and JSR 223
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jython 和 JSR 223
- en: For completeness, we need to mention Jython, a Java implementation of Python
    (as opposed to a more familiar C implementation, also called CPython). Jython
    avoids the problem of passing input/output via OS pipelines by allowing the users
    to compile Python source code to Java byte codes, and running the resulting bytecodes
    on any Java virtual machine. As Scala also runs in Java virtual machine, it can
    use the Jython classes directly, although the reverse is not true in general;
    Scala classes sometimes are not compatible to be used by Java/Jython.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们需要提及 Jython，它是 Python 的 Java 实现（与更熟悉的 C 实现，也称为 CPython 相比）。Jython 通过允许用户将
    Python 源代码编译成 Java 字节码，并在任何 Java 虚拟机上运行这些字节码，避免了通过 OS 管道传递输入/输出的问题。由于 Scala 也在
    Java 虚拟机上运行，它可以直接使用 Jython 类，尽管通常情况下反之不成立；Scala 类有时与 Java/Jython 不兼容。
- en: Note
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**JSR 223**'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**JSR 223**'
- en: In this particular case, the request is for "Scripting for the JavaTM Platform"
    and was originally filed on Nov 15th 2004 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223)).
    At the beginning, it was targeted towards the ability of the Java servlet to work
    with multiple scripting languages. The specification requires the scripting language
    maintainers to provide a Java JAR with corresponding implementations. Portability
    issues hindered practical implementations, particularly when platforms require
    complex interaction with OS, such as dynamic linking in C or Fortran. Currently,
    only a handful languages are supported, with R and Python being supported, but
    in incomplete form.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定案例中，请求的是“JavaTM 平台的脚本”，最初于 2004 年 11 月 15 日提出 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223))。最初，它针对的是
    Java servlet 与多种脚本语言协同工作的能力。规范要求脚本语言维护者提供包含相应实现的 Java JAR。可移植性问题阻碍了实际实施，尤其是在需要与
    OS 进行复杂交互的平台，例如 C 或 Fortran 中的动态链接。目前，只有少数语言受到支持，R 和 Python 被支持，但形式不完整。
- en: 'Since Java 6, JSR 223: Scripting for Java added the `javax.script` package
    that allows multiple scripting languages to be called through the same API as
    long as the language provides a script engine. To add the Jython scripting language,
    download the latest Jython JAR from the Jython site at [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Java 6 以来，JSR 223：Java 脚本添加了 `javax.script` 包，允许通过相同的 API 调用多种脚本语言，只要该语言提供了一个脚本引擎。要添加
    Jython 脚本语言，请从 Jython 网站下载最新的 Jython JAR 文件，网址为 [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html)：
- en: '[PRE45]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, I can use the Jython/Python scripting engine:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以使用 Jython/Python 脚本引擎：
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: It is worth giving a disclaimer here that not all Python modules are available
    in Jython. Modules that require a C/Fortran dynamic linkage for the library that
    doesn't exist in Java are not likely to work in Jython. Specifically, NumPy and
    SciPy are not supported in Jython as they rely on C/Fortran. If you discover some
    other missing modules, you can try copying the `.py` file from a Python distribution
    to a `sys.path` Jython directory—if this works, consider yourself lucky.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里值得声明的是，并非所有 Python 模块都在 Jython 中可用。那些需要 C/Fortran 动态链接库的模块，而这些库在 Java 中不存在，在
    Jython 中可能无法工作。具体来说，NumPy 和 SciPy 在 Jython 中不受支持，因为它们依赖于 C/Fortran。如果你发现其他缺失的模块，可以尝试将
    Python 发行版中的 `.py` 文件复制到 Jython 的 `sys.path` 目录中——如果这样做有效，那么你很幸运。
- en: 'Jython has the advantage of accessing Python-rich modules without the necessity
    of starting the Python runtime on each call, which might result in a significant
    performance saving:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Jython 具有优势，无需在每次调用时启动 Python 运行时即可访问丰富的 Python 模块，这可能会带来显著的性能提升：
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Jython JSR 223 call is 10 times faster!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Jython JSR 223 调用速度提高了 10 倍！
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: R and Python are like bread and butter for a data scientist. Modern frameworks
    tend to be interoperable and borrow from each other's strength. In this chapter,
    I went over the plumbing of interoperability with R and Python. Both of them have
    packages (R) and modules (Python) that became very popular and extend the current
    Scala/Spark functionality. Many consider R and Python existing libraries to be
    crucial for their implementations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: R 和 Python 对于数据科学家来说就像面包和黄油。现代框架往往具有互操作性，并互相借鉴对方的优点。在本章中，我介绍了 R 和 Python 之间的互操作性配置。它们都有（R）包和（Python）模块变得非常流行，并扩展了当前的
    Scala/Spark 功能。许多人认为 R 和 Python 的现有库对于它们的实现至关重要。
- en: This chapter demonstrated a few ways to integrate these packages and provide
    the tradeoffs of using these integrations so that we can proceed on to the next
    chapter, looking at the NLP, where functional programming has been traditionally
    used from the start.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本章演示了几种集成这些包的方法，并提供了使用这些集成所带来的权衡，以便我们能够继续到下一章，探讨自然语言处理（NLP），在该领域中，函数式编程从一开始就被传统地使用。
