- en: Chapter 8. Integrating Scala with R and Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Spark provides MLlib as a library for machine learning, in many practical
    situations, R or Python present a more familiar and time-tested interface for
    statistical computations. In particular, R's extensive statistical library includes
    very popular ANOVA/MANOVA methods of analyzing variance and variable dependencies/independencies,
    sets of statistical tests, and random number generators that are not currently
    present in MLlib. The interface from R to Spark is available under SparkR project.
    Finally, data analysts know Python's NumPy and SciPy linear algebra implementations
    for their efficiency as well as other time-series, optimization, and signal processing
    packages. With R/Python integration, all these familiar functionalities can be
    exposed to Scala/Spark users until the Spark/MLlib interfaces stabilize and the
    libraries make their way into the new framework while benefiting the users with
    Spark's ability to execute workflows in a distributed way across multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: When people program in R or Python, or with any statistical or linear algebra
    packages for this matter, they are usually not specifically focusing on the functional
    programming aspects. As I mentioned in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, Scala should be treated as a high-level
    language and this is where it shines. Integration with highly efficient C and
    Fortran implementations, for example, of the freely available **Basic Linear Algebra
    Subprograms** (**BLAS**), **Linear Algebra** **Package** (**LAPACK**), and **Arnoldi
    Package** (**ARPACK**), is known to find its way into Java and thus Scala ([http://www.netlib.org](http://www.netlib.org),
    [https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java)).
    I would like to leave Scala at what it's doing best. In this chapter, however,
    I will focus on how to use these languages with Scala/Spark.
  prefs: []
  type: TYPE_NORMAL
- en: I will use the publicly available United States Department of Transportation
    flights dataset for this chapter ([http://www.transtats.bts.gov](http://www.transtats.bts.gov)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing R and configuring SparkR if you haven't done so yet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about R (and Spark) DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing linear regression and ANOVA analysis with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing **Generalized Linear Model** (**GLM**) modeling with SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Python if you haven't done so yet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to use PySpark and call Python from Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with many advanced and carefully designed technologies, people usually either
    love or hate R as a language. One of the reason being that R was one of the first
    language implementations that tries to manipulate complex objects, even though
    most of them turn out to be just a list as opposed to struct or map as in more
    mature modern implementations. R was originally created at the University of Auckland
    by Ross Ihaka and Robert Gentleman around 1993, and had its roots in the S language
    developed at Bell Labs around 1976, when most of the commercial programming was
    still done in Fortran. While R incorporates some functional features such as passing
    functions as a parameter and map/apply, it conspicuously misses some others such
    as lazy evaluation and list comprehensions. With all this said, R has a very good
    help system, and if someone says that they never had to go back to the `help(…)`
    command to figure out how to run a certain data transformation or model better,
    they are either lying or just starting in R.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up R and SparkR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run SparkR, you'll need R version 3.0 or later. Follow the given instructions
    for the installation, depending on you operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On a Linux system, detailed installation documentation is available at [https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux).
    However, for example, on a Debian system, one installs it by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To list installed/available packages on the Linux repository site, perform
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'R packages, which are a part of `r-base` and `r-recommended`, are installed
    into the `/usr/lib/R/library` directory. These can be updated using the usual
    package maintenance tools such as `apt-get` or aptitude. The other R packages
    available as precompiled Debian packages, `r-cran-*` and `r-bioc-*`, are installed
    into `/usr/lib/R/site-library`. The following command shows all packages that
    depend on `r-base-core`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This comprises of a large number of contributed packages from CRAN and other
    repositories. If you want to install R packages that are not provided as package,
    or if you want to use newer versions, you need to build them from source that
    requires the `r-base-dev` development package that can be installed by the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This pulls in the basic requirements to compile R packages, such as the development
    tools group install. R packages may then be installed by the local user/admin
    from the CRAN source packages, typically from inside R using the `R> install.packages()`
    function or `R CMD INSTALL`. For example, to install the R `ggplot2` package,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will download and optionally compile the package and its dependencies
    from one of the available sites. Sometime R is confused about the repositories;
    in this case, I recommend creating a `~/.Rprofile` file in the home directory
    pointing to the closest CRAN repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`~/.Rprofile` contains commands to customize your sessions. One of the commands
    I recommend to put in there is `options (prompt="R> ")` to be able to distinguish
    the shell you are working in by the prompt, following the tradition of most tools
    in this book. The list of known mirrors is available at [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it is good practice to specify the directory to install `system/site/user`
    packages via the following command, unless your OS setup does it already by putting
    these commands into `~/.bashrc` or system `/etc/profile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Mac OS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'R for Mac OS can be downloaded, for example, from [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx).
    The latest version at the time of the writing is 3.2.3\. Always check the consistency
    of the downloaded package. To do so, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The environment settings in the preceding subsection also apply to the Mac OS
    setup.
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R for Windows can be downloaded from [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    as an exe installer. Run this executable as an administrator to install R.
  prefs: []
  type: TYPE_NORMAL
- en: One can usually edit the environment setting for **System/User** by following
    the **Control Panel** | **System and Security** | **System** | **Advanced system
    settings** | **Environment Variables** path from the Windows menu.
  prefs: []
  type: TYPE_NORMAL
- en: Running SparkR via scripts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run SparkR, one needs to run install the `R/install-dev.sh` script that
    comes with the Spark git tree. In fact, one only needs the shell script and the
    content of the `R/pkg` directory, which is not always included with the compiled
    Spark distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Running Spark via R's command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, we can also initialize Spark from the R command line directly
    (or from RStudio at [http://rstudio.org/](http://rstudio.org/)) using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As described previously in [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib*, the `SPARK_HOME` environment variable
    needs to point to your local Spark installation directory and `SPARK_MASTER` and
    `YARN_CONF_DIR` to the desired cluster manager (local, standalone, mesos, and
    YARN) and YARN configuration directory if one is using Spark with the YARN cluster
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: Although most all of the distributions come with a UI, in the tradition of this
    book and for the purpose of this chapter I'll use the command line.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DataFrames originally came from R and Python, so it is natural to see them
    in SparkR.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that the implementation of DataFrames in SparkR is on top of RDDs,
    so they work differently than the R DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The question of when and where to store and apply the schema and other metadata
    like types has been a topic of active debate recently. On one hand, providing
    the schema early with the data enables thorough data validation and potentially
    optimization. On the other hand, it may be too restrictive for the original data
    ingest, whose goal is just to capture as much data as possible and perform data
    formatting/cleansing later on, the approach often referred as schema on read.
    The latter approach recently won more ground with the tools to work with evolving
    schemas such as Avro and automatic schema discovery tools, but for the purpose
    of this chapter, I'll assume that we have done the schema discovery part and can
    start working with a DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first download and extract a flight delay dataset from the United States
    Department of Transportation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have Spark running on the cluster, you want to copy the file in HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `flights/readme.html` files gives you detailed metadata information, as
    shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrames](img/B04935_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 08-1: Metadata provided with the On-Time Performance dataset released
    by the US Department of Transportation (for demo purposes only)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I want you to analyze the delays of `SFO` returning flights and possibly
    find the factors contributing to the delay. Let''s start with the R `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you were flying from Salt Lake City on Sunday with Alaska Airlines in July
    2015, consider yourself unlucky (we have only done simple analysis so far, so
    one shouldn't attach too much significance to this result). There may be multiple
    other random factors contributing to the delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we ran the example in SparkR, we still used the R `data.frame`.
    If we want to analyze data across multiple months, we will need to distribute
    the load across multiple nodes. This is where the SparkR distributed DataFrame
    comes into play, as it can be distributed across multiple threads even on a single
    node. There is a direct way to convert the R DataFrame to SparkR DataFrame (and
    thus to RDD):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If I run it on a laptop, I will run out of memory. The overhead is large due
    to the fact that I need to transfer the data between multiple threads/nodes, we
    want to filter it as soon as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run even on my laptop. There is, of course, a reverse conversion
    from Spark''s DataFrame to R''s `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, I can use the `spark-csv` package to read it from the `.csv`
    file, which, if the original `.csv` file is in a distributed filesystem such as
    HDFS, will avoid shuffling the data over network in a cluster setting. The only
    drawback, at least currently, is that Spark cannot read from the `.zip` files
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we loaded the additional `com.databricks:spark-csv_2.10:1.3.0` package
    by supplying the `--package` flag on the command line; we can easily go distributed
    by using a Spark instance over a cluster of nodes or even analyze a larger dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will download and put the on-time performance data in the flight''s directory
    (remember, as we discussed in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory Data
    Analysis"), *Exploratory Data Analysis*, we would like to treat directories as
    big data datasets). We can now run the same analysis over the whole period of
    2015 (for the available data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we used a `cache()` call to pin the dataset to the memory as we will
    use it again later. This time it''s Minneapolis/United on Saturday! However, you
    probably already know why: there is only one record for this combination of `DayOfWeek`,
    `Origin`, and `UniqueCarrier`; it''s most likely an outlier. The average over
    about `30` flights for the previous outlier was reduced to `30` minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Sunday still remains a problem in terms of delays. The limit to the amount of
    data we can analyze now is only the number of cores on the laptop and nodes in
    the cluster. Let's look at more complex machine learning models now.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear methods play an important role in statistical modeling. As the name
    suggests, linear model assumes that the dependent variable is a weighted combination
    of independent variables. In R, the `lm` function performs a linear regression
    and reports the coefficients, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary` function provides even more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: While we considered generalized linear models in [Chapter 3](ch03.xhtml "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, and we will also consider
    the `glm` implementation in R and SparkR shortly, linear models provide more information
    in general and are an excellent tool for working with noisy data and selecting
    the relevant attribute for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data analysis life cycle**'
  prefs: []
  type: TYPE_NORMAL
- en: While most of the statistical books focus on the analysis and best use of available
    data, the results of statistical analysis in general should also affect the search
    for the new sources of information. In the complete data life cycle, discussed
    at the end of [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark and MLlib"),
    *Working with Spark and MLlib*, a data scientist should always transform the latest
    variable importance results into the theories of how to collect data. For example,
    if the ink usage analysis for home printers points to an increase in ink usage
    for photos, one could potentially collect more information about the format of
    the pictures, sources of digital images, and paper the user prefers to use. This
    approach turned out to be very productive in a real business situation even though
    not fully automated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, here is a short description of the output that linear models
    provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Residuals**: These are statistics for the difference between the actual and
    predicted values. A lot of techniques exist to detect the problems with the models
    on patterns of the residual distribution, but this is out of scope of this book.
    A detailed residual table can be obtained with the `resid(model)` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coefficients**: These are the actual linear combination coefficients; the
    t-value represents the ratio of the value of the coefficient to the estimate of
    the standard error: higher values mean a higher likelihood that this coefficient
    has a non-trivial effect on the dependent variable. The coefficients can also
    be obtained with `coef(model)` functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual standard error**: This reports the standard mean square error, the
    metric that is the target of optimization in a straightforward linear regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple R-squared**: This is the fraction of the dependent variable variance
    that is explained by the model. The adjusted value accounts for the number of
    parameters in your model and is considered to be a better metric to avoid overfitting
    if the number of observations does not justify the complexity of the models, which
    happens even for big data problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-statistic**: The measure of model quality. In plain terms, it measures
    how all the parameters in the model explain the dependent variable. The p-value
    provides the probability that the model explains the dependent variable just due
    to random chance. The values under 0.05 (or 5%) are, in general, considered satisfactory.
    While in general, a high value probably means that the model is probably not statistically
    valid and "nothing else matters, the low F-statistic does not always mean that
    the model will work well in practice, so it cannot be directly applied as a model
    acceptance criterion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the linear models are applied, usually more complex `glm` or recursive
    models, such as decision trees and the `rpart` function, are applied to find interesting
    variable interactions. Linear models are good for establishing baseline on the
    other models that can improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, ANOVA is a standard technique to study the variance if the independent
    variables are discrete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The measure of the model quality is F-statistics. While one can always run R
    algorithms with RDD using the pipe mechanism with `Rscript`, I will partially
    cover this functionality with respect to **Java Specification Request** (**JSR**)
    223 Python integration later. In this section, I would like to explore specifically
    a generalized linear regression `glm` function that is implemented both in R and
    SparkR natively.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, you can run either R `glm` or SparkR `glm`. The list of possible
    link and optimization functions for R implementation is provided in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows possible options for R `glm` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Family | Variance | Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| gaussian | gaussian | identity |'
  prefs: []
  type: TYPE_TB
- en: '| binomial | binomial | logit, probit or cloglog |'
  prefs: []
  type: TYPE_TB
- en: '| poisson | poisson | log, identity or sqrt |'
  prefs: []
  type: TYPE_TB
- en: '| Gamma | Gamma | inverse, identity or log |'
  prefs: []
  type: TYPE_TB
- en: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
  prefs: []
  type: TYPE_TB
- en: '| quasi | user-defined | user-defined |'
  prefs: []
  type: TYPE_TB
- en: 'I will use a binary target, `ArrDel15`, which indicates whether the plane was
    more than 15 minutes late for the arrival. The independent variables will be `DepDel15`,
    `DayOfWeek`, `Month`, `UniqueCarrier`, `Origin`, and `Dest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'While you wait for the results, open another shell and run `glm` in the `SparkR`
    mode on the full seven months of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here we try to build a model explaining delays as an effect of carrier, day
    of week, and origin on destination airports, which is captured by the formular
    construct `ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Nulls, big data, and Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the SparkR case of `glm`, I had to explicitly filter out the non-cancelled
    flights and removed the NA—or nulls in the C/Java lingo. While R does this for
    you by default, NAs in big data are very common as the datasets are typically
    sparse and shouldn't be treated lightly. The fact that we have to deal with nulls
    explicitly in MLlib warns us about some additional information in the dataset
    and is definitely a welcome feature. The presence of an NA can carry information
    about the way the data was collected. Ideally, each NA should be accompanied by
    a small `get_na_info` method as to why this particular value was not available
    or collected, which leads us to the `Either` type in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Even though nulls are inherited from Java and a part of Scala, the `Option`
    and `Either` types are new and more robust mechanism to deal with special cases
    where nulls were traditionally used. Specifically, `Either` can provide a value
    or exception message as to why it was not computed; while `Option` can either
    provide a value or be `None`, which can be readily captured by the Scala pattern-matching
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing you will notice is that SparkR will run multiple threads, and even
    on a single node, it will consume CPU time from multiple cores and returns much
    faster even with a larger size of data. In my experiment on a 32-core machine,
    it was able to finish in under a minute (as opposed to 35 minutes for R `glm`).
    To get the results, as in the R model case, we need to run the `summary()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The worst performer is `NK` (Spirit Airlines). Internally, SparkR uses limited-memory
    BFGS, which is a limited-memory quasi-Newton optimization method that is similar
    to the results obtained with R `glm` on the July data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Other parameters of SparkR `glm` implementation are provided in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a list of parameters for SparkR `glm` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Possible Values | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `formula` | A symbolic description like in R | Currently only a subset of
    formula operators are supported: ''`~`'', ''`.`'', ''`:`'', ''`+`'', and ''`-`''
    |'
  prefs: []
  type: TYPE_TB
- en: '| `family` | gaussian or binomial | Needs to be in quotes: gaussian -> linear
    regression, binomial -> logistic regression |'
  prefs: []
  type: TYPE_TB
- en: '| `data` | DataFrame | Needs to be SparkR DataFrame, not `data.frame` |'
  prefs: []
  type: TYPE_TB
- en: '| `lambda` | positive | Regularization coefficient |'
  prefs: []
  type: TYPE_TB
- en: '| `alpha` | positive | Elastic-net mixing parameter (refer to glmnet''s documentation
    for details) |'
  prefs: []
  type: TYPE_TB
- en: '| `standardize` | TRUE or FALSE | User-defined |'
  prefs: []
  type: TYPE_TB
- en: '| `solver` | l-bfgs, normal or auto | auto will choose the algorithm automatically,
    l-bfgs means limited-memory BFGS, normal means using normal equation as an analytical
    solution to the linear regression problem |'
  prefs: []
  type: TYPE_TB
- en: Reading JSON files in SparkR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Schema on Read is one of the convenient features of big data. The DataFrame
    class has the ability to figure out the schema of a text file containing a JSON
    record per line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Writing Parquet files in SparkR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the previous chapter, the Parquet format is an efficient
    storage format, particularly for low cardinality columns. Parquet files can be
    read/written directly from R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the new Parquet file is 66 times smaller that the original
    zip file downloaded from the DoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Invoking Scala from R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's assume that one has an exceptional implementation of a numeric method
    in Scala that we want to call from R. One way of doing this would be to use the
    R `system()` function that invokes `/bin/sh` on Unix-like systems. However, the
    `rscala` package is a more efficient way that starts a Scala interpreter and maintains
    communication over TCP/IP network connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the Scala interpreter maintains the state (memoization) between the calls.
    Similarly, one can define functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'R from Scala can be invoked using the `!` or `!!` Scala operators and `Rscript`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using Rserve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A more efficient way is to use the similar TCP/IP binary transport protocol
    to communicate with R with `Rsclient/Rserve` ([http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)).
    To start `Rserve` on a node that has R installed, perform the following action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By default, `Rserv` opens a connection on `localhost:6311`. The advantage of
    the binary network protocol is that it is platform-independent and multiple clients
    can communicate with the server. The clients can connect to `Rserve`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, while passing the results as a binary object has its advantages,
    you have to be careful with the type mappings between R and Scala. `Rserve` supports
    other clients, including Python, but I will also cover JSR 223-compliant scripting
    at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has slowly established ground as a de-facto tool for data science. It
    has a command-line interface and decent visualization via matplotlib and ggplot,
    which is based on R's ggplot2\. Recently, Wes McKinney, the creator of Pandas,
    the time series data-analysis package, has joined Cloudera to pave way for Python
    in big data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python is usually part of the default installation. Spark requires version 2.7.0+.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t have Python on Mac OS, I recommend installing the Homebrew package
    manager from [http://brew.sh](http://brew.sh):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, on a Unix-like system, Python can be compiled from the source distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It is good practice to place it in a directory different from the default Python
    installation. It is normal to have multiple versions of Python on a single system,
    which usually does not lead to problems as Python separates the installation directories.
    For the purpose of this chapter, as for many machine learning takes, I''ll also
    need a few packages. The packages and specific versions may differ across installations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If everything compiles—SciPy uses a Fortran compiler and libraries for linear
    algebra—we are ready to use Python 2.7.11!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that if one wants to use Python with the `pipe` command in a distributed
    environment, Python needs to be installed on every node in the network.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As `bin/sparkR` launches R with preloaded Spark context, `bin/pyspark` launches
    Python shell with preloaded Spark context and Spark driver running. The `PYSPARK_PYTHON`
    environment variable can be used to point to a specific Python version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'PySpark directly supports most of MLlib functionality on Spark RDDs ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)),
    but it is known to lag a few releases behind the Scala API ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)).
    As of the 1.6.0+ release, it also supports DataFrames ([http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Calling Python from Java/Scala
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As this is really a book about Scala, we should also mention that one can call
    Python code and its interpreter directly from Scala (or Java). There are a few
    options available that will be discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using sys.process._
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scala, as well as Java, can call OS processes via spawning a separate thread,
    which we already used for interactive analysis in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*: the `.!` method will start the process
    and return the exit code, while `.!!` will return the string that contains the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try a more complex SVD computation (similar to the one we used in SVD++
    recommendation engine, but this time, it invokes BLAS C-libraries at the backend).
    I created a Python executable that takes a string representing a matrix and the
    required rank as an input and outputs an SVD approximation with the provided rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s call it `svd.py` and put in in the current directory. Given a matrix
    and rank as an input, it produces an approximation of a given rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To call it from Scala, let''s define the following `#<<<` method in our DSL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `#<<<` operator to call Python''s SVD method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that as we requested the resulting matrix rank to be one, all rows and
    columns are linearly dependent. We can even pass several lines of input at a time,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Spark pipe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SVD decomposition is usually a pretty heavy operation, so the relative overhead
    of calling Python in this case is small. We can avoid this overhead if we keep
    the process running and supply several lines at a time, like we did in the last
    example. Both Hadoop MR and Spark implement this approach. For example, in Spark,
    the whole computation will take only one line, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The whole pipeline is ready to be distributed across a cluster of multicore
    workstations! I think you will be in love with Scala/Spark already.
  prefs: []
  type: TYPE_NORMAL
- en: Note that debugging the pipelined executions might be tricky as the data is
    passed from one process to another using OS pipes.
  prefs: []
  type: TYPE_NORMAL
- en: Jython and JSR 223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For completeness, we need to mention Jython, a Java implementation of Python
    (as opposed to a more familiar C implementation, also called CPython). Jython
    avoids the problem of passing input/output via OS pipelines by allowing the users
    to compile Python source code to Java byte codes, and running the resulting bytecodes
    on any Java virtual machine. As Scala also runs in Java virtual machine, it can
    use the Jython classes directly, although the reverse is not true in general;
    Scala classes sometimes are not compatible to be used by Java/Jython.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**JSR 223**'
  prefs: []
  type: TYPE_NORMAL
- en: In this particular case, the request is for "Scripting for the JavaTM Platform"
    and was originally filed on Nov 15th 2004 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223)).
    At the beginning, it was targeted towards the ability of the Java servlet to work
    with multiple scripting languages. The specification requires the scripting language
    maintainers to provide a Java JAR with corresponding implementations. Portability
    issues hindered practical implementations, particularly when platforms require
    complex interaction with OS, such as dynamic linking in C or Fortran. Currently,
    only a handful languages are supported, with R and Python being supported, but
    in incomplete form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Java 6, JSR 223: Scripting for Java added the `javax.script` package
    that allows multiple scripting languages to be called through the same API as
    long as the language provides a script engine. To add the Jython scripting language,
    download the latest Jython JAR from the Jython site at [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I can use the Jython/Python scripting engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: It is worth giving a disclaimer here that not all Python modules are available
    in Jython. Modules that require a C/Fortran dynamic linkage for the library that
    doesn't exist in Java are not likely to work in Jython. Specifically, NumPy and
    SciPy are not supported in Jython as they rely on C/Fortran. If you discover some
    other missing modules, you can try copying the `.py` file from a Python distribution
    to a `sys.path` Jython directory—if this works, consider yourself lucky.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jython has the advantage of accessing Python-rich modules without the necessity
    of starting the Python runtime on each call, which might result in a significant
    performance saving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Jython JSR 223 call is 10 times faster!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R and Python are like bread and butter for a data scientist. Modern frameworks
    tend to be interoperable and borrow from each other's strength. In this chapter,
    I went over the plumbing of interoperability with R and Python. Both of them have
    packages (R) and modules (Python) that became very popular and extend the current
    Scala/Spark functionality. Many consider R and Python existing libraries to be
    crucial for their implementations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter demonstrated a few ways to integrate these packages and provide
    the tradeoffs of using these integrations so that we can proceed on to the next
    chapter, looking at the NLP, where functional programming has been traditionally
    used from the start.
  prefs: []
  type: TYPE_NORMAL
