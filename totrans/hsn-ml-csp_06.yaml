- en: Color Blending – Self-Organizing Maps and Elastic Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 颜色混合 - 自组织映射和弹性神经网络
- en: '**Self-Organizing Maps** (**SOM**), or **Kohonen maps** as you may have heard,
    are one of the basic types of self-organizing neural networks. The ability to
    self-organize provides adaptation to formerly unseen input data. It has been theorized
    as one of the most natural ways of learning, like that which is used by our brains,
    where no predefined patterns are thought to exist. Those patterns take shape during
    the learning process and are incredibly gifted at representing multidimensional
    data at a much lower level of dimensionality, such as 2D or 1D. Additionally,
    this network stores information in such a way that any topological relationships
    within the training set remain preserved.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自组织映射**（**SOM**），或者如您所听到的**Kohonen图**，是自组织神经网络的基本类型之一。自组织的能力提供了对以前未见过的输入数据的适应性。它被理论化为学习最自然的方式之一，就像我们的大脑所使用的那样，其中没有预定义的模式被认为是存在的。这些模式在学习过程中形成，并且擅长以远低于维度的水平表示多维数据，例如2D或1D。此外，这个网络以这种方式存储信息，即训练集中的任何拓扑关系都保持不变。'
- en: More formally, an SOM is a clustering technique that will help us uncover interesting
    data categories in large datasets. It's a type of unsupervised neural network
    where neurons are arranged in a single, two-dimensional grid. The grid must be
    rectangular, as in, a pure rectangle or a hexagon. Throughout the iterations (which
    we will specify), the neurons in our grid will gradually coalesce around areas
    with a higher density of data points (the left-hand side of our display called
    Points). As the neurons move, they bend and twist the grid until they move more
    closely to the points of interest and reflect the shape of that data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，SOM 是一种聚类技术，它将帮助我们在大数据集中发现有趣的数据类别。它是一种无监督神经网络，其中神经元排列在一个单一的二维网格中。网格必须是矩形的，即，一个纯矩形或六边形。在整个迭代过程中（我们将指定），我们的网格中的神经元将逐渐聚集在数据点密度较高的区域（我们显示的左侧称为点）。随着神经元的移动，它们弯曲和扭曲网格，直到它们更接近感兴趣的点并反映数据的形状。
- en: 'In this chapter we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主题：
- en: Kohonen SOM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohonen SOM
- en: Working with AForge.NET
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AForge.NET 进行工作
- en: Under the hood of an SOM
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SOM 的内部机制
- en: 'So, the inevitable question now arises: how do these things work?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在不可避免的问题出现了：这些是如何工作的？
- en: In a nutshell, we have neurons on the grid; gradually, via iterations, they
    adapt themselves to the shape of our data (in our example, shown in the following
    image on the left-hand side in the Points panel). Let's talk a bit more about
    the iterative process itself.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们在网格上有神经元；通过迭代，它们逐渐适应我们的数据形状（在我们的例子中，如下面的图像所示，在点面板的左侧）。让我们更多地谈谈迭代过程本身。
- en: 'The first step is to randomly position data on the grid. We will randomly be
    placing our grid''s neurons in our data space, as follows:'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是在网格上随机放置数据。我们将随机在我们的数据空间中放置我们的网格神经元，如下所示：
- en: '![](img/d34f3e4a-3095-4a5e-8b75-ddd12ac87458.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d34f3e4a-3095-4a5e-8b75-ddd12ac87458.png)'
- en: The second step is where our algorithm will select a single data point.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是算法将选择一个单一的数据点。
- en: In the third step, we need to find the neuron (data point) that is closest to
    the chosen data point. This then becomes our best matching unit.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三步，我们需要找到与所选数据点最近的神经元（数据点）。然后这将成为我们的最佳匹配单元。
- en: The fourth step is to move our best-matching unit towards that data point. The
    distance that we move is determined by our learning rate, which will ultimately
    decrease after each iteration.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四步是将我们的最佳匹配单元移动到该数据点。我们移动的距离由我们的学习率决定，该学习率在每个迭代后最终会减小。
- en: 'Fifth, we will move the neighbors of our best-matching unit closer to it, with
    the farther-away neurons moving less than those that are closer. The Initial radius
    variable you see on the screen is what we use to identify neighbors. This value,
    just like the Initial learning rate, will decrease over time. If you have **ReflectInsight**
    (**RI**) up and running, you can watch the Initial learning rate decrease over
    time, as shown in the following screenshot:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第五，我们将把最佳匹配单元的邻居移动得更近，距离较远的神经元移动的距离小于较近的神经元。屏幕上您看到的初始半径变量是我们用来识别邻居的。这个值，就像初始学习率一样，会随着时间的推移而减小。如果您已经启动并运行了**ReflectInsight**（**RI**），您可以观察初始学习率随时间的变化，如下面的截图所示：
- en: '![](img/bf11010d-c854-488e-8b8b-13aaf8fc1057.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf11010d-c854-488e-8b8b-13aaf8fc1057.png)'
- en: Our sixth and final step will be to update the Initial learning rate and Initial
    radius, as we have described so far, and then repeat. We will continue this process
    until our data points have stabilized and are in the correct position.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们第六步和最后一步将是更新初始学习率和初始半径，正如我们之前所描述的，然后重复。我们将继续这个过程，直到我们的数据点稳定并处于正确的位置。
- en: Now that we've introduced you to a little bit of intuition on SOMs, let's talk
    a little more about what we're going to do in this chapter. We have chosen a very
    common mechanism for teaching our principals, which is the mapping of colors.
    The colors themselves are 3D objects represented by red, green, and blue, but
    we will be organizing them into two dimensions. There are two key points you will
    see here with the organization of colors. First, the colors are clustered into
    distinct regions, and second, regions of similar properties are usually found
    adjacent to each other.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经向你介绍了一些关于SOMs的直觉，让我们再谈谈本章我们将要做什么。我们选择了一个非常常见的机制来教授我们的原则，即颜色的映射。颜色本身是由红色、绿色和蓝色表示的3D对象，但我们将它们组织到二维中。你将在这里看到两个关于颜色组织的要点。首先，颜色被聚类到不同的区域，其次，具有相似属性的区域通常相邻。
- en: Our second example, which is a little bit more advanced, will use an **artificial
    neural network** (**ANN**) as we described before; this is an advanced form of
    machine learning, used to create an organizational mapping that matches the one
    presented to it. Let's look at our first example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个例子，稍微复杂一些，将使用我们之前描述的**人工神经网络**（**ANN**）；这是一种高级的机器学习方法，用于创建与提供给它的组织映射相匹配的组织映射。让我们看看我们的第一个例子。
- en: 'Here''s a screenshot of our example. As you can see, we have a random pattern
    of colors, which, when finished, will be organized by clusters of similar colors:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们示例的截图。正如你所见，我们有一个随机的颜色模式，完成后，这些颜色将被组织成相似颜色的簇：
- en: '![](img/dcfdf5aa-87f9-4adf-bdff-75186da6c1b7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dcfdf5aa-87f9-4adf-bdff-75186da6c1b7.png)'
- en: 'If we are successful—and we will be—here''s what our result should look like:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们成功——我们会成功的——我们的结果应该如下所示：
- en: '![](img/f8663f07-1bd1-41f1-97be-10f5583fc31f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8663f07-1bd1-41f1-97be-10f5583fc31f.png)'
- en: 'Let us begin by following the steps of the process as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先按照以下步骤遵循处理过程：
- en: 'We will start out by using 500 iterations to achieve our goal. Using a smaller
    number may not produce the blend that we are ultimately after. As an example,
    if we have used 500 iterations, here is what our result would look like:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先使用500次迭代来实现我们的目标。使用较小的数字可能不会产生我们最终想要的混合效果。例如，如果我们使用了500次迭代，我们的结果将如下所示：
- en: '![](img/4d88fb40-b1fd-4d5d-b283-0d845b742520.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d88fb40-b1fd-4d5d-b283-0d845b742520.png)'
- en: As you can see, we are far from being where we need to be. Being able to change
    the iterations allows you to experiment with exactly the right setting. I can
    tell you that 500 is higher than we need, so I will leave it as an exercise for
    you to figure out the number where the progression stops and you are satisfied
    with the organization.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如你所见，我们离我们想要达到的目标还很远。能够改变迭代次数让你可以尝试正确的设置。我可以告诉你500次迭代比我们需要的要多，所以我会把它留给你作为练习，找出进度停止并且你对组织满意的数量。
- en: After setting the number of iterations, all we must do is make sure we have
    the random color pattern we want, which can be achieved by clicking on the Randomize
    button.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置迭代次数后，我们唯一要做的就是确保我们有我们想要的随机颜色模式，这可以通过点击“随机化”按钮来实现。
- en: Once you have the pattern that you want, you simply click on the Start button
    and watch the results.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你得到了你想要的模式，你只需点击“开始”按钮并观察结果。
- en: Once you click on Start, the Stop button will be activated, and you can stop
    the progression anytime you like. The organization will automatically stop once
    you reach the number of iterations that you specified.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你点击“开始”，“停止”按钮将被激活，你可以在任何时候停止进度。当你达到指定的迭代次数时，组织将自动停止。
- en: 'Before we get into the actual code, let me show you some screenshots of what
    some organizational patterns look like. You can accomplish wonderful results by
    simply changing different parameters, which we will describe in detail further
    on. In the following screenshot, we have set the number of Iterations to 3000
    and the Initial radius as 10:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入实际代码之前，让我给你展示一些组织模式的一些截图。通过简单地改变不同的参数，你可以取得惊人的效果，我们将在后面详细描述。在下面的截图中，我们将迭代次数设置为3000，初始半径为10：
- en: '![](img/a02cae80-2113-447b-810b-a16822a87476.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a02cae80-2113-447b-810b-a16822a87476.png)'
- en: 'In the following screenshot we are using 4000 iterations and an Initial radius
    of 18:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们使用了4000次迭代和初始半径为18：
- en: '![](img/b84882b8-9de6-4959-b247-3f80a2eebe18.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b84882b8-9de6-4959-b247-3f80a2eebe18.png)'
- en: 'In the following screenshot we have set the number of Iterations to 4000 and
    the Initial radius as 5:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下屏幕截图中，我们将迭代次数设置为4000，初始半径为5：
- en: '![](img/997956c1-84ed-4704-8435-ee2c0682588e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/997956c1-84ed-4704-8435-ee2c0682588e.png)'
- en: 'Here, we have set the number of Iterations to 5000, the Initial learning rate
    as 0.3, and the Initial radius as 25, as shown in the following screenshot, to
    obtain the desired result:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将迭代次数设置为5000，初始学习率为0.3，初始半径为25，如以下屏幕截图所示，以获得期望的结果：
- en: '![](img/207b50b9-5ac8-4967-958a-3f3c3c91a21f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/207b50b9-5ac8-4967-958a-3f3c3c91a21f.png)'
- en: As promised, let's now dive into the code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，现在让我们深入代码。
- en: In this example, we are going to work with `AForge` and use the `DistanceNetwork`
    object. A distance network is a neural network of only a single distance. As well
    as being used for an SOM, it is used for an ElasticNet operation, which is what
    we will be using to show the elastic connections between objects during progression.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`AForge`并使用`DistanceNetwork`对象。距离网络是一个只有单一距离的神经网络。除了用于SOM之外，它还用于弹性网络操作，这是我们用来展示在进展过程中对象之间弹性连接的。
- en: 'We will create our distance network using three input neurons and `1000` neurons
    that will be doing the work under the hood:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三个输入神经元和`1000`个将在幕后工作的神经元来创建我们的距离网络：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you click on the Randomize button to randomize the colors, here''s what
    happens under the hood:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击随机化按钮来随机化颜色时，下面是幕后发生的事情：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will notice that the randomization range we are dealing with stays within
    the range of any color's red, green, or blue characteristic, which is `255`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们正在处理的随机化范围保持在任何颜色的红色、绿色或蓝色特征的范围内，即`255`。
- en: 'Next, we will look at our learning loop, which looks like this. We''ll do a
    deep dive into it in a moment:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看我们的学习循环，它看起来是这样的。我们稍后会深入探讨：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we look closer, the first object we create is an `SOMLearning` object. This
    object is optimized for square space learning, meaning it expects that the network
    it is working on has the same height as its width. This makes it easier to find
    the square root of the network''s neuron counts:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察，我们首先创建的是一个`SOMLearning`对象。这个对象针对正方形空间学习进行了优化，意味着它期望它正在工作的网络的高度与宽度相同。这使得找到网络神经元数量的平方根变得更容易：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to create variables to hold our red, green, and blue input colors,
    from which we will continually randomize the input colors in order to achieve
    our goals:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建变量来保存我们的红色、绿色和蓝色输入颜色，我们将不断地随机化输入颜色以达到我们的目标：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we enter our `while` loop, we will continually update our variables until
    we reach the total number of iterations we selected. In this update loop, there
    are several things happening. First, we will update the learning rate and learning
    radius, and store it in our `SOMLearning` object:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进入`while`循环，我们将不断地更新我们的变量，直到达到我们选择的迭代总数。在这个更新循环中，有几件事情正在发生。首先，我们将更新学习率和学习半径，并将其存储在我们的`SOMLearning`对象中：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The learning rate determines our speed of learning. The learning radius, which
    can have a pretty dramatic affect on the visual output, determines the number
    of neurons to be updated relative to the distance from the winning neuron. The
    circle of the specified radius consists of neurons, and they are updated continually
    during the learning process. The closer a neuron is to the winning neuron, the
    more updating it will receive. Please note that if, during your experiments, you
    set this value to zero, then only the winning neurons' weights are updated and
    no others.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率决定了我们的学习速度。学习半径，它可以对视觉输出产生相当大的影响，决定了相对于获胜神经元的距离要更新的神经元数量。指定半径的圆圈由神经元组成，它们在学习过程中不断地被更新。一个神经元越接近获胜神经元，它将接收到的更新就越多。请注意，如果在你的实验中，你将此值设置为0，那么只有获胜神经元的权重将被更新，其他神经元则不会。
- en: 'Although we will have a very pretty visual effect to watch, we''ll still need
    to know what''s going on within our application, and that''s where RI comes in:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将有一个非常漂亮的视觉效果可以观看，但我们仍然需要了解我们应用程序内部的情况，这就是RI的作用所在：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: RI, as we mentioned earlier, has a watch panel that lets you continually track
    whatever variables you are interested in. In our case, we are interested in watching
    the learning rate, learning radius, and each RGB color that is randomized. All
    we need to do is supply the label and the value, and RI will do the rest, as we
    will see in a moment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RI，如我们之前提到的，有一个监视面板，让你可以持续跟踪你感兴趣的任何变量。在我们的情况下，我们感兴趣的是监视学习率、学习半径以及每个随机化的RGB颜色。我们只需要提供标签和值，RI就会完成剩下的工作，正如我们稍后将看到的。
- en: 'Finally, as they relate to RI, we want to see the RGB values in our message
    window as well, so we will add a debug message for that:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与RI相关，我们还想在我们的消息窗口中看到RGB值，因此我们将添加一个调试消息：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We now make a training `Run` for this iteration and pass to it the `RGBInput`
    array:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为这次迭代进行一次训练`Run`，并将`RGBInput`数组传递给它：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's talk about learning for a moment. As we mentioned, each iteration will
    try and learn more and more information. This learning iteration returns a learning
    error, which is the difference in the neurons' weights and the input vector `RGBInput`.
    As mentioned previously, the distance is measured according to the distance from
    the winning neuron. The process is as follows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时谈谈学习。正如我们提到的，每次迭代都会尝试学习越来越多的信息。这次学习迭代返回一个学习误差，即神经元权重与输入向量`RGBInput`之间的差异。如前所述，距离是根据获胜神经元（权重值与`RGBInput`中提供的值最接近的神经元）的距离来衡量的。过程如下。
- en: The trainer runs one learning iteration, finds the winning neuron (the neuron
    that has weights with values closest to those provided in the `RGBInput`), and
    updates its weights. It also updates the weights of the neighboring neurons. As
    each learning iteration occurs, the network gets closer and closer to the optimal
    solution.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练器运行一次学习迭代，找到获胜神经元（权重值与`RGBInput`中提供的值最接近的神经元），并更新其权重。它还更新邻近神经元的权重。随着每次学习迭代的进行，网络越来越接近最优解。
- en: Up next is a screenshot of our application running. In the background is RI,
    so that you can see how we are logging each iteration, what color values we are
    using when we update the map, as well as the learning rate and learning radius.
    As your machine learning programs and algorithms get more and more complex, you
    will realize that this kind of insight into your applications becomes incredibly
    invaluable. It is also an indispensable real-time debugging and diagnostic tool!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是应用程序运行的截图。背景是RI，这样你可以看到我们如何记录每个迭代，我们在更新地图时使用什么颜色值，以及学习率和学习半径。随着你的机器学习程序和算法变得越来越复杂，你会意识到这种对应用程序的洞察变得极其宝贵。它也是一个不可或缺的实时调试和诊断工具！
- en: '![](img/c51dd71d-0559-4a4c-b362-f46020cdd2d3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c51dd71d-0559-4a4c-b362-f46020cdd2d3.png)'
- en: Since SOM are, well, self-organizing, our second example is going to be more
    graphical. Our hope is that it will help you to better understand what's happening
    behind the scenes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SOM是自我组织的，我们的第二个例子将更加图形化。我们希望它能帮助你更好地理解幕后发生的事情。
- en: In this example, we'll again use AForge.NET and build a 2D plane of objects,
    organized into a few groups. We will, starting from a single location, visually
    arrive at the location of those shapes. This is conceptually the same as our color
    example, which used points in a 3D space, except that this time, our points are
    in 2D. The visualization occurs in the Map panel and is a top-down view of what
    is happening in 2D space, so as to get to a 1D graphical view.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们再次使用AForge.NET构建一个由几个组组织起来的二维对象平面。我们将从单个位置开始，直观地到达那些形状的位置。从概念上讲，这与我们的颜色示例相同，该示例使用了三维空间中的点，但这次我们的点是二维的。可视化发生在地图面板中，是二维空间中发生情况的俯视图，以便得到一维图形视图。
- en: Initially, neurons in the SOM grid start out at random positions, but they are
    gradually massaged into a molded outlining that is in the shape of our data. This
    is an iterative process, and although putting an animated `.gif` into the book
    is a feat that we have not yet achieved, I have taken screenshots at various points
    in the iteration to show you what happens. You can run the example for yourself
    to see it in real time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在SOM网格中，神经元最初处于随机位置，但它们逐渐被调整成与我们的数据形状相匹配的模具轮廓。这是一个迭代过程，尽管将动画`.gif`放入书中是一项我们尚未实现的壮举，但我已经在不同迭代点拍摄了快照，以展示发生了什么。你可以亲自运行示例，以实时查看。
- en: 'We start out with all our objects in the positions on the left. We will run
    through 500 iterations to show the evolution. We will go from a blank white panel
    to one that, hopefully, resembles the Points panel:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从所有对象在左侧的位置开始。我们将运行500次迭代以展示演变。我们将从一个空白白色面板到一个，希望，类似于点面板的面板：
- en: '![](img/06f51405-b689-445d-bf65-9daa6ffb480d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/06f51405-b689-445d-bf65-9daa6ffb480d.png)'
- en: 'Now we click on the Start button and away it goes! You will see the points
    beginning to organize themselves by moving to their correct locations, which (hopefully)
    will mirror that of the Points we have specified:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们点击“开始”按钮，它就出发了！你会看到点开始移动到它们正确的位置，这（希望）将反映出我们指定的点：
- en: '![](img/a4b44d9d-0dd3-4959-b32e-2f26a7efa8a5.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a4b44d9d-0dd3-4959-b32e-2f26a7efa8a5.png)'
- en: 'After 199 iterations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 经过199次迭代后：
- en: '![](img/fd6dff9a-0d43-463b-9205-b174046a623d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fd6dff9a-0d43-463b-9205-b174046a623d.png)'
- en: 'After 343 iterations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 经过343次迭代后：
- en: '![](img/519621e9-f3fb-47ff-ada9-bdd13e615b6a.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/519621e9-f3fb-47ff-ada9-bdd13e615b6a.png)'
- en: And, after completion, you can see that the objects have organized themselves
    like the pattern that we initially created. If you imagine that you are looking
    down at the map, even though you are on a flat piece of paper, you can see the
    3D experience if you look hard enough. The blue dots are the active neurons, the
    light gray dots are the inactive neurons, and the lines drawn are the elastic
    connections between neurons.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你可以看到物体已经按照我们最初创建的图案组织起来。如果你想象自己正俯瞰地图，即使你站在一张平坦的纸上，只要你足够专注，你就能看到三维体验。蓝色的小点代表活跃的神经元，浅灰色的小点代表不活跃的神经元，画出的线条是神经元之间的弹性连接。
- en: 'The checkboxes below the map allow you to easily choose whether to display
    either or both of these:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 地图下方的复选框允许你轻松选择是否显示这些中的任何一个或两个：
- en: '![](img/1591c8f9-5cdd-4465-9d96-6f41c3b7d6ed.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1591c8f9-5cdd-4465-9d96-6f41c3b7d6ed.png)'
- en: 'If you take a screenshot with the connections and inactive neurons not shown,
    you will see that the organizational patters in the map arrive at the same clustering
    as our objective, which for us means success:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你截图时不显示连接和不活跃的神经元，你会看到地图上的组织模式达到了与我们的目标相同的聚类，对我们来说这意味着成功：
- en: '![](img/25a9b838-fc2f-437a-b441-fd510fa7b254.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/25a9b838-fc2f-437a-b441-fd510fa7b254.png)'
- en: 'How exactly all this works is the next topic we will investigate. As always,
    let''s take a look at our main execution loop. As you can see, we''ll be using
    the same `DistanceNetwork` and `SOMLearning` objects that we previously discussed:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切是如何工作的，是我们接下来要调查的主题。像往常一样，让我们看看我们的主要执行循环。正如你所看到的，我们将使用之前讨论过的相同的`DistanceNetwork`和`SOMLearning`对象：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we mentioned earlier, the `LearningRate` and `LearningRadius` continue to
    evolve through every iteration. This time, let's talk a bit about the `RunEpoch`
    method of the trainer. This method, although very simplistic, is designed to take
    a vector of input values and then return a learning error for that iteration (as
    you can now see, also sometimes called an **epoch**). It does this by calculating
    against each one of the input samples in the vector. The learning error is the
    absolute difference between the neurons' weights and inputs. The difference is
    measured according to the distance from the winning neuron. As mentioned earlier,
    we run this calculation against one learning iteration/epoch, find the winner,
    and update its weights (as well as neighbor weights). I should point out that
    when I say *winner*, I mean the neuron that has weights with values closest to
    the specified input vector, that is, the minimum distance from the network's input.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，`学习率`和`学习半径`会随着每一次迭代而不断进化。这次，让我们谈谈训练器的`RunEpoch`方法。这个方法虽然非常简单，但旨在接受一个输入值向量，然后返回该迭代的
    学习误差（正如你现在可以看到的，有时也称为**epoch**）。它是通过计算向量中的每个输入样本来实现的。学习误差是神经元权重和输入之间的绝对差异。差异是根据获胜神经元的距离来衡量的。如前所述，我们针对一次学习迭代/epoch进行计算，找到获胜者，并更新其权重（以及相邻权重的）。我应该指出，当我提到“获胜者”时，我指的是权重值最接近指定输入向量的神经元，即网络输入的最小距离。
- en: 'Next, we will highlight how we update the `map` itself; our calculated projects
    should match the initial input vector (points):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示如何更新“地图”本身；我们的计算项目应该与初始输入向量（点）相匹配：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from this code, we get the first layer, calculate `map` for all
    the neurons, collect the active neurons so that we can determine the winner, and
    then update `map`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中可以看到，我们获取第一层，计算所有神经元的`map`，收集活跃的神经元以便我们确定获胜者，然后更新`map`。
- en: 'Since we have talked about the winner so much, let me show you just how much
    code is involved in calculating the winner:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经多次提到了获胜者，让我向您展示一下计算获胜者所需的代码量：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That's it! All we are doing is looking for the index of the neuron whose weights
    have the minimum distance from the network's input.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们正在做的只是寻找权重与网络输入距离最小的神经元的索引。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to harness the power of SOMs and elastic neural
    networks. You've now officially crossed up the ladder from machine learning into
    neural networks; congratulations!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何利用SOMs（自组织映射）和弹性神经网络的强大功能。你现在已经正式从机器学习跨越到神经网络；恭喜你！
- en: In our next chapter, we will use some of our knowledge to start facial and motion
    detection programs and have some real fun! You are going to get to work with my
    associate for the chapter, Frenchie!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的章节中，我们将运用一些知识来开始面部和动作检测程序，并享受一些真正的乐趣！你将有机会与我的章节合作伙伴Frenchie一起工作！
