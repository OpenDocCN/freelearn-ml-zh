- en: Color Blending – Self-Organizing Maps and Elastic Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Self-Organizing Maps** (**SOM**), or **Kohonen maps** as you may have heard,
    are one of the basic types of self-organizing neural networks. The ability to
    self-organize provides adaptation to formerly unseen input data. It has been theorized
    as one of the most natural ways of learning, like that which is used by our brains,
    where no predefined patterns are thought to exist. Those patterns take shape during
    the learning process and are incredibly gifted at representing multidimensional
    data at a much lower level of dimensionality, such as 2D or 1D. Additionally,
    this network stores information in such a way that any topological relationships
    within the training set remain preserved.'
  prefs: []
  type: TYPE_NORMAL
- en: More formally, an SOM is a clustering technique that will help us uncover interesting
    data categories in large datasets. It's a type of unsupervised neural network
    where neurons are arranged in a single, two-dimensional grid. The grid must be
    rectangular, as in, a pure rectangle or a hexagon. Throughout the iterations (which
    we will specify), the neurons in our grid will gradually coalesce around areas
    with a higher density of data points (the left-hand side of our display called
    Points). As the neurons move, they bend and twist the grid until they move more
    closely to the points of interest and reflect the shape of that data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kohonen SOM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with AForge.NET
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood of an SOM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, the inevitable question now arises: how do these things work?'
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, we have neurons on the grid; gradually, via iterations, they
    adapt themselves to the shape of our data (in our example, shown in the following
    image on the left-hand side in the Points panel). Let's talk a bit more about
    the iterative process itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to randomly position data on the grid. We will randomly be
    placing our grid''s neurons in our data space, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d34f3e4a-3095-4a5e-8b75-ddd12ac87458.png)'
  prefs: []
  type: TYPE_IMG
- en: The second step is where our algorithm will select a single data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the third step, we need to find the neuron (data point) that is closest to
    the chosen data point. This then becomes our best matching unit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fourth step is to move our best-matching unit towards that data point. The
    distance that we move is determined by our learning rate, which will ultimately
    decrease after each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fifth, we will move the neighbors of our best-matching unit closer to it, with
    the farther-away neurons moving less than those that are closer. The Initial radius
    variable you see on the screen is what we use to identify neighbors. This value,
    just like the Initial learning rate, will decrease over time. If you have **ReflectInsight**
    (**RI**) up and running, you can watch the Initial learning rate decrease over
    time, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf11010d-c854-488e-8b8b-13aaf8fc1057.png)'
  prefs: []
  type: TYPE_IMG
- en: Our sixth and final step will be to update the Initial learning rate and Initial
    radius, as we have described so far, and then repeat. We will continue this process
    until our data points have stabilized and are in the correct position.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we've introduced you to a little bit of intuition on SOMs, let's talk
    a little more about what we're going to do in this chapter. We have chosen a very
    common mechanism for teaching our principals, which is the mapping of colors.
    The colors themselves are 3D objects represented by red, green, and blue, but
    we will be organizing them into two dimensions. There are two key points you will
    see here with the organization of colors. First, the colors are clustered into
    distinct regions, and second, regions of similar properties are usually found
    adjacent to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Our second example, which is a little bit more advanced, will use an **artificial
    neural network** (**ANN**) as we described before; this is an advanced form of
    machine learning, used to create an organizational mapping that matches the one
    presented to it. Let's look at our first example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a screenshot of our example. As you can see, we have a random pattern
    of colors, which, when finished, will be organized by clusters of similar colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcfdf5aa-87f9-4adf-bdff-75186da6c1b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we are successful—and we will be—here''s what our result should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8663f07-1bd1-41f1-97be-10f5583fc31f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us begin by following the steps of the process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start out by using 500 iterations to achieve our goal. Using a smaller
    number may not produce the blend that we are ultimately after. As an example,
    if we have used 500 iterations, here is what our result would look like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4d88fb40-b1fd-4d5d-b283-0d845b742520.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are far from being where we need to be. Being able to change
    the iterations allows you to experiment with exactly the right setting. I can
    tell you that 500 is higher than we need, so I will leave it as an exercise for
    you to figure out the number where the progression stops and you are satisfied
    with the organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After setting the number of iterations, all we must do is make sure we have
    the random color pattern we want, which can be achieved by clicking on the Randomize
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have the pattern that you want, you simply click on the Start button
    and watch the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you click on Start, the Stop button will be activated, and you can stop
    the progression anytime you like. The organization will automatically stop once
    you reach the number of iterations that you specified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we get into the actual code, let me show you some screenshots of what
    some organizational patterns look like. You can accomplish wonderful results by
    simply changing different parameters, which we will describe in detail further
    on. In the following screenshot, we have set the number of Iterations to 3000
    and the Initial radius as 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a02cae80-2113-447b-810b-a16822a87476.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot we are using 4000 iterations and an Initial radius
    of 18:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b84882b8-9de6-4959-b247-3f80a2eebe18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot we have set the number of Iterations to 4000 and
    the Initial radius as 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/997956c1-84ed-4704-8435-ee2c0682588e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have set the number of Iterations to 5000, the Initial learning rate
    as 0.3, and the Initial radius as 25, as shown in the following screenshot, to
    obtain the desired result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/207b50b9-5ac8-4967-958a-3f3c3c91a21f.png)'
  prefs: []
  type: TYPE_IMG
- en: As promised, let's now dive into the code.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are going to work with `AForge` and use the `DistanceNetwork`
    object. A distance network is a neural network of only a single distance. As well
    as being used for an SOM, it is used for an ElasticNet operation, which is what
    we will be using to show the elastic connections between objects during progression.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create our distance network using three input neurons and `1000` neurons
    that will be doing the work under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When you click on the Randomize button to randomize the colors, here''s what
    happens under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that the randomization range we are dealing with stays within
    the range of any color's red, green, or blue characteristic, which is `255`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at our learning loop, which looks like this. We''ll do a
    deep dive into it in a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look closer, the first object we create is an `SOMLearning` object. This
    object is optimized for square space learning, meaning it expects that the network
    it is working on has the same height as its width. This makes it easier to find
    the square root of the network''s neuron counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create variables to hold our red, green, and blue input colors,
    from which we will continually randomize the input colors in order to achieve
    our goals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we enter our `while` loop, we will continually update our variables until
    we reach the total number of iterations we selected. In this update loop, there
    are several things happening. First, we will update the learning rate and learning
    radius, and store it in our `SOMLearning` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The learning rate determines our speed of learning. The learning radius, which
    can have a pretty dramatic affect on the visual output, determines the number
    of neurons to be updated relative to the distance from the winning neuron. The
    circle of the specified radius consists of neurons, and they are updated continually
    during the learning process. The closer a neuron is to the winning neuron, the
    more updating it will receive. Please note that if, during your experiments, you
    set this value to zero, then only the winning neurons' weights are updated and
    no others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we will have a very pretty visual effect to watch, we''ll still need
    to know what''s going on within our application, and that''s where RI comes in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: RI, as we mentioned earlier, has a watch panel that lets you continually track
    whatever variables you are interested in. In our case, we are interested in watching
    the learning rate, learning radius, and each RGB color that is randomized. All
    we need to do is supply the label and the value, and RI will do the rest, as we
    will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as they relate to RI, we want to see the RGB values in our message
    window as well, so we will add a debug message for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We now make a training `Run` for this iteration and pass to it the `RGBInput`
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's talk about learning for a moment. As we mentioned, each iteration will
    try and learn more and more information. This learning iteration returns a learning
    error, which is the difference in the neurons' weights and the input vector `RGBInput`.
    As mentioned previously, the distance is measured according to the distance from
    the winning neuron. The process is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The trainer runs one learning iteration, finds the winning neuron (the neuron
    that has weights with values closest to those provided in the `RGBInput`), and
    updates its weights. It also updates the weights of the neighboring neurons. As
    each learning iteration occurs, the network gets closer and closer to the optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Up next is a screenshot of our application running. In the background is RI,
    so that you can see how we are logging each iteration, what color values we are
    using when we update the map, as well as the learning rate and learning radius.
    As your machine learning programs and algorithms get more and more complex, you
    will realize that this kind of insight into your applications becomes incredibly
    invaluable. It is also an indispensable real-time debugging and diagnostic tool!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c51dd71d-0559-4a4c-b362-f46020cdd2d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Since SOM are, well, self-organizing, our second example is going to be more
    graphical. Our hope is that it will help you to better understand what's happening
    behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll again use AForge.NET and build a 2D plane of objects,
    organized into a few groups. We will, starting from a single location, visually
    arrive at the location of those shapes. This is conceptually the same as our color
    example, which used points in a 3D space, except that this time, our points are
    in 2D. The visualization occurs in the Map panel and is a top-down view of what
    is happening in 2D space, so as to get to a 1D graphical view.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, neurons in the SOM grid start out at random positions, but they are
    gradually massaged into a molded outlining that is in the shape of our data. This
    is an iterative process, and although putting an animated `.gif` into the book
    is a feat that we have not yet achieved, I have taken screenshots at various points
    in the iteration to show you what happens. You can run the example for yourself
    to see it in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start out with all our objects in the positions on the left. We will run
    through 500 iterations to show the evolution. We will go from a blank white panel
    to one that, hopefully, resembles the Points panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06f51405-b689-445d-bf65-9daa6ffb480d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we click on the Start button and away it goes! You will see the points
    beginning to organize themselves by moving to their correct locations, which (hopefully)
    will mirror that of the Points we have specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4b44d9d-0dd3-4959-b32e-2f26a7efa8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After 199 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd6dff9a-0d43-463b-9205-b174046a623d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After 343 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/519621e9-f3fb-47ff-ada9-bdd13e615b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: And, after completion, you can see that the objects have organized themselves
    like the pattern that we initially created. If you imagine that you are looking
    down at the map, even though you are on a flat piece of paper, you can see the
    3D experience if you look hard enough. The blue dots are the active neurons, the
    light gray dots are the inactive neurons, and the lines drawn are the elastic
    connections between neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The checkboxes below the map allow you to easily choose whether to display
    either or both of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1591c8f9-5cdd-4465-9d96-6f41c3b7d6ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you take a screenshot with the connections and inactive neurons not shown,
    you will see that the organizational patters in the map arrive at the same clustering
    as our objective, which for us means success:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25a9b838-fc2f-437a-b441-fd510fa7b254.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How exactly all this works is the next topic we will investigate. As always,
    let''s take a look at our main execution loop. As you can see, we''ll be using
    the same `DistanceNetwork` and `SOMLearning` objects that we previously discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned earlier, the `LearningRate` and `LearningRadius` continue to
    evolve through every iteration. This time, let's talk a bit about the `RunEpoch`
    method of the trainer. This method, although very simplistic, is designed to take
    a vector of input values and then return a learning error for that iteration (as
    you can now see, also sometimes called an **epoch**). It does this by calculating
    against each one of the input samples in the vector. The learning error is the
    absolute difference between the neurons' weights and inputs. The difference is
    measured according to the distance from the winning neuron. As mentioned earlier,
    we run this calculation against one learning iteration/epoch, find the winner,
    and update its weights (as well as neighbor weights). I should point out that
    when I say *winner*, I mean the neuron that has weights with values closest to
    the specified input vector, that is, the minimum distance from the network's input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will highlight how we update the `map` itself; our calculated projects
    should match the initial input vector (points):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we get the first layer, calculate `map` for all
    the neurons, collect the active neurons so that we can determine the winner, and
    then update `map`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have talked about the winner so much, let me show you just how much
    code is involved in calculating the winner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That's it! All we are doing is looking for the index of the neuron whose weights
    have the minimum distance from the network's input.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to harness the power of SOMs and elastic neural
    networks. You've now officially crossed up the ladder from machine learning into
    neural networks; congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will use some of our knowledge to start facial and motion
    detection programs and have some real fun! You are going to get to work with my
    associate for the chapter, Frenchie!
  prefs: []
  type: TYPE_NORMAL
