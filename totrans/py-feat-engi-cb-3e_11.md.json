["```py\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n```", "```py\n    import pandas as pd\n    from sklearn.datasets import fetch_20newsgroups\n    ```", "```py\n    data = fetch_20newsgroups(subset='train')\n    df = pd.DataFrame(data.data, columns=['text'])\n    ```", "```py\n    df['num_char'] = df['text'].str.len()\n    ```", "```py\n    df['num_words'] = df['text'].str.split().str.len()\n    ```", "```py\n    df['num_vocab']df[\n        'text'].str.lower().str.split().apply(\n            set).str.len()\n    ```", "```py\n    df['lexical_div'] = df['num_words'] / df['num_vocab']\n    ```", "```py\n    df['ave_word_length'] = df[\n        'num_char'] / df['num_words']\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\n    df['target'] = data.target\n    ```", "```py\n    def plot_features(df, text_var):\n        nb_rows = 5\n        nb_cols = 4\n        fig, axs = plt.subplots(\n            nb_rows, nb_cols,figsize=(12, 12))\n        plt.subplots_adjust(wspace=None, hspace=0.4)\n        n = 0\n        for i in range(0, nb_rows):\n            for j in range(0, nb_cols):\n                axs[i, j].hist(\n                    df[df.target==n][text_var], bins=30)\n                axs[i, j].set_title(\n                    text_var + ' | ' + str(n))\n                     n += 1\n        plt.show()\n    ```", "```py\n    plot_features(df, 'num_words')\n    ```", "```py\n    import pandas as pd\n    from nltk.tokenize import sent_tokenize\n    from sklearn.datasets import fetch_20newsgroups\n    ```", "```py\n    text = \"\"\"\n    The alarm rang at 7 in the morning as it usually did on Tuesdays. She rolled over, stretched her arm, and stumbled to the button till she finally managed to switch it off. Reluctantly, she got up and went for a shower. The water was cold as the day before the engineers did not manage to get the boiler working. Good thing it was still summer.\n    Upstairs, her cat waited eagerly for his morning snack. Miaow! He voiced with excitement as he saw her climb the stairs.\n    \"\"\"\n    ```", "```py\n    sent_tokenize(text)\n    ```", "```py\n['\\nThe alarm rang at 7 in the morning as it usually did on Tuesdays.',\n 'She rolled over,\\nstretched her arm, and stumbled to the button till she finally managed to switch it off.',\n 'Reluctantly, she got up and went for a shower.',\n 'The water was cold as the day before the engineers\\ndid not manage to get the boiler working.',\n 'Good thing it was still summer.',\n 'Upstairs, her cat waited eagerly for his morning snack.',\n 'Miaow!',\n 'He voiced with excitement\\nas he saw her climb the stairs.']\n```", "```py\n    len(sent_tokenize(text))\n    ```", "```py\n    data = fetch_20newsgroups(subset='train')\n    df = pd.DataFrame(data.data, columns=['text'])\n    ```", "```py\n    df = df.loc[1:10]\n    ```", "```py\n    df['text'] = df['text'].str.split('Lines:').apply(\n        lambda x: x[1])\n    ```", "```py\n    df['num_sent'] = df['text'].apply(\n        sent_tokenize).apply(len)\n    ```", "```py\n    import pandas as pd\n    from sklearn.datasets import fetch_20newsgroups\n    from sklearn.feature_extraction.text import (\n        CountVectorizer\n    )\n    ```", "```py\n    data = fetch_20newsgroups(subset='train')\n    df = pd.DataFrame(data.data, columns=['text'])\n    ```", "```py\n    df['text'] = df['text'].str.replace(\n        ‹[^\\w\\s]›,››, regex=True).str.replace(\n        ‹\\d+›,››, regex=True)\n    ```", "```py\n    vectorizer = CountVectorizer(\n        lowercase=True,\n        stop_words='english',\n        ngram_range=(1, 1),\n        min_df=0.05)\n    ```", "```py\n    vectorizer.fit(df['text'])\n    ```", "```py\n    X = vectorizer.transform(df['text'])\n    ```", "```py\n    bagofwords = pd.DataFrame(\n        X.toarray(),\n        columns = vectorizer.get_feature_names_out()\n    )\n    ```", "```py\n    import pandas as pd\n    from sklearn.datasets import fetch_20newsgroups\n    from sklearn.feature_extraction.text import (\n        TfidfVectorizer\n    )\n    ```", "```py\n    data = fetch_20newsgroups(subset='train')\n    df = pd.DataFrame(data.data, columns=['text'])\n    ```", "```py\n    df['text'] = df['text'].str.replace(\n        ‹[^\\w\\s]›,››, regex=True).str.replace(\n        '\\d+','', regex=True)\n    ```", "```py\n    vectorizer = TfidfVectorizer(\n        lowercase=True,\n        stop_words='english',\n        ngram_range=(1, 1),\n        min_df=0.05)\n    ```", "```py\n    vectorizer.fit(df['text'])\n    ```", "```py\n    X = vectorizer.transform(df['text'])\n    ```", "```py\n    tfidf = pd.DataFrame(\n        X.toarray(),\n        columns = vectorizer.get_feature_names_out()\n    )\n    ```", "```py\n    import pandas as pd\n    from nltk.corpus import stopwords\n    from nltk.stem.snowball import SnowballStemmer\n    from sklearn.datasets import fetch_20newsgroups\n    ```", "```py\n    data = fetch_20newsgroups(subset='train')\n    df = pd.DataFrame(data.data, columns=['text'])\n    ```", "```py\n    df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n    ```", "```py\n    df['text'] = df['text'].str.replace(\n        '\\d+', '', regex=True)\n    ```", "```py\n    df['text'] = df['text'].str.lower()\n    ```", "```py\n    def remove_stopwords(text):\n        stop = set(stopwords.words('english'))\n        text = [word\n        for word in text.split() if word not in stop]\n        text = ‹ ‹.join(x for x in text)\n        return text\n    ```", "```py\n    df['text'] = df['text'].apply(remove_stopwords)\n    ```", "```py\n    stemmer = SnowballStemmer(\"english\")\n    ```", "```py\n    def stemm_words(text):\n        text = [\n            stemmer.stem(word) for word in text.split()\n        ]\n        text = ‹ ‹.join(x for x in text)\n        return text\n    ```", "```py\n    df['text'] = df['text'].apply(stemm_words)\n    ```", "```py\n    irwincmptrclonestarorg irwin arnstein subject recommend duc summari what worth distribut usa expir sat may gmt organ computrac inc richardson tx keyword ducati gts much line line ducati gts model k clock run well paint bronzebrownorang fade leak bit oil pop st hard accel shop fix tran oil leak sold bike owner want think like k opinion pleas email thank would nice stabl mate beemer ill get jap bike call axi motor tuba irwin honk therefor computracrichardsontx irwincmptrclonestarorg dod r\n    ```"]