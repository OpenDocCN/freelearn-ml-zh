- en: 'Chapter 3: Unsupervised Graph Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：无监督图学习
- en: Unsupervised machine learning refers to the subset of machine learning algorithms
    that do not exploit any target information during training. Instead, they work
    on their own to find clusters, discover patterns, detect anomalies, and solve
    many other problems for which there is no teacher and no correct answer known
    *a priori*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习指的是机器学习算法的一个子集，在训练过程中不利用任何目标信息。相反，它们独立工作以找到聚类、发现模式、检测异常，并解决许多其他问题，对于这些问题没有教师和事先已知的正确答案。
- en: As per many other machine learning algorithms, unsupervised models have found
    large applications in the graph representation learning domain. Indeed, they represent
    an extremely useful tool for solving various downstream tasks, such as node classification
    and community detection, among others.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多其他机器学习算法一样，无监督模型在图表示学习领域得到了广泛的应用。事实上，它们是解决各种下游任务（如节点分类和社区检测等）的极其有用的工具。
- en: In this chapter, an overview of recent unsupervised graph embedding methods
    will be provided. Given a graph, the goal of these techniques is to automatically
    learn a latent representation of it, in which the key structural components are
    somehow preserved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将提供最近无监督图嵌入方法的概述。给定一个图，这些技术的目标是自动学习其潜在表示，其中关键的结构组件以某种方式得到保留。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The unsupervised graph embedding roadmap
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督图嵌入路线图
- en: Shallow embedding methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浅层嵌入方法
- en: Autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Graph neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图神经网络
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter notebooks with Python 3.9 for all of our exercises.
    The following is a list of the Python libraries that need to be installed for
    this chapter using `pip`. For example, run `pip install networkx==2.5` on the
    command line, and so on:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 3.9 的 Jupyter 笔记本来进行所有练习。以下是需要使用 `pip` 安装此章节所需的 Python 库的列表。例如，在命令行中运行
    `pip install networkx==2.5`，等等：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the rest of this book, if not clearly stated, we will refer to the Python
    commands `import networkx` as `nx`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，除非明确说明，否则我们将引用 Python 命令 `import networkx` 作为 `nx`。
- en: All the code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与本章相关的代码文件均可在[https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03)找到。
- en: The unsupervised graph embedding roadmap
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督图嵌入路线图
- en: 'Graphs are complex mathematical structures defined in a non-Euclidean space.
    Roughly speaking, this means that it is not always easy to define what is close
    to what; it might also be hard to say what *close* even means. Imagine a social
    network graph: two users can be respectively connected and yet share very different
    features—one might be interested in fashion and clothes, while the other might
    be interested in sports and videogames. Can we consider them as "close"?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图是非欧几里得空间中定义的复杂数学结构。粗略地说，这意味着并不总是容易定义什么是接近什么；甚至可能很难说“接近”这个词究竟是什么意思。想象一下社交网络图：两个用户可能分别连接，但共享非常不同的特征——一个可能对时尚和服装感兴趣，而另一个可能对体育和电子游戏感兴趣。我们能认为他们是“接近”的吗？
- en: For this reason, unsupervised machine learning algorithms have found large applications
    in graph analysis. Unsupervised machine learning is the class of machine learning
    algorithms that can be trained without the need for manually annotated data. Most
    of those models indeed make use of only information in the adjacency matrix and
    the node features, without any knowledge of the downstream machine learning task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无监督机器学习算法在图分析中得到了广泛的应用。无监督机器学习是机器学习算法的一个类别，可以在没有手动标注数据的情况下进行训练。这些模型中的大多数实际上只使用邻接矩阵和节点特征中的信息，而不了解下游机器学习任务。
- en: 'How is this possible? One of the most used solutions is to learn embeddings
    that preserve the graph structure. The learned representation is usually optimized
    so that it can be used to reconstruct the pair-wise node similarity, for example,
    the **adjacency matrix**. These techniques bring an important feature: the learned
    representation can encode latent relationships among nodes or graphs, allowing
    us to discover hidden and complex novel patterns.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？最常用的解决方案之一是学习保留图结构的嵌入。学习到的表示通常被优化，以便它可以用来重建成对节点相似度，例如，**邻接矩阵**。这些技术带来一个重要特性：学习到的表示可以编码节点或图之间的潜在关系，使我们能够发现隐藏和复杂的新的模式。
- en: 'Many algorithms have been developed in relation to unsupervised graph machine
    learning techniques. However, as previously reported by different scientific papers
    ([https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)), those
    algorithms can be grouped into macro-groups: shallow embedding methods, autoencoders,
    and **Graph Neural Networks** (**GNNs**), as graphically described in the following
    chart:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督图机器学习技术相关联的许多算法已经被开发出来。然而，如不同科学论文先前所报告的（[https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)），这些算法可以被分为几个宏观组：浅层嵌入方法、自动编码器和**图神经网络**（**GNNs**），如下面的图表所示：
- en: '![Figure 3.1 – The hierarchical structure of the different unsupervised embedding
    algorithms described in this book](img/B16069_03_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 本书所述的不同无监督嵌入算法的层次结构](img/B16069_03_01.jpg)'
- en: Figure 3.1 – The hierarchical structure of the different unsupervised embedding
    algorithms described in this book
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 本书所述的不同无监督嵌入算法的层次结构
- en: In the following sections, you will learn the main principles behind each group
    of algorithms. We will try to provide the idea behind the most well-known algorithms
    in the field as well as how they can be used for solving real problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将了解每组算法背后的主要原理。我们将尝试提供该领域最著名算法背后的想法以及它们如何用于解决实际问题。
- en: Shallow embedding methods
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浅层嵌入方法
- en: As already introduced in [*Chapter 2*](B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035),
    *Graph Machine Learning*, with shallow embedding methods, we identify a set of
    algorithms that are able to learn and return only the embedding values for the
    learned input data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如已在[*第二章*](B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035)中介绍，*图机器学习*，使用浅层嵌入方法，我们识别出一组能够学习和仅返回学习输入数据的嵌入值的算法。
- en: 'In this section, we will explain in detail some of those algorithms. Moreover,
    we will enrich the descriptions by providing several examples of how to use those
    algorithms in Python. For all the algorithms described in this section, we will
    use the implementation provided in the following libraries: **Graph Embedding
    Methods** (**GEM**), **Node to Vector** (**Node2Vec**), and Karate Club.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细解释其中的一些算法。此外，我们将通过提供几个如何在Python中使用这些算法的示例来丰富描述。本节中描述的所有算法，我们将使用以下库中提供的实现：**图嵌入方法**（**GEM**）、**节点到向量**（**Node2Vec**）和Karate
    Club。
- en: Matrix factorization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: Matrix factorization is a general decomposition technique widely used in different
    domains. A consistent number of graph embedding algorithms use this technique
    in order to compute the node embedding of a graph.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解是一种在多个领域广泛使用的通用分解技术。许多图嵌入算法使用这种技术来计算图的节点嵌入。
- en: We will start by providing a general introduction to the matrix factorization
    problem. After the introduction of the basic principles, we will describe two
    algorithms, namely **Graph Factorization** (**GF**) and **Higher-Order Proximity
    Preserved Embedding** (**HOPE**), which use matrix factorization to build the
    node embedding of a graph.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先提供一个关于矩阵分解问题的通用介绍。在介绍基本原理之后，我们将描述两个算法，即**图分解**（**GF**）和**高阶邻近保持嵌入**（**HOPE**），它们使用矩阵分解来构建图的节点嵌入。
- en: Let ![](img/B16069_03_001.png) be the input data. Matrix factorization decomposes
    ![](img/B16069_03_002.png) with ![](img/B16069_03_003.png) and ![](img/B16069_03_004.png)
    called the **source** and **abundance** matrix, respectively, and ![](img/B16069_03_005.png)
    is the number of dimensions of the generated embedding space. The matrix factorization
    algorithm learns the *V* and *H* matrices by minimizing a loss function that can
    change according to the specific problem we want to solve. In its general formulation,
    the loss function is defined by computing the reconstruction error using the Frobenius
    norm as ![](img/B16069_03_006.png).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, all the unsupervised embedding algorithms based on matrix
    factorization use the same principle. They all factorize an input graph expressed
    as a matrix in different components. The main difference between each method lies
    in the loss function used during the optimization process. Indeed, different loss
    functions allow creating an embedding space that emphasizes specific properties
    of the input graph.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Graph factorization
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GF algorithm was one of the first models to reach good computational performance
    in order to perform the node embedding of a given graph. By following the principle
    of matrix factorization that we previously described, the GF algorithm factorizes
    the adjacency matrix of a given graph.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let ![](img/B16069_03_007.png) be the graph we want to compute the
    node embedding with and let ![](img/B16069_03_008.png) be its adjacency matrix.
    The loss function (*L*) used in this matrix factorization problem is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_009.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/B16069_03_010.png) represents one of the
    edges in *G* while ![](img/B16069_03_011.png) is the matrix containing the *d*-dimensional
    embedding. Each row of the matrix represents the embedding of a given node. Moreover,
    a regularization term (![](img/B16069_03_012.png)) of the embedding matrix is
    used to ensure that the problem remains well-posed even in the absence of sufficient
    data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The loss function used in this method was mainly designed to improve GF performances
    and scalability. Indeed, the solution generated by this method could be noisy.
    Moreover, it should be noted, by looking at its matrix factorization formulation,
    that GF performs a strong symmetric factorization. This property is particularly
    suitable for undirected graphs, where the adjacency matrix is symmetric, but could
    be a potential limitation for undirected graphs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the GEM library:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding example, the following have been done:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '`networkx` is used to generate a **barbell graph** (*G*) used as input for
    the GF factorization algorithm.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `GraphFactorization` class is used to generate a `d=2`-dimensional embedding
    space.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The computation of the node embeddings of the input graph is performed using
    `gf.learn_embedding(G)`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The computed embeddings are extracted by calling the `gf.get_embedding()` method.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results of the previous code are shown in the following graph:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Application of the GF algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)](img/B16069_03_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Application of the GF algorithm to a graph (left) to generate the
    embedding vector of its nodes (right)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3.2*, it is possible to see how nodes belonging to groups 1 and
    3 are mapped together in the same region of space. Those points are separated
    by the nodes belonging to group 2\. This mapping allows us to well separate groups
    1 and 3 from group 2\. Unfortunately, there is no clear separation between groups
    1 and 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Higher-order proximity preserved embedding
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HOPE is another graph embedding technique based on the matrix factorization
    principle. This method allows preserving higher-order proximity and does not force
    its embeddings to have any symmetric properties. Before starting to describe the
    method, let''s understand what first-order proximity and high-order proximity
    mean:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**First-order proximity**: Given a graph, ![](img/B16069_03_013.png), where
    the edges have a weight, ![](img/B16069_03_014.png), for each vertex pair ![](img/B16069_03_015.png),
    we say they have a first-order proximity equal to ![](img/B16069_03_016.png) if
    the edge ![](img/B16069_03_017.png). Otherwise, the first-order proximity between
    the two nodes is 0.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second- and high-order proximity**: With the second-order proximity, we can
    capture the two-step relations between each pair of vertices. For each vertex
    pair ![](img/B16069_03_018.png), we can see the second-order proximity as a two-step
    transition from ![](img/B16069_03_019.png) to ![](img/B16069_03_020.png). High-order
    proximity generalizes this concept and allows us to capture a more global structure.
    As a consequence, high-order proximity can be viewed as a k-step (*k* ≥ 3) transition
    from ![](img/B16069_03_021.png) to ![](img/B16069_03_022.png).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the definition of proximity, we can now describe the HOPE method. Formally,
    let ![](img/B16069_03_023.png) be the graph we want to compute the embedding for
    and let ![](img/B16069_03_024.png) be its adjacency matrix. The loss function
    (*L*) used by this problem is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_025.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/B16069_03_026.png) is a similarity matrix
    generated from graph ![](img/B16069_03_027.png) and ![](img/B16069_03_028.png)
    and ![](img/B16069_03_029.png) are two embedding matrices representing a *d*-dimensional
    embedding space. In more detail, ![](img/B16069_03_030.png) represents the source
    embedding and ![](img/B16069_03_031.png) represents the target embedding.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: HOPE uses those two matrices in order to capture asymmetric proximity in directed
    networks where the direction from a source node and a target node is present.
    The final embedding matrix, ![](img/B16069_03_032.png), is obtained by simply
    concatenating, column-wise, the ![](img/B16069_03_033.png) and ![](img/B16069_03_034.png)
    matrices. Due to this operation, the final embedding space generated by HOPE will
    have ![](img/B16069_03_035.png) dimensions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: As we already stated, the ![](img/B16069_03_036.png) matrix is a similarity
    matrix obtained from the original graph, *G*. The goal of ![](img/B16069_03_037.png)
    is to obtain high-order proximity information. Formally, it is computed as ![](img/B16069_03_038.png),
    where ![](img/B16069_03_039.png) and ![](img/B16069_03_040.png) are both polynomials
    of matrices.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In its original formulation, the authors of HOPE suggested different ways to
    compute ![](img/B16069_03_041.png) and ![](img/B16069_03_042.png). Here we report
    a common and easy method to compute those matrices, **Adamic-Adar** (**AA**).
    In this formulation, ![](img/B16069_03_043.png)(the identity matrix) while ![](img/B16069_03_044.png),
    where ![](img/B16069_03_045.png) is a diagonal matrix computed as ![](img/B16069_03_046.png).
    Other formulations to compute ![](img/B16069_03_047.png)and ![](img/B16069_03_048.png)
    are the **Katz Index**, **Rooted PageRank** (**RPR**), and **Common Neighbors**
    (**CN**).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the GEM library:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code is similar to the one used for GF. The only difference is
    in the class initialization since here we use `HOPE`. According to the implementation
    provided by GEM, the `d` parameter, representing the dimension of the embedding
    space, will define the number of columns of the final embedding matrix, ![](img/B16069_03_049.png),
    obtained after the column-wise concatenation of ![](img/B16069_03_050.png) and
    ![](img/B16069_03_051.png).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'As a consequence, the number of columns of ![](img/B16069_03_052.png) and ![](img/B16069_03_053.png)
    is defined by the floor division (the `//` operator in Python) of the value assigned
    to `d`. The results of the code are shown in the following graph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Application of the HOPE algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)](img/B16069_03_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Application of the HOPE algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the graph is undirected and thus there is no difference between
    the source and target nodes. *Figure 3.3* shows the first two dimensions of the
    `embeddings` matrix representing ![](img/B16069_03_054.png). It is possible to
    see how the embedding space generated by HOPE provides, in this case, a better
    separation of the different nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Graph representation with global structure information
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph representation with global structure information (GraphRep), such as
    HOPE, allows us to preserve higher-order proximity without forcing its embeddings
    to have symmetric properties. Formally, let ![](img/B16069_03_055.png) be the
    graph we want to compute the node embeddings for and let ![](img/B16069_03_056.png)
    be its adjacency matrix. The loss function (*L*) used by this problem is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_057.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/B16069_03_058.png) is a matrix generated
    from graph *G* in order to get the *k*th order of proximity between nodes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_059.png) and ![](img/B16069_03_060.png) are two embedding
    matrices representing a *d*-dimensional embedding space of the *k*th order of
    proximity for the source and target nodes, respectively.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The ![](img/B16069_03_061.png) matrix is computed according to the following
    equation: ![](img/B16069_03_062.png). Here, ![](img/B16069_03_063.png) is a diagonal
    matrix known as the **degree matrix** computed using the following equation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_064.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: '![](img/B16069_03_065.png) represents the (one-step) probability transition
    matrix, where ![](img/B16069_03_066.png) is the probability of a transition from
    ![](img/B16069_03_067.png)to vertex ![](img/B16069_03_068.png) within one step.
    In general, for a generic value of *k*, ![](img/B16069_03_069.png) represents
    the probability of a transition from ![](img/B16069_03_070.png)to vertex ![](img/B16069_03_071.png)
    within *k* steps.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: For each order of proximity, *k*, an independent optimization problem is fitted.
    All the *k* embedding matrices generated are then column-wise concatenated to
    get the final source embedding matrices.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the `karateclub` library:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We initialize the `GraRep` class from the `karateclub` library. In this implementation,
    the `dimension` parameter represents the dimension of the embedding space, while
    the `order` parameter defines the maximum number of orders of proximity between
    nodes. The number of columns of the final embedding matrix (stored, in the example,
    in the `embeddings` variable) is `dimension*order`, since, as we said, for each
    proximity order an embedding is computed and concatenated in the final embedding
    matrix.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify, since two dimensions are computed in the example, `embeddings[:,:2]`
    represents the embedding obtained for *k*=1, `embeddings[:,2:4]` for *k*=2, and
    `embeddings[:,4:]` for *k*=3\. The results of the code are shown in the following
    graph:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: .f
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Application of the GraphRep algorithm to a graph (top) to generate
    the embedding vector of its nodes (bottom) for different values of k](img/B16069_03_04.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Application of the GraphRep algorithm to a graph (top) to generate
    the embedding vector of its nodes (bottom) for different values of k
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding graph, it is easy to see how different orders of proximity
    allow us to get different embeddings. Since the input graph is quite simple, in
    this case, already with *k*=1, a well-separated embedding space is obtained. To
    specify, the nodes belonging to groups 1 and 3 in all the proximity orders have
    the same embedding values (they are overlapping in the scatter plot).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we described some matrix factorization methods for unsupervised
    graph embedding. In the next section, we will introduce a different way to perform
    unsupervised graph embedding using skip-gram models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will provide a quick description of the skip-gram model.
    Since it is widely used by different embedding algorithms, a high-level description
    is needed to better understand the different methods. Before going deep into a
    detailed description, we will first give a brief overview.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The skip-gram model is a simple neural network with one hidden layer trained
    in order to predict the probability of a given word being present when an input
    word is present. The neural network is trained by building the training data using
    a text corpus as a reference. This process is described in the following chart:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Example of the generation of training data from a given corpus.
    In the filled boxes, the target word. In the dash boxes, the context words identified
    by a window size of length 2](img/B16069_03_05.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Example of the generation of training data from a given corpus.
    In the filled boxes, the target word. In the dash boxes, the context words identified
    by a window size of length 2
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The example described in *Figure 3.5* shows how the algorithm to generate the
    training data works. A *target* word is selected and a rolling window of fixed
    size *w* is built around that word. The words inside the rolling windows are known
    as *context* words. Multiple pairs of *(target word, context word)* are then built
    according to the words inside the rolling window.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is generated from the whole corpus, the skip-gram model
    is trained to predict the probability of a word being a context word for the given
    target. During its training, the neural network learns a compact representation
    of the input words. This is why the skip-gram model is also known as **Word to
    Vector** (**Word2Vec**).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the neural network representing the skip-gram model is described
    in the following chart:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Structure of the neural network of the skip-gram model. The
    number of d neurons in the hidden layer represents the final size of the embedding
    space](img/B16069_03_06.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Structure of the neural network of the skip-gram model. The number
    of d neurons in the hidden layer represents the final size of the embedding space
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The input of the neural network is a binary vector of size *m*. Each element
    of the vector represents a word in the dictionary of the language we want to embed
    the words in. When, during the training process, a *(target word, context word)*
    pair is given, the input array will have 0 in all its entries with the exception
    of the entry representing the "target" word, which will be equal to 1\. The hidden
    layer has *d* neurons. The hidden layer will learn the embedding representation
    of each word, creating a *d*-dimensional embedding space.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输入是一个大小为 *m* 的二进制向量。向量的每个元素代表我们想要嵌入的单词语言字典中的一个单词。当在训练过程中给出一个 *(目标词，上下文词)*
    对时，输入数组在其所有条目中都将为0，除了表示“目标”词的条目，它将等于1。隐藏层有 *d* 个神经元。隐藏层将学习每个单词的嵌入表示，创建一个 *d*-维嵌入空间。
- en: Finally, the output layer of the neural network is a dense layer of *m* neurons
    (the same size as the input vector) with a *softmax* activation function. Each
    neuron represents a word of the dictionary. The value assigned by the neuron corresponds
    to the probability of that word being "related" to the input word. Since softmax
    can be hard to compute when the size of *m* increases, a *hierarchical softmax*
    approach is always used.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经网络的输出层是一个包含 *m* 个神经元（与输入向量大小相同）的密集层，并使用 *softmax* 激活函数。每个神经元代表字典中的一个单词。神经元分配的值对应于该单词与输入单词“相关”的概率。由于当
    *m* 的大小增加时softmax可能难以计算，因此通常使用**层次softmax**方法。
- en: 'The final goal of the skip-gram model is not to actually learn the task we
    previously described but to build a compact *d*-dimensional representation of
    the input words. Thanks to this representation, it is possible to easily extract
    an embedding space for the words using the weight of the hidden layer. Another
    common approach to creating a skip-gram model, which will be not described here,
    is *context-based*: **Continuous Bag-of-Words** (**CBOW**).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram模型最终的目标不是真正学习我们之前描述的任务，而是构建输入单词的紧凑 *d*-维表示。得益于这种表示，可以很容易地使用隐藏层的权重提取单词的嵌入空间。另一种常见的创建skip-gram模型的方法（这里将不描述），是基于**连续词袋**（**CBOW**）。
- en: Since the basic concepts behind the skip-gram model have been introduced, we
    can start to describe a series of unsupervised graph embedding algorithms built
    upon this model. Generally speaking, all the unsupervised embedding algorithms
    based on the skip-gram model use the same principle.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了skip-gram模型背后的基本概念之后，我们可以开始描述一系列基于此模型构建的无监督图嵌入算法。一般来说，所有基于skip-gram模型的无监督嵌入算法都使用相同的原理。
- en: Starting from an input graph, they extract from it a set of walks. Those walks
    can be seen as a text corpus where each node represents a word. Two words (representing
    nodes) are near each other in the text if they are connected by an edge in a walk.
    The main difference between each method lies in the way those walks are computed.
    Indeed, as we will see, different walk generation algorithms can emphasize particular
    local or global structures of the graph.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个输入图开始，他们从中提取出一组路径。这些路径可以看作是一个文本语料库，其中每个节点代表一个单词。在路径中通过边连接的两个单词（代表节点）在文本中彼此靠近。每种方法之间的主要区别在于计算这些路径的方式。实际上，正如我们将看到的，不同的路径生成算法可以强调图的特殊局部或全局结构。
- en: DeepWalk
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepWalk
- en: The DeepWalk algorithm generates the node embedding of a given graph using the
    skip-gram model. In order to provide a better explanation of this model, we need
    to introduce the concept of **random walks**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk算法使用skip-gram模型生成给定图的节点嵌入。为了更好地解释这个模型，我们需要介绍**随机游走**的概念。
- en: Formally, let ![](img/B16069_03_072.png) be a graph and let ![](img/B16069_03_073.png)
    be a vertex selected as the starting point. We select a neighbor of ![](img/B16069_03_074.png)
    at random and we move toward it. From this point, we randomly select another point
    to move. This process is repeated ![](img/B16069_03_075.png) times. The random
    sequence of ![](img/B16069_03_076.png) vertices selected in this way is a random
    walk of length ![](img/B16069_03_077.png). It is worth mentioning that the algorithm
    used to generate the random walks does not impose any constraint on how they are
    built. As a consequence, there is no guarantee that the local neighborhood of
    the node is well preserved.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the notion of random walk, the DeepWalk algorithm generates a random
    walk of a size of at most *t* for each node. Those random walks will be given
    as input to the skip-gram model. The embedding generated using skip-gram will
    be used as the final node embedding. In the following figure (*Figure 3.7*), we
    can see a step-by-step graphical representation of the algorithm:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – All the steps used by the DeepWalk algorithm to generate the
    node embedding of a given graph](img/B16069_03_07.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – All the steps used by the DeepWalk algorithm to generate the node
    embedding of a given graph
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a step-by-step explanation of the algorithm graphically described in
    the preceding chart:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Walk Generation**: For each node of input graph *G*, a set of ![](img/B16069_03_078.png)random
    walks with a fixed maximum length (*t*) is computed. It should be noted that the
    length *t* is an upper bound. There are no constraints forcing all the paths to
    have the same length.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Skip-Gram Training**: Using all the random walks generated in the previous
    step, a skip-gram model is trained. As we described earlier, the skip-gram model
    works on words and sentences. When a graph is given as input to the skip-gram
    model, as visible in *Figure 3.7*, a graph can be seen as an input text corpus,
    while a single node of the graph can be seen as a word of the corpus.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A random walk can be seen as a sequence of words (a sentence). The skip-gram
    is then trained using the "fake" sentences generated by the nodes in the random
    walk. The parameters for the skip-gram model previously described (window size,
    *w*, and embed size, *d*) are used in this step.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Embedding Generation**: The information contained in the hidden layers of
    the trained skip-gram model is used in order to extract the embedding of each
    node.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the `karateclub` library:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The code is quite simple. We initialize the `DeepWalk` class from the `karateclub`
    library. In this implementation, the `dimensions` parameter represents the dimension
    of the embedding space. Other parameters worth mentioning that the `DeepWalk`
    class accepts are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '`walk_number`: The number of random walks to generate for each node'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`walk_length`: The length of the generated random walks'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size`: The window size parameter of the skip-gram model'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the model is fitted on graph *G* using `dw.fit(G)` and the embeddings
    are extracted using `dw.get_embedding()`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the code are shown in the following figure:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Application of the DeepWalk algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)](img/B16069_03_08.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Application of the DeepWalk algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: From the previous graph, we can see how DeepWalk is able to separate region
    1 from region 3\. Those two groups are contaminated by the nodes belonging to
    region 2\. Indeed, for those nodes, a clear distinction is not visible in the
    embedding space.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Node2Vec
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Node2Vec** algorithm can be seen as an extension of DeepWalk. Indeed,
    as with DeepWalk, Node2Vec also generates a set of random walks used as input
    to a skip-gram model. Once trained, the hidden layers of the skip-gram model are
    used to generate the embedding of the node in the graph. The main difference between
    the two algorithms lies in the way the random walks are generated.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, if DeepWalk generates random walks without using any bias, in Node2Vec
    a new technique to generate biased random walks on the graph is introduced. The
    algorithm to generate the random walks combines graph exploration by merging **Breadth-First
    Search** (**BFS**) and **Depth-First Search** (**DFS**). The way those two algorithms
    are combined in the random walk's generation is regularized by two parameters,
    ![](img/B16069_03_079.png) and ![](img/B16069_03_080.png). ![](img/B16069_03_081.png)
    defines the probability of a random walk getting back to the previous node, while
    ![](img/B16069_03_082.png) defines the probability that a random walk can pass
    through a previously unseen part of the graph.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Due to this combination, Node2Vec can preserve high-order proximities by preserving
    local structures in the graph as well as global community structures. This new
    method of random walk generation allows solving the limitation of DeepWalk preserving
    the local neighborhood properties of the node.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the `node2vec` library:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Also, for Node2Vec, the code is straightforward. We initialize the `Node2Vec`
    class from the `node2vec` library. In this implementation, the `dimensions` parameter
    represents the dimension of the embedding space. The model is then fitted using
    `node2vec.fit(window=10)`. Finally, the embeddings are obtained using `model.wv`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that `model.wv` is an object of the `Word2VecKeyedVectors`
    class. In order to get the embedding vector of a specific node with `nodeid` as
    the ID, we can use the trained model, as follows: `model.wv[str(nodeId)]`. Other
    parameters worth mentioning that the `Node2Vec` class accepts are as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '`num_walks`: The number of random walks to generate for each node'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`walk_length`: The length of the generated random walks'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p, q`: The *p* and *q* parameters of the random walk''s generation algorithm'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results of the code are shown in *Figure 3.9*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Application of the Node2Vec algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)](img/B16069_03_09.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Application of the Node2Vec algorithm to a graph (left) to generate
    the embedding vector of its nodes (right)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: As is visible from *Figure 3.9*, Node2Vec allows us to obtain a better separation
    between nodes in the embedding space compared to DeepWalk. To specify, regions
    1 and 3 are well clustered in two regions of space. Region 2 instead is well placed
    in the middle of the two groups without any overlap.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Edge2Vec
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to the other embedding function, the **Edge to Vector** (**Edge2Vec**)
    algorithm generates the embedding space on edges, instead of nodes. This algorithm
    is a simple side effect of the embedding generated by using Node2Vec. The main
    idea is to use the node embedding of two adjacent nodes to perform some basic
    mathematical operations in order to extract the embedding of the edge connecting
    them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let ![](img/B16069_03_083.png) and ![](img/B16069_03_084.png) be
    two adjacent nodes and let ![](img/B16069_03_085.png) and ![](img/B16069_03_086.png)
    be their embeddings computed with Node2Vec. The operators described in *Table
    3.1* can be used in order to compute the embedding of their edge:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 3.1 – Edge embedding operators with their equation and class name in
    the Node2Vec library](img/B16069_03_Table_01.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Table 3.1 – Edge embedding operators with their equation and class name in the
    Node2Vec library
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will show how to perform the node embedding of a
    given `networkx` graph using Python and the Node2Vec library:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The code is quite simple. The `HadamardEmbedder` class is instantiated with
    only the `keyed_vectors` parameter. The value of this parameter is the embedding
    model generated by Node2Vec. In order to use other techniques to generate the
    edge embedding, we just need to change the class and select one from the ones
    listed in *Table 3.1*. An example of the application of this algorithm is shown
    in the following figure:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Application of the Edge2Vec algorithm to a graph (top) to generate
    the embedding vector of its nodes (bottom) using different methods](img/B16069_03_11.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Application of the Edge2Vec algorithm to a graph (top) to generate
    the embedding vector of its nodes (bottom) using different methods
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3.11*, we can see how different embedding methods generate completely
    different embedding spaces. `AverageEmbedder` and `HadamardEmbedder`, in this
    example, generate well-separated embeddings for regions 1, 2, and 3\.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: For `WeightedL1Embedder` and `WeightedL2Embedder`, however, the embedding space
    is not well separated since the edge embeddings are concentrated in a single region
    without showing clear clusters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Graph2Vec
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The methods we previously described generated the embedding space for each node
    or edge on a given graph. **Graph to Vector** (**Graph2Vec**) generalizes this
    concept and generates embeddings for the whole graph.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify, given a set of graphs, the Graph2Vec algorithms generate an embedding
    space where each point represents a graph. This algorithm generates its embedding
    using an evolution of the Word2Vec skip-gram model known as **Document to Vector**
    (**Doc2Vec**). We can graphically see a simplification of this model in *Figure
    3.12*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Simplified graphical representation of the Doc2Vec skip-gram
    model. The number of d neurons in the hidden layer represents the final size of
    the embedding space](img/B16069_03_12.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Simplified graphical representation of the Doc2Vec skip-gram model.
    The number of d neurons in the hidden layer represents the final size of the embedding
    space
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the simple Word2Vec, Doc2Vec also accepts another binary array representing
    the document containing the input word. Given a "target" document and a "target"
    word, the model then tries to predict the most probable "context" word with respect
    to the input "target" word and document.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of the Doc2Vec model, we can now describe the Graph2Vec
    algorithm. The main idea behind this method is to view an entire graph as a document
    and each of its subgraphs, generated as an ego graph (see [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014),
    *Getting Started with Graphs*) of each node, as words that comprise the document.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, a graph is composed of subgraphs as a document is composed
    of sentences. According to this description, the algorithm can be summarized into
    the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '**Subgraph generation**: A set of rooted subgraphs is generated around every
    node.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Doc2Vec training**: The Doc2Vec skip-gram is trained using the subgraphs
    generated by the previous step.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embedding generation**: The information contained in the hidden layers of
    the trained Doc2Vec model is used in order to extract the embedding of each node.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, as we already did in [*Chapter 2*](B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035),
    *Graph Machine Learning*, we will show how to perform the node embedding of a
    set of `networkx` graphs using Python and the `karateclub` library:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this example, the following have been done:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 20 Watts-Strogatz graphs have been generated with random parameters.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then initialize the `Graph2Vec` class from the `karateclub` library with
    two dimensions. In this implementation, the `dimensions` parameter represents
    the dimension of the embedding space.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is then fitted on the input data using `model.fit(Gs)`.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vector containing the embeddings is extracted using `model.get_embedding()`.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results of the code are shown in the following figure:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Application of the Graph2Vec algorithm to a graph (left) to
    generate the embedding vector of its nodes (right) using different methods](img/B16069_03_13.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Application of the Graph2Vec algorithm to a graph (left) to generate
    the embedding vector of its nodes (right) using different methods
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3.13*, it is possible to see the embedding space generated for
    the different graphs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we described different shallow embedding methods based on
    matrix factorization and the skip-gram model. However, in the scientific literature,
    a lot of unsupervised embedding algorithms exist, such as Laplacian methods. We
    refer those of you who are interested in exploring those methods to look at the
    paper *Machine Learning on Graphs: A Model and Comprehensive Taxonomy* available
    at [https://arxiv.org/pdf/2005.03675.pdf](https://arxiv.org/pdf/2005.03675.pdf).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We will continue our description of the unsupervised graph embedding method
    in the next sections. We will describe more complex graph embedding algorithms
    based on autoencoders.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders are an extremely powerful tool that can effectively help data
    scientists to deal with high-dimensional datasets. Although first presented around
    30 years ago, in recent years, autoencoders have become more and more widespread
    in conjunction with the general rise of neural network-based algorithms. Besides
    allowing us to compact sparse representations, they can also be at the base of
    generative models, representing the first inception of the famous **Generative
    Adversarial Network** (**GAN**), which is, using the words of Geoffrey Hinton:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '*"The most interesting idea in the last 10 years in machine learning"*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder is a neural network where the inputs and outputs are basically
    the same, but that is characterized by a small number of units in the hidden layer.
    Loosely speaking, it is a neural network that is trained to reconstruct its inputs
    using a significantly lower number of variables and/or degree of freedom.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Since an autoencoder does not need a labeled dataset, it can be seen as an
    example of unsupervised learning and a dimensionality-reduction technique. However,
    different from other techniques such as **Principal Component Analysis** (**PCA**)
    and matrix factorization, autoencoders can learn non-linear transformation thanks
    to the non-linear activation functions of their neurons:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Diagram of the autoencoder structure. The colors in the input
    and output layers represent the fact that the values should be as similar as possible.
    In fact, the training of the network is done in order to match these values and
    minimize the reconstruction error ](img/B16069_03_14.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Diagram of the autoencoder structure. The colors in the input
    and output layers represent the fact that the values should be as similar as possible.
    In fact, the training of the network is done in order to match these values and
    minimize the reconstruction error
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.14* shows a simple example of an autoencoder. You can see how the
    autoencoder can generally be seen as composed of two parts:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: An encoder network that processes the input through one or more units and maps
    it into an encoded representation that reduces the dimension of the inputs (under-complete
    autoencoders) and/or constrains its sparsity (over-complete regularized autoencoders)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder network that reconstructs the input signal from the encoded representation
    of the middle layer
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder-decoder structure is then trained to minimize the ability of the
    full network to reconstruct the input. In order to completely specify an autoencoder,
    we need a loss function. The error between the inputs and the outputs can be computed
    using different metrics and indeed the choice of the correct form for the "reconstruction"
    error is a critical point when building an autoencoder.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Some common choices for the loss functions that measure the reconstruction error
    are **mean square error**, **mean absolute error**, **cross-entropy**, and **KL
    divergence**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will show you how to build an autoencoder starting
    with some basic concepts and then applying those concepts to graph structures.
    But before diving in, we feel compelled to give you a very brief introduction
    to the frameworks that will allow us to do this: TensorFlow and Keras.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and Keras – a powerful combination
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Released as open source by Google in 2017, TensorFlow is now the standard, de
    facto framework that allows symbolic computations and differential programming.
    It basically allows you to build a symbolic structure that describes how inputs
    are combined in order to produce the outputs, defining what is generally called
    a **computational graph** or a **stateful dataflow graph**. In this graph, nodes
    are the variable (scalar, arrays, tensors) and edges represent operations connecting
    the inputs (edge source) to the output (edge target) of a single operation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, such a graph is static (this is indeed one of the main differences
    with respect to another very popular framework in this context: `torch`) and can
    be executed by feeding data into it, as inputs, clearing the "dataflow" attribute
    mentioned previously.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'By abstracting the computation, TensorFlow is a very general tool that can
    run on multiple backends: on machines powered by CPUs, GPUs, or even ad hoc, specifically
    designed processing units such as TPUs. Besides, TensorFlow-powered applications
    can also be deployed on different devices, ranging from single and distributed
    servers to mobile devices.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Besides abstracting computation, TensorFlow also allows you to symbolically
    differentiate your computational graph with respect to any of its variables, resulting
    in a new computational graph that can also be differentiated to produce higher-order
    derivatives. This approach is generally referred to as symbol-to-symbol derivative
    and it is indeed extremely powerful, especially in the context of the optimization
    of the generic loss function, which requires gradient estimations (such as gradient
    descent techniques).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: As you might know, the problem of optimizing a loss function with respect to
    many parameters is central in the training of any neural network via backpropagation.
    This is surely the main reason why TensorFlow has become very popular in the past
    few years and why it was designed and produced in the first place by Google.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Diving in depth into the usage of TensorFlow is beyond the scope of this book
    and indeed you can find out more through the description given in dedicated books.
    In the following sections, we will use some of its main functionalities and provide
    you with the basic tools for building neural networks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Since its last major release, 2.x, the standard way of building a model with
    TensorFlow is using the Keras API. Keras was natively a side external project
    with respect to TensorFlow, aimed at providing a common and simple API to use
    several differential programming frameworks, such as TensorFlow, Teano, and CNTK,
    for implementing a neural network model. It generally abstracts the low-level
    implementation of the computation graph and provides you with the most common
    layers used when building neural networks (although custom layers can also be
    easily implemented), such as the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent layers
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization layers
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras also exposes APIs that are very similar to scikit-learn, the most popular
    library for machine learning in the Python ecosystem, making it very easy for
    data scientists to build, train, and integrate neural network-based models in
    their applications.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will show you how to build and train an autoencoder
    using Keras. We'll start applying these techniques to images in order to progressively
    apply the key concepts to graph structures.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Our first autoencoder
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll start by implementing an autoencoder in its simplest form, that is, a
    simple feed-forward network trained to reconstruct its input. We'll apply this
    to the Fashion-MNIST dataset, which is a dataset similar to the famous MNIST dataset
    that features hand-written numbers on a black and white image.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: MNIST has 10 categories and consists of 60k + 10k (train dataset + test dataset)
    28x28 pixel grayscale images that represent a piece of clothing (`T-shirt`, `Trouser`,
    `Pullover`, `Dress`, `Coat`, `Sandal`, `Shirt`, `Sneaker`, `Bag`, and `Ankle boot`).
    The Fashion-MNIST dataset is a harder task than the original MNIST dataset and
    it is generally used for benchmarking algorithms.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is already integrated in the Keras library and can be easily imported
    using the following code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is usually good practice to rescale the inputs with an order of magnitude
    of around 1 (for which activation functions are most efficient) and make sure
    that the numerical data is in single-precision (32 bits) instead of double-precision
    (64 bits). This is due to the fact that it is generally desirable to promote speed
    rather than precision when training a neural network, which is a computationally
    expensive process. In certain cases, the precision could even be lowered to half-precision
    (16 bits). We transform the input with the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can grasp the type of inputs we are dealing with by plotting some of the
    samples from the training set using the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding code, `classes` represents the mapping between integers and
    class names, for example, `T-shirt`, `Trouser`, `Pullover`, `Dress`, `Coat`, `Sandal`,
    `Shirt`, `Sneaker`, `Bag`, and `Ankle boot`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Some samples taken from the training set of the Fashion-MNIST
    dataset](img/B16069_03_15.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Some samples taken from the training set of the Fashion-MNIST
    dataset
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have imported the inputs, we can build our autoencoder network
    by creating the encoder and the decoder. We will be doing this using the Keras
    functional API, which provides more generality and flexibility compared to the
    so-called Sequential API. We start by defining the encoder network:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our network is composed of a stack of three levels of the same pattern composed
    of the same two-layer building block:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '`Conv2D`, a two-dimensional convolutional kernel that is applied to the input
    and effectively corresponds to having weights shared across all the input neurons.
    After applying the convolutional kernel, the output is transformed using the ReLU
    activation function. This structure is replicated for *n* hidden planes, with
    *n* being 16 in the first stacked layer and 8 in the second and third stacked
    layers.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxPooling2D`, which down-samples the inputs by taking the maximum value over
    the specified window (2x2 in this case).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the Keras API, we can also have an overview of how the layers transformed
    the inputs using the `Model` class, which converts the tensors into a user-friendly
    model ready to be used and explored:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This provides a summary of the encoder network visible in *Figure 3.16*:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Overview of the encoder network'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_03_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – Overview of the encoder network
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen, at the end of the encoding phase, we have a (4, 4, 8) tensor,
    which is more than six times smaller than our original initial inputs (28x28).
    We can now build the decoder network. Note that the encoder and decoder do not
    need to have the same structure and/or shared weights:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, the decoder network resembles the encoder structure where the
    down-sampling of the input achieved using the `MaxPooling2D` layer has been replaced
    by the `UpSampling2D` layer, which basically repeats the input over a specified
    window (2x2 in this case, effectively doubling the tensor in each direction).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now fully defined the network structure with the encoder and decoder
    layers. In order to completely specify our autoencoder, we also need to specify
    a loss function. Moreover, to build the computational graph, Keras also needs
    to know which algorithms should be used in order to optimize the network weights.
    Both bits of information, the loss function and optimizer to be used, are generally
    provided to Keras when *compiling* the model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now finally train our autoencoder. Keras `Model` classes provide APIs
    that are similar to scikit-learn, with a `fit` method to be used to train the
    neural network. Note that, owing to the nature of the autoencoder, we are using
    the same information as the input and output of our network:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the training is finished, we can examine the ability of the network to
    reconstruct the inputs by comparing input images with their reconstructed version,
    which can be easily computed using the `predict` method of the Keras `Model` class
    as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In *Figure 3.17*, we show the reconstructed images. As you can see, the network
    is quite good at reconstructing unseen images, especially when considering the
    large-scale features. Details might have been lost in the compression (see, for
    instance, the logo on the t-shirts) but the overall relevant information has indeed
    been captured by our network:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Examples of the reconstruction done on the test set by the
    trained autoencoder'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_03_17.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Examples of the reconstruction done on the test set by the trained
    autoencoder
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also be very interesting to represent the encoded version of the images
    in a two-dimensional plane using T-SNE:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The coordinates provided by T-SNE are shown in *Figure 3.18*, colored by the
    class the sample belongs to. The clustering of the different clothing can clearly
    be seen, particularly for some classes that are very well separated from the rest:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – T-SNE transformation of the embeddings extracted from the test
    set, colored by the class that the sample belongs to](img/B16069_03_18.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – T-SNE transformation of the embeddings extracted from the test
    set, colored by the class that the sample belongs to
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are, however, rather prone to overfitting, as they tend to re-create
    exactly the images of the training and not generalize well. In the following subsection,
    we will see how overfitting can be prevented in order to build more robust and
    reliable dense representations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides allowing us to compress a sparse representation into a denser vector,
    autoencoders are also widely used to process a signal in order to filter out noise
    and extract only a relevant (characteristic) signal. This can be very useful in
    many applications, especially when identifying anomalies and outliers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Denoising autoencoders are a small variation of what has been implemented.
    As described in the previous section, basic autoencoders are trained using the
    same image as input and output. Denoising autoencoders corrupt the input using
    some noise of various intensity, while keeping the same noise-free target. This
    could be achieved by simply adding some Gaussian noise to the inputs:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The network can then be trained using the corrupted input, while for the output
    the noise-free image is used:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Such an approach is generally valid when datasets are large and when the risk
    of overfitting the noise is rather limited. When datasets are smaller, an alternative
    to avoid the network "learning" the noise as well (thus learning the mapping between
    a static noisy image to its noise-free version) is to add training stochastic
    noise using a `GaussianNoise` layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in this way, the noise may change between epochs and prevent the
    network from learning a static corruption superimposed to our training set. In
    order to do so, we change the first layers of our network in the following way:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The difference is that instead of having statically corrupted samples (that
    do not change in time), the noisy inputs now keep changing between epochs, thus
    avoiding the network learning the noise as well.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The `GaussianNoise` layer is an example of a regularization layer, that is,
    a layer that helps reduce overfitting of a neural network by inserting a random
    part in the network. `GaussianNoise` layers make models more robust and able to
    generalize better, avoiding autoencoders learning the identity function.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Another common example of a regularization layer is the dropout layers that
    effectively set to 0 some of the inputs (at random with a probability, ![](img/B16069_03_087.png))
    and rescale the other inputs by a ![](img/B16069_03_088.png) factor in order to
    (statistically) keep the sum over all the units constant, with and without dropout.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Dropout corresponds to randomly killing some of the connections between layers
    in order to reduce output dependence to specific neurons. You need to keep in
    mind that regularization layers are only active at training, while at test time
    they simply correspond to identity layers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 3.19*, we compare the network reconstruction of a noisy input (input)
    for the previous unregularized trained network and the network with a `GaussianNoise`
    layer. As can be seen (compare, for instance, the images of trousers), the model
    with regularization tends to develop stronger robustness and reconstructs the
    noise-free outputs:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Comparison with reconstruction for noisy samples. Top row:
    noisy input; middle row: reconstructed output using a vanilla autoencoder; bottom
    row: reconstructed output using a denoising autoencoder](img/B16069_03_19.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19 – Comparison with reconstruction for noisy samples. Top row: noisy
    input; middle row: reconstructed output using a vanilla autoencoder; bottom row:
    reconstructed output using a denoising autoencoder'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Regularization layers are often used when dealing with deep neural networks
    that tend to overfit and are able to learn identity functions for autoencoders.
    Often, dropout or `GaussianNoise` layers are introduced, repeating a similar pattern
    composed of regularization and learnable layers that we usually refer to as **stacked
    denoising layers**.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Graph autoencoders
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the basic concepts of autoencoders are understood, we can now turn to apply
    this framework to graph structures. If on one hand the network structure, decomposed
    into an encoder-decoder structure with a low-dimensional representation in between,
    still applies, the definition of the loss function to be optimized needs a bit
    of caution when dealing with networks. First, we need to adapt the reconstruction
    error to a meaningful formulation that can adapt to the peculiarities of graph
    structures. But to do so, let's first introduce the concepts of first- and higher-order
    proximity.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'When applying autoencoders to graph structures, the input and output of the
    network should be a graph representation, as, for instance, the adjacency matrix.
    The reconstruction loss could then be defined as the Frobenius norm of the difference
    between the input and output matrices. However, when applying autoencoders to
    such graph structures and adjacency matrices, two critical issues arise:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Whereas the presence of links indicates a relation or similarity between two
    vertices, their absence does not generally indicate a dissimilarity between vertices.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The adjacency matrix is extremely sparse and therefore the model will naturally
    tend to predict a 0 rather than a positive value.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address such peculiarities of graph structures, when defining the reconstruction
    loss, we need to penalize more errors done for the non-zero elements rather than
    that for zero elements. This can be done using the following loss function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_089.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16069_03_090.png) is the Hadamard element-wise product, where
    ![](img/B16069_03_091.png) if there is an edge between nodes ![](img/B16069_03_092.png)
    and ![](img/B16069_03_093.png), and 0 otherwise. The preceding loss guarantees
    that vertices that share a neighborhood (that is, their adjacency vectors are
    similar) will also be close in the embedding space. Thus, the preceding formulation
    will naturally preserve second-order proximity for the reconstructed graph.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, you can also promote first-order proximity in the reconstructed
    graph, thus enforcing connected nodes to be close in the embedding space. This
    condition can be enforced by using the following loss:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_094.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B16069_03_095.png) and ![](img/B16069_03_096.png) are the two
    representation of nodes ![](img/B16069_03_097.png) and ![](img/B16069_03_098.png)
    in the embedding space. This loss function forces neighboring nodes to be close
    in the embedding space. In fact, if two nodes are tightly connected, ![](img/B16069_03_099.png)
    will be large. As a consequence, their difference in the embedding space, ![](img/B16069_03_100.png),
    should be limited (indicating the two nodes are close in the embedding space)
    to keep the loss function small. The two losses can also be combined into a single
    loss function, where, in order to prevent overfitting, a regularization loss can
    be added that is proportional to the norm of the weight coefficients:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_101.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *W* represents all the weights used across the network.
    The preceding formulation was proposed in 2016 by Wang et al., and it is now known
    as **Structural Deep Network Embedding** (**SDNE**).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the preceding loss could also be directly implemented with TensorFlow
    and Keras, you can already find this network integrated in the GEM package we
    referred to previously. As before, extracting the node embedding can be done similarly
    in a few lines of code, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Although very powerful, these graph autoencoders encounter some issues when
    dealing with large graphs. For these cases, the input of our autoencoder is one
    row of the adjacency matrix that has as many elements as the nodes in the network.
    In large networks, this size can easily be of the order of millions or tens of
    millions.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we describe a different strategy for encoding the network
    information that in some cases may iteratively aggregate embeddings only over
    local neighborhoods, making it scalable to large graphs.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GNNs** are deep learning methods that work on graph-structured data. This
    family of methods is also known as **geometric deep learning** and is gaining
    increasing interest in a variety of applications, including social network analysis
    and computer graphics.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: According to the taxonomy defined in [*Chapter 2*](B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035),
    *Graph Machine Learning*, the encoder part takes as input both the graph structure
    and the node features. Those algorithms can be trained either with or without
    supervision. In this chapter, we will focus on unsupervised training, while the
    supervised setting will be explored in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with the concept of a **Convolutional Neural Network** (**CNN**),
    you might already know that they are able to achieve impressive results when dealing
    with regular Euclidean spaces, such as text (one-dimensional), images (two-dimensional),
    and videos (three-dimensional). A classic CNN consists of a sequence of layers
    and each layer extracts multi-scale localized spatial features. Those features
    are exploited by deeper layers to construct more complex and highly expressive
    representations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, it has been observed that concepts such as multi-layer and
    locality are also useful for processing graph-structured data. However, graphs
    are defined over a *non-Euclidean space*, and finding a generalization of a CNN
    for graphs is not straightforward, as described in *Figure 3.20*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Visual difference between Euclidean and non-Euclidean neighborhoods'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_03_20.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – Visual difference between Euclidean and non-Euclidean neighborhoods
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The original formulation of GNN was proposed by Scarselli et al. back in 2009\.
    It relies on the fact that each node can be described by its features and its
    neighborhood. Information coming from the neighborhood (which represents the concept
    of locality in the graph domain) can be aggregated and used to compute more complex
    and high-level features. Let's understand in more detail how it can be done.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning, each node, ![](img/B16069_03_102.png), is associated with
    a state. Let''s start with a random embedding, ![](img/B16069_03_103.png) (ignoring
    node attributes for simplicity). At each iteration of the algorithm, nodes accumulate
    input from their neighbors using a simple neural network layer:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_104.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16069_03_105.png) and ![](img/B16069_03_106.png) are trainable
    parameters (where *d* is the dimension of the embedding), ![](img/B16069_03_107.png)
    is a non-linear function, and *t* represents the *t*th iteration of the algorithm.
    The equation is applied recursively until a particular objective is reached. Notice
    that, at each iteration, the *previous state* (the state computed at the previous
    iteration) is exploited in order to compute that the new state has happened with
    *recurrent neural networks*.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Variants of GNNs
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from this first idea, several attempts have been made in recent years
    to re-address the problem of learning from graph data. In particular, variants
    of the previously described GNN have been proposed, with the aim of improving
    its representation learning capability. Some of them are specifically designed
    to process specific types of graphs (direct, indirect, weighted, unweighted, static,
    dynamic, and so on).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Also, several modifications have been proposed for the propagation step (convolution,
    gate mechanisms, attention mechanisms, and skip connections, among others), with
    the aim of improving representation at different levels. Also, different training
    methods have been proposed to improve learning.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with unsupervised representation learning, one of the most common
    approaches is to use an encoder to embed the graph (the encoder is formulated
    as one of the GNN variants) and then use a simple decoder to reconstruct the adjacency
    matrix. The loss function is usually formulated as the similarity between the
    original adjacency matrix and the reconstructed one. Formally, it can be defined
    as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_108.jpg)![](img/B16069_03_109.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16069_03_110.png) is the adjacency matrix representation and
    ![](img/B16069_03_111.png) is the matrix of node attributes. Another common variant
    of this approach, especially used when dealing with graph classification/representation
    learning, is to train against a *target distance*. The idea is to embed two pairs
    of graphs simultaneously obtaining a combined representation. The model is then
    trained such that this representation matches the distance. A similar strategy
    can be also adopted when dealing with node classification/representation learning
    by using a node similarity function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph Convolutional Neural Network** (**GCN**)-based encoders are one of
    the most diffused variants of GNN for unsupervised learning. GCNs are GNN models
    inspired by many of the basic ideas behind CNN. Filter parameters are typically
    shared over all locations in the graph and several layers are concatenated to
    form a deep network.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: There are essentially two types of convolutional operations for graph data,
    namely **spectral approaches** and **non-spectral** (**spatial**) approaches.
    The first, as the name suggests, defines convolution in the spectral domain (that
    is, decomposing graphs in a combination of simpler elements). Spatial convolution
    formulates the convolution as aggregating feature information from neighbors.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Spectral graph convolution
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral approaches are related to spectral graph theory, the study of the characteristics
    of a graph in relation to the characteristic polynomial, eigenvalues, and eigenvectors
    of the matrices associated with the graph. The convolution operation is defined
    as the multiplication of a signal (node features) by a kernel. In more detail,
    it is defined in the Fourier domain by determining the *eigendecomposition of
    the graph Laplacian* (think about the graph Laplacian as an adjacency matrix normalized
    in a special way).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: While this definition of spectral convolution has a strong mathematical foundation,
    the operation is computationally expensive. For this reason, several works have
    been done to approximate it in an efficient way. ChebNet by Defferrard et al.,
    for instance, is one of the first seminal works on spectral graph convolution.
    Here, the operation is approximated by using the concept of the Chebyshev polynomial
    of order *K* (a special kind of polynomial used to efficiently approximate functions).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Here, *K* is a very useful parameter because it determines the locality of the
    filter. Intuitively, for *K*=1, only the node features are fed into the network.
    With *K*=2, we average over two-hop neighbors (neighbors of neighbors) and so
    on.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/B16069_03_112.png) be the matrix of node features. In classical
    neural network processing, this signal would be composed of layers of the following
    form:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_113.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B16069_03_114.png) is the layer weights and ![](img/B16069_03_115.png)
    represents some non-linear activation function. The drawback of this operation
    is that it processes each node signal independently without taking into account
    connections between nodes. To overcome this limitation, a simple (yet effective)
    modification can be done, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_116.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: By introducing the adjacency matrix, ![](img/B16069_03_117.png), a new linear
    combination between each node and its corresponding neighbors is added. This way,
    the information depends only on the neighborhood and parameters are applied on
    all the nodes, simultaneously.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that this operation can be repeated in sequence several times,
    thus creating a deep network. At each layer, the node descriptors, *X*, will be
    replaced with the output of the previous layer, ![](img/B16069_03_118.png).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: The preceding presented equation, however, has some limitations and cannot be
    applied as it stands. The first limitation is that by multiplying by *A*, we consider
    all the neighbors of the node but not the node itself. This problem can be easily
    overcome by adding self-loops in the graph, that is, adding the ![](img/B16069_03_119.png)
    identity matrix.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The second limitation is related to the adjacency matrix itself. Since it is
    typically not normalized, we will observe large values in the feature representation
    of high-degree nodes and small values in the feature representation of low-degree
    nodes. This will lead to several problems during training since optimization algorithms
    are often sensitive to feature scale. Several methods have been proposed for normalizing
    *A*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kipf and Welling, 2017 (one of the well-known GCN models), for example,
    the normalization is performed by multiplying *A* by the *diagonal node degree
    matrix* *D*, such that all the rows sum to 1: ![](img/B16069_03_120.png). More
    specifically, they used symmetric normalization ![](img/B16069_03_121.png), such
    that the proposed propagation rule becomes as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069_03_122.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16069_03_123.png) is the diagonal node degree matrix of ![](img/B16069_03_124.png).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will create a GCN as defined in Kipf and Welling
    and we will apply this propagation rule for embedding a well-known network: a
    Zachary''s karate club graph:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, it is necessary to import all the Python modules. We will use `networkx`
    to load the *barbell graph*:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To implement the GC propagation rule, we need an adjacency matrix representing
    `G`. Since this network does not have node features, we will use the ![](img/B16069_03_125.png)
    identity matrix as the node descriptor:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We now add the self-loop and prepare the diagonal node degree matrix:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our GCN will be composed of two layers. Let''s define the layers'' weights
    and the propagation rule. Layer weights, *W*, will be initialized using *Glorot
    uniform initialization* (even if other initialization methods can be also used,
    for example, by sampling from a Gaussian or uniform distribution):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, let''s create our network and compute the forward pass, that is, propagate
    the signal through the network:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`H3` now contains the embedding computed using the GCN propagation rule. Notice
    that we chose `2` as the number of outputs, meaning that the embedding is bi-dimensional
    and can be easily visualized. In *Figure 3.21*, you can see the output:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Application of the graph convolutional layer to a graph (left)
    to generate the embedding vector of its nodes (right)](img/B16069_03_21.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Application of the graph convolutional layer to a graph (left)
    to generate the embedding vector of its nodes (right)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: You can observe the presence of two quite well-separated communities. This is
    a nice result, considering that we have not trained the network yet!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Spectral graph convolution methods have achieved noteworthy results in many
    domains. However, they present some drawbacks. Consider, for example, a very big
    graph with billions of nodes: a spectral approach requires the graph to be processed
    simultaneously, which can be impractical from a computational point of view.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, spectral convolution often assumes a fixed graph, leading to poor
    generalization capabilities on new, different graphs. To overcome these issues,
    spatial graph convolution represents an interesting alternative.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Spatial graph convolution
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spatial graph convolutional networks perform the operations directly on the
    graph by aggregating information from spatially close neighbors. Spatial convolution
    has many advantages: weights can be easily shared across a different location
    of the graph, leading to a good generalization capability on different graphs.
    Furthermore, the computation can be done by considering subsets of nodes instead
    of the entire graph, potentially improving computational efficiency.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphSAGE is one of the algorithms that implement spatial convolution. One
    of the main characteristics is its ability to scale over various types of networks.
    We can think of GraphSAGE as composed of three steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '**Neighborhood sampling**: For each node in a graph, the first step is to find
    its k-neighborhood, where *k* is defined by the user for determining how many
    hops to consider (neighbors of neighbors).'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Aggregation**: The second step is to aggregate, for each node, the node features
    describing the respective neighborhood. Various types of aggregation can be performed,
    including average, pooling (for example, taking the best feature according to
    certain criteria), or an even more complicated operation, such as using recurrent
    units (such as LSTM).'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction**: Each node is equipped with a simple neural network that learns
    how to perform predictions based on the aggregated features from the neighbors.'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GraphSAGE is often used in supervised settings, as we will see in [*Chapter
    4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064), *Supervised Graph Learning*.
    However, by adopting strategies such as using a similarity function as the target
    distance, it can also be effective for learning embedding without explicitly supervising
    the task.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Graph convolution in practice
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, GNNs have been implemented in many machine learning and deep learning
    frameworks, including TensorFlow, Keras, and PyTorch. For the next example, we
    will be using StellarGraph, the Python library for machine learning on graphs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will learn about embedding vectors in an unsupervised
    manner, without a target variable. The method is inspired by Bai et al. 2019 and
    is based on the simultaneous embedding of pairs of graphs. This embedding should
    match a ground-truth distance between graphs:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the required Python modules:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will be using the `PROTEINS` dataset for this example, which is available
    in StellarGraph and consists of 1,114 graphs with 39 nodes and 73 edges on average
    for each graph. Each node is described by four attributes and belongs to one of
    two classes:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The next step is to create the model. It will be composed of two GC layers
    with 64 and 32 output dimensions followed by ReLU activation, respectively. The
    output will be computed as the Euclidean distance of the two embeddings:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It is now time to prepare the dataset for training. To each pair of input graphs,
    we will assign a similarity score. Notice that any notion of graph similarity
    can be used in this case, including graph edit distances. For simplicity, we will
    be using the distance between the spectrum of the Laplacian of the graphs:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, let''s compile and train the model. We will be using an adaptive moment
    estimation optimizer (Adam) with the learning rate parameter set to `1e-2`. The
    loss function we will be using is defined as the minimum squared error between
    the prediction and the ground-truth distance computed as previously. The model
    will be trained for 500 epochs:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After training, we are now ready to inspect and visualize the learned representation.
    Since the output is 32-dimensional, we need a way to qualitatively evaluate the
    embeddings, for example, by plotting them in a bi-dimensional space. We will use
    T-SNE for this purpose:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s plot the embeddings. In the plot, each point (embedded graph) is colored
    according to the corresponding label (blue=0, red=1). The results are visible
    in *Figure 3.22*:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – The PROTEINS dataset embedding using GCNs'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_03_22.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 – The PROTEINS dataset embedding using GCNs
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: This is just one of the possible methods for learning embeddings for graphs.
    More advanced solutions can be experimented with to better fit the problem of
    interest.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how unsupervised machine learning can be effectively
    applied to graphs to solve real problems, such as node and graph representation
    learning.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we first analyzed shallow embedding methods, a set of algorithms
    that are able to learn and return only the embedding values for the learned input
    data.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We then learned how autoencoder algorithms can be used to encode the input by
    preserving important information in a lower-dimensional space. We have also seen
    how this idea can be adapted to graphs, by learning about embeddings that allow
    us to reconstruct the pair-wise node/graph similarity.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced the main concepts behind GNNs. We have seen how well-known
    concepts, such as convolution, can be applied to graphs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will revise these concepts in a supervised setting.
    There, a target label is provided and the objective is to learn a mapping between
    the input and the output.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
