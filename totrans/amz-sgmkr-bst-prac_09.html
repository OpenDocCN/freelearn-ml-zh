<html><head></head><body>
		<div id="_idContainer088">
			<h1 id="_idParaDest-102"><a id="_idTextAnchor133"/>Chapter 7: Profile Training Jobs with Amazon SageMaker Debugger</h1>
			<p>Training <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models involves experimenting with multiple algorithms, with their hyperparameters typically crunching through large volumes of data. Training a model that yields optimal results is both a time- and compute-intensive task. Improved training time yields improved productivity and reduces overall training costs. </p>
			<p>Distributed training, as we discussed in <a href="B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Training and Tuning at Scale</em>, goes a long way in achieving improved training times by using a scalable compute cluster. However, monitoring training infrastructure to identify and debug resource bottlenecks is not trivial. Once a training job has been launched, the process becomes non-transparent, and you don't have much visibility into the model training process. Equally non-trivial is real-time monitoring to detect sub-optimal training jobs and stop them early to avoid wasting training time and resources.  </p>
			<p>Amazon SageMaker Debugger provides visibility into training jobs and the infrastructure a training job is executing on. Real-time training metrics such as <strong class="bold">learning gradients</strong> and <strong class="bold">network weights</strong> captured by SageMaker Debugger provide visibility into a training job in progress, so you can act on conditions such as <strong class="bold">vanishing gradients</strong> and <strong class="bold">overfitting</strong>. </p>
			<p>Debugger also monitors and provides reports about the system's resources such as CPU, GPU, and memory, providing you with insights into resource utilization and bottlenecks. Additionally, if you use TensorFlow or PyTorch for your deep learning training jobs, Debugger provides you with a view into framework metrics that can be used to speed up your training jobs.  </p>
			<p>By the end of this chapter, you will be able to use the capabilities of Amazon SageMaker Debugger and apply best practices to address challenges typical to debugging ML training. These challenges include identifying and reacting to sub-optimal training, gaining visibility into the resource utilization of the training infrastructure, and optimizing training framework parameters. You will also learn how to improve the training time and costs by applying detailed recommendations provided by SageMaker Debugger.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Amazon SageMaker Debugger essentials</li>
				<li>Real-time monitoring of training jobs using built-in and custom rules</li>
				<li>Gain insight into the training infrastructure and training framework</li>
			</ul>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor134"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you have not set up the data science environment for this book yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Science Environments</em>, which will walk you through the setup process.</p>
			<p>The code examples included in this book are available on GitHub at <a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter07">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter07</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor135"/>Amazon SageMaker Debugger essentials</h1>
			<p>In this <a id="_idIndexMarker276"/>section, you will learn about the basic terminology and capabilities of Amazon SageMaker Debugger. Using Debugger with your training jobs involves three high-level steps:  </p>
			<ol>
				<li><em class="italic">Configuring</em> the training job to use SageMaker Debugger.</li>
				<li><em class="italic">Analyzing</em> the collected tensors and metrics.</li>
				<li><em class="italic">Taking</em> action.</li>
			</ol>
			<p>The preceding points are illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B17249_07_01.jpg" alt="Figure 7.1 – Amazon SageMaker Debugger overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Amazon SageMaker Debugger overview</p>
			<p>As we dive into each one of these steps, we will introduce the necessary terminology.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor136"/>Configuring a training job to use SageMaker Debugger</h2>
			<p>The first<a id="_idIndexMarker277"/> step is to configure <a id="_idIndexMarker278"/>training jobs to use Amazon SageMaker Debugger. By now, you are familiar with using the <strong class="source-inline">Estimator</strong> object from SageMaker SDK to launch training jobs. To use Amazon SageMaker Debugger, you must enhance <strong class="source-inline">Estimator</strong> with three additional configuration parameters: <strong class="source-inline">DebuggerHookConfig</strong>, <strong class="source-inline">Rules</strong>, and <strong class="source-inline">ProfilerConfig</strong>. </p>
			<p>With <strong class="source-inline">DebuggerHookConfig</strong>, you can specify which debugging metrics to collect and where to store them, as shown in the following code block:</p>
			<p class="source-code">Estimator(</p>
			<p class="source-code">    …</p>
			<p class="source-code">    debugger_hook_config=DebuggerHookConfig(</p>
			<p class="source-code">        s3_output_path=bucket_path,  # Where the debug data is stored.</p>
			<p class="source-code">        collection_configs=[ # Organize data to collect into collections.</p>
			<p class="source-code">            CollectionConfig(</p>
			<p class="source-code">                name="metrics",</p>
			<p class="source-code">                parameters={</p>
			<p class="source-code">                    "save_interval": str(save_interval)</p>
			<p class="source-code">                }</p>
			<p class="source-code">            )</p>
			<p class="source-code">        ],</p>
			<p class="source-code">    ),</p>
			<p class="source-code">    ….</p>
			<p class="source-code">)</p>
			<p><strong class="source-inline">s3_output_path</strong> is the location where all the collected data is persisted. If this location is not specified, Debugger uses the default path, <strong class="source-inline">s3://&lt;output_path&gt;/debug-output/</strong>, where <strong class="source-inline">&lt;output_path&gt;</strong> is the output path of the <a id="_idIndexMarker279"/>SageMaker training job. The <strong class="source-inline">CollectionConfig</strong> list allows you to organize the debug data or tensors into collections for easier analysis. A tensor represents the state of a training <a id="_idIndexMarker280"/>network at a specific time during the training process. Data is collected at intervals, as specified by <strong class="source-inline">save_interval</strong>, which is the number of steps in a training run.</p>
			<p>How do you know which tensors to collect? SageMaker Debugger comes with a set of built-in collections to capture common training metrics such as <strong class="source-inline">weights</strong>, <strong class="source-inline">layers</strong>, and <strong class="source-inline">outputs</strong>. You can choose to collect all of the available tensors or a subset of them. In the preceding code sample, Debugger is gathering the <strong class="source-inline">metrics</strong> collection.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For a complete list of <a id="_idIndexMarker281"/>built-in collections, refer to <a href="https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection">https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection</a>.</p>
			<p>You can also c<a id="_idIndexMarker282"/>reate a custom <a id="_idIndexMarker283"/>collection of metrics to collect. In the following code block, Debugger captures all the metrics with <strong class="source-inline">relu</strong>, <strong class="source-inline">tanh</strong>, or <strong class="source-inline">weight</strong> in their names:</p>
			<p class="source-code"># Use Debugger CollectionConfig to create a custom collection</p>
			<p class="source-code">collection_configs=[</p>
			<p class="source-code">        CollectionConfig(</p>
			<p class="source-code">            name="custom_collection",</p>
			<p class="source-code">            parameters={</p>
			<p class="source-code">                "include_regex": ".*relu |.*tanh | *weight ",</p>
			<p class="source-code">        })</p>
			<p class="source-code">]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">While it may be tempting to collect all the tensors, this leads to collecting a lot of data, which increases training time, training costs, and storage costs. In this case, using a <strong class="source-inline">ReductionConfig</strong> allows you to save reduced tensors instead of saving the full tensor (<a href="https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection">https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection</a>).</p>
			<p>While <strong class="source-inline">DebuggerHookConfig</strong> allows you to configure and save tensors, a rule analyzes the tensors that are captured during the training for specific conditions such as <strong class="bold">loss not decreasing</strong>. SageMaker Debugger supports two different types of rules: <strong class="bold">built-in</strong> and <strong class="bold">custom</strong>. SageMaker Debugger comes with a set of built-in rules in Python that can detect and report common training problems such as overfitting, underfitting, and vanishing gradients. With custom rules, you write your own rules in Python for SageMaker Debugger to evaluate against the collected tensors.</p>
			<p>For example, in the<a id="_idIndexMarker284"/> following code <a id="_idIndexMarker285"/>block, Debugger collects tensors related to the <strong class="source-inline">metrics</strong> collection and evaluates the tensors to detect whether the training loss is reduced throughout the training process:</p>
			<p class="source-code">Estimator(</p>
			<p class="source-code">    …</p>
			<p class="source-code">    rules=[</p>
			<p class="source-code">        Rule.sagemaker(</p>
			<p class="source-code">            rule_configs.loss_not_decreasing(),</p>
			<p class="source-code">            rule_parameters={</p>
			<p class="source-code">                "collection_names": "metrics",</p>
			<p class="source-code">                "num_steps": str(save_interval * 2),</p>
			<p class="source-code">            },</p>
			<p class="source-code">        ),</p>
			<p class="source-code">    ],</p>
			<p class="source-code">)</p>
			<p>Finally, <strong class="source-inline">ProfilerConfig</strong> allows you to collect system metrics such as CPU, GPU, Memory, I/O, and framework metrics specific to the framework being used in your training job. For the system metrics, you must specify the time interval for which you want to collect metrics, while for framework metrics, you specify the starting step and the number of steps, as shown in the following code block:</p>
			<p class="source-code">Estimator(</p>
			<p class="source-code">    …</p>
			<p class="source-code">    profiler_config = ProfilerConfig(</p>
			<p class="source-code">        ## Monitoring interval in milliseconds</p>
			<p class="source-code">     system_monitor_interval_millis=500,       ## Start collecting metrics from step 2 and collect from the next 7 steps.</p>
			<p class="source-code">      framework_profile_params=FrameworkProfile(</p>
			<p class="source-code">    start_step=2, </p>
			<p class="source-code">    num_steps=7</p>
			<p class="source-code">)     )</p>
			<p>The<a id="_idIndexMarker286"/> following table <a id="_idIndexMarker287"/>summarizes the tensors and metrics that are collected by SageMaker. It shows the different types of metrics, examples of each type, and how to collect and use them:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/011.jpg" alt="Figure 7.2 – Tensors and metrics collected by SageMaker Debugger&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Tensors and metrics collected by SageMaker Debugger</p>
			<p>Using these<a id="_idIndexMarker288"/> configuration <a id="_idIndexMarker289"/>parameters, SageMaker Debugger collects quite a lot of information about your training jobs. But how do you ensure that the data that's been collected is secure?</p>
			<p>A best practice is to encrypt all the data in an S3 bucket, either with a key provided by AWS or your <a id="_idIndexMarker290"/>own key with <strong class="bold">customer-managed key</strong> (<strong class="bold">CMK</strong>). Additionally, the rules that have been configured are executed on isolated Debugger rule containers. The rule containers also execute in the same VPC as the training job and use the IAM role that's used by the training job.</p>
			<p>Once you are satisfied with your Debugger configuration, kick off training using <strong class="source-inline">estimator.fit()</strong>. Next, we will analyze the information that's collected by the Debugger during the training job.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor137"/>Analyzing the collected tensors and metrics</h2>
			<p>All tensors and <a id="_idIndexMarker291"/>metrics that are collected during training are persisted in S3. SageMaker Debugger uses a <strong class="source-inline">trial</strong> object to represent a single training run. A trial object consists of multiple steps, where each step represents a single batch of training data. At each step, a collected tensor has a specific value.   </p>
			<p>To access the tensor values, you get the path to the tensors from the estimator, create a trial, get the list of tensors, find out the steps where you have data for a specific tensor you are interested in, and view the values of the tensor.</p>
			<p>By following this path from the trial to the individual tensor values, you can manually query the tensor values, as shown in the following code block:</p>
			<p class="source-code">tensors_path = estimator.latest_job_debugger_artifacts_path()</p>
			<p class="source-code">print('S3 location of tensors is: ', tensors_path)</p>
			<p class="source-code">trial.tensor_names()</p>
			<p class="source-code">trial.tensor("feature_importance/cover/f1").values()</p>
			<p>You can visualize the tensor values that have been collected even further by using custom plot code in the notebook. The following diagram shows a visualization of the <strong class="bold">train-rmse</strong> and <strong class="bold">validation-rmse</strong> training metrics, which were collected during training:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B17249_07_03.jpg" alt="Figure 7.3 – Training and validation errors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Training and validation errors</p>
			<p>Note that you can <a id="_idIndexMarker292"/>also view the visualizations in SageMaker Studio. Additionally, if you have rules configured, Debugger automatically analyses the tensors to evaluate training conditions and trigger cloud watch alerts. Similarly, when you set the <strong class="source-inline">ProfileConfig</strong> parameter, a detailed profiler report is generated and saved in S3. Next, let's take a look at how to act on the rule results.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor138"/>Taking action</h2>
			<p>Rules evaluate the<a id="_idIndexMarker293"/> collected tensor data. As the rule evaluation's status changes during training, a CloudWatch Event is triggered. You can configure a CloudWatch rule to be triggered for the CloudWatch Event to automate actions in response to the issues found by the rules.  </p>
			<p>Additionally, you can use Debugger's built-in actions to automate the responses. The following code block shows how to use a combination of Debugger's built-in rules and actions to stop a training job if the loss is not continuously reduced during the training process:</p>
			<p class="source-code">built_rules=[</p>
			<p class="source-code">        #Check for loss not decreasing during training and stop the training job.</p>
			<p class="source-code">        Rule.sagemaker(</p>
			<p class="source-code">            rule_configs.loss_not_decreasing(),</p>
			<p class="source-code">            actions = (rule_configs.StopTraining())</p>
			<p class="source-code">        )</p>
			<p class="source-code">]</p>
			<p>On the other hand, when <a id="_idIndexMarker294"/>you have the <strong class="source-inline">ProfilerConfig</strong> parameter configured, a profiler report with a detailed analysis of system metrics and framework metrics is generated and persisted in S3. You can download, review, and apply recommendations to the profiler report. </p>
			<p>In the next two sections, you will learn how to automate responses to rule evaluations and implement recommendations from the profiler report.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor139"/>Real-time monitoring of training jobs using built-in and custom rules</h1>
			<p>In this section, you will<a id="_idIndexMarker295"/> use Debugger capabilities to monitor a job with built-in and custom rules to detect sub-optimal training conditions such as <strong class="source-inline">LossNotDecreasing</strong> and <strong class="source-inline">ExplodingGradients</strong>.</p>
			<p>SageMaker provides a set of built-in rules to identify common training issues such as <strong class="source-inline">class_imbalance</strong>, <strong class="source-inline">loss_no_decreasing</strong>, and <strong class="source-inline">overfitting</strong>.  </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The complete list of SageMaker built-in rules<a id="_idIndexMarker296"/> can be accessed here: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html</a>.</p>
			<p>The following code <a id="_idIndexMarker297"/>sample shows how to configure <strong class="source-inline">built_in</strong> rules with SageMaker Debugger:</p>
			<p class="source-code">#Specify the rules you want to run</p>
			<p class="source-code">built_in_rules=[</p>
			<p class="source-code">        #Check for loss not decreasing during training and stop the training job.</p>
			<p class="source-code">        Rule.sagemaker(</p>
			<p class="source-code">            rule_configs.loss_not_decreasing(),</p>
			<p class="source-code">     </p>
			<p class="source-code">            actions = (rule_configs.StopTraining())</p>
			<p class="source-code">        ),</p>
			<p class="source-code">        #Check for overfit, overtraining and stalled training</p>
			<p class="source-code">        Rule.sagemaker(rule_configs.overfit()),  </p>
			<p class="source-code">   Rule.sagemaker(rule_configs.overtraining()),       </p>
			<p class="source-code">   Rule.sagemaker(rule_configs.stalled_training_rule())     </p>
			<p class="source-code">]</p>
			<p class="source-code">#Create an estimator and pass in the built_in rules.</p>
			<p class="source-code">pt_estimator = PyTorch(</p>
			<p class="source-code">    ...</p>
			<p class="source-code">    rules = built_in_rules</p>
			<p class="source-code">)</p>
			<p>After calling fit, SageMaker starts one training job and one processing job for each configured built-in rule. The rule evaluation status is visible in the training logs in CloudWatch at regular intervals. You can also view the results of the rule execution programmatically using<a id="_idIndexMarker298"/> the following command: </p>
			<p class="source-code">pt_estimator.latest_training_job.rule_job_summary()</p>
			<p>The results from the built-in rules that have been configured should be similar to the following:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B17249_07_04.jpg" alt="Figure 7.4 – Summary of built-in rule execution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Summary of built-in rule execution</p>
			<p>By analyzing the rule summary, you can see that the <strong class="source-inline">LossNotDecreasing</strong> rule is triggered, as indicated by <strong class="source-inline">RuleEvaluationStatus</strong> – <strong class="source-inline">IssuesFound</strong>. Since the action that's been configured is used to stop the training job, you will notice that the training job is stopped before all epochs are executed. You can also see that the other built-in rules – <strong class="source-inline">Overfit</strong>, <strong class="source-inline">Overtraining</strong>, and <strong class="source-inline">StalledTrainingRule</strong> – were not triggered during training.</p>
			<p>Built-in rules are managed by AWS, freeing you from having to manage updates to rules. You simply plug them into the estimator. However, you may want to monitor a metric that is not included in the built-in rules, in which case you must configure a custom rule. A bit <a id="_idIndexMarker299"/>more work is involved with custom rules. For example, let's say you want to track if the gradients are becoming too large during training. To create a custom rule for this, you must extend the <strong class="source-inline">Rule</strong> interface provided by SageMaker Debugger.  </p>
			<p class="callout-heading">Note </p>
			<p class="callout">SageMaker provides two sets of Docker images for rules: one set for evaluating built-in rules and one set for evaluating custom rules. The <strong class="bold">Elastic container registry</strong> (<strong class="bold">ECR</strong>) URLs for <a id="_idIndexMarker300"/>these Docker images are available at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-docker-images-rules.html">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-docker-images-rules.html</a>.</p>
			<p>In the following example, the custom rule will work with the tensors that were collected using the <strong class="source-inline">gradients</strong> collection. The <strong class="source-inline">invoke_at_step</strong> method provides the logic to be executed. At each step, the mean value of the gradient is compared against a threshold. If the gradient value is greater than the threshold, the rule is triggered, as shown in the following code:</p>
			<p class="source-code">class CustomGradientRule(Rule):</p>
			<p class="source-code">    def __init__(self, base_trial, threshold=10.0):</p>
			<p class="source-code">        super().__init__(base_trial)</p>
			<p class="source-code">        self.threshold = float(threshold)</p>
			<p class="source-code">    def invoke_at_step(self, step):</p>
			<p class="source-code">        for tname in self.base_trial.tensor_names(collection="gradients"):</p>
			<p class="source-code">            t = self.base_trial.tensor(tname)</p>
			<p class="source-code">            abs_mean = t.reduction_value(step, "mean", abs=True)</p>
			<p class="source-code">            if abs_mean &gt; self.threshold:</p>
			<p class="source-code">                return True</p>
			<p class="source-code">        return False</p>
			<p>Next, define<a id="_idIndexMarker301"/> the custom rule, as follows:</p>
			<p class="source-code">custom_rule = Rule.custom(</p>
			<p class="source-code">    name='CustomRule', # used to identify the rule</p>
			<p class="source-code">    # rule evaluator container image</p>
			<p class="source-code">image_uri='759209512951.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest',    instance_type='ml.t3.medium',     source='rules/my_custom_rule.py', # path to the rule source file</p>
			<p class="source-code">    rule_to_invoke='CustomGradientRule', # name of the class to invoke in the rule source file</p>
			<p class="source-code">    volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance</p>
			<p class="source-code">    collections_to_save=[CollectionConfig("gradients")],</p>
			<p class="source-code">    # collections to be analyzed by the rule. since this is a first party collection we fetch it as above</p>
			<p class="source-code">    rule_parameters={</p>
			<p class="source-code">       #Threshold to compare the gradient value against</p>
			<p class="source-code">      "threshold": "20.0"     }</p>
			<p class="source-code">)</p>
			<p>Configure the custom rule in the estimator and call the <strong class="source-inline">fit</strong> method, as follows:</p>
			<p class="source-code">pt_estimator_custom = PyTorch(</p>
			<p class="source-code">    ….</p>
			<p class="source-code">    ## New parameter</p>
			<p class="source-code">    rules = [custom_rule]</p>
			<p class="source-code">)</p>
			<p class="source-code">estimator.fit(wait = False)</p>
			<p>After calling <strong class="source-inline">fit</strong>, Amazon <a id="_idIndexMarker302"/>SageMaker starts one training job and one processing job for each configured customer rule. The rule evaluation status is visible in the training logs in CloudWatch at regular intervals. Similar to the rule summary for <strong class="source-inline">built_in</strong> rules, you can view the custom rule summary using the following code:</p>
			<p class="source-code">pt_estimator.latest_training_job.rule_job_summary()</p>
			<p>Using a combination of built-in and custom rules, you can gain insight into the training process and proactively stop the training jobs, without having to run an ineffective training job to completion. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An example notebook that provides a complete walkthrough of using SageMaker Debugger's built-in and custom rules is provided in the following GitHub repository: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/debugger/weather-prediction-debugger-rules.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/debugger/weather-prediction-debugger-rules.ipynb</a>.</p>
			<p>In this section, you got an inside look at the training process and improved the training job based on issues that have been detected by built-in and custom rules. In the next section, you will learn how to gain insight into the infrastructure and framework that's used for training jobs.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor140"/>Gaining insight into the training infrastructure and training framework</h1>
			<p>In this section, you will<a id="_idIndexMarker303"/> learn how to gain visibility into the resource utilization of the training infrastructure and the training framework. You will also learn how to analyze and implement recommendations provided by the <a id="_idIndexMarker304"/>deep profiler capability of SageMaker Debugger.</p>
			<p>Debugger profiler provides you with visibility into the utilization of the infrastructure running ML training jobs on SageMaker. Debugger automatically monitors system resources such as CPU, GPU, network, I/O, and memory. Additionally, Debugger collects metrics specific to the training framework such as step duration, data loading, preprocessing, and operator runtime on CPU and GPU. You can decide to profile the training job in its entirety or just portions of it to collect the necessary framework metrics. </p>
			<p>In addition to collecting the system and framework metrics, behind the scenes, Debugger correlates these metrics automatically, which makes it easy for you to identify possible resource bottlenecks and perform root cause analysis. </p>
			<p>Let's explore this in detail with our example use case – predicting weather using PyTorch. Here, we will explore the system metrics, the framework metrics that are generated by the profiler, and look at implementing recommendations made by the profiler. This kind of deep profiling of training jobs includes the following high-level steps:</p>
			<ol>
				<li value="1">Training a PyTorch model for weather prediction with Debugger enabled.</li>
				<li>Analyzing and visualizing the system and framework metrics generated by the profiler.</li>
				<li>Analyzing the profiler report generated by SageMaker Debugger.</li>
				<li>Reviewing and implementing recommendations from the profiler report.</li>
				<li>Comparing the training jobs.</li>
			</ol>
			<p>Let's look at each of these steps in detail.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor141"/>Training a PyTorch model for weather prediction </h2>
			<p>First, we will train a <a id="_idIndexMarker305"/>deep learning model using the PyTorch framework. Because of the large volumes of data and the deep learning framework, we'll train on GPU instances. We will train on two <strong class="source-inline">ml.p3.2xlarge</strong> instances. Our infrastructure configuration will look as follows:</p>
			<p class="source-code">…</p>
			<p class="source-code">train_instance_type = "ml.p3.2xlarge" </p>
			<p class="source-code">instance_count = 2</p>
			<p>Next, let's define <strong class="source-inline">ProfilerConfig</strong> so that it can collect system and framework metrics:</p>
			<p class="source-code">profiler_config = ProfilerConfig(</p>
			<p class="source-code">    system_monitor_interval_millis=500,</p>
			<p class="source-code">    framework_profile_params=FrameworkProfile(start_step=2, num_steps=7)</p>
			<p class="source-code">)</p>
			<p>Now, we must configure the PyTorch estimator by using the infrastructure and profiler configuration as parameters: </p>
			<p class="source-code">pt_estimator = PyTorch(</p>
			<p class="source-code">    entry_point="train_pytorch.py",</p>
			<p class="source-code">    source_dir="code",</p>
			<p class="source-code">    role=sagemaker.get_execution_role(),</p>
			<p class="source-code">    instance_count=instance_count,</p>
			<p class="source-code">    instance_type=train_instance_type,</p>
			<p class="source-code">    framework_version="1.6",</p>
			<p class="source-code">    py_version="py3",</p>
			<p class="source-code">    volume_size=1024,</p>
			<p class="source-code">    # Debugger-specific parameters</p>
			<p class="source-code">    profiler_config=profiler_config,</p>
			<p class="source-code">)</p>
			<p>Now, let's start the <a id="_idIndexMarker306"/>training job with the <strong class="source-inline">fit()</strong> method:</p>
			<p class="source-code">estimator.fit(inputs, wait= False)</p>
			<p>In the next section, you will analyze and visualize the metrics generated by Debugger.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor142"/>Analyzing and visualizing the system and framework metrics generated by the profiler</h2>
			<p>Once the training <a id="_idIndexMarker307"/>job starts, Debugger starts collecting system and framework metrics. In this section, you will <a id="_idIndexMarker308"/>learn how to query, analyze, and visualize the collected metrics. </p>
			<p>First, let's look at how to analyze the collected metrics manually. The following code block shows how to query for system metrics:</p>
			<p class="source-code">#All collected metrics are persisted in S3.  Define path to the profiler artifacts</p>
			<p class="source-code">path = estimator.latest_job_profiler_artifacts_path()</p>
			<p class="source-code">#Create a reader for the system metrics</p>
			<p class="source-code">system_metrics_reader = S3SystemMetricsReader(path)</p>
			<p class="source-code">#Get the latest event</p>
			<p class="source-code">last_timestamp = system_metrics_reader.get_timestamp_of_latest_available_file()</p>
			<p class="source-code">events = system_metrics_reader.get_events(0, last_timestamp * 1000000)  # UTC time in microseconds</p>
			<p class="source-code">#Show the first system metric event collected</p>
			<p class="source-code">print(</p>
			<p class="source-code">    "Event name:",  events[0].name,</p>
			<p class="source-code">    "\nTimestamp:",  timestamp_to_utc(events[0].timestamp),</p>
			<p class="source-code">    "\nValue:", events[0].value,</p>
			<p class="source-code">)</p>
			<p>The preceding code block results in the following output, which shows the GPU of one of the training instances at a particular time:</p>
			<p class="source-code">Event name: gpu2 </p>
			<p class="source-code">Timestamp: 2021-07-02 18:44:20 </p>
			<p class="source-code">Value: 0.0</p>
			<p>The value of <strong class="source-inline">0.0</strong> indicates that this GPU is not being utilized. </p>
			<p>Similar to the <a id="_idIndexMarker309"/>system metrics, you can review framework metrics as well. The following code block <a id="_idIndexMarker310"/>shows how to query for framework metrics:</p>
			<p class="source-code">#Create a reader for the system metrics</p>
			<p class="source-code">framework_metrics_reader = S3AlgorithmMetricsReader(path)</p>
			<p class="source-code">framework_metrics_reader.refresh_event_file_list()</p>
			<p class="source-code">last_timestamp = framework_metrics_reader.get_timestamp_of_latest_available_file()</p>
			<p class="source-code">events = framework_metrics_reader.get_events(0, last_timestamp)</p>
			<p class="source-code">#We can inspect one of the recorded events to get the following:</p>
			<p class="source-code">print("Event name:", events[0].event_name, </p>
			<p class="source-code">      "\nStart time:", timestamp_to_utc(events[0].start_time/1000000000), </p>
			<p class="source-code">      "\nEnd time:", timestamp_to_utc(events[0].end_time/1000000000), </p>
			<p class="source-code">      "\nDuration:", events[0].duration, "nanosecond")</p>
			<p>The preceding code block results in the following, showing one of the framework metrics at a particular time:</p>
			<p class="source-code">Event name: embeddings.0 </p>
			<p class="source-code">Start time: 1970-01-19 19:27:42 </p>
			<p class="source-code">End time: 1970-01-19 19:27:42 </p>
			<p class="source-code">Duration: 141298 nanosecond</p>
			<p>Once the metrics <a id="_idIndexMarker311"/>have been collected, you can <a id="_idIndexMarker312"/>visualize them using a heat map or custom plots in the notebook. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For a more colorful visualization of the heat map and a more in-depth analysis of system and framework metrics, take a look at the following notebook: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb</a>.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor143"/>Analyzing the profiler report generated by SageMaker Debugger</h2>
			<p>In this section, we <a id="_idIndexMarker313"/>will download and review the profiler report that was generated by Debugger. SageMaker Debugger creates a detailed profiler report and saves it in an S3 bucket at <strong class="source-inline">s3://&lt;your bucket&gt; /&lt;job-name&gt;/profiler-output/</strong>. You can download the report directly from S3. In the following list, we will review a few sections of the downloaded report: </p>
			<ul>
				<li><strong class="bold">Training job summary</strong><p>This section of the report <a id="_idIndexMarker314"/>provides a detailed summary of the training job, including the start and end time of the job and the time that was spent on various phases of training. The following screenshot shows a sample of the training job's summary:</p></li>
			</ul>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B17249_07_05.jpg" alt="Figure 7.5 – Training job summary of the profiler report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Training job summary of the profiler report</p>
			<ul>
				<li><strong class="bold">System metrics summary</strong><p>This section<a id="_idIndexMarker315"/> of the report shows the<a id="_idIndexMarker316"/> resource utilization of the training nodes. The following screenshot shows CPU, GPU, memory utilization, I/O wait time, and the amount of data that was sent and received:</p></li>
			</ul>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B17249_07_06.jpg" alt="Figure 7.6 – System metrics summary of the profiler report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – System metrics summary of the profiler report</p>
			<ul>
				<li><strong class="bold">Framework metrics summary</strong><p>This section of <a id="_idIndexMarker317"/>the report starts <a id="_idIndexMarker318"/>by showing how much time the training job spent in the training and validation phases, as well as the time it spent waiting:</p></li>
			</ul>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B17249_07_07.jpg" alt="Figure 7.7 – Framework metrics summary of the profiler report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Framework metrics summary of the profiler report</p>
			<ul>
				<li><strong class="bold">Rules summary</strong><p>As the training job is<a id="_idIndexMarker319"/> running, Debugger executes a set of rules to profile the training process. This section of the <a id="_idIndexMarker320"/>profiler report summarizes all the debugger rules that have been evaluated, the description of the rule, the number of times each rule was triggered during training, the analysis, and recommendations for improving the training job. The following screenshot shows the rule summary in table format: </p></li>
			</ul>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B17249_07_08.jpg" alt="Figure 7.8 – Rules summary of the profiler report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Rules summary of the profiler report</p>
			<p>In addition to directly querying and visualizing the metrics, as well as downloading the profiler report in your notebook, you can use SageMaker Studio, which provides built-in visualizations for analyzing profiling insights. </p>
			<p>To access <a id="_idIndexMarker321"/>Debugger in Studio, follow these steps: </p>
			<ol>
				<li value="1">On the navigation pane, choose <strong class="bold">Components and registries</strong>.</li>
				<li>Choose <strong class="bold">Experiments and trails</strong>.</li>
				<li>Choose your training job (right-click).</li>
				<li>Choose <strong class="bold">Debugger Insights</strong> from the Debugger tab that opens.</li>
			</ol>
			<p>In the <strong class="bold">Debugger</strong> tab, you will see multiple sections. One of these sections is called <strong class="bold">Training job summary</strong>, as shown in the following screenshot. This built-in visualization shows training job details, such as the start time, end time, duration, and time spent in individual phases of training. The pie chart visualization shows the relative time spent by the training job in the initialization, training, and finalization phases:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B17249_07_09.jpg" alt="Figure 7.9 – Debugger visualization in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Debugger visualization in SageMaker Studio</p>
			<p>In this section, we reviewed a few sections of the downloaded profiler report at a high level. To explore the profiler report in more detail, please run through the notebook in our Git repository.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor144"/>Analyzing and implementing recommendations from the profiler report</h2>
			<p>Now that we have <a id="_idIndexMarker322"/>recommendations from the profiler, let's analyze and implement a recommendation to see if it leads to an improved training job. </p>
			<p>From the rules <a id="_idIndexMarker323"/>summary table in the preceding section, we can see that the rule triggered a maximum number of times during our training is <strong class="source-inline">LowGPUUtilization</strong>. This rule indicates that there is a possibility of bottlenecks occurring due to blocking calls and recommends changing the distributed training strategy or increasing the batch size. The next rule that was triggered the most times was <strong class="source-inline">BatchSize</strong>, which indicates that the GPU utilization could be low because of the smaller batch size. </p>
			<p>The recommendation from the profiler, based on this rule's execution, is to consider running on a smaller instance type and to increase the batch size. Let's combine the profiler recommendations from these two most triggered rules, run two new training jobs with different settings, and check the profiler reports for the new training jobs to see if there is any improvement.  </p>
			<p>We will run the first training job with the same infrastructure, <strong class="source-inline">()</strong>, but with an increased batch size, as shown in the following code block:</p>
			<p class="source-code">train_instance_type='ml.p3.2xlarge'</p>
			<p class="source-code">instance_count = 2</p>
			<p class="source-code">hyperparameters = {"batch_size": 1024}</p>
			<p>For the next<a id="_idIndexMarker324"/> training job, we will use <a id="_idIndexMarker325"/>smaller training instances, <strong class="source-inline">()</strong>, and increase the batch size:</p>
			<p class="source-code">training_job_name=</p>
			<p class="source-code">train_instance_type='ml.p2.8xlarge'</p>
			<p class="source-code">instance_count = 2</p>
			<p class="source-code">hyperparameters = {"batch_size": 1024}</p>
			<p>Using these two different configurations, run two different training jobs using <strong class="source-inline">estimator.fit()</strong>. Once the training jobs are complete, download and analyze the two profiler reports. </p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor145"/>Comparing the two training jobs</h2>
			<p>At this point, we have a total of <a id="_idIndexMarker326"/>three completed training jobs with different configurations. In this section, we'll compare the original training job to the two new training jobs we configured based on the recommendations from the profiler. When comparing these jobs, we will focus on the training time and the resulting training costs. The following table shows the initial and revised training job configurations, along with the training time, resource utilization, and cost comparisons:  </p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/02.jpg" alt="Figure 7.10 – Comparison of training jobs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Comparison of training jobs</p>
			<p>First, let's compare the <a id="_idIndexMarker327"/>original training job with the training job that uses the first revised training configuration. In the revised training configuration, the batch size is increased from <strong class="source-inline">64</strong> to <strong class="source-inline">1024</strong>. This configuration change decreased the training time by <strong class="source-inline">17637</strong> seconds; that is, from <strong class="source-inline">18262</strong> seconds to <strong class="source-inline">895</strong> seconds. Assuming that the training jobs were run in the us-west-2 region, the cost of <strong class="source-inline">p3.2xlarge</strong> is $3.825 at the time of writing. This leads to a cost saving of 26.67%. </p>
			<p>Similarly, if you compare the second revised training configuration, where we updated both the batch size and instance type to the original, the training time increased but the overall training cost improved by 65.36%. If you can tolerate a slight increase in the training time, you can save on training costs by implementing recommendations from the profiler.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An example notebook that provides a complete walkthrough of using the SageMaker Debugger profiler is provided in the following GitHub repository: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb</a>.</p>
			<p class="callout">The results that were discussed in this section are from using the full dataset for PyTorch training. In the notebook, you will have the chance to explore the same functionality but with a smaller dataset. </p>
			<p>In this section, we<a id="_idIndexMarker328"/> implemented a couple of recommendations from the profiler and saw considerable training improvements. There are still more recommendations that you can experiment with.</p>
			<p>Additionally, in this section, we focused on how to kick off an estimator with Debugger enabled. You can also attach a profiler to a running training job using <strong class="source-inline">estimator.enable_default_profiling()</strong>. Similarly, to enable Debugger's built-in rules, system monitoring, and framework profiling with customizable configuration parameters, use <strong class="source-inline">estimator.update_profiler()</strong>.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor146"/>Summary</h1>
			<p>In this chapter, you learned how to use the capabilities of Amazon SageMaker Debugger to gain visibility of the training process, training infrastructure, and training framework. This visibility allows you to react to typical training issues such as overfitting, training loss, and stopping the training jobs from running to completion, only to result in sub-optimal models. Using recommendations from the deep profiler capabilities of Amazon SageMaker, you learned how to improve training jobs with respect to training time and costs.</p>
			<p>Using the debugger capabilities discussed in this chapter, you can continuously improve your training jobs by tweaking the underlying ML framework parameters and the training infrastructure configurations for faster and cost-effective ML training. In the next chapter, you will learn how to manage trained models at scale.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor147"/>Further reading</h1>
			<p>For additional reading material, please review these references<a id="_idTextAnchor148"/>:</p>
			<ul>
				<li>Identify bottlenecks, improve resource utilization, and reduce ML training costs with the deep profiling feature in Amazon SageMaker Debugger:<p><a href="https://aws.amazon.com/blogs/machine-learning/identify-bottlenecks-improve-resource-utilization-and-reduce-ml-training-costs-with-the-new-profiling-feature-in-amazon-sagemaker-debugger/">https://aws.amazon.com/blogs/machine-learning/identify-bottlenecks-improve-resource-utilization-and-reduce-ml-training-costs-with-the-new-profiling-feature-in-amazon-sagemaker-debugger<span id="_idTextAnchor149"/>/</a></p></li>
				<li>ML Explainability with Amazon SageMaker Debugger:<p><a href="https://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/">https://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/</a></p></li>
			</ul>
		</div>
	</body></html>