- en: Chapter 9. Big Data Machine Learning – The Final Frontier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, we have seen an exponential growth in data generated by humans
    and machines. Varied sources, including home sensors, healthcare-related monitoring
    devices, news feeds, conversations on social media, images, and worldwide commerce
    transactions—an endless list—contribute to the vast volumes of data generated
    every day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Facebook had 1.28 billion daily active users in March 2017 sharing close to
    four million pieces of unstructured information as text, images, URLs, news, and
    videos (Source: Facebook). 1.3 billion Twitter users share approximately 500 million
    tweets a day (Source: Twitter). **Internet of Things** (**IoT**) sensors in lights,
    thermostats, sensor in cars, watches, smart devices, and so on, will grow from
    50 billion to 200 billion by 2020 (Source: IDC estimates). YouTube users upload
    300 hours of new video content every five minutes. Netflix has 30 million viewers
    who stream 77,000 hours of video daily. Amazon has sold approximately 480 million
    products and has approximately 244 million customers. In the financial sector,
    the volume of transactional data generated by even a single large institution
    is enormous—approximately 25 million households in the US have Bank of America,
    a major financial institution, as their primary bank, and together produce petabytes
    of data annually. Overall, it is estimated that the global Big Data industry will
    be worth 43 billion US dollars in 2017 (Source: [www.statista.com](http://www.statista.com)).'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the aforementioned companies and many more like them face the real problem
    of storing all this data (structured and unstructured), processing the data, and
    learning hidden patterns from the data to increase their revenue and to improve
    customer satisfaction. We will explore how current methods, tools and technology
    can help us learn from data in Big Data-scale environments and how as practitioners
    in the field we must recognize challenges unique to this problem space.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the characteristics of Big Data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big Data Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'General Big Data Framework:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data cluster deployments frameworks
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HortonWorks Data Platform (HDP)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudera CDH
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Elastic MapReduce (EMR)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft HDInsight
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data acquisition:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish-subscribe framework
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Source-sink framework
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL framework
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Message queueing framework
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom framework
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data storage:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop Distributed File System (HDFS)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data processing and preparation:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive and Hive Query Language (HQL)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time stream processing
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization and analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch Big Data Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'H2O:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O architecture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning in H2O
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Business problems
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning mapping
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments, results, and analysis
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark MLlib:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning in MLlib
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments, results, and analysis
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time Big Data Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scalable Advanced Massive Online Analysis (SAMOA):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SAMOA architecture
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning algorithms
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments, results, and analysis
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of Machine Learning
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the characteristics of Big Data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many characteristics of Big Data that are different than normal data.
    Here we highlight them as four *V*s that characterize Big Data. Each of these
    makes it necessary to use specialized tools, frameworks, and algorithms for data
    acquisition, storage, processing, and analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: One of the characteristic of Big Data is the size of the content,
    structured or unstructured, which doesn''t fit the storage capacity or processing
    power available on a single machine and therefore needs multiple machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: Another characteristic of Big Data is the rate at which the content
    is generated, which contributes to volume but needs to be handled in a time sensitive
    manner. Social media content and IoT sensor information are the best examples
    of high velocity Big Data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: This generally refers to multiple formats in which data exists,
    that is, structured, semi-structured, and unstructured and furthermore, each of
    them has different forms. Social media content with images, video, audio, text,
    and structured information about activities, background, networks, and so on,
    is the best example of where data from various sources must be analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: This refers to a wide variety of factors such as noise, uncertainty,
    biases, and abnormality in the data that must be addressed, especially given the
    volume, velocity, and variety of data. One of the key steps, as we will discuss
    in the context of Big Data Machine Learning, is processing and cleaning such "unclean"
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many have added other characteristics such as value, validity, and volatility
    to the preceding list, but we believe they are largely derived from the previous
    four.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the general flow and components that are required
    for Big Data Machine Learning. Although many of the components, such as data acquisition
    or storage, are not directly related to Machine Learning methodologies, they inevitably
    have an impact on the frameworks and processes. Giving a complete catalog of the
    available components and tools is beyond the scope of this book, but we will discuss
    the general responsibilities of the tasks involved and give some information on
    the techniques and tools available to accomplish them.
  prefs: []
  type: TYPE_NORMAL
- en: General Big Data framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general Big Data framework is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![General Big Data framework](img/B05137_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Big data framework'
  prefs: []
  type: TYPE_NORMAL
- en: The choice of how the Big Data framework is set up and deployed in the cluster
    is one of the decisions that affects the choice of tools, techniques, and cost.
    The data acquisition or collection component is the first step and it consists
    of several techniques, both synchronous and asynchronous, to absorb data into
    the system. Various techniques ranging from publish-subscribe, source-sink, relational
    database queries, and custom data connectors are available in the components.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage choices ranging from distributed filesystems such as HDFS to non-relational
    databases (NoSQL) are available based on various other functional requirements.
    NoSQL databases are described in the section on *Data Storage*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data preparation, or transforming the large volume of stored data so that it
    is consumable by the Machine Learning analytics, is an important processing step.
    This has some dependencies on the frameworks, techniques, and tools used in storage.
    It also has some dependency on the next step: the choice of Machine Learning analytics/frameworks
    that will be used. There are a wide range of choices for processing frameworks
    that will be discussed in the following sub-section.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that, in batch learning, the model is trained simultaneously on a number
    of examples that have been previously collected. In contrast to batch learning,
    in real-time learning model training is continuous, each new instance that arrives
    becoming part of a dynamic training set. See [Chapter 5](ch05.html "Chapter 5. Real-Time
    Stream Machine Learning"), *Real-Time Stream Machine Learning* for details. Once
    the data is collected, stored, and transformed based on the domain requirements,
    different Machine Learning methodologies can be employed, including batch learning,
    real-time learning, and batch-real-time mixed learning. Whether one selects supervised
    learning, unsupervised learning, or a combination of the two also depends on the
    data, the availability of labels, and label quality. These will be discussed in
    detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The results of analytics during the development stage as well as the production
    or runtime stage also need to be stored and visualized for humans and automated
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data cluster deployment frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many frameworks that are built on the core Hadoop (*References* [3])
    open source platform. Each of them provides a number of tools for the Big Data
    components described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Hortonworks Data Platform
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Hortonworks Data Platform** (**HDP**) provides an open source distribution
    comprising various components in its stack, from data acquisition to visualization.
    Apache Ambari is often the user interface used for managing services and provisioning
    and monitoring clusters. The following screenshot depicts Ambari used for configuring
    various services and the health-check dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hortonworks Data Platform](img/B05137_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Ambari dashboard user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Cloudera CDH
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Like HDP, Cloudera CDH (*References* [4]) provides similar services and Cloudera
    Services Manager can be used in a similar way to Ambari for cluster management
    and health checks, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cloudera CDH](img/B05137_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Cloudera Service Manager user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elastic MapReduce
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Amazon Elastic MapReduce (EMR) (*References* [5]) is another Big Data cluster,
    platform similar to HDP and Cloudera, which supports a wide variety of frameworks.
    EMR has two modes—**cluster mode** and **step execution mode**. In cluster mode,
    you choose the Big Data stack vendor EMR or MapR and in step execution mode, you
    give jobs ranging from JARs to SQL queries for execution. In the following screenshot,
    we see the interface for configuring a new cluster as well as defining a new job
    flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Amazon Elastic MapReduce](img/B05137_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Amazon Elastic MapReduce cluster management user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure HDInsight
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Microsoft Azure HDInsight (*References* [6]) is another platform that allows
    cluster management with most of the services that are required, including storage,
    processing, and Machine Learning. The Azure portal, as shown in the following
    screenshot, is used to create, manage, and help in learning the statuses of the
    various components of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Microsoft Azure HDInsight](img/B05137_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Microsoft Azure HDInsight cluster management user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the Big Data framework, the acquisition component plays an important role
    in collecting the data from disparate source systems and storing it in Big Data
    storage. Based on types of source and volume, velocity, functional, and performance-based
    requirements, there are a wide variety of acquisition frameworks and tools. We
    will describe a few of the most well-known frameworks and tools used to give readers
    some insight.
  prefs: []
  type: TYPE_NORMAL
- en: Publish-subscribe frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In publish-subscribe based frameworks, the publishing source pushes the data
    in different formats to the broker, which has different subscribers waiting to
    consume them. The publisher and subscriber are unaware of each other, with the
    broker mediating in between.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Kafka** (*References* [9]) and **Amazon Kinesis** are two well-known
    implementations that are based on this model. Apache Kafka defines the concepts
    of publishers, consumers, and topics—on which things get published and consumed—and
    a broker to manage the topics. Amazon Kinesis is built on similar concepts with
    producers and consumers connected through Kinesis streams, which are similar to
    topics in Kafka.'
  prefs: []
  type: TYPE_NORMAL
- en: Source-sink frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In source-sink models, sources push the data into the framework and the framework
    pushes the system to the sinks. Apache Flume (*References* [7]) is a well-known
    implementation of this kind of framework with a variety of sources, channels to
    buffer the data, and a number of sinks to store the data in the Big Data world.
  prefs: []
  type: TYPE_NORMAL
- en: SQL frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since many traditional data stores are in the form of SQL-based RDBMS, SQL-based
    frameworks provide a generic way to import the data from RDBMS and store it in
    Big Data, mainly in the HDFS format. Apache Sqoop (*References* [10]) is a well-known
    implementation that can import data from any JDBC-based RDBMS and store it in
    HDFS-based systems.
  prefs: []
  type: TYPE_NORMAL
- en: Message queueing frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Message queueing frameworks are push-pull based frameworks similar to publisher-subscriber
    systems. Message queues separate the producers and consumers and can store the
    data in the queue, in an asynchronous communication pattern. Many protocols have
    been developed on this such as Advanced Message Queueing Protocol (AMQP) and ZeroMQ
    Message Transfer Protocol (ZMTP). RabbitMQ, ZeroMQ, Amazon SQS, and so on, are
    some well-known implementations of this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Custom frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Specialized connectors for different sources such as IoT, HTTP, WebSockets,
    and so on, have resulted in many specific connectors such as Amazon IoT Hub, REST-connectors,
    WebSocket, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data storage component plays a key part in connecting the acquisition and
    the rest of the components together. Performance, impact on data processing, cost,
    high-availability, ease of management, and so on, should be taken into consideration
    while deciding on data storage. For pure real-time or near real-time systems there
    are in-memory based frameworks for storage, but for batch-based systems there
    are mainly distributed File Systems such as HDFS or NoSQL.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HDFS can run on a large cluster of nodes and provide all the important features
    such as high-throughput, replications, fail-over, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![HDFS](img/B05137_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic architecture of HDFS has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode**: The HDFS client always sends the request to the NameNode, which
    keeps the metadata of the file while the real data is distributed in blocks on
    the DataNodes. NameNodes are only responsible for handling opening and closing
    a file while the remaining interactions of reading, writing, and appending happen
    between clients and the data nodes. The NameNode stores the metadata in two files:
    `fsimage` and `edit` files. The `fsimage` contains the filesystem metadata as
    a snapshot, while edit files contain the incremental changes to the metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary NameNode**: Secondary NameNode provides redundancy to the metadata
    in the NameNode by keeping a copy of the `fsimage` and `edit` files at every predefined
    checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataNode**: DataNodes manage the actual blocks of data and facilitate read-write
    operation on these datablocks. DataNodes keep communicating with the NameNodes
    using heartbeat signals indicating they are alive. The data blocks stored in DataNodes
    are also replicated for redundancy. Replication of the data blocks in the DataNodes
    is governed by the rack-aware placement policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Non-relational databases, also referred to as NoSQL databases, are gaining enormous
    popularity in the Big Data world. High throughput, better horizontal scaling,
    improved performance on retrieval, and storage at the cost of weaker consistency
    models are notable characteristics of most NoSQL databases. We will discuss some
    important forms of NoSQL database in this section along with implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Key-value databases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Key-value databases are the most prominent NoSQL databases used mostly for semi-structured
    or unstructured data. As the name suggests, the structure of storage is quite
    basic, with unique keys associating the data values that can be of any type including
    string, integer, double precision, and so on—even BLOBS. Hashing the keys for
    quick lookup and retrieval of the values together with partitioning the data across
    multiple nodes gives high throughput and scalability. The query capabilities are
    very limited. Amazon DynamoDB, Oracle NoSQL, MemcacheDB, and so on, are examples
    of key-value databases.
  prefs: []
  type: TYPE_NORMAL
- en: Document databases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Document databases store semi-structured data in the form of XML, JSON, or YAML
    documents, to name some of the most popular formats. The documents have unique
    keys to which they are mapped. Though it is possible to store documents in key-value
    stores, the query capabilities offered by document stores are greater as the primitives
    making up the structure of the document—which may include names or attributes—can
    also be used for retrieval. When the data is ever-changing and has variable numbers
    or lengths of fields, document databases are often a good choice. Document databases
    do not offer join capabilities and hence all information needs to be captured
    in the document values. MongoDB, ElasticSearch, Apache Solr, and so on, are some
    well-known implementations of document databases.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar databases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The use of columns as the basic unit of storage with name, value, and often
    timestamp, differentiates columnar databases from traditional relational databases.
    Columns are further combined to form column families. A row is indexed by the
    row key and has multiple column families associated with the row. Certain rows
    can use only column families that are populated, giving it a good storage representation
    in sparse data. Columnar databases do not have fixed schema-like relational databases;
    new columns and families can be added at any time, giving them a significant advantage.
    **HBase**, **Cassandra**, and **Parquet** are some well-known implementations
    of columnar databases.
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In many applications, the data has an inherent graph structure with nodes and
    links. Storing such data in graph databases makes it more efficient for storage,
    retrieval, and queries. The nodes have a set of attributes and generally represent
    entities, while links represent relationships between the nodes that can be directed
    or undirected. **Neo4J**, **OrientDB**, and **ArangoDB** are some well-known implementations
    of graph databases.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing and preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data preparation step involves various preprocessing steps before the data
    is ready to be consumed by analytics and machine learning algorithms. Some of
    the key tasks involved are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleansing**: Involves everything from correcting errors, type matching,
    normalization of elements, and so on, on the raw data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data scraping and curating**: Converting data elements and normalizing the
    data from one structure to another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: Many analytical algorithms need features that are
    aggregates built on raw or historical data. Transforming and computing those extra
    features are done in this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive and HQL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apache Hive (*References* [11]) is a powerful tool for performing various data
    preparation activities in HDFS systems. Hive organizes the underlying HDFS data
    a of structure that is similar to relational databases. HQL is like SQL and helps
    in performing various aggregates, transformations, cleanup, and normalization,
    and the data is then serialized back to HDFS. The logical tables in Hive are partitioned
    across and sub-divided into buckets for speed-up. Complex joins and aggregate
    queries in Hive are automatically converted into MapReduce jobs for throughput
    and speed-up.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spark SQL, which is a major component of Apache Spark (*References* [1] and
    [2]), provides SQL-like functionality—similar to what HQL provides—for performing
    changes to the Big Data. Spark SQL can work with underlying data storage systems
    such as Hive or NoSQL databases such as Parquet. We will touch upon some aspects
    of Spark SQL in the section on Spark later.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amazon Redshift provides several warehousing capabilities especially on Amazon
    EMR setups. It can process petabytes of data using its **massively parallel processing**
    (**MPP**) data warehouse architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time stream processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In many Big Data deployments, processing and performing the transformations
    specified previously must be done on the stream of data in real time rather than
    from stored batch data. There are various **Stream Processing Engines** (**SPE**)
    such as Apache Storm (*References* [12]) and Apache Samza, and in-memory processing
    engines such as Spark-Streaming that are used for stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Machine learning helps to perform descriptive, predictive, and prescriptive
    analysis on Big Data. There are two broad extremes that will be covered in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning can be done on batch historical data and then the learning/models
    can be applied to new batch/real-time data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning can be done on real-time data and applied simultaneously to
    the real-time data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both topics are covered at length in the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With batch learning done at modeling time and real-time learning done at runtime,
    predictions—the output of applying the models to new data—must be stored in some
    data structure and then analyzed by the users. Visualization tools and other reporting
    tools are frequently used to extract and present information to the users. Based
    on the domain and the requirements of the users, the analysis and visualization
    can be static, dynamic, or interactive.
  prefs: []
  type: TYPE_NORMAL
- en: Lightning is a framework for performing interactive visualizations on the Web
    with different binding APIs using REST for Python, R, Scala, and JavaScript languages.
  prefs: []
  type: TYPE_NORMAL
- en: Pygal and Seaborn are Python-based libraries that help in plotting all possible
    charts and graphs in Python for analysis, reporting, and visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Big Data Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Batch Big Data Machine Learning involves two basic steps, as discussed in [Chapter
    2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised Learning"),
    *Practical Approach to Real-World Supervised Learning*, [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, and [Chapter 4](ch04.html "Chapter 4. Semi-Supervised and
    Active Learning"), *Semi-Supervised and Active Learning*: learning or training
    data from historical datasets and applying the learned models to unseen future
    data. The following figure demonstrates the two environments along with the component
    tasks and some technologies/frameworks that accomplish them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Batch Big Data Machine Learning](img/B05137_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Model time and run time components for Big Data and providers'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss two of the most well-known frameworks for doing Machine Learning
    in the context of batch data and will use the case study to highlight either the
    code or tools to perform modeling.
  prefs: []
  type: TYPE_NORMAL
- en: H2O as Big Data Machine Learning platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: H2O (*References* [13]) is a leading open source platform for Machine Learning
    at Big Data scale, with a focus on bringing AI to the enterprise. The company
    was founded in 2011 and counts several leading lights in statistical learning
    theory and optimization among its scientific advisors. It supports programming
    environments in multiple languages. While the H2O software is freely available,
    customer service and custom extensions to the product can be purchased.
  prefs: []
  type: TYPE_NORMAL
- en: H2O architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following figure gives a high-level architecture of H2O with important components.
    H2O can access data from various data stores such as HDFS, SQL, NoSQL, and Amazon
    S3, to name a few. The most popular deployment of H2O is to use one of the deployment
    stacks discussed earlier with Spark or to run it in a H2O cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: The core of H2O is an optimized way of handling Big Data in memory, so that
    iterative algorithms that go through the same data can be handled efficiently
    and achieve good performance. Important Machine Learning algorithms in supervised
    and unsupervised learning are implemented specially to handle horizontal scalability
    across multiple nodes and JVMs. H2O provides not only its own user interface,
    known as flow, to manage and run modeling tasks, but also has different language
    bindings and connector APIs to Java, R, Python, and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O architecture](img/B05137_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: H2O high level architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most Machine Learning algorithms, optimization algorithms, and utilities use
    the concept of fork-join or MapReduce. As shown in *Figure 8*, the entire dataset
    is considered as a **Data Frame** in H2O, and comprises vectors, which are features
    or columns in the dataset. The rows or instances are made up of one element from
    each Vector arranged side-by-side. The rows are grouped together to form a processing
    unit known as a **Chunk**. Several chunks are combined in one JVM. Any algorithmic
    or optimization work begins by sending the information from the topmost JVM to
    fork on to the next JVM, then on to the next, and so on, similar to the map operation
    in MapReduce. Each JVM works on the rows in the chunks to establish the task and
    finally the results flow back in the reduce operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H2O architecture](img/B05137_09_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: H2O distributed data processing using chunking'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in H2O
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following figure shows all the Machine Learning algorithms supported in
    H2O v3 for supervised and unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in H2O](img/B05137_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: H2O v3 Machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: H2O Flow is an interactive web application that helps data scientists to perform
    various tasks from importing data to running complex models using point and click
    and wizard-based concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O is run in local mode as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The default way to start Flow is to point your browser and go to the following
    URL: `http://192.168.1.7:54321/`. The right-side of Flow captures every user action
    performed under the tab **OUTLINE**. The actions taken can be edited and saved
    as named flows for reuse and collaboration, as shown in *Figure 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and usage](img/B05137_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: H2O Flow in the browser'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11* shows the interface for importing files from the local filesystem
    or HDFS and displays detailed summary statistics as well as next actions that
    can be performed on the dataset. Once the data is imported, it gets a data frame
    reference in the H2O framework with the extension of `.hex`. The summary statistics
    are useful in understanding the characteristics of data such as **missing**, **mean**,
    **max**, **min**, and so on. It also has an easy way to transform the features
    from one type to another, for example, numeric features with a few unique values
    to categorical/nominal types known as `enum` in H2O.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The actions that can be performed on the datasets are:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into different sets such as training, validation, and testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build supervised and unsupervised models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the models to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download and export the files in various formats.![Tools and usage](img/B05137_09_012.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 11: Importing data as frames, summarizations, and actions that can be
    performed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Building supervised or unsupervised models in H2O is done through an interactive
    screen. Every modeling algorithm has its parameters classified into three sections:
    basic, advanced, and expert. Any parameter that supports hyper-parameter searches
    for tuning the model has a checkbox grid next to it, and more than one parameter
    value can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: Some basic parameters such as **training_frame**, **validation_frame**, and
    **response_column**, are common to every supervised algorithm; others are specific
    to model types, such as the choice of solver for GLM, the activation function
    for deep learning, and so on. All such common parameters are available in the
    basic section. Advanced parameters are settings that afford greater flexibility
    and control to the modeler if the default behavior must be overridden. Several
    of these parameters are also common across some algorithms—two examples are the
    choice of method for assigning the fold index (if cross-validation was selected
    in the basic section), and selecting the column containing weights (if each example
    is weighted separately), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expert parameters define more complex elements such as how to handle the missing
    values, model-specific parameters that need more than a basic understanding of
    the algorithms, and other esoteric variables. In *Figure 12*, GLM, a supervised
    learning algorithm, is being configured with 10-fold cross-validation, binomial
    (two-class) classification, efficient LBFGS optimization algorithm, and stratified
    sampling for cross-validation split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and usage](img/B05137_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Modeling algorithm parameters and validations'
  prefs: []
  type: TYPE_NORMAL
- en: The model results screen contains a detailed analysis of the results using important
    evaluation charts, depending on the validation method that was used. At the top
    of the screen are possible actions that can be taken, such as to run the model
    on unseen data for prediction, download the model as POJO format, export the results,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the charts are algorithm-specific, like the scoring history that shows
    how the training loss or the objective function changes over the iterations in
    GLM—this gives the user insight into the speed of convergence as well as into
    the tuning of the iterations parameter. We see the ROC curves and the Area Under
    Curve metric on the validation data in addition to the gains and lift charts,
    which give the cumulative capture rate and cumulative lift over the validation
    sample respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13* shows **SCORING HISTORY**, **ROC CURVE**, and **GAINS/LIFT** charts
    for GLM on 10-fold cross-validation on the `CoverType` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and usage](img/B05137_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Modeling and validation ROC curves, objective functions, and lift/gain
    charts'
  prefs: []
  type: TYPE_NORMAL
- en: The output of validation gives detailed evaluation measures such as accuracy,
    AUC, err, errors, f1 measure, MCC (Mathews Correlation Coefficient), precision,
    and recall for each validation fold in the case of cross-validation as well as
    the mean and standard deviation computed across all.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and usage](img/B05137_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Validation results and summary'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction action runs the model using unseen held-out data to estimate
    the out-of-sample performance. Important measures such as errors, accuracy, area
    under curve, ROC plots, and so on, are given as the output of predictions that
    can be saved or exported.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools and usage](img/B05137_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Running test data, predictions, and ROC curves'
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we use the `CoverType` dataset to demonstrate classification
    and clustering algorithms from H2O, Apache Spark MLlib, and SAMOA Machine Learning
    libraries in Java.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `CoverType` dataset available from the UCI machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype))
    contains unscaled cartographic data for 581,012 cells of forest land 30 x 30 m2
    in dimension, accompanied by actual forest cover type labels. In the experiments
    conducted here, we use the normalized version of the data. Including one-hot encoding
    of two categorical types, there are a total of 54 attributes in each row.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we treat the problem as one of classification using the labels included
    in the dataset and perform several supervised learning experiments. With the models
    generated, we make predictions about the forest cover type of an unseen held out
    test dataset. For the clustering experiments that follow, we ignore the data labels,
    determine the number of clusters to use, and then report the corresponding cost
    using various algorithms implemented in H2O and Spark MLLib.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This dataset was collected using cartographic measurements only and no remote
    sensing. It was derived from data originally collected by the **US Forest Service**
    (**USFS**) and the **US Geological Survey** (**USGS**).
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train and test data—The dataset was split into two sets in the ratio 20% for
    testing and 80% for training.
  prefs: []
  type: TYPE_NORMAL
- en: The categorical Soil Type designation was represented by 40 binary variable
    attributes. A value of 1 indicates the presence of a soil type in the observation;
    a 0 indicates its absence.
  prefs: []
  type: TYPE_NORMAL
- en: The wilderness area designation is likewise a categorical attribute with four
    binary columns, with 1 indicating presence and 0 absence.
  prefs: []
  type: TYPE_NORMAL
- en: All continuous value attributes have been normalized prior to use.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments, results, and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the first set of experiments in this case study, we used the H2O framework.
  prefs: []
  type: TYPE_NORMAL
- en: Feature relevance and analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Though H2O doesn't have explicit feature selection algorithms, many learners
    such as GLM, random forest, GBT, and so on, give feature importance metrics based
    on training/validation of the models. In our analysis, we have used GLM for feature
    selection, as shown in *Figure 16*. It is interesting that the feature **Elevation**
    emerges as the most discriminating feature along with some categorical features
    that are converted into numeric/binary such as **Soil_Type2**, **Soil_Type4**,
    and so on. Many of the soil type categorical features have no relevance and can
    be dropped from the modeling perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning algorithms included in this set of experiments were: **Generalized
    Linear Models** (**GLM**), **Gradient Boosting Machine** (**GBM**), **Random Forest**
    (**RF**), **Naïve Bayes** (**NB**), and **Deep Learning** (**DL**). The deep learning
    model supported by H2O is the **multi-layered perceptron** (**MLP**).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature relevance and analysis](img/B05137_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Feature selection using GLM'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation on test data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The results using all the features are shown in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Parameters | AUC | Max Accuracy | Max F1 | Max Precision | Max
    Recall | Max Specificity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **GLM** | Default | 0.84 | 0.79 | 0.84 | 0.98 | 1.0(1) | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| **GBM** | Default | 0.86 | 0.82 | 0.86 | 1.0(1) | 1.0(1) | 1.0(1) |'
  prefs: []
  type: TYPE_TB
- en: '| **Random Forest** (**RF**) | Default | 0.88(1) | 0.83(1) | 0.87(1) | 0.97
    | 1.0(1) | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| **Naïve Bayes** (**NB**) | Laplace=50 | 0.66 | 0.72 | 0.81 | 0.68 | 1.0(1)
    | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| **Deep Learning** (**DL**) | Rect,300, 300,Dropout | 0. | 0.78 | 0.83 | 0.88
    | 1.0(1) | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| **Deep Learning** (**DL**) | 300, 300,MaxDropout | 0.82 | 0.8 | 0.84 | 1.0(1)
    | 1.0(1) | 1.0(1) |'
  prefs: []
  type: TYPE_TB
- en: 'The results after removing features not scoring well in feature relevance were:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Parameters | AUC | Max Accuracy | Max F1 | MaxPrecision | Max
    Recall | Max Specificity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **GLM** | Default | 0.84 | 0.80 | 0.85 | 1.0 | 1.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **GBM** | Default | 0.85 | 0.82 | 0.86 | 1.0 | 1.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Random Forest** (**RF**) | Default | 0.88 | 0.83 | 0.87 | 1.0 | 1.0 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Naïve Bayes** (**NB**) | Laplace=50 | 0.76 | 0.74 | 0.81 | 0.89 | 1.0 |
    0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| **Deep Learning** (**DL**) | 300,300, RectDropout | 0.81 | 0.79 | 0.84 |
    1.0 | 1.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Deep Learning** (**DL**) | 300, 300, MaxDropout | 0.85 | 0.80 | 0.84 |
    0.89 | 0.90 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Model evaluation results with all features included'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main observations from an analysis of the results obtained are quite instructive
    and are presented here.
  prefs: []
  type: TYPE_NORMAL
- en: The feature relevance analysis shows how the **Elevation** feature is a highly
    discriminating feature, whereas many categorical attributes converted to binary
    features, such as **SoilType_10**, and so on, have near-zero to no relevance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results for experiments with all features included, shown in *Table 1*,
    clearly indicate that the non-linear ensemble technique Random Forest is the best
    algorithm as shown by the majority of the evaluation metrics including accuracy,
    F1, AUC, and recall.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Table 1* also highlights the fact that whereas the faster, linear Naive Bayes
    algorithm may not be best-suited, GLM, which also falls in the category of linear
    algorithms, demonstrates much better performance—this points to some inter-dependence
    among features!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we saw in [Chapter 7](ch07.html "Chapter 7. Deep Learning"), *Deep Learning,*
    algorithms used in deep learning typically need a lot of tuning; however, even
    with a few small tuning changes, the results from DL are comparable to Random
    Forest, especially with MaxDropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Table 2* shows the results of all the algorithms after removing low-relevance
    features from the training set. It can be seen that Naive Bayes—which has the
    most impact due to multiplication of probabilities based on the assumption of
    independence between features—gets the most benefit and highest uplift in performance.
    Most of the other algorithms such as Random Forest have inbuilt feature selection
    as we discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning,*
    and as a result removing the unimportant features has little or no effect on their
    performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark MLlib as Big Data Machine Learning platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Spark, started in 2009 at AMPLab at UC Berkley, was donated to Apache
    Software Foundation in 2013 under Apache License 2.0\. The core idea of Spark
    was to build a cluster computing framework that would overcome the issues of Hadoop,
    especially for iterative and in-memory computations.
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Spark stack as shown in *Figure 17* can use any kind of data stores such
    as HDFS, SQL, NoSQL, or local filesystems. It can be deployed on Hadoop, Mesos,
    or even standalone.
  prefs: []
  type: TYPE_NORMAL
- en: The most important component of Spark is the Spark Core, which provides a framework
    to handle and manipulate the data in a high-throughput, fault-tolerant, and scalable
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Built on top of Spark core are various libraries each meant for various functionalities
    needed in processing data and doing analytics in the Big Data world. Spark SQL
    gives us a language for performing data manipulation in Big Data stores using
    a querying language very much like SQL, the *lingua franca* of databases. Spark
    GraphX provides APIs to perform graph-related manipulations and graph-based algorithms
    on Big Data. Spark Streaming provides APIs to handle real-time operations needed
    in stream processing ranging from data manipulations to queries on the streams.
  prefs: []
  type: TYPE_NORMAL
- en: Spark-MLlib is the Machine Learning library that has an extensive set of Machine
    Learning algorithms to perform supervised and unsupervised tasks from feature
    selection to modeling. Spark has various language bindings such as Java, R, Scala,
    and Python. MLlib has a clear advantage running on top of the Spark engine, especially
    because of caching data in memory across multiple nodes and running MapReduce
    jobs, thus improving performance as compared to Mahout and other large-scale Machine
    Learning engines by a significant factor. MLlib also has other advantages such
    as fault tolerance and scalability without explicitly managing it in the Machine
    Learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark architecture](img/B05137_09_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Apache Spark high level architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark core has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets** (**RDD**): RDDs are the basic immutable
    collection of objects that Spark Core knows how to partition and distribute across
    the cluster for performing tasks. RDDs are composed of "partitions", dependent
    on parent RDDs and metadata about data placement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two distinct operations are performed on RDDs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformations**: Operations that are lazily evaluated and transform one
    RDD into another. Lazy evaluation defers evaluation as long as possible, which
    makes some resource optimizations possible.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: The actual operation that triggers transformations and returns
    output values'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lineage graph**: The pipeline or flow of data describing the computation
    for a particular task, including different RDDs created in transformations and
    actions is known as the lineage graph of the task. The lineage graph plays a key
    role in fault tolerance.![Spark architecture](img/B05137_09_019.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 18: Apache Spark lineage graph'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Spark is agnostic to the cluster management and can work with several implementations—including
    YARN and Mesos—for managing the nodes, distributing the work, and communications.
    The distribution of tasks in Transformations and Actions across the cluster is
    done by the scheduler, starting from the driver node where the Spark context is
    created, to the many worker nodes as shown in *Figure 19*. When running with YARN,
    Spark gives the user the choice of the number of executors, heap, and core allocation
    per JVM at the node level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark architecture](img/B05137_09_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Apace Spark cluster deployment and task distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning in MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spark MLlib has a comprehensive Machine Learning toolkit, offering more algorithms
    than H2O at the time of writing, as shown in *Figure 20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning in MLlib](img/B05137_09_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Apache Spark MLlib machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: Many extensions have been written for Spark, including Spark MLlib, and the
    user community continues to contribute more packages. You can download third-party
    packages or register your own at [https://spark-packages.org/](https://spark-packages.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark MLlib provides APIs for other languages in addition to Java, including
    Scala, Python, and R. When a `SparkContext` is created, it launches a monitoring
    and instrumentation web console at port `4040`, which lets us see key information
    about the runtime, including scheduled tasks and their progress, RDD sizes and
    memory use, and so on. There are also external profiling tools available for use.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments, results, and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The business problem we tackled here is the same as the one described earlier
    for experiments using H2O. We employed five learning algorithms using MLlib, in
    all. The first was k-Means with all features using a *k* value determined from
    computing the cost—specifically, the **Sum of Squared Errors** (**SSE**)—over
    a large number of values of *k* and selecting the "elbow" of the curve. Determining
    the optimal value of *k* is typically not an easy task; often, evaluation measures
    such as silhouette are compared in order to pick the best *k*. Even though we
    know the number of classes in the dataset is *7*, it is instructive to see where
    experiments like this lead if we pretend we did not have labeled data. The optimal
    *k* found using the elbow method was 27\. In the real world, business decisions
    may often guide the selection of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: In the following listings, we show how to use different models from the MLlib
    suite to do cluster analysis and classification. The code is based on examples
    available in the MLlib API Guide ([https://spark.apache.org/docs/latest/mllib-guide.html](https://spark.apache.org/docs/latest/mllib-guide.html)).
    We use the normalized UCI `CoverType` dataset in CSV format. Note that it is more
    natural to use `spark.sql.Dataset` with the newer `spark.ml` package, whereas
    the `spark.mllib` package works more closely with `JavaRDD`. This provides an
    abstraction over RDDs and allows for optimization of the transformations beneath
    the covers. In the case of most unsupervised learning algorithms, this means the
    data must be transformed such that the dataset to be used for training and testing
    should have a column called features by default that contains all the features
    of an observation as a vector. A `VectorAssembler` object can be used for this
    transformation. A glimpse into the use of ML pipelines, which is a way to chain
    tasks together, is given in the source code for training a Random Forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following code fragment for the k-Means experiment uses the algorithm from
    the `org.apache.spark.ml.clustering` package. The code includes minimal boilerplate
    for setting up the `SparkSession`, which is the handle to the Spark runtime. Note
    that eight cores have been specified in local mode in the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The optimal value for the number of clusters was arrived at by evaluating and
    plotting the sum of squared errors for several different values and choosing the
    one at the elbow of the curve. The value used here is *27*.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means with PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the second experiment, we used k-Means again, but first we reduced the number
    of dimensions in the data through PCA. Again, we used a rule of thumb here, which
    is to select a value for the PCA parameter for the number of dimensions such that
    at least 85% of the variance in the original dataset is preserved after the reduction
    in dimensionality. This produced 16 features in the transformed dataset from an
    initial 54, and this dataset was used in this and subsequent experiments. The
    following code shows the relevant code for PCA analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Bisecting k-Means (with PCA)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The third experiment used MLlib''s Bisecting k-Means algorithm. This algorithm
    is similar to a top-down hierarchical clustering technique where all instances
    are in the same cluster at the outset, followed by successive splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Gaussian Mixture Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the next experiment, we used MLlib''s **Gaussian Mixture Model** (**GMM**),
    another clustering model. The assumption inherent to this model is that the data
    distribution in each cluster is Gaussian in nature, with unknown parameters. The
    same number of clusters is specified here, and default values have been used for
    the maximum number of iterations and tolerance, which dictate when the algorithm
    is considered to have converged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Random Forest
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, we ran Random Forest, which is the only available ensemble learner
    that can handle multi-class classification. In the following code, we see that
    this algorithm needs some preparatory tasks to be performed prior to training.
    Pre-processing stages are composed into a pipeline of Transformers and Estimators.
    The pipeline is then used to fit the data. You can learn more about Pipelines
    on the Apache Spark website ([https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The sum of squared errors for the experiments using k-Means and Bisecting k-Means
    are given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | k | Features | SSE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| k-Means | 27 | 54 | 214,702 |'
  prefs: []
  type: TYPE_TB
- en: '| k-Means(PCA) | 27 | 16 | 241,155 |'
  prefs: []
  type: TYPE_TB
- en: '| Bisecting k-Means(PCA) | 27 | 16 | 305,644 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results with k-Means'
  prefs: []
  type: TYPE_NORMAL
- en: The GMM model was used to illustrate the use of the API; it outputs the parameters
    of the Gaussian mixture for every cluster as well as the cluster weight. Output
    for all the clusters can be seen at the website for this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the case of Random Forest these are the results for runs with different
    numbers of trees. All 54 features were used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of trees | Accuracy | F1 measure | Weighted precision | Weighted recall
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0.6806 | 0.6489 | 0.6213 | 0.6806 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.6776 | 0.6470 | 0.6191 | 0.6776 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0.5968 | 0.5325 | 0.5717 | 0.5968 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 0.6547 | 0.6207 | 0.5972 | 0.6547 |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 0.6594 | 0.6272 | 0.6006 | 0.6594 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results for Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As can be seen from *Table 3*, there is a small increase in cost when fewer
    dimensions are used after PCA with the same number of clusters. Varying *k* with
    PCA might suggest a better *k* for the PCA case. Notice also that in this experiment,
    for the same *k*, Bisecting K-Means with PCA-derived features has the highest
    cost of all. The stopping number of clusters used for Bisecting k-Means has simply
    been picked to be the one determined for basic k-Means, but this need not be so.
    A similar search for *k* that yields the best cost may be done independently for
    Bisecting k-Means.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Random Forest, we see the best performance when using *15* trees.
    All trees have a depth of three. This hyper-parameter can be varied to tune the
    model as well. Even though Random Forest is not susceptible to over-fitting due
    to accounting for variance across trees in the training stages, increasing the
    value for the number of trees beyond an optimum number can degrade performance.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time Big Data Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will discuss the real-time version of Big Data Machine Learning
    where data arrives in large volumes and is changing at a rapid rate at the same
    time. Under these conditions, Machine Learning analytics cannot be applied *per*
    the traditional practice of "batch learning and deploy" (*References* [14]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Real-time Big Data Machine Learning](img/B05137_09_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Use case for real-time Big Data Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider a case where labeled data is available for a short duration,
    and we perform the appropriate modeling techniques on the data and then apply
    the most suitable evaluation methods on the resulting models. Next, we select
    the best model and use it for predictions on unseen data at runtime. We then observe,
    with some dismay, that model performance drops significantly over time. Repeating
    the exercise with new data shows a similar degradation in performance! What are
    we to do now? This quandary, combined with large volumes of data motivates the
    need for a different approach: real-time Big Data Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Like the batch learning framework, the real-time framework in big data may have
    similar components up until the data preparation stage. When the computations
    involved in data preparation must take place on streams or combined stream and
    batch data, we require specialized computation engines such as **Spark Streaming**.
    Like stream computations, Machine Learning must work across the cluster and perform
    different Machine Learning tasks on the stream. This adds an additional layer
    of complexity to the implementations of single machine multi-threaded streaming
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Real-time Big Data Machine Learning](img/B05137_09_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Real-time big data components and providers'
  prefs: []
  type: TYPE_NORMAL
- en: SAMOA as a real-time Big Data Machine Learning framework
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a single machine, in [Chapter 5](ch05.html "Chapter 5. Real-Time Stream
    Machine Learning"), *Real-Time Stream Machine Learning*, we discussed the MOA
    framework at length. SAMOA is the distributed framework for performing Machine
    Learning on streams.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, SAMOA is an incubator-level open source project with
    Apache 2.0 license and good integration with different stream processing engines
    such as **Apache Storm**, **Samza**, and **S4**.
  prefs: []
  type: TYPE_NORMAL
- en: SAMOA architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The SAMOA framework offers several key streaming services to an extendable set
    of stream processing engines, with existing implementations for the most popular
    engines of today.
  prefs: []
  type: TYPE_NORMAL
- en: '![SAMOA architecture](img/B05137_09_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: SAMOA high level architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '`TopologyBuilder` is the interface that acts as a factory to create different
    components and connect them together in SAMOA. The core of SAMOA is in building
    processing elements for data streams. The basic unit for processing consists of
    `ProcessingItem` and the `Processor` interface, as shown in *Figure 24*. `ProcessingItem`
    is an encapsulated hidden element, while Processor is the core implementation
    where the logic for handling streams is coded.'
  prefs: []
  type: TYPE_NORMAL
- en: '![SAMOA architecture](img/B05137_09_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: SAMOA processing data streams'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stream** is another interface that connects various **Processors** together
    as the source and destination created by `TopologyBuilder`. A Stream can have
    one source and multiple destinations. Stream supports three forms of communication
    between source and destinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**All**: In this communication, all messages from source are sent to all the
    destinations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key**: In this communication, messages with the same keys are sent to the
    same processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffle**: In this communication, messages are randomly sent to the processors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the messages or events in SAMOA are implementations of the interface `ContentEvent`,
    encapsulating mostly the data in the streams as a value and having some form of
    key for uniqueness.
  prefs: []
  type: TYPE_NORMAL
- en: Each stream processing engine has an implementation for all the key interfaces
    as a plugin and integrates with SAMOA. The Apache Storm implementations StormTopology,
    StormStream, and StormProcessingItem, and so on are shown in the API in *Figure
    25*.
  prefs: []
  type: TYPE_NORMAL
- en: Task is another unit of work in SAMOA, having the responsibility of execution.
    All the classification or clustering evaluation and validation techniques such
    as prequential, holdout, and so on, are implemented as Tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Learner is the interface for implementing all Supervised and Unsupervised Learning
    capability in SAMOA. Learners can be local or distributed and have different extensions
    such as `ClassificationLearner` and `RegressionLearner`.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Machine Learning algorithms](img/B05137_09_027.jpg)![Machine Learning algorithms](img/B05137_09_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: SAMOA machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 25* shows the core components of the SAMOA topology and their implementation
    for various engines.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We continue with the same business problem as before. The command line to launch
    the training job for the `covtype` dataset is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Tools and usage](img/B05137_09_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Bagging model performance'
  prefs: []
  type: TYPE_NORMAL
- en: 'When running with Storm, this is the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Experiments, results, and analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The results of experiments using SAMOA as a stream-based learning platform for
    Big Data are given in *Table 5*.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Best Accuracy | Final Accuracy | Final Kappa Statistic | Final
    Kappa Temporal Statistic |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bagging | 79.16 | 64.09 | 37.52 | -69.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Boosting | 78.05 | 47.82 | 0 | -1215.1 |'
  prefs: []
  type: TYPE_TB
- en: '| VerticalHoeffdingTree | 83.23 | 67.51 | 44.35 | -719.51 |'
  prefs: []
  type: TYPE_TB
- en: '| AdaptiveBagging | 81.03 | 64.64 | 38.99 | -67.37 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Experimental results with Big Data real-time learning using SAMOA'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'From an analysis of the results, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 5* shows that the popular non-linear decision tree-based VHDT on SAMOA
    is the best performing algorithm according to almost all the metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The adaptive bagging algorithm performs better than bagging because it employs
    Hoeffding Adaptive Trees in the implementation, which are more robust than basic
    online stream bagging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The online boosting algorithm with its dependency on the weak learners and no
    adaptability ranked the lowest as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bagging plot in *Figure 25* shows a nice trend of stability achieved as
    the number of examples increased, validating the general consensus that if the
    patterns are stationary, more examples lead to robust models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The impact of Machine Learning on businesses, social interactions, and indeed,
    our day-to-day lives today is undeniable, though not always immediately obvious.
    In the near future, it will be ubiquitous and inescapable. According to a report
    by McKinsey Global Institute published in December 2016 (*References* [15]), there
    is a vast unexploited potential for data and analytics in major industry sectors,
    especially healthcare and the public sector. Machine Learning is one of the key
    technologies poised to help exploit that potential. More compute power is at our
    disposal than ever before. More data is available than ever before, and we have
    cheaper and greater storage capacity than ever before.
  prefs: []
  type: TYPE_NORMAL
- en: Already, the unmet demand for data scientists has spurred changes to college
    curricula worldwide, and has caused an increase of 16% a year in wages for data
    scientists in the US, in the period 2012-2014\. The solution to a wide swathe
    of problems is within reach with Machine Learning, including resource allocation,
    forecasting, predictive analytics, predictive maintenance, and price and product
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same McKinsey report emphasizes the increasing role of Machine Learning,
    including deep learning in a variety of use cases across industries such as agriculture,
    pharma, manufacturing, energy, media, and finance. These scenarios run the gamut:
    predict personalized health outcomes, identify fraud transactions, optimize pricing
    and scheduling, personalize crops to individual conditions, identify and navigate
    roads, diagnose disease, and personalize advertising. Deep learning has great
    potential in automating an increasing number of occupations. Just improving natural
    language understanding would potentially cause a USD 3 trillion impact on global
    wages, affecting jobs like customer service and support worldwide.'
  prefs: []
  type: TYPE_NORMAL
- en: Giant strides in image and voice recognition and language processing have made
    applications such as personal digital assistants commonplace, thanks to remarkable
    advances in deep learning techniques. The symbolism of AlphaGO's success in defeating
    Lee Sedol, alluded to in the opening chapter of this book, is enormous, as it
    is a vivid example of how progress in artificial intelligence is besting our own
    predictions of milestones in AI advancement. Yet this is the tip of the proverbial
    iceberg. Recent work in areas such as transfer learning offers the promise of
    more broadly intelligent systems that will be able to solve a wider range of problems
    rather than narrowly specializing in just one. General Artificial Intelligence,
    where AI can develop objective reasoning, proposes a methodology to solve a problem,
    and learn from its mistakes, is some distance away at this point, but check back
    in a few years and that distance may well have shrunk beyond our current expectations!
    Increasingly, the confluence of transformative advances in technologies incrementally
    enabling each other spells a future of dizzying possibilities that we can already
    glimpse around us. The role of Machine Learning, it would appear, is to continue
    to shape that future in profound and extraordinary ways. Of that, there is little
    doubt.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final chapter of this book deals with Machine Learning adapted to what is
    arguably one of the most significant paradigm shifts in information management
    and analytics to have emerged in the last few decades—Big Data. Much as many other
    areas of computer science and engineering have seen, AI—and Machine Learning in
    particular—has benefited from innovative solutions and dedicated communities adapting
    to face the many challenges posed by Big Data.
  prefs: []
  type: TYPE_NORMAL
- en: One way to characterize Big Data is by volume, velocity, variety, and veracity.
    This demands a new set of tools and frameworks to conduct the tasks of effective
    analytics at large.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Big Data framework involves selecting distributed storage systems,
    data preparation techniques, batch or real-time Machine Learning, as well as visualization
    and reporting tools.
  prefs: []
  type: TYPE_NORMAL
- en: Several open source deployment frameworks are available including Hortonworks
    Data Platform, Cloudera CDH, Amazon Elastic MapReduce, and Microsoft Azure HDInsight.
    Each provides a platform with components supporting data acquisition, data preparation,
    Machine Learning, evaluation, and visualization of results.
  prefs: []
  type: TYPE_NORMAL
- en: Among the data acquisition components, publish-subscribe is a model offered
    by Apache Kafka (*References* [8]) and Amazon Kinesis, which involves a broker
    mediating between subscribers and publishers. Alternatives include source-sink,
    SQL, message queueing, and other custom frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to data storage, several factors contribute to the proper choice
    for whatever your needs may be. HDFS offers a distributed File System with robust
    fault tolerance and high throughput. NoSQL databases also offer high throughput,
    but generally with weak guarantees on consistency. They include key-value, document,
    columnar, and graph databases.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing and preparation come next in the flow, which includes data cleaning,
    scraping, and transformation. Hive and HQL provide these functions in HDFS systems.
    SparkSQL and Amazon Redshift offer similar capabilities. Real-time stream processing
    is available from products such as Storm and Samza.
  prefs: []
  type: TYPE_NORMAL
- en: The learning stage in Big Data analytics can include batch or real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: A variety of rich visualization and analysis frameworks exist that are accessible
    from multiple programming environments.
  prefs: []
  type: TYPE_NORMAL
- en: Two major Machine Learning frameworks on Big Data are H2O and Apache Spark MLlib.
    Both can access data from various sources such as HDFS, SQL, NoSQL, S3, and others.
    H2O supports a number of Machine Learning algorithms that can be run in a cluster.
    For Machine Learning with real-time data, SAMOA is a big data framework with a
    comprehensive set of stream-processing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The role of Machine Learning in the future is going to be a dominant one, with
    a wide-ranging impact on healthcare, finance, energy, and indeed on most industries.
    The expansion in the scope of automation will have inevitable societal effects.
    Increases in compute power, data, and storage per dollar are opening up great
    new vistas to Machine Learning applications that have the potential to increase
    productivity, engender innovation, and dramatically improve living standards the
    world over.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, *Ion
    Stoica:Spark: Cluster Computing with Working Sets*. HotCloud 2010'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
    Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin,
    Ali Ghodsi, Joseph Gonzalez, Scott Shenker, *Ion Stoica:Apache Spark: a unified
    engine for Big Data processing*. Commun. ACM 59(11): 56-65 (2016)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Hadoop: [https://hadoop.apache.org/](https://hadoop.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cloudera: [http://www.cloudera.com/](http://www.cloudera.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hortonworks: [http://hortonworks.com/](http://hortonworks.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Amazon EC2: [http://aws.amazon.com/ec2/](http://aws.amazon.com/ec2/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Microsoft Azure: [http://azure.microsoft.com/](http://azure.microsoft.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Flume: [https://flume.apache.org/](https://flume.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Kafka: [http://kafka.apache.org/](http://kafka.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Sqoop: [http://sqoop.apache.org/](http://sqoop.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Hive: [http://hive.apache.org/](http://hive.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Storm: [https://storm.apache.org/](https://storm.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H2O: [http://h2o.ai/](http://h2o.ai/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shahrivari S, Jalili S. *Beyond batch processing: towards real-time and streaming
    Big Data*. Computers. 2014;3(4):117–29.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*MGI, The Age of Analytics*–—Executive Summary [http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx](http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
