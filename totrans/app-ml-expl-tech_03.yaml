- en: '*Chapter 2*: Model Explainability Methods'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*：模型可解释性方法'
- en: One of the key goals of this book is to empower its readers to design Explainable
    ML systems that can be used in production to solve critical business problems.
    For a robust Explainable ML system, explainability can be provided in multiple
    ways depending on the type of problem and the type of data used. Providing explainability
    for structured tabular data is relatively human-friendly compared to unstructured
    data such as images and text, as image or text data is more complex with less
    interpretable granular features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的一个关键目标是赋予读者设计可解释机器学习系统的能力，这些系统可用于生产环境中解决关键业务问题。对于健壮的可解释机器学习系统，根据问题的类型和使用的数据类型，可解释性可以以多种方式提供。与图像和文本等非结构化数据相比，为结构化表格数据提供可解释性相对更符合人类习惯，因为图像或文本数据更复杂，具有更少的可解释粒度特征。
- en: There are different ways to add explainability to ML models, for instance, by
    extracting information about the data or the model (knowledge extraction), using
    effective visualizations to justify the prediction outcomes (result visualization),
    identifying dominant features in the training data and analyzing its effect on
    the model predictions (influence-based methods), or by comparing model outcomes
    with known scenarios or situations as an example (example-based methods).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为机器学习模型添加可解释性的方法有很多种，例如，通过提取关于数据或模型的信息（知识提取），使用有效的可视化来证明预测结果（结果可视化），识别训练数据中的主导特征并分析其对模型预测的影响（基于影响的方法），或者通过将模型结果与已知场景或情况进行比较作为例子（基于示例的方法）。
- en: So, in this chapter, we are going to discuss various model-agnostic and model-specific
    explanation methods that are used for both structured and unstructured data for
    model explainability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将讨论用于模型可解释性的各种模型无关和模型特定解释方法，这些方法适用于结构化和非结构化数据。
- en: 'This chapter covers the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主要主题：
- en: Types of model explainability methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可解释性方法的类型
- en: Knowledge extraction methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识提取方法
- en: Result visualization methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果可视化方法
- en: Influence-based methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于影响的方法
- en: Example-based methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于示例的方法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The primary goal of this chapter is to provide a conceptual understanding of
    the model explainability methods. However, I will provide certain tutorial examples
    to implement some of these methods in Python on certain interesting datasets.
    We will be using Python Jupyter notebooks to run the code and visualize the output
    throughout this book. The code and dataset resources for [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033)
    can be downloaded or cloned from the following GitHub repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter02](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter02).
    Other important Python frameworks that are required to run the code will be mentioned
    in the notebooks along with other relevant details to understand the code implementations
    within these concepts.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标是提供对模型可解释性方法的概要理解。然而，我将提供一些教程示例，以在Python中实现某些方法，并在一些有趣的数据集上应用。我们将使用Python
    Jupyter笔记本来运行代码并在整本书中可视化输出。[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)的代码和数据集资源可以从以下GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter02](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter02)。其他运行代码所需的Python框架将在笔记本中提及，以及其他相关细节，以理解这些概念中的代码实现。
- en: Types of model explainability methods
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可解释性方法的类型
- en: 'There are different approaches that you can use to provide model explainability.
    Certain techniques are specific to a model, and certain approaches are applied
    to the input and output of the model. In this section, we will discuss the different
    types of methods used to explain ML models:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用不同的方法来提供模型可解释性。某些技术特定于某个模型，而某些方法应用于模型的输入和输出。在本节中，我们将讨论用于解释机器学习模型的不同类型的方法：
- en: '**Knowledge extraction methods**: Extracting key insights and statistical information
    from the data during **Exploratory Data Analysis** (**EDA**) and post-hoc analysis
    is one way of providing model-agnostic explainability. Often, statistical profiling
    methods are applied to extract the mean and median values, standard deviation,
    or variance across the different data points, and certain descriptive statistics
    are used to estimate the expected range of outcomes.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识提取方法**：在**数据探索性分析**（**EDA**）和事后分析期间从数据中提取关键洞察和统计信息是提供模型无关可解释性的方法之一。通常，应用统计配置文件方法来提取平均值和中位数、标准差或方差，以及某些描述性统计来估计预期结果的范围。'
- en: Similarly, other insights using correlation heatmaps, decomposition trees, and
    distribution plots are also used to observe any relationships between the features
    to explain the model's results. For more complex unstructured data, such as images,
    often, these statistical knowledge extraction methods are not sufficient. Human-friendly
    methods using **Concept Activation Vectors** (**CAVs**), as discussed in [*Chapter
    8*](B18216_08_ePub.xhtml#_idTextAnchor154), *Human-Friendly Explanations with
    TCAV*, are more effective.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，使用相关性热图、分解树和分布图等其他洞察力也被用来观察特征之间的任何关系，以解释模型的结果。对于更复杂的非结构化数据，如图像，通常这些统计知识提取方法是不够的。使用**概念激活向量**（**CAVs**），如在第[*第8章*](B18216_08_ePub.xhtml#_idTextAnchor154)“使用TCAV进行人性化的解释”中讨论的，更有效的人性化方法。
- en: However, primarily, knowledge extraction methods extract essential information
    about the input data and the output data from which the expected model outcomes
    are defined. For example, to explain a time series forecasting model, we can consider
    a model performance metric such as the variance in forecasting error over the
    training period. The error rate can be specified within a confidence interval
    of (let's say) +/- 10%. The formation of the confidence interval is only possible
    after extracting key insights from the output training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首先，知识提取方法从输入数据和输出数据中提取关于定义预期模型结果的基本信息。例如，为了解释时间序列预测模型，我们可以考虑一个模型性能指标，比如训练期间预测误差的方差。误差率可以在（比如说）±10%的置信区间内指定。置信区间的形成只有在从输出训练数据中提取关键洞察之后才可能。
- en: '**Result visualization methods**: Plotting model outcomes and comparing them
    with previously predicted values, particularly with surrogate models, is often
    considered an effective model-agnostic explainability method. Predictions from
    black-box ML algorithms are passed to the surrogate model explainers. Usually,
    these are highly interpretable linear models, decision trees, or any rule-based
    heuristic algorithm that can explain the outcome of complex models. The main limitation
    of this approach is that the explainability is solely dependent on the model outcome.
    If there is any abnormality with the data or the modeling process, such dimensions
    of explainability are not captured.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果可视化方法**：绘制模型结果并将其与先前预测的值进行比较，尤其是与代理模型进行比较，通常被认为是一种有效的模型无关的可解释性方法。来自黑盒机器学习算法的预测被传递给代理模型解释器。通常，这些是高度可解释的线性模型、决策树或任何基于规则的启发式算法，可以解释复杂模型的结果。这种方法的主要局限性是可解释性完全依赖于模型结果。如果数据或建模过程中有任何异常，这些可解释性的维度就不会被捕捉到。'
- en: For example, let's suppose a classifier is incorrectly predicting an output.
    It is not feasible for us to understand exactly why the model is behaving in a
    specific manner just from the prediction probability. But these methods are easy
    to apply in practice and even easy to understand as highly interpretable explainer
    algorithms are used.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个分类器错误地预测了一个输出。仅从预测概率中，我们无法确切理解模型为何以特定方式表现。但这些方法在实际应用中易于应用，甚至易于理解，因为使用了高度可解释的解释算法。
- en: '**Influence-based methods**: These are specific techniques that help us to
    understand how certain data features play an important role in influencing or
    controlling the model outcome. Right now, this is one of the most common and effective
    methods applied to provide explainability to ML models. Feature importance, sensitivity
    analysis, key influencer maps, saliency maps, **Class Activation Maps** (**CAMs**),
    and other visual feature maps are used to interpret how the individual features
    within the data are being utilized by the model for its decision-making process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于影响的方法**：这些是帮助我们理解某些数据特征如何对模型结果产生重要影响的特定技术。目前，这是应用最常见和最有效的方法之一，用于为机器学习模型提供可解释性。特征重要性、敏感性分析、关键影响因素图、显著性图、**类激活图（CAMs**）和其他视觉特征图被用来解释数据中的单个特征是如何被模型用于其决策过程的。'
- en: '**Example-based methods**: The three previously discussed model-agnostic explainability
    methods still need some kind of technical knowledge to understand the working
    of the ML models. For non-technical users, the best way to explain something is
    to provide an example that they can relate to. Example-based methods, particularly
    counterfactual example-based methods, try to look at certain single instances
    of the data to explain the ML models'' decision-making process.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于实例的方法**：之前讨论的三种模型无关的可解释性方法仍然需要某种技术知识来理解机器学习模型的工作原理。对于非技术用户来说，解释某事最好的方式是提供一个他们能够与之相关联的例子。基于实例的方法，尤其是基于反事实的实例方法，试图查看某些单个数据实例来解释机器学习模型的决策过程。'
- en: For example, let's say an automated loan approval system powered by ML denies
    a loan request to an applicant. Using example-based explainability methods, the
    applicant will also be suggested that if they pay their credit card bill on time
    for the next three months and increase their monthly income by $2,000, their loan
    request would be granted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个由机器学习驱动的自动贷款审批系统拒绝了一个申请人的贷款申请。使用基于实例的可解释性方法，申请人也会被建议，如果他们在接下来的三个月内按时支付信用卡账单并增加每月收入2000美元，他们的贷款申请将被批准。
- en: Based on the latest trends, model-agnostic techniques are preferred over model-dependent
    approaches as even complex ML algorithms can be explained, to some degree, using
    these techniques. But certain techniques such as saliency maps, tree/forest-based
    feature importance, and activation maps are mostly model-specific. Our choice
    of explanation method is determined by the key problem that we are trying to solve.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最新趋势，模型无关的技术方法比模型依赖的方法更受欢迎，因为即使复杂的机器学习算法也可以在一定程度上使用这些技术进行解释。但某些技术，如显著性图、基于树/森林的特征重要性以及激活图，通常是模型特定的。我们选择解释方法是由我们试图解决的关键问题决定的。
- en: '*Figure 2.1* illustrates the four main types of explainability methods that
    have been applied to explain the working of black-box models, which we are going
    to cover in the following sections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.1*展示了应用于解释黑盒模型工作原理的四种主要类型的可解释性方法，我们将在以下章节中介绍：'
- en: '![Figure 2.1 – Model explainability methods'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 模型可解释性方法'
- en: '](img/B18216_02_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_01.jpg)'
- en: Figure 2.1 – Model explainability methods
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 模型可解释性方法
- en: Now, let's start by discussing each of these model explainability methods in
    more detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地讨论这些模型可解释性方法中的每一个。
- en: Knowledge extraction methods
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识提取方法
- en: Whenever we talk about explainability in any context, it is all about gaining
    knowledge of the problem so as to gain some clarity about the expected outcome.
    Similarly, if we already know the outcome, explainability is all about tracing
    back to the root cause. Knowledge extraction methods in ML are used to extract
    key insights from the input data or utilize the model outcome to trace back and
    map to certain information known to the end users for both structured data and
    unstructured data. Although there are multiple approaches to extracting knowledge,
    in practice, the data-centric process of EDA is one of the most common and popular
    methods for explaining any black-box model. Let's discuss more on how to use the
    EDA process in the context of **XAI**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论在任何语境中讨论可解释性，都是关于获取对问题的了解，以便对预期的结果有更清晰的了解。同样，如果我们已经知道结果，可解释性就是追踪到根本原因。机器学习中的知识提取方法用于从输入数据中提取关键见解，或利用模型结果追踪并映射到终端用户已知的信息，无论是结构化数据还是非结构化数据。尽管有多个提取知识的方法，但在实践中，以数据为中心的EDA过程是解释任何黑盒模型最常见和最受欢迎的方法之一。让我们更详细地讨论如何在**XAI**的语境中使用EDA过程。
- en: EDA
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EDA
- en: I would always argue that EDA is the most important process for any ML workflow.
    EDA allows us to explore the data and draw key insights; using this, we can form
    certain hypotheses from the data. This actually helps us to identify any distinct
    patterns within the data and will, eventually, help us to make the right choice
    of algorithm. Thus, EDA is one of the conventional and model-agnostic approaches
    that explain the nature of the data, and by considering the data, it helps us
    understand what to expect from the model. Detecting any clear anomaly, ambiguous,
    redundant data points and bias in data can be easily observed using EDA. Now,
    let's see some important methods used in EDA to explain ML models for structured
    and unstructured data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是认为EDA对于任何机器学习工作流程来说是最重要的过程。EDA使我们能够探索数据并得出关键见解；利用这些见解，我们可以从数据中形成某些假设。这实际上帮助我们识别数据中的任何独特模式，并最终帮助我们做出正确的算法选择。因此，EDA是传统且模型无关的方法之一，它解释了数据的本质，并且通过考虑数据，它帮助我们了解从模型中可以期待什么。使用EDA可以轻松地观察到任何明显的异常、模糊、冗余数据点和数据偏差。现在，让我们看看在EDA中用于解释结构化和非结构化数据模型的一些重要方法。
- en: EDA on structured data
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化数据上的EDA
- en: EDA on structured data is one of the preliminary steps applied for extracting
    insights to provide explainability. However, the actual techniques applied in
    the EDA process could vary from one problem to another. But generally, for structured
    data, we can use EDA to generate certain descriptive statistics for a better understanding
    of the data and then apply various univariate and multivariate methods to detect
    the importance of each feature, observe the distribution of data to find any biases
    in the data, and look for outliers, duplicate values, missing values, correlation,
    and cardinality between the features, which might impact the model's results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据上进行的EDA（探索性数据分析）是用于提取洞察力以提供可解释性的初步步骤之一。然而，在EDA过程中实际应用的技术可能因问题而异。但一般来说，对于结构化数据，我们可以使用EDA来生成某些描述性统计量，以便更好地理解数据，然后应用各种单变量和多变量方法来检测每个特征的重要性，观察数据的分布以找出数据中的任何偏差，并寻找异常值、重复值、缺失值、特征之间的相关性以及基数，这些都可能影响模型的结果。
- en: 'Information and hypotheses obtained from the EDA step help perform meaningful
    feature engineering and modeling techniques and help set up the right expectation
    for the stakeholders. In this section, we will cover the most popular EDA methods
    and discuss the benefit of using EDA with structured data in the context of XAI.
    I strongly recommend looking at the GitHub repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques))
    to apply some of these techniques in practice for practical use cases. Now, let''s
    look at the important EDA methods in the following list:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从EDA步骤中获得的信息和假设有助于执行有意义的特征工程和建模技术，并帮助为利益相关者设定正确的期望。在本节中，我们将介绍最流行的EDA方法，并讨论在XAI（可解释人工智能）的背景下使用结构化数据EDA的好处。我强烈建议查看GitHub仓库（[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques)）以在实际用例中应用这些技术。现在，让我们看看以下列表中的重要EDA方法：
- en: '**Summary statistics**: Usually, model explainability is presented with respect
    to the features in the data. Observing dataset statistics during the EDA process
    gives an early indication of whether the dataset is sufficient for modeling and
    solving the given problem. It helps to understand the dimensions of the data and
    the type of features present. If the features are numeric, certain descriptive
    statistics such as the mean, standard deviation, coefficient of variation, skewness,
    kurtosis, and inter-quartile ranges are observed.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要统计量**：通常，模型可解释性是相对于数据中的特征来呈现的。在EDA过程中观察数据集统计量可以给出早期迹象，表明数据集是否足够用于建模和解决给定问题。它有助于理解数据的维度和存在的特征类型。如果特征是数值型的，则会观察到某些描述性统计量，如均值、标准差、变异系数、偏度、峰度和四分位数范围。'
- en: Additionally, certain histogram-based distributions are used to monitor any
    skewness or biases in data. For categorical features, the frequency distribution
    of the categorical values is observed. If the dataset is imbalanced, if the dataset
    is biased toward a particular categorical value, if the dataset has outliers,
    or is skewed toward a particular direction, all of these can be easily observed.
    Since all of these factors can impact the model predictions, understanding dataset
    statistics is important for model explainability.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，某些基于直方图的分布被用来监控数据中的任何偏斜或偏差。对于分类特征，观察分类值的频率分布。如果数据集不平衡，如果数据集偏向于某个特定的分类值，如果数据集有异常值，或者偏向于某个特定方向，所有这些都可以很容易地观察到。由于所有这些因素都可能影响模型预测，因此理解数据集统计信息对于模型可解释性很重要。
- en: '*Figure 2.2* shows the summary statistics and visualizations created during
    the EDA step to extract knowledge about the data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.2*显示了在EDA步骤中创建的汇总统计和可视化，以提取关于数据的知识：'
- en: '![Figure 2.2 – Summary statistics and visualizations during EDA'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – EDA过程中的汇总统计和可视化'
- en: '](img/B18216_02_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_02.jpg)'
- en: Figure 2.2 – Summary statistics and visualizations during EDA
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – EDA过程中的汇总统计和可视化
- en: '**Duplicate and missing values**: Duplicate or redundant values can add more
    bias to the model. In contrast, missing values can lead to a loss of information
    and insufficient data to train the model. This might lead to model-overfitting.
    So, before training the model, if missing values or duplicate values are observed,
    and if further actions are not taken to rectify this, then these observations
    might help to explain the reason behind the non-generalization of models.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复值和缺失值**：重复或冗余的值可能会给模型增加更多偏差。相反，缺失值可能导致信息丢失和训练模型的数据不足。这可能会导致模型过拟合。因此，在训练模型之前，如果观察到缺失值或重复值，并且没有采取进一步措施来纠正这种情况，那么这些观察结果可能有助于解释模型泛化不良的原因。'
- en: '**Univariate analysis**: This involves analyzing a single feature through graphical
    techniques such as distribution plots, histograms, box plots, violin plots, pie
    charts, clustering plots, and using non-graphical techniques such as frequency,
    central tendency measures (that is, the mean, standard deviation, and coefficient
    of variation), and interquartile ranges. These methods help us to estimate the
    impact of individual features on the model outcome.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单变量分析**：这涉及到通过图形技术（如分布图、直方图、箱线图、小提琴图、饼图、聚类图）分析单个特征，以及使用非图形技术（如频率、集中趋势度量[即均值、标准差和变异系数]和四分位数范围）。这些方法帮助我们估计单个特征对模型结果的影响。'
- en: '**Multivariate analysis**: This involves analyzing two or more features together
    using graphical and non-graphical methods. It is used for identifying data correlation
    and the dependencies of variables. In the context of XAI, multivariate analysis
    is used to understand complex relationships in the data and provide a detailed
    and granular explanation as compared to univariate analysis methods.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多变量分析**：这涉及到使用图形和非图形方法一起分析两个或更多特征。它用于识别数据相关性以及变量的依赖性。在XAI的背景下，多变量分析用于理解数据中的复杂关系，并提供比单变量分析方法更详细和更细粒度的解释。'
- en: '**Outlier detection**: Outliers are certain abnormal data points that can completely
    skew the model. It is hard to achieve generalization if a model is trained on
    outlier data points. However, model prediction can go completely wrong during
    the model inference time for an anomaly datapoint. Hence, outlier detection during
    both training and inference time is an important part of model explainability.
    Visualization methods such as box plots, scatter plots, and statistical methods
    such as the 1.5xIQR rule ([https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule))
    and Nelson''s Rule ([https://www.leansixsigmadefinition.com/glossary/nelson-rules/](https://www.leansixsigmadefinition.com/glossary/nelson-rules/))
    are used for detecting anomalies.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值检测**：异常值是某些异常数据点，可能会完全扭曲模型。如果模型是在异常数据点上训练的，那么很难实现泛化。然而，在模型推理时间对异常数据点的预测可能会完全错误。因此，在训练和推理时间进行异常值检测是模型可解释性的一个重要部分。可视化方法，如箱线图、散点图，以及统计方法，如1.5倍IQR规则（[https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule)）和纳尔逊规则（[https://www.leansixsigmadefinition.com/glossary/nelson-rules/](https://www.leansixsigmadefinition.com/glossary/nelson-rules/））用于检测异常。'
- en: '**Pareto analysis**: According to the Pareto Principle, 80% of the value or
    impact is driven by 20% of the sample size. So, in XAI, this *80–20 rule* is used
    to interpret the most impactful sub-samples that have the maximum impact on the
    model outcome.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**帕累托分析**：根据帕累托原则，80%的价值或影响是由20%的样本量驱动的。因此，在XAI中，这个*80-20规则*用于解释对模型结果影响最大的子样本。'
- en: '**Frequent** **Itemset Mining**: This is another popular choice of approach
    to extract model explainability. This technique is used frequently for Association
    rule mining to understand how frequently certain observations occur together in
    any given dataset. This method provides some interesting observations that help
    to form important hypotheses from the data and, eventually, contribute a lot to
    explaining model outcomes.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频繁项集挖掘**：这是提取模型可解释性的另一种流行方法。这项技术常用于关联规则挖掘，以了解在给定的数据集中某些观察结果出现的频率。这种方法提供了一些有趣的观察结果，有助于从数据中形成重要的假设，并最终对解释模型结果做出很大贡献。'
- en: Now that we have covered the methods for structured data, let's take a look
    at some of the methods for unstructured data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了结构化数据的方法，让我们来看看一些非结构化数据的方法。
- en: EDA on unstructured data
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非结构化数据的EDA
- en: Interpreting features from unstructured data such as images and text is difficult,
    as ML algorithms try to identify granular-level features that are not intuitively
    explainable to human beings. Yet, there are certain specific methods applied to
    image and text data to form meaningful hypotheses from the data. As discussed
    earlier, the EDA process might change based on the problem and the data, but in
    this chapter, we will discuss the most popular choice of methods in the context
    of XAI.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化数据（如图像和文本）中解释特征是困难的，因为机器学习算法试图识别人类难以直观解释的粒度级特征。然而，有一些特定于图像和文本数据的方法被应用于形成从数据中得出的有意义的假设。如前所述，EDA过程可能会根据问题和数据而变化，但在这章中，我们将讨论XAI背景下最流行的方法选择。
- en: Exploring image data
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索图像数据
- en: 'EDA methods used for images are different from the methods used with tabular
    data. These are some popular choices of EDA steps for image datasets:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像的EDA方法与用于表格数据的EDA方法不同。以下是图像数据集的一些流行的EDA步骤选择：
- en: '**Data dimension analysis**: For consistent and generalized models, understanding
    data dimension is important. Monitoring the number of images and the shape of
    each image is important to explain any observation of overfitting or underfitting.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据维度分析**：对于一致和泛化的模型，理解数据维度很重要。监控图像数量和每个图像的形状对于解释任何过拟合或欠拟合的观察结果都很重要。'
- en: '**Observing data distribution**: Since the majority of problems solved using
    images are classification problems, monitoring class imbalance is important. If
    the distribution of data is not balanced, then the model can be biased toward
    the majority class. For pixel-level classification (for segmentation problems),
    observing pixel intensity distribution is important. This also helps in understanding
    the effect of shadow or non-uniform lighting conditions on images.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察数据分布**：由于使用图像解决的问题中的大多数都是分类问题，因此监控类别不平衡很重要。如果数据分布不平衡，那么模型可能会偏向多数类。对于像素级分类（用于分割问题），观察像素强度分布很重要。这也有助于理解阴影或非均匀照明条件对图像的影响。'
- en: '**Observing average images and contrast images**: For observing dominant regions
    of interest in images, average and contrast images are often used. This is especially
    used for classification-based problems to compare dominant regions of interest.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察平均图像和对比图像**：为了观察图像中的感兴趣区域，经常使用平均图像和对比图像。这在基于分类的问题中尤其有用，用于比较感兴趣区域的占主导地位。'
- en: '**Advanced statistical and algebraic methods**: Apart from the methods discussed
    so far, other statistical methods such as finding the z-score and standard deviation,
    and algebraic methods such as Eigenimages based on eigenvectors are used to visually
    inspect key features in image data, which adds explainability to the final model
    outcome.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级统计和代数方法**：除了前面讨论的方法之外，其他统计方法，如寻找z分数和标准差，以及基于特征向量的特征图像等代数方法，用于视觉检查图像数据中的关键特征，这为最终的模型结果增加了可解释性。'
- en: There are other complex methods to explore image datasets depending on the type
    of the problem. However, the methods discussed in this subsection are the most
    common approaches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题的类型，探索图像数据集还有其他复杂的方法。然而，本小节中讨论的方法是最常见的方法。
- en: Exploring text data
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索文本数据
- en: 'Usually, text data is noisier in comparison to images or tabular datasets.
    Hence, EDA is usually accompanied by some preprocessing or cleaning methods for
    text data. But since we are focusing only on the EDA part, the following list
    details some of the popular approaches to do EDA with text data:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，与图像或表格数据相比，文本数据更嘈杂。因此，EDA通常伴随着一些预处理或清洗方法来处理文本数据。但因为我们只关注EDA部分，以下列表详细介绍了使用文本数据进行EDA的一些流行方法：
- en: '**Data dimension analysis**: Similar to images, text dimension analyses, such
    as checking the number of records and the length of each record, are performed
    to form hypotheses about potential overfitting or underfitting.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据维度分析**：与图像类似，文本维度分析，如检查记录数量和每条记录的长度，以形成关于潜在过拟合或欠拟合的假设。'
- en: '**Observing data distribution**: Visualizing the distribution of word frequency
    using bar plots or word clouds are popular choices in which to observe top words
    in any text data. This technique allows us to avoid any bias of high-frequency
    words as compared to low-frequency words.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察数据分布**：使用条形图或词云可视化词频分布是观察任何文本数据中顶级单词的流行选择。这种技术使我们能够避免与低频词相比高频词的任何偏差。'
- en: '**n-gram analysis**: Considering the nature of text data, often, a phrase or
    collection of words is more interpretable than only a single word. For example,
    for sentiment analysis from movie reviews, individual words with a high frequency
    such as *movie* or *film* are quite ambiguous. In contrast, phrases such as *good
    movie* and *very boring film* are far more interpretable and useful to understand
    the sentiments. Hence, n-gram analysis or taking a collection of "n-words" brings
    more explainability for understanding the model outcome.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n-gram分析**：考虑到文本数据的性质，通常，一个短语或单词集合比单个单词更容易解释。例如，对于从电影评论中进行情感分析，高频单词如*movie*或*film*相当模糊。相比之下，短语如*good
    movie*和*very boring film*则更容易解释且更有用，有助于理解情感。因此，n-gram分析或“n-words”集合的引入为理解模型结果提供了更多的可解释性。'
- en: Usually, EDA does include certain visualization techniques to explain and form
    some important hypotheses from the data. But another important technique to explain
    ML models is by visualizing the model outcome. In the next section, we will discuss
    these result visualization methods in more detail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，EDA确实包括某些可视化技术来解释数据并形成一些重要假设。但解释ML模型的重要技术之一是通过可视化模型结果。在下一节中，我们将更详细地讨论这些结果可视化方法。
- en: Result visualization methods
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果可视化方法
- en: Visualization of the model outcomes is a very common approach applied to interpret
    ML models. Generally, these are model-agnostic, post-hoc analysis methods applied
    on trained black-box models and provide explainability. In the following section,
    we will discuss some of the commonly used result visualization methods for explaining
    ML models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型结果的可视化是应用于解释机器学习模型的一种非常常见的方法。通常，这些是在训练后的黑盒模型上应用的模型无关的事后分析方法，并提供可解释性。在下一节中，我们将讨论一些常用的结果可视化方法，用于解释机器学习模型。
- en: Using comparison analysis
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用对比分析方法
- en: These are mostly post-hoc analysis methods that are used to add model explainability
    by visualizing the model's predicted output after the training process. Mostly,
    these are model-agnostic approaches that can be applied to both intrinsically
    interpretable models and black-box models. Comparison analysis can be used to
    produce both global and local explanations. It is mainly used to compare different
    possibilities of outcomes using various visualization methods.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大多是事后分析的方法，用于在训练过程之后通过可视化模型的预测输出来增加模型的可解释性。大多数情况下，这些是模型无关的方法，可以应用于内在可解释模型和黑盒模型。对比分析可用于生成全局和局部解释。它主要用于使用各种可视化方法比较不同结果的多种可能性。
- en: For example, for classification-based problems, certain methods such as t-SNE
    and PCA are used to visualize and compare the transformed feature spaces of the
    model predicted labels, especially when the error rate is high. For regression
    and time series predictive models, confidence levels are used to compare model
    predicted results with the upper and lower bounds. There are various methods to
    apply comparison analysis and get a clearer idea of the *what-if* scenarios. Some
    prominent methods are mentioned in the project repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Comparison%20Analysis.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Comparison%20Analysis.ipynb)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于基于分类的问题，某些方法如 t-SNE 和 PCA 被用于可视化并比较模型预测标签的转换特征空间，尤其是在误差率较高的情况下。对于回归和时间序列预测模型，使用置信水平来比较模型预测结果与上下限。有各种方法可以应用于对比分析，以获得更清晰的“如果...将会如何”场景的见解。一些突出方法在项目仓库中提到（[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Comparison%20Analysis.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Comparison%20Analysis.ipynb))。
- en: 'As we can see in *Figure 2.3*, result visualization can help to provide a global
    perspective about the model to visualize model predictions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *图 2.3* 中所看到的，结果可视化有助于提供关于模型的全球视角，以可视化模型预测：
- en: '![Figure 2.3 – Comparison analysis using the t-SNE method for the classification
    problem (left-hand side) and the time series prediction model with the confidence
    interval (right-hand side)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 使用 t-SNE 方法对分类问题（左侧）和时间序列预测模型（右侧）的置信区间进行对比分析]'
- en: '](img/B18216_02_03.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_02_03.jpg]'
- en: Figure 2.3 – Comparison analysis using the t-SNE method for the classification
    problem (left-hand side) and the time series prediction model with the confidence
    interval (right-hand side)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 使用 t-SNE 方法对分类问题（左侧）和时间序列预测模型（右侧）的置信区间进行对比分析
- en: In *Figure 2.3*, we can see how visualization methods can be used to provide
    local explanations by visualizing the final outcome of the model and comparing
    the outcome with either other data instances or with possible *what-if* scenarios
    to provide a global perspective of the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 2.3* 中，我们可以看到如何使用可视化方法通过可视化模型的最终结果并与其他数据实例或可能的 *如果...将会如何* 场景进行比较，从而提供模型的全球视角。
- en: Using Surrogate Explainer methods
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用代理解释器方法
- en: In the context of ML, when an external model or algorithm is applied to interpret
    a black-box ML model, the external method is known as the **Surrogate Explainer
    method**. The fundamental idea behind this approach is to apply an intrinsically
    explainable model that is simple and easy to interpret and approximate the predictions
    of the black-box model as accurately as possible. Then, certain visualization
    techniques are used to visualize the outcome from the Surrogate Explainer methods
    to get insights into the model behavior.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，当外部模型或算法被应用于解释黑盒机器学习模型时，外部方法被称为**代理解释方法**。这种方法背后的基本思想是应用一个内在可解释的模型，该模型简单易解释，并且尽可能准确地近似黑盒模型的预测。然后，使用某些可视化技术来可视化代理解释方法的结果，以深入了解模型行为。
- en: But now the question is *can we apply surrogate models directly instead of using
    the black-box model?* The answer is *no!* The main idea behind using the surrogate
    model is to get some information about how the input data is related to the target
    outcomes, without considering the model accuracy. In contrast, the original black-box
    model is more accurate and efficient but not interpretable. So, replacing the
    black-box model completely with the surrogate model would compromise the model
    accuracy, which we don't want.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在的问题是*我们能否直接应用代理模型而不是使用黑盒模型？* 答案是*不！* 使用代理模型的主要思想是获取一些关于输入数据如何与目标结果相关的信息，而不考虑模型精度。相比之下，原始的黑盒模型更准确、更高效，但不可解释。因此，完全用代理模型替换黑盒模型将损害模型精度，这是我们不愿意看到的。
- en: 'Interpretable algorithms such as regression, decision trees, and rule-based
    algorithms are popular choices for Surrogate Explainer methods. To provide explainability,
    mainly three types of relationships between the input features and the target
    outcome are analyzed: linearity, monotonicity, and interaction.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释算法，如回归、决策树和基于规则的算法，是代理解释方法的热门选择。为了提供可解释性，主要分析了输入特征与目标结果之间的三种关系类型：线性、单调性和交互性。
- en: Linearity helps us to inspect whether the input features are linearly related
    to the target outcome. Monotonicity helps us to analyze whether increasing the
    overall input feature values leads to either an increase or a decrease in the
    target outcome. For the entire range of features, this explains whether the relationship
    between the input features and the target always propagates in the same direction.
    Model interactions are extremely helpful when providing explainability, but it
    is tough to achieve. Interactions help us to analyze how individual features interact
    with each other to impact the model decision-making process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 线性关系帮助我们检查输入特征是否与目标结果呈线性相关。单调性帮助我们分析增加整体输入特征值是否会导致目标结果增加或减少。对于整个特征范围，这解释了输入特征与目标结果之间的关系是否始终向同一方向传播。模型交互在提供可解释性时非常有帮助，但实现起来很困难。交互性帮助我们分析单个特征如何相互作用以影响模型决策过程。
- en: Decision trees and rule-based algorithms are used to inspect interactions between
    the input features and the target outcome. *Christoph Molner*, in his book *Interpretable
    Machine Learning* ([https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)),
    has provided a very useful table comparing different intrinsically interpretable
    models, which can be useful for selecting interpretable models as a Surrogate
    Explainer model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和基于规则的算法用于检查输入特征与目标结果之间的交互作用。在《可解释机器学习》一书中，*克里斯托夫·莫尔纳*（[https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/))）提供了一个非常有用的表格，比较了不同的内在可解释模型，这对于选择作为代理解释器的可解释模型非常有用。
- en: 'A simplified version of this is shown in the following table:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表格展示了这一过程的简化版本：
- en: '![Figure 2.4 – Comparing interpretable algorithms for selecting a Surrogate
    Explainer method'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 比较选择代理解释方法的可解释算法'
- en: '](img/B18216_02_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_04.jpg)'
- en: Figure 2.4 – Comparing interpretable algorithms for selecting a Surrogate Explainer
    method
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 比较选择代理解释方法的可解释算法
- en: One of the major advantages of this technique is that it can help to make any
    black-box model interpretable. It is model-agnostic and very easy to implement.
    But when the data is complex, more sophisticated algorithms are used to achieve
    higher modeling accuracy. In such cases, surrogate methods tend to oversimplify
    complicated patterns or relationships between the input features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的最大优点之一是它可以帮助使任何黑盒模型可解释。它是模型无关的，并且非常容易实现。但是，当数据复杂时，会使用更复杂的算法来达到更高的建模精度。在这种情况下，代理方法往往会过度简化输入特征之间的复杂模式或关系。
- en: '*Figure 2.5* illustrates how interpretable algorithms such as decision trees,
    linear regression, or any heuristic rule-based algorithm can be used as surrogate
    models to explain any black-box ML model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.5* 展示了如何使用可解释的算法，如决策树、线性回归或任何基于启发式规则的算法，作为代理模型来解释任何黑盒机器学习模型：'
- en: '![Figure 2.5 – Using Surrogate explainers for model explainability'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – 使用代理解释器进行模型可解释性'
- en: '](img/B18216_02_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_05.jpg)'
- en: Figure 2.5 – Using Surrogate explainers for model explainability
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 使用代理解释器进行模型可解释性
- en: Despite the drawbacks of this approach, visualization of linearity, monotonicity,
    and interaction can justify working on complex black-box models to a great extent.
    In the next section, we will discuss influence-based methods to explain ML models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法存在缺点，但线性、单调性和交互性的可视化在很大程度上可以证明在复杂黑盒模型上工作的合理性。在下一节中，我们将讨论基于影响的方法来解释机器学习模型。
- en: Influence-based methods
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于影响的方法
- en: Influence-based methods are used to understand the impact of features present
    in the dataset on the model's decision-making process. Influence-based methods
    are widely used and preferred in comparison to other methods as this helps to
    identify the dominating attributes from the dataset. Identifying the dominating
    attributes from structured and unstructured data helps us analyze the dominating
    features' role in influencing the model outcome.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于影响的方法被用来理解数据集中特征对模型决策过程的影响。与其它方法相比，基于影响的方法被广泛使用且更受欢迎，因为这有助于从数据集中识别出主导属性。从结构化和非结构化数据中识别主导属性有助于我们分析主导特征在影响模型结果中的作用。
- en: For example, let's say you are working on a classification problem for classifying
    wolves and Siberian huskies. Let's suppose that after the training and evaluation
    process, you have achieved a good model with more than 95% accuracy. But when
    trying to find the important features using influence-based methods for model
    explainability, you observed that the model picked up the surrounding background
    as the dominating feature to classify whether it is a wolf or a husky. In such
    cases, even if your model result seems to be highly accurate, your model is unreliable.
    This is because the features that the model was making the decision on are not
    robust and generalized.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在处理一个分类问题，用于区分狼和西伯利亚雪橇犬。假设在训练和评估过程之后，你已经得到了一个准确率超过95%的良好模型。但在尝试使用基于影响的方法来寻找模型可解释性的重要特征时，你观察到模型将周围背景作为分类是否为狼或雪橇犬的主导特征。在这种情况下，即使你的模型结果看起来高度准确，你的模型也是不可靠的。这是因为模型做出决策的特征并不稳健且不具有普遍性。
- en: Influence-based methods are popularly used for performing root cause analysis
    to debug ML models and detect failures in ML systems. Now, let's discuss some
    of the popular choices of the influence-based methods that are used for model
    explainability.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于影响的方法常用于执行根本原因分析以调试机器学习模型和检测机器学习系统中的故障。现在，让我们讨论一些用于模型可解释性的基于影响方法的流行选择。
- en: Feature importance
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: When applying an ML model, understanding the relative importance of each feature
    in terms of influencing the model outcome is crucial. It is a technique that assigns
    a particular score to the input features present in the dataset based on the usefulness
    of the features in predicting the target value. Feature importance is a very popular
    choice of model-agnostic explainability for modeling structured data. Although
    there are various scoring mechanisms to determine feature importance, such as
    permutation importance scores, statistical correlation scores, decision tree-based
    scoring, and more, in this section, we will mostly focus on the overall method
    and not just on the scoring mechanism.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习模型时，理解每个特征相对于影响模型结果的重要性至关重要。这是一种技术，它根据特征在预测目标值中的有用性，对数据集中存在的输入特征分配特定的分数。特征重要性是结构化数据建模中非常流行的模型无关可解释性选择。尽管有各种评分机制来确定特征重要性，例如排列重要性分数、统计相关性分数、基于决策树的评分等，但在本节中，我们将主要关注整体方法，而不仅仅是评分机制。
- en: In the context of XAI, feature importance can provide global insights into the
    data and the model behavior. It is often used for feature selection and dimensionality
    reduction to improve the efficiency of ML models. By removing less important features
    from the modeling process, it has been observed that, usually, the overall model
    performance is improved.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在XAI的背景下，特征重要性可以提供对数据和模型行为的全局洞察。它通常用于特征选择和降维，以提高机器学习模型的效率。通过从建模过程中移除不太重要的特征，观察到通常整体模型性能得到提高。
- en: The notion of important features can sometimes depend on the type of scoring
    mechanism or the type of model used. So, it is recommended that you validate the
    important features picked up by this technique with domain experts before drawing
    any conclusions. This method is applied to structured datasets, where the features
    are clearly defined. For unstructured data such as text or images, feature importance
    is not very relevant as the features or patterns used by the model are more complex
    and not always human interpretable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要特征的概念有时可能取决于评分机制或所使用的模型类型。因此，建议在得出任何结论之前，您应该使用领域专家验证此技术选择的重要特征。此方法适用于结构化数据集，其中特征被明确定义。对于文本或图像等非结构化数据，特征重要性并不非常相关，因为模型使用的特征或模式更为复杂，并且并不总是可由人类解释。
- en: '*Figure 2.6* illustrates how highlighting the influential features of a dataset
    enables the end user to focus on the values of the important features to justify
    the model outcome:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.6* 展示了如何突出显示数据集的有影响特征，使最终用户能够关注重要特征的价值，以证明模型结果：'
- en: '![Figure 2.6 – A feature importance graph on the diabetes dataset (from the
    code tutorials)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 – 糖尿病数据集上的特征重要性图（来自代码教程）'
- en: '](img/B18216_02_06.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_06.jpg)'
- en: Figure 2.6 – A feature importance graph on the diabetes dataset (from the code
    tutorials)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 糖尿病数据集上的特征重要性图（来自代码教程）
- en: Next, we will cover another important influence-based model explainability method,
    called sensitivity analysis.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍另一种重要的基于影响度的模型可解释性方法，称为敏感性分析。
- en: Sensitivity analysis
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏感性分析
- en: Sensitivity analysis is a quantitative process that approximates uncertainty
    in forecasts by altering the assumptions made about important input features used
    by the forecasting model. In sensitivity analysis, the individual input feature
    variables are increased or decreased to assess the impact of the individual features
    on the target outcome. This technique is very commonly used in predictive modeling
    to optimize the overall performance and robustness of the system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性分析是一种定量过程，通过改变关于预测模型使用的输入特征所做的假设来近似预测的不确定性。在敏感性分析中，单个输入特征变量被增加或减少以评估单个特征对目标结果的影响。这种技术在预测建模中非常常用，用于优化系统的整体性能和鲁棒性。
- en: Conducting sensitivity analysis can be simple yet a very powerful method for
    any data science project, which can provide additional information to business
    stakeholders, especially for multivariate datasets. It helps to understand the
    *what-if* scenarios and observe whether any particular feature is sensitive to
    outliers or any form of adversarial perturbations. It helps in questioning the
    reliability of variable assumptions, can predict the possible outcomes if the
    assumptions are changed, and can measure the significance of altering variable
    assumptions. Sensitivity analysis is a data-driven modeling approach. It indicates
    whether the data is reliable, accurate, and relevant for the modeling process.
    Additionally, it helps to find out whether there are other intervening factors
    that can impact the model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 进行敏感性分析可能很简单，但却是任何数据科学项目非常强大的方法，可以为商业利益相关者提供额外信息，特别是对于多元数据集。它有助于理解“如果...会怎样”的情景，并观察是否有任何特定特征对异常值或任何形式的对抗性扰动敏感。它有助于质疑变量假设的可靠性，预测假设改变时的可能结果，并衡量改变变量假设的重要性。敏感性分析是一种数据驱动的建模方法。它表明数据是否可靠、准确且与建模过程相关。此外，它有助于找出是否有其他可能影响模型的因素。
- en: In the context of XAI, since sensitivity analysis is slightly less common as
    compared to some of the widely used methods, let me try to give my recommendations
    for performing sensitivity analysis in ML. Usually, this is very useful for regression
    problems, but it is quite important for classification-based problems, too.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在XAI的背景下，由于敏感性分析相对于一些广泛使用的方法来说稍微不那么常见，让我尝试给出我在机器学习中进行敏感性分析的建议。通常，这对于回归问题非常有用，但对于基于分类的问题也非常重要。
- en: Please refer to the notebook ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/FeatureImportance_SensitivityAnalysis.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/FeatureImportance_SensitivityAnalysis.ipynb))
    provided in the GitHub repository to get a detailed and practical approach for
    doing sensitivity analysis. The very first step that I recommend you do for sensitivity
    analysis is to calculate the standard deviation (σ) of each attribute that is
    present in the raw dataset. Then, for each attribute, transform the original attribute
    values to -3σ, -2σ, -σ, σ, 2σ, and 3σ, and either observe and plot the percentage
    change in the target outcome for a regression problem or observe the predicted
    class for a classification-based problem.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考GitHub仓库中提供的笔记本（[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/FeatureImportance_SensitivityAnalysis.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/FeatureImportance_SensitivityAnalysis.ipynb)），以获取进行敏感性分析的详细和实用方法。我建议你为敏感性分析做的第一步是计算原始数据集中每个属性的方差（σ）。然后，对于每个属性，将原始属性值转换为-3σ、-2σ、-σ、σ、2σ和3σ，并观察回归问题中目标结果的百分比变化或观察基于分类问题的预测类别。
- en: For a good and robust model, we would want the target outcome to be less sensitive
    to any change in the feature values. Ideally, we would expect the percentage change
    in the target outcome to be less drastic for regression problems, and for classification
    problems, the predicted class should not change much on changing the feature values.
    Any feature value beyond +/- 3σ is considered an outlier, so usually, we vary
    the feature values up to +/- 3σ.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个好的和稳健的模型，我们希望目标结果对特征值的任何变化都不太敏感。理想情况下，我们期望回归问题中目标结果的百分比变化不太剧烈，对于分类问题，预测类别在改变特征值时不应有太大变化。任何超过/低于+/-
    3σ的特征值都被认为是异常值，因此通常我们只改变特征值到+/- 3σ。
- en: '*Figure 2.7* shows how detailed sensitivity analysis helps you analyze factors
    that can easily influence the model outcome:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.7*展示了详细的敏感性分析如何帮助你分析容易影响模型结果的因素：'
- en: '![Figure 2.7 – Sensitivity analysis to understand the influential features
    of the data'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7 – 通过敏感性分析理解数据的有影响特征'
- en: '](img/B18216_02_07.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_02_07.jpg)'
- en: Figure 2.7 – Sensitivity analysis to understand the influential features of
    the data
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 通过敏感性分析理解数据的有影响特征
- en: Apart from sensitivity analysis, in the next section, you will learn about **Partial
    Dependence Plots** (**PDPs**), which can also be used to analyze influential features.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了敏感性分析之外，在下一节中，你将了解**部分依赖图**（**PDPs**），它也可以用来分析有影响特征。
- en: PDPs
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDPs
- en: When using black-box ML models, inspecting functional relations between feature
    attributes and target outcomes can be challenging. Although calculating feature
    importance can be easier, PDPs provide a mechanism to functionally calculate the
    relationship between the predictive features and the predictor variables. It shows
    the marginal effect one or two attributes have on the target outcome.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用黑盒机器学习模型时，检查特征属性与目标结果之间的功能关系可能具有挑战性。尽管计算特征重要性可能更容易，但 PDPs 提供了一种功能上计算预测特征与预测变量之间关系的机制。它显示了单个或两个属性对目标结果的影响。
- en: PDPs can effectively help pick up linear, monotonic, or any complex interaction
    between the predictive variables and the predictor variables and indicate the
    overall impact that the predictor variable has on the predictive variable on average.
    PDP includes the contribution of particular predictor attributes by measuring
    the marginal effect, which does not include the other variables' impact on the
    feature space.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs 可以有效地帮助识别预测变量和预测变量之间的线性、单调或任何复杂交互，并指示预测变量对预测变量的平均影响。PDP 通过测量边际效应来衡量特定预测属性的贡献，该效应不包括其他变量对特征空间的影响。
- en: 'Similar to sensitivity analysis, PDPs help us to approximate the direction
    in which specific features can influence the target outcome. For the sake of simplicity,
    I will not add any complex mathematical representation of partial dependence to
    obtain the average marginal effect of predictor variables; however, I would strongly
    recommend going through *Jerome H. Friedman*''s work on *Greedy Function Approximation:
    A Gradient Boosting Machine* to get more information. One of the most significant
    benefits of PDP is that it is straightforward to use, implement, and understand
    and can be explained to a non-technical business stakeholder or an end user of
    ML models very easily.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '与敏感性分析类似，PDPs 帮助我们近似特定特征可以影响目标结果的方向。为了简化，我将不会添加任何复杂的数学表示来获得预测变量的平均边际效应；然而，我强烈建议阅读
    *Jerome H. Friedman* 的关于 *Greedy Function Approximation: A Gradient Boosting Machine*
    的工作，以获取更多信息。PDP 的一个最显著的好处是它易于使用、实现和理解，并且可以很容易地向非技术业务利益相关者或机器学习模型的最终用户解释。'
- en: But there are certain drawbacks to this approach, too. By default, the approach
    assumes that all features are not correlated and there is no interaction between
    the feature attributes. In any practical scenario, this is highly unlikely to
    happen, as the majority of the time, there will be some interaction or joint effect
    due to the feature variables.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法也存在某些缺点。默认情况下，该方法假设所有特征之间没有相关性，并且特征属性之间没有交互作用。在任何实际场景中，这种情况极不可能发生，因为大多数时候，由于特征变量，总会有一些交互或联合效应。
- en: PDPs are also limited to two-dimensional representations, and PDPs do not show
    any feature distribution. So, if the feature space is not evenly distributed,
    certain effects of bias can get missed while analyzing the outcome. PDPs might
    not show any heterogeneous effects as it only shows the average marginal effects.
    This means that if half of the data for a particular feature has a positive impact
    on the predicted outcome, and the other half has a negative effect on the predicted
    outcome, then the PDP could just be a horizontal line as the effects from both
    halves can cancel each other. This can lead to the conclusion that the feature
    does not have any impact on the target variable, which is misleading.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs 也局限于二维表示，并且 PDPs 并不显示任何特征分布。因此，如果特征空间分布不均匀，分析结果时可能会错过某些偏差的影响。由于 PDPs 只显示平均边际效应，它可能不会显示出任何异质效应。这意味着，如果某个特定特征的半数数据对预测结果有正面影响，而另一半则有负面影响，那么
    PDP 可能仅仅是一条水平线，因为两半的影响可以相互抵消。这可能导致结论认为该特征对目标变量没有任何影响，这是误导性的。
- en: The drawbacks of PDP can be solved by **Accumulated Local Effect Plots** (**ALEP**)
    and **Individual Conditional Expectation Curves** (**ICE curves**). We will not
    be covering these concepts in this chapter, but please refer to the *Reference*
    section, *[Reference – 4,5]*, to get additional resources to help you understand
    these concepts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 **累积局部效应图**（**ALEP**）和 **个体条件期望曲线**（**ICE 曲线**）可以解决 PDP 的缺点。我们不会在本章中涵盖这些概念，但请参考
    *参考文献* 部分，*[参考文献 – 4,5]*，以获取帮助您理解这些概念的额外资源。
- en: 'Let''s look at some sample PDP visualizations from *Figure 2.8*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 *图 2.8* 中的某些样本 PDP 可视化：
- en: '![Figure 2.8 – PDP visualizations (from the code tutorial)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.8 – PDP可视化（来自代码教程）]'
- en: '](img/B18216_02_08.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_02_08.jpg)'
- en: Figure 2.8 – PDP visualizations (from the code tutorial)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – PDP可视化（来自代码教程）
- en: '*Figure 2.8* illustrates PDP visualizations that help us to understand influential
    features from tabular datasets. In the next section, we will discuss the **Layer-wise
    Relevance Propagation** (**LRP**) methods to understand influential features from
    unstructured data.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.8* 展示了有助于我们理解来自表格数据集的有影响力的特征的PDP可视化。在下一节中，我们将讨论**层相关传播（Layer-wise Relevance
    Propagation，LRP**）方法，以理解非结构化数据中的有影响力特征。'
- en: LRP
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LRP
- en: Most of the influence-based methods that we discussed earlier are highly effective
    for structured data. But unfortunately, these methods cannot be applied to unstructured
    data such as images and texts where the features are not always clearly defined,
    especially when using **Deep Convolution Neural Networks** (**DCNNs**). Classical
    ML algorithms are not efficient as compared to deep learning algorithms when applied
    to unstructured data such as images and text. Due to the benefit of automatic
    feature extraction in deep learning as compared to manual feature engineering
    in classical ML, deep learning algorithms are more efficient in terms of model
    accuracy and, hence, more preferred. However, deep learning models are more complex
    and less interpretable than classical ML models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的大多数基于影响的方法对结构化数据非常有效。但不幸的是，这些方法不能应用于图像和文本等非结构化数据，在这些数据中，特征并不总是明确定义的，尤其是在使用**深度卷积神经网络（Deep
    Convolution Neural Networks，DCNNs**）时。与应用于图像和文本等非结构化数据相比，经典机器学习算法的效率不如深度学习算法。由于深度学习在自动特征提取方面的优势，与经典机器学习中的手动特征工程相比，深度学习算法在模型精度方面更高效，因此更受欢迎。然而，深度学习模型比经典机器学习模型更复杂，可解释性也更差。
- en: Providing explainability to deep learning models is also quite challenging;
    usually, there are very few quantitative ways for providing explainability to
    deep learning models. Therefore, we mostly rely on qualitative approaches to visualize
    the key influencing data elements that can impact the process of calculating weights
    and biases, which are the main parameters of any deep learning model. Moreover,
    for deep networks with multiple layers, learning happens when the flow of information
    through the gradient flow process between the layers is maintained consistently.
    So, to explain any deep learning model, particularly for images and text, we would
    try to visualize the *activated* or most influential data elements throughout
    the different layers of the network and qualitatively inspect the functioning
    of the algorithm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习模型提供可解释性也是一个相当大的挑战；通常，为深度学习模型提供可解释性的定量方法非常少。因此，我们主要依靠定性方法来可视化可能影响权重和偏差计算过程的关键影响数据元素，这些是任何深度学习模型的主要参数。此外，对于具有多个层的深度网络，当层之间的梯度流过程的信息流保持一致时，就会发生学习。因此，为了解释任何深度学习模型，特别是图像和文本，我们会尝试可视化网络不同层中的*激活*或最有影响力的数据元素，并定性检查算法的运行情况。
- en: To explain deep learning models, LRP is one of the most prominent approaches.
    Intuitively speaking, this method utilizes the weights in the network and the
    forward pass neural activations to propagate the output back to the input layer
    through the various layers in the network. So, with the help of the network weights,
    we can visualize the data elements (pixels in the case of images and words in
    the case of text data) that contributed most toward the final model output. The
    contribution of these data elements is a qualitative measure of relevance that
    gets propagated throughout the network layers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释深度学习模型，LRP（Layer-wise Relevance Propagation）是其中最突出的方法之一。直观地说，这种方法利用网络中的权重和前向传递的神经激活来通过网络的各个层将输出反向传播到输入层。因此，借助网络权重，我们可以可视化对最终模型输出贡献最大的数据元素（在图像的情况下是像素，在文本数据的情况下是单词）。这些数据元素的贡献是相关性的定性度量，它在网络层之间传播。
- en: Now, we will explore some specific LRP methods that have been applied to explain
    the working of deep learning models. In practice, implementing these methods can
    be challenging. So, I have not included these methods in the code tutorials, as
    this chapter is supposed to help even beginner learners. I have shared some resources
    in the *Reference* section for intermediate or advanced learners for the code
    walk-throughs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨一些已经应用于解释深度学习模型工作原理的具体LRP方法。在实践中，实现这些方法可能具有挑战性。因此，我没有将这些方法包含在代码教程中，因为本章旨在帮助即使是初学者也能学习。我在*参考*部分分享了一些资源，供中级或高级学习者进行代码演练。
- en: Saliency maps
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显著性图
- en: A saliency map is one of the most popularly used approaches for interpreting
    the prediction of **Convolution Neural Networks** (**CNNs**). This technique is
    derived from the concept of image saliency, which refers to the important features
    of an image, such as the pixels, which are visually alluring. So, a saliency map
    is another image derived from the original image in which the pixel brightness
    is directly proportional to the saliency of the image. A saliency map helps to
    highlight regions within the image that play an important role in the final decision-making
    process for the model. It is a visualization technique used specifically in DCNN
    models to differentiate visual features from the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图是用于解释**卷积神经网络**（**CNN**）预测结果的最常用方法之一。这项技术源于图像显著性概念，它指的是图像的重要特征，例如像素，这些像素在视觉上具有吸引力。因此，显著性图是从原始图像派生出的另一张图像，其中像素亮度与图像的显著性成正比。显著性图有助于突出图像中在模型最终决策过程中发挥重要作用的区域。它是一种专门用于DCNN模型的可视化技术，用于区分视觉特征和数据。
- en: Apart from providing explainability to deep learning models, saliency maps can
    be used to identify regions of interest, which can be further used by automated
    image annotation algorithms. Also, saliency maps are used in the audio domain,
    particularly in audio surveillance, to detect unusual sound patterns such as gunshots
    or explosions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为深度学习模型提供可解释性之外，显著性图还可以用于识别感兴趣的区域，这些区域可以进一步由自动图像标注算法使用。此外，显著性图在音频领域，尤其是在音频监控中，用于检测不寻常的声音模式，如枪声或爆炸声。
- en: '*Figure 2.9* shows the saliency map for a given input image, which highlights
    the important pixels used by the model to predict the outcome:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.9*显示了给定输入图像的显著性图，它突出了模型用于预测结果的重要像素：'
- en: '![Figure 2.9 – Saliency maps for an input image'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 – 输入图像的显著性图'
- en: '](img/B18216_02_09.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_09.jpg)'
- en: Figure 2.9 – Saliency maps for an input image
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – 输入图像的显著性图
- en: Next, let's cover another popular LRP method – **Guided Backpropagation** (**Guided
    Backprop**).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来介绍另一种流行的LRP方法——**引导反向传播**（**引导反向传播**）。
- en: Guided backprop
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引导反向传播
- en: Another visualization technique used for explaining deep learning models to
    increase trust and their adoption is **guided backprop**. Guided backprop highlights
    granular visual details in an image to interpret why a particular class was predicted
    by the model. It is also known as guided saliency and actually combines the process
    of vanilla backpropagation and backpropagation through ReLU nonlinearity (also
    referred to as **DeconvNets**). I would strongly recommend looking at this article,
    [https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e](https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e), to
    learn more about the backpropagation and the DeconvNet mechanism if you are not
    aware of these terms.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于解释深度学习模型、增加信任并促进其采用的可视化技术是**引导反向传播**。引导反向传播突出图像中的细粒度视觉细节，以解释模型为何预测了特定的类别。它也被称为引导显著性，实际上结合了标准反向传播和通过ReLU非线性（也称为**反卷积网络**）的反向传播过程。如果您不熟悉这些术语，我强烈建议您阅读这篇文章，[https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e](https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e)，以了解更多关于反向传播和反卷积网络机制的信息。
- en: In this method, the neurons of the network act as the feature detectors and,
    because of the usage of the ReLU activation function, only gradient elements that
    are positive in the feature map are kept. Additionally, the DeconvNets only keep
    the positive error signals. Since the negative gradients are set to zero, only
    the important pixels are highlighted when backpropagating through the ReLU layers.
    Therefore, this method helps to visualize the key regions of the image, the vital
    shapes, and the contours of the object that are to be classified by the algorithm.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，网络的神经元充当特征检测器，由于使用了ReLU激活函数，因此只保留特征图中正的梯度元素。此外，DeconvNets只保留正的错误信号。由于负梯度被设置为零，因此在通过ReLU层反向传播时，只有重要的像素被突出显示。因此，这种方法有助于可视化图像的关键区域、重要的形状以及算法将要分类的对象的轮廓。
- en: '*Figure 2.10* shows a guided backprop map for a given input image that marks
    the contours and some granular visual features used by the model to predict the
    outcome:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.10*显示了给定输入图像的引导反向传播图，该图标记了模型用于预测结果的轮廓和一些粒度视觉特征：'
- en: '![Figure 2.10 – Guided backprop for an input image'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10 – 输入图像的引导反向传播'
- en: '](img/B18216_02_10.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_02_10.jpg](img/B18216_02_10.jpg)'
- en: Figure 2.10 – Guided backprop for an input image
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 输入图像的引导反向传播
- en: Guided backprop is very useful, but in the next section, we will cover another
    useful method to explain unstructured data such as images, called the Gradient
    CAM.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 引导反向传播非常有用，但在下一节中，我们将介绍另一种有用的方法来解释图像等非结构化数据，称为梯度CAM。
- en: Gradient CAM
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度CAM
- en: CAMs are separate visualization methods used for explaining deep learning models.
    Here, the model predicted class scores are traced back to the last convolution
    layer to highlight discriminative regions of interest in the image that are class-specific
    and not even generic to other computer vision or image processing algorithms.
    **Gradient CAM** combines the effect of guided backprop and CAM to highlight class
    discriminative regions of interest without highlighting the granular pixel importance,
    unlike guided backprop. But Grad-CAM can be applied to any CNN architectures,
    unlike CAM, which can be applied to architectures that perform global average
    pooling over output feature maps coming from the convolution layer, just prior
    to the prediction layer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: CAMs是用于解释深度学习模型的独立可视化方法。在这里，模型预测的类别分数被追踪回最后一个卷积层，以突出图像中具有类别特定性且甚至不适用于其他计算机视觉或图像处理算法的感兴趣区域。**梯度CAM**结合了引导反向传播和CAM的效果，在不突出像素粒度重要性时突出感兴趣区域的类别判别性，这与引导反向传播不同。但Grad-CAM可以应用于任何CNN架构，而CAM只能应用于在预测层之前对来自卷积层的输出特征图执行全局平均池化的架构。
- en: '**Grad-CAM** (also referred to as **Gradient-Weighted Class Activation Map**)
    helps to visualize high-resolution details, which are often superimposed on the
    original image to highlight dominating image regions for predicting a particular
    class. It is extremely useful for multi-class classification models. Grad-CAM
    works by inspecting the gradient information flow into the last layer of the model.
    However, for certain cases, it is important to inspect fine-grained pixel activation
    information, too. Since Grad-CAM doesn''t allow us to inspect granular information,
    there is another variant of Grad-CAM, which is known as **Guided Grad-CAM**, used
    to combine the benefits of guided backprop with Grad-CAM to even visualize the
    granular-level class discriminative information in the image.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**Grad-CAM**（也称为**梯度加权类别激活图**）有助于可视化高分辨率细节，这些细节通常叠加在原始图像上，以突出预测特定类别的占主导地位的图像区域。对于多类分类模型来说，它非常有用。Grad-CAM通过检查模型最后一层的梯度信息流来工作。然而，对于某些情况，检查细粒度像素激活信息也很重要。由于Grad-CAM不允许我们检查粒度信息，因此存在Grad-CAM的另一种变体，称为**引导Grad-CAM**，用于结合引导反向传播和Grad-CAM的优点，以可视化图像中的粒度级类别判别信息。'
- en: '*Figure 2.11* shows what a Grad-CAM visualization looks like for any input
    image:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.11*显示了任何输入图像的Grad-CAM可视化效果：'
- en: '![Figure 2.11 – Grad-CAM for an input image'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.11 – 输入图像的Grad-CAM'
- en: '](img/B18216_02_11.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_02_11.jpg](img/B18216_02_11.jpg)'
- en: Figure 2.11 – Grad-CAM for an input image
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 输入图像的Grad-CAM
- en: 'Grad-CAM highlights the important regions in the image, which are used by the
    model to predict the outcome. Another variant of this approach is to use guided
    Grad-CAM, which combines the guided backpropagation and Grad-CAM methods to produce
    interesting visualizations to explain deep learning models:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Grad-CAM突出了图像中的重要区域，这些区域被模型用于预测结果。这种方法的另一种变体是使用引导Grad-CAM，它结合了引导反向传播和Grad-CAM方法，以产生有趣的可视化来解释深度学习模型：
- en: '![Figure 2.12 – Architecture diagram for guided Grad-CAM'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.12 – 引导Grad-CAM的架构图'
- en: '](img/B18216_02_12.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_12.jpg)'
- en: Figure 2.12 – Architecture diagram for guided Grad-CAM
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 引导Grad-CAM的架构图
- en: '*Figure 2.12* shows the architecture diagram for the guided Grad-CAM approach
    that is slightly more complex to understand. But overall, LRP is an important
    approach that can be used to explain the functioning of deep learning models.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.12*显示了引导Grad-CAM方法的架构图，这个图稍微复杂一些，但总体来说，LRP是一种重要的方法，可以用来解释深度学习模型的工作原理。'
- en: Representation-based explanation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于表示的解释
- en: Pattern representation plays an important role in the decision-making process,
    especially for unstructured data such as text and images. Conventionally, hand-engineered
    pattern matching algorithms were used for extracting global features, which human
    beings can relate to. But recently, GoogleAI's model interpretability technique,
    which is based on CAVs, gained great popularity in the field of XAI. In this part,
    we will discuss CAVs in more detail, although extracting features and patterns
    from unstructured data also falls under the representation-based explanations.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 模式表示在决策过程中起着重要作用，尤其是在文本和图像等非结构化数据中。传统上，使用手工设计的模式匹配算法来提取全局特征，这些特征是人类可以关联的。但最近，基于CAVs的GoogleAI模型可解释性技术，在XAI领域获得了极大的普及。在本部分，我们将更详细地讨论CAVs，尽管从非结构化数据中提取特征和模式也属于基于表示的解释。
- en: CAVs
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CAVs
- en: Particularly for unstructured data, most deep learning models work on low-level
    features such as edges, contours, and motifs, and some mid-level and high-level
    features such as certain defined parts and portions of the object of interest.
    Most of the time, these representations are not human-friendly, especially for
    complex deep learning models. Intuitively, CAVs relate the presence of low-level
    and granular features to high-level human-friendly concepts. Therefore, model
    explainability with CAVs provides more realistic explanations to which any human
    being can relate.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于非结构化数据，大多数深度学习模型在低级特征上工作，如边缘、轮廓和模式，以及一些中级和高级特征，如感兴趣对象的一些定义部分和部分。大多数情况下，这些表示不是人类友好的，尤其是对于复杂的深度学习模型。直观上，CAVs将低级和细粒度特征与高级人类友好概念联系起来。因此，使用CAVs的模型可解释性提供了更现实的解释，任何人类都可以与之相关联。
- en: The approach of CAVs is actually implemented using the **Testing with Concept
    Activation Vectors** (**TCAV**) framework from GoogleAI. TCAV utilizes directional
    derivatives to approximate the internal state of the neural network to a human-defined
    concept. For example, if we ask a human being to explain what a zebra looks like,
    they would probably say that a zebra is an animal that looks like a white horse
    with black stripes and is found in grasslands. So, the terms *animal*, *white
    horse*, *black stripes*, and *grasslands* can be important concepts used to represent
    zebras.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: CAVs的方法实际上是通过使用GoogleAI的**概念激活向量测试**（**TCAV**）框架来实现的。TCAV利用方向导数来近似神经网络内部状态到人类定义的概念。例如，如果我们要求一个人解释斑马是什么样子，他们可能会说斑马是一种看起来像白色马匹带有黑色条纹的动物，并且生活在草原上。因此，*动物*、*白色马匹*、*黑色条纹*和*草原*可以代表斑马的重要概念。
- en: Similarly, the TCAV algorithm tries to learn these concepts and learn how much
    of the concept was important for the prediction using the trained model, although
    these concepts might not be used during the training process. So, TCAV tries to
    quantify how sensitive the model is toward the particular concept for a particular
    class. I found the idea of CAVs very appealing. I think it is a step toward creating
    human-friendly explanations of AI models, which any non-technical user can easily
    relate to. We will be discussing the TCAV framework from GoogleAI, in more detail,
    in [*Chapter 8*](B18216_08_ePub.xhtml#_idTextAnchor154), *Human-Friendly Explanations
    with TCAV*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，TCAV算法试图学习这些概念，并使用训练好的模型学习这些概念对预测的重要性，尽管这些概念可能在训练过程中没有被使用。因此，TCAV试图量化模型对特定类别的特定概念敏感度。我发现CAVs的想法非常吸引人。我认为这是创建人类友好型AI模型解释的一步，任何非技术用户都可以轻松地与之相关联。我们将在[*第8章*](B18216_08_ePub.xhtml#_idTextAnchor154)，*使用TCAV进行人类友好型解释*中更详细地讨论GoogleAI的TCAV框架。
- en: '*Figure 2.13* illustrates the idea of using human-friendly concepts to explain
    model predictions. In the next subsection, we will see another visualization approach
    that is used to explain complex deep learning models:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.13*展示了使用人类友好概念来解释模型预测的想法。在下一小节中，我们将看到另一种用于解释复杂深度学习模型的可视化方法：'
- en: '![Figure 2.13 – The fundamental idea behind the CAV'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.13 – The fundamental idea behind the CAV'
- en: '](img/B18216_02_13.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_02_13.jpg](img/B18216_02_13.jpg)'
- en: Figure 2.13 – The fundamental idea behind the CAV
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – CAV背后的基本思想
- en: Next, we will discuss **Visual Attention Maps** (**VAMs**), which can also be
    used with complicated deep learning models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论**视觉注意力图**（**VAMs**），它也可以与复杂的深度学习模型一起使用。
- en: VAMs
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAMs
- en: In recent years, transformer model architectures have gained a lot of popularity
    because of their ability to achieve state-of-the-art model performance on complicated
    unstructured data. Attention networks are the heart of transformer architecture,
    which allows the algorithm to learn more contextual information for producing
    more accurate outcomes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由于在复杂非结构化数据上能够实现最先进的模型性能，转换器模型架构因其能力而获得了大量关注。注意力网络是转换器架构的核心，它允许算法学习更多上下文信息，以产生更准确的结果。
- en: The basic idea is that every portion of the data is not equally important and
    only the important features need more *attention* than the rest of the data. Therefore,
    the attention network filters out irrelevant portions of the data for making a
    better judgment. By the attention mechanism, the network can assign higher weights
    to the important sections by the level of importance to the underlying task. Using
    these attention weights, certain visualizations can be created which explain the
    decision-making process of the complex algorithm. These are called VAMs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是数据中的每一部分并不同等重要，只有重要的特征需要比其他数据更多的*关注*。因此，注意力网络会过滤掉无关的数据部分，以便做出更好的判断。通过注意力机制，网络可以根据对底层任务的重要性级别分配更高的权重给重要部分。使用这些注意力权重，可以创建某些可视化，这些可视化可以解释复杂算法的决策过程。这些被称为VAMs。
- en: This technique is particularly useful for **Multimodal Encoder-Decoder** architectures
    for solving problems such as automated image captioning and visual question answering.
    Applying VAMs can be quite complicated if you have a beginner level of understanding.
    So, I will not cover this technique in much detail in this book or in the code
    tutorials. If you are interested in learning more about how this technique work
    in practice, please refer to the code repository at [https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在解决诸如自动图像标题和视觉问答等问题时，对于**多模态编码器-解码器**架构特别有用。如果你对这种技术的理解处于入门水平，应用VAMs可能会相当复杂。因此，在这本书或代码教程中，我将不会详细介绍这项技术。如果你对了解这种技术在实践中是如何工作的感兴趣，请参考代码仓库[https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)。
- en: 'As we can see in *Figure 2.14*, VAMs provide step-by-step visuals to explain
    the output of complex encoder-decoder models. In the next section, we will explore
    example-based methods, which are used to explain ML models:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2.14*所示，VAMs提供了逐步可视化来解释复杂编码器-解码器模型的输出。在下一节中，我们将探讨基于示例的方法，这些方法用于解释机器学习模型：
- en: '![Figure 2.14 – Using VAMs to explain complex encoder-decoder attention-based
    deep learning models for the task of automated image captioning using a multi-modal
    dataset'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14 – 使用 VAMs 解释用于多模态数据集自动图像标题任务的复杂编码器-解码器注意力深度学习模型'
- en: '](img/B18216_02_14.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_14.jpg)'
- en: Figure 2.14 – Using VAMs to explain complex encoder-decoder attention-based
    deep learning models for the task of automated image captioning using a multi-modal
    dataset
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 – 使用 VAMs 解释用于多模态数据集自动图像标题任务的复杂编码器-解码器注意力深度学习模型
- en: In the next section, we will cover another type of explainability method that
    uses human-friendly examples to interpret predictions from black-box models.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍另一种使用人类友好示例来解释黑盒模型预测的可解释性方法。
- en: Example-based methods
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于示例的方法
- en: Another approach to model explainability is provided by example-based methods.
    The idea of example-based methods is similar to how humans try to explain a new
    concept. As human beings, when we try to explain or introduce something new to
    someone else, often, we try to make use of examples that our audience can relate
    to. Similarly, example-based methods, in the context of XAI, try to select certain
    instances of the dataset to explain the behavior of the model. It assumes that
    observing similarities between the current instance of the data with a historic
    observation can be used to explain black-box models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性的另一种方法是示例方法提供的。示例方法的想法类似于人类尝试解释新概念的方式。作为人类，当我们试图向他人解释或介绍新事物时，我们通常会尝试使用听众能够相关联的例子。同样，在
    XAI 的背景下，示例方法尝试选择数据集的某些实例来解释模型的行为。它假设观察当前数据实例与历史观察之间的相似性可以用来解释黑盒模型。
- en: These are mostly model-agnostic approaches that can be applied to both structured
    and unstructured data. If the structured data is high-dimensional, it becomes
    slightly challenging for these approaches, and all the features cannot be included
    to explain the model. So, it works well only if there is an option to summarize
    the data instance or pick up only selected features.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大多是模型无关的方法，可以应用于结构化和非结构化数据。如果结构化数据是高维的，这些方法会变得稍微具有挑战性，并且无法包含所有特征来解释模型。因此，只有在有选项总结数据实例或仅选择特定特征的情况下，这些方法才能很好地工作。
- en: In this chapter, we will mainly discuss **Counterfactual Explanations** (**CFEs**),
    the most popular example-based explainability method that works for both structured
    and unstructured data. CFEs indicate to what extent a particular feature has to
    change to significantly change the predicted outcome. Typically, this is useful
    for classification-based problems.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要讨论**反事实解释**（**CFEs**），这是最流行的基于示例的可解释性方法，适用于结构化和非结构化数据。CFEs 指出特定特征需要改变到何种程度才能显著改变预测结果。通常，这对于基于分类的问题很有用。
- en: For certain predictive models, CFEs can provide prescriptive insights and recommendations
    that can be very crucial for end users and business stakeholders. For example,
    let's suppose there is an ML model used in an automated loan approval system.
    If the black-box model denies the loan request for a particular applicant, the
    loan applicant might reach out to the provider to learn the exact reason why their
    request was not approved. But instead, if the system suggests the applicant increases
    their salary by 5,000 and pay their credit card bills on time for the next 3 months
    in order to approve the loan request, then the applicant will understand and trust
    the decision-making process of the system and can work toward getting their loan
    request approved.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些预测模型，CFEs 可以提供指导性的见解和建议，这对于最终用户和商业利益相关者可能非常关键。例如，假设有一个用于自动贷款审批系统的 ML 模型。如果黑盒模型拒绝某个申请人的贷款申请，贷款申请人可能会联系提供者以了解他们的申请为何未被批准的确切原因。但相反，如果系统建议申请人增加
    5,000 元的薪水，并在接下来的 3 个月内按时支付信用卡账单，以便批准贷款申请，那么申请人将理解并信任系统的决策过程，并努力使他们的贷款申请获得批准。
- en: CFEs in structured data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化数据中的 CFEs
- en: Using the **Diverse Counterfactual Explanation** (**DiCE**) framework in Python
    ([https://interpret.ml/DiCE/](https://interpret.ml/DiCE/)), a CFE can be provided
    for structured data. It can be applied to both classification and regression-based
    problems for model-agnostic local explainability, and it describes how the smallest
    change in the structured data can change the target outcome.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用**Diverse Counterfactual Explanation** (**DiCE**)框架([https://interpret.ml/DiCE/](https://interpret.ml/DiCE/))，可以为结构化数据提供一个CFE。它可以应用于基于分类和回归的问题，以实现模型无关的局部可解释性，并描述了结构化数据中最小的变化如何改变目标结果。
- en: Another important framework used for CFEs in Python is **Alibi** ([https://docs.seldon.io/projects/alibi/en/stable/](https://docs.seldon.io/projects/alibi/en/stable/)),
    which is also pretty good in terms of implementing the concepts of CFE to explain
    ML models. Although we will be discussing these frameworks and experiencing the
    practical aspects of these frameworks in [*Chapter 9*](B18216_09_ePub.xhtml#_idTextAnchor172),
    *Other Popular XAI Frameworks*, for now, I will discuss some intuitive understanding
    of CFE on structured data. Please refer to the notebook ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_structured_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_structured_data.ipynb))
    provided in the code repository to learn how to code these approaches in Python
    for practical problem-solving.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中用于CFE（Counterfactual Explanations）的重要框架之一是**Alibi** ([https://docs.seldon.io/projects/alibi/en/stable/](https://docs.seldon.io/projects/alibi/en/stable/))，它在实现CFE的概念以解释ML模型方面也相当不错。尽管我们将在[*第9章*](B18216_09_ePub.xhtml#_idTextAnchor172)“其他流行的XAI框架”中讨论这些框架并体验这些框架的实际应用，但现在，我将讨论一些关于结构化数据中CFE的直观理解。请参考代码仓库中提供的笔记本([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_structured_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_structured_data.ipynb))，以了解如何在Python中编码这些方法以解决实际问题。
- en: When used with structured data, the CFE method tries to analyze an input query
    data instance and tries to observe the original target outcome considering the
    same query instance from the historical data. Alternatively, it tries to inspect
    the features and maps them to a similar instance present in the historical data
    to get the target output. Then, the algorithm generates multiple counterfactual
    examples to predict the opposite outcome.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当与结构化数据一起使用时，CFE方法试图分析一个输入查询数据实例，并尝试从历史数据中考虑相同查询实例的原始目标结果。或者，它试图检查特征并将它们映射到历史数据中存在的相似实例，以获取目标输出。然后，算法生成多个反事实示例来预测相反的结果。
- en: For a classification-based problem, this method would try to predict the opposite
    class for a binary classification problem or the nearest or most similar class
    for a multiclass classification problem. For a regression problem, if the target
    outcome is present toward the lower end of the spectrum, the algorithm tries to
    provide a counterfactual example with a target outcome closer to the higher end
    of the spectrum, and vice versa. Hence, this method is very effective for understanding
    *what-if* scenarios and can provide actionable insights along with model explainability.
    The explanations are also very clear and very easy to interpret and implement.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于分类的问题，这种方法将尝试预测二分类问题中的相反类别或对于多分类问题中最接近或最相似的类别。对于回归问题，如果目标结果位于谱系的较低端，算法将尝试提供一个目标结果更接近谱系高端的反事实示例，反之亦然。因此，这种方法对于理解“如果...会怎样”的情景非常有效，并且可以提供可操作性的见解以及模型可解释性。解释也非常清晰，易于理解和实现。
- en: But the major drawback of this approach, particularly for structured data, is
    that it suffers from the *Rashomon effect* ([https://www.dictionary.com/e/pop-culture/the-rashomon-effect/](https://www.dictionary.com/e/pop-culture/the-rashomon-effect/)).
    For any real-world problem, it can find multiple CFEs that can contradict each
    other. With structured data, with multiple features, contradictory CFEs can create
    more confusion rather than explaining ML models! Human intervention and the application
    of domain knowledge to pick up the most relevant example can help in mitigating
    the Rashomon effect. Otherwise, my recommendation is to combine this method along
    with the feature importance method for actionable features to select counterfactual
    examples involving significant changes for providing better actionable explainability.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法的重大缺点，尤其是对于结构化数据，是它受到*罗生门效应*([https://www.dictionary.com/e/pop-culture/the-rashomon-effect/](https://www.dictionary.com/e/pop-culture/the-rashomon-effect/))的影响。对于任何现实世界的问题，它可能会找到多个可以相互矛盾的CFEs。在结构化数据中，由于具有多个特征，相互矛盾的CFEs可能会造成更多的混淆，而不是解释机器学习模型！人类干预以及将领域知识应用于选择最相关的示例，可以帮助减轻罗生门效应。否则，我的建议是将这种方法与特征重要性方法结合起来，用于可操作的特征，以选择涉及重大变化的反事实示例，从而提供更好的可操作解释性。
- en: '*Figure 2.15* illustrates how CFEs can be used to get prescriptive insights
    and actionable recommendations to explain the working of models:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.15*说明了如何使用CFEs来获取规范性见解和可操作的建议，以解释模型的工作原理：'
- en: '![Figure 2.15 – Prescriptive insights obtained from CFEs in structured data'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.15 – 从结构化数据中的CFEs获得的规范性见解'
- en: '](img/B18216_02_15.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_15.jpg)'
- en: Figure 2.15 – Prescriptive insights obtained from CFEs in structured data
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – 从结构化数据中的CFEs获得的规范性见解
- en: CFEs in tabular datasets can be very useful as they can provide actionable suggestions
    to the end user. In the next subsection, we will explore CFEs in unstructured
    data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据集中，CFEs非常有用，因为它们可以为最终用户提供可操作的建议。在下一小节中，我们将探讨非结构化数据中的CFEs。
- en: CFEs in unstructured data
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非结构化数据中的CFEs
- en: In unstructured data such as images and text, implementing CFEs can be quite
    challenging. One of the main reasons for this is that the granular features used
    in images or text by deep learning models are not always well-defined or human-friendly.
    But the **Alibi** ([https://docs.seldon.io/projects/alibi/en/stable/](https://docs.seldon.io/projects/alibi/en/stable/))
    framework does pretty well in generating CFEs on image data. Even the improved
    version of the simple CFE method performs even better by generating a CFE guided
    by class prototypes. It uses an auto-encoder or k-d trees to build a prototype
    for each prediction class using a certain input instance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在非结构化数据，如图像和文本中，实现CFEs可能相当具有挑战性。其中一个主要原因是深度学习模型在图像或文本中使用的细粒度特征并不总是定义良好或对人类友好。但**Alibi**([https://docs.seldon.io/projects/alibi/en/stable/](https://docs.seldon.io/projects/alibi/en/stable/))框架在生成图像数据上的CFEs方面做得相当不错。即使是简单的CFE方法的改进版本，通过生成由类别原型引导的CFE，表现也更好。它使用自动编码器或k-d树，使用某个输入实例为每个预测类别构建原型。
- en: For example, in the MNIST dataset, let's suppose that the input query image
    is for digit 7\. Then, the counterfactual prototype method will build prototypes
    of all the digits from 0 to 9\. Following this, it will try to produce the nearest
    digit other than the original digit of 7 as the counterfactual example. Depending
    upon the data, the nearest hand-written digit can be either 9 or 1; even as human
    beings, we might confuse the digits 7 and 9 or 7 and 1 if the handwriting is not
    clear! It takes an optimization approach to minimize the model's counterfactual
    prediction loss.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在MNIST数据集中，假设输入查询图像是数字7。那么，反事实原型方法将构建从0到9的所有数字的原型。随后，它将尝试生成除原始数字7之外最近的数字作为反事实示例。根据数据，最近的手写数字可以是9或1；即使是我们人类，如果书写不清楚，我们可能会混淆数字7和9或7和1！它采用优化方法来最小化模型的反事实预测损失。
- en: I strongly recommend looking at *Arnaud Van Looveren* and *Janis Klaise's* work,
    *Interpretable Counterfactual Explanations Guided by Prototypes* ([https://arxiv.org/abs/1907.02584](https://arxiv.org/abs/1907.02584)),
    to get more details on how this approach works. This CFE method, guided by prototypes,
    also eliminates any computational constraint that can arise due to the numerical
    gradient evaluation process for black-box deep learning models. Please take a
    look at the notebook ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_unstructured_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_unstructured_data.ipynb))
    in the GitHub repository to learn how to implement this method for practical problems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐查看*阿诺德·范·洛弗伦*和*简尼斯·克莱斯*的工作，*基于原型的可解释反事实解释* ([https://arxiv.org/abs/1907.02584](https://arxiv.org/abs/1907.02584))，以获取更多关于此方法如何工作的细节。此CFE方法，由原型引导，还消除了由于黑盒深度学习模型的数值梯度评估过程可能出现的任何计算约束。请查看GitHub仓库中的笔记本
    ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_unstructured_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter02/Counterfactual_unstructured_data.ipynb))，了解如何将此方法应用于实际问题。
- en: 'The following diagram, which has been taken from the paper *Counterfactual
    Visual Explanations, Goyal et al. 2019* ([https://arxiv.org/pdf/1904.07451.pdf](https://arxiv.org/pdf/1904.07451.pdf)),
    shows how visual CFEs can be an effective approach for explaining image classifiers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表，取自论文*Counterfactual Visual Explanations, Goyal et al. 2019* ([https://arxiv.org/pdf/1904.07451.pdf](https://arxiv.org/pdf/1904.07451.pdf))，展示了视觉CFE如何成为解释图像分类器的有效方法：
- en: '![Figure 2.16 – A counterfactual example-based explanation for images'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.16 – 基于反事实示例的图像解释'
- en: '](img/B18216_02_16.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_16.jpg)'
- en: Figure 2.16 – A counterfactual example-based explanation for images
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 基于反事实示例的图像解释
- en: 'In practice, a CFE with unstructured data is difficult to achieve. It is still
    an area of active research, but I think this approach holds great potential to
    provide human-friendly explanations to even complex models. The following diagram
    shows the mapping of the various methods based on their explainability type:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用非结构化数据的CFE难以实现。这仍然是一个活跃的研究领域，但我认为这种方法具有很大的潜力，可以为甚至复杂的模型提供人性化的解释。以下图表显示了基于可解释性类型的各种方法映射：
- en: '![Figure 2.17 – Mapping various methods based on their explainability type'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.17 – 基于可解释性类型的各种方法映射'
- en: '](img/B18216_02_17.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_02_17.jpg)'
- en: Figure 2.17 – Mapping various methods based on their explainability type
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 基于可解释性类型的各种方法映射
- en: LIME and SHAP are important local and model-agnostic algorithms that are not
    covered in this chapter, but they will be covered in more detail later. The model
    explainability methods discussed in this chapter are widely used with a variety
    of datasets to provide different dimensions of explainability.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: LIME和SHAP是本章未涉及的重要的局部和非模型特定算法，但它们将在稍后进行更详细的介绍。本章讨论的模型可解释性方法广泛应用于各种数据集，以提供不同维度的可解释性。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about the various model explainability methods
    used to explain black-box models. Some of these are model-agnostic, while some
    are model specific. Some of these methods provide global interpretability, while
    some of them provide local interpretability. For most of these methods, visualizations
    through plots, graphs, and transformation maps are used to qualitatively inspect
    the data or the model outcomes; while for some of the methods, certain examples
    are used to provide explanations. Statistics and numerical metrics can also play
    an important role in providing quantitative explanations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了用于解释黑盒模型的多种模型可解释性方法。其中一些是非模型特定的，而另一些是模型特定的。其中一些方法提供全局可解释性，而另一些则提供局部可解释性。对于大多数这些方法，通过图表、图形和转换图进行可视化，用于定性检查数据或模型结果；而对于某些方法，使用某些示例来提供解释。统计数据和数值指标在提供定量解释中也起着重要作用。
- en: In the next chapter, we will discuss the very important concept of data-centric
    XAI and gain a conceptual understanding of how data-centric approaches can be
    leveraged in model explainability.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论非常重要的数据中心化XAI概念，并了解数据中心化方法如何在模型可解释性中发挥作用。
- en: References
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To gain additional information about the topics in this chapter, please refer
    to the following resources:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取本章主题的更多信息，请参阅以下资源：
- en: '*Friedman, Jerome H. "Greedy function approximation: A gradient boosting machine."
    Annals of statistics (2001)*: [https://www.researchgate.net/publication/280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine](https://www.researchgate.net/publication/280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*弗里德曼，杰罗姆·H. "贪婪函数逼近：梯度提升机。" 统计学年鉴（2001）*: [https://www.researchgate.net/publication/280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine](https://www.researchgate.net/publication/280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine)'
- en: '*Identifying outliers with the 1.5xIQR rule*: [https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用1.5倍IQR规则识别异常值*: [https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule)'
- en: '*Nelson rules*: [https://www.leansixsigmadefinition.com/glossary/nelson-rules/](https://www.leansixsigmadefinition.com/glossary/nelson-rules/)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Nelson规则*: [https://www.leansixsigmadefinition.com/glossary/nelson-rules/](https://www.leansixsigmadefinition.com/glossary/nelson-rules/)'
- en: '*Accumulated Local Effects (ALE) – Feature Effects Global Interpretability*:[https://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/](https://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*累积局部效应（ALE）-特征效应的全局可解释性*: [https://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/](https://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/)'
- en: '*Model-Agnostic Local Explanations using Individual Conditional Expectation
    (ICE) Plots*: [https://towardsdatascience.com/how-to-explain-and-affect-individual-decisions-with-ice-curves-1-2-f39fd751546f](https://towardsdatascience.com/how-to-explain-and-affect-individual-decisions-with-ice-curves-1-2-f39fd751546f)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用个体条件期望（ICE）图进行模型无关的局部解释*: [https://towardsdatascience.com/how-to-explain-and-affect-individual-decisions-with-ice-curves-1-2-f39fd751546f](https://towardsdatascience.com/how-to-explain-and-affect-individual-decisions-with-ice-curves-1-2-f39fd751546f)'
- en: '*Figure 2.16: Counterfactual Visual Explanations, Goyal et al. 2019*: [https://arxiv.org/pdf/1904.07451.pdf](https://arxiv.org/pdf/1904.07451.pdf)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图2.16：反事实视觉解释，Goyal等人，2019*: [https://arxiv.org/pdf/1904.07451.pdf](https://arxiv.org/pdf/1904.07451.pdf)'
- en: '*Figure 2.13: Interpretability Beyond Feature Attribution: Quantitative Testing
    with Concept Activation Vectors (TCAV), Kim et al. [2018]*: [https://arxiv.org/abs/1711.11279](https://arxiv.org/abs/1711.11279)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图2.13：超越特征归因的可解释性：概念激活向量（TCAV）的定量测试，Kim等人，[2018]*: [https://arxiv.org/abs/1711.11279](https://arxiv.org/abs/1711.11279)'
- en: '*Grad-CAM class activation visualization*: [https://keras.io/examples/vision/grad_cam/](https://keras.io/examples/vision/grad_cam/)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Grad-CAM类激活可视化*: [https://keras.io/examples/vision/grad_cam/](https://keras.io/examples/vision/grad_cam/)'
- en: '*Generalized way of Interpreting CNNs using Guided Gradient Class Activation
    Maps!!*: [https://medium.com/@chinesh4/generalized-way-of-interpreting-cnns-a7d1b0178709](mailto:https://medium.com/@chinesh4/generalized-way-of-interpreting-cnns-a7d1b0178709)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*广义解释CNN的指导梯度类激活图方法!!*: [https://medium.com/@chinesh4/generalized-way-of-interpreting-cnns-a7d1b0178709](mailto:https://medium.com/@chinesh4/generalized-way-of-interpreting-cnns-a7d1b0178709)'
- en: '*Figure 2.12: Grad-CAM: Visual Explanations from Deep Networks via Gradient-based
    Localization, Ramprasaath et. al -* [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图2.12：Grad-CAM：基于梯度的定位的深度网络视觉解释，Ramprasaath等人-* [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)'
