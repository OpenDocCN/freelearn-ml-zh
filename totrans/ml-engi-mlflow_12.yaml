- en: '*Chapter 9*: Deployment and Inference with MLflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：使用MLflow进行部署和推理'
- en: In this chapter, you will learn about an end-to-end deployment infrastructure
    for our **Machine Learning** (**ML**) system including the inference component
    with the use of MLflow. We will then move to deploy our model in a cloud-native
    ML system (AWS SageMaker) and in a hybrid environment with Kubernetes. The main
    goal of the exposure to these different environments is to equip you with the
    skills to deploy an ML model under the varying environmental (cloud-native, and
    on-premises) constraints of different projects.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解我们机器学习（ML）系统的端到端部署基础设施，包括使用MLflow的推理组件。然后我们将转向在云原生ML系统（AWS SageMaker）和混合环境中部署我们的模型。接触这些不同环境的主要目标是让您具备在不同项目环境（云原生和本地）约束下部署ML模型的能力。
- en: The core of this chapter is to deploy the PsyStock model to predict the price
    of Bitcoin (BTC/USD) based on the previous 14 days of market behavior that you
    have been working on so far throughout the book. We will deploy this in multiple
    environments with the aid of a workflow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的核心是将PsyStock模型部署以预测基于您在本书中迄今为止所做的前14天市场行为的比特币（BTC/USD）价格。我们将借助工作流在多个环境中部署此模型。
- en: 'Specifically, we will look at the following sections in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将在本章中查看以下部分：
- en: Starting up a local model registry
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动本地模型注册库
- en: Setting up a batch inference job
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置批量推理作业
- en: Creating an API process for inference
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建推理的API流程
- en: Deploying your models for batch scoring in Kubernetes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署模型进行批量评分
- en: Making a cloud deployment with AWS SageMaker
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS SageMaker进行云部署
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following prerequisites:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要以下先决条件：
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的机器上安装了最新版本的Docker。如果您尚未安装，请按照[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)上的说明进行操作。
- en: The latest version of `docker-compose` installed. Please follow the instructions
    at https://docs.docker.com/compose/install/.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了最新版本的`docker-compose`。请按照[https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)上的说明进行操作。
- en: Access to Git in the command line, which can be installed as described at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在命令行中访问Git，其安装方法可参考[https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)。
- en: Access to a Bash terminal (Linux or Windows).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问Bash终端（Linux或Windows）。
- en: Access to a browser.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问浏览器。
- en: Python 3.5+ installed.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了Python 3.5+。
- en: The latest version of your ML platform installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066)*, Your Data Science Workbench*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照[*第3章*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066)*，您的数据科学工作台*中所述，在本地安装了最新版本的ML平台。
- en: An AWS account configured to run the MLflow model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个配置好的AWS账户，用于运行MLflow模型。
- en: Starting up a local model registry
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动本地模型注册库
- en: Before executing the following sections in this chapter, you will need to set
    up a centralized model registry and tracking server. We don't need the whole of
    the Data Science Workbench, so we can go directly to a lighter variant of the
    workbench built into the model that we will deploy in the following sections.
    You should be in the root folder of the code for this chapter, available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter09
    .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行本章的以下部分之前，您需要设置一个集中的模型注册库和跟踪服务器。我们不需要整个数据科学工作台，因此可以直接使用内置在以下章节中将要部署的模型中的较轻量级的工作台变体。您应该在代码的根目录中，该目录可在[https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter09](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter09)找到。
- en: 'Next, move to the `gradflow` directory and start a light version of the environment
    to serve your model, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，切换到`gradflow`目录，并启动环境的一个轻量级版本以服务于您的模型，操作如下：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After having set up our infrastructure for API deployment with MLflow with the
    model retrieved from the ML registry, we will next move on to the cases where
    we need to score some batch input data. We will prepare a batch inference job
    with MLflow for the prediction problem at hand.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用MLflow从ML注册表中检索模型并设置我们的API部署基础设施之后，我们将继续处理需要评分的批量输入数据的情况。我们将使用MLflow为当前的预测问题准备一个批量推理作业。
- en: Setting up a batch inference job
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置批量推理作业
- en: 'The code required for this section is in the `pystock-inference-api folder`.
    The MLflow infrastructure is provided in the Docker image accompanying the code
    as shown in the following figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节所需的代码位于 `pystock-inference-api` 文件夹中。MLflow 基础设施由代码附带的 Docker 镜像提供，如图下所示：
- en: '![Figure 9.1 – Layout of a batch scoring deployment'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.1 – 批量评分部署布局'
- en: '](img/image0015.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image0015.jpg]'
- en: Figure 9.1 – Layout of a batch scoring deployment
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 批量评分部署布局
- en: 'If you have direct access to the artifacts, you can do the following. The code
    is available under the `pystock-inference-batch` directory. In order to set up
    a batch inference job, we will follow these steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接访问到这些工件，你可以执行以下操作。代码位于 `pystock-inference-batch` 目录下。为了设置一个批量推理作业，我们将遵循以下步骤：
- en: 'Import the dependencies of your batch job; among the relevant dependencies
    we include `pandas`, `mlflow,` and `xgboost`:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入你的批量作业的依赖项；在相关的依赖项中，我们包括 `pandas`、`mlflow` 和 `xgboost`：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will next load `start_run` by calling `mlflow.start_run` and load the data
    from the `input.csv` scoring input file:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来将通过调用 `mlflow.start_run` 来加载 `start_run` 并从 `input.csv` 评分输入文件加载数据：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we load the model from the registry by specifying the `model_uri` value,
    based on the details of the model:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过指定 `model_uri` 值从注册表中加载模型，基于模型的详细信息：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to predict over the dataset that we just read by running `model.predict`:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以通过运行 `model.predict` 来预测我们刚刚读取的数据集：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Save the batch predictions. This basically involves mapping the probability
    target (of the market going up) in the `y_preds` variable to a value ranging from
    0 to 1:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存批量预测。这基本上涉及到将 `y_preds` 变量中的市场上涨的概率目标映射到 0 到 1 的值：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now need to package the job as a Docker image so we can run it in production
    easily:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要将作业打包成一个 Docker 镜像，这样我们就可以轻松地在生产环境中运行它：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Build your Docker image and tag it so you can reference it:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建你的 Docker 镜像并对其进行标记，以便你可以引用它：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run your Docker image by executing the following command:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令来运行你的 Docker 镜像：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A Docker image in this case provides you with a mechanism to run your batch
    scoring job in any computing environment that supports Docker images in the cloud
    or on-premises.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个 Docker 镜像为你提供了一个机制，可以在支持 Docker 镜像的任何云或本地计算环境中运行你的批量评分作业。
- en: We will move now to illustrate the generation of a dockerized API inference
    environment for MLflow.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将展示如何为 MLflow 生成一个 Docker 化的 API 推理环境。
- en: Creating an API process for inference
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于推理的 API 进程
- en: 'The code required for this section is in the `pystock-inference-api folder`.
    The MLflow infrastructure is provided in the Docker image accompanying the code
    as shown in the following figure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节所需的代码位于 `pystock-inference-api` 文件夹中。MLflow 基础设施由代码附带的 Docker 镜像提供，如图下所示：
- en: '![Figure 9.2 – The structure of the API job'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.2 – API 作业的结构'
- en: '](img/image0026.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image0026.jpg]'
- en: Figure 9.2 – The structure of the API job
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – API 作业的结构
- en: Setting up an API system is quite easy by relying on the MLflow built-in REST
    API environment. We will rely on the artifact store on the local filesystem to
    test the APIs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依赖 MLflow 内置的 REST API 环境，设置一个 API 系统相当简单。我们将依赖本地文件系统上的工件存储来测试 API。
- en: 'With the following set of commands, which at its core consists of using the
    `models serve` command in the CLI, we can serve our models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下命令集，其核心是使用 CLI 中的 `models serve` 命令，我们可以提供我们的模型服务：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We next will package the preceding commands in a Docker image so it can be
    used on any environment for deployment. The steps to achieve this are the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将前面的命令打包到一个 Docker 镜像中，以便在任何环境中进行部署。实现这一目标的步骤如下：
- en: 'Generate a Docker image specifying the work directory and the commands that
    need to be started as an `entry point`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个 Docker 镜像，指定工作目录和需要启动的命令作为 `entry point`：
- en: '[PRE10]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Build your Docker image:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建你的 Docker 镜像：
- en: '[PRE11]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run your Docker image:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行你的 Docker 镜像：
- en: '[PRE12]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this stage, you have dockerized the API infrastructure and can deploy it
    on a compute environment convenient to you.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你已经将 API 基础设施 Docker 化，并且可以在你方便的计算环境中部署它。
- en: After having delved into the interaction of MLflow and a cloud-native deployment
    on the AWS platform, we will now look at a deployment that is independent of any
    provider.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究了 MLflow 与 AWS 平台上的云原生部署的交互之后，我们现在将关注一个不依赖于任何提供商的部署。
- en: Deploying your models for batch scoring in Kubernetes
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署你的模型进行批量评分
- en: We will use Kubernetes to deploy our batch scoring job. We will need to do some
    modifications to make it conform to the Docker format acceptable to the MLflow
    deployment in production through Kubernetes. The prerequisite of this section
    is that you have access to a Kubernetes cluster or can set up a local one. Guides
    for this can be found at [https://kind.sigs.k8s.io/docs/user/quick-start/](https://kind.sigs.k8s.io/docs/user/quick-start/)
    or [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kubernetes 来部署我们的批量评分作业。我们需要进行一些修改，使其符合 Docker 格式，以便通过 Kubernetes 进行生产中的
    MLflow 部署。本节的前提条件是你有权访问 Kubernetes 集群或可以设置一个本地集群。有关指南，请参阅 [https://kind.sigs.k8s.io/docs/user/quick-start/](https://kind.sigs.k8s.io/docs/user/quick-start/)
    或 [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)。
- en: 'You will now execute the following steps to deploy your model from the registry
    in Kubernetes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在需要执行以下步骤，从 Kubernetes 中的注册表中部署你的模型：
- en: 'Prerequisite: Deploy and configure `kubectl` ([https://kubernetes.io/docs/reference/kubectl/overview/](https://kubernetes.io/docs/reference/kubectl/overview/))
    and link it to your Kubernetes cluster.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前提条件：部署和配置 `kubectl` ([https://kubernetes.io/docs/reference/kubectl/overview/](https://kubernetes.io/docs/reference/kubectl/overview/))
    并将其链接到你的 Kubernetes 集群。
- en: 'Create a Kubernetes backend configuration file:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Kubernetes 后端配置文件：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Load the input files and run the model:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载输入文件并运行模型：
- en: '[PRE14]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Having looked at deploying models in Kubernetes, we will now focus on deploying
    our model in a cloud-native ML platform.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看在 Kubernetes 中部署模型之后，我们现在将专注于在云原生 ML 平台上部署我们的模型。
- en: Making a cloud deployment with AWS SageMaker
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS SageMaker 进行云部署
- en: In the last few years, services such as AWS SageMaker have been gaining ground
    as an engine to run ML workloads. MLflow provides integrations and easy-to-use
    commands to deploy your model into the SageMaker infrastructure. The execution
    of this section will take several minutes (5 to 10 minutes depending on your connection)
    due to the need to build large Docker images and push the images to the Docker
    Registry.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，像 AWS SageMaker 这样的服务作为运行 ML 工作负载的引擎已经逐渐占据了一席之地。MLflow 提供了集成和易于使用的命令，可以将你的模型部署到
    SageMaker 基础设施中。由于需要构建大型 Docker 镜像并将其推送到 Docker 仓库，本节的执行将需要几分钟（5 到 10 分钟，具体取决于你的连接速度）。
- en: 'The following is a list of some critical prerequisites for you to follow along:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你需要遵循的一些关键前提条件列表：
- en: The AWS CLI configured locally with a default profile (for more details, you
    can look at [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地配置的 AWS CLI 使用默认配置文件（更多详情，请参阅 [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html))。
- en: AWS access in the account to SageMaker and its dependencies.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 账户中用于 SageMaker 及其依赖项的访问权限。
- en: AWS access in the account to push to Amazon **Elastic Container Registry** (**ECR**)
    service.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 账户中用于推送至 Amazon **弹性容器注册**（**ECR**）服务的访问权限。
- en: Your MLflow server needs to be running as mentioned in the first *Starting up
    a local model registry* section.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的 MLflow 服务器需要按照第一部分“启动本地模型注册”中所述运行。
- en: 'To deploy the model from your local registry into AWS SageMaker, execute the
    following steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要将你的本地注册表中的模型部署到 AWS SageMaker，请执行以下步骤：
- en: Build your `mlflow-pyfunc` image. This is the basic image that will be compatible
    with SageMaker.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 `mlflow-pyfunc` 镜像。这是与 SageMaker 兼容的基本镜像。
- en: 'Build and push a container with an `mlflow pyfunc` message:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并推送一个带有 `mlflow pyfunc` 消息的容器：
- en: '[PRE15]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This command will build the MLflow default image and deploy it to the Amazon
    ECR container.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将构建 MLflow 默认镜像并将其部署到 Amazon ECR 容器。
- en: 'In order to confirm that this command was successful, you can check your ECR
    instance on the console:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了确认此命令成功执行，你可以在控制台上检查你的 ECR 实例：
- en: '![Figure 9.3 – SageMaker deployed image'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.3 – SageMaker 部署的镜像'
- en: '](img/image0036.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/image0036.jpg)'
- en: Figure 9.3 – SageMaker deployed image
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.3 – SageMaker 部署的镜像
- en: 'Run your model locally to test the SageMaker Docker image and export the tracking
    URI:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地运行你的模型以测试 SageMaker Docker 镜像并导出跟踪 URI：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Installing collected packages: mlflow'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装收集的包：mlflow
- en: 'Attempting uninstall: mlflow'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尝试卸载：mlflow
- en: 'Found existing installation: mlflow 1.16.0'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已找到现有安装：mlflow 1.16.0
- en: 'Uninstalling mlflow-1.16.0:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卸载 mlflow-1.16.0：
- en: Successfully uninstalled mlflow-1.16.0
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功卸载 mlflow-1.16.0
- en: Successfully installed mlflow-1.15.0
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功安装 mlflow-1.15.0
- en: pip 20.2.4 from /miniconda/lib/python3.8/site-packages/pip (python 3.8)
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: pip 20.2.4 来自 /miniconda/lib/python3.8/site-packages/pip（python 3.8）
- en: Python 3.8.5
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 3.8.5
- en: 1.15.0
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1.15.0
- en: '[2021-05-08 14:01:43 +0000] [354] [INFO] Starting gunicorn 20.1.0'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[2021-05-08 14:01:43 +0000] [354] [INFO] Starting gunicorn 20.1.0'
- en: '[2021-05-08 14:01:43 +0000] [354] [INFO] Listening at: http://127.0.0.1:8000
    (354)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[2021-05-08 14:01:43 +0000] [354] [INFO] Listening at: http://127.0.0.1:8000
    (354)'
- en: '[PRE17]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Double-check your image through the AWS `cli`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过AWS `cli`双重检查你的镜像：
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You should see your deployed image in the list of images and definitely ready
    to run.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该在图像列表中看到你的已部署镜像，并且肯定可以运行。
- en: You need to configure a role in AWS as specified that allows SageMaker to create
    resources on your behalf (you can find more details at https://docs.databricks.com/administration-guide/cloud-configurations/aws/sagemaker.html#step-1-create-an-aws-iam-role-and-attach-sagemaker-permission-policy).
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要在AWS中配置一个角色，该角色允许SageMaker代表你创建资源（你可以在https://docs.databricks.com/administration-guide/cloud-configurations/aws/sagemaker.html#step-1-create-an-aws-iam-role-and-attach-sagemaker-permission-policy找到更多详细信息）。
- en: 'Next, you need to export your region and roles to the `$REGION` and `$ROLE`
    environment variables with the following commands, specifying the actual values
    of your environment:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要使用以下命令将你的区域和角色导出到`$REGION`和`$ROLE`环境变量中，指定你环境的实际值：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To deploy your model to SageMaker, run the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将你的模型部署到SageMaker，请运行以下命令：
- en: '[PRE20]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This command will load your model from your local registry into SageMaker as
    an internal representation and use the generated Docker image to serve the model
    in the AWS SageMaker infrastructure engine. It will take a few minutes to set
    up all the infrastructure. Upon success, you should see the following message:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将从你的本地注册表中加载你的模型到SageMaker作为内部表示，并使用生成的Docker镜像在AWS SageMaker基础设施引擎中提供模型。设置所有基础设施将需要几分钟。成功后，你应该会看到以下消息：
- en: '[PRE21]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Verify your SageMaker endpoint:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证你的SageMaker端点：
- en: '[PRE22]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can look at the following for an illustrative example of the type of output
    message:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以查看以下内容，以了解输出消息类型的示例：
- en: '[PRE23]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next we need to consume our API with a simple script that basically the features,
    invokes the SageMaker endpoint using the Amazon Boto3 client, and prints the probablity
    of the market pricesgiven the feature vector:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用一个简单的脚本来消费我们的API，该脚本基本上列出功能，使用Amazon Boto3客户端调用SageMaker端点，并打印出基于特征向量的市场价格的概率：
- en: '[PRE24]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After running this previous script you should see the following output:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此之前的脚本后，你应该会看到以下输出：
- en: '[PRE25]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Explore the SageMaker endpoint interface. In its monitoring component, you can
    look at different metrics related to your deployment environment and model as
    shown in *Figure 9.4*:![Figure 9.4 – SageMaker inference instance metrics
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索SageMaker端点接口。在其监控组件中，你可以查看与你的部署环境和模型相关的不同指标，如图*图9.4*所示：![图9.4 – SageMaker推理实例指标
- en: '](img/image0046.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image0046.jpg](img/image0046.jpg)'
- en: Figure 9.4 – SageMaker inference instance metrics
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.4 – SageMaker推理实例指标
- en: 'You can now easily tear down your deployed model, when in need to deploy the
    model or phase it out. All associated resources will be torn down:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以轻松地拆除已部署的模型，当需要部署模型或逐步淘汰时。所有相关资源都将被拆除：
- en: '[PRE26]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Upon deletion, you should see a message similar to the one in the following
    excerpt:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 删除后，你应该会看到类似于以下摘录中的消息：
- en: '[PRE27]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With this section, we concluded the description of the features related to deploying
    an ML model with MLflow in production in different environments from your local
    machine, including Docker and `docker-compose`, public clouds, and the very flexible
    approach of using AWS SageMaker.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本节，我们完成了在不同环境中使用MLflow在生产环境中部署ML模型的相关功能描述，包括从本地机器到Docker和`docker-compose`、公共云，以及使用AWS
    SageMaker的非常灵活的方法。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on production deployments of ML models, the concepts
    behind this, and the different features available for deploying in multiple environments
    with MLflow.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于ML模型的部署，其背后的概念，以及MLflow在多个环境中部署时提供的不同功能。
- en: We explained how to prepare Docker images ready for deployment. We also clarified
    how to interact with Kubernetes and AWS SageMaker to deploy models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了如何准备Docker镜像以便部署。我们还阐明了如何与Kubernetes和AWS SageMaker交互以部署模型。
- en: In the next chapter and the following sections of the book, we will focus on
    using tools to help scale out our MLflow workloads to improve the performance
    of our models' infrastructure.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章和接下来的几节中，我们将专注于使用工具来帮助我们扩展MLflow工作负载，以提高我们模型基础设施的性能。
- en: Further reading
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步扩展您的知识，您可以查阅以下链接中的文档：
- en: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
- en: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
