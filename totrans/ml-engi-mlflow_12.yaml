- en: '*Chapter 9*: Deployment and Inference with MLflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about an end-to-end deployment infrastructure
    for our **Machine Learning** (**ML**) system including the inference component
    with the use of MLflow. We will then move to deploy our model in a cloud-native
    ML system (AWS SageMaker) and in a hybrid environment with Kubernetes. The main
    goal of the exposure to these different environments is to equip you with the
    skills to deploy an ML model under the varying environmental (cloud-native, and
    on-premises) constraints of different projects.
  prefs: []
  type: TYPE_NORMAL
- en: The core of this chapter is to deploy the PsyStock model to predict the price
    of Bitcoin (BTC/USD) based on the previous 14 days of market behavior that you
    have been working on so far throughout the book. We will deploy this in multiple
    environments with the aid of a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting up a local model registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a batch inference job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an API process for inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying your models for batch scoring in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a cloud deployment with AWS SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of `docker-compose` installed. Please follow the instructions
    at https://docs.docker.com/compose/install/.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line, which can be installed as described at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your ML platform installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066)*, Your Data Science Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AWS account configured to run the MLflow model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting up a local model registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before executing the following sections in this chapter, you will need to set
    up a centralized model registry and tracking server. We don't need the whole of
    the Data Science Workbench, so we can go directly to a lighter variant of the
    workbench built into the model that we will deploy in the following sections.
    You should be in the root folder of the code for this chapter, available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter09
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, move to the `gradflow` directory and start a light version of the environment
    to serve your model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After having set up our infrastructure for API deployment with MLflow with the
    model retrieved from the ML registry, we will next move on to the cases where
    we need to score some batch input data. We will prepare a batch inference job
    with MLflow for the prediction problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a batch inference job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code required for this section is in the `pystock-inference-api folder`.
    The MLflow infrastructure is provided in the Docker image accompanying the code
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Layout of a batch scoring deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Layout of a batch scoring deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have direct access to the artifacts, you can do the following. The code
    is available under the `pystock-inference-batch` directory. In order to set up
    a batch inference job, we will follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dependencies of your batch job; among the relevant dependencies
    we include `pandas`, `mlflow,` and `xgboost`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will next load `start_run` by calling `mlflow.start_run` and load the data
    from the `input.csv` scoring input file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the model from the registry by specifying the `model_uri` value,
    based on the details of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to predict over the dataset that we just read by running `model.predict`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the batch predictions. This basically involves mapping the probability
    target (of the market going up) in the `y_preds` variable to a value ranging from
    0 to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to package the job as a Docker image so we can run it in production
    easily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build your Docker image and tag it so you can reference it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run your Docker image by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A Docker image in this case provides you with a mechanism to run your batch
    scoring job in any computing environment that supports Docker images in the cloud
    or on-premises.
  prefs: []
  type: TYPE_NORMAL
- en: We will move now to illustrate the generation of a dockerized API inference
    environment for MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API process for inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code required for this section is in the `pystock-inference-api folder`.
    The MLflow infrastructure is provided in the Docker image accompanying the code
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The structure of the API job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – The structure of the API job
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an API system is quite easy by relying on the MLflow built-in REST
    API environment. We will rely on the artifact store on the local filesystem to
    test the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following set of commands, which at its core consists of using the
    `models serve` command in the CLI, we can serve our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We next will package the preceding commands in a Docker image so it can be
    used on any environment for deployment. The steps to achieve this are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a Docker image specifying the work directory and the commands that
    need to be started as an `entry point`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build your Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run your Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this stage, you have dockerized the API infrastructure and can deploy it
    on a compute environment convenient to you.
  prefs: []
  type: TYPE_NORMAL
- en: After having delved into the interaction of MLflow and a cloud-native deployment
    on the AWS platform, we will now look at a deployment that is independent of any
    provider.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your models for batch scoring in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use Kubernetes to deploy our batch scoring job. We will need to do some
    modifications to make it conform to the Docker format acceptable to the MLflow
    deployment in production through Kubernetes. The prerequisite of this section
    is that you have access to a Kubernetes cluster or can set up a local one. Guides
    for this can be found at [https://kind.sigs.k8s.io/docs/user/quick-start/](https://kind.sigs.k8s.io/docs/user/quick-start/)
    or [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will now execute the following steps to deploy your model from the registry
    in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prerequisite: Deploy and configure `kubectl` ([https://kubernetes.io/docs/reference/kubectl/overview/](https://kubernetes.io/docs/reference/kubectl/overview/))
    and link it to your Kubernetes cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Kubernetes backend configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the input files and run the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having looked at deploying models in Kubernetes, we will now focus on deploying
    our model in a cloud-native ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Making a cloud deployment with AWS SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few years, services such as AWS SageMaker have been gaining ground
    as an engine to run ML workloads. MLflow provides integrations and easy-to-use
    commands to deploy your model into the SageMaker infrastructure. The execution
    of this section will take several minutes (5 to 10 minutes depending on your connection)
    due to the need to build large Docker images and push the images to the Docker
    Registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of some critical prerequisites for you to follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: The AWS CLI configured locally with a default profile (for more details, you
    can look at [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS access in the account to SageMaker and its dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS access in the account to push to Amazon **Elastic Container Registry** (**ECR**)
    service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your MLflow server needs to be running as mentioned in the first *Starting up
    a local model registry* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To deploy the model from your local registry into AWS SageMaker, execute the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Build your `mlflow-pyfunc` image. This is the basic image that will be compatible
    with SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Build and push a container with an `mlflow pyfunc` message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will build the MLflow default image and deploy it to the Amazon
    ECR container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to confirm that this command was successful, you can check your ECR
    instance on the console:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – SageMaker deployed image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image0036.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.3 – SageMaker deployed image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run your model locally to test the SageMaker Docker image and export the tracking
    URI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Installing collected packages: mlflow'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Attempting uninstall: mlflow'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Found existing installation: mlflow 1.16.0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Uninstalling mlflow-1.16.0:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Successfully uninstalled mlflow-1.16.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Successfully installed mlflow-1.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pip 20.2.4 from /miniconda/lib/python3.8/site-packages/pip (python 3.8)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Python 3.8.5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.15.0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[2021-05-08 14:01:43 +0000] [354] [INFO] Starting gunicorn 20.1.0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[2021-05-08 14:01:43 +0000] [354] [INFO] Listening at: http://127.0.0.1:8000
    (354)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Double-check your image through the AWS `cli`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see your deployed image in the list of images and definitely ready
    to run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You need to configure a role in AWS as specified that allows SageMaker to create
    resources on your behalf (you can find more details at https://docs.databricks.com/administration-guide/cloud-configurations/aws/sagemaker.html#step-1-create-an-aws-iam-role-and-attach-sagemaker-permission-policy).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you need to export your region and roles to the `$REGION` and `$ROLE`
    environment variables with the following commands, specifying the actual values
    of your environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To deploy your model to SageMaker, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command will load your model from your local registry into SageMaker as
    an internal representation and use the generated Docker image to serve the model
    in the AWS SageMaker infrastructure engine. It will take a few minutes to set
    up all the infrastructure. Upon success, you should see the following message:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify your SageMaker endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can look at the following for an illustrative example of the type of output
    message:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next we need to consume our API with a simple script that basically the features,
    invokes the SageMaker endpoint using the Amazon Boto3 client, and prints the probablity
    of the market pricesgiven the feature vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this previous script you should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Explore the SageMaker endpoint interface. In its monitoring component, you can
    look at different metrics related to your deployment environment and model as
    shown in *Figure 9.4*:![Figure 9.4 – SageMaker inference instance metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0046.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.4 – SageMaker inference instance metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can now easily tear down your deployed model, when in need to deploy the
    model or phase it out. All associated resources will be torn down:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon deletion, you should see a message similar to the one in the following
    excerpt:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this section, we concluded the description of the features related to deploying
    an ML model with MLflow in production in different environments from your local
    machine, including Docker and `docker-compose`, public clouds, and the very flexible
    approach of using AWS SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on production deployments of ML models, the concepts
    behind this, and the different features available for deploying in multiple environments
    with MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: We explained how to prepare Docker images ready for deployment. We also clarified
    how to interact with Kubernetes and AWS SageMaker to deploy models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter and the following sections of the book, we will focus on
    using tools to help scale out our MLflow workloads to improve the performance
    of our models' infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
