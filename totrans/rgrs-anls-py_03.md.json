["```py\nIn: import numpy as np\n import pandas as pd\n import matplotlib.pyplot as plt\n import matplotlib as mpl\n from sklearn.datasets import load_boston\n from sklearn import linear_model\n\n```", "```py\nIn: %matplotlib inline\n\n```", "```py\nIn: boston = load_boston()\n dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n dataset['target'] = boston.target\n\n```", "```py\nIn: observations = len(dataset)\n variables = dataset.columns[:-1]\n X = dataset.ix[:,:-1]\n y = dataset['target'].values\n\n```", "```py\nIn: import statsmodels.api as sm\n import statsmodels.formula.api as smf\n\n```", "```py\nIn: Xc = sm.add_constant(X)\n linear_regression = sm.OLS(y,Xc)\n fitted_model = linear_regression.fit()\n\n```", "```py\nIn: fitted_model.summary()\nOut:\n\n```", "```py\nlinear_regression = smf.ols(formula = 'target ~ CRIM + ZN +INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data=dataset)\nfitted_model = linear_regression.fit()\n\n```", "```py\nX = dataset.ix[:,:-1]\ncorrelation_matrix = X.corr()\nprint (correlation_matrix)\n\n```", "```py\nIn: \ndef visualize_correlation_matrix(data, hurdle = 0.0):\n R = np.corrcoef(data, rowvar=0)\n R[np.where(np.abs(R)<hurdle)] = 0.0\n heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)\n heatmap.axes.set_frame_on(False)\n heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor=False)\n heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor=False)\n heatmap.axes.set_xticklabels(variables, minor=False)\n plt.xticks(rotation=90)\n heatmap.axes.set_yticklabels(variables, minor=False)\n plt.tick_params(axis='both', which='both', bottom='off', \\top='off', left = 'off', right = 'off')\n plt.colorbar()\n plt.show()\n\nvisualize_correlation_matrix(X, hurdle=0.5)\n\n```", "```py\nIn: corr = np.corrcoef(X, rowvar=0)\n eigenvalues, eigenvectors = np.linalg.eig(corr)\n\n```", "```py\nIn: print (eigenvalues)\nOut: [ 6.12265476  1.43206335  1.24116299  0.85779892  0.83456618  0.65965056  0.53901749  0.39654415  0.06351553  0.27743495  0.16916744  0.18616388  0.22025981]\n\n```", "```py\nIn: print (eigenvectors[:,8])\nOut: [-0.04552843  0.08089873  0.25126664 -0.03590431 -0.04389033 -0.04580522  0.03870705  0.01828389  0.63337285 -0.72024335 -0.02350903  0.00485021 -0.02477196]\n\n```", "```py\nIn: print (variables[2], variables[8], variables[9])\nOut: INDUS RAD TAX\n\n```", "```py\nIn: from sklearn.preprocessing import StandardScaler\n observations = len(dataset)\n variables = dataset.columns\n standardization = StandardScaler()\n Xst = standardization.fit_transform(X)\n original_means = standardization.mean_\n originanal_stds = standardization.std_\n Xst = np.column_stack((Xst,np.ones(observations)))\n y  = dataset['target'].values\n\n```", "```py\nIn: import random\n\n def random_w( p ):\n return np.array([np.random.normal() for j in range(p)])\n def hypothesis(X,w):\n return np.dot(X,w)\n\n def loss(X,w,y):\n return hypothesis(X,w) - y\n\n def squared_loss(X,w,y):\n return loss(X,w,y)**2\n\n def gradient(X,w,y):\n gradients = list()\n n = float(len( y ))\n for j in range(len(w)):\n gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n return gradients\n\n def update(X,w,y, alpha=0.01):\n return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n\n def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):\n w = random_w(X.shape[1])\n path = list()\n for k in range(iterations):\n SSL = np.sum(squared_loss(X,w,y))\n new_w = update(X,w,y, alpha=alpha)\n new_SSL = np.sum(squared_loss(X,new_w,y))\n w = new_w\n if k>=5 and (new_SSL - SSL <= eta and \\new_SSL - SSL >= -eta):\n path.append(new_SSL)\n return w, path\n if k % (iterations / 20) == 0:\n path.append(new_SSL)\n return w, path\n\n alpha = 0.02\n w, path = optimize(Xst, y, alpha, eta = 10**-12, \\iterations = 20000)\n print (\"These are our final standardized coefficients: \" + ', \\'.join(map(lambda x: \"%0.4f\" % x, w))) \n\nOut: These are our final standardized coefficients: -0.9204, 1.0810, 0.1430, 0.6822, -2.0601, 2.6706, 0.0211, -3.1044, 2.6588, -2.0759, -2.0622, 0.8566, -3.7487, 22.5328\n\n```", "```py\nIn: unstandardized_betas = w[:-1] / originanal_stds\n unstandardized_bias  = w[-1]-np.sum((original_means /originanal_stds) * w[:-1])\n print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n for beta,varname in zip(unstandardized_betas, variables):\n print ('%8s: %8.4f' % (varname, beta))\n\nOut:\n\n```", "```py\nIn: linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n\n```", "```py\nIn: from sklearn.preprocessing import StandardScaler\n from sklearn.pipeline import make_pipeline\n standardization = StandardScaler()\n Stand_coef_linear_reg = make_pipeline(standardization,\\linear_regression)\n\n```", "```py\nIn: linear_regression.fit(X,y)\nfor coef, var in sorted(zip(map(abs,linear_regression.coef_), \\\n dataset.columns[:-1]), reverse=True):\n print (\"%6.3f %s\" % (coef,var))\n\nOut: \n\n```", "```py\nIn: Stand_coef_linear_reg.fit(X,y)\nfor coef, var in \\\nsorted(zip(map(abs,Stand_coef_linear_reg.steps[1][1].coef_), \\\ndataset.columns[:-1]), reverse=True):\n print (\"%6.3f %s\" % (coef,var))\n\nOut: \n\n```", "```py\nIn: from sklearn.metrics import r2_score\n linear_regression = linear_model.LinearRegression(normalize=False,\\fit_intercept=True)\ndef r2_est(X,y):\n return r2_score(y,linear_regression.fit(X,y).predict(X))\n\nprint ('Baseline R2: %0.3f' %  r2_est(X,y))\n\nOut:Baseline R2: 0.741\n\n```", "```py\nIn: r2_impact = list()\n for j in range(X.shape[1]):\n selection = [i for i in range(X.shape[1]) if i!=j]\n r2_impact.append(((r2_est(X,y) - \\r2_est(X.values [:,selection],y)) ,dataset.columns[j]))\n for imp, varname in sorted(r2_impact, reverse=True):\n print ('%6.3f %s' %  (imp, varname))\n\nOut: \n\n```", "```py\nIn: from sklearn.preprocessing import PolynomialFeatures\n from sklearn.metrics import r2_score\n linear_regression = linear_model.LinearRegression(normalize=False,\\fit_intercept=True)\n create_interactions = PolynomialFeatures(degree=2, \\interaction_only=True, include_bias=False)\n\n```", "```py\nIn: def r2_est(X,y):\n return r2_score(y,linear_regression.fit(X,y).predict(X))\nbaseline = r2_est(X,y)\nprint ('Baseline R2: %0.3f' % baseline)\n\nOut: Baseline R2: 0.741\n\nin: Xi = create_interactions.fit_transform(X)\n main_effects = create_interactions.n_input_features_\n\n```", "```py\nIn: for k,effect in \\ enumerate(create_interactions.powers_[(main_effects):]):\n termA, termB = variables[effect==1]\n increment = r2_est(Xi[:,list(range(0,main_effects)) \\+[main_effects+k]],y) - baseline\n if increment > 0.01:\n print ('Adding interaction %8s *%8s R2: %5.3f' %  \\\n (termA, termB, increment))\nOut: \n\n```", "```py\nIn: Xi = X\n Xi['interaction'] = X['RM']*X['LSTAT']\n print ('R2 of a model with RM*LSTAT interaction: %0.3f' % \\r2_est(Xi,y))\n\nOut: R2 of a model with RM*LSTAT interaction: 0.805\n\n```", "```py\nIn: \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlinear_regression = linear_model.LinearRegression(normalize=False, \\fit_intercept=True)\ncreate_cubic = PolynomialFeatures(degree=3, interaction_only=False, \\include_bias=False)\ncreate_quadratic = PolynomialFeatures(degree=2, interaction_only=False, \\\ninclude_bias=False)\n\nlinear_predictor = make_pipeline(linear_regression)\nquadratic_predictor = make_pipeline(create_quadratic, \\\nlinear_regression)\ncubic_predictor = make_pipeline(create_cubic, linear_regression)\n\n```", "```py\npredictor = 'LSTAT'\nx = dataset['LSTAT'].values.reshape((observations,1))\nxt = np.arange(0,50,0.1).reshape((50/0.1,1))\nx_range = [dataset[predictor].min(),dataset[predictor].max()]\ny_range = [dataset['target'].min(),dataset['target'].max()]\n\nscatter = dataset.plot(kind='scatter', x=predictor, y='target', \\xlim=x_range, ylim=y_range)\nregr_line = scatter.plot(xt, linear_predictor.fit(x,y).predict(xt), \\'-', color='red', linewidth=2)\n\n```", "```py\nscatter = dataset.plot(kind='scatter', x=predictor, y='target', \\xlim=x_range, ylim=y_range)\nregr_line = scatter.plot(xt, cubic_predictor.fit(x,y).predict(xt), \\'-', color='red', linewidth=2)\n\n```", "```py\nIn: for d in [1,2,3,5,15]:\n create_poly = PolynomialFeatures(degree=d,\\interaction_only=False, include_bias=False)\n poly = make_pipeline(create_poly, StandardScaler(),\\linear_regression)\n model = poly.fit(x,y)\n\nprint (\"R2 degree - %2i polynomial :%0.3f\" \\%(d,r2_score(y,model.predict(x))))\n\nOut: \n\n```", "```py\nIn: scatter = dataset.plot(kind='scatter', x=predictor,\\y='target', xlim=x_range, ylim=y_range)\n regr_line = scatter.plot(xt, model.predict(xt), '-',\\color='red', linewidth=2)\n\n```"]