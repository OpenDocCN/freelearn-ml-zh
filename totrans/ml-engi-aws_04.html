<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer158">
<h1 class="chapter-number" id="_idParaDest-75"><a id="_idTextAnchor078"/><a id="_idTextAnchor079"/><a id="_idTextAnchor080"/>4</h1>
<h1 id="_idParaDest-76"><a id="_idTextAnchor081"/>Serverless Data Management on AWS</h1>
<p>Businesses generally utilize systems that collect and store user information, along with transaction data, inside databases. One good example of this would be an e-commerce startup that has a web application where customers can create an account and use their credit card to make online purchases. The user profiles, transaction data, and purchase history stored in several production databases can be used to build a <strong class="bold">product recommendation engine</strong>, which<a id="_idIndexMarker325"/> can help suggest products that customers would probably want to purchase as well. However, before this stored data is analyzed and used <a id="_idIndexMarker326"/>to train <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, it must be merged and joined into a <strong class="bold">centralized data store</strong> so that<a id="_idIndexMarker327"/> it can be transformed and processed using a variety of tools and services. Several options are frequently used for these types of use <a id="_idIndexMarker328"/>cases, but <a id="_idIndexMarker329"/>we will focus on two of these in this chapter – <strong class="bold">data warehouses</strong> and <strong class="bold">data lakes</strong>. </p>
<p>Data warehouses and data lakes play a crucial role when it <a id="_idIndexMarker330"/>comes to <strong class="bold">data storage</strong> and <strong class="bold">data management</strong>. When <a id="_idIndexMarker331"/>generating reports, companies without a data warehouse or a data lake may end up performing queries in the production database of a running application directly. This approach is not recommended since it could cause service degradation or even unplanned downtime for the application connected to the database. This will inevitably affect the sales numbers since the customers would not be able to use the e-commerce application to purchase products online. Data warehouses and data lakes help us handle and analyze large amounts of data that could come from multiple smaller databases connected to running applications. If you have experience setting up a data warehouse or a data lake, then you probably know that it takes skill, experience, and patience to manage the overall cost, stability, and performance of these types of environments. It is a good thing that <em class="italic">serverless</em> services have started to become available to help us with these types of requirements.</p>
<p>In this chapter, we will focus on data management and use a variety of <em class="italic">serverless</em> services to manage and query our data. We will start by preparing a few prerequisites, including a new IAM user, a VPC, and an S3 bucket where the sample dataset will be stored. Once the prerequisites are ready, we will set up and configure a serverless data warehouse using <strong class="bold">Redshift Serverless</strong>. After that, we will use <strong class="bold">AWS Lake Formation</strong>, <strong class="bold">AWS Glue</strong>, and <strong class="bold">Amazon Athena</strong> to prepare a serverless data lake. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Getting started with serverless data management</li>
<li>Preparing the essential prerequisites</li>
<li>Running analytics at scale with Amazon Redshift Serverless</li>
<li>Setting up Lake Formation</li>
<li>Using Amazon Athena to query data in Amazon S3</li>
</ul>
<p>At this point, you are probably wondering what these services are and how these services are used. Before proceeding, let’s first have a quick discussion on how serverless data management works!</p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor082"/>Technical requirements</h1>
<p>Before we start, we must have the following ready:</p>
<ul>
<li>A web browser (preferably Chrome or Firefox)</li>
<li>Access to the AWS account that was used in the first few chapters of this book</li>
</ul>
<p>The Jupyter notebooks, source code, and other files for each chapter are available in this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor083"/>Getting started with serverless data management</h1>
<p>Years ago, developers, data <a id="_idIndexMarker332"/>scientists, and ML engineers had to spend hours or even days setting up the infrastructure needed for data management and data engineering. If a large dataset stored in S3 needed to be analyzed, a team of data scientists and ML engineers performed the following sequence of steps:</p>
<ol>
<li>Launch and configure a cluster of EC2 instances.</li>
<li>Copy the data from S3 to the volumes attached to the EC2 instances.</li>
<li>Perform queries on the data using one or more of the applications installed in the EC2 instances.</li>
</ol>
<p>One of the known challenges with this approach is that the provisioned resources may end up being underutilized. If the schedule of the data query operations is unpredictable, it would be tricky to manage the uptime, cost, and compute specifications of the setup as well. In addition to these, system administrators and DevOps engineers need to spend time managing the security, stability, performance, and configuration of the applications installed in the cluster.</p>
<p>Nowadays, it is much more practical to utilize <strong class="bold">serverless</strong> and managed services with these types of scenarios and use cases. As shown in the following diagram, we will have more time to focus on what we need to do since we no longer need to worry about server and<a id="_idIndexMarker333"/> infrastructure management when using serverless services:</p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 4.1 – Serverless versus not serverless " height="457" src="image/B18638_04_001.jpg" width="1081"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Serverless versus not serverless</p>
<p>What do we mean by <em class="italic">actual work</em>? Here’s a quick list of what data analysts, data scientists, and data engineers need to work on outside of server management:</p>
<ul>
<li>Generate charts and reports.</li>
<li>Analyze trends and patterns.</li>
<li>Detect and resolve data integrity issues.</li>
<li>Integrate data stores with business intelligence tools.</li>
<li>Make recommendations to management.</li>
<li>Use the data to train an ML model.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">When using serverless services, we also only pay for what we use. This means that we will not pay for the idle time the compute resources are not running. If we were to have both staging and production environments set up, we can be assured that the staging environment would only cost a fraction of the production environment setup since the resources in the production environment would have a higher expected utilization rate.</p>
<p>We can utilize <a id="_idIndexMarker334"/>different AWS services when dealing with the following <em class="italic">serverless</em> data management <a id="_idIndexMarker335"/>and data processing requirements:</p>
<ul>
<li><strong class="bold">Serverless Data Warehouse</strong>: Amazon Redshift Serverless</li>
<li><strong class="bold">Serverless Data Lake</strong>: AWS Lake Formation, AWS Glue, and Amazon Athena</li>
<li><strong class="bold">Serverless Stream Processing</strong>: Amazon Kinesis, AWS Lambda, and DynamoDB</li>
<li><strong class="bold">Serverless Distributed Data Processing</strong>: Amazon EMR Serverless</li>
</ul>
<p>Note that this is just the tip of the iceberg and that there are more serverless services we can use for <a id="_idIndexMarker336"/>our needs. In this chapter, we will focus on setting up and querying a <strong class="bold">serverless data warehouse</strong> and a <strong class="bold">serverless data lake</strong>. Before <a id="_idIndexMarker337"/>we proceed with these, first, let’s prepare the prerequisites.</p>
<p class="callout-heading">Note</p>
<p class="callout">At this point, you might be wondering when to use a data lake and when to use a data warehouse. A data warehouse is best used when the data being queried and processed is relational and defined in advance. The data quality of what is stored in a data warehouse is expected to be high as well. That said, a data warehouse is used as the “source of truth” of data and is generally used for use cases involving batch reporting and business intelligence. On the other hand, a data lake is best used when the data being queried and processed involves both relational and non-relational data from different data sources. Data stored in data lakes may include both raw and clean data. In addition to this, data is stored in a data lake without you having to worry about the data structure and schema during data capture. Finally, data lakes can be used for use cases involving ML, predictive<a id="_idIndexMarker338"/> analytics, and <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>). Since data lakes and data warehouses serve different purposes, some organizations utilize both options for <a id="_idIndexMarker339"/>their data management needs.</p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor084"/>Preparing the essential prerequisites</h1>
<p>In this section, we will ensure that the following prerequisites are ready before proceeding with setting up our data warehouse and data lake in this chapter:</p>
<ul>
<li>A text editor (for example, VS Code) on your local machine</li>
<li>An IAM user with the permissions to create and manage the resources we will use in this chapter</li>
<li>A VPC where we will launch the Redshift Serverless endpoint</li>
<li>A new S3 bucket where our data will be uploaded using AWS CloudShell</li>
</ul>
<p>In this chapter, we will create and manage our resources in the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. Make sure that you have set the correct region before proceeding with the next steps.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor085"/>Opening a text editor on your local machine</h2>
<p>Make sure <a id="_idIndexMarker340"/><a id="_idIndexMarker341"/>you have an open text editor (for<a id="_idIndexMarker342"/> example, <strong class="bold">VS Code</strong>) on your local machine. We will copy some string values into the text editor for later use in this chapter. Here are the values we will have to copy later in this chapter:</p>
<ul>
<li>IAM sign-in link, username, and password (<em class="italic">Preparing the essential prerequisites</em> &gt; <em class="italic">Creating an IAM user</em>)</li>
<li>VPC ID (<em class="italic">Preparing the essential prerequisites</em> &gt; <em class="italic">Creating a new VPC</em>)</li>
<li>Name of the created IAM role currently set as the default role (<em class="italic">Running analytics at scale with Amazon Redshift Serverless</em> &gt; <em class="italic">Setting up a Redshift Serverless endpoint</em>)</li>
<li>AWS Account ID (<em class="italic">Running analytics at scale with Amazon Redshift Serverless</em> &gt; <em class="italic">Unloading data to S3</em>)</li>
</ul>
<p>If you do not <a id="_idIndexMarker343"/>have<a id="_idIndexMarker344"/> VS Code<a id="_idIndexMarker345"/> installed, you can <a id="_idIndexMarker346"/>use <strong class="bold">TextEdit</strong>, <strong class="bold">Notepad</strong>, <strong class="bold">Notepad++</strong>, or <strong class="bold">GEdit</strong>, depending <a id="_idIndexMarker347"/>on what you have installed on your local machine.</p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor086"/>Creating an IAM user</h2>
<p>It is important to<a id="_idIndexMarker348"/> note that we may encounter issues when running queries in <strong class="bold">Redshift Serverless</strong> if we <a id="_idIndexMarker349"/>were to use the root account directly. That said, we will be creating an IAM user in this section. This IAM user will be configured to have the appropriate set of permissions needed to perform all the hands-on solutions in this chapter. </p>
<p class="callout-heading">Note</p>
<p class="callout">Make sure that you use the root account when creating a new IAM user. </p>
<p>Follow these steps to create an IAM user from the IAM console:</p>
<ol>
<li value="1">Navigate to the IAM console using the search bar, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 4.2 – Navigating to the IAM console " height="598" src="image/B18638_04_002.jpg" width="999"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Navigating to the IAM console</p>
<p class="list-inset">After typing <strong class="source-inline">iam</strong> in the <a id="_idIndexMarker350"/>search bar, we must select the <strong class="bold">IAM</strong> service from the list of search results.</p>
<ol>
<li value="2">Locate <strong class="bold">Access management</strong> in the sidebar and then click <strong class="bold">Users</strong> to navigate to the <strong class="bold">Users</strong> list page.</li>
<li>At the top right-hand corner of the screen, locate and click the <strong class="bold">Add users</strong> button.</li>
<li>On the <strong class="bold">Set user details</strong> page, add a new user using a configuration similar to what we have in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 4.3 – Creating a new IAM user " height="654" src="image/B18638_04_003.jpg" width="1026"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Creating a new IAM user</p>
<p class="list-inset">Here, we set <strong class="source-inline">mle-ch4-user</strong> as the value for the <strong class="bold">User name</strong> field. Under <strong class="bold">Select AWS access type</strong>, we ensure that the checkbox for <strong class="bold">Password – AWS Management Console access</strong> is checked under <strong class="bold">Select AWS credential type</strong>. For <strong class="bold">Console password</strong>, we choose <strong class="bold">Autogenerated password</strong>. For <strong class="bold">Require password reset</strong>, we <a id="_idIndexMarker351"/>uncheck <strong class="bold">User must create a new password at next sign-in</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">A more secure configuration would involve requiring a password reset when the IAM user account is used to sign in for the first time. However, we will skip this step in this chapter to reduce the overall number of steps.</p>
<ol>
<li value="5">Click the <strong class="bold">Next: Permissions</strong> button.</li>
<li>On the <strong class="bold">Set permissions</strong> page, select <strong class="bold">Attach existing policies directly</strong>.</li>
<li>Use the search filter to locate and check the checkboxes for the following managed policies: <ul><li><strong class="bold">AmazonS3FullAccess</strong> </li>
<li><strong class="bold">AmazonRedshiftFullAccess</strong> </li>
<li><strong class="bold">AmazonVPCFullAccess</strong> </li>
<li><strong class="bold">AWSCloudShellFullAccess</strong> </li>
<li><strong class="bold">AWSGlueConsoleFullAccess</strong> </li>
<li><strong class="bold">AmazonAthenaFullAccess</strong> </li>
<li><strong class="bold">IAMFullAccess</strong></li>
</ul></li>
</ol>
<p class="list-inset">An example of<a id="_idIndexMarker352"/> this can be seen in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 4.4 – Attaching existing policies directly " height="685" src="image/B18638_04_004.jpg" width="990"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Attaching existing policies directly</p>
<p class="list-inset">These are policies that have been prepared and managed by AWS to make it convenient and easy for AWS account users to manage IAM permissions.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Note that the permission configuration we have in this chapter can be improved further. When managing IAM permissions in a production-level account, make sure that you practice<a id="_idIndexMarker353"/> the <strong class="bold">principle of least privilege</strong>. This means that IAM identities should only have the minimum set of permissions to perform their tasks, which involves granting granular access to specific actions from specific resources when using a service. For more information, feel free to check out <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>.</p>
<ol>
<li value="8">Once you have selected the managed policies, click the <strong class="bold">Next: Tags</strong> button.</li>
<li>On the <strong class="bold">Add tags (optional)</strong> page, click <strong class="bold">Next: Review</strong>.</li>
<li>On the <strong class="bold">Review</strong> page, click the <strong class="bold">Create user</strong> button.</li>
<li>You should<a id="_idIndexMarker354"/> see a success notification, along with the sign-in link and credentials of the new user. Copy the sign-in link (for example, <strong class="source-inline">https://&lt;account&gt;.signin.aws.amazon.com/console</strong>), username, and password to a text editor in your local machine (for example, Visual Studio Code). Click the Close button afterward.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">Do not share the sign-in link, username, and password with anyone. The IAM user with these credentials can easily take over the entire account, given the permissions we have configured for the IAM user during creation.</p>
<ol>
<li value="12">On the <strong class="bold">Users</strong> page, you should see a success notification stating <strong class="bold">The user mle-ch4-user have been created</strong>. Click the <strong class="bold">user name</strong> value (for example, <strong class="source-inline">mle-ch4-user</strong>) in the success notification to navigate to the user details page.</li>
<li><strong class="bold">Add inline policy</strong>, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 4.5 – Adding an inline policy " height="481" src="image/B18638_04_005.jpg" width="916"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Adding an inline policy</p>
<p class="list-inset">In addition to the<a id="_idIndexMarker355"/> directly attached managed policies, we will be attaching an inline policy. We will customize the permissions that are configured with the inline policy in the next step.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information on managed policies and inline policies, check out <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml</a>.</p>
<ol>
<li value="14">On the <strong class="bold">Create policy</strong> page, navigate to the <strong class="bold">JSON</strong> tab, as shown here:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 4.6 – Creating a policy using the JSON editor " height="761" src="image/B18638_04_006.jpg" width="1183"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Creating a policy using the JSON editor</p>
<p class="list-inset">In the policy editor highlighted in the preceding screenshot, specify the following JSON<a id="_idIndexMarker356"/> configuration:</p>
<pre class="list-inset1 source-code">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "<strong class="bold">redshift-serverless</strong>:*",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "<strong class="bold">sqlworkbench</strong>:*",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "<strong class="bold">lakeformation</strong>:*",
            "Resource": "*"
        }
    ]
}</pre>
<p class="list-inset">This policy <a id="_idIndexMarker357"/>gives<a id="_idIndexMarker358"/> our IAM user permission to create and manage <strong class="bold">Redshift Serverless</strong>, <strong class="bold">Lake Formation</strong>, and <strong class="bold">SQL Workbench</strong> (a SQL query tool) resources. Without<a id="_idIndexMarker359"/> this additional <a id="_idIndexMarker360"/>inline policy, we would have issues using Redshift Serverless later in this chapter.</p>
<p class="callout-heading">Note</p>
<p class="callout">You can find a copy of this inline policy in the official GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy</a>.</p>
<ol>
<li value="15">Next, click <strong class="bold">Review policy</strong>.</li>
<li>On the <strong class="bold">Review policy</strong> page, specify <strong class="source-inline">custom-inline-policy</strong> as the value for the <strong class="bold">Name</strong> field. Then, click <strong class="bold">Create policy</strong>.</li>
</ol>
<p>At this point, the <strong class="source-inline">mle-ch4-user</strong> IAM user should have eight policies attached: seven AWS-managed policies and one inline policy. This IAM user should have more than enough permissions to perform all actions and operations until the end of this chapter.</p>
<p>Next, we will sign in using the credentials we have copied to the text editor in our local machine and test if we can sign in successfully:</p>
<ol>
<li value="1">Sign out of the AWS Management Console session by doing the following: <ol><li>Clicking your name in the top right-hand corner of the screen</li>
<li>Clicking the <strong class="bold">Sign out</strong> button</li>
</ol></li>
<li>Navigate to the sign-in link (with a format similar to <strong class="source-inline">https://&lt;account&gt;.signin.aws.amazon.com/console</strong>). Make sure that you <a id="_idIndexMarker361"/>replace <strong class="source-inline">&lt;account&gt;</strong> with the account ID or alias of your AWS account:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 4.7 – Sign in as IAM user " height="554" src="image/B18638_04_007.jpg" width="956"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Sign in as IAM user</p>
<p class="list-inset">This should redirect you to the <strong class="bold">Sign in as IAM user</strong> page, similar to what we have in the preceding screenshot. Input the <strong class="bold">Account ID</strong>, <strong class="bold">IAM user name</strong>, and <strong class="bold">Password</strong> values, and then click the <strong class="bold">Sign in</strong> button. </p>
<p><em class="italic">Wasn’t that easy?</em> Now<a id="_idIndexMarker362"/> that we have successfully created our IAM user, we can create a new VPC. This VPC will be used later when we create our Redshift Serverless endpoint.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor087"/>Creating a new VPC</h2>
<p><strong class="bold">Amazon Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) enables <a id="_idIndexMarker363"/>us to create and configure isolated virtual networks for our resources. In this section, we will create a new VPC from scratch even if we already have an existing one in the current region. This allows our Redshift Serverless instance to be launched in its own isolated network, which allows the network to be configured and secured separately from other existing VPCs.</p>
<p>There are different ways to create and configure a VPC. One of the faster ways is to use the <strong class="bold">VPC Wizard</strong>, which<a id="_idIndexMarker364"/> lets us set up a new VPC in just a few minutes.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, make sure that you are logged in as the <strong class="source-inline">mle-ch4-user</strong> IAM user.</p>
<p>Follow these steps to create a new VPC using the <strong class="bold">VPC Wizard</strong>:</p>
<ol>
<li value="1">Navigate to the region of choice using the region drop-down in the menu bar. In this chapter, we’ll assume that we will create and manage our resources in the <strong class="bold">US West (Oregon)</strong> | <strong class="source-inline">us-west-2</strong> region. </li>
<li>Navigate to the VPC console by doing the following:<ol><li>Typing <strong class="source-inline">VPC</strong> in the search bar of the AWS Management Console</li>
<li>Selecting the <strong class="bold">VPC</strong> service under the list of search results</li>
</ol></li>
<li>Next, click the <strong class="bold">Launch VPC Wizard/Create VPC</strong> button. This will redirect you to the <strong class="bold">Create VPC</strong> wizard, as shown in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 4.8 – The Create VPC wizard " height="803" src="image/B18638_04_008.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – The Create VPC wizard</p>
<p class="list-inset">Here, we can see that we can create and configure the relevant VPC resources with just a <a id="_idIndexMarker365"/>few clicks using the VPC Wizard.</p>
<p class="callout-heading">Note</p>
<p class="callout">You may want to customize and secure this VPC setup further, but this is outside the scope of this chapter. For more information, check out <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml</a>.</p>
<ol>
<li value="4">In the VPC Wizard, leave everything as-is except for the following:<ul><li><strong class="bold">Name tag auto-generation</strong>: <strong class="source-inline">mle-ch4-vpc</strong></li>
<li><strong class="bold">Number of Availability Zones (AZs)</strong>: <strong class="bold">3</strong></li>
<li><strong class="bold">Number of public subnets</strong>: <strong class="bold">3</strong></li>
<li><strong class="bold">Number of private subnets</strong>: <strong class="bold">0</strong></li>
<li><strong class="bold">NAT gateways ($)</strong>: <strong class="bold">None</strong></li>
</ul></li>
<li>Once you are done configuring the VPC, click the <strong class="bold">Create VPC</strong> button located at the bottom of the page.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">VPC creation may take about 1 to 2 minutes to complete.</p>
<ol>
<li value="6">Click <strong class="bold">View VPC</strong>.</li>
<li>Copy the VPC ID (for example, <strong class="source-inline">vpc-abcdefghijklmnop</strong>) into an editor on your local machine (for example, Visual Studio Code). </li>
</ol>
<p>Now that the <a id="_idIndexMarker366"/>required VPC resources have been created, we can proceed with the last set of prerequisites.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor088"/>Uploading the dataset to S3</h2>
<p>In <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, we used<a id="_idIndexMarker367"/> an <strong class="bold">AWS Cloud9</strong> environment<a id="_idIndexMarker368"/> to upload<a id="_idIndexMarker369"/> a sample <a id="_idIndexMarker370"/>dataset to <strong class="bold">Amazon S3</strong>. In this chapter, we will <a id="_idIndexMarker371"/>use <strong class="bold">AWS CloudShell</strong> instead to upload and download data to and from S3. If this is your first time hearing about AWS CloudShell, it is a browser-based shell where we can run different commands to manage our resources. With CloudShell, we can run commands using the AWS CLI without having to worry about infrastructure management.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, make sure that you are using the same region where the VPC resources were created. This chapter assumes that we are<a id="_idIndexMarker372"/> using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. At the same time, please make sure that you are logged in as the <strong class="source-inline">mle-ch4-user</strong> IAM user.</p>
<p>Follow these steps to use CloudShell and the AWS CLI to upload our sample dataset to S3:</p>
<ol>
<li value="1">Navigate to <strong class="bold">CloudShell</strong> by clicking the button highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 4.9 – Launching CloudShell " height="59" src="image/B18638_04_009.jpg" width="430"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Launching CloudShell</p>
<p class="list-inset">You can find this button in the top right-hand corner of the AWS Management Console. You may also use the search bar to navigate to the CloudShell console.</p>
<ol>
<li value="2">If you see the <strong class="bold">Welcome to AWS CloudShell</strong> popup window, click the <strong class="bold">Close</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">It may take a minute or two for the <strong class="bold">CloudShell</strong> environment to load. You should be able to run Terminal commands once you see <strong class="source-inline">[cloudshell-user@ip-XX-XX-XX-XX ~]$</strong>.</p>
<ol>
<li value="3">Run <a id="_idIndexMarker373"/>the <a id="_idIndexMarker374"/>following single-line <strong class="source-inline">wget</strong> command in the Terminal console (after the <strong class="bold">$</strong> sign) to download a CSV file containing 100,000 booking records:<pre class="source-code"><strong class="bold">wget https://bit.ly/3L6FsRg -O synthetic.bookings.100000.csv</strong></pre></li>
<li>Next, inspect the downloaded file using the <strong class="source-inline">head</strong> command:<pre class="source-code"><strong class="bold">head synthetic.bookings.100000.csv</strong></pre></li>
</ol>
<p class="list-inset">This should yield the first few lines of the CSV file, similar to what we have in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 4.10 – Results after using the head command " height="209" src="image/B18638_04_010.jpg" width="793"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Results after using the head command</p>
<p class="list-inset">As we can see, the <strong class="source-inline">head</strong> command displayed the first 10 lines of the <strong class="source-inline">synthetic.bookings.100000.csv</strong> file. Here, we have the header with all the column names of the CSV file in the first line.</p>
<p class="list-inset">Note that this dataset is similar to the hotel bookings dataset we used in <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>. The only major difference is <a id="_idIndexMarker375"/>that<a id="_idIndexMarker376"/> the CSV file we will use in this chapter contains 100,000 records since we want to test how fast we can query data from our data warehouse and data lake.</p>
<ol>
<li value="5">Create a new S3 bucket using the <strong class="source-inline">aws s3 mb</strong> command. Make sure to replace <strong class="source-inline">&lt;INSERT BUCKET NAME&gt;</strong> with a globally unique bucket name – an S3 bucket name that has never been used before by all other AWS users:<pre class="source-code"><strong class="bold">BUCKET_NAME=&lt;INSERT BUCKET NAME&gt;</strong></pre><pre class="source-code"><strong class="bold">aws s3 mb s3://$BUCKET_NAME</strong></pre></li>
</ol>
<p class="list-inset">For more information on S3 bucket naming rules, check out <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Make sure to remember the name of the bucket that was created in this step. We will use this S3 bucket in the different solutions and examples in this chapter.</p>
<ol>
<li value="6">Copy the name of the S3 bucket you created into the text editor on your local machine.</li>
<li>Upload the <strong class="source-inline">synthetic.bookings.100000.csv</strong> file using the <strong class="source-inline">aws s3 cp</strong> command:<pre class="source-code"><strong class="bold">FILE=synthetic.bookings.100000.csv</strong></pre><pre class="source-code"><strong class="bold">aws s3 cp $FILE s3://$BUCKET_NAME/input/$FILE</strong></pre></li>
</ol>
<p>Now that all<a id="_idIndexMarker377"/> the <a id="_idIndexMarker378"/>prerequisites are ready, we can use <strong class="bold">Redshift Serverless</strong> to load and query our data.</p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor089"/>Running analytics at scale with Amazon Redshift Serverless</h1>
<p>Data<a id="_idIndexMarker379"/> warehouses play a<a id="_idIndexMarker380"/> crucial role in data management, data analysis, and data engineering. Data engineers and ML engineers spend time building data warehouses to <a id="_idIndexMarker381"/>work on projects<a id="_idIndexMarker382"/> involving <strong class="bold">batch reporting</strong> and <strong class="bold">business intelligence</strong>.</p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 4.11 – Data warehouse " height="513" src="image/B18638_04_011.jpg" width="1152"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Data warehouse</p>
<p>As shown in the preceding diagram, a data warehouse contains combined data from different relational data sources such as PostgreSQL and MySQL databases. It generally serves as the single source of truth when querying data for reporting and business intelligence requirements. In ML experiments, a data warehouse can serve as the source of clean data where we can extract the dataset used to build and train ML models.</p>
<p class="callout-heading">Note</p>
<p class="callout">When generating reports, businesses and start-ups may end up performing queries directly on the production databases used by running web applications. It is important to note that these queries may cause unplanned downtime for the web applications connected to the databases (since the databases might become “busy” processing the additional queries). To avoid these types of scenarios, it is recommended to join and load the data from the application databases to a central data warehouse, where queries can be run safely. This means that we can generate automated reports and perform read queries on a copy of the data without worrying about any unexpected downtime. </p>
<p>If you need to set up a data warehouse on AWS, Amazon Redshift is one of the primary options<a id="_idIndexMarker383"/> available. With the<a id="_idIndexMarker384"/> announcement of <strong class="bold">Amazon Redshift Serverless</strong>, data engineers and ML engineers no longer need to worry about infrastructure management. Compared to its non-serverless counterparts and alternatives, there is no charge when the data warehouse is idle and not being used.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor090"/>Setting up a Redshift Serverless endpoint</h2>
<p>Getting started<a id="_idIndexMarker385"/> and setting up Redshift Serverless is easy. All we need to do is navigate to the Redshift console and create a new Redshift Serverless endpoint. When creating a new Redshift Serverless endpoint, all we need to worry about is the VPC and the IAM user, which we prepared in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, make sure that you are using the same region where the S3 bucket and VPC resources were created. This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. At the same time, please make sure that you are logged in as the <strong class="source-inline">mle-ch4-user</strong> IAM user.</p>
<p>Follow these steps to set up our Redshift Serverless endpoint:</p>
<ol>
<li value="1">Navigate to the <strong class="bold">Amazon Redshift</strong> console by typing <strong class="source-inline">redshift</strong> in the search bar of the AWS Management Console, and then selecting the <strong class="bold">Redshift</strong> service from the list of results.</li>
<li>Next, click the <strong class="bold">Try Amazon Redshift Serverless</strong> button.</li>
<li>On the <strong class="bold">Get started with Amazon Redshift Serverless</strong> page, configure Redshift <a id="_idIndexMarker386"/><a id="_idIndexMarker387"/>Serverless so that it has the following initial configuration values:<ul><li><strong class="bold">Configuration</strong>: <strong class="source-inline">Customize settings</strong></li>
<li><strong class="bold">Database name</strong>: <strong class="source-inline">dev</strong></li>
<li><strong class="bold">Admin user credentials</strong> &gt; <strong class="bold">Customize admin user credentials</strong>: <em class="italic">[UNCHECKED]</em></li>
</ul></li>
<li>Under <strong class="bold">Permissions</strong>, open the <strong class="bold">Manage IAM roles</strong> dropdown, and then select <strong class="bold">Create IAM role</strong> from the list of options.</li>
<li>In the <strong class="bold">Create the default IAM role</strong> popup window, select <strong class="bold">Any S3 bucket</strong>.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">Note that this configuration needs to be secured further once we need to configure our setup for production use. Ideally, Redshift is configured to access only a limited set of S3 buckets.</p>
<ol>
<li value="6">Click the <strong class="bold">Create IAM role as default</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You should see a notification message similar to <strong class="bold">The IAM role AmazonRedshift-CommandsAccessRole-XXXXXXXXXXXXXXX was successfully created and set as the default.</strong> after clicking the <strong class="bold">Create IAM role as default</strong> button. Make sure that you copy the name of the created IAM role currently set as the default role into the text editor on your local machine.</p>
<ol>
<li value="7">Next, use the following configuration settings for <strong class="bold">Network and security</strong>:<ul><li><strong class="bold">Virtual private cloud (VPC)</strong>: Use the VPC you created in this chapter by selecting the appropriate VPC ID.</li>
<li><strong class="bold">VPC security groups</strong>: Use the default VPC security group.</li>
<li><strong class="bold">Subnet</strong>: Check all the subnets in the list of options available in the dropdown menu.</li>
</ul></li>
<li>Click the <strong class="bold">Save configuration</strong> button.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step may take 3 to 5 minutes to complete. You should see a popup window while you wait for the setup to complete. In the meantime, you may grab a cup of coffee or tea!</p>
<ol>
<li value="9">Once the setup is complete, click the <strong class="bold">Continue</strong> button to close the popup window.</li>
</ol>
<p><em class="italic">Wasn’t that easy?</em> At this point, you might be worried about the costs associated with the <strong class="bold">Redshift Serverless</strong> setup we have right now. The cool thing here is that there is no charge for the compute capacity when our serverless data warehouse is idle. Note that we’ll still be charged for storage, depending on the data stored. Once you have completed the hands-on solutions in this chapter, make sure to delete this setup and perform the relevant AWS resource cleanup steps to avoid any unexpected charges.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information on Redshift Serverless billing, feel free to check out <a href="https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml">https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml</a>.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor091"/>Opening Redshift query editor v2</h2>
<p>There are different<a id="_idIndexMarker388"/> ways to access the Redshift Serverless endpoint we have configured and prepared. One of the more convenient ways is to use the <strong class="bold">Redshift query editor</strong>, which we can access using our web browser.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, make sure that you are using the same region where the S3 bucket and VPC resources were created. This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. At the same time, please make sure that you are logged in as the <strong class="source-inline">mle-ch4-user</strong> IAM user.</p>
<p>Let’s open the Redshift query editor and see what we can do with it:</p>
<ol>
<li value="1">In the <strong class="bold">Serverless dashboard</strong> area, click <strong class="bold">Query data</strong>, as highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 4.12 – Locating the Query data button in the Serverless dashboard " height="548" src="image/B18638_04_012.jpg" width="1049"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Locating the Query data button in the Serverless dashboard</p>
<p class="list-inset">Here, we can see that the <strong class="bold">Query data</strong> button is located near the top right of the <strong class="bold">Serverless dashboard</strong> page. Clicking the <strong class="bold">Query data</strong> button will open the <strong class="bold">Redshift query editor v2</strong> service (in a new tab), as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 4.13 – Redshift query editor v2 " height="726" src="image/B18638_04_013.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Redshift query editor v2</p>
<p class="list-inset">Using the Redshift <a id="_idIndexMarker389"/>query editor is straightforward. We can manage our resources using the options available in the left-hand sidebar and we can run SQL queries on the editor on the right-hand side.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you are having issues opening Redshift query editor v2 after clicking the <strong class="bold">Query data</strong> button, make sure that your browser is not blocking new windows or popups from being opened.</p>
<ol>
<li value="2">Click the arrow sign of the <strong class="bold">Serverless</strong> connection resource, as highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 4.14 – Connecting to the Serverless resource " height="188" src="image/B18638_04_014.jpg" width="331"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Connecting to the Serverless resource</p>
<p class="list-inset">You should see a <strong class="bold">Connecting to Serverless</strong> notification while the Redshift query<a id="_idIndexMarker390"/> editor is connecting to the Redshift Serverless endpoint.</p>
<p>Once the connection has been made, we can proceed with creating a table.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor092"/>Creating a table</h2>
<p>There are different ways to create<a id="_idIndexMarker391"/> a table in Amazon Redshift. Follow these steps to create a table using a CSV file as a reference for the table schema:</p>
<ol>
<li value="1">Download the <strong class="source-inline">synthetic.bookings.10.csv</strong> file from the official GitHub repository to your local machine. You can access the CSV file, which contains 10 sample rows, here: <a href="https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv">https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv</a>.</li>
<li>In <strong class="bold">Redshift query editor v2</strong>, click the <strong class="bold">+ Create</strong> dropdown and then select <strong class="bold">Table</strong> from the list of options.</li>
<li>In the <strong class="bold">Create table</strong> popup window, set the <strong class="bold">Schema</strong> dropdown value to <strong class="bold">public</strong> and the <strong class="bold">Table</strong> field value to <strong class="bold">bookings</strong>.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Schemas are used to manage and group database objects and tables together. A newly created database will have the <strong class="source-inline">PUBLIC</strong> schema by default. In this chapter, we will not create a new schema and simply use the default <strong class="source-inline">PUBLIC</strong> schema.</p>
<ol>
<li value="4">Click the <strong class="bold">Load from CSV</strong> button and select the downloaded <strong class="source-inline">synthetic.bookings.10.csv</strong> file from your local machine: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer133">
<img alt="Figure 4.15 – Load from CSV " height="667" src="image/B18638_04_015.jpg" width="938"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Load from CSV</p>
<p class="list-inset">Here, we can<a id="_idIndexMarker392"/> see that the <strong class="bold">Load from CSV</strong> option used the records stored in the selected CSV file to infer the column names, data types, and encodings that will be used to configure and create the new table.</p>
<ol>
<li value="5">Click <strong class="bold">Create table</strong>. You should see a notification stating <strong class="bold">bookings table is created successfully</strong>.</li>
</ol>
<p>Note that the CSV file used as a reference to create the table should be, ideally, a subset of the larger complete dataset. In our case, we used a CSV file that contains 10 records from the original CSV file with 100,000 records.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor093"/>Loading data from S3</h2>
<p>Now that our<a id="_idIndexMarker393"/> table is ready, we can load the data stored in S3 into our table. Follow these steps to load the data from S3 using <strong class="bold">Redshift query editor v2</strong>:</p>
<ol>
<li value="1">Click the <strong class="bold">Load data</strong> button (beside the <strong class="bold">+ Create</strong> dropdown). A popup window similar to the following should appear:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 4.16 – Loading data from S3 " height="617" src="image/B18638_04_016.jpg" width="694"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Loading data from S3</p>
<p class="list-inset">Here, we can see the different configuration options available for loading data from S3. </p>
<ol>
<li value="2">Open the <strong class="bold">S3 file location</strong> dropdown under <strong class="bold">S3 URI</strong> and select <strong class="bold">us-west-2</strong> from the list of options available.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This setup assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region when performing the hands-on solutions in this chapter. Feel free to change this if the S3 file is located in another region.</p>
<ol>
<li value="3">Next, click the <strong class="bold">Browse S3</strong> button. Locate and select the uploaded <strong class="source-inline">synthetic.bookings.100000.csv</strong> file inside the <strong class="bold">input</strong> folder of the S3 bucket<a id="_idIndexMarker394"/> we created in this chapter: </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 4.17 – Choose archive in S3 " height="487" src="image/B18638_04_017.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17 – Choose archive in S3</p>
<p class="list-inset">After selecting the <strong class="source-inline">synthetic.bookings.100000.csv</strong> file, click the <strong class="bold">Choose</strong> button.</p>
<ol>
<li value="4">Open the <strong class="bold">Choose an IAM role</strong> dropdown and select the IAM role with the same name as what you have copied into the text editor on your local machine in the <em class="italic">Setting up a Redshift Serverless endpoint</em> section.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you were unable to copy the IAM role to a text editor in your local machine, you may open a new tab and navigate to the <strong class="source-inline">default</strong> <strong class="bold">Namespace configuration</strong> page (in the AWS Management Console). You should find the IAM role (tagged as <strong class="bold">Default</strong> under <strong class="bold">Role type</strong>) in the <strong class="bold">Security and encryption</strong> tab.</p>
<ol>
<li value="5">Under <strong class="bold">Advanced settings</strong>, click <strong class="bold">Data conversion parameters</strong>. Ensure that the checkbox for <strong class="bold">Ignore header rows</strong> is <em class="italic">checked</em>. Click <strong class="bold">Done</strong>.</li>
<li>Click <strong class="bold">Select a schema</strong> and then choose <strong class="bold">public</strong> from the list of dropdown options. Next, click <strong class="bold">Select a table</strong> and then select <strong class="bold">bookings</strong> from the list of dropdown <a id="_idIndexMarker395"/>options:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 4.18 – Loading data from the S3 bucket " height="706" src="image/B18638_04_018.jpg" width="808"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18 – Loading data from the S3 bucket</p>
<p class="list-inset">At this point, you should have a set of configuration parameters similar to what’s shown in the preceding screenshot.</p>
<ol>
<li value="7">Click the <strong class="bold">Load data</strong> button. This will close the <strong class="bold">Load data</strong> window and automatically run the load data operation.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You may run the <strong class="source-inline">SELECT * FROM sys_load_error_detail;</strong> SQL statement in the query editor to troubleshoot any issues or errors you may have encountered.</p>
<p>The final step may take about 1 to 2 minutes to complete. If you did not encounter any issues after<a id="_idIndexMarker396"/> running the load data operation, you can proceed with querying the database! </p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor094"/>Querying the database</h2>
<p>Now that we have <a id="_idIndexMarker397"/>successfully loaded the CSV file from the S3 bucket to our Redshift Serverless table, let’s focus on performing queries using SQL statements:</p>
<ol>
<li value="1">Click the <strong class="bold">+</strong> button (located at the left of the first tab) and then select <strong class="bold">Notebook</strong>, as shown here:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 4.19 – Creating a new SQL Notebook " height="543" src="image/B18638_04_019.jpg" width="1028"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19 – Creating a new SQL Notebook</p>
<p class="list-inset"><strong class="bold">SQL Notebooks</strong> help organize and document the results of multiple SQL queries run using the Redshift query editor.</p>
<ol>
<li value="2">Run the following SQL statement:<pre class="source-code"><strong class="bold">SELECT * FROM dev.public.bookings;</strong></pre></li>
</ol>
<p class="list-inset">Make sure to click the <strong class="bold">Run</strong> button, as highlighted in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 4.20 – Running the SQL query " height="120" src="image/B18638_04_020.jpg" width="881"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20 – Running the SQL query</p>
<p class="list-inset">This should <a id="_idIndexMarker398"/>return a set of results, similar to what we have in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 4.21 – The Add SQL button " height="509" src="image/B18638_04_021.jpg" width="1144"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.21 – The Add SQL button</p>
<p class="list-inset">Here, we should only get a maximum of 100 records after running the query since the <strong class="bold">Limit 100</strong> checkbox is toggled <em class="italic">to on</em>.</p>
<ol>
<li value="3">Click the <strong class="bold">Add SQL</strong> button afterward to create a new SQL cell below the current set of results.</li>
<li>Next, run the following SQL statement in the new SQL cell:<pre class="source-code"><strong class="bold">SELECT COUNT(*) FROM dev.public.bookings WHERE is_cancelled = 0;</strong></pre></li>
</ol>
<p class="list-inset">We should get <strong class="source-inline">66987</strong> as the result after running the query.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">You may run the <strong class="source-inline">SELECT * FROM sys_load_error_detail;</strong> SQL statement to troubleshoot and debug any issues.</p>
<ol>
<li value="5">Let’s try reviewing the bookings that have been canceled by guests with at least one previous cancellation. That said, let’s run the following SQL statement in a new SQL cell:<pre class="source-code"><strong class="bold">SELECT * FROM dev.public.bookings WHERE is_cancelled = 1 AND previous_cancellations &gt; 0;</strong></pre></li>
<li>Let’s also review the bookings that have been canceled by guests where the number<a id="_idIndexMarker399"/> of days on the waiting list exceeds 50:<pre class="source-code"><strong class="bold">SELECT * FROM dev.public.bookings WHERE is_cancelled = 1 AND days_in_waiting_list &gt; 50;</strong></pre></li>
<li>Note that we can also check for <strong class="bold">data integrity issues</strong> using queries similar to the following:<pre class="source-code"><strong class="bold">SELECT booking_changes, has_booking_changes, * </strong></pre><pre class="source-code"><strong class="bold">FROM dev.public.bookings </strong></pre><pre class="source-code"><strong class="bold">WHERE </strong></pre><pre class="source-code"><strong class="bold">(booking_changes=0 AND has_booking_changes='True') </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(booking_changes&gt;0 AND has_booking_changes='False');</strong></pre></li>
</ol>
<p class="list-inset">With this query, we should be able to list the records where the <strong class="source-inline">booking_changes</strong> column value does not match the <strong class="source-inline">has_booking_changes</strong> column value.</p>
<ol>
<li value="8">Similarly, we can find other records with data integrity concerns using the following query:<pre class="source-code"><strong class="bold">SELECT total_of_special_requests, has_special_requests, * </strong></pre><pre class="source-code"><strong class="bold">FROM dev.public.bookings </strong></pre><pre class="source-code"><strong class="bold">WHERE </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests=0 AND has_special_requests='True') </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests&gt;0 AND has_special_requests='False');</strong></pre></li>
</ol>
<p class="list-inset">With this query, we <a id="_idIndexMarker400"/>should be able to list the records where the <strong class="source-inline">total_of_special_requests</strong> column value does not match the <strong class="source-inline">has_special_requests</strong> column value.</p>
<p class="callout-heading">Note</p>
<p class="callout">These types of data integrity issues should be fixed before using the data to train an ML model.</p>
<ol>
<li value="9">We can also create a materialized view containing a precomputed result set, which can help speed up repeated queries:<pre class="source-code"><strong class="bold">CREATE MATERIALIZED VIEW data_integrity_issues</strong> <strong class="bold">AS</strong></pre><pre class="source-code"><strong class="bold">SELECT * </strong></pre><pre class="source-code"><strong class="bold">FROM dev.public.bookings </strong></pre><pre class="source-code"><strong class="bold">WHERE</strong></pre><pre class="source-code"><strong class="bold">(booking_changes=0 AND has_booking_changes='True') </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(booking_changes&gt;0 AND has_booking_changes='False')</strong></pre><pre class="source-code"><strong class="bold">OR</strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests=0 AND has_special_requests='True') </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests&gt;0 AND has_special_requests='False');</strong></pre></li>
<li>Finally, we can query the precomputed data in the materialized view using the following query:<pre class="source-code"><strong class="bold">SELECT booking_changes, has_booking_changes, total_of_special_requests, has_special_requests FROM data_integrity_issues;</strong></pre></li>
</ol>
<p class="list-inset">This should give us the list of records where the <strong class="source-inline">total_of_special_requests</strong> column value does not match the <strong class="source-inline">has_special_requests</strong> column value, along with the records where the <strong class="source-inline">booking_changes</strong> column value does not match the <strong class="source-inline">has_booking_changes</strong> column <a id="_idIndexMarker401"/>value.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information about this topic, feel free to check out <a href="https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml">https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml</a>.</p>
<p>Feel free to run other SQL queries to explore the data stored in the <em class="italic">bookings</em> table.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor095"/>Unloading data to S3</h2>
<p>Finally, we will <a id="_idIndexMarker402"/>copy and unload the data stored in the <em class="italic">bookings</em> table to Amazon S3. Here, we will configure and use the <strong class="source-inline">UNLOAD</strong> command to perform this operation in parallel, split the data, and store it in S3 across several files.</p>
<p class="callout-heading">Note</p>
<p class="callout">Once the data has been unloaded to Amazon S3, we can perform other operations on this data using services, tools, and libraries that can load the data directly from S3. In our case, we will use the unloaded data files in the next section, <em class="italic">Setting up Lake Formation</em>, and <a id="_idIndexMarker403"/>use <strong class="bold">AWS Glue</strong> along <a id="_idIndexMarker404"/>with <strong class="bold">Amazon Athena</strong> to process the data files.</p>
<p>Follow these steps to unload the data stored in our Redshift Serverless table into an S3 bucket:</p>
<ol>
<li value="1">Open the menu (<strong class="source-inline">mle-ch4-user@&lt;ACCOUNT ALIAS&gt;</strong>) at the top right-hand side of the screen. Copy the account ID by clicking the boxes highlighted in the following <a id="_idIndexMarker405"/>screenshot. Save the copied account ID value to the text editor on your local machine:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 4.22 – Copying the account ID " height="403" src="image/B18638_04_022.jpg" width="394"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.22 – Copying the account ID</p>
<p class="list-inset">Note that the account ID should have no dashes when copied to the text editor on your local machine.</p>
<ol>
<li value="2">In <strong class="bold">Redshift query editor v2</strong>, click the <strong class="bold">Add SQL</strong> button and then run the following SQL statement in the new SQL cell:<pre class="source-code">UNLOAD ('SELECT * FROM dev.public.bookings;') </pre><pre class="source-code">TO 's3://<strong class="bold">&lt;INSERT BUCKET NAME&gt;</strong>/unloaded/'</pre><pre class="source-code">IAM_ROLE 'arn:aws:iam::<strong class="bold">&lt;ACCOUNT ID&gt;</strong>:role/service-role/<strong class="bold">&lt;ROLE NAME&gt;</strong>'</pre><pre class="source-code">FORMAT AS CSV DELIMITER ',' </pre><pre class="source-code">PARALLEL ON</pre><pre class="source-code">HEADER;</pre></li>
</ol>
<p class="list-inset">Make sure you replace the following values:</p>
<ul>
<li><strong class="source-inline">&lt;INSERT BUCKET NAME&gt;</strong> with the name of the bucket we created in the <em class="italic">Uploading the dataset to S3</em> section</li>
<li><strong class="source-inline">&lt;ACCOUNT ID&gt;</strong> with the account ID of the AWS account</li>
<li><strong class="source-inline">&lt;ROLE NAME&gt;</strong> with the IAM role name you copied into the text editor on your local machine in the <em class="italic">Setting up a Redshift Serverless endpoint</em> section</li>
</ul>
<p class="list-inset">Since <strong class="source-inline">PARALLEL ON</strong> is specified when running the <strong class="source-inline">UNLOAD</strong> command, this <strong class="source-inline">UNLOAD</strong> operation will split the data stored in the <em class="italic">bookings</em> table and store these in multiple <a id="_idIndexMarker406"/>files in parallel.</p>
<ol>
<li value="3">Navigate to <strong class="bold">AWS CloudShell</strong> by clicking the button highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 4.23 – Launching CloudShell " height="66" src="image/B18638_04_023.jpg" width="468"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.23 – Launching CloudShell</p>
<p class="list-inset">We can find this button in the top right-hand corner of the AWS Management Console. You may also use the search bar to navigate to the CloudShell console.</p>
<ol>
<li value="4">Run the following commands to list the files inside the <strong class="source-inline">unloaded</strong> folder of our S3 bucket. Make sure to replace <strong class="source-inline">&lt;INSERT BUCKET NAME&gt;</strong> with the name of the bucket we created in the <em class="italic">Uploading the dataset to S3</em> section:<pre class="source-code"><strong class="bold">BUCKET_NAME=&lt;INSERT BUCKET NAME&gt;</strong></pre><pre class="source-code"><strong class="bold">aws s3 ls s3://$BUCKET_NAME/unloaded/</strong></pre></li>
<li>Move all the files in the current working directory to the <strong class="source-inline">/tmp</strong> directory using the <strong class="source-inline">tmp</strong> command:<pre class="source-code"><strong class="bold">mv * /tmp</strong></pre></li>
<li>Use the <strong class="source-inline">aws s3 cp</strong> command to download a copy of the files stored inside the <strong class="source-inline">unloaded</strong> folder inside the S3 bucket:<pre class="source-code"><strong class="bold">aws s3 cp s3://$BUCKET_NAME/unloaded/ . --recursive</strong></pre></li>
<li>Use the <strong class="source-inline">ls</strong> command to check the filenames of the files that have been downloaded:<pre class="source-code"><strong class="bold">ls</strong></pre></li>
</ol>
<p class="list-inset">This should yield <a id="_idIndexMarker407"/>a list of filenames, similar to what we have in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 4.24 – Results after using the ls command " height="261" src="image/B18638_04_024.jpg" width="1248"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.24 – Results after using the ls command</p>
<p class="list-inset">Here, we can see that the <strong class="source-inline">UNLOAD</strong> operation that was performed in the <em class="italic">Unloading data to S3</em> section divided and stored a copy of the <em class="italic">bookings</em> table in several files.</p>
<ol>
<li value="8">Use the <strong class="source-inline">head</strong> command to inspect the first few lines of each of the downloaded files:<pre class="source-code"><strong class="bold">head *</strong></pre></li>
</ol>
<p class="list-inset">This should yield an output similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<img alt="Figure 4.25 – Results after using the head command " height="554" src="image/B18638_04_025.jpg" width="1349"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.25 – Results after using the head command</p>
<p class="list-inset">Here, we can see that each of the output files has a header with the corresponding names of each column.</p>
<p>Now that we have finished unloading data from the Redshift <em class="italic">bookings</em> table into our S3 bucket, we will<a id="_idIndexMarker408"/> proceed with setting up our data lake using AWS Lake Formation!</p>
<p class="callout-heading">Note</p>
<p class="callout">There is a lot more we can use in Amazon Redshift and Amazon Redshift Serverless. This includes performance tuning techniques (to significantly speed up slow queries), <strong class="bold">Redshift ML</strong> (which <a id="_idIndexMarker409"/>we can use to train and use ML models for inference using SQL<a id="_idIndexMarker410"/> statements), and <strong class="bold">Redshift Spectrum</strong> (which we can use to query data directly from files stored in S3 buckets). These topics are outside the scope of this book, so feel free to check out <a href="https://docs.aws.amazon.com/redshift/index.xhtml">https://docs.aws.amazon.com/redshift/index.xhtml</a> for more information.</p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor096"/>Setting up Lake Formation</h1>
<p>Now, it’s time to<a id="_idIndexMarker411"/> take a closer look at setting up our serverless data lake on AWS! Before we begin, let’s define what a data lake is and what type of data is stored in it. A <strong class="bold">data lake</strong> is a<a id="_idIndexMarker412"/> centralized data store that contains a variety of structured, semi-structured, and unstructured data from different data sources. As shown in the following diagram, data can be stored in a data lake without us having to worry about the structure and format. We can use a variety of file types such as JSON, CSV, and Apache Parquet when storing data in a data lake. In addition to these, data lakes may include both raw and processed (clean) data:</p>
<div>
<div class="IMG---Figure" id="_idContainer144">
<img alt="Figure 4.26 – Getting started with data lakes " height="472" src="image/B18638_04_026.jpg" width="1097"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.26 – Getting started with data lakes</p>
<p>ML engineers and<a id="_idIndexMarker413"/> data scientists can use data lakes as the source of the data used for building and training ML models. Since the data stored in data lakes may be a mixture of both raw and clean data, additional data processing, data cleaning, and data transformation steps are needed before it can be used in ML requirements.</p>
<p>If you are planning to set up and manage data lakes in AWS, <strong class="bold">AWS Lake Formation</strong> is the way to go! AWS Lake Formation is a service that helps set up and secure a data lake using a variety of <a id="_idIndexMarker414"/>services<a id="_idIndexMarker415"/> on AWS such as <strong class="bold">Amazon S3</strong>, <strong class="bold">AWS Glue</strong>, and <strong class="bold">Amazon Athena</strong>. Since we are utilizing <em class="italic">serverless</em> services with AWS Lake Formation, we won’t have to worry about managing any servers while we are setting up our data lake.</p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor097"/>Creating a database</h2>
<p>Similar to <a id="_idIndexMarker416"/>how <a id="_idIndexMarker417"/>databases work in Redshift and other services such as <strong class="bold"> Relational Database Service </strong>(<strong class="bold">RDS</strong>), <strong class="bold">AWS Lake Formation</strong> databases can contain one or more tables. However, before we create a table, we will need to create a new database.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Before proceeding, make sure that you are using the same region where the S3 bucket and VPC resources were created. This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region. At the same time, please make sure that you are logged in as the <strong class="source-inline">mle-ch4-user</strong> IAM user.</p>
<p>Follow these steps to create a new database in AWS Lake Formation:</p>
<ol>
<li value="1">Navigate to the AWS Lake Formation console by typing <strong class="source-inline">lake formation</strong> in the search box of the AWS Management Console and then selecting <strong class="bold">AWS Lake Formation</strong> from the list of results.</li>
<li>In the <strong class="bold">Welcome to Lake Formation</strong> popup window, make sure that the <strong class="bold">Add myself</strong> checkbox is <em class="italic">checked</em>. Click the <strong class="bold">Get started</strong> button.</li>
<li>In the sidebar, locate and click <strong class="bold">Databases</strong> under <strong class="bold">Data catalog</strong>.</li>
<li>Click the <strong class="bold">Create database</strong> button <a id="_idIndexMarker418"/>located in the top right-hand corner of the <strong class="bold">Databases</strong> page.</li>
<li>Under <strong class="bold">Database details</strong>, select the <strong class="bold">Database</strong> option and set <strong class="source-inline">mle-ch4-db</strong> as the value for the <strong class="bold">Name</strong> field. Leave everything else as-is and then click the <strong class="bold">Create database</strong> button:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 4.27 – Creating a Lake Formation database " height="576" src="image/B18638_04_027.jpg" width="1012"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.27 – Creating a Lake Formation database</p>
<p class="list-inset">You should see a success notification stating that your database has been created successfully. You may ignore the <strong class="bold">Unknown error</strong> message notification shown in the preceding screenshot.</p>
<p class="callout-heading">Note</p>
<p class="callout">The <strong class="bold">Unknown error</strong> message is most likely due to the limited permissions allowed with the current IAM user being used to perform the actions.</p>
<p>Now that we <a id="_idIndexMarker419"/>have created our Lake Formation database, let’s proceed with creating a table using an AWS Glue Crawler.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor098"/>Creating a table using an AWS Glue Crawler</h2>
<p><strong class="bold">AWS Glue</strong> is<a id="_idIndexMarker420"/> a serverless <strong class="bold">extract-transform-load</strong> (<strong class="bold">ETL</strong>) service<a id="_idIndexMarker421"/> that provides different relevant components and capabilities for data integration. In this chapter, we will use one of the components of <strong class="bold">AWS Glue</strong> – the <strong class="bold">AWS Glue Crawler</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="Figure 4.28 – How an AWS Glue Crawler works " height="314" src="image/B18638_04_028.jpg" width="1108"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.28 – How an AWS Glue Crawler works</p>
<p>As shown in the preceding diagram, an <strong class="bold">AWS Glue Crawler</strong> processes the files stored in the target data stores and then infers a schema based on the structure and content of the files processed. This schema is used to create a table or a set of tables in <strong class="bold">AWS Glue Data Catalog</strong>. These<a id="_idIndexMarker422"/> tables can then be used by services such as <strong class="bold">Amazon Athena</strong> when querying data <a id="_idIndexMarker423"/>directly in S3.</p>
<p>With these in mind, let’s proceed with creating an AWS Glue Crawler:</p>
<ol>
<li value="1">Navigate to the <strong class="bold">Tables</strong> list page by clicking <strong class="bold">Tables</strong> from the sidebar.</li>
<li>Next, click the <strong class="bold">Create table using a crawler</strong> button (located in the top-left corner of the page). This will open <strong class="bold">AWS Glue Console</strong> in a new tab.</li>
<li>Click <strong class="bold">Crawlers </strong>(<strong class="bold">legacy</strong>), as highlighted in the following screenshot:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="Figure 4.29 – Navigating to the Crawlers page " height="599" src="image/B18638_04_029.jpg" width="1640"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.29 – Navigating to the Crawlers page</p>
<p class="list-inset">As we can see, we can navigate to the <strong class="bold">Crawlers</strong> page using the sidebar on the left-hand side of the screen.</p>
<ol>
<li value="4">Create a new crawler by clicking the <strong class="bold">Add crawler</strong> button.</li>
<li>On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Add information about your crawler</strong> page, specify <strong class="source-inline">mle-ch4-crawler</strong> as the <strong class="bold">Crawler name</strong> field’s value. Then, click <strong class="bold">Next</strong>.</li>
<li>On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Specify crawler source type</strong> page, choose <strong class="bold">Data stores</strong> for <strong class="bold">Crawler source type</strong>. Under <strong class="bold">Repeat crawls of S3 data stores</strong>, select <strong class="bold">Crawl all folders</strong>. Then, click <strong class="bold">Next</strong>.</li>
<li>On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Add a data store</strong> page, click the folder icon to set the S3 path location for the <strong class="bold">Include path</strong> field. This should open the <strong class="bold">Choose S3 path</strong> popup<a id="_idIndexMarker424"/> window, as shown here:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 4.30 – Choose S3 path " height="550" src="image/B18638_04_030.jpg" width="628"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.30 – Choose S3 path</p>
<p class="list-inset">Locate and toggle the checkbox for the <strong class="source-inline">unloaded</strong> folder inside the S3 bucket we created in the <em class="italic">Preparing the essential prerequisites</em> section of this chapter.<a id="_idTextAnchor099"/> Click the <strong class="bold">Select</strong> button afterward.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">If you skipped the <em class="italic">Getting started with Redshift Serverless</em> section of this chapter, you may create an empty <strong class="source-inline">unloaded</strong> folder in the S3 bucket and then upload the <strong class="source-inline">synthetic.bookings.100000.csv</strong> file to the <strong class="source-inline">unloaded</strong> folder. You may manually do this using the AWS Management Console or by using <strong class="bold">AWS CloudShell</strong>, similar to how we uploaded the CSV file to the <strong class="source-inline">input</strong> folder of the S3 bucket with the AWS CLI.</p>
<ol>
<li value="8">Set the <strong class="bold">Sample Size (optional)</strong> value to <strong class="source-inline">100</strong>.</li>
<li>Make sure that the configuration you set on the <strong class="bold">Add a data store</strong> page is similar to <a id="_idIndexMarker425"/>what we have in the following screenshot before proceeding:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="Figure 4.31 – Adding a data store " height="756" src="image/B18638_04_031.jpg" width="538"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.31 – Adding a data store</p>
<p class="list-inset">Once you have reviewed the data store configuration, click <strong class="bold">Next</strong>.</p>
<ol>
<li value="10">On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Add another data store</strong> page, select the <strong class="bold">No</strong> option. Click the <strong class="bold">Next</strong> button afterward.</li>
<li>On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Choose an IAM role</strong> page, select the <strong class="bold">Create an IAM</strong> role option. Set <strong class="source-inline">ch4-iam</strong> as the input field value under <strong class="bold">IAM role</strong> so that the complete IAM role name is <strong class="source-inline">AWSGlueServiceRole-ch4-iam</strong>. After that, click <strong class="bold">Next</strong>.</li>
<li>On the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Create a schedule for this crawler</strong> page, choose <strong class="bold">Run on-demand</strong> from the list of dropdown options under <strong class="bold">Frequency</strong>. Click the <strong class="bold">Next</strong> button afterward.</li>
<li>On <a id="_idIndexMarker426"/>the <strong class="bold">Add crawler</strong> &gt; <strong class="bold">Configure the crawler’s output</strong> page, choose the database we have created (for example, <strong class="bold">mle-ch4-db</strong>) from the list of dropdown options under <strong class="bold">Database</strong>. Click the <strong class="bold">Next</strong> button afterward.</li>
<li>Click <strong class="bold">Finish</strong> to create the AWS Glue crawler using the specified configuration parameters.</li>
<li>Let’s run the Crawler by navigating to the <strong class="bold">Crawlers</strong> page (the new interface / NOT the <strong class="bold">Legacy</strong> pages), selecting the crawler, and then clicking the <strong class="bold">Run</strong> button:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="Figure 4.32 – Running the crawler " height="587" src="image/B18638_04_032.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.32 – Navigating to the Crawlers page</p>
<p class="callout-heading">Note</p>
<p class="callout">This step may take about 1 to 3 minutes to complete.</p>
<ol>
<li value="16">Navigate back to the <strong class="bold">Lake Formation</strong> console (using the search box).</li>
<li>Navigate to the <strong class="bold">Tables</strong> page of the Lake Formation console to view the list of tables available in our data lake. Click the refresh button, as highlighted in the following screenshot, if you cannot see the <strong class="source-inline">unloaded</strong> table that was generated by our AWS Glue crawler:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 4.33 – Refreshing the Tables list page " height="213" src="image/B18638_04_033.jpg" width="1253"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.33 – Refreshing the Tables list page</p>
<p class="list-inset">After clicking<a id="_idIndexMarker427"/> the refresh button, you should see the <strong class="source-inline">unloaded</strong> table in the list of tables.</p>
<ol>
<li value="18">Click the <strong class="bold">unloaded</strong> link to navigate to the <strong class="bold">Table details</strong> page:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 4.34 – Table details and Schema of the unloaded table " height="628" src="image/B18638_04_034.jpg" width="1033"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.34 – Table details and Schema of the unloaded table</p>
<p class="list-inset">As shown in the preceding screenshot, we should see the <strong class="bold">Table details</strong> and <strong class="bold">Schema</strong> information.</p>
<ol>
<li value="19">Open the <strong class="bold">Actions</strong> drop-down menu and select <strong class="bold">View data</strong> from the list of options. This should open the <strong class="bold">Preview data</strong> pop-up window, informing us that we will be taken to the Athena console. Click the <strong class="bold">OK</strong> button to proceed.</li>
</ol>
<p><em class="italic">Wasn’t that easy?</em> Note <a id="_idIndexMarker428"/>that we are just scratching the surface of what <a id="_idIndexMarker429"/>we can do with <strong class="bold">AWS Glue</strong>. For more information, check out <a href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml">https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml</a>.</p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor100"/>Using Amazon Athena to query data in Amazon S3</h1>
<p><strong class="bold">Amazon Athena</strong> is a<a id="_idIndexMarker430"/> serverless<a id="_idIndexMarker431"/> query <a id="_idIndexMarker432"/>service that allows us to use SQL statements to query data from files stored in S3. With Amazon Athena, we don’t have to worry about infrastructure management and it scales automatically to handle our queries:</p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 4.35 – How Amazon Athena works " height="550" src="image/B18638_04_035.jpg" width="828"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.35 – How Amazon Athena works</p>
<p>If you were to set this up yourself, you may need to set up an EC2 instance cluster with an application <a id="_idIndexMarker433"/>such as <strong class="bold">Presto</strong>. In addition to this, you will need to manage the overall cost, security, performance, and<a id="_idIndexMarker434"/> stability <a id="_idIndexMarker435"/>of this<a id="_idIndexMarker436"/> EC2 cluster setup yourself.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor101"/>Setting up the query result location</h2>
<p>If the <strong class="bold">Before you run your first query, you need to set up a query result location in Amazon S3</strong> notification<a id="_idIndexMarker437"/> appears on the <strong class="bold">Editor</strong> page, this means that you must make a quick configuration change on the Amazon Athena <strong class="bold">Settings</strong> page so that Athena can store the query results in a specified S3 bucket location every time there’s a query. These query results are then displayed in the UI in the Athena console. </p>
<p>Follow these steps to set up the query result location where our Amazon Athena queries will be stored:</p>
<ol>
<li value="1">If you see the <strong class="bold">Before you run your first query, you need to set up a query result location in Amazon S3</strong> notification, click <strong class="bold">View settings</strong> to navigate to the <strong class="bold">Settings</strong> page. Otherwise, you may click the <strong class="bold">Settings</strong> tab, as shown in the following screenshot:</li>
</ol>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">f</p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 4.36 – Navigating to the Settings tab " height="118" src="image/B18638_04_036.jpg" width="524"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.36 – Navigating to the Settings tab</p>
<ol>
<li value="2">Click <strong class="bold">Manage</strong> located in <a id="_idIndexMarker438"/>the right-hand corner of the <strong class="bold">Query result and encryption settings</strong> pane:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 4.37 – Managing the query result and encryption settings " height="371" src="image/B18638_04_037.jpg" width="1185"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.37 – Managing the query result and encryption settings</p>
<ol>
<li value="3">Under <strong class="bold">Query result location and encryption</strong> in <strong class="bold">Manage settings</strong>, click <strong class="bold">Browse S3</strong> and locate the S3 bucket you created in this chapter. Toggle on the radio button and click the <strong class="bold">Choose</strong> button.</li>
<li>Click the <strong class="bold">Save</strong> button.</li>
</ol>
<p>Now that we have finished configuring the query result location for Amazon Athena, we can start running our queries.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor102"/>Running SQL queries using Athena</h2>
<p>With<a id="_idIndexMarker439"/> everything ready, we can start using SQL statements <a id="_idIndexMarker440"/>to query the data stored in S3. In this section, we’ll inspect our data and run a few queries to check for existing data integrity issues as well.</p>
<p>Follow these steps to query the data stored in the S3 bucket:</p>
<ol>
<li value="1">Navigate back to the <strong class="bold">Editor</strong> page by clicking the <strong class="bold">Editor</strong> tab.</li>
<li>In the <strong class="bold">Editor</strong> tab, run the following query:<pre class="source-code"><strong class="bold">SELECT * FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" limit 10;</strong></pre></li>
</ol>
<p class="list-inset">Make sure that you click the <strong class="bold">Run</strong> button, as highlighted in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 4.38 – Running the SQL query " height="412" src="image/B18638_04_038.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.38 – Running the SQL query</p>
<p class="list-inset">This should return a set of results similar to what we have in the following screenshot. Note that Amazon Athena may return a different set of results every time the same query is run. You may add an <strong class="source-inline">ORDER BY</strong> clause in the query to ensure that there is consistency in the results that are returned when using the same query:</p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 4.39 – Athena query results " height="564" src="image/B18638_04_039.jpg" width="1017"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.39 – Athena query results</p>
<p class="list-inset">Here, we can<a id="_idIndexMarker441"/> see <a id="_idIndexMarker442"/>that our query was processed in less than half a second. If we were to run the same query without the <strong class="source-inline">LIMIT</strong> clause, the run time may be more than a second.</p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">Performance tuning</strong> is outside the scope of this book, but feel free to check out <a href="https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/">https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/</a> for more information on this topic.</p>
<ol>
<li value="3">Run the following query to count the number of bookings that were not canceled:<pre class="source-code"><strong class="bold">SELECT COUNT(*) FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" WHERE is_cancelled=0;</strong></pre></li>
</ol>
<p class="list-inset">This should give us a result of <strong class="source-inline">66987</strong>, which should be the same result we got when we performed a similar Redshift Serverless query (in the <em class="italic">Running analytics at scale with Amazon Redshift Serverless</em> section). </p>
<ol>
<li value="4">Next, let’s list the bookings that were canceled by guests with at least one previous cancellation:<pre class="source-code"><strong class="bold">SELECT * </strong></pre><pre class="source-code"><strong class="bold">FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" </strong></pre><pre class="source-code"><strong class="bold">WHERE is_cancelled=1 AND previous_cancellations &gt; 0 </strong></pre><pre class="source-code"><strong class="bold">LIMIT 100;</strong></pre></li>
<li>Let’s also review the bookings that were canceled by guests where the number of days on the waiting list exceeds 50:<pre class="source-code"><strong class="bold">SELECT * </strong></pre><pre class="source-code"><strong class="bold">FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" </strong></pre><pre class="source-code"><strong class="bold">WHERE is_cancelled=1 AND days_in_waiting_list &gt; 50 </strong></pre><pre class="source-code"><strong class="bold">LIMIT 100;</strong></pre></li>
<li>Note that <a id="_idIndexMarker443"/>we<a id="_idIndexMarker444"/> can also check for <strong class="bold">data integrity issues</strong> using queries similar to the following:<pre class="source-code"><strong class="bold">SELECT booking_changes, has_booking_changes, * </strong></pre><pre class="source-code"><strong class="bold">FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" </strong></pre><pre class="source-code"><strong class="bold">WHERE </strong></pre><pre class="source-code"><strong class="bold">(booking_changes=0 AND has_booking_changes=true) </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(booking_changes&gt;0 AND has_booking_changes=false)</strong></pre><pre class="source-code"><strong class="bold">LIMIT 100;</strong></pre></li>
</ol>
<p class="list-inset">Using this query, we should be able to list the records where the <strong class="source-inline">booking_changes</strong> column value does not match the <strong class="source-inline">has_booking_changes</strong> column value.</p>
<ol>
<li value="7">On a similar note, we can find other records with data integrity concerns using the following query:<pre class="source-code"><strong class="bold">SELECT total_of_special_requests, has_special_requests, *  </strong></pre><pre class="source-code"><strong class="bold">FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" </strong></pre><pre class="source-code"><strong class="bold">WHERE </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests=0 AND has_special_requests=true) </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests&gt;0 AND has_special_requests=false)</strong></pre><pre class="source-code"><strong class="bold">LIMIT 100;</strong></pre></li>
</ol>
<p class="list-inset">With this <a id="_idIndexMarker445"/>query, we<a id="_idIndexMarker446"/> should be able to list the records where the <strong class="source-inline">total_of_special_requests</strong> column value does not match the <strong class="source-inline">has_special_requests</strong> column value.</p>
<ol>
<li value="8">We can also create a view that can be referenced by future queries:<pre class="source-code"><strong class="bold">CREATE OR REPLACE VIEW data_integrity_issues AS</strong></pre><pre class="source-code"><strong class="bold">SELECT * </strong></pre><pre class="source-code"><strong class="bold">FROM "AwsDataCatalog"."mle-ch4-db"."unloaded" </strong></pre><pre class="source-code"><strong class="bold">WHERE</strong></pre><pre class="source-code"><strong class="bold">(booking_changes=0 AND has_booking_changes=true) </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(booking_changes&gt;0 AND has_booking_changes=false)</strong></pre><pre class="source-code"><strong class="bold">OR</strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests=0 AND has_special_requests=true) </strong></pre><pre class="source-code"><strong class="bold">OR </strong></pre><pre class="source-code"><strong class="bold">(total_of_special_requests&gt;0 AND has_special_requests=false);</strong></pre></li>
</ol>
<p class="list-inset">Note that views do <em class="italic">NOT</em> contain any data – the query defined in the view runs every time the view is referenced by another query.</p>
<ol>
<li value="9">That said, let’s run a sample query that references the view we prepared in the previous step:<pre class="source-code"><strong class="bold">SELECT booking_changes, has_booking_changes, </strong></pre><pre class="source-code"><strong class="bold">total_of_special_requests, has_special_requests </strong></pre><pre class="source-code"><strong class="bold">FROM data_integrity_issues </strong></pre><pre class="source-code"><strong class="bold">LIMIT 100;</strong></pre></li>
</ol>
<p class="list-inset">This should give us the list of records where the <strong class="source-inline">total_of_special_requests</strong> column value does not match the <strong class="source-inline">has_special_requests</strong> column value, along with the records where the <strong class="source-inline">booking_changes</strong> column <a id="_idIndexMarker447"/>value<a id="_idIndexMarker448"/> does not match the <strong class="source-inline">has_booking_changes</strong> column value.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you are wondering if we can programmatically query our data<a id="_idIndexMarker449"/> in S3 using <strong class="bold">boto3</strong> (AWS SDK for Python), then the answer is <em class="italic">yes</em>. We can even generate predictions with a deployed ML model directly in SQL statements using Amazon Athena and Amazon SageMaker. For more information on this topic, check out <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, of the book <em class="italic">Machine Learning with Amazon SageMaker Cookbook</em>. You can also find a quick example of how to use Python and boto3 to run Athena and Athena ML queries here: <a href="https://bit.ly/36AiPpR">https://bit.ly/36AiPpR</a>.</p>
<p><em class="italic">Wasn’t that easy?</em> Setting<a id="_idIndexMarker450"/> up a <strong class="bold">serverless data lake</strong> on AWS is easy, so long as we use the right set of tools and services. Before continuing to the next chapter, make sure that you<a id="_idIndexMarker451"/> review<a id="_idIndexMarker452"/> and delete all the resources that you created in this chapter.</p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor103"/>Summary</h1>
<p>In this chapter, we were able to take a closer look at several AWS services that help enable serverless data management in organizations. When using <strong class="bold">serverless</strong> services, we no longer need to worry about infrastructure management, which helps us focus on what we need to do. </p>
<p>We were able to utilize <strong class="bold">Amazon Redshift Serverless</strong> to prepare a serverless data warehouse. We were also able to use <strong class="bold">AWS Lake Formation</strong>, <strong class="bold">AWS Glue</strong>, and <strong class="bold">Amazon Athena</strong> to create and query data from a serverless data lake. With these <em class="italic">serverless</em> services, we were able to load and query data in just a few minutes.</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor104"/>Further reading</h1>
<p>For more information on the topics that were covered in this chapter, feel free to check out the following resources:</p>
<ul>
<li><em class="italic">Security best practices for your VPC</em> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml</a>)</li>
<li><em class="italic">Introducing Amazon Redshift Serverless</em> (<a href="https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/">https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/</a>)</li>
<li><em class="italic">Security in AWS Lake Formation </em>(<a href="https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml">https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml</a>)</li>
</ul>
</div>
<div>
<div id="_idContainer159">
</div>
</div>
</div>
</body></html>