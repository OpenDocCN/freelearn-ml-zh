- en: Chapter 8. Bayesian Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 贝叶斯神经网络
- en: As the name suggests, artificial neural networks are statistical models built
    taking inspirations from the architecture and cognitive capabilities of biological
    brains. Neural network models typically have a layered architecture consisting
    of a large number of neurons in each layer, and neurons between different layers
    are connected. The first layer is called input layer, the last layer is called
    output layer, and the rest of the layers in the middle are called hidden layers.
    Each neuron has a state that is determined by a nonlinear function of the state
    of all neurons connected to it. Each connection has a weight that is determined
    from the training data containing a set of input and output pairs. This kind of
    layered architecture of neurons and their connections is present in the **neocortex**
    region of human brain and is considered to be responsible for higher functions
    such as sensory perception and language understanding.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，人工神经网络是受生物大脑的架构和认知能力启发的统计模型。神经网络模型通常具有分层架构，每一层包含大量神经元，不同层之间的神经元是相互连接的。第一层被称为输入层，最后一层被称为输出层，中间的其他层被称为隐藏层。每个神经元的状态由与其相连的所有神经元状态的非线性函数确定。每个连接都有一个权重，该权重由包含一组输入和输出对的训练数据确定。这种神经元及其连接的分层架构存在于人脑的**新皮层**区域，并被认为是负责诸如感官感知和语言理解等高级功能的原因。
- en: The first computational model for neural network was proposed by Warren McCulloch
    and Walter Pitts in 1943\. Around the same time, psychologist Donald Hebb created
    a hypothesis of learning based on the mechanism of excitation and adaptation of
    neurons that is known as **Hebb's rule**. The hypothesis can be summarized by
    saying *Neurons that fire together, wire together*. Although there were several
    researchers who tried to implement computational models of neural networks, it
    was Frank Rosenblatt in 1958 who first created an algorithm for pattern recognition
    using a two-layer neural network called **Perceptron**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的第一种计算模型是由沃伦·麦克洛克和沃尔特·皮茨在1943年提出的。大约在同一时间，心理学家唐纳德·赫布基于神经元的兴奋和适应机制提出了一个学习假设，即**赫布规则**。这个假设可以总结为“一起放电的神经元，一起连接”。尽管有几位研究人员试图实现神经网络计算模型，但直到1958年，弗兰克·罗森布拉特才首次使用一个两层神经网络**感知器**创建了一个用于模式识别的算法。
- en: The research and applications of neural networks had both stagnant and great
    periods of progress during 1970-2010\. Some of the landmarks in the history of
    neural networks are the invention of the **backpropagation** algorithm by Paul
    Werbos in 1975, a fast learning algorithm for learning multilayer neural networks
    (also called **deep learning networks**) by Geoffrey Hinton in 2006, and the use
    of GPGPUs to achieve greater computational power required for processing neural
    networks in the latter half of the last decade.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 1970-2010年间，神经网络的研究和应用经历了停滞和快速发展的时期。神经网络历史上的里程碑包括1975年保罗·沃伯斯发明的**反向传播**算法，这是一种用于学习多层神经网络（也称为**深度学习网络**）的快速学习算法，由杰弗里·辛顿在2006年提出，以及在上个十年后半期使用GPGPUs实现处理神经网络所需的更大计算能力。
- en: Today, neural network models and their applications have again taken a central
    stage in artificial intelligence with applications in computer vision, speech
    recognition, and natural language understanding. This is the reason this book
    has devoted one chapter specifically to this subject. The importance of Bayesian
    inference in neural network models will become clear when we go into detail in
    later sections.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，神经网络模型及其应用再次在人工智能领域占据中心舞台，应用于计算机视觉、语音识别和自然语言理解。这就是本书专门用一章来探讨这个主题的原因。当我们深入了解后续章节时，贝叶斯推理在神经网络模型中的重要性将变得清晰。
- en: Two-layer neural networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两层神经网络
- en: 'Let us look at the formal definition of a two-layer neural network. We follow
    the notations and description used by David MacKay (reference 1, 2, and 3 in the
    *References* section of this chapter). The input to the NN is given by ![Two-layer
    neural networks](img/image00529.jpeg). The input values are first multiplied by
    a set of weights to produce a weighted linear combination and then transformed
    using a nonlinear function to produce values of the state of neurons in the hidden
    layer:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看双层神经网络的正式定义。我们遵循David MacKay（本章“参考文献”部分的第1、2和3条参考文献）使用的符号和描述。NN的输入由 ![双层神经网络](img/image00529.jpeg)
    给出。输入值首先乘以一组权重，以产生加权线性组合，然后使用非线性函数转换，以产生隐藏层神经元的状态值：
- en: '![Two-layer neural networks](img/image00530.jpeg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00530.jpeg)'
- en: 'A similar operation is done at the second layer to produce final output values
    ![Two-layer neural networks](img/image00531.jpeg):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二层执行类似的操作以产生最终的输出值 ![双层神经网络](img/image00531.jpeg)：
- en: '![Two-layer neural networks](img/image00532.jpeg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00532.jpeg)'
- en: 'The function ![Two-layer neural networks](img/image00533.jpeg) is usually taken
    as either a **sigmoid** function ![Two-layer neural networks](img/image00534.jpeg)
    or ![Two-layer neural networks](img/image00535.jpeg). Another common function
    used for multiclass classification is **softmax** defined as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 ![双层神经网络](img/image00533.jpeg) 通常取为**sigmoid**函数 ![双层神经网络](img/image00534.jpeg)
    或 ![双层神经网络](img/image00535.jpeg)。用于多类分类的另一个常见函数是**softmax**，其定义如下：
- en: '![Two-layer neural networks](img/image00536.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00536.jpeg)'
- en: This is a normalized exponential function.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个归一化的指数函数。
- en: All these are highly nonlinear functions exhibiting the property that the output
    value has a sharp increase as a function of the input. This nonlinear property
    gives neural networks more computational flexibility than standard linear or generalized
    linear models. Here, ![Two-layer neural networks](img/image00537.jpeg) is called
    a bias parameter. The weights ![Two-layer neural networks](img/image00538.jpeg)
    together with biases ![Two-layer neural networks](img/image00539.jpeg) form the
    weight vector **w** .
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是高度非线性的函数，具有输出值随着输入值急剧增加的性质。这种非线性特性使得神经网络比标准的线性或广义线性模型具有更多的计算灵活性。在这里，![双层神经网络](img/image00537.jpeg)被称为偏置参数。权重
    ![双层神经网络](img/image00538.jpeg) 与偏置 ![双层神经网络](img/image00539.jpeg) 一起形成权重向量**w**。
- en: 'The schematic structure of the two-layer neural network is shown here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 双层神经网络的示意图结构如下所示：
- en: '![Two-layer neural networks](img/image00540.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00540.jpeg)'
- en: 'The learning in neural networks corresponds to finding the value of weight
    vector such as **w**, such that for a given dataset consisting of ground truth
    values input and target (output), ![Two-layer neural networks](img/image00541.jpeg),
    the error of prediction of target values by the network is minimum. For regression
    problems, this is achieved by minimizing the error function:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的学习对应于寻找权重向量如**w**的值，使得对于给定的由真实值输入和目标（输出）组成的训练数据集 ![双层神经网络](img/image00541.jpeg)，网络预测目标值的误差最小。对于回归问题，这是通过最小化误差函数来实现的：
- en: '![Two-layer neural networks](img/image00542.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00542.jpeg)'
- en: 'For the classification task, in neural network training, instead of squared
    error one uses a cross entropy defined as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，在神经网络训练中，人们使用交叉熵而不是平方误差，其定义如下：
- en: '![Two-layer neural networks](img/image00543.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00543.jpeg)'
- en: 'To avoid overfitting, a regularization term is usually also included in the
    objective function. The form of the regularization function is usually ![Two-layer
    neural networks](img/image00544.jpeg), which gives penalty to large values of
    **w**, reducing the chances of overfitting. The resulting objective function is
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，通常也在目标函数中包含一个正则化项。正则化函数的形式通常是 ![双层神经网络](img/image00544.jpeg)，它对大的**w**值给予惩罚，从而降低过拟合的可能性。结果的目标函数如下：
- en: '![Two-layer neural networks](img/image00545.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![双层神经网络](img/image00545.jpeg)'
- en: Here, ![Two-layer neural networks](img/image00546.jpeg) and ![Two-layer neural
    networks](img/image00547.jpeg) are free parameters for which the optimum values
    can be found from cross-validation experiments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![双层神经网络](img/image00546.jpeg) 和 ![双层神经网络](img/image00547.jpeg) 是自由参数，其最佳值可以通过交叉验证实验找到。
- en: To minimize *M(***w***)* with respect to **w**, one uses the backpropagation
    algorithm as described in the classic paper by Rumelhart, Hinton, and Williams
    (reference 3 in the *References* section of this chapter). In the backpropagation
    for each input/output pair, the value of the predicted output is computed using
    a forward pass from the input layer. The error, or the difference between the
    predicted output and actual output, is propagated back and at each node, the weights
    are readjusted so that the error is a minimum.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要最小化关于 **w** 的 *M(***w***)*，可以使用Rumelhart、Hinton和Williams的经典论文中描述的逆传播算法（本章“参考文献”部分的第3个参考文献）。在逆传播的每个输入/输出对中，预测输出的值通过从输入层的前向传递来计算。误差，即预测输出和实际输出之间的差异，被反向传播，并且在每个节点，权重都被重新调整，以便误差最小。
- en: Bayesian treatment of neural networks
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络处理
- en: 'To set the neural network learning in a Bayesian context, consider the error
    function ![Bayesian treatment of neural networks](img/image00548.jpeg) for the
    regression case. It can be treated as a Gaussian noise term for observing the
    given dataset conditioned on the weights **w**. This is precisely the likelihood
    function that can be written as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将神经网络学习置于贝叶斯框架中，考虑回归案例中的误差函数 ![贝叶斯神经网络处理](img/image00548.jpeg)。它可以被视为观察给定数据集的条件权重
    **w** 的高斯噪声项。这正是可以写成以下形式的似然函数：
- en: '![Bayesian treatment of neural networks](img/image00549.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯神经网络处理](img/image00549.jpeg)'
- en: 'Here, ![Bayesian treatment of neural networks](img/image00547.jpeg) is the
    variance of the noise term given by ![Bayesian treatment of neural networks](img/image00550.jpeg)
    and ![Bayesian treatment of neural networks](img/image00551.jpeg)represents a
    probabilistic model. The regularization term can be considered as the log of the
    prior probability distribution over the parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![贝叶斯神经网络处理](img/image00547.jpeg) 是由 ![贝叶斯神经网络处理](img/image00550.jpeg) 给出的噪声项的方差，而
    ![贝叶斯神经网络处理](img/image00551.jpeg) 代表一个概率模型。正则化项可以被认为是参数先验概率分布的对数：
- en: '![Bayesian treatment of neural networks](img/image00552.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯神经网络处理](img/image00552.jpeg)'
- en: 'Here, ![Bayesian treatment of neural networks](img/image00553.jpeg) is the
    variance of the prior distribution of weights. It can be easily shown using Bayes''
    theorem that the objective function *M(***w***)* then corresponds to the posterior
    distribution of parameters **w**:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![贝叶斯神经网络处理](img/image00553.jpeg) 是权重先验分布的方差。可以使用贝叶斯定理轻松证明，目标函数 *M(***w***)*
    然后对应于参数 **w** 的后验分布：
- en: '![Bayesian treatment of neural networks](img/image00554.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯神经网络处理](img/image00554.jpeg)'
- en: 'In the neural network case, we are interested in the local maxima of ![Bayesian
    treatment of neural networks](img/image00555.jpeg). The posterior is then approximated
    as a Gaussian around each maxima ![Bayesian treatment of neural networks](img/image00556.jpeg),
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的情况下，我们感兴趣的是 ![贝叶斯神经网络处理](img/image00555.jpeg) 的局部极大值。然后，后验被近似为围绕每个极大值
    ![贝叶斯神经网络处理](img/image00556.jpeg) 的高斯分布，如下所示：
- en: '![Bayesian treatment of neural networks](img/image00557.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯神经网络处理](img/image00557.jpeg)'
- en: Here, *A* is a matrix of the second derivative of *M(***w***)* with respect
    to **w** and represents an inverse of the covariance matrix. It is also known
    by the name **Hessian** matrix.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*A* 是 *M(***w***)* 关于 **w** 的二阶导数的矩阵，它代表协方差矩阵的逆。它也被称为**海森矩阵**。
- en: 'The value of hyper parameters ![Bayesian treatment of neural networks](img/image00546.jpeg)
    and ![Bayesian treatment of neural networks](img/image00547.jpeg) is found using
    the **evidence framework**. In this, the probability ![Bayesian treatment of neural
    networks](img/image00558.jpeg) is used as a evidence to find the best values of
    ![Bayesian treatment of neural networks](img/image00546.jpeg) and ![Bayesian treatment
    of neural networks](img/image00547.jpeg) from data *D*. This is done through the
    following Bayesian rule:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数 ![贝叶斯神经网络处理](img/image00546.jpeg) 和 ![贝叶斯神经网络处理](img/image00547.jpeg) 的值是通过**证据框架**找到的。在这里，概率
    ![贝叶斯神经网络处理](img/image00558.jpeg) 被用作证据，以从数据 *D* 中找到 ![贝叶斯神经网络处理](img/image00546.jpeg)
    和 ![贝叶斯神经网络处理](img/image00547.jpeg) 的最佳值。这是通过以下贝叶斯规则完成的：
- en: '![Bayesian treatment of neural networks](img/image00559.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯神经网络处理](img/image00559.jpeg)'
- en: 'By using the evidence framework and Gaussian approximation of posterior (references
    2 and 5 in the *References* section of this chapter), one can show that the best
    value of ![Bayesian treatment of neural networks](img/image00560.jpeg) satisfies
    the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用证据框架和后验的高斯近似（本章“参考文献”部分的第2和第5条参考文献），可以证明![神经网络贝叶斯处理](img/image00560.jpeg)的最佳值满足以下条件：
- en: '![Bayesian treatment of neural networks](img/image00561.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络贝叶斯处理](img/image00561.jpeg)'
- en: 'Also, the best value of ![Bayesian treatment of neural networks](img/image00562.jpeg)
    satisfies the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，![神经网络贝叶斯处理](img/image00562.jpeg)的最佳值也满足以下条件：
- en: '![Bayesian treatment of neural networks](img/image00563.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络贝叶斯处理](img/image00563.jpeg)'
- en: In these equations, ![Bayesian treatment of neural networks](img/image00564.jpeg)
    is the number of well-determined parameters given by ![Bayesian treatment of neural
    networks](img/image00565.jpeg) where *k* is the length of **w**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方程中，![神经网络贝叶斯处理](img/image00564.jpeg)是![神经网络贝叶斯处理](img/image00565.jpeg)给出的确定参数的数量，其中*k*是**w**的长度。
- en: The brnn R package
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: brnn R包
- en: 'The **brnn** package was developed by Paulino Perez Rodriguez and Daniel Gianola,
    and it implements the two-layer Bayesian regularized neural network described
    in the previous section. The main function in the package is `brnn( )` that can
    be called using the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**brnn**包是由Paulino Perez Rodriguez和Daniel Gianola开发的，它实现了上一节中描述的两层贝叶斯正则化神经网络。包中的主要函数是`brnn(
    )`，可以通过以下命令调用：'
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, *x* is an *n x p* matrix where *n* is the number of data points and *p*
    is the number of variables; *y* is an *n* dimensional vector containing target
    values. The number of neurons in the hidden layer of the network can be specified
    by the variable `neurons`. If the indicator function `normalize` is `TRUE`, it
    will normalize the input and output, which is the default option. The maximum
    number of iterations during model training is specified using `epochs`. If the
    indicator binary variable `Monte_Carlo` is true, then an MCMC method is used to
    estimate the trace of the inverse of the Hessian matrix A.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*是一个*n x p*矩阵，其中*n*是数据点的数量，*p*是变量的数量；*y*是一个包含目标值的*n*维向量。网络隐藏层中的神经元数量可以通过变量`neurons`指定。如果指示函数`normalize`为`TRUE`，则将归一化输入和输出，这是默认选项。模型训练期间的最大迭代次数由`epochs`指定。如果指示二进制变量`Monte_Carlo`为真，则使用MCMC方法估计Hessian矩阵逆的迹。
- en: 'Let us try an example with the Auto MPG dataset that we used in [Chapter 5](part0041.xhtml#aid-173721
    "Chapter 5. Bayesian Regression Models"), *Bayesian Regression Models*. The following
    R code will import data, create training and test sets, train a neural network
    model using training data, and make predictions for the test set:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个例子，使用我们在[第5章](part0041.xhtml#aid-173721 "第5章。贝叶斯回归模型")中使用的Auto MPG数据集，*贝叶斯回归模型*。下面的R代码将导入数据，创建训练集和测试集，使用训练数据训练神经网络模型，并对测试集进行预测：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Deep belief networks and deep learning
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度信念网络和深度学习
- en: 'Some of the pioneering advancements in neural networks research in the last
    decade have opened up a new frontier in machine learning that is generally called
    by the name **deep learning** (references 5 and 7 in the *References* section
    of this chapter). The general definition of deep learning is, *a class of machine
    learning techniques, where many layers of information processing stages in hierarchical
    supervised architectures are exploited for unsupervised feature learning and for
    pattern analysis/classification. The essence of deep learning is to compute hierarchical
    features or representations of the observational data, where the higher-level
    features or factors are defined from lower-level ones* (reference 8 in the *References*
    section of this chapter). Although there are many similar definitions and architectures
    for deep learning, two common elements in all of them are: *multiple layers of
    nonlinear information processing* and *supervised or unsupervised learning of
    feature representations at each layer from the features learned at the previous
    layer*. The initial works on deep learning were based on multilayer neural network
    models. Recently, many other forms of models have also been used, such as deep
    kernel machines and deep Q-networks.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，神经网络研究的一些开创性进展为机器学习开辟了一个新的前沿，通常被称为**深度学习**（本章“参考文献”部分的第5和第7条参考文献）。深度学习的一般定义是，*一类机器学习技术，其中在层次监督架构中利用许多信息处理阶段，用于无监督特征学习和模式分析/分类。深度学习的本质是计算观测数据的层次特征或表示，其中高级特征或因素由低级特征定义*（本章“参考文献”部分的第8条参考文献）。尽管深度学习有许多类似定义和架构，但所有这些定义中都包含两个共同元素：*多层非线性信息处理*和*在每个层次上从上一层次学习到的特征中进行监督或无监督学习特征表示*。深度学习的最初工作基于多层神经网络模型。最近，还使用了许多其他形式的模型，例如深度核机和深度Q网络。
- en: Even in previous decades, researchers have experimented with multilayer neural
    networks. However, two reasons limited any progress with learning using such architectures.
    The first reason is that the learning of the network parameters is a non-convex
    optimization problem. Starting from random initial conditions, one gets stuck
    at local minima during minimization of error. The second reason is that the associated
    computational requirements were huge. A breakthrough for the first problem came
    when Geoffrey Hinton developed a fast algorithm for learning a special class of
    neural networks called **deep belief nets** (**DBN**). We will describe DBNs in
    more detail in later sections. The high computational power requirements were
    met with the advancement in computing using **general purpose graphical processing
    units** (**GPGPUs**). What made deep learning so popular for practical applications
    is the significant improvement in accuracy achieved in automatic speech recognition
    and computer vision. For example, the **word error rate** in automatic speech
    recognition of a switchboard conversational speech had reached a saturation of
    around 40% after years of research.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在几十年前，研究人员也尝试过多层神经网络。然而，有两个原因限制了使用这种架构进行学习的任何进展。第一个原因是网络参数的学习是一个非凸优化问题。从随机初始条件开始，在最小化误差的过程中会陷入局部最小值。第二个原因是相关的计算需求巨大。对于第一个问题的突破发生在Geoffrey
    Hinton开发了一种快速算法来学习一种称为**深度信念网络**（**DBN**）的特殊类别的神经网络时。我们将在后面的章节中更详细地描述DBN。通过使用**通用图形处理单元**（**GPGPU**）的计算能力的提升，满足了高计算能力需求。深度学习之所以在实用应用中如此受欢迎，是因为在自动语音识别和计算机视觉中实现了显著的准确性提升。例如，自动语音识别的**词错误率**在经过多年的研究后，达到了大约40%的饱和状态。
- en: However, using deep learning, the word error rate reduced dramatically to close
    to 10% in a matter of a few years. Another well-known example is how **deep convolution
    neural network** achieved the least error rate of 15.3% in the 2012 ImageNet Large
    Scale Visual Recognition Challenge compared to state-of-the-art methods that gave
    26.2% as the least error rate (reference 7 in the *References* section of this
    chapter).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用深度学习，词错误率在短短几年内大幅降低，接近10%。另一个众所周知的例子是**深度卷积神经网络**在2012年ImageNet大规模视觉识别挑战赛中的表现，其最低错误率为15.3%，而当时最先进的方法的最低错误率为26.2%（本章“参考文献”部分的第7条参考文献）。
- en: In this chapter, we will describe one class of deep learning models called deep
    belief networks. Interested readers may wish to read the book by Li Deng and Dong
    Yu (reference 9 in the *References* section of this chapter) for a detailed understanding
    of various methods and applications of deep learning. We will follow their notations
    in the rest of the chapter. We will also illustrate the use of DBN with the R
    package **darch**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述一类深度学习模型，称为深度信念网络。感兴趣的读者可能希望阅读李登和董宇所著的书籍（本章“参考文献”部分的第9条参考文献），以详细了解深度学习的各种方法和应用。我们将在本章的其余部分遵循他们的符号。我们还将使用R包**darch**来展示DBN的使用。
- en: Restricted Boltzmann machines
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: 'A **restricted Boltzmann machine** (**RBM**) is a two-layer network (bi-partite
    graph), in which one layer is a visible layer (*v*) and the second layer is a
    hidden layer (*h*). All nodes in the visible layer and all nodes in the hidden
    layer are connected by undirected edges, and there no connections between nodes
    in the same layer:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**受限玻尔兹曼机**（**RBM**）是一个两层网络（二部图），其中一层是可见层（*v*），另一层是隐藏层（*h*）。可见层中的所有节点和隐藏层中的所有节点通过无向边连接，同一层中的节点之间没有连接：'
- en: '![Restricted Boltzmann machines](img/image00566.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00566.jpeg)'
- en: 'An RBM is characterized by the joint distribution of states of all visible
    units ![Restricted Boltzmann machines](img/image00567.jpeg) and states of all
    hidden units ![Restricted Boltzmann machines](img/image00568.jpeg) given by:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RBM的特征是所有可见单元![受限玻尔兹曼机](img/image00567.jpeg)和所有隐藏单元![受限玻尔兹曼机](img/image00568.jpeg)的状态的联合分布。
- en: '![Restricted Boltzmann machines](img/image00569.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00569.jpeg)'
- en: Here, ![Restricted Boltzmann machines](img/image00570.jpeg) is called the **energy
    function** and ![Restricted Boltzmann machines](img/image00571.jpeg) is the normalization
    constant known by the name **partition function** from Statistical Physics nomenclature.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![受限玻尔兹曼机](img/image00570.jpeg)被称为**能量函数**，而![受限玻尔兹曼机](img/image00571.jpeg)是名为**配分函数**的归一化常数，来自统计物理学的术语。
- en: 'There are mainly two types of RBMs. In the first one, both *v* and *h* are
    Bernoulli random variables. In the second type, *h* is a Bernoulli random variable
    whereas *v* is a Gaussian random variable. For Bernoulli RBM, the energy function
    is given by:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: RBM主要有两种类型。在第一种类型中，*v*和*h*都是伯努利随机变量。在第二种类型中，*h*是伯努利随机变量，而*v*是高斯随机变量。对于伯努利RBM，能量函数如下所示：
- en: '![Restricted Boltzmann machines](img/image00572.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00572.jpeg)'
- en: 'Here, ![Restricted Boltzmann machines](img/image00573.jpeg) represents the
    weight of the edge between nodes ![Restricted Boltzmann machines](img/image00574.jpeg)
    and ![Restricted Boltzmann machines](img/image00575.jpeg); ![Restricted Boltzmann
    machines](img/image00576.jpeg) and ![Restricted Boltzmann machines](img/image00577.jpeg)
    are bias parameters for the visible and hidden layers respectively. For this energy
    function, the exact expressions for the conditional probability can be derived
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![受限玻尔兹曼机](img/image00573.jpeg)代表节点![受限玻尔兹曼机](img/image00574.jpeg)和![受限玻尔兹曼机](img/image00575.jpeg)之间边的权重；![受限玻尔兹曼机](img/image00576.jpeg)和![受限玻尔兹曼机](img/image00577.jpeg)分别是可见层和隐藏层的偏置参数。对于这个能量函数，条件概率的精确表达式可以推导如下：
- en: '![Restricted Boltzmann machines](img/image00578.jpeg)![Restricted Boltzmann
    machines](img/image00579.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00578.jpeg)![受限玻尔兹曼机](img/image00579.jpeg)'
- en: Here, ![Restricted Boltzmann machines](img/image00580.jpeg) is the logistic
    function ![Restricted Boltzmann machines](img/image00581.jpeg).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![受限玻尔兹曼机](img/image00580.jpeg)是逻辑函数![受限玻尔兹曼机](img/image00581.jpeg)。
- en: 'If the input variables are continuous, one can use the Gaussian RBM; the energy
    function of it is given by:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入变量是连续的，可以使用高斯RBM；其能量函数如下所示：
- en: '![Restricted Boltzmann machines](img/image00582.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00582.jpeg)'
- en: 'Also, in this case, the conditional probabilities of ![Restricted Boltzmann
    machines](img/image00574.jpeg) and ![Restricted Boltzmann machines](img/image00575.jpeg)
    will become as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，![受限玻尔兹曼机](img/image00574.jpeg)和![受限玻尔兹曼机](img/image00575.jpeg)的条件概率将变为以下形式：
- en: '![Restricted Boltzmann machines](img/image00578.jpeg)![Restricted Boltzmann
    machines](img/image00583.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00578.jpeg)![受限玻尔兹曼机](img/image00583.jpeg)'
- en: This is a normal distribution with mean ![Restricted Boltzmann machines](img/image00584.jpeg)
    and variance 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个均值为![受限玻尔兹曼机](img/image00584.jpeg)和方差为1的正态分布。
- en: 'Now that we have described the basic architecture of an RBM, how is it that
    it is trained? If we try to use the standard approach of taking the gradient of
    log-likelihood, we get the following update rule:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了RBM的基本架构，那么它是如何被训练的呢？如果我们尝试使用标准方法，即对对数似然函数求梯度，我们得到以下更新规则：
- en: '![Restricted Boltzmann machines](img/image00585.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/image00585.jpeg)'
- en: Here, ![Restricted Boltzmann machines](img/image00586.jpeg) is the expectation
    of ![Restricted Boltzmann machines](img/image00587.jpeg) computed using the dataset
    and ![Restricted Boltzmann machines](img/image00588.jpeg) is the same expectation
    computed using the model. However, one cannot use this exact expression for updating
    weights because ![Restricted Boltzmann machines](img/image00588.jpeg) is difficult
    to compute.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![受限玻尔兹曼机](img/image00586.jpeg)是使用数据集计算得到的![受限玻尔兹曼机](img/image00587.jpeg)的期望，而![受限玻尔兹曼机](img/image00588.jpeg)是使用模型计算得到的相同期望。然而，由于![受限玻尔兹曼机](img/image00588.jpeg)难以计算，因此不能使用这个精确的表达式来更新权重。
- en: The first breakthrough came to solve this problem and, hence, to train deep
    neural networks, when Hinton and team proposed an algorithm called **Contrastive
    Divergence** (**CD**) (reference 7 in the *References* section of this chapter).
    The essence of the algorithm is described in the next paragraph.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个突破是在解决这个问题的同时，训练深度神经网络时，当Hinton及其团队提出了一种名为**对比散度**（**CD**）的算法（本章“参考文献”部分的第7条）时出现的。算法的精髓将在下一段中描述。
- en: 'The idea is to approximate ![Restricted Boltzmann machines](img/image00588.jpeg)
    by using values of ![Restricted Boltzmann machines](img/image00574.jpeg) and ![Restricted
    Boltzmann machines](img/image00575.jpeg) generated using Gibbs sampling from the
    conditional distributions mentioned previously. One scheme of doing this is as
    follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是使用Gibbs采样从之前提到的条件分布中生成的![受限玻尔兹曼机](img/image00574.jpeg)和![受限玻尔兹曼机](img/image00575.jpeg)的值来近似![受限玻尔兹曼机](img/image00588.jpeg)。实现这一策略的一种方案如下：
- en: Initialize ![Restricted Boltzmann machines](img/image00589.jpeg) from the dataset.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中初始化![受限玻尔兹曼机](img/image00589.jpeg)。
- en: Find ![Restricted Boltzmann machines](img/image00590.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00591.jpeg).
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从条件分布![受限玻尔兹曼机](img/image00591.jpeg)中采样来找到![受限玻尔兹曼机](img/image00590.jpeg)。
- en: Find ![Restricted Boltzmann machines](img/image00592.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00593.jpeg).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从条件分布![受限玻尔兹曼机](img/image00593.jpeg)中采样来找到![受限玻尔兹曼机](img/image00592.jpeg)。
- en: Find ![Restricted Boltzmann machines](img/image00594.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00595.jpeg).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从条件分布![受限玻尔兹曼机](img/image00595.jpeg)中采样来找到![受限玻尔兹曼机](img/image00594.jpeg)。
- en: Once we find the values of ![Restricted Boltzmann machines](img/image00592.jpeg)
    and ![Restricted Boltzmann machines](img/image00594.jpeg), use ![Restricted Boltzmann
    machines](img/image00596.jpeg), which is the product of *i*^(th) component of
    ![Restricted Boltzmann machines](img/image00592.jpeg) and *j*^(th) component of
    ![Restricted Boltzmann machines](img/image00594.jpeg), as an approximation for
    ![Restricted Boltzmann machines](img/image00588.jpeg). This is called **CD-1 algorithm**.
    One can generalize this to use the values from the *k*^(th) step of Gibbs sampling
    and it is known as **CD-k algorithm**. One can easily see the connection between
    RBMs and Bayesian inference. Since the CD algorithm is like a posterior density
    estimate, one could say that RBMs are trained using a Bayesian inference approach.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了![受限玻尔兹曼机](img/image00592.jpeg)和![受限玻尔兹曼机](img/image00594.jpeg)的值，可以使用![受限玻尔兹曼机](img/image00596.jpeg)，即![受限玻尔兹曼机](img/image00592.jpeg)的第*i*个分量与![受限玻尔兹曼机](img/image00594.jpeg)的第*j*个分量的乘积，作为![受限玻尔兹曼机](img/image00588.jpeg)的近似。这被称为**CD-1算法**。可以将此推广到使用Gibbs采样的*k*步的值，这被称为**CD-k算法**。可以很容易地看到RBM与贝叶斯推理之间的联系。由于CD算法类似于后验密度估计，可以说RBM是使用贝叶斯推理方法进行训练的。
- en: Although the Contrastive Divergence algorithm looks simple, one needs to be
    very careful in training RBMs, otherwise the model can result in overfitting.
    Readers who are interested in using RBMs in practical applications should refer
    to the technical report (reference 10 in the *References* section of this chapter),
    where this is discussed in detail.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对比散度算法看起来很简单，但在训练RBMs时需要非常小心，否则模型可能导致过拟合。对在实用应用中使用RBMs感兴趣的读者应参考本章*参考文献*部分的第10条参考文献中的技术报告，其中对此进行了详细讨论。
- en: Deep belief networks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: 'One can stack several RBMs, one on top of each other, such that the values
    of hidden units in the layer ![Deep belief networks](img/image00597.jpeg) would
    become values of visible units in the *n*^(th) layer ![Deep belief networks](img/image00598.jpeg),
    and so on. The resulting network is called a deep belief network. It was one of
    the main architectures used in early deep learning networks for pretraining. The
    idea of pretraining a NN is the following: in the standard three-layer (input-hidden-output)
    NN, one can start with random initial values for the weights and using the backpropagation
    algorithm, can find a good minimum of the log-likelihood function. However, when
    the number of layers increases, the straightforward application of backpropagation
    does not work because starting from output layer, as we compute the gradient values
    for the layers deep inside, their magnitude becomes very small. This is called
    the **gradient vanishing** problem. As a result, the network will get trapped
    in some poor local minima. Backpropagation still works if we are starting from
    the neighborhood of a good minimum. To achieve this, a DNN is often pretrained
    in an unsupervised way, using a DBN. Instead of starting from random values of
    weights, train a DBN in an unsupervised way and use weights from the DBN as initial
    weights for a corresponding supervised DNN. It was seen that such DNNs pretrained
    using DBNs perform much better (reference 8 in the *References* section of this
    chapter).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可以堆叠多个RBMs，一个叠在另一个上面，使得层![深度信念网络](img/image00597.jpeg)中隐藏单元的值成为第*n*层![深度信念网络](img/image00598.jpeg)中可见单元的值，依此类推。所得到的网络称为深度信念网络。它是早期深度学习网络预训练中使用的几种主要架构之一。预训练神经网络的想法如下：在标准的三层（输入-隐藏-输出）神经网络中，可以从权重的随机初始值开始，使用反向传播算法找到对数似然函数的良好最小值。然而，当层数增加时，直接应用反向传播算法不起作用，因为从输出层开始，当我们计算深层层的梯度值时，它们的幅度变得非常小。这被称为**梯度消失**问题。因此，网络将陷入某些较差的局部最小值。如果我们从良好最小值附近开始，反向传播仍然有效。为了实现这一点，DNN通常以无监督的方式预训练，使用DBN。不是从权重的随机值开始，而是以无监督的方式训练DBN，并使用DBN的权重作为相应监督DNN的初始权重。观察到使用DBN预训练的DNN表现更好（本章*参考文献*部分的第8条参考文献）。
- en: The layer-wise pretraining of a DBN proceeds as follows. Start with the first
    RBM and train it using input data in the visible layer and the CD algorithm (or
    its latest better variants). Then, stack a second RBM on top of this. For this
    RBM, use values sample from ![Deep belief networks](img/image00599.jpeg) as the
    values for the visible layer. Continue this process for the desired number of
    layers. The outputs of hidden units from the top layer can also be used as inputs
    for training a supervised model. For this, add a conventional NN layer at the
    top of DBN with the desired number of classes as the number of output nodes. Input
    for this NN would be the output from the top layer of DBN. This is called **DBN-DNN
    architecture**. Here, a DBN's role is generating highly efficient features (the
    output of the top layer of DBN) automatically from the input data for the supervised
    NN in the top layer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DBN的逐层预训练过程如下。从第一个RBM开始，使用可见层中的输入数据和使用CD算法（或其最新的更好变体）对其进行训练。然后，在这个RBM的上面堆叠第二个RBM。对于这个RBM，使用![深度信念网络](img/image00599.jpeg)中采样的值作为可见层的值。继续这个过程，直到达到所需的层数。顶层隐藏单元的输出也可以用作训练监督模型的输入。为此，在DBN的顶部添加一个具有所需类别数作为输出节点数的传统神经网络层。这个NN的输入将是DBN顶层的输出。这被称为**DBN-DNN架构**。在这里，DBN的作用是从输入数据自动生成高度有效的特征（DBN顶层输出）供顶层监督NN使用。
- en: 'The architecture of a five-layer DBN-DNN for a binary classification task is
    shown in the following figure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于二分类任务的五层DBN-DNN架构如图所示：
- en: '![Deep belief networks](img/image00600.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![深度信念网络](img/image00600.jpeg)'
- en: The last layer is trained using the backpropagation algorithm in a supervised
    manner for the two classes ![Deep belief networks](img/image00601.jpeg) and ![Deep
    belief networks](img/image00602.jpeg). We will illustrate the training and classification
    with such a DBN-DNN using the darch R package.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个层使用监督方式使用反向传播算法对两个类别![深度信念网络](img/image00601.jpeg)和![深度信念网络](img/image00602.jpeg)进行训练。我们将使用darch
    R包通过这样的DBN-DNN来展示训练和分类。
- en: The darch R package
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: darch R包
- en: The darch package, written by Martin Drees, is one of the R packages using which
    one can begin doing deep learning in R. It implements the DBN described in the
    previous section (references 5 and 7 in the *References* section of this chapter).
    The package can be downloaded from [https://cran.r-project.org/web/packages/darch/index.html](https://cran.r-project.org/web/packages/darch/index.html).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由Martin Drees编写的darch包是R包之一，使用它可以开始使用R进行深度学习。它实现了上一节中描述的DBN（参考文献5和7在本章“参考文献”部分的引用）。该包可以从[https://cran.r-project.org/web/packages/darch/index.html](https://cran.r-project.org/web/packages/darch/index.html)下载。
- en: 'The main class in the darch package implements deep architectures and provides
    the ability to train them with Contrastive Divergence and fine-tune with backpropagation,
    resilient backpropagation, and conjugate gradients. The new instances of the class
    are created with the `newDArch` constructor. It is called with the following arguments:
    a vector containing the number of nodes in each layers, the batch size, a Boolean
    variable to indicate whether to use the **ff** package for computing weights and
    outputs, and the name of the function for generating the weight matrices. Let
    us create a network having two input units, four hidden units, and one output
    unit:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: darch包中的主要类实现了深度架构，并提供了使用对比散度训练以及使用反向传播、弹性反向传播和共轭梯度微调进行微调的能力。该类的新的实例是通过`newDArch`构造函数创建的。它使用以下参数调用：一个包含每层节点数的向量，批量大小，一个布尔变量，用于指示是否使用**ff**包来计算权重和输出，以及生成权重矩阵的函数名称。让我们创建一个具有两个输入单元、四个隐藏单元和一个输出单元的网络：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us train the DBN with a toy dataset. We are using this because for training
    any realistic examples, it would take a long time: hours, if not days. Let us
    create an input data set containing two columns and four rows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个玩具数据集来训练DBN。我们之所以这样做，是因为训练任何真实示例将花费很长时间：如果不是几天，可能就是几个小时。让我们创建一个包含两列和四行的输入数据集：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let us pretrain the DBN, using the input data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用输入数据对DBN进行预训练：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can have a look at the weights learned at any layer using the `getLayerWeights(
    )` function. Let us see how the hidden layer looks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`getLayerWeights( )`函数查看任何层的权重。让我们看看隐藏层看起来如何：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let''s do a backpropagation for supervised learning. For this, we need
    to first set the layer functions to `sigmoidUnitDerivatives`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为监督学习进行一次反向传播。为此，我们首先需要将层函数设置为`sigmoidUnitDerivatives`：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, the following two lines perform the backpropagation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下两行执行反向传播：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see the prediction quality of DBN on the training data itself by running
    `darch` as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`darch`来查看DBN在训练数据本身上的预测质量，如下所示：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Comparing with the actual output, DBN has predicted the wrong output for the
    first and second input rows. Since this example was just to illustrate how to
    use the darch package, we are not worried about the 50% accuracy here.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与实际输出相比，DBN对第一行和第二行输入的输出预测是错误的。由于这个例子只是为了说明如何使用darch包，所以我们在这里并不担心50%的准确率。
- en: Other deep learning packages in R
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R中的其他深度学习包
- en: Although there are other deep learning packages in R, such as **deepnet** and
    **RcppDL**, compared with libraries in other languages such as **Cuda** (C++)
    and **Theano** (Python), R yet does not have good native libraries for deep learning.
    The only available package is a wrapper for the Java-based deep learning open
    source project H2O. This R package, **h2o**, allows running H2O via its REST API
    from within R. Readers who are interested in serious deep learning projects and
    applications should use H2O using h2o packages in R. One needs to install H2O
    in your machine to use h2o. We will cover H2O in the next chapter when we discuss
    Big Data and the distributed computing platform called Spark.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the Auto MPG dataset, compare the performance of predictive models using
    ordinary regression, Bayesian GLM, and Bayesian neural networks.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MacKay D. J. C. *Information Theory, Inference and Learning Algorithms*. Cambridge
    University Press. 2003\. ISBN-10: 0521642981'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MacKayD. J. C. "The Evidence Framework Applied to Classification Networks".
    Neural Computation. Volume 4(3), 698-714\. 1992
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MacKay D. J. C. "Probable Networks and Plausible Predictions – a review of
    practical Bayesian methods for supervised neural networks". Network: Computation
    in neural systems'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Rumelhart D. E., and Williams R. J. "Learning Representations
    by Back Propagating Errors". Nature. Volume 323, 533-536\. 1986
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MacKay D. J. C. "Bayesian Interpolation". Neural Computation. Volume 4(3), 415-447\.
    1992
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Krizhevsky A., and Sutskever I. "ImageNet Classification with
    Deep Convolutional Neural Networks". Advances In Neural Information Processing
    Systems (NIPS). 2012
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G., Osindero S., and Teh Y. "A Fast Learning Algorithm for Deep Belief
    Nets". Neural Computation. 18:1527–1554\. 2006
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. and Salakhutdinov R. "Reducing the Dimensionality of Data with Neural
    Networks". Science. 313(5786):504–507\. 2006
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Li Deng and Dong Yu. *Deep Learning: Methods and Applications (Foundations
    and Trends(r) in Signal Processing)*. Now Publishers Inc. Vol 7, Issue 3-4\. 2014\.
    ISBN-13: 978-1601988140'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. "A Practical Guide to Training Restricted Boltzmann Machines". UTML
    Tech Report 2010-003\. Univ. Toronto. 2010
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about an important class of machine learning model,
    namely neural networks, and their Bayesian implementation. These models are inspired
    by the architecture of the human brain and they continue to be an area of active
    research and development. We also learned one of the latest advances in neural
    networks that is called deep learning. It can be used to solve many problems such
    as computer vision and natural language processing that involves highly cognitive
    elements. The artificial intelligent systems using deep learning were able to
    achieve accuracies comparable to human intelligence in tasks such as speech recognition
    and image classification. With this chapter, we have covered important classes
    of Bayesian machine learning models. In the next chapter, we will look at a different
    aspect: large scale machine learning and some of its applications in Bayesian
    models.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一个重要的机器学习模型类别，即神经网络，以及它们的贝叶斯实现。这些模型受到人脑架构的启发，并且继续是活跃的研究和开发领域。我们还学习了一种最新的神经网络进展，称为深度学习。它可以用于解决许多问题，例如涉及高度认知元素的计算机视觉和自然语言处理。使用深度学习的人工智能系统能够在语音识别和图像分类等任务中达到与人类智能相当的高精度。通过本章，我们已经涵盖了贝叶斯机器学习模型的重要类别。在下一章中，我们将探讨不同的方面：大规模机器学习及其在贝叶斯模型中的某些应用。
