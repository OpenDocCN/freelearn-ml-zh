- en: Chapter 8. Bayesian Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, artificial neural networks are statistical models built
    taking inspirations from the architecture and cognitive capabilities of biological
    brains. Neural network models typically have a layered architecture consisting
    of a large number of neurons in each layer, and neurons between different layers
    are connected. The first layer is called input layer, the last layer is called
    output layer, and the rest of the layers in the middle are called hidden layers.
    Each neuron has a state that is determined by a nonlinear function of the state
    of all neurons connected to it. Each connection has a weight that is determined
    from the training data containing a set of input and output pairs. This kind of
    layered architecture of neurons and their connections is present in the **neocortex**
    region of human brain and is considered to be responsible for higher functions
    such as sensory perception and language understanding.
  prefs: []
  type: TYPE_NORMAL
- en: The first computational model for neural network was proposed by Warren McCulloch
    and Walter Pitts in 1943\. Around the same time, psychologist Donald Hebb created
    a hypothesis of learning based on the mechanism of excitation and adaptation of
    neurons that is known as **Hebb's rule**. The hypothesis can be summarized by
    saying *Neurons that fire together, wire together*. Although there were several
    researchers who tried to implement computational models of neural networks, it
    was Frank Rosenblatt in 1958 who first created an algorithm for pattern recognition
    using a two-layer neural network called **Perceptron**.
  prefs: []
  type: TYPE_NORMAL
- en: The research and applications of neural networks had both stagnant and great
    periods of progress during 1970-2010\. Some of the landmarks in the history of
    neural networks are the invention of the **backpropagation** algorithm by Paul
    Werbos in 1975, a fast learning algorithm for learning multilayer neural networks
    (also called **deep learning networks**) by Geoffrey Hinton in 2006, and the use
    of GPGPUs to achieve greater computational power required for processing neural
    networks in the latter half of the last decade.
  prefs: []
  type: TYPE_NORMAL
- en: Today, neural network models and their applications have again taken a central
    stage in artificial intelligence with applications in computer vision, speech
    recognition, and natural language understanding. This is the reason this book
    has devoted one chapter specifically to this subject. The importance of Bayesian
    inference in neural network models will become clear when we go into detail in
    later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Two-layer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us look at the formal definition of a two-layer neural network. We follow
    the notations and description used by David MacKay (reference 1, 2, and 3 in the
    *References* section of this chapter). The input to the NN is given by ![Two-layer
    neural networks](img/image00529.jpeg). The input values are first multiplied by
    a set of weights to produce a weighted linear combination and then transformed
    using a nonlinear function to produce values of the state of neurons in the hidden
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00530.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A similar operation is done at the second layer to produce final output values
    ![Two-layer neural networks](img/image00531.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00532.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The function ![Two-layer neural networks](img/image00533.jpeg) is usually taken
    as either a **sigmoid** function ![Two-layer neural networks](img/image00534.jpeg)
    or ![Two-layer neural networks](img/image00535.jpeg). Another common function
    used for multiclass classification is **softmax** defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00536.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is a normalized exponential function.
  prefs: []
  type: TYPE_NORMAL
- en: All these are highly nonlinear functions exhibiting the property that the output
    value has a sharp increase as a function of the input. This nonlinear property
    gives neural networks more computational flexibility than standard linear or generalized
    linear models. Here, ![Two-layer neural networks](img/image00537.jpeg) is called
    a bias parameter. The weights ![Two-layer neural networks](img/image00538.jpeg)
    together with biases ![Two-layer neural networks](img/image00539.jpeg) form the
    weight vector **w** .
  prefs: []
  type: TYPE_NORMAL
- en: 'The schematic structure of the two-layer neural network is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00540.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The learning in neural networks corresponds to finding the value of weight
    vector such as **w**, such that for a given dataset consisting of ground truth
    values input and target (output), ![Two-layer neural networks](img/image00541.jpeg),
    the error of prediction of target values by the network is minimum. For regression
    problems, this is achieved by minimizing the error function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00542.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the classification task, in neural network training, instead of squared
    error one uses a cross entropy defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00543.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To avoid overfitting, a regularization term is usually also included in the
    objective function. The form of the regularization function is usually ![Two-layer
    neural networks](img/image00544.jpeg), which gives penalty to large values of
    **w**, reducing the chances of overfitting. The resulting objective function is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-layer neural networks](img/image00545.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Two-layer neural networks](img/image00546.jpeg) and ![Two-layer neural
    networks](img/image00547.jpeg) are free parameters for which the optimum values
    can be found from cross-validation experiments.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize *M(***w***)* with respect to **w**, one uses the backpropagation
    algorithm as described in the classic paper by Rumelhart, Hinton, and Williams
    (reference 3 in the *References* section of this chapter). In the backpropagation
    for each input/output pair, the value of the predicted output is computed using
    a forward pass from the input layer. The error, or the difference between the
    predicted output and actual output, is propagated back and at each node, the weights
    are readjusted so that the error is a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian treatment of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To set the neural network learning in a Bayesian context, consider the error
    function ![Bayesian treatment of neural networks](img/image00548.jpeg) for the
    regression case. It can be treated as a Gaussian noise term for observing the
    given dataset conditioned on the weights **w**. This is precisely the likelihood
    function that can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00549.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Bayesian treatment of neural networks](img/image00547.jpeg) is the
    variance of the noise term given by ![Bayesian treatment of neural networks](img/image00550.jpeg)
    and ![Bayesian treatment of neural networks](img/image00551.jpeg)represents a
    probabilistic model. The regularization term can be considered as the log of the
    prior probability distribution over the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00552.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Bayesian treatment of neural networks](img/image00553.jpeg) is the
    variance of the prior distribution of weights. It can be easily shown using Bayes''
    theorem that the objective function *M(***w***)* then corresponds to the posterior
    distribution of parameters **w**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00554.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the neural network case, we are interested in the local maxima of ![Bayesian
    treatment of neural networks](img/image00555.jpeg). The posterior is then approximated
    as a Gaussian around each maxima ![Bayesian treatment of neural networks](img/image00556.jpeg),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00557.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *A* is a matrix of the second derivative of *M(***w***)* with respect
    to **w** and represents an inverse of the covariance matrix. It is also known
    by the name **Hessian** matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of hyper parameters ![Bayesian treatment of neural networks](img/image00546.jpeg)
    and ![Bayesian treatment of neural networks](img/image00547.jpeg) is found using
    the **evidence framework**. In this, the probability ![Bayesian treatment of neural
    networks](img/image00558.jpeg) is used as a evidence to find the best values of
    ![Bayesian treatment of neural networks](img/image00546.jpeg) and ![Bayesian treatment
    of neural networks](img/image00547.jpeg) from data *D*. This is done through the
    following Bayesian rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00559.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'By using the evidence framework and Gaussian approximation of posterior (references
    2 and 5 in the *References* section of this chapter), one can show that the best
    value of ![Bayesian treatment of neural networks](img/image00560.jpeg) satisfies
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00561.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the best value of ![Bayesian treatment of neural networks](img/image00562.jpeg)
    satisfies the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian treatment of neural networks](img/image00563.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In these equations, ![Bayesian treatment of neural networks](img/image00564.jpeg)
    is the number of well-determined parameters given by ![Bayesian treatment of neural
    networks](img/image00565.jpeg) where *k* is the length of **w**.
  prefs: []
  type: TYPE_NORMAL
- en: The brnn R package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **brnn** package was developed by Paulino Perez Rodriguez and Daniel Gianola,
    and it implements the two-layer Bayesian regularized neural network described
    in the previous section. The main function in the package is `brnn( )` that can
    be called using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, *x* is an *n x p* matrix where *n* is the number of data points and *p*
    is the number of variables; *y* is an *n* dimensional vector containing target
    values. The number of neurons in the hidden layer of the network can be specified
    by the variable `neurons`. If the indicator function `normalize` is `TRUE`, it
    will normalize the input and output, which is the default option. The maximum
    number of iterations during model training is specified using `epochs`. If the
    indicator binary variable `Monte_Carlo` is true, then an MCMC method is used to
    estimate the trace of the inverse of the Hessian matrix A.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try an example with the Auto MPG dataset that we used in [Chapter 5](part0041.xhtml#aid-173721
    "Chapter 5. Bayesian Regression Models"), *Bayesian Regression Models*. The following
    R code will import data, create training and test sets, train a neural network
    model using training data, and make predictions for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Deep belief networks and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the pioneering advancements in neural networks research in the last
    decade have opened up a new frontier in machine learning that is generally called
    by the name **deep learning** (references 5 and 7 in the *References* section
    of this chapter). The general definition of deep learning is, *a class of machine
    learning techniques, where many layers of information processing stages in hierarchical
    supervised architectures are exploited for unsupervised feature learning and for
    pattern analysis/classification. The essence of deep learning is to compute hierarchical
    features or representations of the observational data, where the higher-level
    features or factors are defined from lower-level ones* (reference 8 in the *References*
    section of this chapter). Although there are many similar definitions and architectures
    for deep learning, two common elements in all of them are: *multiple layers of
    nonlinear information processing* and *supervised or unsupervised learning of
    feature representations at each layer from the features learned at the previous
    layer*. The initial works on deep learning were based on multilayer neural network
    models. Recently, many other forms of models have also been used, such as deep
    kernel machines and deep Q-networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Even in previous decades, researchers have experimented with multilayer neural
    networks. However, two reasons limited any progress with learning using such architectures.
    The first reason is that the learning of the network parameters is a non-convex
    optimization problem. Starting from random initial conditions, one gets stuck
    at local minima during minimization of error. The second reason is that the associated
    computational requirements were huge. A breakthrough for the first problem came
    when Geoffrey Hinton developed a fast algorithm for learning a special class of
    neural networks called **deep belief nets** (**DBN**). We will describe DBNs in
    more detail in later sections. The high computational power requirements were
    met with the advancement in computing using **general purpose graphical processing
    units** (**GPGPUs**). What made deep learning so popular for practical applications
    is the significant improvement in accuracy achieved in automatic speech recognition
    and computer vision. For example, the **word error rate** in automatic speech
    recognition of a switchboard conversational speech had reached a saturation of
    around 40% after years of research.
  prefs: []
  type: TYPE_NORMAL
- en: However, using deep learning, the word error rate reduced dramatically to close
    to 10% in a matter of a few years. Another well-known example is how **deep convolution
    neural network** achieved the least error rate of 15.3% in the 2012 ImageNet Large
    Scale Visual Recognition Challenge compared to state-of-the-art methods that gave
    26.2% as the least error rate (reference 7 in the *References* section of this
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will describe one class of deep learning models called deep
    belief networks. Interested readers may wish to read the book by Li Deng and Dong
    Yu (reference 9 in the *References* section of this chapter) for a detailed understanding
    of various methods and applications of deep learning. We will follow their notations
    in the rest of the chapter. We will also illustrate the use of DBN with the R
    package **darch**.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **restricted Boltzmann machine** (**RBM**) is a two-layer network (bi-partite
    graph), in which one layer is a visible layer (*v*) and the second layer is a
    hidden layer (*h*). All nodes in the visible layer and all nodes in the hidden
    layer are connected by undirected edges, and there no connections between nodes
    in the same layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00566.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'An RBM is characterized by the joint distribution of states of all visible
    units ![Restricted Boltzmann machines](img/image00567.jpeg) and states of all
    hidden units ![Restricted Boltzmann machines](img/image00568.jpeg) given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00569.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/image00570.jpeg) is called the **energy
    function** and ![Restricted Boltzmann machines](img/image00571.jpeg) is the normalization
    constant known by the name **partition function** from Statistical Physics nomenclature.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly two types of RBMs. In the first one, both *v* and *h* are
    Bernoulli random variables. In the second type, *h* is a Bernoulli random variable
    whereas *v* is a Gaussian random variable. For Bernoulli RBM, the energy function
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00572.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Restricted Boltzmann machines](img/image00573.jpeg) represents the
    weight of the edge between nodes ![Restricted Boltzmann machines](img/image00574.jpeg)
    and ![Restricted Boltzmann machines](img/image00575.jpeg); ![Restricted Boltzmann
    machines](img/image00576.jpeg) and ![Restricted Boltzmann machines](img/image00577.jpeg)
    are bias parameters for the visible and hidden layers respectively. For this energy
    function, the exact expressions for the conditional probability can be derived
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00578.jpeg)![Restricted Boltzmann
    machines](img/image00579.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/image00580.jpeg) is the logistic
    function ![Restricted Boltzmann machines](img/image00581.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'If the input variables are continuous, one can use the Gaussian RBM; the energy
    function of it is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00582.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, in this case, the conditional probabilities of ![Restricted Boltzmann
    machines](img/image00574.jpeg) and ![Restricted Boltzmann machines](img/image00575.jpeg)
    will become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00578.jpeg)![Restricted Boltzmann
    machines](img/image00583.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is a normal distribution with mean ![Restricted Boltzmann machines](img/image00584.jpeg)
    and variance 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have described the basic architecture of an RBM, how is it that
    it is trained? If we try to use the standard approach of taking the gradient of
    log-likelihood, we get the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/image00585.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/image00586.jpeg) is the expectation
    of ![Restricted Boltzmann machines](img/image00587.jpeg) computed using the dataset
    and ![Restricted Boltzmann machines](img/image00588.jpeg) is the same expectation
    computed using the model. However, one cannot use this exact expression for updating
    weights because ![Restricted Boltzmann machines](img/image00588.jpeg) is difficult
    to compute.
  prefs: []
  type: TYPE_NORMAL
- en: The first breakthrough came to solve this problem and, hence, to train deep
    neural networks, when Hinton and team proposed an algorithm called **Contrastive
    Divergence** (**CD**) (reference 7 in the *References* section of this chapter).
    The essence of the algorithm is described in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to approximate ![Restricted Boltzmann machines](img/image00588.jpeg)
    by using values of ![Restricted Boltzmann machines](img/image00574.jpeg) and ![Restricted
    Boltzmann machines](img/image00575.jpeg) generated using Gibbs sampling from the
    conditional distributions mentioned previously. One scheme of doing this is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize ![Restricted Boltzmann machines](img/image00589.jpeg) from the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find ![Restricted Boltzmann machines](img/image00590.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00591.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find ![Restricted Boltzmann machines](img/image00592.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00593.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find ![Restricted Boltzmann machines](img/image00594.jpeg) by sampling from
    the conditional distribution ![Restricted Boltzmann machines](img/image00595.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we find the values of ![Restricted Boltzmann machines](img/image00592.jpeg)
    and ![Restricted Boltzmann machines](img/image00594.jpeg), use ![Restricted Boltzmann
    machines](img/image00596.jpeg), which is the product of *i*^(th) component of
    ![Restricted Boltzmann machines](img/image00592.jpeg) and *j*^(th) component of
    ![Restricted Boltzmann machines](img/image00594.jpeg), as an approximation for
    ![Restricted Boltzmann machines](img/image00588.jpeg). This is called **CD-1 algorithm**.
    One can generalize this to use the values from the *k*^(th) step of Gibbs sampling
    and it is known as **CD-k algorithm**. One can easily see the connection between
    RBMs and Bayesian inference. Since the CD algorithm is like a posterior density
    estimate, one could say that RBMs are trained using a Bayesian inference approach.
  prefs: []
  type: TYPE_NORMAL
- en: Although the Contrastive Divergence algorithm looks simple, one needs to be
    very careful in training RBMs, otherwise the model can result in overfitting.
    Readers who are interested in using RBMs in practical applications should refer
    to the technical report (reference 10 in the *References* section of this chapter),
    where this is discussed in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One can stack several RBMs, one on top of each other, such that the values
    of hidden units in the layer ![Deep belief networks](img/image00597.jpeg) would
    become values of visible units in the *n*^(th) layer ![Deep belief networks](img/image00598.jpeg),
    and so on. The resulting network is called a deep belief network. It was one of
    the main architectures used in early deep learning networks for pretraining. The
    idea of pretraining a NN is the following: in the standard three-layer (input-hidden-output)
    NN, one can start with random initial values for the weights and using the backpropagation
    algorithm, can find a good minimum of the log-likelihood function. However, when
    the number of layers increases, the straightforward application of backpropagation
    does not work because starting from output layer, as we compute the gradient values
    for the layers deep inside, their magnitude becomes very small. This is called
    the **gradient vanishing** problem. As a result, the network will get trapped
    in some poor local minima. Backpropagation still works if we are starting from
    the neighborhood of a good minimum. To achieve this, a DNN is often pretrained
    in an unsupervised way, using a DBN. Instead of starting from random values of
    weights, train a DBN in an unsupervised way and use weights from the DBN as initial
    weights for a corresponding supervised DNN. It was seen that such DNNs pretrained
    using DBNs perform much better (reference 8 in the *References* section of this
    chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: The layer-wise pretraining of a DBN proceeds as follows. Start with the first
    RBM and train it using input data in the visible layer and the CD algorithm (or
    its latest better variants). Then, stack a second RBM on top of this. For this
    RBM, use values sample from ![Deep belief networks](img/image00599.jpeg) as the
    values for the visible layer. Continue this process for the desired number of
    layers. The outputs of hidden units from the top layer can also be used as inputs
    for training a supervised model. For this, add a conventional NN layer at the
    top of DBN with the desired number of classes as the number of output nodes. Input
    for this NN would be the output from the top layer of DBN. This is called **DBN-DNN
    architecture**. Here, a DBN's role is generating highly efficient features (the
    output of the top layer of DBN) automatically from the input data for the supervised
    NN in the top layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a five-layer DBN-DNN for a binary classification task is
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep belief networks](img/image00600.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The last layer is trained using the backpropagation algorithm in a supervised
    manner for the two classes ![Deep belief networks](img/image00601.jpeg) and ![Deep
    belief networks](img/image00602.jpeg). We will illustrate the training and classification
    with such a DBN-DNN using the darch R package.
  prefs: []
  type: TYPE_NORMAL
- en: The darch R package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The darch package, written by Martin Drees, is one of the R packages using which
    one can begin doing deep learning in R. It implements the DBN described in the
    previous section (references 5 and 7 in the *References* section of this chapter).
    The package can be downloaded from [https://cran.r-project.org/web/packages/darch/index.html](https://cran.r-project.org/web/packages/darch/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main class in the darch package implements deep architectures and provides
    the ability to train them with Contrastive Divergence and fine-tune with backpropagation,
    resilient backpropagation, and conjugate gradients. The new instances of the class
    are created with the `newDArch` constructor. It is called with the following arguments:
    a vector containing the number of nodes in each layers, the batch size, a Boolean
    variable to indicate whether to use the **ff** package for computing weights and
    outputs, and the name of the function for generating the weight matrices. Let
    us create a network having two input units, four hidden units, and one output
    unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us train the DBN with a toy dataset. We are using this because for training
    any realistic examples, it would take a long time: hours, if not days. Let us
    create an input data set containing two columns and four rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us pretrain the DBN, using the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have a look at the weights learned at any layer using the `getLayerWeights(
    )` function. Let us see how the hidden layer looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s do a backpropagation for supervised learning. For this, we need
    to first set the layer functions to `sigmoidUnitDerivatives`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the following two lines perform the backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the prediction quality of DBN on the training data itself by running
    `darch` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Comparing with the actual output, DBN has predicted the wrong output for the
    first and second input rows. Since this example was just to illustrate how to
    use the darch package, we are not worried about the 50% accuracy here.
  prefs: []
  type: TYPE_NORMAL
- en: Other deep learning packages in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although there are other deep learning packages in R, such as **deepnet** and
    **RcppDL**, compared with libraries in other languages such as **Cuda** (C++)
    and **Theano** (Python), R yet does not have good native libraries for deep learning.
    The only available package is a wrapper for the Java-based deep learning open
    source project H2O. This R package, **h2o**, allows running H2O via its REST API
    from within R. Readers who are interested in serious deep learning projects and
    applications should use H2O using h2o packages in R. One needs to install H2O
    in your machine to use h2o. We will cover H2O in the next chapter when we discuss
    Big Data and the distributed computing platform called Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the Auto MPG dataset, compare the performance of predictive models using
    ordinary regression, Bayesian GLM, and Bayesian neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MacKay D. J. C. *Information Theory, Inference and Learning Algorithms*. Cambridge
    University Press. 2003\. ISBN-10: 0521642981'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MacKayD. J. C. "The Evidence Framework Applied to Classification Networks".
    Neural Computation. Volume 4(3), 698-714\. 1992
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MacKay D. J. C. "Probable Networks and Plausible Predictions – a review of
    practical Bayesian methods for supervised neural networks". Network: Computation
    in neural systems'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Rumelhart D. E., and Williams R. J. "Learning Representations
    by Back Propagating Errors". Nature. Volume 323, 533-536\. 1986
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MacKay D. J. C. "Bayesian Interpolation". Neural Computation. Volume 4(3), 415-447\.
    1992
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Krizhevsky A., and Sutskever I. "ImageNet Classification with
    Deep Convolutional Neural Networks". Advances In Neural Information Processing
    Systems (NIPS). 2012
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G., Osindero S., and Teh Y. "A Fast Learning Algorithm for Deep Belief
    Nets". Neural Computation. 18:1527–1554\. 2006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. and Salakhutdinov R. "Reducing the Dimensionality of Data with Neural
    Networks". Science. 313(5786):504–507\. 2006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Li Deng and Dong Yu. *Deep Learning: Methods and Applications (Foundations
    and Trends(r) in Signal Processing)*. Now Publishers Inc. Vol 7, Issue 3-4\. 2014\.
    ISBN-13: 978-1601988140'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. "A Practical Guide to Training Restricted Boltzmann Machines". UTML
    Tech Report 2010-003\. Univ. Toronto. 2010
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about an important class of machine learning model,
    namely neural networks, and their Bayesian implementation. These models are inspired
    by the architecture of the human brain and they continue to be an area of active
    research and development. We also learned one of the latest advances in neural
    networks that is called deep learning. It can be used to solve many problems such
    as computer vision and natural language processing that involves highly cognitive
    elements. The artificial intelligent systems using deep learning were able to
    achieve accuracies comparable to human intelligence in tasks such as speech recognition
    and image classification. With this chapter, we have covered important classes
    of Bayesian machine learning models. In the next chapter, we will look at a different
    aspect: large scale machine learning and some of its applications in Bayesian
    models.'
  prefs: []
  type: TYPE_NORMAL
