["```py\nmkdir data\ncurl -O https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\ngzip -d mnist.pkl.gz\nmv mnist.pkl data/          \n```", "```py\nfrom __future__ import print_function \nimport tensorflow as tf\nimport pickle # for handling the new data source\nimport numpy as np\nfrom datetime import datetime # for filename conventions\nfrom tensorflow.python.lib.io import file_io # for better file I/O\nimport sys\n```", "```py\nf = file_io.FileIO('data/mnist.pkl', mode='r')\ndata = pickle.load(f)\n```", "```py\n(x_train, y_train), (x_test, y_test) = data\n# Converting the data from a 28 x 28 shape to 784 columns\nx_train = x_train.reshape(60000, 784)\nx_train = x_train.astype('float32')\n# Scaling the train dataset\nx_train /= 255\n# Reshaping the test dataset\nx_test = x_test.reshape(10000, 784)\nx_test = x_test.astype('float32')\n# Scaling the test dataset\nx_test /= 255\n# Specifying the type of labels\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\n```", "```py\n# Creating the estimator input functions for train and test datasets \ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n x={\"x2\": np.array(x_train)},\n y=np.array(y_train),\n num_epochs=None,\n batch_size=1024,\n shuffle=True)\ntest_input_fn = tf.estimator.inputs.numpy_input_fn(\n x={\"x2\": np.array(x_test)},\n y=np.array(y_test),\n num_epochs=1,\n shuffle=False)\n```", "```py\nfeature_x = tf.feature_column.numeric_column(\"x2\", shape=(784))\nfeature_columns = [feature_x]\n```", "```py\nnum_hidden_units = [1000]\nlr=0.1\nnum_steps=200\n# Building the estimator using DNN classifier\n# This is where the learning rate hyper parameter is passed\nmodel = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n                 hidden_units=num_hidden_units,\n                 activation_fn=tf.nn.relu,\n                 n_classes=10,\n                 optimizer=tf.train.AdagradOptimizer(learning_rate = lr))\nmodel.train(input_fn=train_input_fn, steps=num_steps) \n# Fetching the model results\nresult = model.evaluate(input_fn=test_input_fn)\nprint('Test loss:', result['average_loss'])\nprint('Test accuracy:', result['accuracy'])\n```", "```py\nnum_hidden_units = [1000]\nlr=0.01\nnum_steps=2000\n# Building the estimator using DNN classifier\n# This is where the learning rate hyper parameter is passed\nmodel = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n hidden_units=num_hidden_units,\n activation_fn=tf.nn.relu,\n n_classes=10,\n optimizer=tf.train.AdagradOptimizer(learning_rate = lr))\nmodel.train(input_fn=train_input_fn, steps=num_steps) \n# Fetching the model results\nresult = model.evaluate(input_fn=test_input_fn)\nprint('Test loss:', result['average_loss'])\nprint('Test accuracy:', result['accuracy']) \n```", "```py\ngsutil mb gs://my-mnist-bucket\ngsutil cp -r data/mnist.pkl gs://my-mnist-bucket/data/mnist.pkl\n```", "```py\nmkdir trainer\ncd trainer\n```", "```py\nvim mnist_mlp_lr_numsteps.py\n```", "```py\nfrom __future__ import print_function\n\nimport argparse\nimport pickle \nfrom datetime import datetime \nimport numpy as np\nfrom tensorflow.python.lib.io import file_io # for better file I/O\nimport sys\nimport tensorflow as tf\n\ndef train_model(train_file='data/mnist.pkl',job_dir='./tmp/mnist_mlp', num_steps = 1, lr=0.1, **args):\n  # logs_path gives access to the logs that are generated by the previous epochs of model\n  logs_path = job_dir + '/logs/' + str(datetime.now().isoformat())\n  print('Using logs_path located at {}'.format(logs_path))\n  # by default floats are considered as string\n  # Good idea to convert them back into floats\n  lr=float(lr)\n  num_steps=float(num_steps)\n  batch_size = 1024\n  num_classes = 10\n  # Reading in the pickle file. Pickle works differently with Python 2 vs 3\n  # In Python 2 the following code would be:\n  # f = file_io.FileIO(train_file, mode='r')\n  # data = pickle.load(f)\n  f = file_io.FileIO(train_file, mode='rb') \n  data = pickle.load(f,encoding='bytes') \n  (x_train, y_train), (x_test, y_test) = data\n  # Converting the data from a 28X28 shape to 784 columns\n  x_train = x_train.reshape(60000, 784)\n  x_train = x_train.astype('float32')\n  x_test = x_test.reshape(10000, 784)\n  x_test = x_test.astype('float32')\n  x_train /= 255\n  x_test /= 255\n  # Specifying the type of following labels\n  y_train = y_train.astype(np.int32)\n  y_test = y_test.astype(np.int32)\n\n  # Creating the estimator following input functions \n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x2\": np.array(x_train)},\n    y=np.array(y_train),\n    num_epochs=None,\n    batch_size=batch_size,\n    shuffle=True)\n  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x2\": np.array(x_test)},\n    y=np.array(y_test),\n    num_epochs=1,\n    shuffle=False)\n  # Specifying the columns as numeric columns\n  feature_x = tf.feature_column.numeric_column(\"x2\", shape=(784))\n  feature_columns = [feature_x]\n  num_hidden_units = [1000]\n  # Building the estimator using DNN classifier\n  # This is where the learning rate hyper parameter is passed\n  model = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n                                   hidden_units=num_hidden_units,\n                                   activation_fn=tf.nn.relu,\n                                   n_classes=num_classes,\n                   optimizer=tf.train.AdagradOptimizer(learning_rate = lr))\n  # Passing the other parameter: num_steps\n  model.train(input_fn=train_input_fn, steps=num_steps) \n  # Fetching the model results\n  result = model.evaluate(input_fn=test_input_fn)\n  print('Test loss:', result['average_loss'])\n  print('Test accuracy:', result['accuracy'])\n\nif __name__ == '__main__':\n  # Parse the input arguments for common Cloud ML Engine options\n  # There are 4 arguments that we need to give, as per the preceding model specification \n  # training file location, job directory, number of steps and learning rate\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n    '--train-file',\n    help='Cloud Storage bucket or local path to training data')\n  parser.add_argument(\n    '--job-dir',\n    help='Cloud storage bucket to export the model and store temp files')\n  parser.add_argument(\n    '--num-steps',\n    help='number of steps')\n  parser.add_argument(\n    '--lr',\n    help='learning rate') \n\n  args = parser.parse_args()\n  arguments = args.__dict__\n  train_model(**arguments)\n```", "```py\nvim hptune.yaml\n```", "```py\ntrainingInput:\n  pythonVersion: \"3.5\"\n  scaleTier: CUSTOM\n  masterType: standard_gpu\n  hyperparameters:\n    goal: MAXIMIZE\n    hyperparameterMetricTag: accuracy\n    maxTrials: 10\n    maxParallelTrials: 1\n    params:\n      - parameterName: num-steps\n        type: INTEGER\n        minValue: 200\n        maxValue: 10000\n        scaleType: UNIT_LINEAR_SCALE\n      - parameterName: lr\n        type: DOUBLE\n        minValue: 0.001\n        maxValue: 0.1\n        scaleType: UNIT_LOG_SCALE\n```", "```py\nfrom setuptools import setup, find_packages\nsetup(name='mnist_mlp_lr_numsteps',\n      version='1.0',\n      packages=find_packages(),\n      include_package_data=True,\n      install_requires=[\n          'keras',\n          'h5py'],\n      zip_safe=False)\n```", "```py\ntouch trainer/__init__.py\n```", "```py\nexport BUCKET_NAME=my-mnist-bucket\nexport JOB_NAME=\"mnist_mlp_hpt_train_$(date +%Y%m%d_%H%M%S)\"\nexport JOB_DIR=gs://$BUCKET_NAME/$JOB_NAME\nexport REGION=us-east1\nexport HPTUNING_CONFIG=hptune.yaml\ngcloud ml-engine jobs submit training $JOB_NAME \\\n --job-dir $JOB_DIR \\\n --runtime-version 1.6 \\\n --config $HPTUNING_CONFIG \\\n --module-name trainer.mnist_mlp_lr_numsteps \\\n --package-path ./trainer \\\n --region $REGION \\\n -- \\\n --train-file gs://$BUCKET_NAME/data/mnist.pkl \\\n --num-steps 100 \\\n --lr 0.01\n```"]