["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.linear_model import LinearRegression\n    from sklearn.feature_selection import SelectFromModel\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    ```", "```py\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    un_income_gap = pd.read_csv(\"data/un_income_gap.csv\")\n    un_income_gap.set_index('country', inplace=True)\n    un_income_gap['incomeratio'] = \\\n      un_income_gap.femaleincomepercapita / \\\n        un_income_gap.maleincomepercapita\n    un_income_gap['educratio'] = \\\n      un_income_gap.femaleyearseducation / \\\n         un_income_gap.maleyearseducation\n    un_income_gap['laborforcepartratio'] = \\\n      un_income_gap.femalelaborforceparticipation / \\\n         un_income_gap.malelaborforceparticipation\n    un_income_gap['humandevratio'] = \\\n      un_income_gap.femalehumandevelopment / \\\n         un_income_gap.malehumandevelopment\n    un_income_gap.dropna(subset=['incomeratio'], inplace=True)\n    ```", "```py\n    num_cols = ['educratio','laborforcepartratio',\n    'humandevratio','genderinequality','maternalmortality',\n      'adolescentbirthrate','femaleperparliament',\n    'incomepercapita']\n    gap_sub = un_income_gap[['incomeratio'] + num_cols]\n    gap_sub.head()\n    incomeratio  educratio  laborforcepartratio  humandevratio\\\n    country\n    Norway         0.78    1.02    0.89    1.00\n    Australia      0.66    1.02    0.82    0.98\n    Switzerland    0.64    0.88    0.83    0.95\n    Denmark        0.70    1.01    0.88    0.98\n    Netherlands    0.48    0.95    0.83    0.95\n    genderinequality  maternalmortality  adolescentbirthrate\\\n    country\n    Norway        0.07    4.00    7.80\n    Australia     0.11    6.00    12.10\n    Switzerland   0.03    6.00    1.90\n    Denmark       0.05    5.00    5.10\n    Netherlands   0.06    6.00    6.20\n                 femaleperparliament  incomepercapita  \n    country                                            \n    Norway       39.60    64992\n    Australia    30.50    42261\n    Switzerland  28.50    56431\n    Denmark      38.00    44025\n    Netherlands  36.90    45435\n    ```", "```py\n    gap_sub.\\\n      agg(['count','min','median','max']).T\n                        count  min    median    max\n    incomeratio         177.00 0.16   0.60      0.93\n    educratio           169.00 0.24   0.93      1.35\n    laborforcepartratio 177.00 0.19   0.75      1.04\n    humandevratio       161.00 0.60   0.95      1.03\n    genderinequality    155.00 0.02   0.39      0.74\n    maternalmortality   174.00 1.00   60.00     1,100.00\n    adolescentbirthrate 177.00 0.60   40.90     204.80\n    femaleperparliament 174.00 0.00   19.35     57.50\n    incomepercapita     177.00 581.00 10,512.00 123,124.00\n    ```", "```py\n    corrmatrix = gap_sub.corr(method=\"pearson\")\n    corrmatrix\n    sns.heatmap(corrmatrix, xticklabels=corrmatrix.columns,\n    yticklabels=corrmatrix.columns, cmap=\"coolwarm\")\n    plt.title('Heat Map of Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(gap_sub[num_cols],\\\n      gap_sub[['incomeratio']], test_size=0.2, random_state=0)\n    ```", "```py\n    knnreg = KNeighborsRegressor()\n    feature_sel = SelectFromModel(LinearRegression(), threshold=\"0.8*mean\")\n    pipe1 = make_pipeline(OutlierTrans(3), \\\n      SimpleImputer(strategy=\"median\"), StandardScaler(), \\\n      feature_sel, knnreg)\n    ```", "```py\n    knnreg_params = {\n     'kneighborsregressor__n_neighbors': \\\n         np.arange(3, 21, 2),\n     'kneighborsregressor__metric': \\\n         ['euclidean','manhattan','minkowski']\n    }\n    ```", "```py\nrs = RandomizedSearchCV(pipe1, knnreg_params, cv=4, n_iter=20, \\\n  scoring='neg_mean_absolute_error', random_state=1)\nrs.fit(X_train, y_train)\nrs.best_params_\n{'kneighborsregressor__n_neighbors': 11,\n 'kneighborsregressor__metric': 'manhattan'}\nrs.best_score_\n-0.05419731104389228\n```", "```py\n    selected = rs.best_estimator_['selectfrommodel'].get_support()\n    np.array(num_cols)[selected]\n    array(['laborforcepartratio', 'humandevratio'], dtype='<U19')\n    ```", "```py\n    rs.best_estimator_['selectfrommodel'].\\\n      get_feature_names_out(np.array(num_cols))\n    array(['laborforcepartratio', 'humandevratio'], dtype=object)\n    ```", "```py\n    results = \\\n      pd.DataFrame(rs.cv_results_['mean_test_score'], \\\n        columns=['meanscore']).\\\n      join(pd.DataFrame(rs.cv_results_['params'])).\\\n      sort_values(['meanscore'], ascending=False)\n    results.head(3).T\n    13       1      3\n    Meanscore   -0.05   -0.05   -0.05\n    regressor__kneighborsregressor__n_neighbors  11  13  9\n    regressor__kneighborsregressor__metric  manhattan  manhattan  euclidean\n    ```", "```py\n    pred = rs.predict(X_test)\n    preddf = pd.DataFrame(pred, columns=['prediction'],\n      index=X_test.index).join(X_test).join(y_test)\n    preddf['resid'] = preddf.incomeratio-preddf.prediction\n    preddf.resid.agg(['mean','median','skew','kurtosis'])\n    mean            -0.01\n    median          -0.01\n    skew            -0.61\n    kurtosis         0.23\n    Name: resid, dtype: float64\n    ```", "```py\n    plt.hist(preddf.resid, color=\"blue\")\n    plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Histogram of Residuals for Gax Tax Model\")\n    plt.xlabel(\"Residuals\")\n    plt.ylabel(\"Frequency\")\n    plt.xlim()\n    plt.show()\n    ```", "```py\n    plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n    plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Scatterplot of Predictions and Residuals\")\n    plt.xlabel(\"Predicted Income Gap\")\n    plt.ylabel(\"Residuals\")\n    plt.show()\n    ```", "```py\npreddf.loc[np.abs(preddf.resid)>=0.1,\n  ['incomeratio', 'prediction', 'resid', \n   'laborforcepartratio', 'humandevratio']].T\ncountry                     Afghanistan    Netherlands\nincomeratio                  0.16           0.48\nprediction                   0.32           0.65\nresid                       -0.16          -0.17\nlaborforcepartratio          0.20           0.83\nhumandevratio                0.60           0.95\n```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.tree import DecisionTreeRegressor, plot_tree\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.linear_model import LinearRegression\n    from sklearn.feature_selection import SelectFromModel\n    ```", "```py\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    un_income_gap = pd.read_csv(\"data/un_income_gap.csv\")\n    un_income_gap.set_index('country', inplace=True)\n    un_income_gap['incomeratio'] = \\\n      un_income_gap.femaleincomepercapita / \\\n        un_income_gap.maleincomepercapita\n    un_income_gap['educratio'] = \\\n      un_income_gap.femaleyearseducation / \\\n         un_income_gap.maleyearseducation\n    un_income_gap['laborforcepartratio'] = \\\n      un_income_gap.femalelaborforceparticipation / \\\n         un_income_gap.malelaborforceparticipation\n    un_income_gap['humandevratio'] = \\\n      un_income_gap.femalehumandevelopment / \\\n         un_income_gap.malehumandevelopment\n    un_income_gap.dropna(subset=['incomeratio'], \n      inplace=True)\n    num_cols = ['educratio','laborforcepartratio',\n      'humandevratio', 'genderinequality', \n      'maternalmortality', 'adolescentbirthrate', \n      'femaleperparliament', 'incomepercapita']\n    gap_sub = un_income_gap[['incomeratio'] + num_cols]\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(gap_sub[num_cols],\\\n      gap_sub[['incomeratio']], test_size=0.2, \n        random_state=0)\n    ```", "```py\n    dtreg_example = DecisionTreeRegressor(\n      min_samples_leaf=5,\n      max_depth=3)\n    pipe0 = make_pipeline(OutlierTrans(3),\n      SimpleImputer(strategy=\"median\"))\n    X_train_imp = pipe0.fit_transform(X_train)\n    dtreg_example.fit(X_train_imp, y_train)\n    plot_tree(dtreg_example, \n      feature_names=X_train.columns,\n      label=\"root\", fontsize=10)\n    ```", "```py\n    dtreg = DecisionTreeRegressor()\n    feature_sel = SelectFromModel(LinearRegression(),\n      threshold=\"0.8*mean\")\n    pipe1 = make_pipeline(OutlierTrans(3),\n      SimpleImputer(strategy=\"median\"),\n      feature_sel, dtreg)\n    dtreg_params={\n     \"decisiontreeregressor__max_depth\": np.arange(2, 20),\n     \"decisiontreeregressor__min_samples_leaf\": np.arange(5, 11)\n    }\n    ```", "```py\n    rs = RandomizedSearchCV(pipe1, dtreg_params, cv=4, n_iter=20,\n      scoring='neg_mean_absolute_error', random_state=1)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'decisiontreeregressor__min_samples_leaf': 5,\n     'decisiontreeregressor__max_depth': 9}\n    rs.best_score_\n    -0.05268976358459662\n    ```", "```py\n    rfreg = RandomForestRegressor()\n    rfreg_params = {\n     'randomforestregressor__max_depth': np.arange(2, 20),\n     'randomforestregressor__max_features': ['auto', 'sqrt'],\n     'randomforestregressor__min_samples_leaf':  np.arange(5, 11)\n    }\n    pipe2 = make_pipeline(OutlierTrans(3), \n      SimpleImputer(strategy=\"median\"),\n      feature_sel, rfreg)\n    ```", "```py\n    rs = RandomizedSearchCV(pipe2, rfreg_params, cv=4, n_iter=20,\n      scoring='neg_mean_absolute_error', random_state=1)\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'randomforestregressor__min_samples_leaf': 5,\n     'randomforestregressor__max_features': 'auto',\n     'randomforestregressor__max_depth': 9}\n    rs.best_score_\n    -0.04930503752638253\n    ```", "```py\n    pred = rs.predict(X_test)\n    preddf = pd.DataFrame(pred, columns=['prediction'],\n      index=X_test.index).join(X_test).join(y_test)\n    preddf['resid'] = preddf.incomegap-preddf.prediction\n    plt.hist(preddf.resid, color=\"blue\", bins=5)\n    plt.axvline(preddf.resid.mean(), color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Histogram of Residuals for Income Gap\")\n    plt.xlabel(\"Residuals\")\n    plt.ylabel(\"Frequency\")\n    plt.xlim()\n    plt.show()\n    ```", "```py\n    plt.scatter(preddf.prediction, preddf.resid, color=\"blue\")\n    plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n    plt.title(\"Scatterplot of Predictions and Residuals\")\n    plt.xlabel(\"Predicted Income Gap\")\n    plt.ylabel(\"Residuals\")\n    plt.show()\n    ```", "```py\n    preddf.loc[np.abs(preddf.resid)>=0.12,\n      ['incomeratio','prediction','resid',\n      'laborforcepartratio', 'humandevratio']].T\n    country              Netherlands\n    incomeratio                 0.48\n    prediction                  0.66\n    resid                      -0.18\n    laborforcepartratio         0.83\n    humandevratio               0.95\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.impute import SimpleImputer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.compose import ColumnTransformer\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.ensemble import GradientBoostingRegressor\n    from xgboost import XGBRegressor\n    from sklearn.linear_model import LinearRegression\n    from sklearn.feature_selection import SelectFromModel\n    import matplotlib.pyplot as plt\n    from scipy.stats import randint\n    from scipy.stats import uniform\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import OutlierTrans\n    ```", "```py\n    housing = pd.read_csv(\"data/kc_house_data.csv\")\n    housing.set_index('id', inplace=True)\n    num_cols = ['bedrooms', 'bathrooms', 'sqft_living', \n      'sqft_lot', 'floors', 'view', 'condition', \n      'sqft_above', 'sqft_basement', 'yr_built', \n      'yr_renovated', 'sqft_living15', 'sqft_lot15']\n    cat_cols = ['waterfront']\n    housing[['price'] + num_cols + cat_cols].\\\n      head(3).T\n    id              7129300520  6414100192  5631500400\n    price           221,900     538,000     180,000\n    bedrooms        3           3           2\n    bathrooms       1           2           1\n    sqft_living     1,180       2,570       770\n    sqft_lot        5,650       7,242       10,000\n    floors          1           2           1\n    view            0           0           0\n    condition       3           3           3\n    sqft_above      1,180       2,170       770\n    sqft_basement   0           400         0\n    yr_built        1,955       1,951       1,933\n    yr_renovated    0           1,991       0\n    sqft_living15   1,340       1,690       2,720\n    sqft_lot15      5,650       7,639       8,062\n    waterfront      0           0           0\n    ```", "```py\n    housing[['price'] + num_cols].\\\n      agg(['count','min','median','max']).T\n                    count   min      median    max\n    price          21,613   75,000   450,000   7,700,000\n    bedrooms       21,613   0        3         33\n    bathrooms      21,613   0        2         8\n    sqft_living    21,613   290      1,910     13,540\n    sqft_lot       21,613   520      7,618     1,651,359\n    floors         21,613   1        2         4\n    view           21,613   0        0         4\n    condition      21,613   1        3         5\n    sqft_above     21,613   290      1,560     9,410\n    sqft_basement  21,613   0        0         4,820\n    yr_built       21,613   1,900    1,975     2,015\n    yr_renovated   21,613   0        0         2,015\n    sqft_living15  21,613   399      1,840     6,210\n    sqft_lot15     21,613   651      7,620     871,200\n    ```", "```py\n    plt.hist(housing.price/1000)\n    plt.title(\"Housing Price (in thousands)\")\n    plt.xlabel('Price')\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    ```", "```py\n    housing['price_log'] = np.log(housing['price'])\n    plt.hist(housing.price_log)\n    plt.title(\"Housing Price Log\")\n    plt.xlabel('Price Log')\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    ```", "```py\n    housing[['price','price_log']].agg(['kurtosis','skew'])\n                     price       price_log\n    kurtosis         34.59            0.69\n    skew              4.02            0.43\n    ```", "```py\n    corrmatrix = housing[['price_log'] + num_cols].\\\n       corr(method=\"pearson\")\n    sns.heatmap(corrmatrix, \n      xticklabels=corrmatrix.columns,\n      yticklabels=corrmatrix.columns, cmap=\"coolwarm\")\n    plt.title('Heat Map of Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    target = housing[['price_log']]\n    features = housing[num_cols + cat_cols]\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(features,\\\n      target, test_size=0.2, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop='first', sparse=False)\n    standtrans = make_pipeline(OutlierTrans(2),\n      SimpleImputer(strategy=\"median\"),\n      MinMaxScaler())\n    cattrans = make_pipeline(ohe)\n    coltrans = ColumnTransformer(\n      transformers=[\n        (\"stand\", standtrans, num_cols),\n        (\"cat\", cattrans, cat_cols)\n      ]\n    )\n    ```", "```py\n    gbr = GradientBoostingRegressor(random_state=0)\n    feature_sel = SelectFromModel(LinearRegression(),\n      threshold=\"0.6*mean\")\n    gbr_params = {\n     'gradientboostingregressor__learning_rate': uniform(loc=0.01, scale=0.5),\n     'gradientboostingregressor__n_estimators': randint(500, 2000),\n     'gradientboostingregressor__max_depth': randint(2, 20),\n     'gradientboostingregressor__min_samples_leaf': randint(5, 11)\n    }\n    pipe1 = make_pipeline(coltrans, feature_sel, gbr)\n    ```", "```py\n    rs1 = RandomizedSearchCV(pipe1, gbr_params, cv=5, n_iter=20,\n      scoring='neg_mean_squared_error', random_state=0)\n    rs1.fit(X_train, y_train.values.ravel())\n    rs1.best_params_\n    {'gradientboostingregressor__learning_rate': 0.118275177212,\n     'gradientboostingregressor__max_depth': 2,\n     'gradientboostingregressor__min_samples_leaf': 5,\n     'gradientboostingregressor__n_estimators': 1577}\n    rs1.best_score_\n    -0.10695077555421204\n    y_test.mean()\n    price_log   13.03\n    dtype: float64\n    ```", "```py\n    print(\"fit time: %.3f, score time: %.3f\"  %\n      (np.mean(rs1.cv_results_['mean_fit_time']),\\\n      np.mean(rs1.cv_results_['mean_score_time'])))\n    fit time: 35.695, score time: 0.152\n    ```", "```py\n    xgb = XGBRegressor()\n    xgb_params = {\n     'xgbregressor__learning_rate': uniform(loc=0.01, scale=0.5),\n     'xgbregressor__n_estimators': randint(500, 2000),\n     'xgbregressor__max_depth': randint(2, 20)\n    }\n    pipe2 = make_pipeline(coltrans, feature_sel, xgb)\n    ```", "```py\n    rs2 = RandomizedSearchCV(pipe2, xgb_params, cv=5, n_iter=20,\n      scoring='neg_mean_squared_error', random_state=0)\n    rs2.fit(X_train, y_train.values.ravel())\n    rs2.best_params_\n    {'xgbregressor__learning_rate': 0.019394900218177573,\n     'xgbregressor__max_depth': 7,\n     'xgbregressor__n_estimators': 1256}\n    rs2.best_score_\n    -0.10574300757906044\n    print(\"fit time: %.3f, score time: %.3f\"  %\n      (np.mean(rs2.cv_results_['mean_fit_time']),\\\n      np.mean(rs2.cv_results_['mean_score_time'])))\n    fit time: 3.931, score time: 0.046\n    ```"]