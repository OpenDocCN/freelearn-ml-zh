["```py\nIn [1]: from sklearn.ensemble import BaggingClassifier...     from sklearn.neighbors import KNeighborsClassifier...     bag_knn = BaggingClassifier(KNeighborsClassifier(),...                                 n_estimators=10)\n```", "```py\nIn [7]: from sklearn.ensemble import BaggingRegressor\n...     from sklearn.tree import DecisionTreeRegressor\n...     bag_tree = BaggingRegressor(DecisionTreeRegressor(),\n...                                 max_features=0.5, n_estimators=10, \n...                                 random_state=3)\n```", "```py\nIn [8]: from sklearn.datasets import load_boston\n...     dataset = load_boston()\n...     X = dataset.data\n...     y = dataset.target\nIn [9]: from sklearn.model_selection import train_test_split\n...     X_train, X_test, y_train, y_test = train_test_split(\n...         X, y, random_state=3\n...     )\n```", "```py\nIn [10]: bag_tree.fit(X_train, y_train)\n...      bag_tree.score(X_test, y_test)\nOut[10]: 0.82704756225081688\n```", "```py\nIn [11]: from sklearn.ensemble import GradientBoostingClassifier...      boost_class = GradientBoostingClassifier(n_estimators=10,...                                               random_state=3)\n```", "```py\nIn [15]: from sklearn.ensemble import GradientBoostingRegressor\n...      boost_reg = GradientBoostingRegressor(n_estimators=10,\n...                                            random_state=3)\n```", "```py\nIn [16]: dataset = load_boston()\n...      X = dataset.data\n...      y = dataset.target\nIn [17]: X_train, X_test, y_train, y_test = train_test_split(\n...          X, y, random_state=3\n...     )\n```", "```py\nIn [18]: boost_reg.fit(X_train, y_train)\n...      boost_reg.score(X_test, y_test)\nOut[18]: 0.71991199075668488\n```", "```py\nIn [19]: boost_reg = GradientBoostingRegressor(n_estimators=100)\n```", "```py\nIn [20]: boost_reg.fit(X_train, y_train)\n...      boost_reg.score(X_test, y_test)\nOut[20]: 0.89984081091774459\n```", "```py\nIn [1]: from sklearn.datasets import make_moons...     X, y = make_moons(n_samples=100, noise=0.25,...                       random_state=100)\n```", "```py\nIn [7]: import cv2\n...     rtree = cv2.ml.RTrees_create()\n```", "```py\nIn [8]: n_trees = 10\n...     eps = 0.01\n...     criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n...                 n_trees, eps)\n...     rtree.setTermCriteria(criteria)\n```", "```py\nIn [9]: rtree.train(X_train.astype(np.float32), cv2.ml.ROW_SAMPLE,\n                    y_train);\n```", "```py\nIn [10]: _, y_hat = rtree.predict(X_test.astype(np.float32))\n```", "```py\nIn [11]: from sklearn.metrics import accuracy_score\n...      accuracy_score(y_test, y_hat)\nOut[11]: 0.83999999999999997\n```", "```py\nIn [12]: plot_decision_boundary(rtree, X_test, y_test)\n```", "```py\nIn [13]: from sklearn.ensemble import RandomForestClassifier...      forest = RandomForestClassifier(n_estimators=10, random_state=200)\n```", "```py\nIn [16]: from sklearn.ensemble import ExtraTreesClassifier\n...      extra_tree = ExtraTreesClassifier(n_estimators=10, random_state=100)\n```", "```py\nIn [17]: from sklearn.datasets import load_iris\n...      iris = load_iris()\n...      X = iris.data[:, [0, 2]]\n...      y = iris.target\nIn [18]: X_train, X_test, y_train, y_test = train_test_split(\n...          X, y, random_state=100\n...      )\n```", "```py\nIn [19]: extra_tree.fit(X_train, y_train)\n...      extra_tree.score(X_test, y_test)\nOut[19]: 0.92105263157894735\n```", "```py\nIn [20]: forest = RandomForestClassifier(n_estimators=10,\n                                        random_state=100)\n...      forest.fit(X_train, y_train)\n...      forest.score(X_test, y_test)\nOut[20]: 0.92105263157894735\n```", "```py\nIn [21]: tree = DecisionTreeClassifier()\n...      tree.fit(X_train, y_train)\n...      tree.score(X_test, y_test)\nOut[21]: 0.92105263157894735\n```", "```py\nIn [22]: classifiers = [\n...          (1, 'decision tree', tree),\n...          (2, 'random forest', forest),\n...          (3, 'extremely randomized trees', extra_tree)\n...      ]\n```", "```py\nIn [23]: for sp, name, model in classifiers:\n...      plt.subplot(1, 3, sp)\n...      plot_decision_boundary(model, X_test, y_test)\n...      plt.title(name)\n...      plt.axis('off')\n\n```", "```py\nIn [1]: from sklearn.datasets import fetch_olivetti_faces\n...     dataset = fetch_olivetti_faces()\nIn [2]: X = dataset.data\n...     y = dataset.target\n```", "```py\nIn [3]: import numpy as np\n...     np.random.seed(21)\n...     idx_rand = np.random.randint(len(X), size=8)\n```", "```py\nIn [4]: import matplotlib.pyplot as plt\n...     %matplotlib inline\n...     for p, i in enumerate(idx_rand):\n...         plt.subplot(2, 4, p + 1)\n... plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')\n...         plt.axis('off')\n```", "```py\nIn [5]: n_samples, n_features = X.shape[:2]...     X -= X.mean(axis=0)\n```", "```py\nIn [6]: X -= X.mean(axis=1).reshape(n_samples, -1)\n```", "```py\nIn [7]: for p, i in enumerate(idx_rand):...         plt.subplot(2, 4, p + 1)...         plt.imshow(X[i, :].reshape((64, 64)), cmap='gray')... plt.axis('off') ...\n```", "```py\nIn [8]: from sklearn.model_selection import train_test_split\n...     X_train, X_test, y_train, y_test = train_test_split(\n...         X, y, random_state=21\n...     )\n```", "```py\nIn [9]: import cv2\n...     rtree = cv2.ml.RTrees_create()\n```", "```py\nIn [10]: n_trees = 50\n...      eps = 0.01\n...      criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n...                  n_trees, eps)\n...      rtree.setTermCriteria(criteria)\n```", "```py\nIn [10]: rtree.setMaxCategories(len(np.unique(y)))\n```", "```py\nIn [11]: rtree.setMinSampleCount(2)\n```", "```py\nIn [12]: rtree.setMaxDepth(1000)\n```", "```py\nIn [13]: rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);\n```", "```py\nIn [13]: rtree.getMaxDepth()\nOut[13]: 25\n```", "```py\nIn [14]: _, y_hat = tree.predict(X_test)\nIn [15]: from sklearn.metrics import accuracy_score\n...      accuracy_score(y_test, y_hat)\nOut[15]: 0.87\n```", "```py\nIn [16]: from sklearn.tree import DecisionTreeClassifier\n...      tree = DecisionTreeClassifier(random_state=21, max_depth=25)\n...      tree.fit(X_train, y_train)\n...      tree.score(X_test, y_test)\nOut[16]: 0.46999999999999997\n```", "```py\nIn [18]: num_trees = 1000\n... eps = 0.01\n... criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS,\n... num_trees, eps)\n... rtree.setTermCriteria(criteria)\n... rtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train);\n... _, y_hat = rtree.predict(X_test)\n... accuracy_score(y_test, y_hat)\nOut[18]: 0.94\n```", "```py\nIn [1]: img_bgr = cv2.imread('data/lena.jpg', cv2.IMREAD_COLOR)\n...     img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n```", "```py\nIn [2]: import cv2\n...     filename = 'data/haarcascade_frontalface_default.xml'\n...     face_cascade = cv2.CascadeClassifier(filename)\n```", "```py\nIn [3]: faces = face_cascade.detectMultiScale(img_gray, 1.1, 5)\n```", "```py\nIn [4]: color = (255, 0, 0)\n...     thickness = 2\n...     for (x, y, w, h) in faces:\n...         cv2.rectangle(img_bgr, (x, y), (x + w, y + h),\n...                       color, thickness)\n```", "```py\nIn [5]: import matplotlib.pyplot as plt\n...     %matplotlib inline\n...     plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB));\n```", "```py\nIn [6]: from sklearn.ensemble import AdaBoostClassifier...     ada = AdaBoostClassifier(n_estimators=50,...                              random_state=456)\n```", "```py\nIn [7]: from sklearn.datasets import load_breast_cancer...     cancer = load_breast_cancer()...     X = cancer.data...     y = cancer.targetIn [8]: from sklearn.model_selection import train_test_split...     X_train, X_test, y_train, y_test = train_test_split(...         X, y, random_state=456...     )\n```", "```py\nIn [9]: ada.fit(X_train, y_train)...     ada.score(X_test, y_test)\n```", "```py\nIn [1]: from sklearn.datasets import load_breast_cancer\n...     cancer = load_breast_cancer()\n...     X = cancer.data\n...     y = cancer.target\nIn [2]: from sklearn.model_selection import train_test_split\n...     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)\n```", "```py\n In [3]: from sklearn.linear_model import LogisticRegression\n...     model1 = LogisticRegression(random_state=13)\n In [4]: from sklearn.naive_bayes import GaussianNB\n...     model2 = GaussianNB()\nIn [5]: from sklearn.ensemble import RandomForestClassifier\n...     model3 = RandomForestClassifier(random_state=13)\n```", "```py\nIn [6]: from sklearn.ensemble import VotingClassifier\n...     vote = VotingClassifier(estimators=[('lr', model1),\n...                                ('gnb', model2),('rfc', model3)],voting='hard')\n```", "```py\nIn [7]: vote.fit(X_train, y_train)\n...     vote.score(X_test, y_test)\nOut[7]: 0.95104895104895104\n```", "```py\nIn [8]: model1.fit(X_train, y_train)\n...     model1.score(X_test, y_test)\nOut[8]: 0.94405594405594406\n```", "```py\nIn [9]:  model2.fit(X_train, y_train)\n...      model2.score(X_test, y_test)\nOut[9]:  0.93006993006993011\n```", "```py\nIn [10]: model3.fit(X_train, y_train)\n... model3.score(X_test, y_test)\nOut[10]: 0.94405594405594406\n```"]