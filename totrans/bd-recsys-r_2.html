<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Data Mining Techniques Used in Recommender Systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Data Mining Techniques Used in Recommender Systems</h1></div></div></div><p>Though the primary objective of this book is to build recommender systems, a walkthrough of the commonly used data-mining techniques is a necessary step before jumping into building recommender systems. In this chapter, you will learn about popular data preprocessing techniques, data-mining techniques, and data-evaluation techniques commonly used in recommender systems. The first section of the chapter tells you how a data analysis problem is solved, followed by data preprocessing steps such as similarity measures and dimensionality reduction. The next section of the chapter deals with data mining techniques and their evaluation techniques.</p><p>Similarity measures include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Euclidean distance</li><li class="listitem" style="list-style-type: disc">Cosine distance</li><li class="listitem" style="list-style-type: disc">Pearson correlation</li></ul></div><p>Dimensionality reduction techniques include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Principal component analysis</li></ul></div><p>Data-mining techniques include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">k-means clustering</li><li class="listitem" style="list-style-type: disc">Support vector machine</li><li class="listitem" style="list-style-type: disc">Ensemble methods, such as bagging, boosting, and random forests</li></ul></div><div class="section" title="Solving a data analysis problem"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Solving a data analysis problem</h1></div></div></div><p>Any data<a class="indexterm" id="id13"/> analysis problem involves a series of steps such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identifying a business problem.</li><li class="listitem" style="list-style-type: disc">Understanding the problem domain with the help of a domain expert.</li><li class="listitem" style="list-style-type: disc">Identifying data <a class="indexterm" id="id14"/>sources and data variables suitable for the analysis.</li><li class="listitem" style="list-style-type: disc">Data preprocessing or a cleansing step, such as identifying missing values, quantitative and qualitative variables and transformations, and so on.</li><li class="listitem" style="list-style-type: disc">Performing exploratory analysis to understand the data, mostly through visual graphs such as box plots or histograms.</li><li class="listitem" style="list-style-type: disc">Performing basic statistics such as mean, median, modes, variances, standard deviations, correlation among the variables, and covariance to understand the nature of the data.</li><li class="listitem" style="list-style-type: disc">Dividing the data into training and testing datasets and running a model using machine-learning algorithms with training datasets, using cross-validation techniques.</li><li class="listitem" style="list-style-type: disc">Validating the model using the test data to evaluate the model on the new data. If needed, improve the model based on the results of the validation step.</li><li class="listitem" style="list-style-type: disc">Visualize the results and deploy the model for real-time predictions.</li></ul></div><p>The following image displays the resolution to a data analysis problem:</p><div class="mediaobject"><img alt="Solving a data analysis problem" src="graphics/B03888_02_01.jpg"/><div class="caption"><p>Data analysis steps</p></div></div></div></div>
<div class="section" title="Data preprocessing techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Data preprocessing techniques</h1></div></div></div><p>Data preprocessing is a crucial step for any data analysis problem. The model's accuracy depends mostly on the <a class="indexterm" id="id15"/>quality of the data. In general, any data preprocessing step involves data cleansing, transformations, identifying missing values, and how they should be treated. Only the preprocessed data can be fed into a machine-learning algorithm. In this section, we will focus mainly on data preprocessing techniques. These techniques include similarity measurements (such as Euclidean distance, Cosine distance, and Pearson coefficient) and dimensionality-reduction techniques, such as <a class="indexterm" id="id16"/>
<span class="strong"><strong>Principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>), which are widely used in recommender systems. Apart from PCA, we have<a class="indexterm" id="id17"/> <span class="strong"><strong>singular value decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>), subset feature selection methods to reduce the dimensions of the dataset, but we limit our study to PCA.</p><div class="section" title="Similarity measures"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec08"/>Similarity measures</h2></div></div></div><p>As discussed in the <a class="indexterm" id="id18"/>previous chapter, every recommender system works on the concept of similarity between items or users. In this <a class="indexterm" id="id19"/>section, let's explore some similarity measures such as Euclidian distance, Cosine distance, and Pearson correlation.</p><div class="section" title="Euclidian distance"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec01"/>Euclidian distance</h3></div></div></div><p>The simplest<a class="indexterm" id="id20"/> technique for calculating the similarity between two items<a class="indexterm" id="id21"/> is by calculating its Euclidian distance. The Euclidean distance between two points/objects (point <span class="emphasis"><em>x</em></span> and point <span class="emphasis"><em>y</em></span>) in a dataset is defined by the following equation:</p><div class="mediaobject"><img alt="Euclidian distance" src="graphics/B03888_02_02.jpg"/></div><p>In this equation, (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>) are two consecutive data points, and <span class="emphasis"><em>n</em></span> is the number of attributes for the dataset.</p><p>R script to calculate the Euclidean distance is as follows:</p><div class="informalexample"><pre class="programlisting">x1 &lt;- rnorm(30)
x2 &lt;- rnorm(30)
Euc_dist = dist(rbind(x1,x2) ,method="euclidean")</pre></div></div><div class="section" title="Cosine distance"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec02"/>Cosine distance</h3></div></div></div><p>Cosine similarity is a measure of similarity between two vectors of an inner product space that<a class="indexterm" id="id22"/> measures the cosine of the angle between them. Cosine <a class="indexterm" id="id23"/>similarity is given by this equation:</p><div class="mediaobject"><img alt="Cosine distance" src="graphics/B03888_02_03.jpg"/></div><p>R script to calculate the cosine distance is as follows:</p><div class="informalexample"><pre class="programlisting">vec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )
vec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 )
library(lsa)
cosine(vec1,vec2)</pre></div><p>In this equation, <span class="emphasis"><em>x</em></span> is the matrix containing all variables in a dataset. The <code class="literal">cosine</code> function is available in the <code class="literal">lsa</code> package.</p></div><div class="section" title="Pearson correlation"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec03"/>Pearson correlation</h3></div></div></div><p>Similarity between two products can also be given by the correlation existing between their variables. Pearson's<a class="indexterm" id="id24"/> correlation coefficient is a popular<a class="indexterm" id="id25"/> correlation coefficient calculated between two variables as the covariance of the two variables divided by the product of their standard deviations. This is given by <span class="emphasis"><em>ƿ</em></span>
<span class="emphasis"><em> (rho)</em></span>:</p><div class="mediaobject"><img alt="Pearson correlation" src="graphics/B03888_02_04.jpg"/></div><p>R script is given by these lines of code:</p><div class="informalexample"><pre class="programlisting">Coef = cor(mtcars, method="pearson")
where mtcars is the dataset</pre></div><p>Empirical studies showed that Pearson coefficient outperformed other similarity measures for user-based collaborative filtering recommender systems. The studies also show that Cosine similarity consistently performs well in item-based collaborative filtering.</p></div></div><div class="section" title="Dimensionality reduction"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec09"/>Dimensionality reduction</h2></div></div></div><p>One of the most commonly faced problems while building recommender systems is high-dimensional and sparse data. At many times, we face a situation where we have a large set of features<a class="indexterm" id="id26"/> and fewer data points. In such situations, when we fit a model to the dataset, the predictive power of the model will be lower. This scenario is often termed as the curse of dimensionality. In<a class="indexterm" id="id27"/> general, adding more data points or decreasing the feature space, also known as dimensionality reduction, often reduces the effects of the curse of dimensionality. In this chapter, we will discuss PCA, a popular dimensionality reduction technique to reduce the effects of the curse of dimensionality.</p><div class="section" title="Principal component analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec04"/>Principal component analysis</h3></div></div></div><p>Principal component analysis is a classical statistical technique for dimensionality reduction. The PCA<a class="indexterm" id="id28"/> algorithm transforms the data with high-dimensional space to a space with fewer dimensions. The algorithm linearly transforms m-dimensional input space to n-dimensional (<span class="emphasis"><em>n&lt;m</em></span>) output space, with the objective to minimize the amount of information/variance lost by discarding (<span class="emphasis"><em>m-n</em></span>) dimensions. PCA allows us to discard the variables/features that<a class="indexterm" id="id29"/> have less variance.</p><p>Technically speaking, PCA uses orthogonal projection of highly correlated variables to a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables. This linear transformation is defined in such a way that the first principal component has the largest possible variance. It accounts for as much of the variability in the data as possible by considering highly correlated features. Each succeeding component in turn has the highest variance using the features that are less correlated with the first principal component and that are orthogonal to the preceding component.</p><p>Let's understand this in simple terms. Assume we have three dimensional data space with two features more correlated with each other than with the third. We now want to reduce the data to two-dimensional space using PCA. The first principal component is created in such a way that it explains maximum variance using the two correlated variables along the data. In the following graph, the first principal component (bigger line) is along the data explaining most variance. To choose the second principal component, we need to choose another line that has the highest variance, is uncorrelated, and is orthogonal to the first principal component. The implementation and technical details of PCA are beyond the scope of this book, so we will discuss how it is used in R.</p><div class="mediaobject"><img alt="Principal component analysis" src="graphics/B03888_02_05.jpg"/></div><p>We will illustrate<a class="indexterm" id="id30"/> PCA using the USArrests dataset. The USArrests dataset contains crime-related<a class="indexterm" id="id31"/> statistics, such as Assault, Murder, Rape, and UrbanPop per 100,000 residents in 50 states in the US:</p><div class="informalexample"><pre class="programlisting">#PCA
data(USArrests)
head(states)
[1] "Alabama"    "Alaska"     "Arizona"    "Arkansas"   "California" "Colorado"

names(USArrests)
[1] "Murder"   "Assault"  "UrbanPop" "Rape"

#let us use apply() to the USArrests dataset row wise to calculate the variance to see how each variable is varying
apply(USArrests , 2, var)

Murder    Assault   UrbanPop       Rape
  18.97047 6945.16571  209.51878   87.72916
#We observe that Assault has the most variance. It is important to note at this point that

#Scaling the features is a very step while applying PCA.

#Applying PCA after scaling the feature as below
pca =prcomp(USArrests , scale =TRUE)

pca</pre></div><p><span class="strong"><strong>Standard deviations</strong></span>:</p><div class="informalexample"><pre class="programlisting">[1] 1.5748783 0.9948694 0.5971291 0.4164494</pre></div><p><span class="strong"><strong>Rotation</strong></span>:</p><div class="informalexample"><pre class="programlisting">                PC1        PC2        PC3         PC4
Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
Rape     -0.5434321 -0.1673186  0.8177779  0.08902432

#Now lets us understand the components of pca output.

names(pca)
[1] "sdev"     "rotation" "center"   "scale"    "x"

#Pca$rotation  contains the principal component loadings matrix which explains

#proportion of each variable along each principal component.

#now let us learn interpreting the results of pca using biplot graph. Biplot is used to how the proportions of each variable along the two principal components.

#below code changes the directions of the biplot, if we donot include the below two lines the plot will be mirror image to the below one.
pca$rotation=-pca$rotation
pca$x=-pca$x
biplot (pca , scale =0)</pre></div><p>The output<a class="indexterm" id="id32"/> of the preceding<a class="indexterm" id="id33"/> code is as follows:</p><div class="mediaobject"><img alt="Principal component analysis" src="graphics/B03888_02_06.jpg"/></div><p>In the preceding<a class="indexterm" id="id34"/> image, known as a biplot, we can see the two principal components (<span class="strong"><strong>PC1</strong></span> and <span class="strong"><strong>PC2</strong></span>) of the USArrests dataset. The red arrows represent the loading vectors, which represent how<a class="indexterm" id="id35"/> the feature space varies along the principal component vectors.</p><p>From the plot, we can see that the first principal component vector, <span class="strong"><strong>PC1</strong></span>, more or less places equal weight on three features: <span class="strong"><strong>Rape</strong></span>, <span class="strong"><strong>Assault</strong></span>, and <span class="strong"><strong>Murder</strong></span>. This means that these three features are more correlated with each other than the <span class="strong"><strong>UrbanPop</strong></span> feature. In the second principal component, <span class="strong"><strong>PC2</strong></span> places more weight on <span class="strong"><strong>UrbanPop</strong></span> than the remaining 3 features are less correlated with them.</p></div></div></div>
<div class="section" title="Data mining techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Data mining techniques</h1></div></div></div><p>In this section, we<a class="indexterm" id="id36"/> will look at commonly used data-mining algorithms, such as k-means clustering, support vector machines, decision trees, bagging, boosting, and random forests. Evaluation techniques such as cross validation, regularization, confusion matrix, and model comparison are explained in brief.</p></div>
<div class="section" title="Cluster analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Cluster analysis</h1></div></div></div><p>Cluster analysis<a class="indexterm" id="id37"/> is the process of grouping objects together in a way that objects in one group are more similar than objects in other groups.</p><p>An example would be identifying and grouping clients with similar booking activities on a travel portal, as shown in the following figure.</p><p>In the preceding example, each group is called a cluster, and each member (data point) of the cluster behaves in a manner similar to its group members.</p><div class="mediaobject"><img alt="Cluster analysis" src="graphics/B03888_02_07.jpg"/><div class="caption"><p>Cluster analysis</p></div></div><p>Cluster analysis is an unsupervised learning method. In supervised methods, such as regression analysis, we have input variables and response variables. We fit a statistical model to the input variables to predict the response variable. Whereas in unsupervised learning methods, however, we do not have any response variable to predict; we only have input variables. Instead of fitting a model to the input variables to predict the response variable, we just try to find patterns within the dataset. There are three popular clustering algorithms: hierarchical cluster analysis, <span class="emphasis"><em>k</em></span>-means cluster analysis, and two-step cluster analysis. In the following section, we will learn about <span class="emphasis"><em>k</em></span>-means clustering.</p><div class="section" title="Explaining the k-means cluster algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>Explaining the k-means cluster algorithm</h2></div></div></div><p><span class="emphasis"><em>k</em></span>-means is an <a class="indexterm" id="id38"/>unsupervised, iterative algorithm where k is<a class="indexterm" id="id39"/> the number of clusters to be formed from the data. Clustering is achieved in two steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Cluster assignment step</strong></span>: In this step, we randomly choose two cluster points (red dot and green dot) and<a class="indexterm" id="id40"/> assign each data point to the cluster point that is closer to it (top part of the following image).</li><li class="listitem"><span class="strong"><strong>Move centroid step</strong></span>: In this step, we take the average of the points of all the examples in each group and<a class="indexterm" id="id41"/> move the centroid to the new position, that is, mean position calculated (bottom part of the following image).</li></ol></div><p>The preceding steps are repeated until all the data points are grouped into two groups and the mean of the data points after moving the centroid doesn't change.</p><div class="mediaobject"><img alt="Explaining the k-means cluster algorithm" src="graphics/B03888_02_08_new.jpg"/><div class="caption"><p>Steps of cluster analysis</p></div></div><p>The preceding image shows how a clustering algorithm works on data to form clusters. See the R implementation of <span class="emphasis"><em>k</em></span>-means clustering on iris dataset as follows:</p><div class="informalexample"><pre class="programlisting">#k-means clustering
library(cluster)
data(iris)
iris$Species = as.numeric(iris$Species)
kmeans&lt;- kmeans(x=iris, centers=5)
clusplot(iris,kmeans$cluster, color=TRUE, shade=TRUE,labels=13, lines=0)</pre></div><p>The output of the preceding code is as follows:</p><div class="mediaobject"><img alt="Explaining the k-means cluster algorithm" src="graphics/B03888_02_09.jpg"/><div class="caption"><p>Cluster analysis results</p></div></div><p>The preceding<a class="indexterm" id="id42"/> image shows the formation of clusters on the iris data, and the clusters account for 95 percent of the data. In the preceding example, the number of clusters of <span class="emphasis"><em>k</em></span> value is selected using the <code class="literal">elbow</code> method, as shown here:</p><div class="informalexample"><pre class="programlisting">library(cluster)
library(ggplot2)
data(iris)
iris$Species = as.numeric(iris$Species)
cost_df &lt;- data.frame()
for(i in 1:100){
kmeans&lt;- kmeans(x=iris, centers=i, iter.max=50)
cost_df&lt;- rbind(cost_df, cbind(i, kmeans$tot.withinss))
}
names(cost_df) &lt;- c("cluster", "cost")
#Elbow method to identify the idle number of Cluster
#Cost plot
ggplot(data=cost_df, aes(x=cluster, y=cost, group=1)) +
theme_bw(base_family="Garamond") +
geom_line(colour = "darkgreen") +
theme(text = element_text(size=20)) +
ggtitle("Reduction In Cost For Values of 'k'\n") +
xlab("\nClusters") +
ylab("Within-Cluster Sum of Squares\n")</pre></div><p>The following image shows the cost reduction for <span class="emphasis"><em>k</em></span> values:</p><div class="mediaobject"><img alt="Explaining the k-means cluster algorithm" src="graphics/B03888_02_10_new.jpg"/></div><p>From the <a class="indexterm" id="id43"/>preceding figure, we can observe that the direction of the cost function is changed at cluster number 5. Hence, we choose 5 as our number of clusters k. Since the number of optimal clusters is found at the elbow of the graph, we call it the elbow method.</p><div class="section" title="Support vector machine"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec05"/>Support vector machine</h3></div></div></div><p>Support vector machine algorithms are a form of supervised learning algorithms employed<a class="indexterm" id="id44"/> to solve classification problems. SVM is generally treated as one of the best algorithms to deal with classification problems. Given a set of training examples, where each data point falls into one of two categories, an SVM training algorithm builds a model that assigns new data points into one<a class="indexterm" id="id45"/> category or the other. This model is a representation of the examples as a points in space, mapped so that the examples of the separate categories are divided by a margin that is as wide as possible, as shown in the following image. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on. In this section, we will go through an overview and implementation of SVMs without going into mathematical details.</p><p>When SVM is applied to a p-dimensional dataset, the data is mapped to a <span class="emphasis"><em>p-1</em></span> dimensional hyperplane, and the algorithm finds a clear boundary with a sufficient margin between classes. Unlike other classification algorithms that also create a separating boundary to<a class="indexterm" id="id46"/> classify data points, SVM tries to choose a boundary that has the maximum margin to separate the classes, as shown<a class="indexterm" id="id47"/> in the following image:</p><div class="mediaobject"><img alt="Support vector machine" src="graphics/B03888_02_11.jpg"/></div><p>Consider a two-dimensional dataset having two classes, as shown in the preceding image. Now, when the SVM algorithm is applied, first it checks whether a one-dimensional hyperplane exists to map all the data points. If the hyperplane exists, the linear classifier creates a decision boundary with a margin to separate the classes. In the preceding image, the thick red line is the decision boundary, and the thinner blue and red lines are the margins of each class from the boundary. When new test data is used to predict the class, the new data falls into one of the two classes.</p><p>Here are some key points to be noted:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Though an infinite number of hyperplanes can be created, SVM chooses only one hyperplane that has the maximum margin, that is, the separating hyperplane that is farthest from the training observations.</li><li class="listitem" style="list-style-type: disc">This classifier is only dependent on the data points that lie on the margins of the hyperplane, that is, on thin margins in the image, but not on other observations in the dataset. These points are called support vectors.</li><li class="listitem" style="list-style-type: disc">The decision boundary is affected only by the support vectors but not by other observations located away from the boundaries. If we change the data points other than the support vectors, there would not be any effect on the decision boundary. However, if the support vectors are changed, the decision boundary changes.</li><li class="listitem" style="list-style-type: disc">A large margin on the training data will also have a large margin on the test data to classify the test data correctly.</li><li class="listitem" style="list-style-type: disc">Support vector machines also perform well with non-linear datasets. In this case, we use radial kernel functions.</li></ul></div><p>See the R<a class="indexterm" id="id48"/> implementation of SVM on the iris dataset in the following code snippet. We used the <code class="literal">e1071</code> package to run SVM. In R, the <code class="literal">SVM()</code> function contains the implementation<a class="indexterm" id="id49"/> of support vector machines present in the <code class="literal">e1071</code> package.</p><p>Now, we will see that the <code class="literal">SVM()</code> method is called with the <code class="literal">tune()</code> method, which does cross validation and runs the model on different values of the cost parameters.</p><p>The cross-validation method is used to evaluate the accuracy of the predictive model before testing on future unseen data:</p><div class="informalexample"><pre class="programlisting">  #SVM
library(e1071)
data(iris)
sample = iris[sample(nrow(iris)),]
train = sample[1:105,]
test = sample[106:150,]
tune =tune(svm,Species~.,data=train,kernel ="radial",scale=FALSE,ranges =list(cost=c(0.001,0.01,0.1,1,5,10,100)))
tune$best.model</pre></div><p><span class="strong"><strong>Call</strong></span>:</p><div class="informalexample"><pre class="programlisting">best.tune(method = svm, train.x = Species ~ ., data = train, ranges = list(cost = c(0.001,
    0.01, 0.1, 1, 5, 10, 100)), kernel = "radial", scale = FALSE)</pre></div><p><span class="strong"><strong>Parameters</strong></span>:</p><div class="informalexample"><pre class="programlisting">   SVM-Type:  C-classification
 SVM-Kernel:  radial
       cost:  10
      gamma:  0.25

Number of Support Vectors:  25

summary(tune)

Parameter tuning of 'svm':
- sampling method: 10-fold cross validation
- best parameters:
 cost
   10
- best performance: 0.02909091
- Detailed performance results:
   cost      error dispersion
1 1e-03 0.72909091 0.20358585
2 1e-02 0.72909091 0.20358585
3 1e-01 0.04636364 0.08891242
4 1e+00 0.04818182 0.06653568
5 5e+00 0.03818182 0.06538717
6 1e+01 0.02909091 0.04690612
7 1e+02 0.07636364 0.08679584

model =svm(Species~.,data=train,kernel ="radial",cost=10,scale=FALSE)
// cost =10 is chosen from summary result of tune variable</pre></div><p>The <code class="literal">tune$best.model</code> object tells us that the model works best with the cost parameter as <code class="literal">10</code> and total <a class="indexterm" id="id50"/>number of support <a class="indexterm" id="id51"/>vectors as <code class="literal">25</code>:</p><div class="informalexample"><pre class="programlisting">pred = predict(model,test)</pre></div></div></div></div>
<div class="section" title="Decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Decision trees</h1></div></div></div><p>Decision trees are a <a class="indexterm" id="id52"/>simple, fast, tree-based supervised learning algorithm to solve classification problems. Though not very accurate when compared to other logistic regression methods, this algorithm comes in handy while dealing with recommender systems.</p><p>We define the decision trees with an example. Imagine a situation where you have to predict the class of flower based on its features such as petal length, petal width, sepal length, and sepal width. We will apply the decision tree methodology to solve this problem:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Consider the entire <a class="indexterm" id="id53"/>data at the start of the algorithm.</li><li class="listitem">Now, choose a suitable question/variable to divide the data into two parts. In our case, we chose to divide the data based on petal length <span class="emphasis"><em>&gt; 2.45</em></span> and <span class="emphasis"><em>&lt;= 2.45</em></span>. This separates flower class <code class="literal">setosa</code> from the rest of the classes.</li><li class="listitem">Now, further divide the data having petal length <span class="emphasis"><em>&gt;2.45</em></span>, based on the same variable with petal length <span class="emphasis"><em>&lt; 4.5</em></span> and <span class="emphasis"><em>&gt;= 4.5</em></span>, as shown in the following image.</li><li class="listitem">This splitting of the data will be further divided by narrowing down the data space until we reach a point where all the bottom points represent the response variables or where further logical split cannot be done on the data.</li></ol></div><p>In the following<a class="indexterm" id="id54"/> decision tree image, we have one root node, four internal nodes where data split occurred, and five terminal nodes where data split cannot be done any further. They are defined as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Petal.Length &lt;2.45</strong></span> as root node</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Petal.Length &lt;4.85</strong></span>, <span class="strong"><strong>Sepal.Length &lt;5.15,</strong></span> and <span class="strong"><strong>Petal.Width &lt;1.75</strong></span> are called internal nodes</li><li class="listitem" style="list-style-type: disc">Final nodes having the class of the flowers are called terminal nodes</li><li class="listitem" style="list-style-type: disc">The lines connecting the nodes are called the branches of the tree</li></ul></div><p>While predicting responses on new data using the previously built model, each new data point is taken through each node, a question is asked, and a logical path is taken to reach its logical class, as shown in the following figure:</p><div class="mediaobject"><img alt="Decision trees" src="graphics/B03888_02_12.jpg"/></div><p>See the <a class="indexterm" id="id55"/>decision tree implementation in R on the iris dataset using the tree package available from <span class="strong"><strong>Comprehensive R Archive Network</strong></span> (<span class="strong"><strong>CRAN</strong></span>).</p><p>The summary of the mode is given here. It tells us that the misclassification rate is 0.0381, indicating that the model is accurate:</p><div class="informalexample"><pre class="programlisting">library(tree)
data(iris)
sample = iris[sample(nrow(iris)),]
train = sample[1:105,]
test = sample[106:150,]
model = tree(Species~.,train)
summary(model)</pre></div><p><span class="strong"><strong>Classification tree</strong></span>:</p><div class="informalexample"><pre class="programlisting">tree(formula = Species ~ ., data = train, x = TRUE, y = TRUE)
Variables actually used in tree construction:
[1] "Petal.Length" "Sepal.Length" "Petal.Width"
Number of terminal nodes:  5
Residual mean deviance:  0.1332 = 13.32 / 100
Misclassification error rate: 0.0381 = 4 / 105 '
//plotting the decision tree
plot(model)text(model)
pred = predict(model,test[,-5],type="class")
&gt; pred
 [1] setosa     setosa     virginica  setosa     setosa     setosa     versicolor
 [8] virginica  virginica  setosa     versicolor versicolor virginica  versicolor
[15] virginica  virginica  setosa     virginica  virginica  versicolor virginica
[22] versicolor setosa     virginica  setosa     versicolor virginica  setosa    
[29] versicolor versicolor versicolor virginica  setosa     virginica  virginica
[36] versicolor setosa     versicolor setosa     versicolor versicolor setosa    
[43] versicolor setosa     setosa    
Levels: setosa versicolor virginica</pre></div></div>
<div class="section" title="Ensemble methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Ensemble methods</h1></div></div></div><p>In data mining, we<a class="indexterm" id="id56"/> use ensemble methods, which means using multiple learning algorithms to obtain better predictive results than applying any single learning algorithm on any statistical problem. This section will provide an overview of popular ensemble methods such as bagging, boosting, and random forests</p><div class="section" title="Bagging"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Bagging</h2></div></div></div><p>Bagging is also known as<a class="indexterm" id="id57"/> Bootstrap aggregating. It is designed to improve the stability and accuracy of <a class="indexterm" id="id58"/>machine-learning algorithms. It helps avoid over fitting and reduces variance. This is mostly used with decision trees.</p><p>Bagging involves randomly generating Bootstrap samples from the dataset and trains the models individually. Predictions are then made by aggregating or averaging all the response variables:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For example, consider a dataset (<span class="emphasis"><em>Xi, Yi</em></span>), where <span class="emphasis"><em>i=1 …n</em></span>, contains <span class="emphasis"><em>n</em></span> data points.</li><li class="listitem" style="list-style-type: disc">Now, randomly select B samples with replacements from the original dataset using Bootstrap technique.</li><li class="listitem" style="list-style-type: disc">Next, train<a class="indexterm" id="id59"/> the B samples with regression/classification models independently. Then, predictions are made on the test set by averaging the responses from all the B models generated in the case of regression. Alternatively, the most often occurring class among B samples is generated in the case of classification.</li></ul></div></div><div class="section" title="Random forests"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Random forests</h2></div></div></div><p>Random forests are<a class="indexterm" id="id60"/> improvised supervised algorithms than bootstrap aggregation or bagging methods, though they are built on a similar approach. Unlike selecting all the variables in all the B samples generated using the Bootstrap technique in bagging, we select only a few predictor variables randomly from the total variables for each of the B samples. Then, these samples are trained with the models. Predictions are made by averaging the result of each model. The number of predictors in each sample is decided using the formula <span class="emphasis"><em>m = √p</em></span>, where <span class="emphasis"><em>p</em></span> is the total variable count in the original dataset.</p><p>Here are some key notes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">This approach removes the condition of dependency of strong predictors in the dataset as we intentionally select fewer variables than all the variables for every iteration</li><li class="listitem" style="list-style-type: disc">This approach also de-correlates variables, resulting in less variability in the model and, hence, more reliability</li></ul></div><p>Refer to the R implementation of random forests on the iris dataset using the <code class="literal">randomForest</code> package available from CRAN:</p><div class="informalexample"><pre class="programlisting">#randomForest
library(randomForest)
data(iris)
sample = iris[sample(nrow(iris)),]
train = sample[1:105,]
test = sample[106:150,]
model =randomForest(Species~.,data=train,mtry=2,importance =TRUE,proximity=TRUE)
model</pre></div><p><span class="strong"><strong>Call</strong></span>:</p><div class="informalexample"><pre class="programlisting"> randomForest(formula = Species ~ ., data = train, mtry = 2, importance = TRUE,      proximity = TRUE)
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 5.71%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         40          0         0  0.00000000
versicolor      0         28         3  0.09677419
virginica       0          3        31  0.08823529

pred = predict(model,newdata=test[,-5])
pred
pred
       119         77         88         90         51         20         96
 virginica versicolor versicolor versicolor versicolor     setosa versicolor
         1          3        118        127          6        102          5
    setosa     setosa  virginica  virginica     setosa  virginica     setosa
        91          8         23        133         17         78         52
versicolor     setosa     setosa  virginica     setosa  virginica versicolor
        63         82         84        116         70         50        129
versicolor versicolor  virginica  virginica versicolor     setosa  virginica
       150         34          9        120         41         26        121
 virginica     setosa     setosa  virginica     setosa     setosa  virginica
       145        138         94          4        104         81        122
 virginica  virginica versicolor     setosa  virginica versicolor  virginica
        18        105        100
    setosa  virginica versicolor
Levels: setosa versicolor virginica</pre></div></div><div class="section" title="Boosting"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Boosting</h2></div></div></div><p>Unlike with bagging, where multiple copies of Bootstrap samples are created, a new model is fitted for<a class="indexterm" id="id61"/> each copy of the dataset, and all the individual models are combined to create a single predictive model, each new model is built using information from previously built models. Boosting can be understood as an iterative method involving two steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A new model is built on the residuals of previous models instead of the response variable</li><li class="listitem" style="list-style-type: disc">Now, the residuals are calculated from this model and updated to the residuals used in the previous step</li></ul></div><p>The preceding two <a class="indexterm" id="id62"/>steps are repeated for multiple iterations, allowing each new model to learn from its previous mistakes, thereby improving the model accuracy:</p><div class="informalexample"><pre class="programlisting">#Boosting in R
library(gbm)
data(iris)
sample = iris[sample(nrow(iris)),]
train = sample[1:105,]
test = sample[106:150,]
model = gbm(Species~.,data=train,distribution="multinomial",n.trees=5000,interaction.depth=4)
summary(model)</pre></div><div class="mediaobject"><img alt="Boosting" src="graphics/B03888_02_13.jpg"/></div><p>The output of the preceding code is as follows:</p><div class="mediaobject"><img alt="Boosting" src="graphics/B03888_02_14.jpg"/></div><p>In the following code snippet, the output value for the <code class="literal">predict()</code> function is used in the <code class="literal">apply()</code> function to pick the response with the highest probability among each row in the <code class="literal">pred</code> matrix. The<a class="indexterm" id="id63"/> resultant output from the <code class="literal">apply()</code> function is the prediction for the response variable:</p><div class="informalexample"><pre class="programlisting">//the preceding summary states the relative importance of the variables of the model.

pred = predict(model,newdata=test[,-5],n.trees=5000)

pred[1:5,,]
        setosa versicolor virginica
[1,]  5.630363  -2.947531 -5.172975
[2,]  5.640313  -3.533578 -5.103582
[3,] -5.249303   3.742753 -3.374590
[4,] -5.271020   4.047366 -3.770332
[5,] -5.249324   3.819050 -3.439450

//pick the response with the highest probability from the resulting pred matrix, by doing apply(.., 1, which.max) on the vector output from prediction.
p.pred &lt;- apply(pred,1,which.max)
p.pred
[1] 1 1 3 3 2 2 3 1 3 1 3 2 2 1 2 3 2 2 3 3 1 1 3 1 3 3 3 1 1 2 2 2 2 2 2 2 1 1 3 1 2
[42] 1 3 2 3</pre></div></div></div>
<div class="section" title="Evaluating data-mining algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Evaluating data-mining algorithms</h1></div></div></div><p>In the previous sections, we have seen various data-mining techniques used in recommender systems. In this section, you will learn how to evaluate models built using data-mining techniques. The ultimate goal for any data analytics model is to perform well on future data. This <a class="indexterm" id="id64"/>objective could be achieved only if we build a model that is efficient and robust during the development stage.</p><p>While evaluating any model, the most important things we need to consider are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Whether the model is over fitting or under fitting</li><li class="listitem" style="list-style-type: disc">How well the model fits the future data or test data</li></ul></div><p>Under fitting, also known as bias, is a scenario when the model doesn't even perform well on training data. This means that we fit a less robust model to the data. For example, say the data is distributed non-linearly and we are fitting the data with a linear model. From the following image, we see that data is non-linearly distributed. Assume that we have fitted a linear model (orange line). In this case, during the model building stage itself, the predictive power will be low.</p><p>Over fitting is a scenario when the model performs well on training data, but does really bad on test data. This scenario arises when the model memorizes the data pattern rather than learning from data. For example, say the data is distributed in a non-linear pattern, and we have fitted a complex model, shown using the green line. In this case, we observe that the model is fitted very close to the data distribution, taking care of all the ups and downs. In this case, the model is most likely to fail on previously unseen data.</p><div class="mediaobject"><img alt="Evaluating data-mining algorithms" src="graphics/B03888_02_15.jpg"/></div><p>The preceding image shows simple, complex, and appropriate fitted models' training data. The green fit represents overfitting, the orange line represents underfitting, the black and blue lines represent the<a class="indexterm" id="id65"/> appropriate model, which is a trade-off between underfit and overfit.</p><p>Any fitted model is evaluated to avoid previously mentioned scenarios using cross validation, regularization, pruning, model comparisons, ROC curves, confusion matrices, and so on .</p><p><span class="strong"><strong>Cross validation</strong></span>: This is a<a class="indexterm" id="id66"/> very popular technique for model evaluation for almost all models. In this technique, we divide the data into two datasets: a training dataset and a test dataset. The model is built using the training dataset and evaluated using the test dataset. This process is repeated many times. The test errors are calculated for every iteration. The averaged test error is calculated to generalize the model accuracy at the end of all the iterations.</p><p><span class="strong"><strong>Regularization</strong></span>: In this technique, the data variables are penalized to reduce the complexity of the model <a class="indexterm" id="id67"/>with the objective to minimize the cost function. There are two most popular regularization techniques: ridge regression and lasso regression. In both techniques, we try to reduce the variable co-efficient to zero. Thus, a smaller number of variables will fit the data optimally.</p><p><span class="strong"><strong>Confusion matrix</strong></span>: This technique is popularly used in evaluating a classification model. We build a confusion <a class="indexterm" id="id68"/>matrix using the results of the model. We calculate precision and recall/sensitivity/specificity to evaluate the model.</p><p><span class="strong"><strong>Precision</strong></span>: This is the probability <a class="indexterm" id="id69"/>whether the truly classified records are relevant.</p><p><span class="strong"><strong>Recall/Sensitivity</strong></span>: This is the <a class="indexterm" id="id70"/>probability whether the relevant records are truly classified.</p><p><span class="strong"><strong>Specificity</strong></span>: Also known <a class="indexterm" id="id71"/>as true negative rate, this is the proportion of truly classified wrong records.</p><p>A confusion<a class="indexterm" id="id72"/> matrix shown in the following image is constructed using the results of classification models discussed in the previous section:</p><div class="mediaobject"><img alt="Evaluating data-mining algorithms" src="graphics/B03888_02_16.jpg"/></div><p>Let's understand the confusion matrix:</p><p><span class="strong"><strong>TRUE POSITVE (TP)</strong></span>: This is a count of all the responses where the actual response is negative and the model predicted is positive</p><p><span class="strong"><strong>FALSE POSITIVE (FP)</strong></span>: This is<a class="indexterm" id="id73"/> a count of all the responses where the actual response is negative, but the model predicted is positive. It is, in general, a <span class="strong"><strong>FALSE ALARM</strong></span>.</p><p><span class="strong"><strong>FALSE NEGATIVE (FN)</strong></span>: This is a count of all the responses where the actual response is positive, but the model predicted is negative. It is, in general, <span class="strong"><strong>A MISS</strong></span>.</p><p><span class="strong"><strong>TRUE NEGATIVE (TN)</strong></span>: This is a count of all the responses where the actual response is negative, and the model predicted is negative.</p><p>Mathematically, precision and recall/specificity is calculated as follows:</p><div class="mediaobject"><img alt="Evaluating data-mining algorithms" src="graphics/B03888_02_17.jpg"/></div><p><span class="strong"><strong>Model comparison</strong></span>: A<a class="indexterm" id="id74"/> classification problem can be solved using one or more statistical models. For example, a classification problem can be solved using logistic regression, a decision tree, ensemble<a class="indexterm" id="id75"/> methods, and SVM. How do you choose which model fits the data well? A number of approaches are available for a suitable model selection, such as <a class="indexterm" id="id76"/>
<span class="strong"><strong>Akaike information criteria</strong></span> (<span class="strong"><strong>AIC</strong></span>), <span class="strong"><strong>Bayesian</strong></span><a class="indexterm" id="id77"/>
<span class="strong"><strong> information criteria</strong></span> (<span class="strong"><strong>BIC</strong></span>), and Adjusted R^2, Cᵨ. For each model, AIC / BIC / Adjusted R^2 is calculated. The model with least of these values is selected as the best model.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p><span class="strong"><strong>Downloading the example code</strong></span></p><p>You can download the example code fies from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the fies e-mailed directly to you.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Summary</h1></div></div></div><p>In this chapter, you learned about popular data preprocessing techniques, data-mining techniques, and evaluation techniques commonly used in recommender systems. In the next chapter, you will learn about the recommender systems introduced in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Recommender Systems">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Recommender Systems</em></span>, in more detail.</p></div></body></html>