<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using Machine Learning in Real-Time Applications</h1>
                </header>
            
            <article>
                
<p>Throughout this book, you have learned many ML algorithms and techniques. What remains, however, is to deploy these algorithms into real-world applications. This chapter is dedicated to those pieces of advice related to using ML in the real world, in real applications, and in production environments. </p>
<p>There are many differences between idealized usage of ML algorithms and real-world usage. In our examples, we both train and execute models in one step, in response to one command. We assume that the models do not need to be serialized, saved, or reloaded in any way. We have not thought about user interface responsiveness, executing on mobile devices, or building API interfaces between clients and servers.</p>
<p>Real applications may also have a scope several orders of magnitude larger than the examples we've discussed. How do you train an ANN with billions of data points in a dataset? How do you collect, store, and process that amount of information? </p>
<p>In this chapter, we'll discuss the following topics:</p>
<ul>
<li>Frontend architecture</li>
<li>Backend architecture</li>
<li>Data pipelining</li>
<li>Tools and services you can use to build a production ML system</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serializing models</h1>
                </header>
            
            <article>
                
<p>Our examples throughout this book have built, trained, and tested models only to destroy them a millisecond later. We can get away with this because our examples use limited training data and, at worst, take only a few minutes to train. Production applications will typically use much more data and require more time to train. In production applications, the trained model itself is a valuable asset that should be stored, saved, and loaded on demand. In other words, our models must be serializable. </p>
<p>Serialization itself is typically not a difficult issue. Models are essentially a compressed version of the training data. Some models can indeed be very large, but they will still be a fraction of the size of the data that trained them. What makes the topic of serialization challenging is that it opens up many other architectural questions that you will have to consider, the first being the question of where and how to store the model.</p>
<p>Disappointingly, there's no right answer. Models can be stored nearly anywhere depending on their size, complexity, frequency of use, available technology, and so on. Naive Bayes classifiers require only the storage of token and document counts and use only key/value lookups with no advanced querying, so a single Redis server can host a huge classifier trained on billions of documents. <span>Very large models can be serialized into a dedicated database, perhaps even a dedicated graph database cluster</span>. Moderately sized models may be serialized as JSON or a binary format and stored in a database BLOB field, hosted on a file server or API such as Amazon S3, or stored in browser local storage if it is small enough. </p>
<p>Most ML libraries have serialization and deserialization built in, as ultimately this functionality is dependent on the implementation details of the library. Most libraries include methods such as <kbd>save()</kbd> and <kbd>load()</kbd>, though you will want to refer to the documentation of the specific library you're using.</p>
<p>Make sure to include serialization functionality when writing your own libraries. If you want to support multiple storage backends, it would be best to decouple the serialization functionality from the core logic and implement a driver and interface architecture instead.</p>
<p>This is just the first of the questions we'll need to answer now that we have a serializable model. Serializable models are also portable, which means they can be moved from machine to machine. You can download a pretrained model onto a smartphone for offline usage, for instance. Your JavaScript application can use a web worker to download and maintain a ready-to-use model for speech detection, ask for microphone permission, and make a website navigable solely by voice commands—all through a Chrome extension.</p>
<p>In this section, we'll discuss the various architectural considerations that arise once your model is serializable and portable. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training models on the server</h1>
                </header>
            
            <article>
                
<p>Due to the time, data, processing power, and memory requirements involved in training sophisticated models, it's often desirable to train models on the server rather than the client. Depending on the use case, even the evaluation of models may need to occur on the server.</p>
<p>There are a few paradigms to consider in terms of where to train and where to evaluate models. Your options, in general, will be to train and evaluate fully on the server, train and evaluate fully on the client, or to train on the server but evaluate on the client. Let's explore some examples of each paradigm. </p>
<p>The simplest implementation is to both train and evaluate models on the server. The main advantage of this approach is that you get to determine and control the entire execution environment of the model. You can easily analyze the server load required to train and execute a model and scale your servers as necessary. It's easier for a server you fully control to get access to a large corpus of training data, as the data is most likely in a database that you also control. You won't have to worry about which version of JavaScript your clients are running or whether you will have access to the client's GPU for training. Training and executing models on the server also means that there is no additional load on the client machine due to the model. </p>
<p>The primary downside of a fully server-side approach is that it requires a well-designed and robust API. If you have an application which requires quick response times for a model evaluation, you will need to ensure that your API can serve results quickly and reliably. This approach also means that offline evaluation of models is not possible; the client will require a connection to your server in order for anything to work. Most applications or products billed as <strong>Software as a Service</strong> (<strong>SaaS</strong>) will use the server-side model, and this approach should be the first one you consider if you are providing a paid service to customers. </p>
<p>Models can conversely be fully trained and evaluated on the client. In this case, the client itself will need access to the training data and it will need sufficient processing power to train a model. This approach is generally not appropriate for models that require large training sets or long training times, as there is no way to ensure that the client's device will be able to process the data. You will also have to contend with older devices which may not have a GPU or the processing power to train even simple models.</p>
<p>However, client-side training and evaluation is a good approach for applications which require a high level of data privacy or data ownership in cases where the training data originates from the device itself. Restricting the processing to the client device ensures that the user's data is not transmitted to any third-party server and can be deleted directly by the user. Applications such as fingerprint scanning, biometrics analysis, location data analysis, phone call analysis, and so on, are good candidates for a fully client-side approach. This approach also ensures that models can be trained and evaluated offline, with no need for an internet connection. </p>
<p>A hybrid approach can blend the best of both worlds in some cases. Advanced models that require a lot of training data can be trained on a server and serialized. A client, when it first connects to your application, can download and store the trained model for offline usage. The client itself becomes responsible for evaluating the model, but does not need to train the model in this case. </p>
<p>The hybrid approach allows you to train and periodically update sophisticated models on the server. A serialized model is much smaller than the original training data, and therefore can be delivered to a client for offline evaluation. As long as both the client and the server use compatible libraries or algorithms (that is, <kbd>TensorFlow.js</kbd> on both sides), the client can take advantage of the processing power of the server for training but use its own offline processing capabilities for the much less demanding evaluation step. </p>
<p>An example use case for the hybrid model is speech or image recognition, perhaps for an AI assistant or <strong>Augmented Reality</strong> (<strong>AR</strong>) application. In the case of an AR application, the server is responsible for maintaining millions of training images and training (for example) an RNN to classify objects. Once the training is complete, this model can be serialized, stored, and downloaded by the client. </p>
<p>Let's imagine an AR application that connects to the device's camera and displays an annotated video feed that identifies objects. When the application first starts, the client downloads the AR RNN model and stores it in the device's local storage along with version information. When the video feed first starts, the application retrieves the model from storage and deserializes it into the client's own RNN implementation. Ideally, the client's RNN implementation will use the same library and version as the library on the server.</p>
<p>In order to classify and annotate every frame of the video, the client would need to do all the necessary work in just 16 ms (for 60 FPS video). This is achievable, but in practice not every frame is used for classification; 1 of every 3 frames (50 ms apart) would suffice. The hybrid approach shines here; the <span>application would suffer a major performance penalty if each frame of a video had to be uploaded to a server, evaluated, and then returned. Even with a fantastic model performance—a model that evaluates in, say, 5 ms—you could experience an additional 100 ms lag due to the round-trip time required by an HTTP request.</span></p>
<p>Under the hybrid approach, the client does not need to ship the image to a server for evaluation but instead can evaluate the image immediately, based on the previously trained model now loaded into memory. A well-designed client will periodically check the server for updates to the model and update it when necessary, but will still allow outdated models to run offline. Users are happiest when applications <em>just work</em>, and the hybrid model gives you both performance and resilience. Servers are relied upon only for tasks that can happen asynchronously, such as downloading updated models or sending information back to the server. </p>
<p>The hybrid approach, therefore, is best for use cases where a large, sophisticated model is needed but the evaluation of the model either needs to happen very quickly or offline. This is not a hard-and-fast rule, of course. There are many other situations where a hybrid approach makes the most sense; if you have many clients and cannot afford the server resources to process all their evaluations, you might use the hybrid approach to offload your processing responsibilities. </p>
<p>Care must be taken when designing a client application that performs model training or evaluation. While evaluation is a lot faster than training, it is still nontrivial and may cause UI performance issues on the client if not implemented correctly. In the next section, we'll look at a modern web browser feature called <strong>web workers</strong> that can be used to perform processing in a standalone thread, keeping your UI responsive. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Web workers</h1>
                </header>
            
            <article>
                
<p>If you're developing for a web browser application, you'll certainly want to use a web worker to manage the model in the background. Web workers are a browser-specific feature intended to allow background processing, which is exactly what we want when we're dealing with large models. </p>
<p>Web workers can interact with <kbd>XMLHttpRequest</kbd>, <kbd>IndexedDB</kbd>,<em> </em>and <kbd>postMessage</kbd>. A web worker can download a model from the server with <kbd>XMLHttpRequest</kbd>, store it locally with <kbd>IndexedDB</kbd>, and communicate with the UI thread with <kbd>postMessage</kbd>. These three tools, used together, provide a complete foundation for a responsive, performant, and potentially offline experience. <span>Other JavaScript platforms, such as </span>React Native<span>, also have similar faculties for HTTP requests, data storage, and interprocess communication. </span></p>
<p>Web workers can be combined with other browser-specific features such as <strong>service workers</strong> and device APIs to provide a full offline experience. Service workers can cache specific assets for offline use or intelligently switch between online and offline evaluation. The browser extension platforms, as well as mobile platforms such as React Native, also provide a number of mechanisms for supporting cached data, background threads, and offline use. </p>
<p>Regardless of the platform, the concepts are the same: the application should download and upload data asynchronously when an internet connection is available; the application should cache (and version) everything it needs to function, like a pretrained model; and the application should evaluate the model independently of the UI.</p>
<p>It would be easy to mistakenly assume that a model is small and fast enough to run in the same thread as the UI. If your average evaluation time is only 5 ms and you only need one evaluation every 50 ms, it's tempting to become complacent and skip the additional detail of evaluating your model in a separate thread. However, the range of devices on the market today make it so you cannot even assume an order-of-magnitude similarity in performance. If you've tested your application on a modern phone with a GPU, for example, you may not be able to accurately assess how it will perform on an older phone's CPU. The evaluation time might jump from 5 ms to 100 ms. In a poorly designed application this will result in UI lag or freezing, but in a well-designed application, the UI will remain responsive but with less frequent updates.</p>
<p>Fortunately, web workers and the <kbd>postMessage</kbd> API are simple to use. The <kbd>IndexedDB</kbd> API is a low-level API and may be daunting to use initially, but there are many user-friendly libraries that abstract the details away. The specific manner in which you download and store the pretrained model is solely dependent on the implementation details of your application and the specific ML algorithm you've chosen. Smaller models can be serialized as JSON and stored in <kbd>IndexedDB</kbd>; more advanced models can be integrated directly into <kbd>IndexedDB</kbd>. Make sure to include a mechanism for comparing version information in your server-side API; you should have a way to ask the server what the current version of the model is and be able to compare that to your own copy so that you can invalidate and update the model.</p>
<p>Put some thought into the design of your web worker's message-passing API as well. You will use the <kbd>postMessage</kbd><em> </em>API (available in all major browsers) for communication between the UI thread and the background thread. This communication should, at the very least, include some way to check on the status of the model and a way to send a data point to the model for evaluation. But you'll also want to look forward to future functionality and make your API flexible and future-proof.</p>
<p>Two examples of functionality you might want to plan for are continually improving models, that retrain themselves based on user feedback, and per-user models that learn the behaviors or preferences of individual users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continually improving and per-user models</h1>
                </header>
            
            <article>
                
<p>Throughout the life cycle of your application, it's likely that the end user is going to interact with your model in some way. Often, this interaction can be used as feedback to further train the model. The interaction may also be used to customize the model to the user, tailoring it to their own interests and behaviors. </p>
<p>A good example of both concepts is the spam filter. A spam filter should continually improve as users mark messages as spam. Spam filters are most powerful when they have lots of data points to train against, and this data can come from other users of the application. Any time a user marks a message as spam, that knowledge should be applied to the model and other users should also be able to enjoy the automatic improvement in their own spam filtering. </p>
<p>Spam filters are also a good example of models that should be customizable per user. What I think is spam may not be the same as what you think is spam. I aggressively mark marketing emails and newsletters that I haven't signed up for as spam, but other users may want to see those types of messages in their own inbox. At the same time, there are messages that everyone agrees are spam, so it would be good to design our application to use a central, continually updating model that can be locally refined to better fit the behavior of the specific user.</p>
<p>Bayesian classifiers fit this description very well, as Bayes' theorem is designed to be updated by new information. In <a href="8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml" target="_blank">Chapter 5</a>, <em>Classification Algorithms</em>, we discussed an implementation of the Naive Bayes classifier that handles rare words gracefully. In that scheme, a weight factor skewed word probabilities towards neutral so that rare words wouldn't influence the model too strongly. A per-user spam filter can use this same technique but instead of skewing words towards neutrality, we can skew them towards the central model's probability.</p>
<p>The rare word weight factor, in this usage, becomes a weight factor that balances the central model against the local model. The larger you make the weight factor, the more important the central model becomes and the longer it will take for the user to affect the local model. A smaller weight factor will be more responsive to user feedback, but may also cause irregularities in performance. In a typical rare word implementation the weight factor is in the range from 3 to 10. In a per-user model, however, the weight factor should be larger—perhaps 50 – 1,000—<span>in consideration of the fact that the central model is trained by millions of examples and should not be easily overridden by just a handful of local examples. </span></p>
<p>Care must be taken when sending data back to the server for continual model improvements. You should not transmit the email message back to the server, as that would create an unnecessary security risk—<span><em>especially</em> if your product is not an email hosting provider but only an email client. If you are also the email hosting provider, then you can simply send the email ID back to the server to be marked as spam and given to the model for training; the client and the server will maintain their own models separately. If you are not the email hosting provider, then you should take extra care to secure your user's data. If you must transmit a token stream back to the server, then you should encrypt it in transit as well as anonymize it. You may also consider using a tokenizer that salts and hashes the tokens (for example, with sha1 or hmac) after tokenizing and stemming the content. The classifier will work just as well with hashed data as it does with readable data, but will add an additional layer of obfuscation. Finally, make sure that the HTTP request and raw token data is not logged. Once the data enters the model (in the form of token counts) it is sufficiently anonymized, but make sure there is no way a spy can relate a specific token stream to a specific user. </span></p>
<p>Naive Bayes classifiers are not the only models that can be continually updated or customized per user, of course. Continual updating of a model is possible with most ML algorithms. If a user indicates that an RNN got an image classification wrong, that user's data point can be added to the model's training set and the model can either be fully retrained at periodic intervals, or can be batch-updated with newer training examples.</p>
<p>Some algorithms support truly live updates of the model. The Naive Bayes classifier requires only an update to token and document counts, which might even be stored in memory. The knn and k-means algorithms similarly allow data points to be added to the model at any time. Some ANNs, like those used in reinforcement learning, also rely on live feedback. </p>
<p>Other algorithms are better updated periodically in batches. These algorithms typically rely on gradient descent or stochastic methods and require a feedback loop over many examples during training; examples are ANNs and random forests. An ANN model can indeed be retrained with a single data point, but batch training is far more effective. Be careful not to overfit models as you update them; too much training is not always a good thing.</p>
<p>In some cases, it is better to fully retrain a model based on the updated training set. One reason to do this is to avoid overfitting short-term trends in training data. By fully retraining a model, you ensure that recent training examples have the same weight as old training examples; this may or may not be desired. If models are periodically and automatically retrained, make sure that the training algorithm is looking at the right signals. It should be able to balance accuracy, loss, and variance in order to develop reliable models. As much of ML training is stochastic in nature, there is no guarantee that two training runs will finish to the same level of quality or in similar amounts of time. Your training algorithm should control for these factors and be able to discard bad models if necessary, for instance if a target accuracy or loss was not achieved within a maximum limit on the number of training epochs. </p>
<p>A new question arises at this point: how do you collect, store, and process gigabytes or terabytes of training data? How and where do you store and distribute serialized models to clients? How do you collect new training examples from millions of users? This topic is called data pipelining, which we'll discuss next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data pipelines</h1>
                </header>
            
            <article>
                
<p>When developing a production ML system, it's not likely that you will have the training data handed to you in a ready-to-process format. Production ML systems are typically part of larger application systems, and the data that you use will probably originate from several different sources. The training set for an ML algorithm may be a subset of your larger database, combined with images hosted on a <strong>Content Delivery Network</strong> (<strong>CDN</strong>) and event data from an Elasticsearch server. In our examples, we have been given an isolated training set, but in the real world we will need to generate the training set in an automated and repeatable manner. </p>
<p>The process of ushering data through various stages of a life cycle is called <strong>data pipelining</strong>. Data pipelining may include data selectors that run SQL or Elasticsearch queries for objects, event subscriptions which allow data to flow in from event-or log-based data, aggregations, joins, combining data with data from third-party APIs, sanitization, normalization, and storage. </p>
<p>In an ideal implementation, the data pipeline acts as an abstraction layer between the larger application environment and the ML process. The ML algorithm should be able to read the output of the data pipeline without any knowledge of the original source of the data, similar to our examples. Under this approach, the ML algorithm will not need to understand the implementation details of the application; it is the pipeline itself that is responsible for knowing how the application is built.</p>
<p>As there are many possible data sources and infinite ways to architect an application, there is no one-size-fits-all data pipeline. However, most data pipelines will contain these components, which we will discuss in the following sections:</p>
<ul>
<li>Data querying and event subscription</li>
<li>Data joining or aggregation</li>
<li>Transformation and normalization</li>
<li>Storage and delivery</li>
</ul>
<p>Let's take a look at each of these concepts and introduce some tools and techniques that can achieve them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data querying</h1>
                </header>
            
            <article>
                
<p>Imagine an application such as Disqus, which is an embeddable comment form that website owners can use to add comment functionality to blog posts or other pages. The primary functionality of Disqus is to allow users to like or leave comments on posts, however, as an additional feature and revenue stream, Disqus can make content recommendations and display them alongside sponsored content. The content recommendation system is an example of an ML system that is only one feature of a larger application.</p>
<p>A content recommendation system in an application such as Disqus does not necessarily need to interact with the comment data, but might use the user's likes history to generate recommendations similar to the current page. Such a system would also need to analyze the text content of the liked pages and compare that to the text content of all pages in the network in order to make recommendations. Disqus does not need the post's content in order to provide comment functionality, but does need to store metadata about the page (like its URL and title) in its database. The post content may therefore not reside in the application's main database, though the likes and page metadata would likely be stored there.</p>
<p>A data pipeline built around Disqus's recommendation system needs first to query the main database for pages the user has liked—or pages that were liked by users who liked the current page—and return their metadata. In order to find similar content, however, the system will need to use the text content of each liked post. This data might be stored in a separate system, perhaps a secondary database such as MongoDB or Elasticsearch, or in Amazon S3 or some other data warehouse. The pipeline will need to retrieve the text content based on the metadata returned by the main database, and associate the content with the metadata. </p>
<p>This is an example of multiple data selectors or data sources in the early stages of a data pipeline. One data source is the primary application data, which stores post and likes metadata. The other data source is a secondary server which stores the post's text content. </p>
<p>The next step in this pipeline might involve finding a number of candidate posts similar to the ones the user has liked, perhaps through a request to Elasticsearch or some other service that can find similar content. Similar content is not necessarily the correct content to serve, however, so these candidate articles will ultimately be ranked by an (hypothetical) ANN in order to determine the best content to display. In this example, the input to the data pipeline is the current page and the output from the data pipeline is a list of, say, 200 similar pages that the ANN will then rank.</p>
<p>If all the necessary data resides in the primary database, the entire pipeline can be achieved with an SQL statement and some JOINs. Even in this case, care should be taken to develop a degree of abstraction between the ML algorithm and the data pipeline, as you may decide to update the application's architecture in the future. In other cases, however, the data will reside in separate locations and a more considered pipeline should be developed.</p>
<p>There are many ways to build this data pipeline. You could develop a JavaScript module that performs all the pipeline tasks, and in some cases, you could even write a bash script using standard Unix tools to accomplish the task. On the other end of the complexity spectrum, there are purpose-built tools for data pipelining such as <em>Apache Kafka</em> and <em>AWS Pipeline</em>. These systems are designed modularly and allow you to define a specific data source, query, transformation, and aggregation modules as well as the workflows that connect them. In AWS Pipeline, for instance, you define <em>data nodes</em> that understand how to interact with the various data sources in your application.</p>
<p>The earliest stage of a pipeline is typically some sort of data query operation. Training examples must be extracted from a larger database, keeping in mind that not every record in a database is necessarily a training example. In the case of a spam filter, for instance, you should only select messages that have been marked as spam or not spam by a user. Messages that were automatically marked as spam by a spam filter should probably not be used for training, as that might cause a positive feedback loop that ultimately causes an unacceptable false positive rate. </p>
<p>Similarly, you may want to prevent users that have been blocked or banned by your system from influencing your model training. A bad actor could intentionally mislead an ML model by taking inappropriate actions on their own data, so you should disqualify these data points as training examples. </p>
<p>Alternatively, if your application is such that recent data points should take precedence over older training points, your data query operation might set a time-based limit on the data to use for training, or select a fixed limit ordered reverse chronologically. No matter the situation, make sure you carefully consider your data queries as they are an essential first step in your data pipeline. </p>
<p>Not all data needs to come from database queries, however. Many applications use a <em>pub/sub</em> or event subscription architecture to capture streaming data. This data could be activity logs aggregated from a number of servers, or live transaction data from a number of sources. In these cases, an event subscriber will be an early part of your data pipeline. Note that event subscription and data querying are not mutually exclusive operations. Events that come in through a pub/sub system can still be filtered based on various criteria; this is still a form of data querying.</p>
<p>One potential issue with an event subscription model arises when it's combined with a batch-training scheme. If you require 5,000 data points but receive only 100 per second, your pipeline will need to maintain a buffer of data points until the target size is reached. There are various message-queuing systems that can assist with this, such as RabbitMQ or Redis. A pipeline requiring this type of functionality might hold messages in a queue until the target of 5,000 messages is achieved, and only then release the messages for batch processing through the rest of the pipeline. </p>
<p>In the case that data is collected from multiple sources, it most likely will need to be joined or aggregated in some manner. Let's now take a look at a situation where data needs to be joined to data from an external API. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data joining and aggregation</h1>
                </header>
            
            <article>
                
<p>Let's return to our example of the Disqus content recommendation system. Imagine that the data pipeline is able to query likes and post metadata directly from the primary database, but that no system in the applications stores the post's text content. Instead, a microservice was developed in the form of an API that accepts a post ID or URL and returns the page's sanitized text content. </p>
<p>In this case, the data pipeline will need to interact with the microservice API in order to get the text content for each post. This approach is perfectly valid, though if the frequency of post content requests is high, some caching or storage should probably be implemented. </p>
<p>The data pipeline will need to employ an approach similar to the buffering of messages in the event subscription model. The pipeline can use a message queue to queue posts that still require content, and make requests to the content microservice for each post in the queue until the queue is depleted. As each post's content is retrieved it is added to the post metadata and stored in a separate queue for completed requests. Only when the source queue is depleted and the sink queue is full should the pipeline move on to the next step.</p>
<p>Data joining does not necessarily need to involve a microservice API. If the pipeline collects data from two separate sources that need to be combined, a similar approach can be employed. The pipeline is the only component that needs to understand the relationship between the two data sources and formats, leaving both the data sources and the ML algorithm to operate independently of those details.</p>
<p>The queue approach also works well when a data aggregation is required. An example of this situation is a pipeline in which the input is streaming input data and the output is token counts or value aggregations. Using a message queue is desirable in these situations as most message queues ensure that a message can be consumed only once, therefore preventing any duplication by the aggregator. This is especially valuable when the event stream is very high frequency, such that tokenizing each event as it comes in would lead to backups or server overload. </p>
<p>Because message queues ensure that each message is consumed only once, high-frequency event data can stream directly into a queue where messages are consumed by multiple workers in parallel. Each worker might be responsible for tokenizing the event data and then pushing the token stream to a different message queue. The message queue software ensures that no two workers process the same event, and each worker can operate as an independent unit that is only concerned with tokenization. </p>
<p>As the tokenizers push their results onto a new message queue, another worker can consume those messages and aggregate token counts, delivering its own results to the next step in the pipeline every second or minute or 1,000 events, whatever is appropriate for the application. The output of this style of pipeline might be fed into a continually updating Bayesian model, for example. </p>
<p>One benefit of a data pipeline designed in this manner is performance. If you were to attempt to subscribe to high-frequency event data, tokenize each message, aggregate token counts, and update a model all in one system, you might be forced to use a very powerful (and expensive) single server. The server would simultaneously need a high-performance CPU, lots of RAM, and a high-throughput network connection. </p>
<p>By breaking up the pipeline into stages, however, you can optimize each stage of the pipeline for its specific task and load condition. The message queue that receives the source event stream needs only to receive the event stream but does not need to process it. The tokenizer workers do not necessarily need to be high-performance servers, as they can be run in parallel. The aggregating queue and worker will process a large volume of data but will not need to retain data for longer than a few seconds and therefore may not need much RAM. The final model, which is a compressed version of the source data, can be stored on a more modest machine. Many components of the data pipeline can be built of commodity hardware simply because a data pipeline encourages modular design.</p>
<p>In many cases, you will need to transform your data from format to format throughout the pipeline. That could mean converting from native data structures to JSON, transposing or interpolating values, or hashing values. Let's now discuss several types of data transformations that may occur in the data pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transformation and normalization</h1>
                </header>
            
            <article>
                
<p>As your data makes its way through a pipeline, it may need to be converted into a structure compatible with your algorithm's input layer. There are many possible transformations that can be performed on the data in the pipeline. For example, in order to protect sensitive user data before it reaches a token-based classifier, you might apply a cryptographic hashing function to the tokens so that they are no longer human readable. </p>
<p>More typically, the types of transformations will be related to sanitization, normalization, or transposition. A sanitization operation might involve removing unnecessary whitespace or HTML tags, removing email addresses from a token stream, and removing unnecessary fields from the data structure. If your pipeline has subscribed to an event stream as the source of the data and the event stream attaches source server IP addresses to event data, it would be a good idea to remove these values from the data structure, both in order to save space and to minimize the surface area for potential data leaks. </p>
<p>Similarly, if email addresses are not necessary for your classification algorithm, the pipeline should remove that data so that it interacts with the fewest possible servers and systems. If you've designed a spam filter, you may want to look into using only the domain portion of the email address instead of the fully qualified address. Alternately, the email addresses or domains may be hashed by the pipeline so that the classifier can still recognize them but a human cannot.</p>
<p>Make sure to audit your data for other potential security and privacy issues as well. If your application collects the end user's IP address as part of its event stream, but the classifier does not need that data, remove it from the pipeline as early as possible. These considerations are becoming ever more important with the implementation of new European privacy laws, and every developer should be aware of privacy and compliance concerns.</p>
<p>A common category of data transformation is normalization. When working with a range of numerical values for a given field or feature, it's often desirable to normalize the range such that it has a known minimum and maximum bound. One approach is to normalize all values of the same field to the range [0,1], using the maximum encountered value as the divisor (for example, the sequence <em>1, 2, 4</em> can be normalized to <em>0.25, 0.5, 1</em>). Whether data needs to be normalized in this manner will depend entirely on the algorithm that consumes the data.</p>
<p>Another approach to normalization is to convert values into percentiles. In this scheme, very large outlying values will not skew the algorithm too drastically. If most values lie between 0 and 100 but a few points include values such as 50,000, an algorithm may give outsized precedence to the large values. If the data is normalized as a percentile, however, you are guaranteed to not have any values exceeding 100 and the outliers are brought into the same range as the rest of the data. Whether or not this is a good thing depends on the algorithm.</p>
<p>The data pipeline is also a good place to calculate derived or second-order features. Imagine a random forest classifier that uses Instagram profile data to determine if the profile belongs to a human or a bot. The Instagram profile data will include fields such as the user's followers count, friends count, posts count, website, bio, and username. A random forest classifier will have difficulty using those fields in their original representations, however, by applying some simple data transformations, you can achieve accuracies of 90%.</p>
<p>In the Instagram case, one type of helpful data transformation is calculating ratios. Followers count and friends count, as separate features or signals, may not be useful to the classifier since they are treated somewhat independently. But the friends-to-followers <em>ratio</em> can turn out to be a very strong signal that may expose bot users. An Instagram user with 1,000 friends doesn't raise any flags, nor would an Instagram user with 50 followers; treated independently, these features are not strong signals. However, an Instagram user with a friends-to-followers ratio of 20 (or 1,000/50) is almost certainly a bot designed to follow other users. Similarly, a ratio such as posts-versus-followers or posts-versus-friends may end up being a stronger signal than any of those features independently. </p>
<p>Text content such as the Instagram user's profile bio, website, or username is made useful by deriving second-order features from them as well. A classifier may not be able to do anything with a website's URL, but perhaps a Boolean <em>has_profile_website</em> feature can be used as a signal instead. If, in your research, you notice that usernames of bots tend to have a lot of numbers in them, you can derive features from the username itself. One feature can calculate the ratio of letters to numbers in the username, another Boolean feature can represent whether the username has a number at the end or beginning, and a more advanced feature could determine if dictionary words were used in the username or not (therefore distinguishing between <kbd>@themachinelearningwriter</kbd> and something gibberish like <kbd>@panatoe234</kbd>).</p>
<p>Derived features can be of any level of sophistication or simplicity. Another simple feature could be whether the Instagram profile contains a URL in the profile bio field (as opposed to the dedicated website field); this can be detected with a regex and the Boolean value used as the feature. A more advanced feature could automatically detect whether the language used in the user's content is the same as the language specified by the user's locale setting. If the user claims they're in France but always writes captions in Russian it may indeed be a Russian living in France, but when combined with other signals like a friends-to-followers ratio far from 1, this information may be indicative of a bot user.</p>
<p>There are lower level transformations that may need to be applied to the data in the pipeline as well. If the source data is in an XML format but the classifier requires JSON formatting, the pipeline should take responsibility for the parsing and conversion of formats.</p>
<p>Other mathematical transformations may also be applied. If the native format of the data is row-oriented but the classifier needs column-oriented data, the pipeline can perform a vector transposition operation as part of the processing. </p>
<p>Similarly, the pipeline can use mathematical interpolation to fill in missing values. If your pipeline subscribes to events emitted by a suite of sensors in a laboratory setting and a single sensor goes offline for a couple of measurements, it may be reasonable to interpolate between the two known values in order to fill in the missing data. In other cases, missing values can be replaced with the population's mean or median value. Replacing missing values with a mean or median will often result in the classifier deprioritizing that feature for that data point, as opposed to breaking the classifier by giving it a null value. </p>
<p>In general, there are two things to consider in terms of transformation and normalization within a data pipeline. The first is the mechanical details of the source data and the target format: XML data must be transformed to JSON, rows must be converted to columns, images must be converted from JPEG to BMP formats, and so on. The mechanical details are not too tricky to work out, as you will already be aware of the source and target formats required by the system.</p>
<p>The other consideration is the semantic or mathematical transformation of your data. This is an exercise in feature selection and feature engineering, and is not as straightforward as the mechanical transformation. Determining which second-order features to derive is both art and science. The art is coming up with new ideas for derived features, and the science is to rigorously test and experiment with your work. In my experience with Instagram bot detection, for instance, I found that the letters-to-numbers ratio in Instagram usernames was a very weak signal. I abandoned that idea after some experimentation in order to avoid adding unnecessary dimensionality to the problem.</p>
<p>At this point, we have a hypothetical data pipeline that collects data, joins and aggregates it, processes it, and normalizes it. We're almost done, but the data still needs to be delivered to the algorithm itself. Once the algorithm is trained, we might also want to serialize the model and store it for later use. In the next section, we'll discuss a few considerations to make when transporting and storing training data or serialized models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing and delivering data</h1>
                </header>
            
            <article>
                
<p>Once your data pipeline has applied all the necessary processing and transformations, it has one task left to do: deliver the data to your algorithm. Ideally, the algorithm will not need to know about the implementation details of the data pipeline. The algorithm should have a single location that it can interact with in order to get the fully processed data. This location could be a file on disk, a message queue, a service such as Amazon S3, a database, or an API endpoint. The approach you choose will depend on the resources available to you, the topology or architecture of your server system, and the format and size of the data.</p>
<p>Models that are trained only periodically are typically the simplest case to handle. If you're developing an image recognition RNN that learns labels for a number of images and only needs to be retrained every few months, a good approach would be to store all the images as well as a manifest file (relating image names to labels) in a service such as Amazon S3 or a dedicated path on disk. The algorithm would first load and parse the manifest file and then load the images from the storage service as needed. </p>
<p>Similarly, an Instagram bot detection algorithm may only need to be retrained every week or every month. The algorithm can read training data directly from a database table or a JSON or CSV file stored on S3 or a local disk.</p>
<p>It is rare to have to do this, but in some exotic data pipeline implementations you could also provide the algorithm with a dedicated API endpoint built as a microservice; the algorithm would simply query the API endpoint first for a list of training point references, and then request each in turn from the API.</p>
<p>Models which require online updates or near-real-time updates, on the other hand, are best served by a message queue. If a Bayesian classifier requires live updates, the algorithm can subscribe to a message queue and apply updates as they come in. Even when using a sophisticated multistage pipeline, it is possible to process new data and update a model in fractions of a second if you've designed all the components well.</p>
<p>Returning to the spam filter example, we can design a highly performant data pipeline like so: first, an API endpoint receives feedback from a user. In order to keep the user interface responsive, this API endpoint is responsible only for placing the user's feedback into a message queue and can finish its task in under a millisecond. The data pipeline in turn subscribes to the message queue, and in another few milliseconds is made aware of a new message. The pipeline then applies a few simple transformations to the message, like tokenizing, stemming, and potentially even hashing the tokens.</p>
<p>The next stage of the pipeline transforms the token stream into a hashmap of tokens and their counts (for example, from <em>hey hey there</em> to <em>{hey: 2, there: 1}</em>); this avoids the need for the classifier to update the same token's count more than once. This stage of processing will only require another couple of milliseconds at worst. Finally, the fully processed data is placed in a separate message queue which the classifier subscribes to. Once the classifier is made aware of the data it can immediately apply the updates to the model. If the classifier is backed by Redis, for instance, this final stage will also require only a few milliseconds. </p>
<p>The entire process we have described, from the time the user's feedback reaches the API server to the time the model is updated, may only require 20 ms. Considering that communication over the internet (or any other means) is limited by the speed of light, the best-case scenario for a TCP packet making a round-trip between New York and San Francisco is 40 ms; in practice, the average cross-country latency for a good internet connection is about 80 ms. Our data pipeline and model is therefore capable of updating itself based on user feedback a full 20 ms before the user will even receive their HTTP response.</p>
<p>Not every application requires real-time processing. Managing separate servers for an API, a data pipeline, message queues, a Redis store, and hosting the classifier might be overkill both in terms of effort and budget. You'll have to determine what's best for your use case. </p>
<p>The last thing to consider is not related to the data pipeline but rather the storage and delivery of the model itself, in the case of a hybrid approach where a model is trained on the server but evaluated on the client. The first question to ask yourself is whether the model is considered public or private. Private models should not be stored on a public Amazon S3 bucket, for instance; instead, the S3 bucket should have access control rules in place and your application will need to procure a signed download link with an expiration time (the S3 API assists with this).</p>
<p>The next consideration is how large the model is and how often it will be downloaded by clients. If a public model is downloaded frequently but updated infrequently, it might be best to use a CDN in order to take advantage of edge caching. If your model is stored on Amazon S3, for example, then the Amazon CloudFront CDN would be a good choice. </p>
<p>Of course, you can always build your own storage and delivery solution. In this chapter, I have assumed a cloud architecture, however if you have a single dedicated or collocated server then you may simply want to store the serialized model on disk and serve it either through your web server software or through your application's API. When dealing with large models, make sure to consider what will happen if many users attempt to download the model simultaneously. You may inadvertently saturate your server's network connection if too many people request the file at once, you might overrun any bandwidth limits set by your server's ISP, or you might end up with your server's CPU stuck in I/O wait while it moves data around.</p>
<p>As mentioned previously, there's no one-size-fits-all solution for data pipelining. If you're a hobbyist developing applications for fun or just a few users, you have lots of options for data storage and delivery. If you're working in a professional capacity on a large enterprise project, however, you will have to consider all aspects of the data pipeline and how they will impact your application's performance. </p>
<p>I will offer one final piece of advice to the hobbyists reading this section. While it's true that you don't need a sophisticated, real-time data pipeline for hobby projects, you should build one anyway. Being able to design and build real-time data pipelines is a highly marketable and valuable skill that not many people possess, and if you're willing to put in the practice to learn ML algorithms then you should also practice building performant data pipelines. I'm not saying that you should build a big, fancy data pipeline for every single hobby project—just that you should do it a few times, using several different approaches, until you're comfortable not just with the concepts but also the implementation. Practice makes perfect, and practice means getting your hands dirty. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed a number of practical matters related to ML applications in production. Learning ML algorithms is, of course, central to building an ML application, but there's much more to building an application than simply implementing an algorithm. Applications ultimately need to interact with users across a variety of devices, so it is not enough to consider only what your application does —<em> </em>you must also plan for how and where it will be used.</p>
<p>We began the chapter with a discussion about serializable and portable models, and you learned about the different architectural approaches to the training and evaluation of models. We discussed the fully server-side approach (common with SaaS products), the fully client-side approach (useful for sensitive data), and a hybrid approach by which a model is trained on the server but evaluated on the client. You also learned about web workers, which are a useful browser-specific feature that you can use to ensure a performant and responsive UI when evaluating models on the client. </p>
<p>We also discussed models which continually update or get periodically retrained, and various approaches to communicating feedback between the client and the server. You also learned about per-user models, or algorithms which can be trained by a central source of truth but refined by an individual user's specific behaviors.</p>
<p>Finally, you learned about data pipelines and various mechanisms that manage the collection, combining, transformation, and delivery of data from one system to the next. A central theme in our data pipeline discussion was the concept of using the data pipeline as a layer of abstraction between ML algorithms and the rest of your production systems. </p>
<p>The final topic I'd like to discuss is one that many ML students are curious about: how exactly do you choose the right ML algorithm for a given problem? Experts in ML typically develop an intuition that guides their decisions, but that intuition can take years to form. In the next chapter, we'll discuss practical techniques you can use to narrow in on the appropriate ML algorithm to use for any given problem.</p>
<p> </p>
<p> </p>
<p> </p>


            </article>

            
        </section>
    </body></html>