- en: Social Media Insight using Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Naive Bayes 进行社交媒体洞察
- en: Text-based documents contain lots of information. Examples include books, legal
    documents, social media, and e-mail. Extracting information from text-based documents
    is critically important to modern AI systems, for example in search engines, legal
    AI, and automated news services.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的文档包含大量信息。例如包括书籍、法律文件、社交媒体和电子邮件。从基于文本的文档中提取信息对于现代人工智能系统至关重要，例如在搜索引擎、法律人工智能和自动化新闻服务中。
- en: Extraction of useful features from text is a difficult problem. Text is not
    numerical in nature, therefore a model must be used to create features that can
    be used with data mining algorithms. The good news is that there are some simple
    models that do a great job at this, including the bag-of-words model that we will
    use in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取有用特征是一个难题。文本本质上不是数值的，因此必须使用模型来创建可以与数据挖掘算法一起使用的特征。好消息是，有一些简单的模型在这方面做得很好，包括我们将在本章中使用的词袋模型。
- en: In this chapter, we look at extracting features from text for use in data mining
    applications. The specific problem we tackle in this chapter is term disambiguation
    on social media - determining which meaning a word has based on its context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨从文本中提取特征以用于数据挖掘应用。本章我们要解决的具体问题是社交媒体中的词义消歧——根据上下文确定一个词的含义。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Downloading data from social network APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从社交网络 API 下载数据
- en: Transformers and models for text data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于文本数据的 Transformer 和模型
- en: The Naive Bayes classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naive Bayes 分类器
- en: Using JSON for saving and loading datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JSON 保存和加载数据集
- en: The NLTK library for feature creation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于特征创建的 NLTK 库
- en: The F-measure for evaluation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估的 F 度量
- en: Disambiguation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消歧
- en: Text data is often called an *unstructured format*. There is a lot of information
    in text, but it is just *there*; no headings, no required format (save for normal
    grammatical rules), loose syntax, and other problems prohibit the easy extraction
    of information from text. The data is also highly connected, with lots of mentions
    and cross-references—just not in a format that allows us to easily extract it!
    Even seemingly easy problems, such as determining if a word is a noun, have lots
    of weird edge cases that make it difficult to do reliably.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据通常被称为 *非结构化格式*。文本中包含大量信息，但它只是 *存在*；没有标题，没有必需的格式（除了正常的语法规则），松散的语法，以及其他问题阻碍了从文本中轻松提取信息。数据也高度连接，有很多提及和交叉引用——只是不是一种让我们能够轻松提取的格式！即使是看似简单的问题，例如确定一个词是否是名词，也有很多奇怪的边缘情况，使得它难以可靠地完成。
- en: We can compare the information stored in a book with that stored in a large
    database to see the difference. In the book, there are characters, themes, places,
    and lots of information. However, a book needs to be read and interpreted, with
    cultural context, to gain this information. In contrast, a database sits on your
    server with column names and data types. All the information is there and the
    level of interpretation needed to extract specific information is quite low.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将存储在书中的信息与存储在大数据库中的信息进行比较，以查看差异。在书中，有角色、主题、地点和大量信息。然而，一本书需要被阅读和解读，包括文化背景，才能获得这些信息。相比之下，数据库位于您的服务器上，具有列名和数据类型。所有信息都在那里，提取特定信息所需的解释水平相当低。
- en: Information about the data, such as its type or its meaning, is called metadata.
    Text lacks metadata. A book also contains some metadata in the form of a table
    of contents and index but the degree of information included in these sections
    is significantly lower than that of a database.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据的信息，如其类型或其含义，称为元数据。文本缺少元数据。一本书也包含一些元数据，以目录和索引的形式存在，但这些部分包含的信息量显著低于数据库。
- en: One of the problems in working with text is **term disambiguation**. When a
    person uses the word *bank*, is this a financial message or an environmental message
    (such as river bank)? This type of disambiguation is quite easy in many circumstances
    for humans (although there are still troubles), but much harder for computers
    to do.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文本时，一个问题是 **词义消歧**。当一个人使用单词 *bank* 时，这是否是一个金融信息或环境信息（如河岸）？这种类型的消歧在许多情况下对人类来说相当容易（尽管仍然存在一些问题），但对于计算机来说则要困难得多。
- en: 'In this chapter, we will look at disambiguating the use of the term Python
    on Twitter''s stream. When people talk about Python, they could be talking about
    the following things:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在Twitter流中对Python术语的使用进行消歧。当人们谈论Python时，他们可能谈论以下内容：
- en: The programming language Python
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程语言Python
- en: Monty Python, the classic comedy group
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙提·派森，这个经典的喜剧团体
- en: The snake Python
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蛇Python
- en: A make of shoe called Python
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种名为Python的鞋
- en: There can be many other things called Python. The aim of our experiment is to
    take a tweet mentioning Python and determine whether it is talking about the programming
    language, based only on the content of the tweet.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还有许多其他被称为Python的东西。我们实验的目标是取一条提到Python的推文，并仅根据推文的内容确定它是否在谈论编程语言。
- en: A message on Twitter is called a *tweet* and is limited to 140 characters. Tweets
    include lots of metadata, such as the time and date of posting, who posted it,
    and so on. However in regards to the topic of the tweet, there is not much in
    this regard.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter上的消息被称为*tweet*，限制为140个字符。推文包括大量元数据，如发布的时间日期、发布者等。然而，在推文主题方面，这方面的内容并不多。
- en: 'In this chapter, we are going to perform a data mining experiment consisting
    of the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将执行一个数据挖掘实验，包括以下步骤：
- en: Download a set of tweets from Twitter.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Twitter下载一组推文。
- en: Manually classify them to create a dataset.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动分类它们以创建数据集。
- en: Save the dataset so that we can replicate our research.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存数据集，以便我们可以复制我们的研究。
- en: Use the Naive Bayes classifier to create a classifier to perform term disambiguation.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器创建一个分类器以执行词义消歧。
- en: Downloading data from a social network
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从社交网络下载数据
- en: We are first going to download a corpus of data from Twitter and use it to sort
    out spam from useful content. Twitter provides a robust API for collecting information
    from its servers and this API is free for small-scale usage. It is, however, subject
    to some conditions that you'll need to be aware of if you start using Twitter's
    data in a commercial setting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将从Twitter下载一个数据语料库，并使用它来区分垃圾邮件和有用内容。Twitter提供了一个强大的API，用于从其服务器收集信息，并且这个API对于小规模使用是免费的。然而，如果您开始在商业环境中使用Twitter的数据，您需要了解一些条件。
- en: First, you'll need to sign up for a Twitter account (which is free). Go to [http://twitter.com](http://twitter.com) 
    and register an account if you do not already have one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要注册一个Twitter账户（这是免费的）。如果您还没有账户，请访问[http://twitter.com](http://twitter.com)并注册一个账户。
- en: Next, you'll need to ensure that you only make a certain number of requests
    per minute. This limit is currently 15 requests per 15 minutes (it depends on
    the exact API). It can be tricky ensuring that you don't breach this limit, so
    it is highly recommended that you use a library to talk to Twitter's API.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要确保您每分钟只发出一定数量的请求。当前限制为每15分钟15个请求（具体取决于API的确切情况）。确保您不会超过这个限制可能会很棘手，因此强烈建议您使用库与Twitter的API进行通信。
- en: If you are using your own code (that is making the web calls with your own code)
    to connect with a web-based API, ensure that you read the documentation about
    rate limiting their documentation and understand the limitations. In Python, you
    can use the `time` library to perform a pause between calls to ensure you do not
    breach the limit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用自己的代码（即使用自己的代码进行网络调用）来连接基于Web的API，请确保您阅读有关速率限制的文档，并理解限制。在Python中，您可以使用`time`库在调用之间执行暂停，以确保您不会超过限制。
- en: You will then need a key to access Twitter's data. Go to [http://twitter.com](http://twitter.com)
    and sign in to your account. When you are logged in, go to [https://apps.twitter.com/](https://apps.twitter.com/)
    and click on Create New App. Create a name and description for your app, along
    with a website address.If you don't have a website to use, insert a placeholder.
    Leave the Callback URL field blank for this app—we won't need it. Agree to the
    terms of use (if you do) and click on Create your Twitter application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将需要一个密钥来访问Twitter的数据。访问[http://twitter.com](http://twitter.com)并登录您的账户。登录后，前往[https://apps.twitter.com/](https://apps.twitter.com/)并点击创建新应用。为您的应用创建一个名称和描述，以及一个网站地址。如果您没有可用的网站，请插入一个占位符。对于此应用，请留空回调URL字段——我们不需要它。同意使用条款（如果有的话）并点击创建您的Twitter应用。
- en: Keep the resulting website open—you'll need the access keys that are on this
    page. Next, we need a library to talk to Twitter. There are many options; the
    one I like is simply called `twitter`, and is the official Twitter Python library.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 保持生成的网站打开——您需要此页上的访问密钥。接下来，我们需要一个库来与Twitter通信。有许多选择；我喜欢的一个简单地称为`twitter`，是官方的Twitter
    Python库。
- en: You can install `twitter` using pip3 install twitter (on the command line) if
    you are using pip to install your packages. At the time of writing, Anaconda does
    not include twitter, therefore you can't use `conda` to install it. If you are
    using another system or want to build from source, check the documentation at
    [https://github.com/sixohsix/twitter](https://github.com/sixohsix/twitter)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用pip安装包，可以使用pip3 install twitter（在命令行中）安装`twitter`。在编写本文时，Anaconda不包括twitter，因此您不能使用`conda`来安装它。如果您使用其他系统或想从源代码构建，请检查[https://github.com/sixohsix/twitter](https://github.com/sixohsix/twitter)上的文档。
- en: Create a new Jupyter Notebook to download the data. We will create several notebooks
    in this chapter for various different purposes, so it might be a good idea to
    also create a folder to keep track of them. This first notebook, `ch6_get_twitter`,
    is specifically for downloading new Twitter data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的Jupyter Notebook以下载数据。在本章中，我们将为不同的目的创建几个笔记本，因此创建一个文件夹来跟踪它们可能是个好主意。这个第一个笔记本`ch6_get_twitter`专门用于下载新的Twitter数据。
- en: 'First, we import the twitter library and set our authorization tokens. The
    consumer key and consumer secret will be available on the Keys and Access Tokens
    tab on your Twitter app''s page. To get the access tokens, you''ll need to click
    on the Create my access token button, which is on the same page. Enter the keys
    into the appropriate places in the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入twitter库并设置我们的授权令牌。消费者密钥和消费者密钥将在您的Twitter应用页面的“密钥和访问令牌”选项卡上提供。要获取访问令牌，您需要点击同一页面上的“创建我的访问令牌”按钮。将密钥输入以下代码中的适当位置：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We are going to get our tweets from Twitter''s search function. We will create
    a reader that connects to twitter using our authorization, and then use that reader
    to perform searches. In the Notebook, we set the filename where the tweets will
    be stored:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Twitter的搜索功能获取推文。我们将创建一个读者，使用我们的授权连接到Twitter，然后使用该读者执行搜索。在Notebook中，我们设置了存储推文的文件名：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, create an object that can read from Twitter. We create this object with
    our authorization object that we set up earlier:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个可以读取Twitter数据的对象。我们使用之前设置的授权对象来创建这个对象：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then open our output file for writing. We open it for appending—this allows
    us to rerun the script to obtain more tweets. We then use our Twitter connection
    to perform a search for the word Python. We only want the statuses that are returned
    for our dataset. This code takes the tweet, uses the json library to create a
    string representation using the dumps function, and then writes it to the file.
    It then creates a blank line under the tweet so that we can easily distinguish
    where one tweet starts and ends in our file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打开输出文件以进行写入。我们以追加模式打开它——这允许我们重新运行脚本以获取更多推文。然后，我们使用Twitter连接执行对单词Python的搜索。我们只想获取数据集中返回的状态。此代码获取推文，使用json库的dumps函数创建字符串表示形式，并将其写入文件。然后，在推文下方创建一个空白行，以便我们可以在文件中轻松区分一条推文的开头和结尾：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding loop, we also perform a check to see whether there is text
    in the tweet or not. Not all of the objects returned by twitter will be actual
    tweets (for example, some responses will be actions to delete tweets). The key
    difference is the inclusion of text as a key, which we test for. Running this
    for a few minutes will result in 100 tweets being added to the output file.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的循环中，我们还执行了一个检查，以查看推文中是否有文本。Twitter返回的并非所有对象都是实际的推文（例如，一些回复可能是删除推文的行为）。关键区别在于包含文本作为键，我们对此进行了测试。运行几分钟将导致100条推文被添加到输出文件。
- en: You can keep re-running this script to add more tweets to your dataset, keeping
    in mind that you may get some duplicates in the output file if you rerun it too
    fast (that is before Twitter gets new tweets to return!). For our initial experiment,
    100 tweets will be enough, but you will probably want to come back and rerun this
    code to get that up to about 1000.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以反复运行此脚本以向您的数据集添加更多推文，但请注意，如果您运行得太快（即在Twitter获取新推文返回之前），输出文件中可能会出现一些重复的推文！对于我们最初的实验，100条推文就足够了，但您可能需要回来重新运行此代码，以将数量增加到大约1000条。
- en: Loading and classifying the dataset
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和分类数据集
- en: After we have collected a set of tweets (our dataset), we need labels to perform
    classification. We are going to label the dataset by setting up a form in a Jupyter
    Notebook to allow us to enter the labels. We do this by loading the tweets we
    collected in the previous section, iterating over them and providing (manually)
    a classification on whether they refer to Python the programming language or not.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们收集了一组推文（我们的数据集）之后，我们需要标签来进行分类。我们将通过在Jupyter Notebook中设置一个表单来标记数据集，以便我们输入标签。我们通过加载上一节中收集的推文，遍历它们，并（手动）提供是否涉及Python编程语言的分类来完成这项工作。
- en: The dataset we have stored is nearly, but not quite, in a **JSON** format. JSON
    is a format for data that doesn't impose much structure on the contents, just
    on the syntax. The idea behind JSON is that the data is in a format directly readable
    in JavaScript (hence the name, *JavaScript Object Notation*). JSON defines basic
    objects such as numbers, strings, lists, and dictionaries, making it a good format
    for storing datasets, if they contain data that isn't numerical. If your dataset
    is fully numerical, you would save space and time using a matrix-based format
    like in NumPy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储的数据集几乎，但并非完全，是**JSON**格式。JSON是一种对内容结构要求不多的数据格式，只对语法有要求。JSON背后的理念是数据以可以直接在JavaScript中读取的格式存在（因此得名，*JavaScript对象表示法*）。JSON定义了基本对象，如数字、字符串、列表和字典，这使得它成为存储非数值数据的集合的良好格式。如果你的数据集完全由数值组成，你可以使用NumPy这样的基于矩阵的格式来节省空间和时间。
- en: A key difference between our dataset and real JSON is that we included newlines
    between tweets. The reason for this was to allow us to easily append new tweets
    (the actual JSON format doesn't allow this easily). Our format is a JSON representation
    of a Tweet, followed by a newline, followed by the next Tweet, and so on.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集与真实JSON之间的一个关键区别是我们包含了推文之间的换行符。这样做的原因是允许我们轻松地追加新的推文（实际的JSON格式并不容易做到这一点）。我们的格式是一个推文的JSON表示，后跟一个换行符，然后是下一个推文，依此类推。
- en: 'To parse it, we can use the json library but we will have to first split the
    file by newlines to get the actual tweet objects themselves. Set up a new Jupyter
    Notebook, I called mine ch6_label_twitter. Within it, we will first load the data
    from our input filename by iterating over the file, storing tweets as we loop.
    The code below does a basic check that there is actual text in the tweet. If it
    does, we use the json library to load the tweet and then we add it to a list:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解析它，我们可以使用json库，但我们必须首先通过换行符分割文件，以获取实际的推文对象。设置一个新的Jupyter Notebook，我称之为ch6_label_twitter。在其中，我们首先通过遍历文件，在循环中存储推文来加载数据。下面的代码执行了一个基本的检查，以确定推文中是否有实际文本。如果有，我们使用json库来加载推文，然后将其添加到列表中：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now interested in manually classifying whether an item is relevant to
    us or not (in this case, relevant means refers to the programming language Python).
    We will use the Jupyter Notebook''s ability to embed HTML and talk between JavaScript
    and Python to create a viewer of tweets to allow us to easily and quickly classify
    the tweets as spam or not. The code will present a new tweet to the user (you)
    and ask for a label: *is it relevant or not?* It will then store the input and
    present the next tweet to be labeled.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们感兴趣的是手动分类一个项目是否与我们相关（在这种情况下，相关意味着涉及编程语言Python）。我们将使用Jupyter Notebook嵌入HTML和JavaScript与Python之间通信的能力来创建一个推文查看器，以便我们能够轻松快速地将推文分类为垃圾邮件或非垃圾邮件。代码将向用户（你）展示一个新的推文并要求一个标签：*它是相关的还是不相关？*
    然后，它将存储输入并展示下一个需要标记的推文。
- en: First, we create a list for storing the labels. These labels will be stored
    whether or not the given tweet refers to the programming language Python, and
    it will allow our classifier to learn how to differentiate between meanings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个用于存储标签的列表。这些标签将存储给定推文是否涉及编程语言Python，这将允许我们的分类器学习如何区分含义。
- en: 'We also check if we have any labels already and load them. This helps if you
    need to close the notebook down midway through labeling. This code will load the
    labels from where you left off. It is generally a good idea to consider how to
    save at midpoints for tasks like this. Nothing hurts quite like losing an hour
    of work because your computer crashed before you saved the labels! The code to
    do this loading follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还检查是否有任何已标记的标签并加载它们。这有助于你在标记过程中中途关闭笔记本。此代码将从你停止的地方加载标签。对于此类任务，考虑如何在中途保存通常是一个好主意。没有什么比因为你的电脑在你保存标签之前崩溃而丢失一个小时的工作更痛苦了！执行此加载的代码如下：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first time you run this, nothing will happen. After manually classifying
    some examples you can save your progress and then close the Notebook. After that,
    you can reopen the Notebook and return to where you were up to.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行此操作时，不会有任何动作。在手动分类一些示例后，你可以保存你的进度并关闭笔记本。之后，你可以重新打开笔记本并返回到之前的位置。
- en: If you make one or two mistakes classifying, don't worry too much. If you make
    lots of mistakes and want to start again, delete just python_classes.json and
    the above code will pick up with an empty set of classifications. If you need
    to delete all of your data and start again with new tweets, make sure to delete
    (or move) both files - python_tweets.json and python_classes.json. Otherwise,
    this Notebook will get confused, giving classes from the old dataset to the new
    tweets.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分类时犯了一个或两个错误，不必过于担心。如果你犯了很多错误并想重新开始，只需删除python_classes.json文件，上述代码将使用一个空的分类集合继续执行。如果你需要删除所有数据并使用新的推文重新开始，请确保删除（或移动）两个文件
    - python_tweets.json和python_classes.json。否则，这个笔记本会感到困惑，将旧数据集的分类信息提供给新的推文。
- en: 'Next, we create a simple function that will return the next tweet that needs
    to be labeled. We can work out which is the next tweet by finding the first one
    that hasn''t yet been labeled. The code is pretty straight-forward. We determine
    how many tweets we have labeled (with `len(labels)`), and get the next tweet in
    the tweet_sample list:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个简单的函数，该函数将返回下一个需要标记的推文。我们可以通过找到第一个尚未标记的推文来确定下一个推文。代码相当直接。我们通过`len(labels)`确定已标记的推文数量，并从tweet_sample列表中获取下一个推文：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The next step in our experiment is to collect information from the user (you!)
    on which tweets are referring to Python (the programming language) and which are
    not.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的下一步是从用户（你！）那里收集有关哪些推文是指Python（编程语言）以及哪些不是的信息。
- en: As of yet, there is not a good, straightforward way to get interactive feedback
    with pure Python in Jupyter Notebooks for such a large number of text documents.
    For this reason, we will use some JavaScript and HTML to get this input from the
    user. There are many ways to do this, below is just one example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还没有一种好的、直接的方法在Jupyter笔记本中使用纯Python与如此大量的文本文档进行交互式反馈。因此，我们将使用一些JavaScript和HTML来从用户那里获取这个输入。有许多方法可以做到这一点，下面只是一个例子。
- en: 'To get the feedback, we need a JavaScript component to load the next tweet
    and show it. We also need a HTML component to create the HTML elements to display
    that tweet. I won''t go into the details of the code here, except to give this
    general workflow:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取反馈，我们需要一个JavaScript组件来加载下一个推文并显示它。我们还需要一个HTML组件来创建显示该推文的HTML元素。这里我不会深入代码的细节，只是给出这个一般的工作流程：
- en: Obtain the next tweet that needs to be classified with `load_next_tweet`
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`load_next_tweet`获取下一个需要分类的推文
- en: Show it to the user with `handle_output`
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`handle_output`将其展示给用户
- en: Wait for the user to press either 0 or 1 with `$("input#capture").keypress`
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待用户按下0或1，使用`$("input#capture").keypress`
- en: Store that result in the classes list with `set_label`
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`set_label`将结果存储在classes列表中
- en: 'This keeps happening until we reach the end of the list (at which point an
    IndexError occurs, indicating we have no more tweets to classify). The code is
    below (remember that you can get the code from Packt or from the official GitHub
    repository):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这会一直发生，直到我们到达列表的末尾（此时会引发IndexError，表示没有更多的推文可以分类）。以下是代码（记住，你可以从Packt或官方GitHub仓库获取代码）：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will need to enter all of this code into a single cell (or copy it from
    the code bundle). It contains the mix of HTML and JavaScript necessary to get
    input from you to manually classify the tweets. If you need to stop or save your
    progress, run the following code in the next cell. It will save your progress
    (and doesn''t interrupt the above HTML code either, which can be left running):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将所有这些代码输入到一个单独的单元中（或从代码包中复制）。它包含了从你那里获取输入以手动分类推文所需的HTML和JavaScript的混合代码。如果你需要停止或保存进度，请在下一个单元中运行以下代码。它将保存你的进度（并且不会中断上面的HTML代码，可以继续运行）：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating a replicable dataset from Twitter
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Twitter创建可复制的数据集
- en: In data mining, there are lots of variables. These aren't the parameters of
    the data mining algorithms - they are the methods of data collection, how the
    environment is set up, and many other factors. Being able to replicate your results
    is important as it enables you to verify or improve upon your results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据挖掘中，有很多变量。这些不是数据挖掘算法的参数 - 这些是数据收集的方法、环境设置的方式以及许多其他因素。能够复制你的结果是重要的，因为它使你能够验证或改进你的结果。
- en: Getting 80 percent accuracy on one dataset with algorithm X, and 90 percent
    accuracy on another dataset with algorithm Y doesn't mean that Y is better. We
    need to be able to test on the same dataset in the same conditions to be able
    to properly compare. With running the preceding code, you will get a different
    dataset to the one I created and used. The main reasons are that Twitter will
    return different search results for you than me based on the time you performed
    the search.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用算法 X 在一个数据集上获得 80% 的准确率，在另一个数据集上使用算法 Y 获得 90% 的准确率，并不意味着 Y 比 X 更好。我们需要能够在相同条件下对同一数据集进行测试，以便能够正确比较。运行前面的代码，你将得到与我创建和使用的不同数据集。主要原因在于，Twitter
    会根据你搜索的时间返回与我不同的搜索结果。
- en: Even after that, your labeling of tweets might be different from what I do.
    While there are obvious examples where a given tweet relates to the python programming
    language, there will always be gray areas where the labeling isn't obvious. One
    tough gray area I ran into was tweets in non-English languages that I couldn't
    read. In this specific instance, there are options in Twitter's API for setting
    the language, but even these aren't going to be perfect.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，你对推文的标注可能与我的不同。虽然有一些明显的例子表明某个推文与 Python 编程语言相关，但总会存在一些灰色区域，标注并不明显。我遇到的一个棘手的灰色区域是无法阅读的非英语语言的推文。在这个特定的情况下，Twitter
    的 API 中有设置语言的选项，但这些选项也不完美。
- en: Due to these factors, it is difficult to replicate experiments on databases
    that are extracted from social media, and Twitter is no exception. Twitter explicitly
    disallows sharing datasets directly. One solution to this is to share tweet IDs
    only, which you can share freely. In this section, we will first create a tweet
    ID dataset that we can freely share. Then, we will see how to download the original
    tweets from this file to recreate the original dataset. First, we save the replicable
    dataset of tweet IDs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些因素，在从社交媒体提取的数据库上复制实验很困难，Twitter 也不例外。Twitter 明确禁止直接共享数据集。一种解决方案是仅共享推文 ID，你可以自由地分享。在本节中，我们首先创建一个可以自由分享的推文
    ID 数据集。然后，我们将了解如何从该文件下载原始推文以重新创建原始数据集。首先，我们保存可复制的推文 ID 数据集。
- en: 'After creating another new Jupyter Notebook, first set up the filenames as
    before. This is done in the same way we did labeling but there is a new filename
    where we can store the replicable dataset. The code is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建另一个新的 Jupyter Notebook 后，首先设置文件名与之前相同。这以与之前进行标注相同的方式进行，但有一个新的文件名，我们可以在这里存储可复制的数据集。代码如下：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We load the tweets and labels as we did in the previous notebook:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与之前笔记本相同的方式加载推文和标签：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we create a dataset by looping over both the tweets and labels at the same
    time and saving those in a list. An important side-effect of this code is, by
    putting labels first in the zip function, it will only load enough tweets for
    the labels we have created. In other words, you can run this code on partially
    classified data:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过同时遍历推文和标签并将它们保存在列表中来创建一个数据集。这个代码的一个重要副作用是，通过在 zip 函数中首先放置标签，它将只加载与我们创建的标签数量相等的推文。换句话说，你可以在部分分类的数据上运行此代码。
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we save the results in our file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果保存在我们的文件中：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have the Tweet IDs and labels saved, we can recreate the original
    dataset. If you are looking to recreate the dataset I used for this chapter, it
    can be found in the code bundle that comes with this book. Loading the preceding
    dataset is not difficult but it can take some time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经保存了推文 ID 和标签，我们可以重新创建原始数据集。如果你想要重新创建本书这一章所使用的原始数据集，它可以在本书附带代码包中找到。加载前面的数据集并不困难，但可能需要一些时间。
- en: Start a new Jupyter Notebook and set the dataset, label, and tweet ID filenames
    as before. I've adjusted the filenames here to ensure that you don't overwrite
    your previously collected dataset, but feel free to change these if you do want
    to override.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 开始一个新的 Jupyter Notebook，并将数据集、标签和推文 ID 文件名设置为之前一样。我已经调整了这里的文件名，以确保你不覆盖之前收集的数据集，但如果你确实想覆盖，也可以随意更改。
- en: 'The code is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, load the tweet IDs from the file using JSON:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用JSON从文件中加载推文ID：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Saving the labels is very easy. We just iterate through this dataset and extract
    the IDs. We could do this quite easily with just two lines of code (open file
    and save tweets). However, we can't guarantee that we will get all the tweets
    we are after (for example, some may have been changed to private since collecting
    the dataset) and therefore the labels will be incorrectly indexed against the
    data. As an example, I tried to recreate the dataset just one day after collecting
    them and already two of the tweets were missing (they might be deleted or made
    private by the user). For this reason, it is important to only print out the labels
    that we need.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 保存标签非常简单。我们只需遍历这个数据集并提取ID。我们可以用两行代码轻松做到这一点（打开文件并保存推文）。然而，我们无法保证我们会得到我们想要的全部推文（例如，一些可能在我们收集数据集后已被改为私密状态），因此标签将不正确地与数据索引。例如，我尝试在收集数据后仅一天就重新创建数据集，已经有两条推文丢失（它们可能已被删除或用户设置为私密）。因此，仅打印出我们需要的标签是很重要的。
- en: 'To do this, we first create an empty actual labels list to store the labels
    for tweets that we actually recover from twitter, and then create a dictionary
    mapping the tweet IDs to the labels. The code is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先创建一个空的实际标签列表来存储我们从推特实际恢复的推文的标签，然后创建一个将推文ID映射到标签的字典。代码如下：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we are going to create a twitter server to collect all of these tweets.
    This is going to take a little longer. Import the twitter library that we used
    before, creating an authorization token and using that to create the twitter object:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个推特服务器来收集所有这些推文。这需要一点时间。导入我们之前使用的推特库，创建一个授权令牌，并使用它来创建推特对象：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, we will loop through each of the tweet ids, and ask twitter to recover
    the original tweet. A good feature of twitter's API is that we can ask for 100
    tweets at a time, drastically reducing the number of API calls. Interestingly,
    from twitter's point of view, it is the same number of calls to get one tweet
    or 100 tweets, as long as its a single request.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历每个推文ID，并要求推特恢复原始推文。推特API的一个好特点是我们可以一次请求100条推文，这大大减少了API调用次数。有趣的是，从推特的角度来看，无论是获取一条推文还是100条推文，只要是一个单一请求，它们的调用次数是相同的。
- en: The following code will loop through our tweets in groups of 100, join together
    the id values, and get the tweet information for each of them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将按100条推文为一组循环遍历我们的推文，将ID值连接起来，并获取每组的推文信息。
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this code, we then check each tweet to see if it is a valid tweet and then
    save it to our file if it is. Our final step is to save our resulting labels:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们检查每条推文是否为有效推文，如果是，则将其保存到我们的文件中。我们的最后一步是保存我们的结果标签：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Text transformers
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本转换器
- en: Now that we have our dataset, how are we going to perform data mining on it?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了我们的数据集，我们该如何对它进行数据挖掘呢？
- en: Text-based datasets include books, essays, websites, manuscripts, programming
    code, and other forms of written expression. All of the algorithms we have seen
    so far deal with numerical or categorical features, so how do we convert our text
    into a format that the algorithm can deal with? There are a number of measurements
    that could be taken.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据集包括书籍、论文、网站、手稿、编程代码和其他形式的书面表达。我们之前看到的所有算法都处理数值或分类特征，那么我们如何将我们的文本转换为算法可以处理的形式呢？可以采取多种测量方法。
- en: For instance, average word and average sentence length are used to predict the
    readability of a document. However, there are lots of feature types such as word
    occurrence which we will now investigate.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，平均单词长度和平均句子长度被用来预测文档的可读性。然而，还有很多特征类型，例如单词出现频率，我们现在将研究这些。
- en: Bag-of-words models
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: One of the simplest but highly effective models is to simply count each word
    in the dataset. We create a matrix, where each row represents a document in our
    dataset and each column represents a word. The value of the cell is the frequency
    of that word in the document. This is known as the **bag-of-words model**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个最简单但非常有效的模型就是简单地计算数据集中每个单词的频率。我们创建一个矩阵，其中每一行代表数据集中的一篇文档，每一列代表一个单词。单元格的值是该单词在文档中的频率。这被称为**词袋模型**。
- en: Here's an excerpt from <q>The Lord of the Rings, J.R.R. Tolkien: </q>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是《魔戒》的摘录，作者J.R.R. 托尔金：
- en: <q>Three Rings for the Elven-kings under the sky,</q><q>Seven for the Dwarf-lords
    in halls of stone, Nine for Mortal Men, doomed to die,</q><q>One for the Dark
    Lord on his dark throne In the Land of Mordor where the Shadows lie.</q><q>One
    Ring to rule them all, One Ring to find them,</q><q>One Ring to bring them all
    and in the darkness bind them.</q>*In the Land of Mordor where the Shadows lie.*<q> 
                                                                    - J.R.R. Tolkien's
    epigraph to The Lord of The Rings</q>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <q>天空中精灵王的三个戒指，</q><q>石厅中矮人王的七个戒指，</q><q>凡人的九个戒指，注定要死去，</q><q>黑暗领主在他黑暗的王座上，在莫多尔的土地上，阴影之地。</q><q>一个戒指统治它们，一个戒指寻找它们，</q><q>一个戒指将它们全部带来，在黑暗中束缚它们。</q>*在莫多尔的土地上，阴影之地.*<q> 
                                                                    - J.R.R. 托尔金的《魔戒》引言</q>
- en: The word the appears nine times in this quote, while the words in, for, to,
    and one each appear four times. The word ring appears three times, as does the
    word of.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个引语中，单词the出现了九次，而单词in、for、to和one各出现了四次。单词ring和word各出现了三次。
- en: 'We can create a dataset from this, choosing a subset of words and counting
    the frequency:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些数据中创建一个数据集，选择单词的子集并计数频率：
- en: '| Word | the | one | ring | to |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | the | one | ring | to |'
- en: '| Frequency | 9 | 4 | 3 | 4 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 频率 | 9 | 4 | 3 | 4 |'
- en: 'To do this for all words in a single document, we can use the **Counter** class.
    When counting words, it is normal to convert all letters to lowercase, which we
    do when creating the string. The code is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要对单个文档中的所有单词都这样做，我们可以使用**Counter**类。在计数单词时，通常将所有字母转换为小写，我们在创建字符串时就是这样做的。代码如下：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Printing `c.most_common(5)` gives the list of the top five most frequently occurring
    words. Ties are not handled well as only five are given and a very large number
    of words all share a tie for fifth place.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 打印`c.most_common(5)`会给出出现频率最高的前五个单词的列表。由于只给出了五个，并且大量单词都共享第五名的平局，所以处理得并不好。
- en: The bag-of-words model has three major types, with many variations and alterations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型有三种主要类型，有许多变化和修改。
- en: The first is to use the raw frequencies, as shown in the preceding example.
    This has the same drawback as any non-normalised data - words with high variance
    due to high overall values (such as) *the* overshadow lower frequency (and therefore
    lower-variance) words, even though the presence of the word *the* rarely has much
    importance.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法是使用原始频率，如前例所示。这和非归一化数据有相同的缺点——由于整体值高（如）*the*这样的单词具有高方差，它们会掩盖低频率（因此低方差）的单词，尽管单词*the*的存在很少有很大的重要性。
- en: The second model is to use the normalized frequency, where each document's sum
    equals 1\. This is a much better solution as the length of the document doesn't
    matter as much, but it still means words like the overshadow lower frequency words.
    The third type is to simply use binary features—a value is 1 if it occurs, and
    0 otherwise. We will use binary representation in this chapter.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种模型是使用归一化频率，其中每个文档的总和等于1。这是一个更好的解决方案，因为文档的长度并不那么重要，但它仍然意味着像*the*这样的单词会掩盖低频率的单词。第三种类型是简单地使用二进制特征——如果出现，则值为1，否则为0。我们将在本章中使用二进制表示。
- en: Another (arguably more popular) method for performing normalization is called
    **term frequency-inverse document frequency** (**tf-idf**). In this weighting
    scheme, term counts are first normalized to frequencies and then divided by the
    number of documents in which it appears in the corpus. We will use tf-idf in [Chapter
    10](lrn-dtmn-py-2e_ch10.html)*, Clustering News Articles*.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种（可以说是更受欢迎）的归一化方法是称为**词频-逆文档频率**（**tf-idf**）。在这个加权方案中，首先将词频归一化到频率，然后除以它在语料库中出现的文档数。我们将在[第10章](lrn-dtmn-py-2e_ch10.html)*，聚类新闻文章*中使用tf-idf。
- en: n-gram features
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n-gram特征
- en: One variation on the standard bag-of-words model is called the n-gram model.
    An n-grams model addresses the deficiency of context in the bag-of-words model.
    With a bag-of-words model, only individual words are counted by themselves. This
    means that common word pairs, such as *United States*, lose meaning they have
    in the sentence because they are treated as individual words.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 标准词袋模型的一种变体被称为n-gram模型。n-gram模型解决了词袋模型中上下文不足的问题。在词袋模型中，只有单个单词本身被计数。这意味着像*美国*这样的常见词对，因为它们被视为单个单词，所以失去了在句子中的意义。
- en: There are algorithms that can read a sentence, parse it into a tree-like structure,
    and use this to create very accurate representations of the meaning behind words.
    Unfortunately, these algorithms are computationally expensive. This makes it difficult
    to apply them to large datasets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些算法可以读取一个句子，将其解析成树状结构，并使用它来创建非常精确的词语背后的意义表示。不幸的是，这些算法在计算上很昂贵。这使得将它们应用于大型数据集变得困难。
- en: To compensate for these issues of context and complexity, the n-grams model
    fits into the middle ground. It has more context than the bag-of-words model,
    while only being slightly more expensive computationally.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些上下文和复杂性的问题，n-gram模型处于中间地带。它比词袋模型有更多的上下文，而在计算上仅略贵一些。
- en: An n-gram is a subsequence of *n* consecutive, overlapping, tokens. In this
    experiment, we use word n-grams, which are n-grams of word-tokens. They are counted
    the same way as a bag-of-words, with the n-grams forming a *word* that is put
    in the bag. The value of a cell in this dataset is the frequency that a particular
    n-gram appears in the given document.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram是*n*个连续、重叠的标记的子序列。在这个实验中，我们使用词n-gram，即词标记的n-gram。它们的计数方式与词袋相同，n-gram形成一个*词*放入袋中。这个数据集中单元格的值是特定n-gram在给定文档中出现的频率。
- en: The value of n is a parameter. For English, setting it to between 2 to 5 is
    a good start, although some applications call for higher values. Higher values
    for n result in sparse datasets, as when n increases it is less likely to have
    the same n-gram appear across multiple documents. Having n=1 results in simply
    the bag-of-words model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: n的值是一个参数。对于英语，将其设置为2到5之间是一个好的开始，尽管某些应用需要更高的值。n的值越高，数据集就越稀疏，因为当n增加时，相同的n-gram出现在多份文档中的可能性就越小。n=1的结果就是简单的词袋模型。
- en: 'As an example, `for n=3`, we extract the first few n-grams in the following
    quote:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于`n=3`，我们在以下引文中提取了前几个n-gram：
- en: '*Always look on the bright side of life.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*Always look on the bright side of life.*'
- en: The first n-gram (of size 3) is *Always look on*, the second is *look on the*,
    the third is *on the bright*. As you can see, the n-grams overlap and cover three
    words each. Word n-grams have advantages over using single words. This simple
    concept introduces some context to word use by considering its local environment,
    without a large overhead of understanding the language computationally.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个n-gram（大小为3）是*Always look on*，第二个是*look on the*，第三个是*on the bright*。正如你所见，n-gram重叠并覆盖了三个单词。词n-gram相对于使用单个单词有优势。这个简单概念通过考虑其局部环境为单词使用引入了一些上下文，而不需要大量理解语言的计算开销。
- en: A disadvantage of using n-grams is that the matrix becomes even sparser—word
    n-grams are unlikely to appear twice (especially in tweets and other short documents!). Specifically
    for social media and other short documents, word n-grams are unlikely to appear
    in too many different tweets, unless it is a retweet. However, in larger documents,
    word n-grams are quite effective for many applications. Another form of n-gram
    for text documents is that of a character n-gram. That said, you'll see shortly
    that word n-grams are quite effective in practice.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用n-gram的一个缺点是矩阵变得更加稀疏——词n-gram不太可能重复出现（尤其是在推文和其他短文档中！）。特别是对于社交媒体和其他短文档，除非是转发，否则词n-gram不太可能出现在太多的不同推文中。然而，在更大的文档中，词n-gram对于许多应用来说非常有效。文本文档的另一种n-gram形式是字符n-gram。尽管如此，你很快就会看到词n-gram在实践中非常有效。
- en: Rather than using sets of words, we simply use sets of characters (although
    character n-grams have lots of options for how they are computed!). This type
    of model can help identify words that are misspelled, as well as providing other
    benefits to classification. We will test character n-grams in this chapter and
    see them again in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*, Authorship Attribution*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用单词集合，而是简单地使用字符集合（尽管字符n-gram在计算方式上有许多选择！）。这种类型的模型可以帮助识别拼写错误的单词，并为分类提供其他好处。我们将在本章测试字符n-gram，并在[第9章](lrn-dtmn-py-2e_ch09.html)*，作者归属*中再次看到它们。
- en: Other text features
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他文本特征
- en: There are other features that can be extracted too. These include syntactic
    features, such as the usage of particular words in sentences. Part-of-speech tags
    are also popular for data mining applications that need to understand meaning
    in text. Such feature types won't be covered in this book. If you are interested
    in learning more, I recommend *Python 3 Text Processing with NLTK 3 Cookbook,
    Jacob Perkins, Packt publication*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以提取其他特征。这些包括句法特征，例如句子中特定单词的使用。词性标注在需要理解文本意义的文本挖掘应用中也非常流行。这类特征类型本书不会涉及。如果你有兴趣了解更多，我推荐阅读*《Python
    3 文本处理与 NLTK 3 烹饪书》，作者 Jacob Perkins，Packt 出版*。
- en: There are a number of libraries for working with text data in Python. The most
    commonly known one is called the Natural Language ToolKit (NLTK). The scikit-learn
    library also has the CountVectorizer class that performs a similar action, and
    it is recommended you take a look at it (we will use it in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*,
    Authorship Attribution*). NLTK has more features for word tokenization and part
    of speech tagging (that is identifying which words are nouns, verbs and so on).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中有许多用于处理文本数据的库。最广为人知的一个叫做自然语言工具包（NLTK）。scikit-learn 库也有一个 CountVectorizer
    类，它执行类似的功能，我建议你看看它（我们将在[第 9 章](lrn-dtmn-py-2e_ch09.html)*，作者归属*中使用它）。NLTK 在单词分词和词性标注（即识别哪些单词是名词、动词等）方面有更多功能。
- en: The library we are going to use is called spaCy. It is designed from the ground
    up to be fast and reliable for natural language processing. Its less-well-known
    than NLTK, but is rapidly growing in popularity. It also simplifies some of the
    decisions, but has a slightly more difficult syntax to use, compared to NLTK.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的库叫做 spaCy。它从头开始设计，旨在为自然语言处理提供快速和可靠的性能。它不如 NLTK 为人所知，但正迅速增长其受欢迎程度。它还简化了一些决策，但与
    NLTK 相比，它的语法稍微复杂一些。
- en: For production systems, I recommend using spaCy, which is faster than NLTK.
    NLTK was built for teaching, while spaCy was built for production. They have different
    syntaxes, meaning it can be difficult to port code from one library to another.
    If you aren't looking into experimenting with different types of natural language
    parsers, I recommend using spaCy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，我推荐使用spaCy，它比NLTK更快。NLTK是为了教学而构建的，而spaCy是为了生产而构建的。它们有不同的语法，这意味着将代码从一个库移植到另一个库可能会有困难。如果你不打算尝试不同的自然语言解析器，我推荐使用spaCy。
- en: Naive Bayes
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真贝叶斯
- en: 'Naive Bayes is a probabilistic model that is, unsurprisingly, built upon a
    naive interpretation of Bayesian statistics. Despite the naive aspect, the method
    performs very well in a large number of contexts. Because of the naive aspect,
    it works quite quickly. It can be used for classification of many different feature
    types and formats, but we will focus on one in this chapter: binary features in
    the bag-of-words model.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 天真贝叶斯是一种概率模型，不出所料，它基于对贝叶斯统计的天真解释。尽管有天真的一面，该方法在许多情况下表现都非常出色。正因为有天真的一面，它运行得相当快。它可以用于许多不同特征类型和格式的分类，但我们将在本章中关注一种：词袋模型中的二元特征。
- en: Understanding Bayes' theorem
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解贝叶斯定理
- en: For most of us, when we were taught statistics, we started from a **frequentist
    approach**. In this approach, we assume the data comes from some distribution
    and we aim to determine what the parameters are for that distribution. However,
    those parameters are (perhaps incorrectly) assumed to be fixed. We use our model
    to describe the data, even testing to ensure the data fits our model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们中的大多数人来说，当我们学习统计学时，我们是从**频率统计方法**开始的。在这个方法中，我们假设数据来自某个分布，我们的目标是确定该分布的参数。然而，这些参数（可能是不正确地）被认为是固定的。我们使用我们的模型来描述数据，甚至测试以确保数据符合我们的模型。
- en: Bayesian statistics instead model how people (at least, non-frequentist statisticians)
    actually reason. We have some data, and we use that data to update our model about
    how likely something is to occur. In Bayesian statistics, we use the data to describe
    the model rather than using a model and confirming it with data (as per the frequentist
    approach).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯统计反而模型化了人们（至少，非频率统计学家）实际推理的方式。我们有一些数据，我们使用这些数据来更新我们对某事发生的可能性的模型。在贝叶斯统计中，我们使用数据来描述模型，而不是使用模型并用数据来验证它（如频率统计方法）。
- en: It should be noted that frequentist statistics and Bayesian statistics ask and
    answer slightly different questions. A direct comparison is not always correct.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意，频率统计和贝叶斯统计提出和回答的问题略有不同。直接的比较并不总是正确的。
- en: Bayes' theorem computes the value of P(A|B). That is, knowing that B has occurred,
    what is the probability of event A occurring. In most cases, B is an observed
    event such as *it rained yesterday*, and A is a prediction it will rain today.
    For data mining, B is usually *we observed this sample* and A is *does the sample
    belong to this class* (the class prediction). We will see how to use Bayes' theorem
    for data mining in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理计算 P(A|B) 的值。也就是说，知道 B 已经发生，事件 A 发生的概率是多少。在大多数情况下，B 是一个观察事件，如 *昨天下雨了*，而
    A 是预测今天会下雨。在数据挖掘中，B 通常是指 *我们观察到了这个样本*，而 A 是 *这个样本是否属于这个类别*（类别预测）。我们将在下一节中看到如何使用贝叶斯定理进行数据挖掘。
- en: 'The equation for Bayes'' theorem is given as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理的公式如下：
- en: '![](img/B06162_06_01.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_06_01.png)'
- en: As an example, we want to determine the probability that an e-mail containing
    the word drugs is spam (as we believe that such a tweet may be a pharmaceutical
    spam).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们想要确定包含单词 drugs 的电子邮件是垃圾邮件的概率（因为我们认为这样的推文可能是药品垃圾邮件）。
- en: '*A*, in this context, is the probability that this tweet is spam. We can compute
    *P(A)*, called the prior belief directly from a training data by computing the
    percentage of tweets in our dataset that are spam. If our dataset contains 30
    spam messages for every 100 e-mails, *P(A)* is 30/100 or 0.3.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，*A* 是这条推文是垃圾邮件的概率。我们可以通过直接从训练数据计算数据集中推文是垃圾邮件的百分比来计算 *P(A*)，称为先验信念。如果我们的数据集中每
    100 封电子邮件中有 30 封垃圾邮件，那么 *P(A)* 就是 30/100 或 0.3。
- en: '*B*, in this context, is this tweet contains the word *drugs*. Likewise, we
    can compute *P(B)* by computing the percentage of tweets in our dataset containing
    the word drugs. If 10 e-mails in every 100 of our training dataset contain the
    word drugs, *P(B)* is 10/100 or 0.1\. Note that we don''t care if the e-mail is
    spam or not when computing this value.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，*B* 是这条推文包含单词 *drugs*。同样，我们可以通过计算我们数据集中包含单词 drugs 的推文百分比来计算 *P(B)*。如果我们训练数据集中的每
    100 封电子邮件中有 10 封包含单词 drugs，那么 *P(B)* 就是 10/100 或 0.1。请注意，在计算这个值时，我们并不关心电子邮件是否是垃圾邮件。
- en: '*P(B|A)* is the probability that an e-mail contains the word drugs if it is
    spam. This is also easy to compute from our training dataset. We look through
    our training set for spam e-mails and compute the percentage of them that contain
    the word drugs. Of our 30 spam e-mails, if 6 contain the word drugs, then *P(B|A)*
    is calculated as 6/30 or 0.2.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(B|A)* 是如果电子邮件是垃圾邮件，那么它包含单词 drugs 的概率。这也可以通过我们的训练数据集轻松计算。我们查看我们的训练集以找到垃圾邮件，并计算其中包含单词
    drugs 的百分比。在我们的 30 封垃圾邮件中，如果有 6 封包含单词 drugs，那么 *P(B|A)* 就被计算为 6/30 或 0.2。'
- en: From here, we use Bayes' theorem to compute *P(A|B)*, which is the probability
    that a tweet containing the word drugs is spam. Using the previous equation, we
    see the result is 0.6\. This indicates that if an e-mail has the word drugs in
    it, there is a 60 percent chance that it is spam.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们使用贝叶斯定理来计算 *P(A|B)*，这是包含单词 drugs 的推文是垃圾邮件的概率。使用前面的方程，我们看到结果是 0.6。这表明如果一封电子邮件中包含单词
    drugs，那么它有 60% 的可能是垃圾邮件。
- en: Note the empirical nature of the preceding example—we use evidence directly
    from our training dataset, not from some preconceived distribution. In contrast,
    a frequentist view of this would rely on us creating a distribution of the probability
    of words in tweets to compute similar equations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意前面示例的经验性质——我们直接使用来自我们的训练数据集的证据，而不是来自某种先验分布。相比之下，频率派对此的看法将依赖于我们创建推文中单词概率的分布来计算类似的方程。
- en: Naive Bayes algorithm
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法
- en: Looking back at our Bayes' theorem equation, we can use it to compute the probability
    that a given sample belongs to a given class. This allows the equation to be used
    as a classification algorithm.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的贝叶斯定理方程，我们可以使用它来计算给定样本属于给定类别的概率。这使得方程可以用作分类算法。
- en: With *C* as a given class and *D* as a sample in our dataset, we create the
    elements necessary for Bayes' theorem, and subsequently Naive Bayes. Naive Bayes
    is a classification algorithm that utilizes Bayes' theorem to compute the probability
    that a new data sample belongs to a particular class.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以 *C* 作为给定的类别，以 *D* 作为我们数据集中的样本，我们创建了贝叶斯定理所需的元素，随后是朴素贝叶斯。朴素贝叶斯是一种分类算法，它利用贝叶斯定理来计算新数据样本属于特定类别的概率。
- en: '*P(D)* is the probability of a given data sample. It can be difficult to compute
    this, as the sample is a complex interaction between different features, but luckily
    it is constant across all classes. Therefore, we don''t need to compute it at
    all, as all we do in the final step is compare relative values.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(D)* 是给定数据样本的概率。计算这个值可能很困难，因为样本是不同特征之间复杂交互的结果，但幸运的是，它在所有类别中都是恒定的。因此，我们根本不需要计算它，因为在最后一步我们只是比较相对值。'
- en: '*P(D|C)* is the probability of the data point belonging to the class. This
    could also be difficult to compute due to the different features. However, this
    is where we introduce the naive part of the Naive Bayes algorithm. We naively
    assume that each feature is independent of each other. Rather than computing the
    full probability of *P(D|C)*, we compute the probability of each feature *D1,
    D2, D3, ...* and so on. Then, we just multiply them together:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(D|C)* 是数据点属于该类别的概率。由于不同特征的存在，这可能也很难计算。然而，这正是我们引入朴素贝叶斯算法朴素部分的地方。我们天真地假设每个特征都是相互独立的。我们不是计算
    *P(D|C)* 的完整概率，而是计算每个特征 *D1, D2, D3, ...* 等等的概率。然后，我们只是将它们相乘：'
- en: P*(D|C) = P(D1|C) x P(D2|C).... x P(Dn|C)*
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: P*(D|C) = P(D1|C) x P(D2|C).... x P(Dn|C)*
- en: Each of these values is relatively easy to compute with binary features; we
    simply compute the percentage of times it is equal in our sample dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值对于二元特征来说相对容易计算；我们只需计算在我们的样本数据集中它等于的百分比。
- en: In contrast, if we were to perform a non-naive Bayes version of this part, we
    would need to compute the correlations between different features for each class.
    Such computation is infeasible at best, and nearly impossible without vast amounts
    of data or adequate language analysis models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们对这部分进行非朴素贝叶斯版本的实现，我们需要计算每个类别中不同特征之间的相关性。这种计算在最好情况下也是不可行的，在没有大量数据或适当的语言分析模型的情况下几乎是不可能的。
- en: From here, the algorithm is straightforward. We compute P(C|D) for each possible
    class, ignoring the P(D) term entirely. Then we choose the class with the highest
    probability. As the P(D) term is consistent across each of the classes, ignoring
    it has no impact on the final prediction.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，算法很简单。我们计算每个可能类别的 P(C|D)，完全忽略 P(D) 项。然后我们选择概率最高的类别。由于 P(D) 项在各个类别中是一致的，忽略它对最终预测没有影响。
- en: How it works
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'As an example, suppose we have the following (binary) feature values from a
    sample in our dataset: [0, 0, 0, 1].'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们从数据集中的一个样本中获取以下（二元）特征值：[0, 0, 0, 1]。
- en: 'Our training dataset contains two classes with 75 percent of samples belonging
    to the class 0, and 25 percent belonging to the class 1\. The likelihood of the
    feature values for each class are as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据集包含两个类别，其中 75% 的样本属于类别 0，25% 属于类别 1。每个类别的特征值概率如下：
- en: 'For class 0: [0.3, 0.4, 0.4, 0.7]'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别 0：[0.3, 0.4, 0.4, 0.7]
- en: 'For class 1: [0.7, 0.3, 0.4, 0.9]'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别 1：[0.7, 0.3, 0.4, 0.9]
- en: 'These values are to be interpreted as: for feature 1, it has a value of 1 in
    30 percent of cases for samples with class 0\. It is a value of 1 in 70 percent
    of samples with class 1.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值可以这样解释：对于特征 1，在类别 0 的样本中有 30% 的情况下其值为 1。在类别 1 的样本中有 70% 的情况下其值为 1。
- en: 'We can now compute the probability that this sample should belong to the class
    0\. P(C=0) = 0.75 which is the probability that the class is 0\. Again, P(D) isn''t
    needed for the Naive Bayes algorithm and is simply removed from the equation.
    Let''s take a look at the calculation:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算这个样本属于类别 0 的概率。P(C=0) = 0.75，这是类别为 0 的概率。同样，P(D) 对于朴素贝叶斯算法不是必需的，所以我们简单地从方程中移除。让我们看看计算过程：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The second and third values are 0.6, because the value of that feature in the
    sample was 0\. The listed probabilities are for values of 1 for each feature.
    Therefore, the probability of a 0 is its inverse: *P(0) = 1 – P(1)*.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个和第三个值是 0.6，因为该特征在样本中的值为 0。列出的概率是每个特征值为 1 的情况。因此，0 的概率是其倒数：*P(0) = 1 – P(1)*。
- en: 'Now, we can compute the probability of the data point belonging to this class.
    Let''s take a look at the calculation:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算数据点属于这个类别的概率。让我们看看计算过程：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we compute the same values for the class 1:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算类别 1 的相同值：
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Normally, P(C=0|D) + P(C=1|D) should equal to 1\. After all, those are the only
    two possible options! However, the probabilities are not 1 due to the fact we
    haven't included the computation of P(D) in our equations here.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，P(C=0|D) + P(C=1|D) 应该等于 1。毕竟，那只有两个可能的选择！然而，由于我们没有在我们的方程中包含 P(D) 的计算，这些概率并不等于
    1。
- en: As the value for *P(C=1|D)* is more than *P(C=0|D)*, the data point should be
    classified as belonging to the class 1\. You may have guessed this while going
    through the equations anyway; however, you may have been a bit surprised that
    the final decision was so close. After all, the probabilities in computing *P(D|C)*
    were much, much higher for the class 1\. This is because we introduced a prior
    belief that most samples generally belong to the class 0.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *P(C=1|D)* 的值大于 *P(C=0|D)*，数据点应被分类为属于类别 1。尽管在推导公式时你可能已经猜到了这一点；然而，你可能对最终的决定如此接近感到有些惊讶。毕竟，在计算
    *P(D|C)* 时，类别 1 的概率要高得多。这是因为我们引入了一个先验信念，即大多数样本通常属于类别 0。
- en: If the classes had been equal sizes, the resulting probabilities would be much
    different. Try it yourself by changing both *P(C=0)* and *P(C=1)* to 0.5 for equal
    class sizes and computing the result again.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别的尺寸相等，结果概率将大不相同。尝试将 *P(C=0)* 和 *P(C=1)* 都改为 0.5 以实现相等的类别尺寸，并再次计算结果。
- en: Applying of Naive Bayes
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用朴素贝叶斯
- en: We will now create a pipeline that takes a tweet and determines whether it is
    relevant or not, based only on the content of that tweet.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个管道，它接受一条推文并确定它是否相关，仅基于该推文的内容。
- en: To perform the word extraction, we will be using the spaCy, a library that contains
    a large number of tools for performing analysis on natural language. We will use
    spaCy in future chapters as well.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行单词提取，我们将使用 spaCy，这是一个包含大量用于对自然语言进行分析的工具的库。我们将在未来的章节中继续使用 spaCy。
- en: 'To get spaCy on your computer, use pip to install the package: pip install
    spacy'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的计算机上安装 spaCy，请使用 pip 安装该包：pip install spacy
- en: If that doesn't work, see the spaCy installation instructions at [https://spacy.io/](https://spacy.io/) for
    information specific to your platform.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不起作用，请参阅 [https://spacy.io/](https://spacy.io/) 上的 spaCy 安装说明，以获取针对您平台的具体信息。
- en: 'We are going to create a pipeline to extract the word features and classify
    the tweets using Naive Bayes. Our pipeline has the following steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个管道来提取单词特征并使用朴素贝叶斯对推文进行分类。我们的管道有以下步骤：
- en: Transform the original text documents into a dictionary of counts using spaCy's
    word tokenization.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 的单词分词将原始文本文档转换为计数字典。
- en: Transform those dictionaries into a vector matrix using the `DictVectorizer`
    transformer in scikit-learn. This is necessary to enable the Naive Bayes classifier
    to read the feature values extracted in the first step.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 中的 `DictVectorizer` 转换器将这些字典转换为向量矩阵。这是必要的，以便使朴素贝叶斯分类器能够读取第一步中提取的特征值。
- en: Train the Naive Bayes classifier, as we have seen in previous chapters.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照前几章所见，训练朴素贝叶斯分类器。
- en: We will need to create another Notebook (last one for the chapter!) called `ch6_classify_twitter`
    for performing the classification.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建另一个名为 `ch6_classify_twitter` 的 Notebook（这是本章的最后一个 Notebook！），用于执行分类。
- en: Extracting word counts
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取单词计数
- en: We are going to use spaCy to extract our word counts. We still want to use it
    in a pipeline, but spaCy doesn't conform to our transformer interface. We will need
    to create a basic transformer to do this to obtain both fit and transform methods,
    enabling us to use this in a pipeline.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 spaCy 来提取我们的单词计数。我们仍然想在管道中使用它，但 spaCy 不符合我们的转换器接口。我们将需要创建一个基本的转换器来完成这项工作，以获得
    fit 和 transform 方法，使我们能够将其用于管道。
- en: First, set up the transformer class. We don't need to *fit* anything in this
    class, as this transformer simply extracts the words in the document. Therefore,
    our fit is an empty function, except that it returns self which is necessary for
    transformer objects to conform to the scikit-learn API.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置转换器类。在这个类中我们不需要 *fit* 任何东西，因为这个转换器只是提取文档中的单词。因此，我们的 fit 是一个空函数，除了它返回 self，这对于转换器对象来说是为了符合
    scikit-learn API。
- en: Our transform is a little more complicated. We want to extract each word from
    each document and record True if it was discovered. We are only using the binary
    features here—True if in the document, False otherwise. If we wanted to use the
    frequency we would set up counting dictionaries, as we have done in several of
    the past chapters.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的转换稍微复杂一些。我们想要从每个文档中提取每个单词，并记录如果它被发现则为 True。我们在这里只使用二元特征——如果在文档中则为 True，否则为
    False。如果我们想使用频率，我们将设置计数字典，就像我们在过去几章中做的那样。
- en: 'Let''s take a look at the code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码：
- en: '[PRE23]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The result is a list of dictionaries, where the first dictionary is the list
    of words in the first tweet, and so on. Each dictionary has a word as key and
    the value True to indicate this word was discovered. Any word not in the dictionary
    will be assumed to have not occurred in the tweet. Explicitly stating that a word's
    occurrence is False will also work, but will take up needless space to store.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个字典列表，其中第一个字典是第一个推文中的单词列表，依此类推。每个字典都有一个单词作为键，值为True表示该单词被发现。不在字典中的任何单词都将假定没有出现在推文中。明确指出单词的出现为False也可以，但这会占用不必要的空间来存储。
- en: Converting dictionaries to a matrix
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将字典转换为矩阵
- en: The next step converts the dictionaries built as per the previous step into
    a matrix that can be used with a classifier. This step is made quite simple through
    the DictVectorizer transformer that is provided as part of scikit-learn.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将根据前一步构建的字典转换为可以用于分类器的矩阵。这一步通过scikit-learn提供的DictVectorizer转换器变得相当简单。
- en: The `DictVectorizer` class simply takes a list of dictionaries and converts
    them into a matrix. The features in this matrix are the keys in each of the dictionaries,
    and the values correspond to the occurrence of those features in each sample.
    Dictionaries are easy to create in code, but many data algorithm implementations
    prefer matrices. This makes `DictVectorizer` a very useful class.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`DictVectorizer`类简单地将字典列表转换为矩阵。这个矩阵中的特征是每个字典中的键，而值对应于这些特征在每个样本中的出现次数。在代码中创建字典很容易，但许多数据算法实现更喜欢矩阵。这使得`DictVectorizer`成为一个非常有用的类。'
- en: In our dataset, each dictionary has words as keys and only occurs if the word
    actually occurs in the tweet. Therefore, our matrix will have each word as a feature
    and a value of True in the cell if the word occurred in the tweet.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，每个字典都有单词作为键，并且只有在单词实际上出现在推文中时才会出现。因此，我们的矩阵将具有每个单词作为特征，如果单词出现在推文中，则单元格中的值为True。
- en: 'To use `DictVectorizer`, simply import it using the following command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`DictVectorizer`，只需使用以下命令导入：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Putting it all together
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: 'Finally, we need to set up a classifier and we are using Naive Bayes for this
    chapter. As our dataset contains only binary features, we use the `BernoulliNB`
    classifier that is designed for binary features. As a classifier, it is very easy
    to use. As with `DictVectorizer`, we simply import it and add it to our pipeline:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要设置一个分类器，我们在这个章节中使用朴素贝叶斯。由于我们的数据集只包含二元特征，我们使用专门为二元特征设计的`BernoulliNB`分类器。作为一个分类器，它非常容易使用。与`DictVectorizer`一样，我们只需导入它并将其添加到我们的管道中：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now comes the moment to put all of these pieces together. In our Jupyter Notebook,
    set the filenames and load the dataset and classes as we have done before. Set
    the filenames for both the tweets themselves (not the IDs!) and the labels that
    we assigned to them. The code is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将这些部件组合在一起了。在我们的Jupyter Notebook中，设置文件名并加载数据集和类别，就像我们之前做的那样。设置推文本身（不是ID！）和分配给它们的标签的文件名。代码如下：
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Load the tweets themselves. We are only interested in the content of the tweets,
    so we extract the text value and store only that. The code is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 加载推文本身。我们只对推文的内容感兴趣，所以我们提取文本值并仅存储该值。代码如下：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, create a pipeline putting together the components from before. Our pipeline
    has three parts:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个将之前组件组合在一起的管道。我们的管道有三个部分：
- en: The NLTKBOW transformer we created.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建的NLTKBOW转换器。
- en: A DictVectorizer transformer.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个DictVectorizer转换器。
- en: A BernoulliNB classifier.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个BernoulliNB分类器。
- en: 'The code is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can nearly run our pipeline now, which we will do with `cross_val_score` 
    as we have done many times before. Before we perform the data mining, we will
    introduce a better evaluation metric than the accuracy metric we used before.
    As we will see, the use of accuracy is not adequate for datasets when the number
    of samples in each class is different.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在几乎可以运行我们的管道了，我们将使用`cross_val_score`来完成，就像我们之前多次做的那样。在我们进行数据挖掘之前，我们将介绍一个比我们之前使用的准确度指标更好的评估指标。正如我们将看到的，当每个类中的样本数量不同时，使用准确度是不够的。
- en: Evaluation using the F1-score
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用F1分数进行评估
- en: When choosing an evaluation metric, it is always important to consider cases
    where that evaluation metric is not useful. Accuracy is a good evaluation metric
    in many cases, as it is easy to understand and simple to compute. However, it
    can be easily faked. In other words, in many cases, you can create algorithms
    that have a high accuracy but have a poor utility.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择评估指标时，始终重要的是要考虑那些评估指标不适用的情况。在许多情况下，准确率是一个好的评估指标，因为它容易理解且易于计算。然而，它很容易被伪造。换句话说，在许多情况下，你可以创建具有高准确率但效用差的算法。
- en: While our dataset of tweets (typically, your results may vary) contains about
    50 percent programming-related and 50 percent nonprogramming, many datasets aren't
    as *balanced* as this.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的推文数据集（通常，你的结果可能会有所不同）包含大约50%与编程相关的和50%非编程的，但许多数据集并不像这样*平衡*。
- en: As an example, an e-mail spam filter may expect to see more than 80 percent
    of incoming e-mails be spam. A spam filter that simply labels everything as spam
    is quite useless; however, it will obtain an accuracy of 80 percent!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个电子邮件垃圾邮件过滤器可能期望看到80%以上的 incoming e-mails 是垃圾邮件。一个简单地将所有内容标记为垃圾邮件的垃圾邮件过滤器相当无用；然而，它将获得80%的准确率！
- en: To get around this problem, we can use other evaluation metrics. One of the
    most commonly employed is called an **f1-score** (also called f-score, f-measure,
    or one of many other variations on this term).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用其他评估指标。最常用的一个被称为**f1分数**（也称为f-score、f-measure或这个术语的许多其他变体）。
- en: 'The F1-score is defined on a per-class basis and is based on two concepts:
    the precision and recall. The precision is the percentage of all the samples that
    were predicted as belonging to a specific class, that were actually from that
    class. The recall is the percentage of samples in the dataset that are in a class and
    actually labeled as belonging to that class.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是基于每个类别的定义，并基于两个概念：精确度和召回率。精确度是所有预测属于特定类别的样本中，实际上属于该类别的百分比。召回率是数据集中属于某个类别的样本中，实际上被标记为属于该类别的百分比。
- en: In the case of our application, we could compute the value for both classes
    (python-programming and not python-programming).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用中，我们可以计算两个类别（python-programming 和 not python-programming）的值。
- en: 'Our precision computation becomes the question: <q>of all the tweets that were
    predicted as being relevant, what percentage were actually relevant?</q>'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的精确度计算成为问题：<q>在所有被预测为相关的推文中，有多少实际上是相关的？</q>
- en: 'Likewise, the recall becomes the question: <q>of all the relevant tweets in
    the data set, how many were predicted as being relevant?</q>'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，召回率成为问题：<q>在数据集中所有相关的推文中，有多少被预测为相关？</q>
- en: 'After you compute both the precision and recall, the f1-score is the harmonic
    mean of the precision and recall:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了精确度和召回率之后，f1分数是精确度和召回率的调和平均数：
- en: 'To use the f1-score in scikit-learn methods, simply set the scoring parameter
    to f1\. By default, this will return the f1-score of the class with label 1\.
    Running the code on our dataset, we simply use the following line of code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要在scikit-learn方法中使用f1分数，只需将scoring参数设置为f1。默认情况下，这将返回标签为1的类的f1分数。在我们的数据集上运行代码，我们只需使用以下代码行：
- en: '[PRE29]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The result is 0.684, which means we can accurately determine if a tweet using
    Python relates to the programing language nearly 70 percent of the time. This
    is using a dataset with only 300 tweets in it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是0.684，这意味着我们几乎70%的时间可以准确地确定使用Python的推文是否与编程语言相关。这是使用只有300条推文的数据集实现的。
- en: Go back and collect more data and you will find that the results increase! Keep
    in mind that your dataset may differ, and therefore your results would too.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 回去收集更多的数据，你会发现结果会增加！记住，你的数据集可能不同，因此你的结果也会不同。
- en: More data usually means a better accuracy, but it is not guaranteed!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 更多数据通常意味着更高的准确率，但这并不保证！
- en: Getting useful features from models
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从模型中获取有用的特征
- en: One question you may ask is, what are the best features for determining if a
    tweet is relevant or not? We can extract this information from our Naive Bayes
    model and find out which features are the best individually, according to Naive
    Bayes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的一个问题是，判断一条推文是否相关，最好的特征是什么？我们可以从我们的朴素贝叶斯模型中提取这些信息，并找出哪些特征在朴素贝叶斯中是最佳的。
- en: 'First, we fit a new model. While the `cross_val_score` gives us a score across
    different folds of cross-validated testing data, it doesn''t easily give us the
    trained models themselves. To do this, we simply fit our pipeline with the tweets,
    creating a new model. The code is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们拟合一个新的模型。虽然`cross_val_score`给出了不同交叉验证测试数据折叠的分数，但它并不容易给出训练模型本身。为此，我们只需用推文拟合我们的管道，创建一个新的模型。代码如下：
- en: '[PRE30]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that we aren't really evaluating the model here, so we don't need to be
    as careful with the training/testing split. However, before you put these features
    into practice, you should evaluate on a separate test split. We skip over that
    here for the sake of clarity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里并不是真正评估模型，所以我们不需要对训练/测试分割那么小心。然而，在将这些特征投入实践之前，你应该在单独的测试分割上进行评估。我们在这里跳过这一点是为了清晰起见。
- en: 'A pipeline gives you access to the individual steps through the `named_steps`
    attribute and the name of the step (we defined these names ourselves when we created
    the pipeline object itself). For instance, we can get the Naive Bayes model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 管道通过`named_steps`属性和步骤的名称（我们在创建管道对象本身时定义了这些名称）提供了对各个步骤的访问。例如，我们可以获取朴素贝叶斯模型：
- en: '[PRE31]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: From this model, we can extract the probabilities for each word. These are stored
    as log probabilities, which is simply *log(P(A|f))*, where *f* is a given feature.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个模型中，我们可以提取每个单词的概率。这些存储为对数概率，即*log(P(A|f))*，其中*f*是给定的特征。
- en: The reason these are stored as log probabilities is because the actual values
    are very low. For instance, the first value is -3.486, which correlates to a probability
    under 0.03 percent. Logarithm probabilities are used in computation involving
    small probabilities like this as they stop underflow errors where very small values
    are just rounded to zeros. Given that all of the probabilities are multiplied
    together, a single value of 0 will result in the whole answer always being 0!
    Regardless, the relationship between values is still the same; the higher the
    value, the more useful that feature is.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征被存储为对数概率的原因是实际值非常低。例如，第一个值是-3.486，这对应于低于0.03%的概率。在对数概率的计算中，使用这种小概率，它们可以防止下溢错误，其中非常小的值被四舍五入为零。鉴于所有概率都是相乘的，单个值为0将导致整个答案总是为0！尽管如此，值之间的关系仍然是相同的；值越高，该特征就越有用。
- en: 'We can get the most useful features by sorting the array of logarithm probabilities.
    We want descending order, so we simply negate the values first. The code is as
    follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对数概率数组进行排序来获取最有用的特征。我们想要降序排列，所以我们首先对值取反。代码如下：
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code will just give us the indices and not the actual feature
    values. This isn''t very useful, so we will map the feature''s indices to the
    actual values. The key is the DictVectorizer step of the pipeline, which created
    the matrices for us. Luckily this also records the mapping, allowing us to find
    the feature names that correlate to different columns. We can extract the features
    from that part of the pipeline:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码只会给我们索引，而不是实际的特征值。这并不很有用，因此我们将特征索引映射到实际值。关键是管道中的DictVectorizer步骤，它为我们创建了矩阵。幸运的是，这也记录了映射，使我们能够找到与不同列相关联的特征名称。我们可以从管道的这部分提取特征：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'From here, we can print out the names of the top features by looking them up
    in the `feature_names_` attribute of DictVectorizer. Enter the following lines
    into a new cell and run it to print out a list of the top features:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以通过在DictVectorizer的`feature_names_`属性中查找来打印出顶级特征的名称。将以下行输入到一个新单元中并运行它，以打印出顶级特征列表：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The first few features include : RT, and even Python. These are likely to be
    noise (although the use of a colon is not very common outside programming), based
    on the data we collected. Collecting more data is critical to smoothing out these
    issues. Looking through the list though, we get a number of more obvious programming
    features:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的一些特征包括：RT，甚至还有Python。根据我们收集的数据，这些很可能是噪声（尽管在编程之外使用冒号并不常见），收集更多数据对于平滑这些问题至关重要。然而，查看列表时，我们发现了一些更明显的编程特征：
- en: '[PRE35]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There are some others too that refer to Python in a work context, and therefore
    might be referring to the programming language (although freelance snake handlers
    may also use similar terms, they are less common on Twitter).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的工作环境中提及Python的，因此可能是指编程语言（尽管自由职业的蛇类处理者也可能使用类似的术语，但在Twitter上使用较少）。
- en: 'That last one is usually in the format: <q>We''re looking for a candidate for
    this job</q>.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个通常是这样的格式：<q>我们正在寻找这个职位的候选人</q>。
- en: Looking through these features gives us quite a few benefits. We could train
    people to recognize these tweets, look for commonalities (which give insight into
    a topic), or even get rid of features that make no sense. For example, the word
    RT appears quite high in this list; however, this is a common Twitter phrase for
    retweet (that is, forwarding on someone else's tweet). An expert could decide
    to remove this word from the list, making the classifier less prone to the noise
    we introduced by having a small dataset.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些功能给我们带来了很多好处。我们可以训练人们识别这些推文，寻找共性（这有助于了解一个主题），甚至去除那些没有意义的特征。例如，单词RT在这个列表中出现的频率相当高；然而，这是Twitter上用于转发（即转发他人的推文）的常见短语。专家可能会决定从列表中删除这个单词，使分类器对由小数据集引入的噪声更加不敏感。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at text mining—how to extract features from text,
    how to use those features, and ways of extending those features. In doing this,
    we looked at putting a tweet in context—was this tweet mentioning python referring
    to the programming language? We downloaded data from a web-based API, getting
    tweets from the popular microblogging website Twitter. This gave us a dataset
    that we labeled using a form we built directly in the Jupyter Notebook.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了文本挖掘——如何从文本中提取特征，如何使用这些特征，以及扩展这些特征的方法。在这个过程中，我们考虑了将推文置于上下文中——这条推文是否提到了python，指的是编程语言？我们从基于Web的API下载了数据，获取了来自流行的微博网站Twitter的推文。这为我们提供了一个数据集，我们使用在Jupyter
    Notebook中直接构建的表格对其进行标记。
- en: We also looked at reproducibility of experiments. While Twitter doesn't allow
    you to send copies of your data to others, it allows you to send the tweet's IDs.
    Using this, we created code that saved the IDs and recreated most of the original
    dataset. Not all tweets were returned; some had been deleted in the time since
    the ID list was created and the dataset was reproduced.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了实验的可重复性。虽然Twitter不允许你将数据副本发送给其他人，但它允许你发送推文的ID。利用这一点，我们创建了保存ID并重新创建大部分原始数据集的代码。并非所有推文都被返回；有些在创建ID列表和复制数据集的时间间隔内已被删除。
- en: We used a Naive Bayes classifier to perform our text classification. This is
    built upon the Bayes' theorem that uses data to update the model, unlike the frequentist
    method that often starts with the model first. This allows the model to incorporate
    and update new data, and incorporate a prior belief. In addition, the naive part
    allows to easily compute the frequencies without dealing with complex correlations
    between features.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用朴素贝叶斯分类器进行文本分类。这是基于贝叶斯定理的，它使用数据来更新模型，与通常先从模型开始的频率主义方法不同。这允许模型结合和更新新数据，并纳入先验信念。此外，朴素的部分允许我们轻松计算频率，而不必处理特征之间的复杂相关性。
- en: The features we extracted were word occurrences—did this word occur in this
    tweet? This model is called bag-of-words. While this discards information about
    where a word was used, it still achieves a high accuracy on many datasets. This
    entire pipeline of using the bag-of-words model with Naive Bayes is quite robust.
    You will find that it can achieve quite good scores on most text-based tasks.
    It is a great baseline for you, before trying more advanced models. As another
    advantage, the Naive Bayes classifier doesn't have any parameters that need to
    be set (although there are some if you wish to do some tinkering).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取的特征是单词出现频率——这个单词是否出现在这条推文中？这个模型被称为词袋模型。虽然这丢弃了关于单词使用位置的信息，但它仍然在许多数据集上实现了很高的准确率。使用词袋模型与朴素贝叶斯结合的整个流程非常稳健。你会发现它在大多数基于文本的任务上都能取得相当好的分数。在尝试更高级的模型之前，这是一个很好的基线。作为另一个优点，朴素贝叶斯分类器没有需要设置的参数（尽管如果你愿意做一些调整，也有一些参数）。
- en: To extend the work we did in this chapter, first start by collecting more data.
    You'll need to manually classify these as well, but you'll find some similarities
    between tweets that might make it easier. For example, there is a field of study
    called Locality Sensitive Hashes, that determines whether two tweets are similar.
    Two similar tweets are likely about the same topic. Another method for extending
    the research is to consider how you would build a model that incorporates the
    twitter user's history into the equation - in other words, if the user often tweets
    about python-as-a-programming-language, then they are more likely to be using
    python in a future tweet.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展本章所做的工作，首先开始收集更多的数据。你还需要手动对这些数据进行分类，但你可能会发现推文之间存在一些相似之处，这可能会使分类变得更容易。例如，有一个名为局部敏感哈希的研究领域，它可以判断两条推文是否相似。两条相似的推文很可能涉及相同的话题。另一种扩展研究的方法是考虑如何构建一个模型，将推特用户的歷史纳入方程中——换句话说，如果用户经常在推文中提及python作为编程语言，那么他们未来更有可能在推文中使用python。
- en: In the next chapter, we will look at extracting features from another type of
    data, graphs, in order to make recommendations on who to follow on social media.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨从另一种类型的数据，即图，中提取特征，以便在社交媒体上对关注谁提出建议。
