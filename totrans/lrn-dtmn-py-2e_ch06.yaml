- en: Social Media Insight using Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text-based documents contain lots of information. Examples include books, legal
    documents, social media, and e-mail. Extracting information from text-based documents
    is critically important to modern AI systems, for example in search engines, legal
    AI, and automated news services.
  prefs: []
  type: TYPE_NORMAL
- en: Extraction of useful features from text is a difficult problem. Text is not
    numerical in nature, therefore a model must be used to create features that can
    be used with data mining algorithms. The good news is that there are some simple
    models that do a great job at this, including the bag-of-words model that we will
    use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we look at extracting features from text for use in data mining
    applications. The specific problem we tackle in this chapter is term disambiguation
    on social media - determining which meaning a word has based on its context.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data from social network APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers and models for text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using JSON for saving and loading datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLTK library for feature creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F-measure for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disambiguation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data is often called an *unstructured format*. There is a lot of information
    in text, but it is just *there*; no headings, no required format (save for normal
    grammatical rules), loose syntax, and other problems prohibit the easy extraction
    of information from text. The data is also highly connected, with lots of mentions
    and cross-references—just not in a format that allows us to easily extract it!
    Even seemingly easy problems, such as determining if a word is a noun, have lots
    of weird edge cases that make it difficult to do reliably.
  prefs: []
  type: TYPE_NORMAL
- en: We can compare the information stored in a book with that stored in a large
    database to see the difference. In the book, there are characters, themes, places,
    and lots of information. However, a book needs to be read and interpreted, with
    cultural context, to gain this information. In contrast, a database sits on your
    server with column names and data types. All the information is there and the
    level of interpretation needed to extract specific information is quite low.
  prefs: []
  type: TYPE_NORMAL
- en: Information about the data, such as its type or its meaning, is called metadata.
    Text lacks metadata. A book also contains some metadata in the form of a table
    of contents and index but the degree of information included in these sections
    is significantly lower than that of a database.
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems in working with text is **term disambiguation**. When a
    person uses the word *bank*, is this a financial message or an environmental message
    (such as river bank)? This type of disambiguation is quite easy in many circumstances
    for humans (although there are still troubles), but much harder for computers
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at disambiguating the use of the term Python
    on Twitter''s stream. When people talk about Python, they could be talking about
    the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: The programming language Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monty Python, the classic comedy group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The snake Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A make of shoe called Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be many other things called Python. The aim of our experiment is to
    take a tweet mentioning Python and determine whether it is talking about the programming
    language, based only on the content of the tweet.
  prefs: []
  type: TYPE_NORMAL
- en: A message on Twitter is called a *tweet* and is limited to 140 characters. Tweets
    include lots of metadata, such as the time and date of posting, who posted it,
    and so on. However in regards to the topic of the tweet, there is not much in
    this regard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to perform a data mining experiment consisting
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download a set of tweets from Twitter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manually classify them to create a dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the dataset so that we can replicate our research.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Naive Bayes classifier to create a classifier to perform term disambiguation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading data from a social network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are first going to download a corpus of data from Twitter and use it to sort
    out spam from useful content. Twitter provides a robust API for collecting information
    from its servers and this API is free for small-scale usage. It is, however, subject
    to some conditions that you'll need to be aware of if you start using Twitter's
    data in a commercial setting.
  prefs: []
  type: TYPE_NORMAL
- en: First, you'll need to sign up for a Twitter account (which is free). Go to [http://twitter.com](http://twitter.com) 
    and register an account if you do not already have one.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you'll need to ensure that you only make a certain number of requests
    per minute. This limit is currently 15 requests per 15 minutes (it depends on
    the exact API). It can be tricky ensuring that you don't breach this limit, so
    it is highly recommended that you use a library to talk to Twitter's API.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using your own code (that is making the web calls with your own code)
    to connect with a web-based API, ensure that you read the documentation about
    rate limiting their documentation and understand the limitations. In Python, you
    can use the `time` library to perform a pause between calls to ensure you do not
    breach the limit.
  prefs: []
  type: TYPE_NORMAL
- en: You will then need a key to access Twitter's data. Go to [http://twitter.com](http://twitter.com)
    and sign in to your account. When you are logged in, go to [https://apps.twitter.com/](https://apps.twitter.com/)
    and click on Create New App. Create a name and description for your app, along
    with a website address.If you don't have a website to use, insert a placeholder.
    Leave the Callback URL field blank for this app—we won't need it. Agree to the
    terms of use (if you do) and click on Create your Twitter application.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the resulting website open—you'll need the access keys that are on this
    page. Next, we need a library to talk to Twitter. There are many options; the
    one I like is simply called `twitter`, and is the official Twitter Python library.
  prefs: []
  type: TYPE_NORMAL
- en: You can install `twitter` using pip3 install twitter (on the command line) if
    you are using pip to install your packages. At the time of writing, Anaconda does
    not include twitter, therefore you can't use `conda` to install it. If you are
    using another system or want to build from source, check the documentation at
    [https://github.com/sixohsix/twitter](https://github.com/sixohsix/twitter)
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Jupyter Notebook to download the data. We will create several notebooks
    in this chapter for various different purposes, so it might be a good idea to
    also create a folder to keep track of them. This first notebook, `ch6_get_twitter`,
    is specifically for downloading new Twitter data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the twitter library and set our authorization tokens. The
    consumer key and consumer secret will be available on the Keys and Access Tokens
    tab on your Twitter app''s page. To get the access tokens, you''ll need to click
    on the Create my access token button, which is on the same page. Enter the keys
    into the appropriate places in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to get our tweets from Twitter''s search function. We will create
    a reader that connects to twitter using our authorization, and then use that reader
    to perform searches. In the Notebook, we set the filename where the tweets will
    be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create an object that can read from Twitter. We create this object with
    our authorization object that we set up earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then open our output file for writing. We open it for appending—this allows
    us to rerun the script to obtain more tweets. We then use our Twitter connection
    to perform a search for the word Python. We only want the statuses that are returned
    for our dataset. This code takes the tweet, uses the json library to create a
    string representation using the dumps function, and then writes it to the file.
    It then creates a blank line under the tweet so that we can easily distinguish
    where one tweet starts and ends in our file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding loop, we also perform a check to see whether there is text
    in the tweet or not. Not all of the objects returned by twitter will be actual
    tweets (for example, some responses will be actions to delete tweets). The key
    difference is the inclusion of text as a key, which we test for. Running this
    for a few minutes will result in 100 tweets being added to the output file.
  prefs: []
  type: TYPE_NORMAL
- en: You can keep re-running this script to add more tweets to your dataset, keeping
    in mind that you may get some duplicates in the output file if you rerun it too
    fast (that is before Twitter gets new tweets to return!). For our initial experiment,
    100 tweets will be enough, but you will probably want to come back and rerun this
    code to get that up to about 1000.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and classifying the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we have collected a set of tweets (our dataset), we need labels to perform
    classification. We are going to label the dataset by setting up a form in a Jupyter
    Notebook to allow us to enter the labels. We do this by loading the tweets we
    collected in the previous section, iterating over them and providing (manually)
    a classification on whether they refer to Python the programming language or not.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we have stored is nearly, but not quite, in a **JSON** format. JSON
    is a format for data that doesn't impose much structure on the contents, just
    on the syntax. The idea behind JSON is that the data is in a format directly readable
    in JavaScript (hence the name, *JavaScript Object Notation*). JSON defines basic
    objects such as numbers, strings, lists, and dictionaries, making it a good format
    for storing datasets, if they contain data that isn't numerical. If your dataset
    is fully numerical, you would save space and time using a matrix-based format
    like in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: A key difference between our dataset and real JSON is that we included newlines
    between tweets. The reason for this was to allow us to easily append new tweets
    (the actual JSON format doesn't allow this easily). Our format is a JSON representation
    of a Tweet, followed by a newline, followed by the next Tweet, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parse it, we can use the json library but we will have to first split the
    file by newlines to get the actual tweet objects themselves. Set up a new Jupyter
    Notebook, I called mine ch6_label_twitter. Within it, we will first load the data
    from our input filename by iterating over the file, storing tweets as we loop.
    The code below does a basic check that there is actual text in the tweet. If it
    does, we use the json library to load the tweet and then we add it to a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now interested in manually classifying whether an item is relevant to
    us or not (in this case, relevant means refers to the programming language Python).
    We will use the Jupyter Notebook''s ability to embed HTML and talk between JavaScript
    and Python to create a viewer of tweets to allow us to easily and quickly classify
    the tweets as spam or not. The code will present a new tweet to the user (you)
    and ask for a label: *is it relevant or not?* It will then store the input and
    present the next tweet to be labeled.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a list for storing the labels. These labels will be stored
    whether or not the given tweet refers to the programming language Python, and
    it will allow our classifier to learn how to differentiate between meanings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also check if we have any labels already and load them. This helps if you
    need to close the notebook down midway through labeling. This code will load the
    labels from where you left off. It is generally a good idea to consider how to
    save at midpoints for tasks like this. Nothing hurts quite like losing an hour
    of work because your computer crashed before you saved the labels! The code to
    do this loading follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first time you run this, nothing will happen. After manually classifying
    some examples you can save your progress and then close the Notebook. After that,
    you can reopen the Notebook and return to where you were up to.
  prefs: []
  type: TYPE_NORMAL
- en: If you make one or two mistakes classifying, don't worry too much. If you make
    lots of mistakes and want to start again, delete just python_classes.json and
    the above code will pick up with an empty set of classifications. If you need
    to delete all of your data and start again with new tweets, make sure to delete
    (or move) both files - python_tweets.json and python_classes.json. Otherwise,
    this Notebook will get confused, giving classes from the old dataset to the new
    tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a simple function that will return the next tweet that needs
    to be labeled. We can work out which is the next tweet by finding the first one
    that hasn''t yet been labeled. The code is pretty straight-forward. We determine
    how many tweets we have labeled (with `len(labels)`), and get the next tweet in
    the tweet_sample list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The next step in our experiment is to collect information from the user (you!)
    on which tweets are referring to Python (the programming language) and which are
    not.
  prefs: []
  type: TYPE_NORMAL
- en: As of yet, there is not a good, straightforward way to get interactive feedback
    with pure Python in Jupyter Notebooks for such a large number of text documents.
    For this reason, we will use some JavaScript and HTML to get this input from the
    user. There are many ways to do this, below is just one example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the feedback, we need a JavaScript component to load the next tweet
    and show it. We also need a HTML component to create the HTML elements to display
    that tweet. I won''t go into the details of the code here, except to give this
    general workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the next tweet that needs to be classified with `load_next_tweet`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show it to the user with `handle_output`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for the user to press either 0 or 1 with `$("input#capture").keypress`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store that result in the classes list with `set_label`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This keeps happening until we reach the end of the list (at which point an
    IndexError occurs, indicating we have no more tweets to classify). The code is
    below (remember that you can get the code from Packt or from the official GitHub
    repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to enter all of this code into a single cell (or copy it from
    the code bundle). It contains the mix of HTML and JavaScript necessary to get
    input from you to manually classify the tweets. If you need to stop or save your
    progress, run the following code in the next cell. It will save your progress
    (and doesn''t interrupt the above HTML code either, which can be left running):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Creating a replicable dataset from Twitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data mining, there are lots of variables. These aren't the parameters of
    the data mining algorithms - they are the methods of data collection, how the
    environment is set up, and many other factors. Being able to replicate your results
    is important as it enables you to verify or improve upon your results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting 80 percent accuracy on one dataset with algorithm X, and 90 percent
    accuracy on another dataset with algorithm Y doesn't mean that Y is better. We
    need to be able to test on the same dataset in the same conditions to be able
    to properly compare. With running the preceding code, you will get a different
    dataset to the one I created and used. The main reasons are that Twitter will
    return different search results for you than me based on the time you performed
    the search.
  prefs: []
  type: TYPE_NORMAL
- en: Even after that, your labeling of tweets might be different from what I do.
    While there are obvious examples where a given tweet relates to the python programming
    language, there will always be gray areas where the labeling isn't obvious. One
    tough gray area I ran into was tweets in non-English languages that I couldn't
    read. In this specific instance, there are options in Twitter's API for setting
    the language, but even these aren't going to be perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Due to these factors, it is difficult to replicate experiments on databases
    that are extracted from social media, and Twitter is no exception. Twitter explicitly
    disallows sharing datasets directly. One solution to this is to share tweet IDs
    only, which you can share freely. In this section, we will first create a tweet
    ID dataset that we can freely share. Then, we will see how to download the original
    tweets from this file to recreate the original dataset. First, we save the replicable
    dataset of tweet IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating another new Jupyter Notebook, first set up the filenames as
    before. This is done in the same way we did labeling but there is a new filename
    where we can store the replicable dataset. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the tweets and labels as we did in the previous notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create a dataset by looping over both the tweets and labels at the same
    time and saving those in a list. An important side-effect of this code is, by
    putting labels first in the zip function, it will only load enough tweets for
    the labels we have created. In other words, you can run this code on partially
    classified data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save the results in our file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the Tweet IDs and labels saved, we can recreate the original
    dataset. If you are looking to recreate the dataset I used for this chapter, it
    can be found in the code bundle that comes with this book. Loading the preceding
    dataset is not difficult but it can take some time.
  prefs: []
  type: TYPE_NORMAL
- en: Start a new Jupyter Notebook and set the dataset, label, and tweet ID filenames
    as before. I've adjusted the filenames here to ensure that you don't overwrite
    your previously collected dataset, but feel free to change these if you do want
    to override.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the tweet IDs from the file using JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Saving the labels is very easy. We just iterate through this dataset and extract
    the IDs. We could do this quite easily with just two lines of code (open file
    and save tweets). However, we can't guarantee that we will get all the tweets
    we are after (for example, some may have been changed to private since collecting
    the dataset) and therefore the labels will be incorrectly indexed against the
    data. As an example, I tried to recreate the dataset just one day after collecting
    them and already two of the tweets were missing (they might be deleted or made
    private by the user). For this reason, it is important to only print out the labels
    that we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we first create an empty actual labels list to store the labels
    for tweets that we actually recover from twitter, and then create a dictionary
    mapping the tweet IDs to the labels. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to create a twitter server to collect all of these tweets.
    This is going to take a little longer. Import the twitter library that we used
    before, creating an authorization token and using that to create the twitter object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will loop through each of the tweet ids, and ask twitter to recover
    the original tweet. A good feature of twitter's API is that we can ask for 100
    tweets at a time, drastically reducing the number of API calls. Interestingly,
    from twitter's point of view, it is the same number of calls to get one tweet
    or 100 tweets, as long as its a single request.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will loop through our tweets in groups of 100, join together
    the id values, and get the tweet information for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we then check each tweet to see if it is a valid tweet and then
    save it to our file if it is. Our final step is to save our resulting labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Text transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our dataset, how are we going to perform data mining on it?
  prefs: []
  type: TYPE_NORMAL
- en: Text-based datasets include books, essays, websites, manuscripts, programming
    code, and other forms of written expression. All of the algorithms we have seen
    so far deal with numerical or categorical features, so how do we convert our text
    into a format that the algorithm can deal with? There are a number of measurements
    that could be taken.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, average word and average sentence length are used to predict the
    readability of a document. However, there are lots of feature types such as word
    occurrence which we will now investigate.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest but highly effective models is to simply count each word
    in the dataset. We create a matrix, where each row represents a document in our
    dataset and each column represents a word. The value of the cell is the frequency
    of that word in the document. This is known as the **bag-of-words model**.
  prefs: []
  type: TYPE_NORMAL
- en: Here's an excerpt from <q>The Lord of the Rings, J.R.R. Tolkien: </q>
  prefs: []
  type: TYPE_NORMAL
- en: <q>Three Rings for the Elven-kings under the sky,</q><q>Seven for the Dwarf-lords
    in halls of stone, Nine for Mortal Men, doomed to die,</q><q>One for the Dark
    Lord on his dark throne In the Land of Mordor where the Shadows lie.</q><q>One
    Ring to rule them all, One Ring to find them,</q><q>One Ring to bring them all
    and in the darkness bind them.</q>*In the Land of Mordor where the Shadows lie.*<q> 
                                                                    - J.R.R. Tolkien's
    epigraph to The Lord of The Rings</q>
  prefs: []
  type: TYPE_NORMAL
- en: The word the appears nine times in this quote, while the words in, for, to,
    and one each appear four times. The word ring appears three times, as does the
    word of.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a dataset from this, choosing a subset of words and counting
    the frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Word | the | one | ring | to |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency | 9 | 4 | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: 'To do this for all words in a single document, we can use the **Counter** class.
    When counting words, it is normal to convert all letters to lowercase, which we
    do when creating the string. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Printing `c.most_common(5)` gives the list of the top five most frequently occurring
    words. Ties are not handled well as only five are given and a very large number
    of words all share a tie for fifth place.
  prefs: []
  type: TYPE_NORMAL
- en: The bag-of-words model has three major types, with many variations and alterations.
  prefs: []
  type: TYPE_NORMAL
- en: The first is to use the raw frequencies, as shown in the preceding example.
    This has the same drawback as any non-normalised data - words with high variance
    due to high overall values (such as) *the* overshadow lower frequency (and therefore
    lower-variance) words, even though the presence of the word *the* rarely has much
    importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second model is to use the normalized frequency, where each document's sum
    equals 1\. This is a much better solution as the length of the document doesn't
    matter as much, but it still means words like the overshadow lower frequency words.
    The third type is to simply use binary features—a value is 1 if it occurs, and
    0 otherwise. We will use binary representation in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another (arguably more popular) method for performing normalization is called
    **term frequency-inverse document frequency** (**tf-idf**). In this weighting
    scheme, term counts are first normalized to frequencies and then divided by the
    number of documents in which it appears in the corpus. We will use tf-idf in [Chapter
    10](lrn-dtmn-py-2e_ch10.html)*, Clustering News Articles*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n-gram features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One variation on the standard bag-of-words model is called the n-gram model.
    An n-grams model addresses the deficiency of context in the bag-of-words model.
    With a bag-of-words model, only individual words are counted by themselves. This
    means that common word pairs, such as *United States*, lose meaning they have
    in the sentence because they are treated as individual words.
  prefs: []
  type: TYPE_NORMAL
- en: There are algorithms that can read a sentence, parse it into a tree-like structure,
    and use this to create very accurate representations of the meaning behind words.
    Unfortunately, these algorithms are computationally expensive. This makes it difficult
    to apply them to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To compensate for these issues of context and complexity, the n-grams model
    fits into the middle ground. It has more context than the bag-of-words model,
    while only being slightly more expensive computationally.
  prefs: []
  type: TYPE_NORMAL
- en: An n-gram is a subsequence of *n* consecutive, overlapping, tokens. In this
    experiment, we use word n-grams, which are n-grams of word-tokens. They are counted
    the same way as a bag-of-words, with the n-grams forming a *word* that is put
    in the bag. The value of a cell in this dataset is the frequency that a particular
    n-gram appears in the given document.
  prefs: []
  type: TYPE_NORMAL
- en: The value of n is a parameter. For English, setting it to between 2 to 5 is
    a good start, although some applications call for higher values. Higher values
    for n result in sparse datasets, as when n increases it is less likely to have
    the same n-gram appear across multiple documents. Having n=1 results in simply
    the bag-of-words model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, `for n=3`, we extract the first few n-grams in the following
    quote:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Always look on the bright side of life.*'
  prefs: []
  type: TYPE_NORMAL
- en: The first n-gram (of size 3) is *Always look on*, the second is *look on the*,
    the third is *on the bright*. As you can see, the n-grams overlap and cover three
    words each. Word n-grams have advantages over using single words. This simple
    concept introduces some context to word use by considering its local environment,
    without a large overhead of understanding the language computationally.
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of using n-grams is that the matrix becomes even sparser—word
    n-grams are unlikely to appear twice (especially in tweets and other short documents!). Specifically
    for social media and other short documents, word n-grams are unlikely to appear
    in too many different tweets, unless it is a retweet. However, in larger documents,
    word n-grams are quite effective for many applications. Another form of n-gram
    for text documents is that of a character n-gram. That said, you'll see shortly
    that word n-grams are quite effective in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using sets of words, we simply use sets of characters (although
    character n-grams have lots of options for how they are computed!). This type
    of model can help identify words that are misspelled, as well as providing other
    benefits to classification. We will test character n-grams in this chapter and
    see them again in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*, Authorship Attribution*.
  prefs: []
  type: TYPE_NORMAL
- en: Other text features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other features that can be extracted too. These include syntactic
    features, such as the usage of particular words in sentences. Part-of-speech tags
    are also popular for data mining applications that need to understand meaning
    in text. Such feature types won't be covered in this book. If you are interested
    in learning more, I recommend *Python 3 Text Processing with NLTK 3 Cookbook,
    Jacob Perkins, Packt publication*.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of libraries for working with text data in Python. The most
    commonly known one is called the Natural Language ToolKit (NLTK). The scikit-learn
    library also has the CountVectorizer class that performs a similar action, and
    it is recommended you take a look at it (we will use it in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*,
    Authorship Attribution*). NLTK has more features for word tokenization and part
    of speech tagging (that is identifying which words are nouns, verbs and so on).
  prefs: []
  type: TYPE_NORMAL
- en: The library we are going to use is called spaCy. It is designed from the ground
    up to be fast and reliable for natural language processing. Its less-well-known
    than NLTK, but is rapidly growing in popularity. It also simplifies some of the
    decisions, but has a slightly more difficult syntax to use, compared to NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: For production systems, I recommend using spaCy, which is faster than NLTK.
    NLTK was built for teaching, while spaCy was built for production. They have different
    syntaxes, meaning it can be difficult to port code from one library to another.
    If you aren't looking into experimenting with different types of natural language
    parsers, I recommend using spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is a probabilistic model that is, unsurprisingly, built upon a
    naive interpretation of Bayesian statistics. Despite the naive aspect, the method
    performs very well in a large number of contexts. Because of the naive aspect,
    it works quite quickly. It can be used for classification of many different feature
    types and formats, but we will focus on one in this chapter: binary features in
    the bag-of-words model.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For most of us, when we were taught statistics, we started from a **frequentist
    approach**. In this approach, we assume the data comes from some distribution
    and we aim to determine what the parameters are for that distribution. However,
    those parameters are (perhaps incorrectly) assumed to be fixed. We use our model
    to describe the data, even testing to ensure the data fits our model.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian statistics instead model how people (at least, non-frequentist statisticians)
    actually reason. We have some data, and we use that data to update our model about
    how likely something is to occur. In Bayesian statistics, we use the data to describe
    the model rather than using a model and confirming it with data (as per the frequentist
    approach).
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that frequentist statistics and Bayesian statistics ask and
    answer slightly different questions. A direct comparison is not always correct.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem computes the value of P(A|B). That is, knowing that B has occurred,
    what is the probability of event A occurring. In most cases, B is an observed
    event such as *it rained yesterday*, and A is a prediction it will rain today.
    For data mining, B is usually *we observed this sample* and A is *does the sample
    belong to this class* (the class prediction). We will see how to use Bayes' theorem
    for data mining in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for Bayes'' theorem is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06162_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: As an example, we want to determine the probability that an e-mail containing
    the word drugs is spam (as we believe that such a tweet may be a pharmaceutical
    spam).
  prefs: []
  type: TYPE_NORMAL
- en: '*A*, in this context, is the probability that this tweet is spam. We can compute
    *P(A)*, called the prior belief directly from a training data by computing the
    percentage of tweets in our dataset that are spam. If our dataset contains 30
    spam messages for every 100 e-mails, *P(A)* is 30/100 or 0.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*B*, in this context, is this tweet contains the word *drugs*. Likewise, we
    can compute *P(B)* by computing the percentage of tweets in our dataset containing
    the word drugs. If 10 e-mails in every 100 of our training dataset contain the
    word drugs, *P(B)* is 10/100 or 0.1\. Note that we don''t care if the e-mail is
    spam or not when computing this value.'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(B|A)* is the probability that an e-mail contains the word drugs if it is
    spam. This is also easy to compute from our training dataset. We look through
    our training set for spam e-mails and compute the percentage of them that contain
    the word drugs. Of our 30 spam e-mails, if 6 contain the word drugs, then *P(B|A)*
    is calculated as 6/30 or 0.2.'
  prefs: []
  type: TYPE_NORMAL
- en: From here, we use Bayes' theorem to compute *P(A|B)*, which is the probability
    that a tweet containing the word drugs is spam. Using the previous equation, we
    see the result is 0.6\. This indicates that if an e-mail has the word drugs in
    it, there is a 60 percent chance that it is spam.
  prefs: []
  type: TYPE_NORMAL
- en: Note the empirical nature of the preceding example—we use evidence directly
    from our training dataset, not from some preconceived distribution. In contrast,
    a frequentist view of this would rely on us creating a distribution of the probability
    of words in tweets to compute similar equations.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking back at our Bayes' theorem equation, we can use it to compute the probability
    that a given sample belongs to a given class. This allows the equation to be used
    as a classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: With *C* as a given class and *D* as a sample in our dataset, we create the
    elements necessary for Bayes' theorem, and subsequently Naive Bayes. Naive Bayes
    is a classification algorithm that utilizes Bayes' theorem to compute the probability
    that a new data sample belongs to a particular class.
  prefs: []
  type: TYPE_NORMAL
- en: '*P(D)* is the probability of a given data sample. It can be difficult to compute
    this, as the sample is a complex interaction between different features, but luckily
    it is constant across all classes. Therefore, we don''t need to compute it at
    all, as all we do in the final step is compare relative values.'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(D|C)* is the probability of the data point belonging to the class. This
    could also be difficult to compute due to the different features. However, this
    is where we introduce the naive part of the Naive Bayes algorithm. We naively
    assume that each feature is independent of each other. Rather than computing the
    full probability of *P(D|C)*, we compute the probability of each feature *D1,
    D2, D3, ...* and so on. Then, we just multiply them together:'
  prefs: []
  type: TYPE_NORMAL
- en: P*(D|C) = P(D1|C) x P(D2|C).... x P(Dn|C)*
  prefs: []
  type: TYPE_NORMAL
- en: Each of these values is relatively easy to compute with binary features; we
    simply compute the percentage of times it is equal in our sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if we were to perform a non-naive Bayes version of this part, we
    would need to compute the correlations between different features for each class.
    Such computation is infeasible at best, and nearly impossible without vast amounts
    of data or adequate language analysis models.
  prefs: []
  type: TYPE_NORMAL
- en: From here, the algorithm is straightforward. We compute P(C|D) for each possible
    class, ignoring the P(D) term entirely. Then we choose the class with the highest
    probability. As the P(D) term is consistent across each of the classes, ignoring
    it has no impact on the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an example, suppose we have the following (binary) feature values from a
    sample in our dataset: [0, 0, 0, 1].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our training dataset contains two classes with 75 percent of samples belonging
    to the class 0, and 25 percent belonging to the class 1\. The likelihood of the
    feature values for each class are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For class 0: [0.3, 0.4, 0.4, 0.7]'
  prefs: []
  type: TYPE_NORMAL
- en: 'For class 1: [0.7, 0.3, 0.4, 0.9]'
  prefs: []
  type: TYPE_NORMAL
- en: 'These values are to be interpreted as: for feature 1, it has a value of 1 in
    30 percent of cases for samples with class 0\. It is a value of 1 in 70 percent
    of samples with class 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now compute the probability that this sample should belong to the class
    0\. P(C=0) = 0.75 which is the probability that the class is 0\. Again, P(D) isn''t
    needed for the Naive Bayes algorithm and is simply removed from the equation.
    Let''s take a look at the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The second and third values are 0.6, because the value of that feature in the
    sample was 0\. The listed probabilities are for values of 1 for each feature.
    Therefore, the probability of a 0 is its inverse: *P(0) = 1 – P(1)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compute the probability of the data point belonging to this class.
    Let''s take a look at the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the same values for the class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Normally, P(C=0|D) + P(C=1|D) should equal to 1\. After all, those are the only
    two possible options! However, the probabilities are not 1 due to the fact we
    haven't included the computation of P(D) in our equations here.
  prefs: []
  type: TYPE_NORMAL
- en: As the value for *P(C=1|D)* is more than *P(C=0|D)*, the data point should be
    classified as belonging to the class 1\. You may have guessed this while going
    through the equations anyway; however, you may have been a bit surprised that
    the final decision was so close. After all, the probabilities in computing *P(D|C)*
    were much, much higher for the class 1\. This is because we introduced a prior
    belief that most samples generally belong to the class 0.
  prefs: []
  type: TYPE_NORMAL
- en: If the classes had been equal sizes, the resulting probabilities would be much
    different. Try it yourself by changing both *P(C=0)* and *P(C=1)* to 0.5 for equal
    class sizes and computing the result again.
  prefs: []
  type: TYPE_NORMAL
- en: Applying of Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now create a pipeline that takes a tweet and determines whether it is
    relevant or not, based only on the content of that tweet.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the word extraction, we will be using the spaCy, a library that contains
    a large number of tools for performing analysis on natural language. We will use
    spaCy in future chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get spaCy on your computer, use pip to install the package: pip install
    spacy'
  prefs: []
  type: TYPE_NORMAL
- en: If that doesn't work, see the spaCy installation instructions at [https://spacy.io/](https://spacy.io/) for
    information specific to your platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to create a pipeline to extract the word features and classify
    the tweets using Naive Bayes. Our pipeline has the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Transform the original text documents into a dictionary of counts using spaCy's
    word tokenization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform those dictionaries into a vector matrix using the `DictVectorizer`
    transformer in scikit-learn. This is necessary to enable the Naive Bayes classifier
    to read the feature values extracted in the first step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the Naive Bayes classifier, as we have seen in previous chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will need to create another Notebook (last one for the chapter!) called `ch6_classify_twitter`
    for performing the classification.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting word counts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to use spaCy to extract our word counts. We still want to use it
    in a pipeline, but spaCy doesn't conform to our transformer interface. We will need
    to create a basic transformer to do this to obtain both fit and transform methods,
    enabling us to use this in a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: First, set up the transformer class. We don't need to *fit* anything in this
    class, as this transformer simply extracts the words in the document. Therefore,
    our fit is an empty function, except that it returns self which is necessary for
    transformer objects to conform to the scikit-learn API.
  prefs: []
  type: TYPE_NORMAL
- en: Our transform is a little more complicated. We want to extract each word from
    each document and record True if it was discovered. We are only using the binary
    features here—True if in the document, False otherwise. If we wanted to use the
    frequency we would set up counting dictionaries, as we have done in several of
    the past chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The result is a list of dictionaries, where the first dictionary is the list
    of words in the first tweet, and so on. Each dictionary has a word as key and
    the value True to indicate this word was discovered. Any word not in the dictionary
    will be assumed to have not occurred in the tweet. Explicitly stating that a word's
    occurrence is False will also work, but will take up needless space to store.
  prefs: []
  type: TYPE_NORMAL
- en: Converting dictionaries to a matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step converts the dictionaries built as per the previous step into
    a matrix that can be used with a classifier. This step is made quite simple through
    the DictVectorizer transformer that is provided as part of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The `DictVectorizer` class simply takes a list of dictionaries and converts
    them into a matrix. The features in this matrix are the keys in each of the dictionaries,
    and the values correspond to the occurrence of those features in each sample.
    Dictionaries are easy to create in code, but many data algorithm implementations
    prefer matrices. This makes `DictVectorizer` a very useful class.
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, each dictionary has words as keys and only occurs if the word
    actually occurs in the tweet. Therefore, our matrix will have each word as a feature
    and a value of True in the cell if the word occurred in the tweet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `DictVectorizer`, simply import it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we need to set up a classifier and we are using Naive Bayes for this
    chapter. As our dataset contains only binary features, we use the `BernoulliNB`
    classifier that is designed for binary features. As a classifier, it is very easy
    to use. As with `DictVectorizer`, we simply import it and add it to our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the moment to put all of these pieces together. In our Jupyter Notebook,
    set the filenames and load the dataset and classes as we have done before. Set
    the filenames for both the tweets themselves (not the IDs!) and the labels that
    we assigned to them. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the tweets themselves. We are only interested in the content of the tweets,
    so we extract the text value and store only that. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a pipeline putting together the components from before. Our pipeline
    has three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The NLTKBOW transformer we created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A DictVectorizer transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A BernoulliNB classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can nearly run our pipeline now, which we will do with `cross_val_score` 
    as we have done many times before. Before we perform the data mining, we will
    introduce a better evaluation metric than the accuracy metric we used before.
    As we will see, the use of accuracy is not adequate for datasets when the number
    of samples in each class is different.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation using the F1-score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When choosing an evaluation metric, it is always important to consider cases
    where that evaluation metric is not useful. Accuracy is a good evaluation metric
    in many cases, as it is easy to understand and simple to compute. However, it
    can be easily faked. In other words, in many cases, you can create algorithms
    that have a high accuracy but have a poor utility.
  prefs: []
  type: TYPE_NORMAL
- en: While our dataset of tweets (typically, your results may vary) contains about
    50 percent programming-related and 50 percent nonprogramming, many datasets aren't
    as *balanced* as this.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, an e-mail spam filter may expect to see more than 80 percent
    of incoming e-mails be spam. A spam filter that simply labels everything as spam
    is quite useless; however, it will obtain an accuracy of 80 percent!
  prefs: []
  type: TYPE_NORMAL
- en: To get around this problem, we can use other evaluation metrics. One of the
    most commonly employed is called an **f1-score** (also called f-score, f-measure,
    or one of many other variations on this term).
  prefs: []
  type: TYPE_NORMAL
- en: 'The F1-score is defined on a per-class basis and is based on two concepts:
    the precision and recall. The precision is the percentage of all the samples that
    were predicted as belonging to a specific class, that were actually from that
    class. The recall is the percentage of samples in the dataset that are in a class and
    actually labeled as belonging to that class.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of our application, we could compute the value for both classes
    (python-programming and not python-programming).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our precision computation becomes the question: <q>of all the tweets that were
    predicted as being relevant, what percentage were actually relevant?</q>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, the recall becomes the question: <q>of all the relevant tweets in
    the data set, how many were predicted as being relevant?</q>'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you compute both the precision and recall, the f1-score is the harmonic
    mean of the precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the f1-score in scikit-learn methods, simply set the scoring parameter
    to f1\. By default, this will return the f1-score of the class with label 1\.
    Running the code on our dataset, we simply use the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The result is 0.684, which means we can accurately determine if a tweet using
    Python relates to the programing language nearly 70 percent of the time. This
    is using a dataset with only 300 tweets in it.
  prefs: []
  type: TYPE_NORMAL
- en: Go back and collect more data and you will find that the results increase! Keep
    in mind that your dataset may differ, and therefore your results would too.
  prefs: []
  type: TYPE_NORMAL
- en: More data usually means a better accuracy, but it is not guaranteed!
  prefs: []
  type: TYPE_NORMAL
- en: Getting useful features from models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One question you may ask is, what are the best features for determining if a
    tweet is relevant or not? We can extract this information from our Naive Bayes
    model and find out which features are the best individually, according to Naive
    Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we fit a new model. While the `cross_val_score` gives us a score across
    different folds of cross-validated testing data, it doesn''t easily give us the
    trained models themselves. To do this, we simply fit our pipeline with the tweets,
    creating a new model. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that we aren't really evaluating the model here, so we don't need to be
    as careful with the training/testing split. However, before you put these features
    into practice, you should evaluate on a separate test split. We skip over that
    here for the sake of clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline gives you access to the individual steps through the `named_steps`
    attribute and the name of the step (we defined these names ourselves when we created
    the pipeline object itself). For instance, we can get the Naive Bayes model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: From this model, we can extract the probabilities for each word. These are stored
    as log probabilities, which is simply *log(P(A|f))*, where *f* is a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: The reason these are stored as log probabilities is because the actual values
    are very low. For instance, the first value is -3.486, which correlates to a probability
    under 0.03 percent. Logarithm probabilities are used in computation involving
    small probabilities like this as they stop underflow errors where very small values
    are just rounded to zeros. Given that all of the probabilities are multiplied
    together, a single value of 0 will result in the whole answer always being 0!
    Regardless, the relationship between values is still the same; the higher the
    value, the more useful that feature is.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the most useful features by sorting the array of logarithm probabilities.
    We want descending order, so we simply negate the values first. The code is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will just give us the indices and not the actual feature
    values. This isn''t very useful, so we will map the feature''s indices to the
    actual values. The key is the DictVectorizer step of the pipeline, which created
    the matrices for us. Luckily this also records the mapping, allowing us to find
    the feature names that correlate to different columns. We can extract the features
    from that part of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can print out the names of the top features by looking them up
    in the `feature_names_` attribute of DictVectorizer. Enter the following lines
    into a new cell and run it to print out a list of the top features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few features include : RT, and even Python. These are likely to be
    noise (although the use of a colon is not very common outside programming), based
    on the data we collected. Collecting more data is critical to smoothing out these
    issues. Looking through the list though, we get a number of more obvious programming
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There are some others too that refer to Python in a work context, and therefore
    might be referring to the programming language (although freelance snake handlers
    may also use similar terms, they are less common on Twitter).
  prefs: []
  type: TYPE_NORMAL
- en: 'That last one is usually in the format: <q>We''re looking for a candidate for
    this job</q>.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking through these features gives us quite a few benefits. We could train
    people to recognize these tweets, look for commonalities (which give insight into
    a topic), or even get rid of features that make no sense. For example, the word
    RT appears quite high in this list; however, this is a common Twitter phrase for
    retweet (that is, forwarding on someone else's tweet). An expert could decide
    to remove this word from the list, making the classifier less prone to the noise
    we introduced by having a small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at text mining—how to extract features from text,
    how to use those features, and ways of extending those features. In doing this,
    we looked at putting a tweet in context—was this tweet mentioning python referring
    to the programming language? We downloaded data from a web-based API, getting
    tweets from the popular microblogging website Twitter. This gave us a dataset
    that we labeled using a form we built directly in the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at reproducibility of experiments. While Twitter doesn't allow
    you to send copies of your data to others, it allows you to send the tweet's IDs.
    Using this, we created code that saved the IDs and recreated most of the original
    dataset. Not all tweets were returned; some had been deleted in the time since
    the ID list was created and the dataset was reproduced.
  prefs: []
  type: TYPE_NORMAL
- en: We used a Naive Bayes classifier to perform our text classification. This is
    built upon the Bayes' theorem that uses data to update the model, unlike the frequentist
    method that often starts with the model first. This allows the model to incorporate
    and update new data, and incorporate a prior belief. In addition, the naive part
    allows to easily compute the frequencies without dealing with complex correlations
    between features.
  prefs: []
  type: TYPE_NORMAL
- en: The features we extracted were word occurrences—did this word occur in this
    tweet? This model is called bag-of-words. While this discards information about
    where a word was used, it still achieves a high accuracy on many datasets. This
    entire pipeline of using the bag-of-words model with Naive Bayes is quite robust.
    You will find that it can achieve quite good scores on most text-based tasks.
    It is a great baseline for you, before trying more advanced models. As another
    advantage, the Naive Bayes classifier doesn't have any parameters that need to
    be set (although there are some if you wish to do some tinkering).
  prefs: []
  type: TYPE_NORMAL
- en: To extend the work we did in this chapter, first start by collecting more data.
    You'll need to manually classify these as well, but you'll find some similarities
    between tweets that might make it easier. For example, there is a field of study
    called Locality Sensitive Hashes, that determines whether two tweets are similar.
    Two similar tweets are likely about the same topic. Another method for extending
    the research is to consider how you would build a model that incorporates the
    twitter user's history into the equation - in other words, if the user often tweets
    about python-as-a-programming-language, then they are more likely to be using
    python in a future tweet.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at extracting features from another type of
    data, graphs, in order to make recommendations on who to follow on social media.
  prefs: []
  type: TYPE_NORMAL
