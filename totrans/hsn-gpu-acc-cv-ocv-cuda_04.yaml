- en: Advanced Concepts in CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA的高级概念
- en: In the last chapter, we looked at memory architecture in CUDA and saw how it
    can be used efficiently to accelerate applications. Up until now, we have not
    seen a method to measure the performance of CUDA programs. In this chapter, we
    will discuss how we can do that using CUDA events. The Nvidia Visual Profiler
    will also be discussed, as well as how to resolve errors in CUDA programs from
    within the CUDA code and using debugging tools. How we can improve the performance
    of CUDA programs will also be discussed. This chapter will describe how CUDA streams
    can be used for multitasking and how we can use them to accelerate applications.
    You will also learn how array-sorting algorithms can be accelerated using CUDA.
    Image processing is an application where we need to process a large amount of
    data in a very small amount of time, so CUDA can be an ideal choice for this kind
    of application to manipulate pixel values in an image. This chapter describes
    the acceleration of a simple and widely used image-processing function a histogram
    calculation, using CUDA.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了CUDA的内存架构，并看到了如何有效地使用它来加速应用程序。到目前为止，我们还没有看到一种测量CUDA程序性能的方法。在本章中，我们将讨论如何使用CUDA事件来做到这一点。还将讨论Nvidia
    Visual Profiler，以及如何在CUDA代码内部和调试工具中使用它来解决CUDA程序中的错误。我们还将讨论如何提高CUDA程序的性能。本章将描述如何使用CUDA流进行多任务处理，以及如何使用它们来加速应用程序。你还将学习如何使用CUDA加速数组排序算法。图像处理是一个需要在大约很短的时间内处理大量数据的领域，因此CUDA可以成为这类应用中操纵图像像素值的理想选择。本章描述了使用CUDA加速一个简单且广泛使用的图像处理函数——直方图计算。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Performance measurement in CUDA
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA中的性能测量
- en: Error handling in CUDA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA中的错误处理
- en: Performance improvement of CUDA programs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA程序的性能改进
- en: CUDA streams and how they can be used to accelerate applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA流及其如何用于加速应用程序
- en: Acceleration of sorting algorithms using CUDA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA加速排序算法
- en: Introduction to image processing applications with CUDA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CUDA介绍图像处理应用
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires familiarity with the basic C or C++ programming language
    and all the code examples explained in the previous chapters. All the code used
    in this chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, although it has only been tested
    on Windows 10 and Ubuntu. Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求熟悉基本的C或C++编程语言以及前几章中解释的所有代码示例。本章中使用的所有代码都可以从以下GitHub链接下载：[https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA)。代码可以在任何操作系统上执行，尽管它只在Windows
    10和Ubuntu上进行了测试。查看以下视频以查看代码的实际运行情况：
- en: '[http://bit.ly/2Nt4DEy](http://bit.ly/2Nt4DEy)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2Nt4DEy](http://bit.ly/2Nt4DEy)'
- en: Performance measurement of CUDA programs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA程序的性能测量
- en: Up until now, we have not determined the performance of the CUDA programs explicitly.
    In this section, we will see how to measure the performance of CUDA programs using
    CUDA Events and also visualize the performance using the Nvidia Visual Profiler.
    This is a very important concept in CUDA because it will allow you to choose the
    best-performing algorithms for a particular application from many options. First,
    we will measure performance using CUDA Events.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有明确确定CUDA程序的性能。在本节中，我们将看到如何使用CUDA事件来测量CUDA程序的性能，并使用Nvidia Visual Profiler来可视化性能。这在CUDA中是一个非常重要的概念，因为它将允许你从许多选项中选择特定应用程序的最佳性能算法。首先，我们将使用CUDA事件来测量性能。
- en: CUDA Events
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA事件
- en: We can use a CPU timer for measuring the performance of CUDA programs, but it
    will not give accurate results. It will include thread latency overhead and scheduling
    in the OS, among many other factors. The time measured using the CPU will also
    depend on the availability of a high precision CPU timer. Many times, the host
    is performing asynchronous computation while the GPU kernel is running, hence
    CPU timers may not give the correct time for kernel executions. So, to measure
    the GPU kernel computation time, CUDA provides an event API.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用CPU计时器来测量CUDA程序的性能，但它不会给出准确的结果。它将包括线程延迟开销和操作系统的调度，以及其他许多因素。使用CPU测量的时间也将取决于高精度CPU计时器的可用性。很多时候，当GPU内核运行时，主机正在执行异步计算，因此CPU计时器可能无法给出内核执行的正确时间。所以，为了测量GPU内核的计算时间，CUDA提供了一个事件API。
- en: 'A CUDA event is a GPU timestamp that''s recorded at a specified point from
    your CUDA program. In this API, the GPU records the timestamp, which eliminates
    the issues that were present when using CPU timers for measuring performance.
    There are two steps to measure time using CUDA Events: creating an event and recording
    an event. We will record two events, one at the start of our code and one at the
    end. Then, we will try to calculate the difference in time between these two events,
    and that will give the overall performance of our code.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA事件是在您的CUDA程序中指定点记录的GPU时间戳。在这个API中，GPU记录时间戳，消除了使用CPU计时器测量性能时存在的问题。使用CUDA事件测量时间有两个步骤：创建事件和记录事件。我们将记录两个事件，一个在代码的开始处，一个在结束处。然后，我们将尝试计算这两个事件之间时间差，这将给出代码的整体性能。
- en: 'In your CUDA code, you can include the following lines to measure performance
    using the CUDA event API:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的CUDA代码中，您可以通过包含以下行来使用CUDA事件API来测量性能：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will create two events, `e_start` and `e_stop`, for starting and ending the
    code. `cudaEvent_t` is used to define event objects. To create an event, we will
    use the `cudaEventCreate` API. We can pass in event objects as arguments to this
    API. At the beginning of the code, we will record the GPU timestamp in the `e_start`
    event; this will be done using the `cudaEventRecord` API. The second argument
    to this function is zero, which indicates the CUDA stream number, which we will
    discuss later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建两个事件，`e_start`和`e_stop`，用于开始和结束代码。`cudaEvent_t`用于定义事件对象。要创建一个事件，我们将使用`cudaEventCreate`
    API。我们可以将事件对象作为参数传递给此API。在代码的开始处，我们将记录GPU时间戳在`e_start`事件中；这将通过`cudaEventRecord`
    API来完成。此函数的第二个参数是零，它表示CUDA流号，我们将在本章后面讨论。
- en: After recording the timestamp in the beginning, you can start writing your GPU
    code. After the code ends, we will again record the time in the `e_stop` event.
    This will be done with the `cudaEventRecord(e_stop, 0)` line. Once we have recorded
    both the start and end times, the difference between them should give us the actual
    performance of the code. But there's still one issue with directly calculating
    the difference in time between these two events.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在记录开始时的时间戳后，您可以开始编写您的GPU代码。在代码结束时，我们将在`e_stop`事件中再次记录时间。这将通过`cudaEventRecord(e_stop,
    0)`行来完成。一旦我们记录了开始和结束时间，它们之间的差异应该会给我们代码的实际性能。但在这两个事件之间直接计算时间差仍然存在一个问题。
- en: As we have discussed in previous chapters, execution in CUDA C can be asynchronous.
    When the GPU is executing the kernel, the CPU might be executing the next lines
    of our code until the GPU finishes its execution. So, measuring time directly
    without synchronizing the GPU and CPU may give incorrect results. `CudaEventRecord()`
    will record a timestamp when all GPU instructions prior to its call finish. We
    should not read the `e_stop` event until this point, when prior work on the GPU
    is finished. So, to synchronize CPU operations with the GPU, we will use `cudaEventSynchronize(e_stop)`.
    It ensures that the correct timestamp is recorded in the `e_stop` event.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中讨论的那样，CUDA C中的执行可以是异步的。当GPU执行内核时，CPU可能会执行我们的代码的下一行，直到GPU完成其执行。所以，如果不同步GPU和CPU就直接测量时间可能会得到错误的结果。`CudaEventRecord()`会在其调用之前的所有GPU指令完成时记录一个时间戳。我们不应该在GPU上的先前工作完成之前读取`e_stop`事件。因此，为了同步CPU操作与GPU，我们将使用`cudaEventSynchronize(e_stop)`。这确保了在`e_stop`事件中记录了正确的时间戳。
- en: 'Now, to calculate the difference between these two timestamps, CUDA provides
    an API called `cudaEventElapsedTime`. It has three arguments. The first is the
    variable in which we want to store the difference, the second is the start event,
    and the third is the end event. After calculating this time, we will print it
    on the console in the next line. We added this performance measurement code to
    the vector addition code seen in the previous chapter, using multiple threads
    and blocks. The output after adding these lines is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算这两个时间戳之间的差异，CUDA提供了一个名为`cudaEventElapsedTime`的API。它有三个参数。第一个是我们想要存储差异的变量，第二个是开始事件，第三个是结束事件。计算完这个时间后，我们将在下一行将其打印到控制台。我们将此性能测量代码添加到上一章中看到的向量加法代码中，使用了多个线程和块。添加这些行后的输出如下：
- en: '![](img/07822ae7-a24a-4f50-b732-a4468731e6d8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07822ae7-a24a-4f50-b732-a4468731e6d8.png)'
- en: The time taken to add 50,000 elements on the GPU is around 0.9 ms. This output
    will depend on your system configurations, hence you might get a different output
    in the red box. So, you can include this performance measurement code in all the
    code examples we have seen in this book to measure their performance. You can
    also quantify performance gains using constant and texture memories by using this
    event API.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上添加50,000个元素所需的时间大约为0.9毫秒。此输出将取决于您的系统配置，因此您可能在红色框中得到不同的输出。因此，您可以将此性能测量代码包含在本书中看到的所有代码示例中，以测量它们的性能。您还可以通过使用此事件API来量化使用常量和纹理内存的性能提升。
- en: It should be kept in mind that CUDA Events can only be used to measure the timing
    of device code blocks. This only includes memory allocation, memory copies, and
    kernel execution. It should not be used to measure the timings of the host code.
    As the GPU is recording time in the event API, using it to measure the performance
    of the host code may give incorrect results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应当记住，CUDA事件只能用来测量设备代码块的执行时间。这仅包括内存分配、内存复制和内核执行。它不应用于测量主机代码的执行时间。因为GPU在事件API中记录时间，使用它来测量主机代码的性能可能会得到错误的结果。
- en: The Nvidia Visual Profiler
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nvidia视觉分析器
- en: 'We now know that CUDA provides an efficient way to improve the performance
    of parallel computing applications. However, sometimes, it may happen that even
    after incorporating CUDA in your application, the performance of the code does
    not improve. In this kind of scenario, it is very useful to visualize which part
    of the code is taking the most time to complete. This is called **profiling of
    kernel execution code**. Nvidia provides a tool for this and it comes with the
    standard CUDA installation. This tool is called the **Nvidia Visual Profiler**.
    In the standard CUDA 9.0 installation on Windows 10, it can be found on the following
    path: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp`. You can
    run the `nvvp` application available on this path, which will open the Nvidia
    Visual Profile tool as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道CUDA提供了一个有效的方法来提高并行计算应用程序的性能。然而，有时，即使将CUDA集成到您的应用程序中，代码的性能也可能不会提高。在这种情况下，可视化代码中哪个部分花费了最多时间完成是非常有用的。这被称为**内核执行代码分析**。Nvidia提供了一个用于此的工具，并且它包含在标准的CUDA安装中。这个工具被称为**Nvidia视觉分析器**。在Windows
    10上的标准CUDA 9.0安装中，它可以在以下路径找到：`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp`。您可以在该路径上运行`nvvp`应用程序，这将打开Nvidia视觉分析工具，如下所示：
- en: '![](img/fb1a410f-a1dd-4536-bec1-67a77db0610b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb1a410f-a1dd-4536-bec1-67a77db0610b.png)'
- en: 'This tool will execute your code and, based on the performance of your GPU,
    give you a detailed report on the execution time of each kernel, detailed timestamps
    for each operation in your code, the memory used in your code, and the memory
    bandwidth, among others. To visualize and get detailed reports for any applications
    you have developed, you can go to File -> New Session. Select the `.exe` file
    of the application. We have selected the vector addition example seen in the previous
    chapter. The result will be as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此工具将执行您的代码，并根据您的GPU性能，为您提供每个内核的执行时间、代码中每个操作的详细时间戳、代码使用的内存以及内存带宽等详细信息。要为任何您开发的应用程序可视化和获取详细报告，您可以转到文件
    -> 新会话。选择应用程序的`.exe`文件。我们选择了上一章中看到的向量加法示例。结果如下：
- en: '![](img/ac336aad-6c71-4343-a7ef-fb8ae154a36f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac336aad-6c71-4343-a7ef-fb8ae154a36f.png)'
- en: 'The result displays the timing of all operations in the program. It can be
    seen that the `cudaMalloc` operation takes the most time to complete. It also
    displays the order in which each operation is performed in your code. It shows
    that the kernel is called only once and it needs an average of 192.041 microseconds
    to execute. The details of memory copy operations can also be visualized. The
    properties of memory copy operations from the host to the device are shown as
    follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了程序中所有操作的计时。可以看到，`cudaMalloc`操作完成所需时间最长。它还显示了你的代码中每个操作执行的顺序。它显示内核只被调用了一次，平均需要192.041微秒来执行。内存复制操作的详细信息也可以可视化。从主机到设备的内存复制操作属性如下所示：
- en: '![](img/dfaaa8a7-df80-4009-9164-ac73eebdee5a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfaaa8a7-df80-4009-9164-ac73eebdee5a.png)'
- en: It can be seen that, as we are copying two arrays from the host to the device,
    the memory copy operation is invoked twice. The total number of bytes copied are
    400 KB with a throughput of 1.693 GB/s. This tool is very important in the analysis
    of kernel execution. It can also be used to compare the performance of two kernels.
    It will show you the exact operation that is slowing down the performance of your
    code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，当我们从主机复制两个数组到设备时，内存复制操作被调用了两次。总共复制的字节数为400 KB，吞吐量为1.693 GB/s。这个工具在内核执行分析中非常重要。它也可以用来比较两个内核的性能。它将显示导致你的代码性能下降的确切操作。
- en: To summarize, in this section, we have seen two methods to measure and analyze
    CUDA code. CUDA Events is an efficient API for measuring the timing of device
    code. The Nvidia Visual Profiler gives a detailed analysis and profiling of CUDA
    code, which can be used to analyze performance. In the next section, we will see
    how to handle errors in CUDA code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，我们看到了两种测量和分析CUDA代码的方法。CUDA事件是一个用于测量设备代码时序的效率API。Nvidia Visual Profiler提供了对CUDA代码的详细分析和性能分析，可用于性能分析。在下一节中，我们将看到如何处理CUDA代码中的错误。
- en: Error handling in CUDA
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA中的错误处理
- en: We have not checked the availability of GPU devices or memory for our CUDA programs.
    It may happen that, when you run your CUDA program, the GPU device is not available
    or is out of memory. In that case, you may find it difficult to understand the
    reason for the termination of your program. Therefore, it is a good practice to
    add error handling code in CUDA programs. In this section, we will try to understand
    how we can add this error handling code to CUDA functions. When the code is not
    giving the intended output, it is useful to check the functionality of the code
    line-by-line or by adding a breakpoint in the program. This is called **debugging**.
    CUDA provides debugging tools that can help. So, in the following section, we
    will see some debugging tools that are provided by Nvidia with CUDA.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有检查CUDA程序中GPU设备和内存的可用性。可能发生的情况是，当你运行CUDA程序时，GPU设备不可用或内存不足。在这种情况下，你可能难以理解程序终止的原因。因此，在CUDA程序中添加错误处理代码是一个好习惯。在本节中，我们将尝试了解如何将此错误处理代码添加到CUDA函数中。当代码没有给出预期输出时，逐行检查代码的功能或通过在程序中添加断点来检查是有用的。这被称为**调试**。CUDA提供了可以帮助的调试工具。因此，在接下来的部分，我们将看到Nvidia与CUDA一起提供的某些调试工具。
- en: Error handling from within the code
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码中的错误处理
- en: 'When we discussed CUDA API functions in [Chapter 2](bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml),
    *Parallel Programming using CUDA C*, we saw that they also return the flag that
    indicates whether the operation has finished successfully or not. This can be
    used to handle errors from within CUDA programs. Of course, this will not help
    in resolving errors, but it will indicate which CUDA operation is causing errors.
    It is a very good practice to wrap CUDA functions with this error handling code.
    The sample error handling code for a `cudaMalloc` function is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第2章](bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml)中讨论CUDA API函数时，*使用CUDA
    C进行并行编程*，我们看到了它们也返回一个标志，表示操作是否成功完成。这可以用来在CUDA程序中处理错误。当然，这不会帮助解决错误，但它会指示哪个CUDA操作导致了错误。将错误处理代码包装在CUDA函数中是一个非常良好的实践。以下是一个`cudaMalloc`函数的示例错误处理代码：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `cudaError_t` API is used to create an error object that will store the
    return value of all CUDA operations. So the output of a `cudaMalloc` function
    is assigned to this error object. If the error object is not equal to `cudaSuccess`,
    then there was some error in assigning memory on the device. This is handled by
    an `if` statement. It will print the error on the console and jump to the end
    of the program. The wrapper code for error handling during a memory copy operation
    is shown as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaError_t` API 用于创建一个错误对象，该对象将存储所有 CUDA 操作的返回值。因此，`cudaMalloc` 函数的输出被分配给这个错误对象。如果错误对象不等于
    `cudaSuccess`，则表示在设备上分配内存时出现了错误。这通过一个 `if` 语句来处理。它将在控制台上打印错误并跳转到程序的末尾。以下是一个在内存复制操作期间进行错误处理的包装代码示例：'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Again, it has a similar structure to the error handling code for `cudaMalloc`.
    The wrapper code for the kernel call is shown as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，它与 `cudaMalloc` 的错误处理代码具有类似的结构。以下是一个内核调用包装代码的示例：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The kernel call does not return a flag that indicates success or failure, so
    it is not directly assigned to an error object. Instead, if there is any error
    during the kernel''s launch, then we can fetch it with the `cudaGetLastError()`
    API, which is used to handle an error during kernel calls. It is assigned to the
    `cudaStatus` error object and, if it is not equal to `cudaSuccess`, it prints
    the error on the console and jumps to the end of the program. All the error handling
    code jumps to the code section defined by the `Error` label. It can be defined
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 内核调用不返回表示成功或失败的标志，因此它不会直接分配给错误对象。相反，如果在内核的启动过程中出现任何错误，则可以使用 `cudaGetLastError()`
    API 获取它，该 API 用于处理内核调用期间的错误。它被分配给 `cudaStatus` 错误对象，如果它不等于 `cudaSuccess`，它将在控制台上打印错误并跳转到程序的末尾。所有错误处理代码都会跳转到由
    `Error` 标签定义的代码部分。它可以定义如下：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Whenever any error is encountered in a program, we jump to this section. We
    free up memory allocated on the device and then exit the `main` function. This
    is a very efficient way of writing CUDA programs. We suggest that you use this
    method for writing your CUDA code. It was not explained earlier to avoid unnecessary
    complexity in the code examples. The addition of error handling code in CUDA programs
    will make them longer, but it will be able to pinpoint which CUDA operation is
    causing problems in the code.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论在程序中遇到任何错误，我们都会跳转到这个部分。我们将释放设备上分配的内存，然后退出 `main` 函数。这是一种编写 CUDA 程序的非常有效的方法。我们建议您使用这种方法来编写您的
    CUDA 代码。之前没有解释这一点是为了避免在代码示例中引入不必要的复杂性。在 CUDA 程序中添加错误处理代码会使它们变得更长，但它能够确定是哪个 CUDA
    操作在代码中引起问题。
- en: Debugging tools
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试工具
- en: 'There are always two types of errors that we may encounter in programming:
    **syntax** errors and **semantic** errors. Syntax errors can be handled by compilers,
    but semantic errors are difficult to find and debug. Semantic errors cause the
    program to work unexpectedly. When your CUDA program is not working as intended,
    then there is a need to execute your code line-by-line to visualize the output
    after each line. This is called **debugging**. It is a very important operation
    for any kind of programming. CUDA provides debugging tools that help resolve this
    kind of error.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程中，我们可能会遇到两种类型的错误：**语法**错误和**语义**错误。语法错误可以通过编译器处理，但语义错误很难找到和调试。语义错误会导致程序出现意外的行为。当您的
    CUDA 程序没有按预期工作，就需要逐行执行代码以可视化每行后的输出。这被称为**调试**。这对于任何类型的编程来说都是一个非常重要的操作。CUDA 提供了调试工具，有助于解决这类错误。
- en: For Linux-based systems, Nvidia provides a very helpful debugger known as **CUDA-GDB**.
    It has a similar interface to the normal GDB debugger used for C code. It helps
    you in debugging your kernel directly on the GPU with features such as setting
    breakpoints, inspecting the GPU memory, and inspecting blocks and threads. It
    also provides a memory checker to check illegal memory accesses.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 Linux 的系统，Nvidia 提供了一个非常有用的调试器，称为 **CUDA-GDB**。它具有与用于 C 代码的正常 GDB 调试器类似的界面。它通过设置断点、检查
    GPU 内存、检查块和线程等功能，帮助您在 GPU 上直接调试内核。它还提供了一个内存检查器来检查非法内存访问。
- en: For Windows-based systems, Nvidia provides the Nsight debugger integrated with
    Microsoft Visual Studio. Again, it has features for adding breakpoints in the
    program and inspecting blocks or thread execution. The device's global memory
    can be viewed from a Visual Studio memory interface.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 Windows 的系统，Nvidia 提供了与 Microsoft Visual Studio 集成的 Nsight 调试器。同样，它具有在程序中添加断点和检查块或线程执行的功能。可以从
    Visual Studio 内存界面查看设备的全局内存。
- en: To summarize, in this section, we have seen two methods for handling errors
    in CUDA. One method helps in solving GPU hardware-related errors, such as the
    device or memory not being available, from CUDA programs. The second method of
    using debugging helps when the program is not working as per expectations. In
    the next section, we will see some advanced concepts that can help improve the
    performance of CUDA programs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，我们看到了两种处理CUDA中错误的方法。一种方法有助于解决与GPU硬件相关的错误，例如设备或内存不可用等CUDA程序中的错误。第二种使用调试的方法有助于当程序不符合预期时。在下一节中，我们将看到一些可以帮助提高CUDA程序性能的高级概念。
- en: Performance improvement of CUDA programs
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA程序的性能提升
- en: In this section, we will see some basic guidelines that we can follow to improve
    the performance of CUDA programs. These are explained one by one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到一些基本指南，我们可以遵循这些指南来提高CUDA程序的性能。这些将逐一解释。
- en: Using an optimum number of blocks and threads
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最佳数量的块和线程
- en: 'We have seen two parameters that need to be specified during a kernel call:
    the number of blocks and the number of threads per block. GPU resources should
    not be idle during a kernel call; only then it will give the optimum performance.
    If resources remain idle, then it may degrade the performance of the program.
    The number of blocks and threads per block help in keeping GPU resources busy.
    It has been researched that if the number of blocks are double the number of multiprocessors
    on the GPU, it will give the best performance. The total number of multiprocessors
    on the GPU can be found out by using device properties, as seen in [Chapter 2](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml),
    *Parallel Programming Using CUDA C*. In the same way, the maximum number of threads
    per block should be equal to the `maxThreadperblock` device property. These values
    are just for guidance. You can play around with these two parameters to get optimum
    performance in your application.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在内核调用过程中看到了两个需要指定的参数：每个块的数量和每个块中的线程数。在内核调用期间，GPU资源不应空闲；只有这样，它才能提供最佳性能。如果资源保持空闲，则可能会降低程序的性能。每个块和每个块中的线程数有助于保持GPU资源忙碌。研究表明，如果块的数量是GPU上多处理器数量的两倍，将提供最佳性能。GPU上多处理器的总数可以通过使用设备属性找到，如[第2章](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml)中所述，*使用CUDA
    C进行并行编程*。同样，每个块的最大线程数应等于`maxThreadperblock`设备属性。这些值仅作为指导。您可以通过调整这两个参数来获得应用程序中的最佳性能。
- en: Maximizing arithmetic efficiency
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大化算术效率
- en: '**Arithmetic efficiency** is defined as the ratio of the number of mathematical
    computations to the number of memory access operations. The value of arithmetic
    efficiency should be as high as possible for good performance. It can be increased
    by maximizing computations per thread and minimizing the time spent on the memory
    per thread. Sometimes, the opportunity to maximize computations per thread is
    limited, but certainly, you can reduce the time spent on the memory. You can minimize
    it by storing frequently accessed data in fast memory.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**算术效率**定义为数学运算次数与内存访问操作次数的比率。算术效率的值应尽可能高以获得良好的性能。可以通过最大化每个线程的运算次数和最小化每个线程在内存上的时间来实现。有时，最大化每个线程的运算次数的机会有限，但当然，您可以减少在内存上的时间。您可以通过将频繁访问的数据存储在快速内存中来最小化它。'
- en: We saw in the last chapter that the local memory and register files are the
    fastest memory types available on the GPU. So, they can be used to store data
    that needs frequent access. We also saw the use of shared memory, constant memory,
    and texture memory for performance improvement. Caching also helps in reducing
    memory access time. Ultimately, if we reduce the bandwidth of global memory, we
    can reduce the time spent on the memory. Efficient memory usage is very important
    in improving the performance of CUDA programs, as memory bandwidth is the biggest
    bottleneck in fast execution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中看到，局部内存和寄存器文件是GPU上可用的最快内存类型。因此，它们可以用来存储需要频繁访问的数据。我们还看到了使用共享内存、常量内存和纹理内存来提高性能。缓存也有助于减少内存访问时间。最终，如果我们减少全局内存的带宽，我们可以减少在内存上的时间。在提高CUDA程序性能方面，有效的内存使用非常重要，因为内存带宽是快速执行中的最大瓶颈。
- en: Using coalesced or strided memory access
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用归一化或步进内存访问
- en: 'Coalesced memory access means that every thread reads or writes into contiguous
    memory locations. GPU is most efficient when this memory access method is used.
    If threads use memory locations that are offset by a constant value, then this
    is called **strided memory access**. It still gives better performance than random
    memory access. So, if you try to use coalesced memory access in the program, it
    can drastically improve performance. Here are examples of these memory access
    patterns:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 合并内存访问意味着每个线程都读取或写入连续的内存位置。当使用这种内存访问方法时，GPU效率最高。如果线程使用偏移量为常数的内存位置，则这被称为**步进内存访问**。它仍然比随机内存访问有更好的性能。因此，如果您在程序中尝试使用合并内存访问，它可以显著提高性能。以下是一些这些内存访问模式的示例：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Avoiding thread divergence
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免线程发散
- en: 'Thread divergence happens when all threads in a kernel call follow different
    paths of execution. It can happen in the following kernel code scenarios:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核中的所有线程调用不同的执行路径时，会发生线程发散。它可以在以下内核代码场景中发生：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the first code snippet, there is separate code for odd and even number threads
    because of the condition in the `if` statement. This makes odd and even number
    threads follow different paths for execution. After the `if` statement, these
    threads will again merge. This will incur time overhead because fast threads will
    have to wait for slow threads.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个代码片段中，由于`if`语句中的条件，存在针对奇数和偶数线程的单独代码。这使得奇数和偶数线程遵循不同的执行路径。在`if`语句之后，这些线程将再次合并。这将产生时间开销，因为快速线程将不得不等待慢速线程。
- en: In the second example, using the `for` loop, each thread runs the `for` loop
    for a different number of iterations, hence all threads will take different amounts
    of time to finish. `Post loop code` has to wait for all these threads to finish.
    It will incur time overhead. So, as much as possible, avoid this kind of thread
    divergence in your code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，使用`for`循环，每个线程运行`for`循环的迭代次数不同，因此所有线程完成所需的时间不同。`循环后的代码`必须等待所有这些线程完成。这将产生时间开销。因此，尽可能避免在您的代码中这种类型的线程发散。
- en: Using page-locked host memory
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用页面锁定主机内存
- en: 'In every example until this point, we have used the `malloc` function to allocate
    memory on the host, which allocates standard pageable memory on the host. CUDA
    provides another API called `cudaHostAlloc()`, which allocates page-locked host
    memory or what is sometimes referred to as pinned memory. It guarantees that the
    operating system will never page this memory out of this disk and that it will
    remain in physical memory. So, any application can access the physical address
    of the buffer. This property helps the GPU copy data to and from the host via
    **Direct Memory Access (DMA)** without CPU intervention. This helps improve the
    performance of memory transfer operations. But page-locked memory should be used
    with proper care because this memory is not swapped out of disk; your system may
    run out of memory. It may effect the performance of other applications running
    on the system. You can use this API to allocate memory that is used to transfer
    data to a device, using the `Memcpy` operation. The syntax of using this API is
    shown as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前的每个示例中，我们使用`malloc`函数在主机上分配内存，这在主机上分配标准可分页内存。CUDA提供了一个名为`cudaHostAlloc()`的另一个API，它分配页面锁定主机内存或有时称为固定内存。它保证操作系统永远不会将此内存从磁盘页出，并且它将保留在物理内存中。因此，任何应用程序都可以访问缓冲区的物理地址。这种属性有助于GPU通过**直接内存访问（DMA）**将数据从主机复制到主机，而无需CPU干预。这有助于提高内存传输操作的性能。但是，应该小心使用固定内存，因为这种内存不会被换出到磁盘；您的系统可能耗尽内存。它可能影响系统上运行的其他应用程序的性能。您可以使用此API分配用于通过`Memcpy`操作将数据传输到设备的内存。使用此API的语法如下：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The syntax of `cudaHostAlloc` is similar to a simple `malloc` function. The
    last argument, `cudaHostAllocDefault`, which is a flag used to modify the behavior
    of pinned memory, is added. `cudaFreeHost` is used to free memory allocated using
    a `cudaHostAlloc` function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaHostAlloc`的语法类似于简单的`malloc`函数。最后一个参数，`cudaHostAllocDefault`，是一个用于修改固定内存行为的标志。`cudaFreeHost`用于释放使用`cudaHostAlloc`函数分配的内存。'
- en: CUDA streams
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA流
- en: We have seen that the GPU provides a great performance improvement in data parallelism
    when a single instruction operates on multiple data items. We have not seen task
    parallelism where more than one kernel function, which are independent of each
    other, operate in parallel. For example, one function may be computing pixel values
    while another function is downloading something from the internet. We know that
    the CPU provides a very flexible method for this kind of task parallelism. The
    GPU also provides this capability, but it is not as flexible as the CPU. This
    task parallelism is achieved by using CUDA streams, which we will see in detail
    in this section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当单个指令对多个数据项进行操作时，GPU在数据并行性方面提供了极大的性能提升。我们还没有看到任务并行性，其中多个相互独立的内核函数并行运行。例如，一个函数可能正在计算像素值，而另一个函数正在从互联网上下载某些内容。我们知道CPU提供了非常灵活的方法来处理这种类型的任务并行性。GPU也提供了这种能力，但它的灵活性不如CPU。这种任务并行性是通过使用CUDA流实现的，我们将在本节中详细探讨。
- en: A CUDA stream is nothing but a queue of GPU operations that execute in a specific
    order. These functions include kernel functions, memory copy operations, and CUDA
    event operations. The order in which they are added to the queue will determine
    the order of their execution. Each CUDA stream can be considered a single task,
    so we can start multiple streams to do multiple tasks in parallel. We will look
    at how multiple streams work in CUDA in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA流实际上是一个GPU操作的队列，这些操作按特定顺序执行。这些函数包括内核函数、内存复制操作和CUDA事件操作。它们被添加到队列中的顺序将决定它们的执行顺序。每个CUDA流可以被视为一个单独的任务，因此我们可以启动多个流以并行执行多个任务。我们将在下一节中查看CUDA中多个流的工作方式。
- en: Using multiple CUDA streams
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个CUDA流
- en: 'We will understand the working of CUDA streams by using multiple CUDA streams
    in the vector addition program that we developed in the previous chapter. The
    kernel function for this is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在上一章开发的向量加法程序中使用多个CUDA流来理解CUDA流的工作原理。这个内核函数如下所示：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `kernel` function is similar to what we developed earlier. It is just that
    multiple streams will execute this kernel in parallel. It should be noted that
    not all GPU devices support CUDA streams. GPU devices that support the `deviceOverlap`
    property can perform memory transfer operations and kernel executions simultaneously.
    This property will be used in CUDA streams for task parallelism. Before proceeding
    further in this code, please ensure that your GPU device supports this property.
    You can use code from [Chapter 2](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml),
    *Parallel Programming Using CUDA C*, to verify this property. We will use two
    parallel streams, which will execute this kernel in parallel and operate on half
    of the input data each. We will start by creating these two streams in the main
    function, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`内核`函数与我们之前开发的类似。它只是多个流将并行执行这个内核。需要注意的是，并非所有GPU设备都支持CUDA流。支持`deviceOverlap`属性的GPU设备可以同时执行内存传输操作和内核执行。这个属性将在CUDA流中用于任务并行。在继续此代码之前，请确保您的GPU设备支持此属性。您可以使用[第2章](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml)中的代码，*使用CUDA
    C进行并行编程*，来验证此属性。我们将使用两个并行流，它们将并行执行此内核，并对输入数据的一半进行操作。我们将在主函数中首先创建这两个流，如下所示：'
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Two stream objects, `stream 0` and `stream 1`, are defined using the `cudaStream_t`
    and `cudaStreamCreate` APIs. We have also defined host pointers and two sets of
    device pointers, which will be used for each stream separately. We defined and
    created two events for performance measurement of this program. Now, we need to
    allocate memory for these pointers. The code is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cudaStream_t`和`cudaStreamCreate` API定义了两个流对象，`stream 0`和`stream 1`。我们还定义了主机指针和两组设备指针，它们将分别用于每个流。我们定义并创建了两个事件来测量此程序的性能。现在，我们需要为这些指针分配内存。代码如下：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'CUDA streams need an access to page-locked memory for its memory copy operation,
    hence we are defining host memory using a `cudaHostAlloc` function instead of
    simple `malloc`. We have seen the advantage of page-locked memory in the last
    section. Two sets of device pointers are allocated memory using `cudaMalloc`.
    It should be noted that host pointers hold the entire data so their size is `2*N*sizeof(int)`,
    whereas each device pointer operates on half the data elements so their size is
    only `N*sizeof(int)`. We have also initialized host arrays with some random values
    for addition. Now, we will try to enqueue memory copy operations and kernel execution
    operations in both of the streams. The code for this is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA流在进行内存复制操作时需要访问页锁定内存，因此我们使用`cudaHostAlloc`函数而不是简单的`malloc`来定义主机内存。我们在上一节中看到了页锁定内存的优势。使用`cudaMalloc`分配了两组设备指针的内存。需要注意的是，主机指针持有全部数据，因此其大小为`2*N*sizeof(int)`，而每个设备指针只操作一半的数据元素，因此其大小仅为`N*sizeof(int)`。我们还用一些随机值初始化了主机数组以进行加法操作。现在，我们将尝试在两个流中同时排队内存复制操作和内核执行操作。相应的代码如下：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Instead of using a simple `cudaMemcpy` API, we are using a `cudaMemcpyAsync`
    API, which is used for asynchronous memory transfer. It enqueues a request of
    a memory copy operation in a given stream specified as the last argument to the
    function. When this function returns, the memory copy operation may not have even
    started, hence it is called an asynchronous operation. It just puts a request
    of the memory copy in a queue. As we can see in the memory copy operations, `stream0`
    operates on data between `0` to `N` and `stream 1` operates on data from `N+1`
    to `2N`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用简单的`cudaMemcpy` API，而是使用`cudaMemcpyAsync` API，它用于异步内存传输。它将一个内存复制操作的请求排队到由函数的最后一个参数指定的给定流中。当这个函数返回时，内存复制操作可能还没有开始，因此它被称为异步操作。它只是将内存复制的请求放入队列中。正如我们可以在内存复制操作中看到的那样，`stream0`操作从`0`到`N`的数据，而`stream
    1`操作从`N+1`到`2N`的数据。
- en: 'The order of operation is important in stream operations as we want to overlap
    memory copy operations with kernel execution operations. So, instead of enqueuing
    all the `stream0` operations and then enqueuing the `stream 1` operations, we
    are first enqueuing memory copy operations in both streams and then enqueuing
    both kernel computation operations. This will ensure that memory copy and kernel
    computation overlaps with each other. If both operations take the same amount
    of time, we can achieve two times the speedup. We can get a better idea about
    the order of operations by looking at the following diagram:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在流操作中，操作顺序很重要，因为我们希望内存复制操作与内核执行操作重叠。因此，我们不是先排队所有`stream0`操作，然后排队`stream 1`操作，而是首先在两个流中排队内存复制操作，然后排队内核计算操作。这将确保内存复制和内核计算相互重叠。如果这两个操作花费相同的时间，我们可以实现两倍的速度提升。我们可以通过查看以下图表来更好地了解操作顺序：
- en: '![](img/e068bb15-3dcf-4826-8563-cb0c96994f04.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e068bb15-3dcf-4826-8563-cb0c96994f04.png)'
- en: 'The time increases from top to bottom. We can see that two memory copy operations
    and kernel execute operations are performed in the same time period, which will
    accelerate your program. We have also seen that the memory copy operations, defined
    by `cudaMemcpyAsync`, are asynchronous; so, when one stream returns, the memory
    copy operation may not have started. If we want to use the result of the last
    memory copy operation, then we have to wait for both streams to finish their queue
    operations. This can be ensured by using the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 时间从上到下增加。我们可以看到，在同一时间段内执行了两个内存复制操作和内核执行操作，这将加速你的程序。我们还看到，由`cudaMemcpyAsync`定义的内存复制操作是异步的；因此，当一个流返回时，内存复制操作可能还没有开始。如果我们想使用最后一个内存复制操作的结果，那么我们必须等待两个流完成它们的队列操作。这可以通过使用以下代码来确保：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`cudaStreamSynchronize` ensures that all operations in the stream are finished
    before proceeding to the next line. To measure the performance of the code, the
    following code is inserted:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaStreamSynchronize`确保在继续到下一行之前，流中的所有操作都已完成。为了测量代码的性能，我们插入以下代码：'
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It will record the stop time and, based on the difference between the start
    and stop time, it will calculate the overall execution time for this program and
    print the output on the console. To check whether the program has calculated the
    correct output, we will insert the following code for verification:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 它将记录停止时间，并根据开始和停止时间之间的差异，计算该程序的总体执行时间，并在控制台上打印输出。为了检查程序是否计算了正确的输出，我们将插入以下代码进行验证：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The verification code is similar to what we have seen earlier. Memory allocated
    on the device is freed up using `cudaFree` and memory allocated on the host using
    `cudaHostAlloc` is freed up using the `cudaFreeHost` function. This is mandatory,
    otherwise your system may run out of memory very quickly. The output of the program
    is shown as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 验证代码与我们之前看到的类似。使用`cudaFree`释放设备上分配的内存，使用`cudaHostAlloc`在主机上分配的内存使用`cudaFreeHost`函数释放。这是强制性的，否则您的系统可能会很快耗尽内存。程序输出如下所示：
- en: '![](img/e2d48765-0845-4e40-8995-8f961ec3fc48.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2d48765-0845-4e40-8995-8f961ec3fc48.png)'
- en: As can be seen in the preceding screenshot, 0.9 ms is needed to add 100,000
    elements, which is a double increment over code without streams, which needed
    0.9 ms to add 50,000 numbers, as seen in the first section of this chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个截图所示，需要0.9毫秒来添加100,000个元素，这是在没有流的情况下代码的两倍增加，如本章第一部分所示，添加50,000个数字需要0.9毫秒。
- en: To summarize, we have seen CUDA streams in this section, which helps achieve
    task parallelism on the GPU. The order in which operations are queued in streams
    is very important to achieve speedup using CUDA streams.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中我们看到了CUDA流，它有助于在GPU上实现任务并行。在流中排队操作的顺序对于使用CUDA流实现加速非常重要。
- en: Acceleration of sorting algorithms using CUDA
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA加速排序算法
- en: Sorting algorithms are widely used in many computing applications. There are
    many sorting algorithms, such as enumeration or rank sort, bubble sort, and merge
    sort. All algorithms have different levels of complexity, so it takes a different
    amount of time to sort a given array. For a large array, all algorithms take a
    long time to complete. If this can be accelerated using CUDA, then it can be of
    great help in any computing application.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 排序算法在许多计算应用中被广泛使用。有许多排序算法，例如枚举或排名排序、冒泡排序和归并排序。所有算法都有不同的复杂度级别，因此对给定数组进行排序所需的时间不同。对于大型数组，所有算法都需要很长时间才能完成。如果可以使用CUDA进行加速，那么它将对任何计算应用都有很大帮助。
- en: To show an example of how CUDA can accelerate different sorting algorithms,
    we will implement a rank sort algorithm in CUDA.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示CUDA如何加速不同的排序算法，我们将实现一个排名排序算法。
- en: Enumeration or rank sort algorithms
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 枚举或排名排序算法
- en: 'In this algorithm, we count every element in an array to find out how many
    elements in an array are less than the current element. From that, we can get
    the position of the current element in a sorted array. Then, we put this element
    in that position. We repeat this process for all elements in an array to get a
    sorted array. This is implemented as the `kernel` function, which is shown as
    follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，我们计算数组中的每个元素，以找出数组中有多少元素小于当前元素。从那里，我们可以得到当前元素在排序数组中的位置。然后，我们将此元素放在那个位置。我们重复这个过程，直到数组中的所有元素，以得到一个排序数组。这被实现为`kernel`函数，如下所示：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `Kernel` function takes two arrays as parameters. `d_a` is an input array
    and `d_b` is the output array. The `count` variable is taken, which stores the
    position of the current element in the sorted array. The current thread index
    in the block is stored in `tid` and the unique thread index among all blocks is
    stored in `ttid`. Shared memory is used to reduce the time in accessing data from
    global memory. The size of shared memory is equal to the number of threads in
    a block, as discussed earlier. The `value` variable holds the current element.
    Shared memory is filled up with the values of global memory. These values are
    compared with the `value` variable and the count of the number of values that
    are less is stored in the `count` variable. This continues until all elements
    in an array are compared with the `value` variable. After the loop finishes, the
    `count` variable has the position of the element in a sorted array, and the current
    element is stored at that position in `d_b`, which is an output array.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kernel`函数接受两个数组作为参数。`d_a`是输入数组，`d_b`是输出数组。`count`变量被取用，它存储当前元素在排序数组中的位置。当前线程在块中的索引存储在`tid`中，所有块中唯一的线程索引存储在`ttid`中。使用共享内存来减少从全局内存访问数据的时间。共享内存的大小等于块中线程的数量，如前所述。`value`变量持有当前元素。共享内存被填充为全局内存中的值。这些值与`value`变量进行比较，并将小于的值的数量存储在`count`变量中。这会一直持续到数组中的所有元素都与`value`变量进行比较。循环结束后，`count`变量有元素在排序数组中的位置，并将当前元素存储在输出数组`d_b`中的那个位置。'
- en: 'The `main` function for this code is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码的`main`函数如下：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `main` function should be very familiar to you by now. We are defining
    host and device arrays and allocating memory on the device for device arrays.
    The host array is initialized with some random values and copied to the device''s
    memory. The kernel is launched by passing device pointers as parameters. The kernel
    computes the sorted array by the rank sort algorithm and returns it to the host.
    This sorted array is printed on the console as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数你现在应该已经很熟悉了。我们正在定义主机和设备数组，并在设备上为设备数组分配内存。主机数组使用一些随机值初始化，并将其复制到设备的内存中。通过传递设备指针作为参数来启动内核。内核通过排名排序算法计算排序后的数组，并将其返回到主机。这个排序后的数组如下所示在控制台上打印：'
- en: '![](img/a545b18f-26fd-4176-9963-5af2a4752a5e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a545b18f-26fd-4176-9963-5af2a4752a5e.png)'
- en: This is a very trivial case and you might not see any performance improvement
    between the CPU and GPU. But if you go on increasing the value of `arraySize`
    , then the GPU will drastically improve the performance of this algorithm. It
    can be a hundredfold improvement for an array size equal to 15,000.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的情况，你可能看不到 CPU 和 GPU 之间有任何性能提升。但是，如果你继续增加 `arraySize` 的值，那么 GPU 将会极大地提高这个算法的性能。对于大小等于
    15,000 的数组，它可以实现百倍的性能提升。
- en: Rank sort is the simplest sorting algorithm available. This discussion will
    help you in developing code for other sorting algorithms, such as bubble sort
    and merge sort.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 排名排序是可用的最简单的排序算法。这次讨论将帮助你开发其他排序算法的代码，例如冒泡排序和归并排序。
- en: Image processing using CUDA
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CUDA 进行图像处理
- en: Today, we live in an age of high definition camera sensors that capture high-resolution
    images. An image can have a size of up to 1920 x 1920 pixels. So, processing of
    these pixels on computers in real time involves billions of floating point operations
    to be performed per second. This is difficult for even the fastest of CPUs. A
    GPU can help in this kind of situation. It provides high computation power, which
    can be leveraged using CUDA in your code.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们生活在一个高清摄像头传感器时代，它可以捕捉高分辨率的图像。一个图像可以达到 1920 x 1920 像素的大小。因此，在计算机上实时处理这些像素需要每秒执行数十亿次的浮点运算。即使是速度最快的
    CPU 也难以做到这一点。GPU 可以在这种情况下提供帮助。它提供了高计算能力，这可以通过 CUDA 在你的代码中利用。
- en: Images are stored as multidimensional arrays in a computer with two dimensions
    for a grayscale image and three dimensions for a color image. CUDA also supports
    multidimensional grid blocks and threads. So, we can process an image by launching
    multidimensional blocks and threads, as seen previously. The number of blocks
    and threads can vary depending on the size of an image. It will also depend on
    your GPU specifications. If it supports 1,024 threads per block, then 32 x 32
    threads per block can be launched. The number of blocks can be determined by dividing
    the image size by the number of these threads. As discussed many times previously,
    the choice of parameters affects the performance of your code. So, they should
    be chosen properly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机中，图像以多维数组的形式存储，灰度图像有两个维度，彩色图像有三个维度。CUDA 也支持多维网格块和线程。因此，我们可以通过启动多维块和线程来处理图像，就像之前看到的那样。块和线程的数量可以取决于图像的大小。它也将取决于你的
    GPU 规格。如果它支持每个块 1,024 个线程，那么可以启动每个块 32 x 32 个线程。块的数量可以通过将图像大小除以这些线程的数量来确定。正如之前多次讨论的那样，参数的选择会影响你代码的性能。因此，它们应该被适当地选择。
- en: 'It is very easy to convert a simple image processing code developed in C or
    C++ to a CUDA code. It can be done by an inexperienced programmer also by following
    a set pattern. Image processing code has a set pattern, as in the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将用 C 或 C++ 开发的简单图像处理代码转换为 CUDA 代码非常容易。即使是不经验验的程序员也可以通过遵循一个固定的模式来完成。图像处理代码有一个固定的模式，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Images are nothing but multidimensional matrices stored on a computer, so getting
    a single pixel value from an image involves using nested `for` loops to iterate
    over all pixels. To convert this code to CUDA, we want to launch a number of threads
    equal to the number of pixels in an image. The pixel value in a thread can be
    obtained by the following code in the `kernel` function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图像不过是存储在计算机上的多维矩阵，因此从图像中获取单个像素值需要使用嵌套的 `for` 循环来遍历所有像素。为了将此代码转换为 CUDA，我们希望启动与图像中像素数量相等的线程数量。在
    `kernel` 函数中，可以通过以下代码在线程中获取像素值：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `i` and `j` values can be used as an index to an image array to find the
    pixel values. So, as can be seen in the previous code, with a simple conversion
    process of converting `for` loops into a thread index, we can write device code
    for a CUDA program. We are going to develop many image processing applications
    using the `OpenCV` library from the next section onward. We will not cover actual
    image manipulation in this chapter, but we will end this chapter by developing
    a CUDA program for the very important statistical operation of calculating a histogram.
    Histogram calculation is very important for image processing applications as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`i`和`j`值可以用作图像数组的索引来查找像素值。所以，如前述代码所示，通过将`for`循环转换为线程索引的简单转换过程，我们可以编写CUDA程序的设备代码。从下一节开始，我们将使用`OpenCV`库开发许多图像处理应用程序。在本章中，我们不会涵盖实际的图像操作，但我们将通过开发一个用于计算直方图这一重要统计操作的CUDA程序来结束本章。直方图计算对于图像处理应用程序也非常重要。'
- en: Histogram calculation on the GPU using CUDA
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA在GPU上计算直方图
- en: A histogram is a very important statistical concept used in a variety of applications
    such as machine learning, computer vision, data science, and image processing.
    It represents a count of the frequency of each element in a given dataset. It
    shows which data items occur the most frequently and which occur the least frequently.
    You can also get an idea about the distribution of data by just looking at the
    values of the histogram. In this section, we will develop an algorithm that calculates
    the histogram of a given data distribution.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是一个非常重要的统计概念，在机器学习、计算机视觉、数据科学和图像处理等多种应用中使用。它表示给定数据集中每个元素频率的计数。它显示了哪些数据项出现频率最高，哪些出现频率最低。你也可以通过查看直方图的值来了解数据的分布。在本节中，我们将开发一个算法，用于计算给定数据分布的直方图。
- en: 'We will start by calculating a histogram on the CPU so that you can get an
    idea of how to calculate a histogram. Let''s assume that we have data with 1,000
    elements, and each element has a value between 0 to 15\. We want to calculate
    the histogram of this distribution. The sample code for this calculation on the
    CPU is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在CPU上计算直方图，这样你可以了解如何计算直方图。假设我们有一组包含1,000个元素的数，每个元素的价值在0到15之间。我们想要计算这个分布的直方图。在CPU上计算这个计算的示例代码如下：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have 1,000 data elements and they are stored in `h_a`. The `h_a` array contains
    values between `0` and `15`; it has 16 distinct values. So the number of bins,
    which indicates the number of distinct values for which histogram needs to be
    calculated, is 16\. So we have defined a histogram array that will store the final
    histogram with a size equal to the number of bins. This array needs to be initialized
    to zero as it will be incremented with each occurrence. This is done in the first
    `for` loop that runs from `0` to the number of bins.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有1,000个数据元素，它们存储在`h_a`中。`h_a`数组包含`0`到`15`之间的值；它有16个不同的值。因此，bin的数量，即需要计算直方图的唯一值的数量，是16。因此，我们定义了一个大小等于bin数量的直方图数组，用于存储最终的直方图。这个数组需要初始化为零，因为它将在每次发生时递增。这是在从`0`到bin数量的第一个`for`循环中完成的。
- en: For the calculation of the histogram, we need to iterate through all of the
    elements in `h_a`. Whichever value is found in `h_a`, the particular index in
    that histogram array needs to be incremented. That is done by the second `for`
    loop, which calculates the final histogram by running from 0 to the size of the
    array and incrementing the histogram array indexed by the value found in `h_a`.
    The histogram array will contain the frequency of each element between `0 to 15`
    after the `for` loop finishes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直方图的计算，我们需要遍历`h_a`中的所有元素。在`h_a`中找到的任何值，都需要增加该直方图数组中特定索引的值。这是通过第二个`for`循环完成的，该循环从`0`到数组大小运行，并增加由`h_a`中找到的值索引的直方图数组。在`for`循环完成后，直方图数组将包含`0`到`15`之间每个元素的频率。
- en: 'Now, we will develop this same code for the GPU. We will try to develop this
    code using three different methods. The kernel code for the first two methods
    is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为GPU开发相同的代码。我们将尝试使用三种不同的方法来开发这个代码。前两种方法的内核代码如下：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The first function is the simplest kernel function for histogram computation.
    Each thread is operating on one data element. The value of the data element is
    fetched using the thread ID as an index to the input array. This value is used
    as an index into the `d_b` output array, which is incremented. The `d_b` array
    should contain the frequency of each value between `0` to `15` in the input data.
    But if you recall from [Chapter 3](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml),
    *Threads, Synchronization, and Memory*, this may not give you a correct answer,
    as many threads are trying to modify the same memory location simultaneously.
    In this example, 1,000 threads are trying to modify 16 memory locations simultaneously.
    We need to use the atomic `add` operation for this kind of scenario.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数是直方图计算的最简单内核函数。每个线程都在操作一个数据元素。使用线程ID作为索引从输入数组中获取数据元素的值。这个值被用作`d_b`输出数组的索引，该数组被递增。`d_b`数组应该包含输入数据中每个值（`0`到`15`）的频率。但如果你回想一下[第3章](0cea3d6f-76ab-449e-8264-85dbdb57de9c.xhtml)，*线程、同步和内存*，这可能不会给你一个正确答案，因为许多线程正在同时尝试修改相同的内存位置。在这个例子中，1,000个线程正在同时尝试修改16个内存位置。我们需要在这种情况下使用原子的`add`操作。
- en: 'The second device function is developed using the atomic `add` operation. This
    kernel function will give you the correct answer but it will take more time to
    complete, as the atomic operation is a blocking operation. All other threads have
    to wait when one thread is using a particular memory location. So, this second
    kernel function will add overhead time, which makes it even slower than the CPU
    version. To complete the code, we will try to write the `main` function for it
    as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个设备函数是使用原子`add`操作开发的。这个内核函数将给出正确答案，但完成所需的时间会更长，因为原子操作是一个阻塞操作。当有一个线程正在使用特定的内存位置时，所有其他线程都必须等待。因此，这个第二个内核函数将增加开销时间，使其比CPU版本还要慢。为了完成代码，我们将尝试按照以下方式编写它的`main`函数：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We have started the `main` function by defining the host and device arrays
    and allocating memory for it. The `h_a` input data array is initialized with values
    from `0 to 15` in the first `for` loop. We are using the modulo operation, hence
    1,000 elements will be evenly divided between values of `0 to 15`. The second
    array, which will store the histogram, is initialized to zero. These two arrays
    are copied to the device memory. The kernel will compute the histogram and return
    it to the host. We will print this histogram on the console. The output is shown
    here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义主机和设备数组并为它们分配内存来启动了`main`函数。在第一个`for`循环中，`h_a`输入数据数组被初始化为从`0`到`15`的值。我们使用了取模运算，因此1,000个元素将均匀地分配到`0`到`15`的值之间。第二个数组，用于存储直方图，被初始化为零。这两个数组被复制到设备内存中。内核将计算直方图并将其返回到主机。我们将在控制台上打印这个直方图。输出如下所示：
- en: '![](img/23a248c6-ceb8-47b0-ab44-bb8f91850aca.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23a248c6-ceb8-47b0-ab44-bb8f91850aca.png)'
- en: 'When we try to measure the performance of this code using the atomic operation
    and compare it with CPU performance, it is slower than the CPU for large-sized
    arrays. That begs the question: should we use CUDA for histogram computation,
    or is it possible to make this computation faster?'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试使用原子操作来测量这段代码的性能并与CPU性能进行比较时，对于大型数组，它比CPU慢。这引发了一个问题：我们应该使用CUDA进行直方图计算，还是有可能使这种计算更快？
- en: 'The answer to this question is: *yes*. If we use shared memory for computing
    a histogram for a given block and then add this block histogram to the overall
    histogram on global memory, then it can speed up the operation. This is possible
    because addition is a cumulative operation. The kernel code of using shared memory
    for histogram computation is shown as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案是：*是*。如果我们为给定的块计算直方图使用共享内存，然后将这个块直方图添加到全局内存上的整体直方图中，那么可以加快操作速度。这是因为加法是一个累积操作。以下是用共享内存进行直方图计算的内核代码：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this code, the number of bins are 256 instead of 16, for more capacity. We
    are defining shared memory of a size equal to the number of threads in a block,
    which is equal to 256 bins. We will calculate a histogram for the current block,
    so shared memory is initialized to zero and a histogram is computed for this block
    in the same way as discussed earlier. But, this time, the result is stored in
    shared memory and not in global memory. In this case, only 256 threads are trying
    to access 256 memory elements in shared memory, instead of 1,000 elements, as
    in the previous code. This will help reduce the time overhead in the atomic operation.
    The final atomic `add` operation in the last line will add a histogram of one
    block to the overall histogram values. As addition is a cumulative operation,
    we do not have to worry about the order in which each block is executed. The `main`
    function for this is similar to the previous function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，桶的数量为256而不是16，以提供更大的容量。我们定义的共享内存大小等于一个块中的线程数，即256个桶。我们将计算当前块的直方图，因此共享内存初始化为零，并按前面讨论的方式计算此块的直方图。但是，这次结果存储在共享内存中，而不是全局内存中。在这种情况下，只有256个线程试图访问共享内存中的256个内存元素，而不是像前一个代码中的1,000个元素。这将有助于减少原子操作中的时间开销。最后一行的最终原子`add`操作将一个块的直方图添加到整体直方图值中。由于加法是一个累积操作，我们不必担心每个块执行的顺序。此`main`函数与前面的函数类似。
- en: 'The output for this `kernel` function is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`内核`函数的输出如下：
- en: '![](img/2bfad7b5-3977-4a99-977d-d7ef4d5e290c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2bfad7b5-3977-4a99-977d-d7ef4d5e290c.png)'
- en: If you measure the performance of the preceding program, it will beat both GPU
    versions without shared memory and the CPU implementation for large array sizes.
    You can check whether the histogram computed by the GPU is correct or not by comparing
    results with the CPU computation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你测量前面程序的性能，它将击败没有共享内存的GPU版本和大型数组大小的CPU实现。你可以通过将GPU计算的直方图结果与CPU计算结果进行比较来检查GPU计算的直方图是否正确。
- en: This section demonstrated the implementation of the histogram on the GPU. It
    also re-emphasized the use of shared memory and atomic operations in CUDA programs.
    It also demonstrated how CUDA is helpful in image processing applications and
    how easy it is to convert existing CPU code into CUDA code.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本节演示了在GPU上实现直方图的过程。它还强调了在CUDA程序中使用共享内存和原子操作的重要性。它还展示了CUDA在图像处理应用中的帮助以及将现有CPU代码转换为CUDA代码的简便性。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw some advanced concepts in CUDA that can help us to develop
    a complex application using CUDA. We saw the method for measuring the performance
    of the device code and how to see a detailed profile of the kernel function using
    the Nvidia Visual Profiler tool. It helps us in identifying the operation that
    slows down the performance of our program. We saw the methods to handle errors
    in hardware operation from the CUDA code itself, and we saw methods of debugging
    the code using certain tools. The CPU provides efficient task parallelism where
    two completely different functions execute in parallel. We saw that the GPU also
    provides this functionality using CUDA streams and achieves a twofold speedup
    on the same vector addition program using CUDA streams.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一些CUDA的高级概念，这些概念可以帮助我们使用CUDA开发复杂的应用程序。我们看到了测量设备代码性能的方法，以及如何使用Nvidia
    Visual Profiler工具查看内核函数的详细配置文件。这有助于我们识别降低程序性能的操作。我们看到了从CUDA代码本身处理硬件操作错误的方法，以及使用某些工具调试代码的方法。CPU提供了有效的任务并行性，其中两个完全不同的函数可以并行执行。我们还看到GPU也通过CUDA流提供这种功能，并在相同的向量加法程序中使用CUDA流实现了两倍的速度提升。
- en: Then, we saw an acceleration of sorting algorithms using CUDA, which is an important
    concept to understand in order to build complex computing applications. Image
    processing is a computationally intensive task, which needs to be performed in
    real time. Almost all image processing algorithms can utilize parallelism of the
    GPU and CUDA. So, in the last section, we saw the use of CUDA in the acceleration
    of image processing application and how we can convert existing C++ code to CUDA
    code. We have also developed CUDA code for histogram calculation, which is an
    important image processing application.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看到了使用CUDA加速排序算法的例子，这是构建复杂计算应用时需要理解的重要概念。图像处理是一个计算密集型任务，需要实时执行。几乎所有的图像处理算法都可以利用GPU和CUDA的并行性。因此，在最后一节中，我们看到了CUDA在加速图像处理应用中的应用，以及如何将现有的C++代码转换为CUDA代码。我们还开发了用于直方图计算的CUDA代码，这是一个重要的图像处理应用。
- en: This chapter also marks an end to concepts related to CUDA programming. From
    the next chapter onward, we will start with the exciting part of developing computer
    vision applications using the OpenCV library, which utilizes the CUDA acceleration
    concepts that we have seen up until this point. We will start dealing with real
    images and not matrices from the next chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章也标志着与CUDA编程相关概念的结束。从下一章开始，我们将开始使用OpenCV库开发计算机视觉应用，该库利用了我们到目前为止所看到的CUDA加速概念。从下一章开始，我们将处理真实图像而不是矩阵。
- en: Questions
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why aren't CPU timers used to measure the performance of kernel functions?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么不使用CPU计时器来衡量内核函数的性能？
- en: Try to visualize the performance of the matrix multiplication code implemented
    in the previous chapter using the Nvidia Visual Profiler tool.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用Nvidia Visual Profiler工具可视化上一章中实现的矩阵乘法代码的性能。
- en: Give different examples of semantic errors encountered in programs.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给出程序中遇到的不同语义错误示例。
- en: What is the drawback of thread divergence in kernel functions? Explain with
    an example.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内核函数中线程发散的缺点是什么？请用例子解释。
- en: What is the drawback of using the `cudahostAlloc` function to allocate memory
    on the host?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cudahostAlloc`函数在主机上分配内存的缺点是什么？
- en: 'Justify the following statement: the order of operations in CUDA streams is
    very important to improve the performance of a program.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明以下陈述的正确性：CUDA流中的操作顺序对于提高程序性能非常重要。
- en: How many blocks and threads should be launched for a 1024 x 1024 image for good
    performance using CUDA?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在CUDA中实现良好的性能，对于1024 x 1024的图像，应该启动多少个块和线程？
