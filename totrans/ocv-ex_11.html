<html><head></head><body><div class="chapter" title="Chapter&#xA0;11.&#xA0;Text Recognition with Tesseract"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Text Recognition with Tesseract</h1></div></div></div><p>In the previous chapter, we covered the very basic OCR processing functions. Although they are quite useful for scanned or photographed documents, they are almost useless when dealing with text that casually appears in a picture.</p><p>In this chapter, we'll explore the OpenCV 3.0 text module, which deals specifically with scene text detection. Using this API, it is possible to detect text that appears in a webcam video, or to analyze photographed images (like the ones in Street View or taken by a surveillance camera) to extract text information in real time. This allows a wide range of applications to be created, from accessibility to marketing and even robotics fields.</p><p>By the end of this chapter, you will be able to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understand what is scene text recognition</li><li class="listitem" style="list-style-type: disc">Understand how the text API works</li><li class="listitem" style="list-style-type: disc">Use the OpenCV 3.0 text API to detect text</li><li class="listitem" style="list-style-type: disc">Extract the detected text to an image</li><li class="listitem" style="list-style-type: disc">Use the text API and Tesseract integration to identify letters</li></ul></div><div class="section" title="How the text API works"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec74"/>How the text API works</h1></div></div></div><p>The <a id="id490" class="indexterm"/>text API implements <a id="id491" class="indexterm"/>the algorithm proposed by Lukás Neumann and Jiri Matas in the article called <span class="emphasis"><em>Real-Time Scene Text Localization and Recognition</em></span> during the <a id="id492" class="indexterm"/>
<span class="strong"><strong>CVPR</strong></span> (<span class="strong"><strong>Computer Vision and Pattern Recognition)</strong></span> Conference in 2012. This algorithm represented a significant increase in scene text detection, performing the state-of-the art detection both in the CVPR database as well as in the Google Street View database.</p><p>Before we use the API, let's take a look at how this algorithm works under the hood, and how it addresses the scene text detection problem.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>
<span class="strong"><strong>Remember</strong></span> that the OpenCV 3.0 text API does not come with the standard OpenCV modules. It's an additional module present in the OpenCV contribute package. If you need to install OpenCV using the Windows Installer, refer to<a class="link" href="ch01.html" title="Chapter 1. Getting Started with OpenCV"> Chapter 1</a>, <span class="emphasis"><em>Getting Started with OpenCV</em></span>, which will help you install these modules.</p></div></div><div class="section" title="The scene detection problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec58"/>The scene detection problem</h2></div></div></div><p>Detecting<a id="id493" class="indexterm"/> text that randomly appears in a scene is a problem harder than it looks. There are several new variables when we compare them to identify scanned text, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Tri-dimensionality</strong></span>: The <a id="id494" class="indexterm"/>text can be in any scale, orientation, or perspective. Also, the text can be partially occluded or interrupted. There are literally thousands of possible regions where it can appear in the image.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Variety</strong></span>: Text can be in <a id="id495" class="indexterm"/>several different fonts and colors. The font can have outline borders or not. The background can be a dark, light, or a complex image.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Illumination and shadows</strong></span>: The<a id="id496" class="indexterm"/> sunlight position and apparent color changes over the time. Different weather conditions such as fog or rain can generate noise. Illumination can be a problem even in closed spaces, since light reflects over colored objects and hits the text.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Blurring</strong></span>: Text can <a id="id497" class="indexterm"/>appear in a region that is not prioritized by the auto focus lenses. Blurring is also common in moving cameras, in perspective text, or in the presence of fog.</li></ul></div><p>The following image, taken from Google Street View, illustrates these problems. Notice how several of these situations occur simultaneously in just a single image:</p><div class="mediaobject"><img src="graphics/B04283_11_01.jpg" alt="The scene detection problem"/></div><p>Performing a text detection to deal with such situations may prove computationally expensive, since <a id="id498" class="indexterm"/>there are <span class="emphasis"><em>2n</em></span> subsets of pixels where the text can be, <span class="emphasis"><em>n</em></span> being the number of pixels in the image.</p><p>In order to reduce the complexity, two strategies are commonly applied, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Use a sliding window to search a subset of image rectangles. This strategy just reduces the number of subsets to a smaller amount. The amount of regions varies according to the complexity of text being considered. Algorithms that deal just with text rotation can use small values, as compared to the ones that also deal with rotation, skewing, perspective, and so on. The advantage of this approach is its simplicity, but it is usually limited to a narrow range of fonts, and often, to a lexicon of specific words.</li><li class="listitem" style="list-style-type: disc">Use of the connected component analysis. This approach assumes that pixels can be grouped into regions where pixels have similar properties. These regions are supposed to have higher chances of being identified as characters. The advantage of this approach is that it does not depend on several text properties (orientation, scale, and fonts), and they also provide a segmentation region that can be used to crop text to the OCR. This was the approach used in the previous chapter.</li><li class="listitem" style="list-style-type: disc">The OpenCV 3.0 algorithm uses the second one by performing the connected component analysis and searching for extremal regions.</li></ul></div></div><div class="section" title="Extremal regions"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec59"/>Extremal regions</h2></div></div></div><p>Extremal regions <a id="id499" class="indexterm"/>are connected areas that are characterized by <a id="id500" class="indexterm"/>uniform intensity and surrounded by a contrast background. The stability of a region can be measured by calculating how resistant the region is to the thresholding variance. This variance can be measured with a simple algorithm:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Applying the threshold generates an image A. Detect its connected pixel regions (extremal regions).</li><li class="listitem">Increasing the threshold by a delta amount generates an image B. Detect its connected pixel regions (extremal regions).</li><li class="listitem">Compare image B with A. If a region in image A is similar to the same region in image B, then add it to the same branch in the tree. The criteria of similarity may vary from implementation to implementation, but it's usually related to the image area or general shape. If a region in image A appears to be split in image B, create two new branches in the tree for the new regions, and associate them with the previous branch.</li><li class="listitem">Set <span class="emphasis"><em>A = B</em></span> and go back to step 2, until a maximum threshold is applied.<p>This will assemble a tree of regions, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B04283_11_02.jpg" alt="Extremal regions"/><div class="caption"><p>Image source:  <a class="ulink" href="http://docs.opencv.org/master/da/d56/group__text__detect.html#gsc.tab=0">http://docs.opencv.org/master/da/d56/group__text__detect.html#gsc.tab=0</a>
</p></div></div></li></ol></div><p>The resistance to variance is determined by counting the number of nodes that are in the same level.</p><p>By analyzing this tree, it's also possible to determine the <a id="id501" class="indexterm"/>
<span class="strong"><strong>MSERs</strong></span> (<span class="strong"><strong>Maximally Stable Extremal Regions</strong></span>), that is, the regions where the area remains stable in a wide variety of thresholds. In the previous image, it is clear that these areas will contain the letters <span class="emphasis"><em>O</em></span>, <span class="emphasis"><em>N</em></span>, and <span class="emphasis"><em>Y</em></span>. The main disadvantage of MSERs is that they are weak in the presence of blur. OpenCV provides a MSER feature detector in the <code class="literal">feature2d</code> module.</p><p>Extremal regions <a id="id502" class="indexterm"/>are interesting because they are strongly invariant to <a id="id503" class="indexterm"/>illumination, scale, and orientation. They are also good candidates for text because they are also invariant of the type of font used, even when the font is styled. Each region can also be analyzed in order to determine its boundary ellipsis and have properties, such as affine transformation and area that can be numerically determined. Finally, it's worth mentioning that this entire process is fast, which makes it a very good candidate for real-time applications.</p></div><div class="section" title="Extremal region filtering"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec60"/>Extremal region filtering</h2></div></div></div><p>Although MSERs are a <a id="id504" class="indexterm"/>common approach to define which extremal regions <a id="id505" class="indexterm"/>are worth working with, the Neumann and Matas algorithm uses a different approach by submitting all extremal regions to a sequential classifier that is trained for character detection. This classifier works in the following two different stages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first stage incrementally computes descriptors (the bounding box, perimeter, area, and Euler number) for each region. These descriptors are submitted to a classifier that estimates how probable the region is for it to be a character in the alphabet. Then, only the regions of high probability are selected to stage 2.</li><li class="listitem" style="list-style-type: disc">In this stage, the features of the whole area ratio, convex hull ratio, and the number of outer boundary inflexion points are calculated. This provides a more detailed information that allows the classifier to discard <code class="literal">nontext</code> characters, but they are also much slower to calculate.</li></ul></div><p>In OpenCV, this process is implemented in an <code class="literal">ERFilter</code> class. It is also possible to use different image single-channel projections such as R, G, B, luminance, or grayscale conversion to increase the character recognition rates.</p><p>Finally, all the characters must be grouped in text blocks (such as words or paragraphs). OpenCV 3.0 provides two algorithms for this purpose:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Prune Exhaustive Search</strong></span>: This <a id="id506" class="indexterm"/>was also proposed by Mattas in 2011. This algorithm does not need any previous training or classification, but is limited to a horizontally aligned text.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hierarchical Method for Oriented Text</strong></span>: This deals with texts in any orientation, but<a id="id507" class="indexterm"/> needs a trained classifier.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>Since these operations require classifiers, it is also necessary to provide a trained set as an input. OpenCV3.0 provides some of these trained sets in the <code class="literal">sample</code> package. This also means that this algorithm is sensitive to the fonts used in classifier training.</p></div></div><p>A demonstration of this <a id="id508" class="indexterm"/>algorithm can be seen in the video provided by Neumann at <a class="ulink" href="https://youtu.be/ejd5gGea2Fo">https://youtu.be/ejd5gGea2Fo</a>.</p><p>Once the text is segmented, it just needs to be sent to an OCR, such as Tesseract, similar to what we did in the previous chapter. The only difference is that now we will use OpenCV text module classes to interact with Tesseract, since they provide a way to encapsulate the specific OCR engine we are using.</p></div></div></div>
<div class="section" title="Using the text API"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec75"/>Using the text API</h1></div></div></div><p>Enough of theory. It's <a id="id509" class="indexterm"/>time to see how the text module works in practice. Let's study how to use it to perform text detection, extraction, and identification.</p><div class="section" title="Text detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec61"/>Text detection</h2></div></div></div><p>Let's start <a id="id510" class="indexterm"/>with creating a simple program to perform text segmentation using <code class="literal">ERFilters</code>. In this program, we will use the trained classifiers from text API samples. You can download them from the OpenCV repository, but they are also available in the book's companion code.</p><p>First, we start with including all the necessary <code class="literal">libs</code> and using:</p><div class="informalexample"><pre class="programlisting">#include  "opencv2/highgui.hpp"
#include  "opencv2/imgproc.hpp"
#include  "opencv2/text.hpp"

#include  &lt;vector&gt;
#include  &lt;iostream&gt;

using namespace std;
using namespace cv;
using namespace cv::text;</pre></div><p>Recall from our previous section that the <code class="literal">ERFilter</code> works separately in each image channel. So, we must<a id="id511" class="indexterm"/> provide a way to separate each desired channel in a different single <code class="literal">cv::Mat</code> channel. This is done by the <code class="literal">separateChannels</code> function:</p><div class="informalexample"><pre class="programlisting">vector&lt;Mat&gt; separateChannels(Mat&amp; src)
{
  vector&lt;Mat&gt; channels;
  //Grayscale images
  if (src.type() == CV_8U || src.type() == CV_8UC1) {
    channels.push_back(src);
    channels.push_back(255-src);
    return channels;
  }

  //Colored images
  if (src.type() == CV_8UC3) {
    computeNMChannels(src, channels);
    int size = static_cast&lt;int&gt;(channels.size())-1;
    for (int c = 0; c &lt; size; c++)
      channels.push_back(255-channels[c]);
    return channels;
  }

  //Other types
  cout &lt;&lt; "Invalid image format!" &lt;&lt; endl;
  exit(-1);
}</pre></div><p>First, we verify that the image is a single channel image (for example, a grayscale image). If that's the case, we just add this image and its negative to be processed.</p><p>Otherwise, we check whether it's an RGB image. For colored images, we call the <code class="literal">computeNMChannels</code> to split the image in its several channels. The function is defined as follows:</p><div class="informalexample"><pre class="programlisting">void computeNMChannels(InputArray src, OutputArrayOfArrays channels, int mode = ERFILTER_NM_RGBLGrad);</pre></div><p>Its parameters are described as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">src</code>: This is the source input array. It should be a colored image of type 8UC3.</li><li class="listitem" style="list-style-type: disc"><code class="literal">channels</code>: This is a vector of mats that will be filled with the resulting channels.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mode</code>: This defines the channels that will be computed. There are two possible values that can be used, which are as follows:</li><li class="listitem" style="list-style-type: disc"><code class="literal">ERFILTER_NM_RGBLGrad</code>: This indicates that the algorithm uses an RGB color, lightness, and gradient magnitude as channels (default).</li><li class="listitem" style="list-style-type: disc"><code class="literal">ERFILTER_NM_IHSGrad</code>: This indicates that the image will be split by its intensity, hue, saturation, and gradient magnitude.</li></ul></div><p>We also <a id="id512" class="indexterm"/>append the negatives of all color components in the vector. Finally, if another kind of image is provided, the function will terminate the program with an error message.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>Negatives are appended so the algorithms will cover both bright text on a dark background and dark text on a bright background. There is no point in adding a negative to the gradient magnitude.</p></div></div><p>Let's proceed to the <code class="literal">main</code> method. We'll use the program to segment the <code class="literal">easel.png</code> image, which is provided with the source code:</p><div class="mediaobject"><img src="graphics/B04283_11_03.jpg" alt="Text detection"/></div><p>This image was taken by a mobile phone camera, while I was walking on the street. Let's code so that you <a id="id513" class="indexterm"/>can also use a different image easily by providing its name in the first program argument:</p><div class="informalexample"><pre class="programlisting">int main(int argc, const char * argv[])
{
  char* image = argc &lt; 2 ? "easel.png" : argv[1];
   auto input = imread(image);</pre></div><p>Next, we'll convert the image to grayscale and separate its channels by calling the <code class="literal">separateChannels</code> function:</p><div class="informalexample"><pre class="programlisting">  Mat processed;
  cvtColor(input, processed, CV_RGB2GRAY);
 auto channels = separateChannels(processed);</pre></div><p>If you want to work with all the channels in a colored image, just replace the first two lines of the preceding code with the following code:</p><div class="informalexample"><pre class="programlisting">Mat processed = input;</pre></div><p>We will need to analyze six channels (RGB + inverted) instead of two (gray + inverted). Actually the processing time will increase much more than the improvements that we can <code class="literal">get.With</code> the channels in hand, we need to create <code class="literal">ERFilters</code> for both the stages of the algorithm. Luckily, the <code class="literal">opencv text</code> contribution module provides functions for this:</p><div class="informalexample"><pre class="programlisting">// Create ERFilter objects with the 1st and 2nd stage classifiers
auto filter1 = createERFilterNM1(loadClassifierNM1("trained_classifierNM1.xml"),  15, 0.00015f, 0.13f, 0.2f,true,0.1f);

auto filter2 = createERFilterNM2( loadClassifierNM2("trained_classifierNM2.xml"),0.5);</pre></div><p>For the first stage, we call the <code class="literal">loadClassifierNM1</code> function to load a previously trained classification model. The XML containing the training data is its only argument. Then, we call <code class="literal">createERFilterNM1</code> to create an instance of the <code class="literal">ERFilter</code> class that will perform the classification. The function has the following signature:</p><div class="informalexample"><pre class="programlisting">Ptr&lt;ERFilter&gt; createERFilterNM1(const Ptr&lt;ERFilter::Callback&gt;&amp; cb,
    int thresholdDelta = 1, 
    float minArea = 0.00025, float maxArea = 0.13, 
    float minProbability = 0.4, bool nonMaxSuppression = true, float minProbabilityDiff = 0.1);</pre></div><p>The parameters are described as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">cb</code>: This is the classification model. This is the same model that we loaded with the <code class="literal">loadCassifierNM1</code> function.</li><li class="listitem" style="list-style-type: disc"><code class="literal">thresholdDelta</code>: This is the amount to be added to the threshold in each algorithm iteration. The default value is <span class="emphasis"><em>1</em></span>, but we'll use <span class="emphasis"><em>15</em></span> in our example.</li><li class="listitem" style="list-style-type: disc"><code class="literal">minArea</code>: This is the minimum area of the ER where text can be found. This is measured in <span class="emphasis"><em>%</em></span> of the image size. ERs with areas smaller than this are immediately discarded.</li><li class="listitem" style="list-style-type: disc"><code class="literal">maxArea</code>: This is the maximum area of the ER where text can be found. This is also measured in <span class="emphasis"><em>%</em></span> of the image size. ERs with areas greater than this are immediately discarded.</li><li class="listitem" style="list-style-type: disc"><code class="literal">minProbability</code>: This is the minimum probability that a region must have to be a character in order to remain for the next stage.</li><li class="listitem" style="list-style-type: disc"><code class="literal">nonMaxSupression</code>: This indicates that non-maximum suppression will be done in each branch probability.</li><li class="listitem" style="list-style-type: disc"><code class="literal">minProbabilityDiff</code>: This is the minimum probability difference between the minimum and maximum extreme region.</li></ul></div><p>The process <a id="id514" class="indexterm"/>for the second stage is similar. We call <code class="literal">loadClassifierNM2</code> to load the classifier model for the second stage and <code class="literal">createERFilterNM2</code> to create the second stage classifier. This function only takes the loaded classification model and a minimum probability that a region must achieve to be considered a character as input parameters.</p><p>So, let's call these algorithms in each channel to identify all possible text regions:</p><div class="informalexample"><pre class="programlisting">//Extract text regions using Newmann &amp; Matas algorithm
cout &lt;&lt; "Processing " &lt;&lt; channels.size() &lt;&lt; " channels...";
cout &lt;&lt; endl;
vector&lt;vector&lt;ERStat&gt; &gt; regions(channels.size());
for (int c=0; c &lt; channels.size(); c++)
{
    cout &lt;&lt; "    Channel " &lt;&lt; (c+1) &lt;&lt; endl;
    filter1-&gt;run(channels[c], regions[c]);
    filter2-&gt;run(channels[c], regions[c]);
}    
filter1.release();
filter2.release();</pre></div><p>In the previous code, we used the <code class="literal">run</code> function of the <code class="literal">ERFilter</code> class. This function takes the following two arguments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The input channel</strong></span>: This <a id="id515" class="indexterm"/>is the image to be processed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The regions</strong></span>: In the first stage <a id="id516" class="indexterm"/>algorithm, this argument will be filled with the detected regions. In the second stage (performed by <code class="literal">filter2</code>), this argument must contain the regions selected in stage 1, which will be processed and filtered by stage 2.</li></ul></div><p>Finally, we <a id="id517" class="indexterm"/>release both the filters, since they will not be needed anymore in the program.</p><p>The final segmentation step is to group all <code class="literal">ERRegions</code> into possible words and define their bounding boxes. This is done by calling the <code class="literal">erGrouping</code> function:</p><div class="informalexample"><pre class="programlisting">//Separate character groups from regions
vector&lt; vector&lt;Vec2i&gt; &gt; groups;
vector&lt;Rect&gt; groupRects;
erGrouping(input, channels, regions, groups, groupRects, ERGROUPING_ORIENTATION_HORIZ);</pre></div><p>This function has the following signature:</p><div class="informalexample"><pre class="programlisting">void erGrouping(InputArray img, InputArrayOfArrays channels,
    std::vector&lt;std::vector&lt;ERStat&gt; &gt; &amp;regions,
    std::vector&lt;std::vector&lt;Vec2i&gt; &gt; &amp;groups,
    std::vector&lt;Rect&gt; &amp;groups_rects,
    int method = ERGROUPING_ORIENTATION_HORIZ,
    const std::string&amp; filename = std::string(),
    float minProbablity = 0.5);</pre></div><p>Let's take a look at the definition of each parameter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">img</code>: This is the original input image. You can refer to the following observations.</li><li class="listitem" style="list-style-type: disc"><code class="literal">regions</code>: This is a vector of single-channel images where regions are extracted.</li><li class="listitem" style="list-style-type: disc"><code class="literal">groups</code>: This is an output vector of indexes of grouped regions. Each group region contains all extremal regions of a single word.</li><li class="listitem" style="list-style-type: disc"><code class="literal">groupRects</code>: This is a list of rectangles with the detected text regions.</li><li class="listitem" style="list-style-type: disc"><code class="literal">method</code>: This is a method of grouping. It can be as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">ERGROUPING_ORIENTATION_HORIZ</code>: This is the default value. This only generates groups with horizontally oriented text by performing an exhaustive search, as proposed originally by Neumann and Matas.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ERGROUPING_ORIENTATION_ANY</code>: This generates groups with text in any orientation, using <a id="id518" class="indexterm"/><span class="strong"><strong>Single Linkage Clustering</strong></span> and <a id="id519" class="indexterm"/><span class="strong"><strong>classifiers</strong></span>. If you use this method, the filename of the classifier model must be provided in the next parameter.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Filename</code>: This is the name of the classifier model. It is only needed if <code class="literal">ERGROUPING_ORIENTATION_ANY</code> is selected.</li><li class="listitem" style="list-style-type: disc"><code class="literal">minProbability</code>: This is the minimum detected probability of accepting a group. Also, it is only needed if the <code class="literal">ERGROUPING_ORIENTATION_ANY</code> is used.</li></ul></div></li></ul></div><p>The code also <a id="id520" class="indexterm"/>provides a call to the second method, but it's commented. You can switch between the two to test it. Just comment the previous call and uncomment this one:</p><div class="informalexample"><pre class="programlisting">erGrouping(input, channels, regions, 
    groups, groupRects, ERGROUPING_ORIENTATION_ANY, 
    "trained_classifier_erGrouping.xml", 0.5);</pre></div><p>For this call, we also used the default trained classifier provided in the text module sample package.</p><p>Finally, we draw the region boxes and show the results:</p><div class="informalexample"><pre class="programlisting">// draw groups boxes
for (auto rect : groupRects)
    rectangle(input, rect, Scalar(0, 255, 0), 3);
imshow("grouping",input);
waitKey(0);</pre></div><p>The output of the program is shown in the following image:</p><div class="mediaobject"><img src="graphics/B04283_11_04.jpg" alt="Text detection"/></div><p>You can check the <a id="id521" class="indexterm"/>complete source code in the <code class="literal">detection.cpp</code> file.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>While most OpenCV text module functions are written to support both grayscale and colored images as their input parameters, by the time this book was written, there were bugs that prevented using grayscale images in functions, such as erGrouping; for instance. Refer to <a class="ulink" href="https://github.com/Itseez/opencv_contrib/issues/309">https://github.com/Itseez/opencv_contrib/issues/309</a>.</p><p>Always remember that the OpenCV contribute modules package is not as stable as the default <code class="literal">opencv</code> packages.</p></div></div></div><div class="section" title="Text extraction"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec62"/>Text extraction</h2></div></div></div><p>Now that we detected <a id="id522" class="indexterm"/>the regions, we must crop the text before we submit it to the OCR. We can simply use a function such as <code class="literal">getRectSubpix</code> or <code class="literal">Mat::copy</code> using each region rectangle as a <a id="id523" class="indexterm"/>
<span class="strong"><strong>ROI</strong></span> (<span class="strong"><strong>region of interest</strong></span>), but since the letters are skewed, some undesired text may be cropped as well; for instance, this is what one of the regions will look like if we just extract the ROI based in its given rectangle:</p><div class="mediaobject"><img src="graphics/B04283_11_05.jpg" alt="Text extraction"/></div><p>Fortunately, the <code class="literal">ERFilter</code> provides us with an object called <code class="literal">ERStat</code>, which contains pixels inside <a id="id524" class="indexterm"/>each extremal region. With these pixels, we can use the OpenCV <code class="literal">floodFill</code> function to reconstruct each letter. This function is capable of painting similar colored pixels based in a seed point, just like the <code class="literal">bucket</code> tool of most drawing applications. This is what the function signature looks like:</p><div class="informalexample"><pre class="programlisting">int floodFill(InputOutputArray image, InputOutputArray mask,
    Point seedPoint, Scalar newVal, CV_OUT Rect* rect=0,
    Scalar loDiff = Scalar(), Scalar upDiff = Scalar(),
    int flags = 4
);</pre></div><p>Let's understand these parameters and see how they can be used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">image</code>: This is<a id="id525" class="indexterm"/> the input image. We'll use the channel image where the extremal region was taken. This is where the function normally does the flood fill, unless the <code class="literal">FLOODFILL_MASK_ONLY</code> is supplied. In this case, the image remains untouched and the drawing occurs in the mask. That's exactly what we will do.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mask</code>: The <a id="id526" class="indexterm"/>mask must be an image two rows and columns greater than the input image. When flood fill draws a pixel, it verifies that the corresponding pixel in the mask is zero. In that case, it will draw and mark this pixel as one (or the other value passed in the flags). If the pixel is not zero, flood fill does not paint the pixel. In our case, we'll provide a blank mask, so every letter will get painted in the mask.</li><li class="listitem" style="list-style-type: disc"><code class="literal">seedPoint</code>: This is<a id="id527" class="indexterm"/> the starting point. It's similar to the place where you click when you want to use the "bucket" tool of a graphic application.</li><li class="listitem" style="list-style-type: disc"><code class="literal">newVal</code>: This is<a id="id528" class="indexterm"/> the new value of the repainted pixels.</li><li class="listitem" style="list-style-type: disc"><code class="literal">loDiff and upDiff</code>: These <a id="id529" class="indexterm"/>parameters represent the lower and upper <a id="id530" class="indexterm"/>difference between the pixels being processed and their neighbors. The neighbor will be painted if it falls in this range. If the <code class="literal">FLOODFILL_FIXED_RANGE</code> flag is used, the difference between the seed point and the pixels being processed will be used instead.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rect</code>: This is <a id="id531" class="indexterm"/>the optional parameter that limits the region where the flood fill will be applied.</li><li class="listitem" style="list-style-type: disc"><code class="literal">flags</code>: This <a id="id532" class="indexterm"/>value is represented by a bit mask.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The least significant eight bits of the flag contain a connectivity value. A value of 4 indicates that all the four edge pixels will be used, and a value of 8 will indicates that diagonal pixels must also be taken into account. We'll use four for this parameter.</li><li class="listitem" style="list-style-type: disc">The next 8 to 16 bits contain a value from 1 to 255 and are used to fill the mask. Since we want to fill the mask with white, we'll use 255 &lt;&lt; 8 for this value.</li><li class="listitem" style="list-style-type: disc">There are two more bits that can be set by adding the <code class="literal">FLOODFILL_FIXED_RANGE</code> and <code class="literal">FLOODFILL_MASK_ONLY</code> flags, as described earlier.</li></ul></div></li></ul></div><p>We'll create a function called<a id="id533" class="indexterm"/> <code class="literal">drawER</code>. This function will receive four parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A vector with all processed channels</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ERStat</code> regions</li><li class="listitem" style="list-style-type: disc">The group that must be drawn</li><li class="listitem" style="list-style-type: disc">The group rectangle</li></ul></div><p>This function <a id="id534" class="indexterm"/>will return an image with the word represented by this group. Let's start with this function by creating the mask image and defining the flags:</p><div class="informalexample"><pre class="programlisting">Mat out = Mat::zeros(channels[0].rows+2, channels[0].cols+2, CV_8UC1);
int flags = 4                      //4 neighbors
  + (255 &lt;&lt; 8)                    //paint mask in white (255)
  + FLOODFILL_FIXED_RANGE         //fixed range
  + FLOODFILL_MASK_ONLY;          //Paint just the mask</pre></div><p>Then, we'll loop though each group. It's necessary to find the region index and its stats. There's a chance that this extreme region will be the root, which does not contain any points. In this case, we'll just ignore it:</p><div class="informalexample"><pre class="programlisting">for (int g=0; g &lt; group.size(); g++)
{
int idx = group[g][0];
   ERStat er = regions[idx][group[g][1]];
//Ignore root region
   if (er.parent == NULL) 
    continue;</pre></div><p>Now, we <a id="id535" class="indexterm"/>can read the pixel coordinate from the <code class="literal">ERStat</code> object. It's represented by the pixel number, counting from top to bottom, left to right. This linear index must be converted to a <span class="emphasis"><em>row (y)</em></span> and <span class="emphasis"><em>column (z)</em></span> notation, using a formula similar to the one that we discussed in <a class="link" href="ch02.html" title="Chapter 2. An Introduction to the Basics of OpenCV">Chapter 2</a>, <span class="emphasis"><em>An Introduction to the Basics of OpenCV</em></span>:</p><div class="informalexample"><pre class="programlisting">int px = er.pixel % channels[idx].cols;
int py = er.pixel / channels[idx].cols;
Point p(px, py);</pre></div><p>Then, we can call the <code class="literal">floodFill</code> function. The <code class="literal">ERStat</code> object gives us the value that we need to use in the <code class="literal">loDiff</code> parameter:</p><div class="informalexample"><pre class="programlisting">floodFill(
    channels[idx], out,          //Image and mask
    p, Scalar(255),              //Seed and color
    nullptr,                       /No rect
    Scalar(er.level),Scalar(0),    //LoDiff and upDiff
    flags                          //Flags</pre></div><p>After we do this for all regions in the group, we'll end it with an image a little bigger than the original one, with a black background and the word in white letters. Now, let's crop just the area of the letters. Since the region rectangle was given, we start with defining it as our region of interest:</p><div class="informalexample"><pre class="programlisting">out = out(rect);</pre></div><p>Then, we'll find all nonzero pixels. This is the value that we'll use in the <code class="literal">minAreaRect</code> function to get the rotated rectangle around the letters. Finally, we borrow the previous chapter's <code class="literal">deskewAndCrop</code> function to crop and rotate the image for us:</p><div class="informalexample"><pre class="programlisting">  vector&lt;Point&gt; points;
  findNonZero(out, points);
  //Use deskew and crop to crop it perfectly
  return deskewAndCrop(out, minAreaRect(points));
}</pre></div><p>This is the<a id="id536" class="indexterm"/> result of the process for the easel image:</p><div class="mediaobject"><img src="graphics/B04283_11_06.jpg" alt="Text extraction"/></div></div><div class="section" title="Text recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec63"/>Text recognition</h2></div></div></div><p>In <a class="link" href="ch10.html" title="Chapter 10. Developing Segmentation Algorithms for Text Recognition">Chapter 10</a>, <span class="emphasis"><em>Developing Segmentation Algorithms for Text Recognition</em></span>, we used the Tesseract API <a id="id537" class="indexterm"/>directly to recognize the text regions. This time, we'll use OpenCV classes to accomplish the same goal.</p><p>In OpenCV, all OCR-specific classes are derived from the <code class="literal">BaseOCR</code> virtual class. This class provides a common interface for the OCR execution method itself.</p><p>Specific implementations must inherit from this class. By default, the text module provides three different implementations: <code class="literal">OCRTesseract</code>, <code class="literal">OCRHMMDecoder</code>, and <code class="literal">OCRBeamSearchDecoder</code>.</p><p>This hierarchy is depicted in the following class diagram:</p><div class="mediaobject"><img src="graphics/B04283_11_07.jpg" alt="Text recognition"/></div><p>With this approach, we can separate the part of the code where the OCR mechanism is created from the execution itself. This makes it easier to change the OCR implementation in the future.</p><p>So, let's start <a id="id538" class="indexterm"/>with creating a method that decides which implementation we'll use based on a string. We will currently support Tesseract. However, you can take a look at the chapter code where a demonstration with <code class="literal">HMMDecoder</code> is also provided. We are also accepting the OCR engine name in a string parameter, but we can improve our application flexibility by reading it from an external <code class="literal">JSON</code> or <code class="literal">XML</code> configuration file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cv::Ptr&lt;BaseOCR&gt; initOCR2(const string&amp; ocr) </strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  if (ocr == "tesseract") {</strong></span>
<span class="strong"><strong>    return OCRTesseract::create(nullptr, "eng+por");</strong></span>
  }
  throw string("Invalid OCR engine: ") + ocr;
}</pre></div><p>You may notice that the function returns a <code class="literal">Ptr&lt;BaseOCR&gt;</code>. Now, take a look at the highlighted code. It calls the create method to initialize a Tesseract OCR instance. Let's take a look at its official signature, since it allows several specific parameters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Ptr&lt;OCRTesseract&gt; create(const char* datapath=NULL, const char* language=NULL, const char* char_whitelist=NULL, </strong></span>int oem=3, int psmode=3);</pre></div><p>Let's dissect each one of these parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">datapath</code>: This is<a id="id539" class="indexterm"/> the path to the <code class="literal">tessdata</code> files of the root directory. The path must end with a backslash <code class="literal">/</code> character. The <code class="literal">tessdata</code> directory contains the language files you installed. Passing <code class="literal">nullptr</code> to this parameter will make Tesseract search in its installation directory, which is the location where this folder is normally present. It's common to change this value to <code class="literal">args[0]</code> when deploying an application and include the <code class="literal">tessdata</code> folder in your application path.</li><li class="listitem" style="list-style-type: disc"><code class="literal">language</code>: This <a id="id540" class="indexterm"/>is a <a id="id541" class="indexterm"/>three letter word with the language code (for example, eng for English, por for Portuguese, or hin for Hindi). Tesseract supports loading of multiple language codes using the + sign. So, passing <span class="emphasis"><em>eng+por</em></span> will load both English and Portuguese languages. Of course, you can only use languages that you previously installed; otherwise, the loading will fail. A language configuration file can specify that two or more languages must be loaded together. To prevent this, you can use a tilde ~. For example, you can use <span class="emphasis"><em>hin+~eng</em></span> to guarantee that English is not loaded with Hindi, even if it is configured to do so.</li><li class="listitem" style="list-style-type: disc"><code class="literal">whitelist</code>: This is<a id="id542" class="indexterm"/> the character set to be considered for recognition. If <code class="literal">nullptr</code> is passed, the characters will be <code class="literal">0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">oem</code>: These <a id="id543" class="indexterm"/>are OCR algorithms that will be used. They can have one of the following values:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">OEM_TESSERACT_ONLY</code>: This uses just Tesseract. It's the fastest method, but it also has less precision.</li><li class="listitem" style="list-style-type: disc"><code class="literal">OEM_CUBE_ONLY</code>: This uses the cube engine. It's slower, but more precise. This will only work if your language was trained to support this engine mode. To check whether that's the case, look for .<code class="literal">cube</code> files for your language in the <code class="literal">tessdata</code> folder. The support for English language is guaranteed.</li><li class="listitem" style="list-style-type: disc"><code class="literal">OEM_TESSERACT_CUBE_COMBINED</code>: This combines both Tesseract and cube to achieve the best possible OCR classification. This engine has the best accuracy and the slowest execution time.</li><li class="listitem" style="list-style-type: disc"><code class="literal">OEM_DEFAULT</code>: This tries to infer the strategy based in the language <code class="literal">config</code> file, command line <code class="literal">config</code> file, or in the absence of both, uses <code class="literal">OEM_TESSERACT_ONLY</code>.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">psmode</code>: This is<a id="id544" class="indexterm"/> the segmentation mode. The modes are as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_OSD_ONLY</code>: Using this mode, Tesseract will just run its preprocessing algorithms to detect orientation and script detection.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_AUTO_OSD</code>: This tells Tesseract to do automatic page segmentation with orientation and script detection.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_AUTO_ONLY</code>: This does page segmentation, but avoids doing orientation, script detection, or OCR. This is the default value.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_AUTO</code>: This does page segmentation and OCR, but avoids doing orientation or script detection.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_COLUMN</code>: This assumes that the text of variable sizes is displayed in a single column.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_BLOCK_VERT_TEXT</code>: This treats the image as a single uniform block of a vertically aligned text.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_BLOCK</code>: This is a single block of text. This is the default configuration. We will use this flag since our preprocessing phase guarantees this condition.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_LINE</code>: This indicates that the image contains only one line of text.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_WORD</code>: This indicates that the image contains just one word.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_WORD_CIRCLE</code>: This informs that the image is a just one word disposed in a circle.</li><li class="listitem" style="list-style-type: disc"><code class="literal">PSM_SINGLE_CHAR</code>: This indicates that the image contains a single character.</li></ul></div></li></ul></div><p>For the last two <a id="id545" class="indexterm"/>parameters, the <code class="literal">#include tesseract</code> directory recommends you to use the constant names instead of directly inserting their values.</p><p>The last step is to add text detection to our <code class="literal">main</code> function. To do this, just add the following code to the end of the <code class="literal">main</code> method:</p><div class="informalexample"><pre class="programlisting">auto ocr = initOCR("tesseract");
for (int i = 0; i &lt; groups.size(); i++)
{
  Mat wordImage = drawER(channels, regions, groups[i], groupRects[i]);
  string word;
  ocr-&gt;run(wordImage, word);
  cout &lt;&lt; word &lt;&lt; endl;
}</pre></div><p>In this code, we started by calling our <code class="literal">initOCR</code> method to create a Tesseract instance. Notice that the remaining code will not change if we choose a different OCR engine, since the <a id="id546" class="indexterm"/>run method signature is guaranteed by the <code class="literal">BaseOCR</code> class.</p><p>Next, we iterate over each detected <code class="literal">ERFilter</code> group. Since each group represents a different word, we:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Call the previously created <code class="literal">drawER</code> function to create an image with the word.</li><li class="listitem" style="list-style-type: disc">Create a text string called <code class="literal">word</code>, and call the <code class="literal">run</code> function to recognize the word image. The recognized word will be stored in the string.</li><li class="listitem" style="list-style-type: disc">Print the text string on the screen.</li></ul></div><p>Let's take a look at the run method signature. This method is defined in the <code class="literal">BaseOCR</code> class and will be equal for all specific OCR implementations, even the ones that might be implemented in the future:</p><div class="informalexample"><pre class="programlisting">virtual void run(Mat&amp; image, std::string&amp; output_text, std::vector&lt;Rect&gt;* component_rects=NULL, std::vector&lt;std::string&gt;* component_texts=NULL, std::vector&lt;float&gt;* component_confidences=NULL,
     int component_level=0) = 0;</pre></div><p>Of course, this is a pure virtual function that must be implemented by each specific class (such as the <code class="literal">OCRTesseract</code> class that we just used):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">image</code>: This is the input image. It must be an RGB or a grayscale image</li><li class="listitem" style="list-style-type: disc"><code class="literal">component_rects</code>: We can provide a vector to be filled with the bounding box of each component (words or text lines) detected by the OCR engine</li><li class="listitem" style="list-style-type: disc"><code class="literal">component_texts</code>: If given, this vector will be filled with the text strings of each component detected by the OCR</li><li class="listitem" style="list-style-type: disc"><code class="literal">component_confidences</code>: If given, the vector will be filled with floats and the confidence values of each component</li><li class="listitem" style="list-style-type: disc"><code class="literal">component_level</code>: This defines what a component is. It may have the <code class="literal">OCR_LEVEL_WORD</code> (by default) or <code class="literal">OCR_LEVEL_TEXT_LINE</code> values</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>If necessary, we prefer changing the component level to a word or line in the <code class="literal">run()</code> method instead of doing the same thing in the <code class="literal">psmode</code> parameter of the <code class="literal">create()</code> function. This is preferable since the <code class="literal">run</code> method will be supported by any OCR engine that decides to implement the <code class="literal">BaseOCR</code> class. Always remember that the <code class="literal">create()</code> method is where vendor specific configurations are set.</p></div></div><p>This is the program's final output:</p><div class="mediaobject"><img src="graphics/B04283_11_08.jpg" alt="Text recognition"/></div><p>Despite a <a id="id547" class="indexterm"/>minor confusion with the <code class="literal">&amp;</code> symbol, every word was perfectly recognized! You can check the complete source code in the <code class="literal">ocr.cpp</code> file, in the chapter code.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec76"/>Summary</h1></div></div></div><p>In this chapter, we saw that scene text recognition is a far more difficult OCR situation than working with scanned texts. We studied how the text module addresses this problem with extremal region identification using the <span class="strong"><strong>Newmann and Matas</strong></span> algorithm. We also saw how to use this API with the <code class="literal">floodfill</code> function to extract the text to an image and submit it to Tesseract OCR. Finally, we studied how the OpenCV text module integrates with Tesseract and other OCR engines, and how we can use its classes to identify what's written in the image.</p><p>This ends our journey with OpenCV. From the beginning to the end of this book, we expected you to have a glance about the Computer Vision area and have a better understanding of how several applications work. We also sought to show you that, although OpenCV is quite an impressive library, the field is already full of opportunities for improvement and research.</p><p>Thank you for reading! No matter whether you use OpenCV for creating impressive commercial programs based on Computer Vision, or if you use it in a research that will change the world, we hope you will find this content useful. Just keep working with your skills—this was just the beginning!</p></div></body></html>