["```py\ngenre_top, and counted the number of records for each genre:\n```", "```py\nvar genreCount = featuresDF.AggregateRowsBy<string, int>(\n    new string[] { \"genre_top\" },\n    new string[] { \"track_id\" },\n    x => x.ValueCount\n).SortRows(\"track_id\");\n\ngenreCount.Print();\n\nvar barChart = DataBarBox.Show(\n    genreCount.GetColumn<string>(\"genre_top\").Values.ToArray().Select(x => x.Substring(0,3)),\n    genreCount[\"track_id\"].Values.ToArray()\n).SetTitle(\n    \"Genre Count\"\n);\n```", "```py\nforeach (string col in featuresDF.ColumnKeys)\n{\n    if (col.StartsWith(\"mfcc\"))\n    {\n        int idx = int.Parse(col.Split('.')[2]);\n        if(idx <= 4)\n        {\n            Console.WriteLine(String.Format(\"\\n\\n-- {0} Distribution -- \", col));\n            double[] quantiles = Accord.Statistics.Measures.Quantiles(\n                featuresDF[col].ValuesAll.ToArray(),\n                new double[] { 0, 0.25, 0.5, 0.75, 1.0 }\n            );\n            Console.WriteLine(\n                \"Min: \\t\\t\\t{0:0.00}\\nQ1 (25% Percentile): \\t{1:0.00}\\nQ2 (Median): \\t\\t{2:0.00}\\nQ3 (75% Percentile): \\t{3:0.00}\\nMax: \\t\\t\\t{4:0.00}\",\n                quantiles[0], quantiles[1], quantiles[2], quantiles[3], quantiles[4]\n            );\n        }\n    }\n}\n```", "```py\nstring[] attributes = new string[] { \"kurtosis\", \"min\", \"max\", \"mean\", \"median\", \"skew\", \"std\" };\nforeach (string attribute in attributes)\n{\n    string[] featureColumns = featuresDF.ColumnKeys.Where(x => x.Contains(attribute)).ToArray();\n    foreach (string genre in genreCount.GetColumn<string>(\"genre_top\").Values)\n    {\n        var genreDF = featuresDF.Rows[\n            featuresDF.GetColumn<string>(\"genre_top\").Where(x => x.Value == genre).Keys\n        ].Columns[featureColumns];\n\n        ScatterplotBox.Show(\n            BuildXYPairs(\n                genreDF.Columns[featureColumns].ToArray2D<double>(),\n                genreDF.RowCount,\n                genreDF.ColumnCount\n            )\n        ).SetTitle(String.Format(\"{0}-{1}\", genre, attribute));\n    }\n}\n```", "```py\nprivate static double[][] BuildXYPairs(double[,] ary2D, int rowCount, int columnCount)\n{\n    double[][] ary = new double[rowCount*columnCount][];\n    for (int i = 0; i < rowCount; i++)\n    {\n        for (int j = 0; j < columnCount; j++)\n        {\n            ary[i * columnCount + j] = new double[2];\n            ary[i * columnCount + j][0] = j + 1;\n            ary[i * columnCount + j][1] = ary2D[i, j];\n        }\n    }\n    return ary;\n}\n```", "```py\n// 1\\. Train a LogisticRegression Classifier\nConsole.WriteLine(\"\\n---- Logistic Regression Classifier ----\\n\");\nvar logitSplitSet = new SplitSetValidation<MultinomialLogisticRegression, double[]>()\n{\n    Learner = (s) => new MultinomialLogisticLearning<GradientDescent>()\n    {\n        MiniBatchSize = 500\n    },\n\n    Loss = (expected, actual, p) => new ZeroOneLoss(expected).Loss(actual),\n\n    Stratify = false,\n\n    TrainingSetProportion = 0.8,\n\n    ValidationSetProportion = 0.2,\n\n};\n\nvar logitResult = logitSplitSet.Learn(input, output);\n\nvar logitTrainedModel = logitResult.Model;\n\n// Store train & test set indexes to train other classifiers on the same train set\n// and test on the same validation set\nint[] trainSetIDX = logitSplitSet.IndicesTrainingSet;\nint[] testSetIDX = logitSplitSet.IndicesValidationSet;\n```", "```py\n// Get in-sample & out-of-sample predictions and prediction probabilities for each class\ndouble[][] trainProbabilities = new double[trainSetIDX.Length][];\nint[] logitTrainPreds = new int[trainSetIDX.Length];\nfor (int i = 0; i < trainSetIDX.Length; i++)\n{\n    logitTrainPreds[i] = logitTrainedModel.Decide(input[trainSetIDX[i]]);\n    trainProbabilities[i] = logitTrainedModel.Probabilities(input[trainSetIDX[i]]);\n}\n\ndouble[][] testProbabilities = new double[testSetIDX.Length][];\nint[] logitTestPreds = new int[testSetIDX.Length];\nfor (int i = 0; i < testSetIDX.Length; i++)\n{\n    logitTestPreds[i] = logitTrainedModel.Decide(input[testSetIDX[i]]);\n    testProbabilities[i] = logitTrainedModel.Probabilities(input[testSetIDX[i]]);\n}\n```", "```py\nIDictionary<string, int> targetVarCodes = new Dictionary<string, int>\n{\n    { \"Electronic\", 0 },\n    { \"Experimental\", 1 },\n    { \"Folk\", 2 },\n    { \"Hip-Hop\", 3 },\n    { \"Instrumental\", 4 },\n    { \"International\", 5 },\n    { \"Pop\", 6 },\n    { \"Rock\", 7 }\n};\nfeaturesDF.AddColumn(\"target\", featuresDF.GetColumn<string>(\"genre_top\").Select(x => targetVarCodes[x.Value]));\n```", "```py\n// 2\\. Train a Gaussian SVM Classifier\nConsole.WriteLine(\"\\n---- Gaussian SVM Classifier ----\\n\");\nvar teacher = new MulticlassSupportVectorLearning<Gaussian>()\n{\n    Learner = (param) => new SequentialMinimalOptimization<Gaussian>()\n    {\n        Epsilon = 2,\n        Tolerance = 1e-2,\n        Complexity = 1000,\n        UseKernelEstimation = true\n    }\n};\n// Train SVM model using the same train set that was used for Logistic Regression Classifier\nvar svmTrainedModel = teacher.Learn(\n    input.Where((x,i) => trainSetIDX.Contains(i)).ToArray(),\n    output.Where((x, i) => trainSetIDX.Contains(i)).ToArray()\n);\n```", "```py\n// Get in-sample & out-of-sample predictions and prediction probabilities for each class\ndouble[][] svmTrainProbabilities = new double[trainSetIDX.Length][];\nint[] svmTrainPreds = new int[trainSetIDX.Length];\nfor (int i = 0; i < trainSetIDX.Length; i++)\n{\n    svmTrainPreds[i] = svmTrainedModel.Decide(input[trainSetIDX[i]]);\n    svmTrainProbabilities[i] = svmTrainedModel.Probabilities(input[trainSetIDX[i]]);\n}\n\ndouble[][] svmTestProbabilities = new double[testSetIDX.Length][];\nint[] svmTestPreds = new int[testSetIDX.Length];\nfor (int i = 0; i < testSetIDX.Length; i++)\n{\n    svmTestPreds[i] = svmTrainedModel.Decide(input[testSetIDX[i]]);\n    svmTestProbabilities[i] = svmTrainedModel.Probabilities(input[testSetIDX[i]]);\n}\n```", "```py\n// 3\\. Train a NaiveBayes Classifier\nConsole.WriteLine(\"\\n---- NaiveBayes Classifier ----\\n\");\nvar nbTeacher = new NaiveBayesLearning<NormalDistribution>();\n\nvar nbTrainedModel = nbTeacher.Learn(\n    input.Where((x, i) => trainSetIDX.Contains(i)).ToArray(),\n    output.Where((x, i) => trainSetIDX.Contains(i)).ToArray()\n);\n```", "```py\n// Get in-sample & out-of-sample predictions and prediction probabilities for each class\ndouble[][] nbTrainProbabilities = new double[trainSetIDX.Length][];\nint[] nbTrainPreds = new int[trainSetIDX.Length];\nfor (int i = 0; i < trainSetIDX.Length; i++)\n{\n    nbTrainProbabilities[i] = nbTrainedModel.Probabilities(input[trainSetIDX[i]]);\n    nbTrainPreds[i] = nbTrainedModel.Decide(input[trainSetIDX[i]]);\n}\n\ndouble[][] nbTestProbabilities = new double[testSetIDX.Length][];\nint[] nbTestPreds = new int[testSetIDX.Length];\nfor (int i = 0; i < testSetIDX.Length; i++)\n{\n    nbTestProbabilities[i] = nbTrainedModel.Probabilities(input[testSetIDX[i]]);\n    nbTestPreds[i] = nbTrainedModel.Decide(input[testSetIDX[i]]);\n}\n```", "```py\n// 4\\. Ensembling Base Models\nConsole.WriteLine(\"\\n-- Building Meta Model --\");\ndouble[][] combinedTrainProbabilities = new double[trainSetIDX.Length][];\nfor (int i = 0; i < trainSetIDX.Length; i++)\n{\n    List<double> combined = trainProbabilities[i]\n        .Concat(svmTrainProbabilities[i])\n        .Concat(nbTrainProbabilities[i])\n        .ToList();\n    combined.Add(logitTrainPreds[i]);\n    combined.Add(svmTrainPreds[i]);\n    combined.Add(nbTrainPreds[i]);\n\n    combinedTrainProbabilities[i] = combined.ToArray();\n}\n\ndouble[][] combinedTestProbabilities = new double[testSetIDX.Length][];\nfor (int i = 0; i < testSetIDX.Length; i++)\n{\n    List<double> combined = testProbabilities[i]\n        .Concat(svmTestProbabilities[i])\n        .Concat(nbTestProbabilities[i])\n        .ToList();\n    combined.Add(logitTestPreds[i]);\n    combined.Add(svmTestPreds[i]);\n    combined.Add(nbTestPreds[i]);\n\n    combinedTestProbabilities[i] = combined.ToArray();\n}\nConsole.WriteLine(\"\\n* input shape: ({0}, {1})\\n\", combinedTestProbabilities.Length, combinedTestProbabilities[0].Length);\n```", "```py\n// Build meta-model using NaiveBayes Learning Algorithm\nvar metaModelTeacher = new NaiveBayesLearning<NormalDistribution>();\nvar metamodel = metaModelTeacher.Learn(\n    combinedTrainProbabilities, \n    output.Where((x, i) => trainSetIDX.Contains(i)).ToArray()\n);\n```", "```py\n// Get in-sample & out-of-sample predictions and prediction probabilities for each class\ndouble[][] metaTrainProbabilities = new double[trainSetIDX.Length][];\nint[] metamodelTrainPreds = new int[trainSetIDX.Length];\nfor (int i = 0; i < trainSetIDX.Length; i++)\n{\n    metaTrainProbabilities[i] = metamodel.Probabilities(combinedTrainProbabilities[i]);\n    metamodelTrainPreds[i] = metamodel.Decide(combinedTrainProbabilities[i]);\n}\n\ndouble[][] metaTestProbabilities = new double[testSetIDX.Length][];\nint[] metamodelTestPreds = new int[testSetIDX.Length];\nfor (int i = 0; i < testSetIDX.Length; i++)\n{\n    metaTestProbabilities[i] = metamodel.Probabilities(combinedTestProbabilities[i]);\n    metamodelTestPreds[i] = metamodel.Decide(combinedTestProbabilities[i]);\n}\n```", "```py\nConsole.WriteLine(String.Format(\"train accuracy: {0:0.0000}\", 1-logitResult.Training.Value));\nConsole.WriteLine(String.Format(\"validation accuracy: {0:0.0000}\", 1-logitResult.Validation.Value));\n```", "```py\nConsole.WriteLine(\n    String.Format(\n        \"train accuracy: {0:0.0000}\",\n        1 - new ZeroOneLoss(output.Where((x, i) => trainSetIDX.Contains(i)).ToArray()).Loss(nbTrainPreds)\n    )\n);\nConsole.WriteLine(\n    String.Format(\n        \"validation accuracy: {0:0.0000}\",\n        1 - new ZeroOneLoss(output.Where((x, i) => testSetIDX.Contains(i)).ToArray()).Loss(nbTestPreds)\n    )\n);\n```", "```py\n// Build confusion matrix\nstring[] confMatrix = BuildConfusionMatrix(\n    output.Where((x, i) => testSetIDX.Contains(i)).ToArray(), logitTestPreds, 8\n);\n\nSystem.IO.File.WriteAllLines(Path.Combine(dataDirPath, \"logit-conf-matrix.csv\"), confMatrix);\n```", "```py\nprivate static string[] BuildConfusionMatrix(int[] actual, int[] preds, int numClass)\n{\n    int[][] matrix = new int[numClass][];\n    for(int i = 0; i < numClass; i++)\n    {\n        matrix[i] = new int[numClass];\n    }\n\n    for(int i = 0; i < actual.Length; i++)\n    {\n        matrix[actual[i]][preds[i]] += 1;\n    }\n\n    string[] lines = new string[numClass];\n    for(int i = 0; i < matrix.Length; i++)\n    {\n        lines[i] = string.Join(\",\", matrix[i]);\n    }\n\n    return lines;\n}\n```", "```py\n// Calculate evaluation metrics\nint[][] logitTrainPredRanks = GetPredictionRanks(trainProbabilities);\nint[][] logitTestPredRanks = GetPredictionRanks(testProbabilities);\n\ndouble logitTrainMRRScore = ComputeMeanReciprocalRank(\n    logitTrainPredRanks,\n    output.Where((x, i) => trainSetIDX.Contains(i)).ToArray()\n);\ndouble logitTestMRRScore = ComputeMeanReciprocalRank(\n    logitTestPredRanks,\n    output.Where((x, i) => testSetIDX.Contains(i)).ToArray()\n);\n\nConsole.WriteLine(\"\\n---- Logistic Regression Classifier ----\\n\");\nConsole.WriteLine(String.Format(\"train MRR score: {0:0.0000}\", logitTrainMRRScore));\nConsole.WriteLine(String.Format(\"validation MRR score: {0:0.0000}\", logitTestMRRScore));\n```", "```py\nprivate static int[][] GetPredictionRanks(double[][] predProbabilities)\n{\n    int[][] rankOrdered = new int[predProbabilities.Length][];\n\n    for(int i = 0; i< predProbabilities.Length; i++)\n    {\n        rankOrdered[i] = Matrix.ArgSort<double>(predProbabilities[i]).Reversed();\n    }\n\n    return rankOrdered;\n}\n```", "```py\nprivate static double ComputeMeanReciprocalRank(int[][] rankOrderedPreds, int[] actualClasses)\n{\n    int num = rankOrderedPreds.Length;\n    double reciprocalSum = 0.0;\n\n    for(int i = 0; i < num; i++)\n    {\n        int predRank = 0;\n        for(int j = 0; j < rankOrderedPreds[i].Length; j++)\n        {\n            if(rankOrderedPreds[i][j] == actualClasses[i])\n            {\n                predRank = j + 1;\n            }\n        }\n        reciprocalSum += 1.0 / predRank;\n    }\n\n    return reciprocalSum / num;\n}\n```"]