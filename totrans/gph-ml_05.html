<html><head></head><body>
		<div id="_idContainer287">
			<h1 id="_idParaDest-44"><a id="_idTextAnchor046"/>Chapter 3: Unsupervised Graph Learning</h1>
			<p>Unsupervised machine learning refers to the subset of machine learning algorithms that do not exploit any target information during training. Instead, they work on their own to find clusters, discover patterns, detect anomalies, and solve many other problems for which there is no teacher and no correct answer known <em class="italic">a priori</em>.</p>
			<p>As per many other machine learning algorithms, unsupervised models have found large applications in the graph representation learning domain. Indeed, they represent an extremely useful tool for solving various downstream tasks, such as node classification and community detection, among others.</p>
			<p>In this chapter, an overview of recent unsupervised graph embedding methods will be provided. Given a graph, the goal of these techniques is to automatically learn a latent representation of it, in which the key structural components are somehow preserved.</p>
			<p><a id="_idTextAnchor047"/>The following topics will be covered in this chapter:</p>
			<ul>
				<li>The unsupervised graph embedding roadmap </li>
				<li>Shallow embedding methods </li>
				<li>Autoencoders</li>
				<li>Graph neural networks</li>
			</ul>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor048"/>Technical requirements</h1>
			<p>We will be using Jupyter notebooks with Python 3.9 for all of our exercises. The following is a list of the Python libraries that need to be installed for this chapter using <strong class="source-inline">pip</strong>. For example, run <strong class="source-inline">pip install networkx==2.5</strong> on the command line, and so on:</p>
			<p class="source-code">Jupyter==1.0.0</p>
			<p class="source-code">networkx==2.5</p>
			<p class="source-code">matplotlib==3.2.2</p>
			<p class="source-code">karateclub==1.0.19</p>
			<p class="source-code">node2vec==0.3.3</p>
			<p class="source-code">tensorflow==2.4.0</p>
			<p class="source-code">scikit-learn==0.24.0</p>
			<p class="source-code">git+https://github.com/palash1992/GEM.git</p>
			<p class="source-code">git+https://github.com/stellargraph/stellargraph.git</p>
			<p>In the rest of this book, if not clearly stated, we will refer to the Python commands <strong class="source-inline">import networkx</strong> as <strong class="source-inline">nx</strong>.</p>
			<p>All the code files relevant to this chapter are available at <a href="https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03">https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter03</a>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor049"/>The unsupervised graph embedding roadmap</h1>
			<p>Graphs are <a id="_idIndexMarker249"/>complex mathematical structures defined in a non-Euclidean space. Roughly speaking, this means that it is not always easy to define what is close to what; it might also be hard to say what <em class="italic">close</em> even means. Imagine a social network graph: two users can be respectively connected and yet share very different features—one might be interested in fashion and clothes, while the other might be interested in sports and videogames. Can we consider them as "close"?</p>
			<p>For this reason, unsupervised machine learning algorithms have found large applications in graph analysis. Unsupervised machine learning is the class of machine learning algorithms that can be trained without the need for manually annotated data. Most of those models indeed make use of only information in the adjacency matrix and the node features, without any knowledge of the downstream machine learning task.</p>
			<p>How is this possible? One of the most used solutions is to learn embeddings that preserve the graph structure. The learned representation is usually optimized so that it can be used to reconstruct the pair-wise node similarity, for example, the <strong class="bold">adjacency matrix</strong>. These<a id="_idIndexMarker250"/> techniques bring an important feature: the learned representation can encode latent relationships among nodes or graphs, allowing us to discover hidden and complex novel patterns. </p>
			<p>Many algorithms have been developed in relation to unsupervised graph machine learning techniques. However, as previously reported by different scientific papers (<a href="https://arxiv.org/abs/2005.03675">https://arxiv.org/abs/2005.03675</a>), those algorithms can be grouped into macro-groups: shallow <a id="_idIndexMarker251"/>embedding methods, autoencoders, and <strong class="bold">Graph Neural Networks</strong> (<strong class="bold">GNNs</strong>), as graphically<a id="_idIndexMarker252"/> described in the following chart:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B16069_03_01.jpg" alt="Figure 3.1 – The hierarchical structure of the different unsupervised embedding algorithms described in this book"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – The hierarchical structure of the different unsupervised embedding algorithms described in this book</p>
			<p>In the following sections, you will learn the main principles behind each group of algorithms. We will try to provide the idea behind the most well-known algorithms in the field as well as how they can be used for solving real problems.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor050"/>Shallow embedding methods</h1>
			<p>As already<a id="_idIndexMarker253"/> introduced in <a href="B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Graph Machine Learning</em>, with shallow embedding methods, we identify a set of algorithms that are able to learn and return only the embedding values for the learned input data. </p>
			<p>In this section, we will explain in detail some of those algorithms. Moreover, we will enrich the descriptions by providing several examples of how to use those algorithms in Python. For <a id="_idIndexMarker254"/>all the algorithms described in this section, we <a id="_idIndexMarker255"/>will use the implementation provided in the following<a id="_idIndexMarker256"/> libraries: <strong class="bold">Graph Embedding Methods</strong> (<strong class="bold">GEM</strong>), <strong class="bold">Node to Vector</strong> (<strong class="bold">Node2Vec</strong>), and Karate Club.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor051"/>Matrix factorization</h2>
			<p>Matrix <a id="_idIndexMarker257"/>factorization is a general decomposition technique widely used in <a id="_idIndexMarker258"/>different domains. A consistent number of graph embedding algorithms use this technique in order to compute the node embedding of a graph. </p>
			<p>We will start by providing a general introduction to the matrix factorization problem. After the introduction of the basic principles, we will describe two algorithms, namely <strong class="bold">Graph Factorization</strong> (<strong class="bold">GF</strong>) and <strong class="bold">Higher-Order Proximity Preserved Embedding</strong> (<strong class="bold">HOPE</strong>), which <a id="_idIndexMarker259"/>use <a id="_idIndexMarker260"/>matrix factorization to build the node embedding of a graph.</p>
			<p>Let <img src="image/B16069_03_001.png" alt=""/> be the input data. Matrix factorization decomposes <img src="image/B16069_03_002.png" alt=""/> with <img src="image/B16069_03_003.png" alt=""/> and <img src="image/B16069_03_004.png" alt=""/> called<a id="_idIndexMarker261"/> the <strong class="bold">source</strong> and <strong class="bold">abundance</strong> matrix, respectively, and <img src="image/B16069_03_005.png" alt=""/> is the<a id="_idIndexMarker262"/> number of dimensions of the generated embedding space. The matrix factorization algorithm learns the <em class="italic">V</em> and <em class="italic">H</em> matrices by minimizing a loss function that can change according to the specific problem we want to solve. In its general formulation, the loss function is defined by computing the reconstruction error using the Frobenius norm as <img src="image/B16069_03_006.png" alt=""/>.</p>
			<p>Generally speaking, all the unsupervised embedding algorithms based on matrix factorization use the same principle. They all factorize an input graph expressed as a matrix in different <a id="_idIndexMarker263"/>components. The main difference between each method lies in the loss function used during the optimization process. Indeed, different loss functions allow creating an <a id="_idIndexMarker264"/>embedding space that emphasizes specific properties of the input graph.</p>
			<h3>Graph factorization</h3>
			<p>The GF <a id="_idIndexMarker265"/>algorithm was one of the first <a id="_idIndexMarker266"/>models to reach good computational performance in order to perform the node embedding of a given graph. By following the principle of matrix factorization that we previously described, the GF algorithm factorizes the adjacency matrix of a given graph. </p>
			<p>Formally, let <img src="image/B16069_03_007.png" alt=""/> be the graph we want to compute the node embedding with and let <img src="image/B16069_03_008.png" alt=""/> be its adjacency matrix. The loss function (<em class="italic">L</em>) used in this matrix factorization problem is as follows:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B16069_03_009.jpg" alt=""/>
				</div>
			</div>
			<p>In the preceding equation, <img src="image/B16069_03_010.png" alt=""/> represents one of the edges in <em class="italic">G</em> while <img src="image/B16069_03_011.png" alt=""/> is the matrix containing the <em class="italic">d</em>-dimensional embedding. Each row of the matrix represents the embedding of a given node. Moreover, a regularization term (<img src="image/B16069_03_012.png" alt=""/>) of the embedding matrix is used to ensure that the problem remains well-posed even in the absence of sufficient data. </p>
			<p>The loss function used in this method was mainly designed to improve GF performances and scalability. Indeed, the solution generated by this method could be noisy. Moreover, it should be noted, by looking at its matrix factorization formulation, that GF performs a strong symmetric factorization. This property is particularly suitable for undirected graphs, where the adjacency matrix is symmetric, but could be a potential limitation for undirected graphs.</p>
			<p>In the following code, we will show how to perform the node embedding of a given <strong class="source-inline">networkx</strong> graph <a id="_idIndexMarker267"/>using Python and the<a id="_idIndexMarker268"/> GEM library:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from gem.embedding.gf import GraphFactorization</p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4)</p>
			<p class="source-code">gf = GraphFactorization(d=2, data_set=None, max_iter=10000, eta=1*10**-4, regu=1.0)</p>
			<p class="source-code">gf.learn_embedding(G)</p>
			<p class="source-code">embeddings = gf.get_embedding()</p>
			<p>In the preceding example, the following have been done:</p>
			<ol>
				<li><strong class="source-inline">networkx</strong> is used to<a id="_idIndexMarker269"/> generate a <strong class="bold">barbell graph</strong> (<em class="italic">G</em>) used as input for the GF factorization algorithm.</li>
				<li>The <strong class="source-inline">GraphFactorization</strong> class is used to generate a <strong class="source-inline">d=2</strong>-dimensional embedding space.</li>
				<li>The computation of the node embeddings of the input graph is performed using <strong class="source-inline">gf.learn_embedding(G)</strong>.</li>
				<li>The computed embeddings are extracted by calling the <strong class="source-inline">gf.get_embedding()</strong> method. </li>
			</ol>
			<p>The results of the previous code are shown in the following graph:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B16069_03_02.jpg" alt="Figure 3.2 – Application of the GF algorithm to a graph (left) to generate the embedding vector of its nodes (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Application of the GF algorithm to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>From <em class="italic">Figure 3.2</em>, it is possible to see how nodes belonging to groups 1 and 3 are mapped <a id="_idIndexMarker270"/>together in the same region of space. Those<a id="_idIndexMarker271"/> points are separated by the nodes belonging to group 2. This mapping allows us to well separate groups 1 and 3 from group 2. Unfortunately, there is no clear separation between groups 1 and 3.</p>
			<h3>Higher-order proximity preserved embedding</h3>
			<p>HOPE is <a id="_idIndexMarker272"/>another <a id="_idIndexMarker273"/>graph embedding technique based on the matrix factorization principle. This method allows preserving higher-order proximity and does not force its embeddings to have any symmetric properties. Before starting to describe the method, let's understand what first-order proximity and high-order proximity mean:</p>
			<ul>
				<li><strong class="bold">First-order proximity</strong>: Given <a id="_idIndexMarker274"/>a graph, <img src="image/B16069_03_013.png" alt=""/>, where the edges have a weight, <img src="image/B16069_03_014.png" alt=""/>, for each vertex pair <img src="image/B16069_03_015.png" alt=""/>, we say they have a first-order proximity equal to <img src="image/B16069_03_016.png" alt=""/> if the edge <img src="image/B16069_03_017.png" alt=""/>. Otherwise, the first-order proximity between the two nodes is 0.</li>
				<li><strong class="bold">Second- and high-order proximity</strong>: With the second-order proximity, we can capture<a id="_idIndexMarker275"/> the two-step relations between each pair of vertices. For each vertex pair <img src="image/B16069_03_018.png" alt=""/>, we can see the second-order proximity as a two-step transition from <img src="image/B16069_03_019.png" alt=""/> to <img src="image/B16069_03_020.png" alt=""/>. High-order proximity generalizes this concept and allows us to capture a more global structure. As a consequence, high-order proximity<a id="_idIndexMarker276"/> can<a id="_idIndexMarker277"/> be viewed as a k-step (<em class="italic">k</em> ≥ 3) transition from <img src="image/B16069_03_021.png" alt=""/> to <img src="image/B16069_03_022.png" alt=""/>.</li>
			</ul>
			<p>Given the definition of proximity, we can now describe the HOPE method. Formally, let <img src="image/B16069_03_023.png" alt=""/> be the graph we want to compute the embedding for and let <img src="image/B16069_03_024.png" alt=""/> be its adjacency matrix. The loss function (<em class="italic">L</em>) used by this problem is as follows:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B16069_03_025.jpg" alt=""/>
				</div>
			</div>
			<p>In the preceding equation,  <img src="image/B16069_03_026.png" alt=""/> is a similarity matrix generated from graph <img src="image/B16069_03_027.png" alt=""/> and <img src="image/B16069_03_028.png" alt=""/> and <img src="image/B16069_03_029.png" alt=""/> are two embedding matrices representing a <em class="italic">d</em>-dimensional embedding space. In more detail, <img src="image/B16069_03_030.png" alt=""/> represents the source embedding and <img src="image/B16069_03_031.png" alt=""/> represents<a id="_idIndexMarker278"/> the target <a id="_idIndexMarker279"/>embedding. </p>
			<p>HOPE uses those two matrices in order to capture asymmetric proximity in directed networks where the direction from a source node and a target node is present. The final embedding matrix, <img src="image/B16069_03_032.png" alt=""/>, is obtained by simply concatenating, column-wise, the <img src="image/B16069_03_033.png" alt=""/> and <img src="image/B16069_03_034.png" alt=""/> matrices. Due to this operation, the final embedding space generated by HOPE will have <img src="image/B16069_03_035.png" alt=""/> dimensions. </p>
			<p>As we already stated, the <img src="image/B16069_03_036.png" alt=""/> matrix is a similarity matrix obtained from the original graph, <em class="italic">G</em>. The goal of <img src="image/B16069_03_037.png" alt=""/> is to obtain high-order proximity information. Formally, it is computed as <img src="image/B16069_03_038.png" alt=""/>, where <img src="image/B16069_03_039.png" alt=""/> and <img src="image/B16069_03_040.png" alt=""/> are both polynomials of matrices. </p>
			<p>In its <a id="_idIndexMarker280"/>original formulation, the <a id="_idIndexMarker281"/>authors of HOPE suggested different ways to compute <img src="image/B16069_03_041.png" alt=""/> and <img src="image/B16069_03_042.png" alt=""/>. Here we report a common and easy method to compute <a id="_idIndexMarker282"/>those matrices, <strong class="bold">Adamic-Adar</strong> (<strong class="bold">AA</strong>). In this formulation, <img src="image/B16069_03_043.png" alt=""/>(the identity matrix) while <img src="image/B16069_03_044.png" alt=""/>, where <img src="image/B16069_03_045.png" alt=""/> is a diagonal<a id="_idIndexMarker283"/> matrix <a id="_idIndexMarker284"/>computed as <img src="image/B16069_03_046.png" alt=""/>. Other<a id="_idIndexMarker285"/> formulations to compute <img src="image/B16069_03_047.png" alt=""/>and <img src="image/B16069_03_048.png" alt=""/> are the <strong class="bold">Katz Index</strong>, <strong class="bold">Rooted PageRank</strong> (<strong class="bold">RPR</strong>), and <strong class="bold">Common Neighbors</strong> (<strong class="bold">CN</strong>).</p>
			<p>In the following code, we will show how to perform the node embedding of a given <strong class="source-inline">networkx</strong> graph using Python and the GEM library:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from gem.embedding.hope import HOPE</p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4)</p>
			<p class="source-code">gf = HOPE(d=4, beta=0.01)</p>
			<p class="source-code">gf.learn_embedding(G)</p>
			<p class="source-code">embeddings = gf.get_embedding()</p>
			<p>The<a id="_idIndexMarker286"/> preceding code is similar<a id="_idIndexMarker287"/> to the one used for GF. The only difference is in the class initialization since here we use <strong class="source-inline">HOPE</strong>. According to the implementation provided by GEM, the <strong class="source-inline">d</strong> parameter, representing the dimension of the embedding space, will define the number of columns of the final embedding matrix, <img src="image/B16069_03_049.png" alt=""/>, obtained after the column-wise concatenation of <img src="image/B16069_03_050.png" alt=""/> and <img src="image/B16069_03_051.png" alt=""/>. </p>
			<p>As a consequence, the number of columns of <img src="image/B16069_03_052.png" alt=""/> and <img src="image/B16069_03_053.png" alt=""/> is defined by the floor division (the <strong class="source-inline">//</strong> operator in Python) of the value assigned to <strong class="source-inline">d</strong>. The results of the code are shown in the following graph:</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B16069_03_03.jpg" alt="Figure 3.3 – Application of the HOPE algorithm to a graph (left) to generate the embedding vector of its nodes (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Application of the HOPE algorithm to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>In this case, the graph is undirected and thus there is no difference between the source and target <a id="_idIndexMarker288"/>nodes. <em class="italic">Figure 3.3</em> shows the first two dimensions of the <strong class="source-inline">embeddings</strong> matrix<a id="_idIndexMarker289"/> representing <img src="image/B16069_03_054.png" alt=""/>. It is possible to see how the embedding space generated by HOPE provides, in this case, a better separation of the different nodes.</p>
			<h3>Graph representation with global structure information</h3>
			<p>Graph<a id="_idIndexMarker290"/> representation<a id="_idIndexMarker291"/> with global structure information (GraphRep), such as HOPE, allows us to preserve higher-order proximity without forcing its embeddings to have symmetric properties. Formally, let <img src="image/B16069_03_055.png" alt=""/> be the graph we want to compute the node embeddings for and let <img src="image/B16069_03_056.png" alt=""/> be its adjacency matrix. The loss function (<em class="italic">L</em>) used by this problem is as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B16069_03_057.jpg" alt=""/>
				</div>
			</div>
			<p>In the preceding equation, <img src="image/B16069_03_058.png" alt=""/> is a matrix generated from graph <em class="italic">G</em> in order to get the <em class="italic">k</em>th order of proximity between nodes. </p>
			<p><img src="image/B16069_03_059.png" alt=""/> and <img src="image/B16069_03_060.png" alt=""/> are two embedding matrices representing a <em class="italic">d</em>-dimensional<a id="_idIndexMarker292"/> embedding<a id="_idIndexMarker293"/> space of the <em class="italic">k</em>th order of proximity for the source and target nodes, respectively. </p>
			<p>The <img src="image/B16069_03_061.png" alt=""/> matrix is computed according to the following equation: <img src="image/B16069_03_062.png" alt=""/>. Here, <img src="image/B16069_03_063.png" alt=""/> is a diagonal matrix<a id="_idIndexMarker294"/> known as the <strong class="bold">degree matrix</strong> computed using the following equation: </p>
			<p>  </p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B16069_03_064.jpg" alt=""/>
				</div>
			</div>
			<p> <img src="image/B16069_03_065.png" alt=""/> represents the (one-step) probability transition matrix, where <img src="image/B16069_03_066.png" alt=""/> is the probability of a transition from <img src="image/B16069_03_067.png" alt=""/>to vertex <img src="image/B16069_03_068.png" alt=""/> within one step. In general, for a generic value of <em class="italic">k</em>, <img src="image/B16069_03_069.png" alt=""/> represents the probability of a transition from <img src="image/B16069_03_070.png" alt=""/>to vertex <img src="image/B16069_03_071.png" alt=""/> within <em class="italic">k</em> steps.</p>
			<p>For each order <a id="_idIndexMarker295"/>of <a id="_idIndexMarker296"/>proximity, <em class="italic">k</em>, an independent optimization problem is fitted. All the <em class="italic">k</em> embedding matrices generated are then column-wise concatenated to get the final source embedding matrices. </p>
			<p>In the following code, we will show how to perform the node embedding of a given <strong class="source-inline">networkx</strong> graph using Python and the <strong class="source-inline">karateclub</strong> library: </p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from karateclub.node_embedding.neighbourhood.grarep import GraRep</p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4)</p>
			<p class="source-code">gr = GraRep(dimensions=2, order=3)</p>
			<p class="source-code">gr.fit(G)</p>
			<p class="source-code">embeddings = gr.get_embedding()</p>
			<p>We initialize the <strong class="source-inline">GraRep</strong> class from the <strong class="source-inline">karateclub</strong> library. In this implementation, the <strong class="source-inline">dimension</strong> parameter represents the dimension of the embedding space, while the <strong class="source-inline">order</strong> parameter defines the maximum number of orders of proximity between nodes. The number of columns of the final embedding matrix (stored, in the example, in the <strong class="source-inline">embeddings</strong> variable) is <strong class="source-inline">dimension*order</strong>, since, as we said, for each proximity order an embedding is computed and concatenated in the final embedding matrix. </p>
			<p>To specify, since two dimensions are computed in the example, <strong class="source-inline">embeddings[:,:2]</strong> represents the embedding obtained for <em class="italic">k</em>=1, <strong class="source-inline">embeddings[:,2:4]</strong> for <em class="italic">k</em>=2, and <strong class="source-inline">embeddings[:,4:]</strong> for <em class="italic">k</em>=3. The<a id="_idIndexMarker297"/> results <a id="_idIndexMarker298"/>of the code are shown in the following graph:</p>
			<p class="figure-caption">.f</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B16069_03_04.jpg" alt="Figure 3.4 – Application of the GraphRep algorithm to a graph (top) to generate the embedding vector of its nodes (bottom) for different values of k"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Application of the GraphRep algorithm to a graph (top) to generate the embedding vector of its nodes (bottom) for different values of k</p>
			<p>From the preceding graph, it is easy to see how different orders of proximity allow us to get different embeddings. Since the input graph is quite simple, in this case, already with <em class="italic">k</em>=1, a well-separated embedding space is obtained. To specify, the nodes belonging to groups 1 and 3 in all the proximity orders have the same embedding values (they are overlapping in the scatter plot). </p>
			<p>In this section, we <a id="_idIndexMarker299"/>described <a id="_idIndexMarker300"/>some matrix factorization methods for unsupervised graph embedding. In the next section, we will introduce a different way to perform unsupervised graph embedding using skip-gram models.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor052"/>Skip-gram</h2>
			<p>In this section, we <a id="_idIndexMarker301"/>will provide a quick description of the skip-gram model. Since it is<a id="_idIndexMarker302"/> widely used by different embedding algorithms, a high-level description is needed to better understand the different methods. Before going deep into a detailed description, we will first give a brief overview. </p>
			<p>The skip-gram model is a simple neural network with one hidden layer trained in order to predict the probability of a given word being present when an input word is present. The neural network is trained by building the training data using a text corpus as a reference. This process is described in the following chart:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/B16069_03_05.jpg" alt="Figure 3.5 – Example of the generation of training data from a given corpus. In the filled boxes, the target word. In the dash boxes, the context words identified by a window size of length 2"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Example of the generation of training data from a given corpus. In the filled boxes, the target word. In the dash boxes, the context words identified by a window size of length 2</p>
			<p>The example described in <em class="italic">Figure 3.5</em> shows how the algorithm to generate the training data works. A <em class="italic">target</em> word is selected and a rolling window of fixed size <em class="italic">w</em> is built around that word. The words inside the rolling windows are known as <em class="italic">context</em> words. Multiple pairs of <em class="italic">(target word, context word)</em> are then built according to the words inside the rolling window. </p>
			<p>Once the training data<a id="_idIndexMarker303"/> is generated from the whole corpus, the skip-gram model is trained to predict the probability of a word being a context word for the given target. During its training, the neural network learns a compact representation of the input words. This is why the skip-gram <a id="_idIndexMarker304"/>model is also known as <strong class="bold">Word to Vector</strong> (<strong class="bold">Word2Vec</strong>).</p>
			<p>The structure of the<a id="_idIndexMarker305"/> neural network representing the skip-gram model is described in the following chart:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B16069_03_06.jpg" alt="Figure 3.6 – Structure of the neural network of the skip-gram model. The number of d neurons in the hidden layer represents the final size of the embedding space"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Structure of the neural network of the skip-gram model. The number of d neurons in the hidden layer represents the final size of the embedding space</p>
			<p>The input of the neural network is a binary vector of size <em class="italic">m</em>. Each element of the vector represents a word in the dictionary of the language we want to embed the words in. When, during the training process, a <em class="italic">(target word, context word)</em> pair is given, the input array will have 0 in all its entries with the exception of the entry representing the "target" word, which will be equal to 1. The hidden layer has <em class="italic">d</em> neurons. The hidden layer will learn the embedding representation of each word, creating a <em class="italic">d</em>-dimensional embedding space. </p>
			<p>Finally, the output layer of the neural network is a dense layer of <em class="italic">m</em> neurons (the same size as the input vector) with a <em class="italic">softmax</em> activation function. Each neuron represents a word of the dictionary. The value assigned by the neuron corresponds to the probability of that word being "related" to the input word. Since softmax can be hard to compute when the size of <em class="italic">m</em> increases, a <em class="italic">hierarchical softmax</em> approach is always used.</p>
			<p>The final goal of the skip-gram model is not to actually learn the task we previously described but to build a compact <em class="italic">d</em>-dimensional representation of the input words. Thanks to this representation, it is possible to easily extract an embedding space for the words using the weight of the hidden layer. Another common approach to creating a skip-gram model, which<a id="_idIndexMarker306"/> will be not described here, is <em class="italic">context-based</em>: <strong class="bold">Continuous Bag-of-Words</strong> (<strong class="bold">CBOW</strong>).</p>
			<p>Since the basic concepts behind the skip-gram model have been introduced, we can start to describe a series of unsupervised graph embedding algorithms built upon this model. Generally <a id="_idIndexMarker307"/>speaking, all the unsupervised embedding algorithms<a id="_idIndexMarker308"/> based on the skip-gram model use the same principle. </p>
			<p>Starting from an input graph, they extract from it a set of walks. Those walks can be seen as a text corpus where each node represents a word. Two words (representing nodes) are near each other in the text if they are connected by an edge in a walk. The main difference between each method lies in the way those walks are computed. Indeed, as we will see, different walk generation algorithms can emphasize particular local or global structures of the graph.</p>
			<h3>DeepWalk</h3>
			<p>The DeepWalk<a id="_idIndexMarker309"/> algorithm generates the node embedding of a<a id="_idIndexMarker310"/> given graph using the skip-gram model. In order to provide a better explanation of this model, we need to introduce the <a id="_idIndexMarker311"/>concept of <strong class="bold">random walks</strong>. </p>
			<p>Formally, let <img src="image/B16069_03_072.png" alt=""/> be a graph and let <img src="image/B16069_03_073.png" alt=""/> be a vertex selected as the starting point. We select a neighbor of <img src="image/B16069_03_074.png" alt=""/> at random and we move toward it. From this point, we randomly select another point to move. This process is repeated <img src="image/B16069_03_075.png" alt=""/> times. The random sequence of <img src="image/B16069_03_076.png" alt=""/> vertices selected in this way is a random walk of length <img src="image/B16069_03_077.png" alt=""/>. It is worth mentioning that the algorithm used to generate the random walks does not impose any constraint on how they are built. As a consequence, there is no guarantee that the local neighborhood of the node is well preserved.</p>
			<p>Using the notion of random walk, the DeepWalk algorithm generates a random walk of a size of at most <em class="italic">t</em> for each node. Those random walks will be given as input to the skip-gram model. The embedding generated using skip-gram will be used as the final node embedding. In the following figure (<em class="italic">Figure 3.7</em>), we can see a step-by-step graphical representation of <a id="_idIndexMarker312"/>the<a id="_idIndexMarker313"/> algorithm:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B16069_03_07.jpg" alt="Figure 3.7 – All the steps used by the DeepWalk algorithm to generate the node embedding of a given graph"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – All the steps used by the DeepWalk algorithm to generate the node embedding of a given graph</p>
			<p>Here is a step-by-step explanation of the algorithm graphically described in the preceding chart:</p>
			<ol>
				<li value="1"><strong class="bold">Random Walk Generation</strong>: For <a id="_idIndexMarker314"/>each node of input graph <em class="italic">G</em>, a set of <img src="image/B16069_03_078.png" alt=""/>random walks with a fixed maximum length (<em class="italic">t</em>) is computed. It should be noted that the length <em class="italic">t</em> is an upper bound. There are no constraints forcing all the paths to have the same length. </li>
				<li><strong class="bold">Skip-Gram Training</strong>: Using all the random walks generated in the previous step, a skip-gram<a id="_idIndexMarker315"/> model is trained. As we described earlier, the skip-gram model works on words and sentences. When a graph is given as input to the skip-gram model, as visible in <em class="italic">Figure 3.7</em>, a graph can be seen as an input text corpus, while a single node of the graph can be seen as a word of the corpus. <p>A random walk can be seen as a sequence of words (a sentence). The skip-gram is then trained using the "fake" sentences generated by the nodes in the random walk. The parameters for the skip-gram model previously described (window size, <em class="italic">w</em>, and embed size, <em class="italic">d</em>) are used in this step.</p></li>
				<li><strong class="bold">Embedding Generation</strong>: The<a id="_idIndexMarker316"/> information contained in the hidden layers of the trained skip-gram model is used in order to extract the embedding of each node.</li>
			</ol>
			<p>In the following code, we <a id="_idIndexMarker317"/>will show how to perform the node<a id="_idIndexMarker318"/> embedding of a given <strong class="source-inline">networkx</strong> graph using Python and the <strong class="source-inline">karateclub</strong> library:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk</p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4)</p>
			<p class="source-code">dw = DeepWalk(dimensions=2)</p>
			<p class="source-code">dw.fit(G)</p>
			<p class="source-code">embeddings = dw.get_embedding()</p>
			<p>The code is quite simple. We initialize the <strong class="source-inline">DeepWalk</strong> class from the <strong class="source-inline">karateclub</strong> library. In this implementation, the <strong class="source-inline">dimensions</strong> parameter represents the dimension of the embedding space. Other parameters worth mentioning that the <strong class="source-inline">DeepWalk</strong> class accepts are as follows: </p>
			<ul>
				<li><strong class="source-inline">walk_number</strong>: The number of random walks to generate for each node</li>
				<li><strong class="source-inline">walk_length</strong>: The length of the generated random walks</li>
				<li><strong class="source-inline">window_size</strong>: The window size parameter of the skip-gram model</li>
			</ul>
			<p>Finally, the model is fitted on graph <em class="italic">G</em> using <strong class="source-inline">dw.fit(G)</strong> and the embeddings are extracted using <strong class="source-inline">dw.get_embedding()</strong>.</p>
			<p>The results of the code are shown in the following figure:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B16069_03_08.jpg" alt="Figure 3.8 – Application of the DeepWalk algorithm to a graph (left) to generate the embedding vector of its nodes (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Application of the DeepWalk algorithm to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>From the previous<a id="_idIndexMarker319"/> graph, we can see how DeepWalk is able to <a id="_idIndexMarker320"/>separate region 1 from region 3. Those two groups are contaminated by the nodes belonging to region 2. Indeed, for those nodes, a clear distinction is not visible in the embedding space.</p>
			<h3>Node2Vec</h3>
			<p>The <strong class="bold">Node2Vec</strong> algorithm <a id="_idIndexMarker321"/>can be seen as an extension of <a id="_idIndexMarker322"/>DeepWalk. Indeed, as with DeepWalk, Node2Vec also generates a set of random walks used as input to a skip-gram model. Once trained, the hidden layers of the skip-gram model are used to generate the embedding of the node in the graph. The main difference between the two algorithms lies in the way the random walks are generated. </p>
			<p>Indeed, if DeepWalk generates random walks without using any bias, in Node2Vec a new technique to generate biased random walks on the graph is introduced. The algorithm to generate the random walks combines graph exploration by merging <strong class="bold">Breadth-First Search</strong> (<strong class="bold">BFS</strong>) and <strong class="bold">Depth-First Search</strong> (<strong class="bold">DFS</strong>). The way those two algorithms<a id="_idIndexMarker323"/> are <a id="_idIndexMarker324"/>combined in the random walk's generation is regularized by two parameters, <img src="image/B16069_03_079.png" alt=""/> and <img src="image/B16069_03_080.png" alt=""/>. <img src="image/B16069_03_081.png" alt=""/> defines the probability of a random walk getting back to the previous node, while <img src="image/B16069_03_082.png" alt=""/> defines the probability that a random walk can pass through a previously unseen part of the graph. </p>
			<p>Due to this <a id="_idIndexMarker325"/>combination, Node2Vec can preserve high-order <a id="_idIndexMarker326"/>proximities by preserving local structures in the graph as well as global community structures. This new method of random walk generation allows solving the limitation of DeepWalk preserving the local neighborhood properties of the node.</p>
			<p>In the following code, we will show how to perform the node embedding of a given <strong class="source-inline">networkx</strong> graph using Python and the <strong class="source-inline">node2vec</strong> library:</p>
			<p class="source-code">import networkx as nx</p>
			<p class="source-code">from node2vec import Node2Vec</p>
			<p class="source-code">G = nx.barbell_graph(m1=10, m2=4)</p>
			<p class="source-code">draw_graph(G)</p>
			<p class="source-code">node2vec = Node2Vec(G, dimensions=2)</p>
			<p class="source-code">model = node2vec.fit(window=10)</p>
			<p class="source-code">embeddings = model.wv</p>
			<p>Also, for Node2Vec, the code is straightforward. We initialize the <strong class="source-inline">Node2Vec</strong> class from the <strong class="source-inline">node2vec</strong> library. In this implementation, the <strong class="source-inline">dimensions</strong> parameter represents the dimension of the embedding space. The model is then fitted using <strong class="source-inline">node2vec.fit(window=10)</strong>. Finally, the embeddings are obtained using <strong class="source-inline">model.wv</strong>. </p>
			<p>It should be noted that <strong class="source-inline">model.wv</strong> is an object of the <strong class="source-inline">Word2VecKeyedVectors</strong> class. In order to get the embedding vector of a specific node with <strong class="source-inline">nodeid</strong> as the ID, we can use the trained model, as follows: <strong class="source-inline">model.wv[str(nodeId)]</strong>. Other parameters worth mentioning that the <strong class="source-inline">Node2Vec</strong> class accepts are as follows: </p>
			<ul>
				<li><strong class="source-inline">num_walks</strong>: The number of random walks to generate for each node</li>
				<li><strong class="source-inline">walk_length</strong>: The length of the generated random walks</li>
				<li><strong class="source-inline">p, q</strong>: The <em class="italic">p</em> and <em class="italic">q</em> parameters of the random walk's generation algorithm</li>
			</ul>
			<p>The results <a id="_idIndexMarker327"/>of <a id="_idIndexMarker328"/>the code are shown in <em class="italic">Figure 3.9</em>:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B16069_03_09.jpg" alt="Figure 3.9 – Application of the Node2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Application of the Node2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>As is visible from <em class="italic">Figure 3.9</em>, Node2Vec allows us to obtain a better separation between nodes in the embedding space compared to DeepWalk. To specify, regions 1 and 3 are well clustered in two regions of space. Region 2 instead is well placed in the middle of the two groups without any overlap.</p>
			<h3>Edge2Vec</h3>
			<p>Contrary to <a id="_idIndexMarker329"/>the other embedding<a id="_idIndexMarker330"/> function, the <strong class="bold">Edge to Vector</strong> (<strong class="bold">Edge2Vec</strong>) algorithm generates the embedding space on edges, instead of nodes. This algorithm is a simple side effect of the embedding generated by using Node2Vec. The main idea is to use the node embedding of two adjacent nodes to perform some basic mathematical operations in order to extract the embedding of the edge connecting them. </p>
			<p>Formally, let <img src="image/B16069_03_083.png" alt=""/> and <img src="image/B16069_03_084.png" alt=""/> be two adjacent nodes and let <img src="image/B16069_03_085.png" alt=""/> and <img src="image/B16069_03_086.png" alt=""/> be their embeddings computed with Node2Vec. The operators described in <em class="italic">Table 3.1</em> can be used in order<a id="_idIndexMarker331"/> to compute the embedding <a id="_idIndexMarker332"/>of their edge:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/B16069_03_Table_01.jpg" alt="Table 3.1 – Edge embedding operators with their equation and class name in the Node2Vec library"/>
				</div>
			</div>
			<p class="figure-caption">Table 3.1 – Edge embedding operators with their equation and class name in the Node2Vec library</p>
			<p>In the following code, we will show how to perform the node embedding of a given <strong class="source-inline">networkx</strong> graph using Python and the Node2Vec library:</p>
			<p class="source-code">from node2vec.edges import HadamardEmbedder</p>
			<p class="source-code">embedding = HadamardEmbedder(keyed_vectors=model.wv)</p>
			<p>The code is quite simple. The <strong class="source-inline">HadamardEmbedder</strong> class is instantiated with only the <strong class="source-inline">keyed_vectors</strong> parameter. The value of this parameter is the embedding model generated by Node2Vec. In order to use other techniques to generate the edge embedding, we just need to change the class and select one from the ones listed in <em class="italic">Table 3.1</em>. An <a id="_idIndexMarker333"/>example of the application of<a id="_idIndexMarker334"/> this algorithm is shown in the following figure:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B16069_03_11.jpg" alt="Figure 3.11 – Application of the Edge2Vec algorithm to a graph (top) to generate the embedding vector of its nodes (bottom) using different methods"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Application of the Edge2Vec algorithm to a graph (top) to generate the embedding vector of its nodes (bottom) using different methods</p>
			<p>From <em class="italic">Figure 3.11</em>, we can see how different embedding methods generate completely different embedding spaces. <strong class="source-inline">AverageEmbedder</strong> and <strong class="source-inline">HadamardEmbedder</strong>, in this example, generate well-separated embeddings for regions 1, 2, and 3. </p>
			<p>For <strong class="source-inline">WeightedL1Embedder</strong> and <strong class="source-inline">WeightedL2Embedder</strong>, however, the embedding space is not well<a id="_idIndexMarker335"/> separated since the edge embeddings are concentrated in a single region without showing clear clusters.</p>
			<h3>Graph2Vec</h3>
			<p>The methods<a id="_idIndexMarker336"/> we previously described<a id="_idIndexMarker337"/> generated the embedding space for each node or edge on a given graph. <strong class="bold">Graph to Vector</strong> (<strong class="bold">Graph2Vec</strong>) generalizes this concept and generates embeddings for the whole graph. </p>
			<p>To specify, given a set of graphs, the Graph2Vec algorithms generate an embedding space where each point represents a graph. This algorithm generates its embedding using an evolution of the Word2Vec skip-gram model known as <strong class="bold">Document to Vector</strong> (<strong class="bold">Doc2Vec</strong>). We <a id="_idIndexMarker338"/>can graphically see a simplification of this model in <em class="italic">Figure 3.12</em>: </p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B16069_03_12.jpg" alt="Figure 3.12 – Simplified graphical representation of the Doc2Vec skip-gram model. The number of d neurons in the hidden layer represents the final size of the embedding space"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Simplified graphical representation of the Doc2Vec skip-gram model. The number of d neurons in the hidden layer represents the final size of the embedding space</p>
			<p>Compared to the<a id="_idIndexMarker339"/> simple Word2Vec, Doc2Vec also accepts another binary array representing the document containing the input word. Given a "target" document and a "target" word, the model then tries to predict the most probable "context" word with respect to the input "target" word and document.</p>
			<p>With the introduction of the Doc2Vec model, we can now describe the Graph2Vec algorithm. The main idea behind this method is to view an entire graph as a document and each of its subgraphs, generated as an ego graph (see <a href="B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Graphs</em>) of each node, as words that comprise the document. </p>
			<p>In other words, a graph is composed of subgraphs as a document is composed of sentences. According to this description, the algorithm can be summarized into the following steps:</p>
			<ol>
				<li value="1"><strong class="bold">Subgraph generation</strong>: A set <a id="_idIndexMarker340"/>of rooted subgraphs is generated around every node.</li>
				<li><strong class="bold">Doc2Vec training</strong>: The <a id="_idIndexMarker341"/>Doc2Vec skip-gram is trained using the subgraphs generated by the previous step. </li>
				<li><strong class="bold">Embedding generation</strong>: The <a id="_idIndexMarker342"/>information contained in the hidden layers of the trained Doc2Vec model is used in order to extract the embedding of each node.</li>
			</ol>
			<p>In the following code, as we already did in <a href="B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Graph Machine Learning</em>, we will show<a id="_idIndexMarker343"/> how to perform the node embedding of<a id="_idIndexMarker344"/> a set of <strong class="source-inline">networkx</strong> graphs using Python and the <strong class="source-inline">karateclub</strong> library:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">from karateclub import Graph2Vec</p>
			<p class="source-code">n_graphs = 20</p>
			<p class="source-code">def generate_random():</p>
			<p class="source-code">    n = random.randint(5, 20)</p>
			<p class="source-code">    k = random.randint(5, n)</p>
			<p class="source-code">    p = random.uniform(0, 1)</p>
			<p class="source-code">    return nx.watts_strogatz_graph(n,k,p)</p>
			<p class="source-code">Gs = [generate_random() for x in range(n_graphs)]</p>
			<p class="source-code">model = Graph2Vec(dimensions=2)</p>
			<p class="source-code">model.fit(Gs)</p>
			<p class="source-code">embeddings = model.get_embedding()</p>
			<p>In this example, the following have been done:</p>
			<ol>
				<li value="1">20 Watts-Strogatz graphs have been generated with random parameters. </li>
				<li>We then initialize the <strong class="source-inline">Graph2Vec</strong> class from the <strong class="source-inline">karateclub</strong> library with two dimensions. In this implementation, the <strong class="source-inline">dimensions</strong> parameter represents the dimension of the embedding space.</li>
				<li>The model is then fitted on the input data using <strong class="source-inline">model.fit(Gs)</strong>. </li>
				<li>The vector containing the embeddings is extracted using <strong class="source-inline">model.get_embedding()</strong>. <p>The results<a id="_idIndexMarker345"/> of the code are shown in the<a id="_idIndexMarker346"/> following figure:</p></li>
			</ol>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B16069_03_13.jpg" alt="Figure 3.13 – Application of the Graph2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right) using different methods"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Application of the Graph2Vec algorithm to a graph (left) to generate the embedding vector of its nodes (right) using different methods</p>
			<p>From <em class="italic">Figure 3.13</em>, it is possible to see the embedding space generated for the different graphs. </p>
			<p>In this section, we described different shallow embedding methods based on matrix factorization and the skip-gram model. However, in the scientific literature, a lot of unsupervised embedding algorithms exist, such as Laplacian methods. We refer those of you who are interested in exploring those methods to look at the paper <em class="italic">Machine Learning on Graphs: A Model and Comprehensive Taxonomy</em> available at <a href="https://arxiv.org/pdf/2005.03675.pdf">https://arxiv.org/pdf/2005.03675.pdf</a>.</p>
			<p>We will continue our description of the unsupervised graph embedding method in the next sections. We<a id="_idIndexMarker347"/> will describe more complex graph<a id="_idIndexMarker348"/> embedding algorithms based on autoencoders.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor053"/>Autoencoders</h1>
			<p>Autoencoders are<a id="_idIndexMarker349"/> an extremely powerful tool that can effectively help data scientists to deal with high-dimensional datasets. Although first presented around 30 years ago, in recent years, autoencoders have become more and more widespread in conjunction with the general rise of neural network-based algorithms. Besides allowing us to compact sparse representations, they can also be at the base of generative models, representing the first inception of the famous <strong class="bold">Generative Adversarial Network</strong> (<strong class="bold">GAN</strong>), which<a id="_idIndexMarker350"/> is, using the words of Geoffrey Hinton:</p>
			<p class="author-quote"> <em class="italic">"The most interesting idea in the last 10 years in machine learning"</em></p>
			<p>An autoencoder is a neural network where the inputs and outputs are basically the same, but that is characterized by a small number of units in the hidden layer. Loosely speaking, it is a neural network that is trained to reconstruct its inputs using a significantly lower number of variables and/or degree of freedom. </p>
			<p>Since an autoencoder does not need a labeled dataset, it can be seen as an example of unsupervised learning and a dimensionality-reduction technique. However, different from other techniques such <a id="_idIndexMarker351"/>as <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) and matrix factorization, autoencoders can learn non-linear transformation thanks to the non-linear activation functions of their neurons:</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B16069_03_14.jpg" alt="Figure 3.14 – Diagram of the autoencoder structure. The colors in the input and output layers represent the fact that the values should be as similar as possible. In fact, the training of the network is done in order to match these values and minimize the reconstruction error "/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Diagram of the autoencoder structure. The colors in the input and output layers represent the fact that the values should be as similar as possible. In fact, the training of the network is done in order to match these values and minimize the reconstruction error </p>
			<p><em class="italic">Figure 3.14</em> shows a <a id="_idIndexMarker352"/>simple example of an autoencoder. You can see how the autoencoder can generally be seen as composed of two parts: </p>
			<ul>
				<li>An encoder network that processes the input through one or more units and maps it into an encoded representation that reduces the dimension of the inputs (under-complete autoencoders) and/or constrains its sparsity (over-complete regularized autoencoders)</li>
				<li>A decoder network that reconstructs the input signal from the encoded representation of the middle layer </li>
			</ul>
			<p>The encoder-decoder structure is then trained to minimize the ability of the full network to reconstruct the input. In order to completely specify an autoencoder, we need a loss function. The error between the inputs and the outputs can be computed using different metrics and indeed the choice of the correct form for the "reconstruction" error is a critical point when building an autoencoder. </p>
			<p>Some <a id="_idIndexMarker353"/>common<a id="_idIndexMarker354"/> choices for the loss functions<a id="_idIndexMarker355"/> that <a id="_idIndexMarker356"/>measure the reconstruction error are <strong class="bold">mean square error</strong>, <strong class="bold">mean absolute error</strong>, <strong class="bold">cross-entropy</strong>, and <strong class="bold">KL divergence</strong>. </p>
			<p>In the following <a id="_idIndexMarker357"/>sections, we will show you how to build an autoencoder starting with some basic concepts and then applying those concepts to graph structures. But before diving in, we feel compelled to give you a very brief introduction to the frameworks that will allow us to do this: TensorFlow and Keras. </p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor054"/>TensorFlow and Keras – a powerful combination</h2>
			<p>Released as<a id="_idIndexMarker358"/> open source by Google in 2017, TensorFlow is <a id="_idIndexMarker359"/>now the standard, de facto framework that allows symbolic computations and differential programming. It basically allows you to build a symbolic structure that describes how inputs are combined in order to produce<a id="_idIndexMarker360"/> the <a id="_idIndexMarker361"/>outputs, defining what is generally called a <strong class="bold">computational graph</strong> or a <strong class="bold">stateful dataflow graph</strong>. In this graph, nodes are the variable (scalar, arrays, tensors) and edges represent operations connecting the inputs (edge source) to the output (edge target) of a single operation. </p>
			<p>In TensorFlow, such a graph is static (this is indeed one of the main differences with respect to another very popular framework in this context: <strong class="source-inline">torch</strong>) and can be executed by feeding data into it, as inputs, clearing the "dataflow" attribute mentioned previously. </p>
			<p>By abstracting the computation, TensorFlow is a very general tool that can run on multiple backends: on machines powered by CPUs, GPUs, or even ad hoc, specifically designed processing units such as TPUs. Besides, TensorFlow-powered applications can also be deployed on different devices, ranging from single and distributed servers to mobile devices.</p>
			<p>Besides abstracting computation, TensorFlow also allows you to symbolically differentiate your computational graph with respect to any of its variables, resulting in a new computational graph that can also be differentiated to produce higher-order derivatives. This approach is generally referred to as symbol-to-symbol derivative and it is indeed extremely powerful, especially in the context of the optimization of the generic loss function, which requires gradient estimations (such as gradient descent techniques). </p>
			<p>As you might know, the problem of optimizing a loss function with respect to many parameters is central in the training of any neural network via backpropagation. This is surely the main reason why TensorFlow has become very popular in the past few years and why it was designed and produced in the first place by Google. </p>
			<p>Diving in depth into the usage of TensorFlow is beyond the scope of this book and indeed you can find out<a id="_idIndexMarker362"/> more through the description given in dedicated books. In the following sections, we will use some of its main functionalities and provide you with the basic tools<a id="_idIndexMarker363"/> for building neural networks. </p>
			<p>Since its last <a id="_idIndexMarker364"/>major release, 2.x, the standard way of building a model with TensorFlow is using the<a id="_idIndexMarker365"/> Keras API. Keras was natively a side external project with respect to TensorFlow, aimed at providing a common and simple API to use several differential programming frameworks, such as TensorFlow, Teano, and CNTK, for implementing a neural network model. It generally abstracts the low-level implementation of the computation graph and provides you with the most common layers used when building neural networks (although custom layers can also be easily implemented), such as the following:</p>
			<ul>
				<li>Convolutional layers</li>
				<li>Recurrent layers</li>
				<li>Regularization layers</li>
				<li>Loss functions</li>
			</ul>
			<p>Keras also exposes APIs that are very similar to scikit-learn, the most popular library for machine learning in the <a id="_idIndexMarker366"/>Python ecosystem, making it very easy for data scientists to build, train, and integrate neural network-based models <a id="_idIndexMarker367"/>in their applications. </p>
			<p>In the next section, we will show you how to build and train an autoencoder using Keras. We'll start applying these techniques to images in order to progressively apply the key concepts to graph structures.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor055"/>Our first autoencoder</h2>
			<p>We'll start by<a id="_idIndexMarker368"/> implementing an autoencoder in its simplest form, that is, a simple feed-forward network trained to reconstruct its input. We'll apply this to the Fashion-MNIST dataset, which is a dataset similar to the famous MNIST dataset that features hand-written numbers on a black and white image. </p>
			<p>MNIST has 10 categories and consists of 60k + 10k (train dataset + test dataset) 28x28 pixel grayscale images that represent a piece of clothing (<strong class="source-inline">T-shirt</strong>, <strong class="source-inline">Trouser</strong>, <strong class="source-inline">Pullover</strong>, <strong class="source-inline">Dress</strong>, <strong class="source-inline">Coat</strong>, <strong class="source-inline">Sandal</strong>, <strong class="source-inline">Shirt</strong>, <strong class="source-inline">Sneaker</strong>, <strong class="source-inline">Bag</strong>, and <strong class="source-inline">Ankle boot</strong>). The Fashion-MNIST dataset is a harder task than the original MNIST dataset and it is generally used for benchmarking algorithms. </p>
			<p>The dataset is already integrated in the Keras library and can be easily imported using the following code: </p>
			<p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p>
			<p class="source-code">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() </p>
			<p>It is usually good practice to rescale the inputs with an order of magnitude of around 1 (for which activation functions are most efficient) and make sure that the numerical data is in single-precision (32 bits) instead of double-precision (64 bits). This is due to the fact that it is generally desirable to promote speed rather than precision when training a neural network, which is a computationally expensive process. In certain cases, the precision could even be lowered to half-precision (16 bits). We transform the input with the following:</p>
			<p class="source-code">x_train = x_train.astype('float32') / 255.</p>
			<p class="source-code">x_test = x_test.astype('float32') / 255.</p>
			<p>We can grasp the type of inputs we are dealing with by plotting some of the samples from the training set using the following code: </p>
			<p class="source-code">n = 10</p>
			<p class="source-code">plt.figure(figsize=(20, 4))</p>
			<p class="source-code">for i in range(n):</p>
			<p class="source-code">    ax = plt.subplot(1, n, i + 1)</p>
			<p class="source-code">    plt.imshow(x_train[i])</p>
			<p class="source-code">    plt.title(classes[y_train[i]])</p>
			<p class="source-code">    plt.gray()</p>
			<p class="source-code">    ax.get_xaxis().set_visible(False)</p>
			<p class="source-code">    ax.get_yaxis().set_visible(False)</p>
			<p class="source-code">plt.show()</p>
			<p>In the preceding <a id="_idIndexMarker369"/>code, <strong class="source-inline">classes</strong> represents the mapping between integers and class names, for example, <strong class="source-inline">T-shirt</strong>, <strong class="source-inline">Trouser</strong>, <strong class="source-inline">Pullover</strong>, <strong class="source-inline">Dress</strong>, <strong class="source-inline">Coat</strong>, <strong class="source-inline">Sandal</strong>, <strong class="source-inline">Shirt</strong>, <strong class="source-inline">Sneaker</strong>, <strong class="source-inline">Bag</strong>, and <strong class="source-inline">Ankle boot</strong>:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B16069_03_15.jpg" alt="Figure 3.15 – Some samples taken from the training set of the Fashion-MNIST dataset"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Some samples taken from the training set of the Fashion-MNIST dataset</p>
			<p>Now that we have imported the inputs, we can build our autoencoder network by creating the encoder and the decoder. We will be doing this using the Keras functional API, which provides more generality and flexibility compared to the so-called Sequential API. We start by defining the encoder network:</p>
			<p class="source-code">from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, Input</p>
			<p class="source-code">input_img = Input(shape=(28, 28, 1))</p>
			<p class="source-code">x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)</p>
			<p class="source-code">x = MaxPooling2D((2, 2), padding='same')(x)</p>
			<p class="source-code">x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)</p>
			<p class="source-code">x = MaxPooling2D((2, 2), padding='same')(x)</p>
			<p class="source-code">x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)</p>
			<p class="source-code">encoded = MaxPooling2D((2, 2), padding='same')(x)</p>
			<p>Our network is composed of a stack of three levels of the same pattern composed of the same two-layer <a id="_idIndexMarker370"/>building block:</p>
			<ul>
				<li><strong class="source-inline">Conv2D</strong>, a two-dimensional convolutional kernel that is applied to the input and effectively corresponds to having weights shared across all the input neurons. After applying the convolutional kernel, the output is transformed using the ReLU activation function. This structure is replicated for <em class="italic">n</em> hidden planes, with <em class="italic">n</em> being 16 in the first stacked layer and 8 in the second and third stacked layers.</li>
				<li><strong class="source-inline">MaxPooling2D</strong>, which down-samples the inputs by taking the maximum value over the specified window (2x2 in this case).</li>
			</ul>
			<p>Using the Keras API, we can also have an overview of how the layers transformed the inputs using the <strong class="source-inline">Model</strong> class, which converts the tensors into a user-friendly model ready to be used and explored:</p>
			<p class="source-code">Model(input_img, encoded).summary()</p>
			<p>This provides a summary of the encoder network visible in <em class="italic">Figure 3.16</em>:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B16069_03_16.jpg" alt="Figure 3.16 – Overview of the encoder network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Overview of the encoder network</p>
			<p>As can be seen, at<a id="_idIndexMarker371"/> the end of the encoding phase, we have a (4, 4, 8) tensor, which is more than six times smaller than our original initial inputs (28x28). We can now build the decoder network. Note that the encoder and decoder do not need to have the same structure and/or shared weights: </p>
			<p class="source-code">x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)</p>
			<p class="source-code">x = UpSampling2D((2, 2))(x)</p>
			<p class="source-code">x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)</p>
			<p class="source-code">x = UpSampling2D((2, 2))(x)</p>
			<p class="source-code">x = Conv2D(16, (3, 3), activation='relu')(x)</p>
			<p class="source-code">x = UpSampling2D((2, 2))(x)</p>
			<p class="source-code">decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) </p>
			<p>In this case, the decoder network resembles the encoder structure where the down-sampling of the input achieved using the <strong class="source-inline">MaxPooling2D</strong> layer has been replaced by the <strong class="source-inline">UpSampling2D</strong> layer, which basically repeats the input over a specified window (2x2 in this case, effectively doubling the tensor in each direction).</p>
			<p>We have now fully defined the network structure with the encoder and decoder layers. In order to completely specify our autoencoder, we also need to specify a loss function. Moreover, to build the <a id="_idIndexMarker372"/>computational graph, Keras also needs to know which algorithms should be used in order to optimize the network weights. Both bits of information, the loss function and optimizer to be used, are generally provided to Keras when <em class="italic">compiling</em> the model:</p>
			<p class="source-code">autoencoder = Model(input_img, decoded)</p>
			<p class="source-code">autoencoder.compile(optimizer='adam', loss='binary_crossentropy')</p>
			<p>We can now finally train our autoencoder. Keras <strong class="source-inline">Model</strong> classes provide APIs that are similar to scikit-learn, with a <strong class="source-inline">fit</strong> method to be used to train the neural network. Note that, owing to the nature of the autoencoder, we are using the same information as the input and output of our network:</p>
			<p class="source-code">autoencoder.fit(x_train, x_train,</p>
			<p class="source-code">                epochs=50,</p>
			<p class="source-code">                batch_size=128,</p>
			<p class="source-code">                shuffle=True,</p>
			<p class="source-code">                validation_data=(x_test, x_test))</p>
			<p>Once the training is finished, we can examine the ability of the network to reconstruct the inputs by comparing input images with their reconstructed version, which can be easily computed using the <strong class="source-inline">predict</strong> method of the Keras <strong class="source-inline">Model</strong> class as follows:</p>
			<p class="source-code">decoded_imgs = autoencoder.predict(x_test)</p>
			<p>In <em class="italic">Figure 3.17</em>, we show the reconstructed images. As you can see, the network is quite good at reconstructing unseen images, especially when considering the large-scale features. Details might have been lost in the compression (see, for instance, the logo on the t-shirts) but the overall relevant information has indeed been captured by our network: </p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B16069_03_17.jpg" alt="Figure 3.17 – Examples of the reconstruction done on the test set by the trained autoencoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Examples of the reconstruction done on the test set by the trained autoencoder</p>
			<p>It can also be very <a id="_idIndexMarker373"/>interesting to represent the encoded version of the images in a two-dimensional plane using T-SNE:</p>
			<p class="source-code">from tensorflow.keras.layers import Flatten</p>
			<p class="source-code">embed_layer = Flatten()(encoded)</p>
			<p class="source-code">embeddings = Model(input_img, embed_layer).predict(x_test)</p>
			<p class="source-code">tsne = TSNE(n_components=2)</p>
			<p class="source-code">emb2d = tsne.fit_transform(embeddings)</p>
			<p class="source-code">x, y = np.squeeze(emb2d[:, 0]), np.squeeze(emb2d[:, 1])</p>
			<p>The coordinates provided by T-SNE are shown in <em class="italic">Figure 3.18</em>, colored by the class the sample belongs to. The clustering of the different clothing can clearly be seen, particularly for some classes that are very well separated from the rest:</p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B16069_03_18.jpg" alt="Figure 3.18 – T-SNE transformation of the embeddings extracted from the test set, colored by the class that the sample belongs to"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – T-SNE transformation of the embeddings extracted from the test set, colored by the class that the sample belongs to</p>
			<p>Autoencoders <a id="_idIndexMarker374"/>are, however, rather prone to overfitting, as they tend to re-create exactly the images of the training and not generalize well. In the following subsection, we will see how overfitting can be prevented in order to build more robust and reliable dense representations. </p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor056"/>Denoising autoencoders</h2>
			<p>Besides allowing <a id="_idIndexMarker375"/>us to compress a sparse representation into a denser vector, autoencoders are also widely used to process a signal in order to filter out noise and extract only a relevant (characteristic) signal. This can be very useful in many applications, especially when identifying anomalies and outliers. </p>
			<p>Denoising autoencoders are a small variation of what has been implemented. As described in the previous section, basic autoencoders are trained using the same image as input and output. Denoising autoencoders corrupt the input using some noise of various intensity, while keeping the same noise-free target. This could be achieved by simply adding some Gaussian noise to the inputs:</p>
			<p class="source-code">noise_factor = 0.1</p>
			<p class="source-code">x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) </p>
			<p class="source-code">x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) </p>
			<p class="source-code">x_train_noisy = np.clip(x_train_noisy, 0., 1.)</p>
			<p class="source-code">x_test_noisy = np.clip(x_test_noisy, 0., 1.)</p>
			<p>The network can then<a id="_idIndexMarker376"/> be trained using the corrupted input, while for the output the noise-free image is used:</p>
			<p class="source-code">noisy_autoencoder.fit(x_train_noisy, x_train,</p>
			<p class="source-code">                epochs=50,</p>
			<p class="source-code">                batch_size=128,</p>
			<p class="source-code">                shuffle=True,</p>
			<p class="source-code">                validation_data=(x_test_noisy, x_test))</p>
			<p>Such an approach is generally valid when datasets are large and when the risk of overfitting the noise is rather limited. When datasets are smaller, an alternative to avoid the network "learning" the noise as well (thus learning the mapping between a static noisy image to its noise-free version) is to add training stochastic noise using a <strong class="source-inline">GaussianNoise</strong> layer. </p>
			<p>Note that in this way, the noise may change between epochs and prevent the network from learning a static corruption superimposed to our training set. In order to do so, we change the first layers of our network in the following way:</p>
			<p class="source-code">input_img = Input(shape=(28, 28, 1))</p>
			<p class="source-code">noisy_input = GaussianNoise(0.1)(input_img)</p>
			<p class="source-code">x = Conv2D(16, (3, 3), activation='relu', padding='same')(noisy_input)</p>
			<p>The difference is that instead of having statically corrupted samples (that do not change in time), the noisy inputs now keep changing between epochs, thus avoiding the network learning the noise as well. </p>
			<p>The <strong class="source-inline">GaussianNoise</strong> layer is an example of a regularization layer, that is, a layer that helps reduce overfitting of a neural network by inserting a random part in the network. <strong class="source-inline">GaussianNoise</strong> layers make models more robust and able to generalize better, avoiding autoencoders learning the identity function.</p>
			<p>Another common example of a regularization layer is the dropout layers that effectively set to 0 some of the inputs (at random with a probability, <img src="image/B16069_03_087.png" alt=""/>) and rescale the other inputs by a <img src="image/B16069_03_088.png" alt=""/> factor in order to (statistically) keep the sum over all the units constant, with and<a id="_idIndexMarker377"/> without dropout. </p>
			<p>Dropout corresponds to randomly killing some of the connections between layers in order to reduce output dependence to specific neurons. You need to keep in mind that regularization layers are only active at training, while at test time they simply correspond to identity layers. </p>
			<p>In <em class="italic">Figure 3.19</em>, we compare the network reconstruction of a noisy input (input) for the previous unregularized trained network and the network with a <strong class="source-inline">GaussianNoise</strong> layer. As can be seen (compare, for instance, the images of trousers), the model with regularization tends to develop stronger robustness and reconstructs the noise-free outputs:</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B16069_03_19.jpg" alt="Figure 3.19 – Comparison with reconstruction for noisy samples. Top row: noisy input; middle row: reconstructed output using a vanilla autoencoder; bottom row: reconstructed output using a denoising autoencoder"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19 – Comparison with reconstruction for noisy samples. Top row: noisy input; middle row: reconstructed output using a vanilla autoencoder; bottom row: reconstructed output using a denoising autoencoder</p>
			<p>Regularization layers <a id="_idIndexMarker378"/>are often used when dealing with deep neural networks that tend to overfit and are able to learn identity functions for autoencoders. Often, dropout or <strong class="source-inline">GaussianNoise</strong> layers are introduced, repeating a similar pattern composed of <a id="_idIndexMarker379"/>regularization and learnable layers that we usually refer to as <strong class="bold">stacked denoising layers</strong>.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor057"/>Graph autoencoders</h2>
			<p>Once the basic<a id="_idIndexMarker380"/> concepts of autoencoders are understood, we can now turn to<a id="_idIndexMarker381"/> apply this framework to graph structures. If on one hand the network structure, decomposed into an encoder-decoder structure with a low-dimensional representation in between, still applies, the definition of the loss function to be optimized needs a bit of caution when dealing with networks. First, we need to adapt the reconstruction error to a meaningful formulation that can adapt to the peculiarities of graph structures. But to do so, let's first introduce the concepts of first- and higher-order proximity.</p>
			<p>When applying autoencoders to graph structures, the input and output of the network should be a graph representation, as, for instance, the adjacency matrix. The reconstruction loss could then be defined as the Frobenius norm of the difference between the input and output matrices. However, when applying autoencoders to such graph structures and adjacency matrices, two critical issues arise:</p>
			<ul>
				<li>Whereas the presence of links indicates a relation or similarity between two vertices, their absence does not generally indicate a dissimilarity between vertices.</li>
				<li>The adjacency matrix is extremely sparse and therefore the model will naturally tend to predict a 0 rather than a positive value.</li>
			</ul>
			<p>To address such peculiarities of graph structures, when defining the reconstruction loss, we need to penalize more errors<a id="_idIndexMarker382"/> done for the non-zero elements rather than that for zero elements. This <a id="_idIndexMarker383"/>can be done using the following loss function:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/B16069_03_089.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_090.png" alt=""/> is the Hadamard element-wise product, where <img src="image/B16069_03_091.png" alt=""/> if there is an edge between nodes <img src="image/B16069_03_092.png" alt=""/> and <img src="image/B16069_03_093.png" alt=""/>, and 0 otherwise. The preceding loss guarantees that vertices that share a neighborhood (that is, their adjacency vectors are similar) will also be close in the embedding space. Thus, the preceding formulation will naturally preserve second-order proximity for the reconstructed graph. </p>
			<p>On the other hand, you can also promote first-order proximity in the reconstructed graph, thus enforcing connected nodes to be close in the embedding space. This condition can be enforced by using the following loss:</p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B16069_03_094.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_095.png" alt=""/> and <img src="image/B16069_03_096.png" alt=""/> are the two representation of nodes <img src="image/B16069_03_097.png" alt=""/> and <img src="image/B16069_03_098.png" alt=""/> in the embedding space. This loss function forces neighboring nodes to be close in the embedding space. In fact, if two nodes are tightly connected, <img src="image/B16069_03_099.png" alt=""/> will be large. As a consequence, their difference in the embedding space, <img src="image/B16069_03_100.png" alt=""/>, should be limited (indicating the two nodes are close in the embedding space) to keep the loss function small. The two losses can also be combined into a single <a id="_idIndexMarker384"/>loss function, where, in order to prevent overfitting, a regularization loss can<a id="_idIndexMarker385"/> be added that is proportional to the norm of the weight coefficients:</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/B16069_03_101.jpg" alt=""/>
				</div>
			</div>
			<p>In the preceding equation, <em class="italic">W</em> represents all the weights used across the network. The preceding <a id="_idIndexMarker386"/>formulation was proposed in 2016 by Wang et al., and it is now known as <strong class="bold">Structural Deep Network Embedding</strong> (<strong class="bold">SDNE</strong>). </p>
			<p>Although the preceding loss could also be directly implemented with TensorFlow and Keras, you can already find this network integrated in the GEM package we referred to previously. As before, extracting the node embedding can be done similarly in a few lines of code, as follows:</p>
			<p class="source-code">G=nx.karate_club_graph()</p>
			<p class="source-code">sdne=SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6,</p>
			<p class="source-code">          K=3,n_units=[50, 15,], rho=0.3, n_iter=10, </p>
			<p class="source-code">          xeta=0.01,n_batch=100,</p>
			<p class="source-code">          modelfile=['enc_model.json','dec_model.json'],</p>
			<p class="source-code">          weightfile=['enc_weights.hdf5','dec_weights.hdf5'])</p>
			<p class="source-code">sdne.learn_embedding(G)</p>
			<p class="source-code">embeddings = m1.get_embedding()</p>
			<p>Although very<a id="_idIndexMarker387"/> powerful, these graph autoencoders encounter some issues <a id="_idIndexMarker388"/>when dealing with large graphs. For these cases, the input of our autoencoder is one row of the adjacency matrix that has as many elements as the nodes in the network. In large networks, this size can easily be of the order of millions or tens of millions. </p>
			<p>In the next section, we describe a different strategy for encoding the network information that in some cases may iteratively aggregate embeddings only over local neighborhoods, making it scalable to large graphs.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor058"/>Graph neural networks</h1>
			<p><strong class="bold">GNNs</strong> are deep <a id="_idIndexMarker389"/>learning methods that work on graph-structured data. This family of methods is also known <a id="_idIndexMarker390"/>as <strong class="bold">geometric deep learning</strong> and is gaining increasing interest in a variety of applications, including social network analysis and computer graphics.</p>
			<p>According to the taxonomy defined in <a href="B16069_02_Final_JM_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Graph Machine Learning</em>, the encoder part takes as input both the graph structure and the node features. Those algorithms can be trained either with or without supervision. In this chapter, we will focus on unsupervised training, while the supervised setting will be explored in <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>. </p>
			<p>If you are familiar with<a id="_idIndexMarker391"/> the concept of a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>), you might already know that they are able to achieve impressive results when dealing with regular Euclidean spaces, such as text (one-dimensional), images (two-dimensional), and videos (three-dimensional). A classic CNN consists of a sequence of layers and each layer extracts multi-scale localized spatial features. Those features are exploited by deeper layers to construct more complex and highly expressive representations.</p>
			<p>In recent years, it has been observed that concepts such as multi-layer and locality are also useful for processing graph-structured data. However, graphs are defined over a <em class="italic">non-Euclidean space</em>, and finding a generalization of a CNN for graphs is not straightforward, as described in <em class="italic">Figure 3.20</em>:</p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/B16069_03_20.jpg" alt="Figure 3.20 – Visual difference between Euclidean and non-Euclidean neighborhoods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Visual difference between Euclidean and non-Euclidean neighborhoods</p>
			<p>The original <a id="_idIndexMarker392"/>formulation of GNN was proposed by Scarselli et al. back in 2009. It relies on the fact that each node can be described by its features and its neighborhood. Information coming from the neighborhood (which represents the concept of locality in the graph domain) can be aggregated and used to compute more complex and high-level features. Let's understand in more detail how it can be done.</p>
			<p>At the beginning, each node, <img src="image/B16069_03_102.png" alt=""/>, is associated with a state. Let's start with a random embedding, <img src="image/B16069_03_103.png" alt=""/> (ignoring node attributes for simplicity). At each iteration of the algorithm, nodes accumulate input from their neighbors using a simple neural network layer:</p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/B16069_03_104.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_105.png" alt=""/> and <img src="image/B16069_03_106.png" alt=""/> are trainable parameters (where <em class="italic">d</em> is the dimension of the embedding), <img src="image/B16069_03_107.png" alt=""/> is a non-linear function, and <em class="italic">t</em> represents the <em class="italic">t</em>th iteration of the algorithm. The equation is applied recursively until a particular objective is reached. Notice that, at each<a id="_idIndexMarker393"/> iteration, the <em class="italic">previous state</em> (the state computed at the previous iteration) is exploited in order to compute that the new state has happened with <em class="italic">recurrent neural networks</em>.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor059"/>Variants of GNNs</h2>
			<p>Starting from this<a id="_idIndexMarker394"/> first idea, several attempts have been made in recent years to re-address the problem of learning from graph data. In particular, variants of the previously described GNN have been proposed, with the aim of improving its representation learning capability. Some of them are specifically designed to process specific types of graphs (direct, indirect, weighted, unweighted, static, dynamic, and so on). </p>
			<p>Also, several modifications have been proposed for the propagation step (convolution, gate mechanisms, attention mechanisms, and skip connections, among others), with the aim of improving representation at different levels. Also, different training methods have been proposed to improve learning.</p>
			<p>When dealing with unsupervised representation learning, one of the most common approaches is to use an encoder to embed the graph (the encoder is formulated as one of the GNN variants) and then use a simple decoder to reconstruct the adjacency matrix. The loss function is usually formulated as the similarity between the original adjacency matrix and the reconstructed one. Formally, it can be defined as follows:</p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/B16069_03_108.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B16069_03_109.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_110.png" alt=""/> is the adjacency matrix representation and <img src="image/B16069_03_111.png" alt=""/> is the matrix of node attributes. Another common variant of this approach, especially used when dealing with graph classification/representation learning, is to train against a <em class="italic">target distance</em>. The idea is to embed two pairs of graphs simultaneously obtaining a combined representation. The model is then trained such that this representation matches the distance. A similar strategy can be also adopted when dealing with node classification/representation <a id="_idIndexMarker395"/>learning by using a node similarity function.</p>
			<p><strong class="bold">Graph Convolutional Neural Network</strong> (<strong class="bold">GCN</strong>)-based encoders are one of the most diffused variants <a id="_idIndexMarker396"/>of GNN for unsupervised learning. GCNs are GNN models inspired by many of the basic ideas behind CNN. Filter parameters are typically shared over all locations in the graph and several layers are concatenated to form a deep network. </p>
			<p>There are <a id="_idIndexMarker397"/>essentially two types of convolutional operations for graph <a id="_idIndexMarker398"/>data, namely <strong class="bold">spectral approaches</strong> and <strong class="bold">non-spectral</strong> (<strong class="bold">spatial</strong>) approaches. The first, as the name suggests, defines convolution in the spectral domain (that is, decomposing graphs in a combination of simpler elements). Spatial convolution formulates the convolution as aggregating feature information from neighbors. </p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor060"/>Spectral graph convolution</h2>
			<p>Spectral <a id="_idIndexMarker399"/>approaches are related to<a id="_idIndexMarker400"/> spectral graph theory, the study of the characteristics of a graph in relation to the characteristic polynomial, eigenvalues, and eigenvectors of the matrices associated with the graph. The convolution operation is defined as the multiplication of a signal (node features) by a kernel. In more detail, it is defined in the Fourier domain by determining the <em class="italic">eigendecomposition of the graph Laplacian</em> (think about the graph Laplacian as an adjacency matrix normalized in a special way).</p>
			<p>While this definition of spectral convolution has a strong mathematical foundation, the operation is computationally expensive. For this reason, several works have been done to approximate it in an efficient way. ChebNet by Defferrard et al., for instance, is one of the first seminal works on spectral graph convolution. Here, the operation is approximated by using the concept of the Chebyshev polynomial of order <em class="italic">K</em> (a special kind of polynomial used to efficiently approximate functions).</p>
			<p>Here, <em class="italic">K</em> is a very useful parameter because it determines the locality of the filter. Intuitively, for <em class="italic">K</em>=1, only the node features are fed into the network. With <em class="italic">K</em>=2, we average over two-hop neighbors (neighbors of neighbors) and so on.</p>
			<p>Let <img src="image/B16069_03_112.png" alt=""/> be the matrix of node features. In classical neural network processing, this signal would be <a id="_idIndexMarker401"/>composed of layers<a id="_idIndexMarker402"/> of the following form:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B16069_03_113.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_114.png" alt=""/> is the layer weights and <img src="image/B16069_03_115.png" alt=""/> represents some non-linear activation function. The drawback of this operation is that it processes each node signal independently without taking into account connections between nodes. To overcome this limitation, a simple (yet effective) modification can be done, as follows:</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B16069_03_116.jpg" alt=""/>
				</div>
			</div>
			<p>By introducing the adjacency matrix, <img src="image/B16069_03_117.png" alt=""/>, a new linear combination between each node and its corresponding neighbors is added. This way, the information depends only on the neighborhood and parameters are applied on all the nodes, simultaneously.</p>
			<p> It is worth noting that this operation can be repeated in sequence several times, thus creating a deep network. At each layer, the node descriptors, <em class="italic">X</em>, will be replaced with the output of the previous layer, <img src="image/B16069_03_118.png" alt=""/>.</p>
			<p>The preceding presented equation, however, has some limitations and cannot be applied as it stands. The first limitation is that by multiplying by <em class="italic">A</em>, we consider all the neighbors of the node but not the node itself. This problem can be easily overcome by adding self-loops<a id="_idIndexMarker403"/> in<a id="_idIndexMarker404"/> the graph, that is, adding the <img src="image/B16069_03_119.png" alt=""/> identity matrix.</p>
			<p>The second limitation is related to the adjacency matrix itself. Since it is typically not normalized, we will observe large values in the feature representation of high-degree nodes and small values in the feature representation of low-degree nodes. This will lead to several problems during training since optimization algorithms are often sensitive to feature scale. Several methods have been proposed for normalizing <em class="italic">A</em>. </p>
			<p>In Kipf and Welling, 2017 (one of the well-known GCN models), for example, the normalization is performed by multiplying <em class="italic">A</em> by the <em class="italic">diagonal node degree matrix</em> <em class="italic">D</em>, such that all the rows sum to 1: <img src="image/B16069_03_120.png" alt=""/>. More specifically, they used symmetric normalization <img src="image/B16069_03_121.png" alt=""/>, such that the proposed propagation rule becomes as follows:</p>
			<div>
				<div id="_idContainer281" class="IMG---Figure">
					<img src="image/B16069_03_122.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B16069_03_123.png" alt=""/> is the diagonal node degree matrix of <img src="image/B16069_03_124.png" alt=""/>.</p>
			<p>In the following <a id="_idIndexMarker405"/>example, we will <a id="_idIndexMarker406"/>create a GCN as defined in Kipf and Welling and we will apply this propagation rule for embedding a well-known network: a Zachary's karate club graph:</p>
			<ol>
				<li value="1">To begin, it is necessary to import all the Python modules. We will use <strong class="source-inline">networkx</strong> to load the <em class="italic">barbell graph</em>:<p class="source-code">import networkx as nx</p><p class="source-code">import numpy as np</p><p class="source-code">G = nx.barbell_graph(m1=10,m2=4)</p></li>
				<li>To implement the GC propagation rule, we need an adjacency matrix representing <strong class="source-inline">G</strong>. Since this network does not have node features, we will use the <img src="image/B16069_03_125.png" alt=""/> identity matrix as the node descriptor:<p class="source-code">A = nx.to_numpy_matrix(G)</p><p class="source-code"> I = np.eye(G.number_of_nodes())</p></li>
				<li>We now add the self-loop and prepare the diagonal node degree matrix:<p class="source-code">from scipy.linalg import sqrtm</p><p class="source-code">A_hat = A + I</p><p class="source-code">D_hat = np.array(np.sum(A_hat, axis=0))[0]</p><p class="source-code"> D_hat = np.array(np.diag(D_hat))</p><p class="source-code"> D_hat = np.linalg.inv(sqrtm(D_hat))</p><p class="source-code"> A_norm = D_hat @ A_hat @ D_hat</p></li>
				<li>Our GCN will be composed of two layers. Let's define the layers' weights and the propagation<a id="_idIndexMarker407"/> rule. Layer <a id="_idIndexMarker408"/>weights, <em class="italic">W</em>, will be initialized using <em class="italic">Glorot uniform initialization</em> (even if other initialization methods can be also used, for example, by sampling from a Gaussian or uniform distribution):<p class="source-code">def glorot_init(nin, nout):</p><p class="source-code">     sd = np.sqrt(6.0 / (nin + nout))</p><p class="source-code">     return np.random.uniform(-sd, sd, size=(nin, nout))</p><p class="source-code">class GCNLayer():</p><p class="source-code">  def __init__(self, n_inputs, n_outputs):</p><p class="source-code">      self.n_inputs = n_inputs</p><p class="source-code">      self.n_outputs = n_outputs</p><p class="source-code">      self.W = glorot_init(self.n_outputs, self.n_inputs)</p><p class="source-code">      self.activation = np.tanh</p><p class="source-code">  def forward(self, A, X):</p><p class="source-code">      self._X = (A @ X).T</p><p class="source-code">      H = self.W @ self._X </p><p class="source-code">      H = self.activation(H)</p><p class="source-code">      return H.T # (n_outputs, N)</p></li>
				<li>Finally, let's create our network and compute the forward pass, that is, propagate the signal through the network:<p class="source-code">gcn1 = GCNLayer(G.number_of_nodes(), 8)</p><p class="source-code"> gcn2 = GCNLayer(8, 4)</p><p class="source-code"> gcn3 = GCNLayer(4, 2)</p><p class="source-code">H1 = gcn1.forward(A_norm, I)</p><p class="source-code"> H2 = gcn2.forward(A_norm, H1)</p><p class="source-code">H3 = gcn3.forward(A_norm, H2)</p></li>
			</ol>
			<p><strong class="source-inline">H3</strong> now contains<a id="_idIndexMarker409"/> the embedding computed<a id="_idIndexMarker410"/> using the GCN propagation rule. Notice that we chose <strong class="source-inline">2</strong> as the number of outputs, meaning that the embedding is bi-dimensional and can be easily visualized. In <em class="italic">Figure 3.21</em>, you can see the output:</p>
			<div>
				<div id="_idContainer285" class="IMG---Figure">
					<img src="image/B16069_03_21.jpg" alt="Figure 3.21 – Application of the graph convolutional layer to a graph (left) to generate the embedding vector of its nodes (right)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – Application of the graph convolutional layer to a graph (left) to generate the embedding vector of its nodes (right)</p>
			<p>You can observe the presence of two quite well-separated communities. This is a nice result, considering that we have not trained the network yet!</p>
			<p>Spectral graph convolution methods have achieved noteworthy results in many domains. However, they present some drawbacks. Consider, for example, a very big graph with billions of nodes: a spectral approach requires the graph to be processed simultaneously, which can be impractical from a computational point of view. </p>
			<p>Furthermore, spectral convolution often assumes a fixed graph, leading to poor generalization capabilities on new, different graphs. To overcome these issues, spatial graph <a id="_idIndexMarker411"/>convolution represents an<a id="_idIndexMarker412"/> interesting alternative.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor061"/>Spatial graph convolution</h2>
			<p>Spatial graph<a id="_idIndexMarker413"/> convolutional <a id="_idIndexMarker414"/>networks perform the operations directly on the graph by aggregating information from spatially close neighbors. Spatial convolution has many advantages: weights can be easily shared across a different location of the graph, leading to a good generalization capability on different graphs. Furthermore, the computation can be done by considering subsets of nodes instead of the entire graph, potentially improving computational efficiency.</p>
			<p>GraphSAGE is one of the algorithms that implement spatial convolution. One of the main characteristics is its ability to scale over various types of networks. We can think of GraphSAGE as composed of three steps:</p>
			<ol>
				<li value="1"><strong class="bold">Neighborhood sampling</strong>: For <a id="_idIndexMarker415"/>each node in a graph, the first step is to find its k-neighborhood, where <em class="italic">k</em> is defined by the user for determining how many hops to consider (neighbors of neighbors).</li>
				<li><strong class="bold">Aggregation</strong>: The second<a id="_idIndexMarker416"/> step is to aggregate, for each node, the node features describing the respective neighborhood. Various types of aggregation can be performed, including average, pooling (for example, taking the best feature according to certain criteria), or an even more complicated operation, such as using recurrent units (such as LSTM).</li>
				<li><strong class="bold">Prediction</strong>: Each node is equipped with a simple neural network that learns how to perform predictions <a id="_idIndexMarker417"/>based on the aggregated features from the neighbors.</li>
			</ol>
			<p>GraphSAGE is often used in supervised settings, as we will see in <a href="B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064"><em class="italic">Chapter 4</em></a>, <em class="italic">Supervised Graph Learning</em>. However, by adopting strategies such as using a similarity function as the target distance, it can also be effective for learning embedding without explicitly supervising the task.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor062"/>Graph convolution in practice</h2>
			<p>In practice, GNNs <a id="_idIndexMarker418"/>have been implemented in many machine learning and deep learning frameworks, including TensorFlow, Keras, and PyTorch. For the next example, we will be using StellarGraph, the Python library for machine learning on graphs.</p>
			<p>In the following example, we will learn about embedding vectors in an unsupervised manner, without a target variable. The method is inspired by Bai et al. 2019 and is based on the simultaneous embedding of pairs of graphs. This embedding should match a ground-truth distance between graphs:</p>
			<ol>
				<li value="1">First, let's load the required Python modules:<p class="source-code">import numpy as np</p><p class="source-code">import stellargraph as sg</p><p class="source-code">from stellargraph.mapper import FullBatchNodeGenerator</p><p class="source-code">from stellargraph.layer import GCN</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers, optimizers, losses, metrics, Model</p></li>
				<li>We will be using the <strong class="source-inline">PROTEINS</strong> dataset for this example, which is available in StellarGraph and consists of 1,114 graphs with 39 nodes and 73 edges on average for each graph. Each node is described by four attributes and belongs to one of two classes:<p class="source-code">dataset = sg.datasets.PROTEINS()</p><p class="source-code">graphs, graph_labels = dataset.load()</p></li>
				<li>The next step is to create the model. It will be composed of two GC layers with 64 and 32 output dimensions followed by ReLU activation, respectively. The output will be computed as the Euclidean distance of the two embeddings:<p class="source-code">generator = sg.mapper.PaddedGraphGenerator(graphs)</p><p class="source-code"> </p><p class="source-code"># define a GCN model containing 2 layers of size 64 and 32, respectively. </p><p class="source-code"># ReLU activation function is used to add non-linearity between layers</p><p class="source-code">gc_model = sg.layer.GCNSupervisedGraphClassification(</p><p class="source-code"> [64, 32], ["relu", "relu"], generator, pool_all_layers=True)</p><p class="source-code"># retrieve the input and the output tensor of the GC layer such that they can be connected to the next layer</p><p class="source-code">inp1, out1 = gc_model.in_out_tensors()</p><p class="source-code">inp2, out2 = gc_model.in_out_tensors()</p><p class="source-code">vec_distance = tf.norm(out1 - out2, axis=1)</p><p class="source-code"> </p><p class="source-code"># create the model. It is also useful to create a specular model in order to easily retrieve the embeddings</p><p class="source-code">pair_model = Model(inp1 + inp2, vec_distance)</p><p class="source-code"> embedding_model = Model(inp1, out1)</p></li>
				<li>It is now time<a id="_idIndexMarker419"/> to prepare the dataset for training. To each pair of input graphs, we will assign a similarity score. Notice that any notion of graph similarity can be used in this case, including graph edit distances. For simplicity, we will be using the distance between the spectrum of the Laplacian of the graphs:<p class="source-code">def graph_distance(graph1, graph2):</p><p class="source-code">   spec1 = nx.laplacian_spectrum(graph1.to_networkx(feature_attr=None))</p><p class="source-code">   spec2 = nx.laplacian_spectrum(graph2.to_networkx(feature_attr=None))</p><p class="source-code">   k = min(len(spec1), len(spec2))</p><p class="source-code">   return np.linalg.norm(spec1[:k] - spec2[:k])</p><p class="source-code">graph_idx = np.random.RandomState(0).randint(len(graphs), size=(100, 2))</p><p class="source-code">targets = [graph_distance(graphs[left], graphs[right]) for left, right in graph_idx]</p><p class="source-code">train_gen = generator.flow(graph_idx, batch_size=10, targets=targets)</p></li>
				<li>Finally, let's <a id="_idIndexMarker420"/>compile and train the model. We will be using an adaptive moment estimation optimizer (Adam) with the learning rate parameter set to <strong class="source-inline">1e-2</strong>. The loss function we will be using is defined as the minimum squared error between the prediction and the ground-truth distance computed as previously. The model will be trained for 500 epochs:<p class="source-code">pair_model.compile(optimizers.Adam(1e-2), loss="mse")</p><p class="source-code">pair_model.fit(train_gen, epochs=500, verbose=0)</p></li>
				<li>After training, we are now ready to inspect and visualize the learned representation. Since the output is 32-dimensional, we need a way to qualitatively evaluate the embeddings, for example, by plotting them in a bi-dimensional space. We will use T-SNE for this purpose:<p class="source-code"># retrieve the embeddings</p><p class="source-code">embeddings = embedding_model.predict(generator.flow(graphs))</p><p class="source-code"># TSNE is used for dimensionality reduction</p><p class="source-code">from sklearn.manifold import TSNE</p><p class="source-code">tsne = TSNE(2)</p><p class="source-code"> two_d = tsne.fit_transform(embeddings)</p></li>
			</ol>
			<p>Let's plot the embeddings. In the plot, each point (embedded graph) is colored according to the<a id="_idIndexMarker421"/> corresponding label (blue=0, red=1). The results are visible in <em class="italic">Figure 3.22</em>:</p>
			<div>
				<div id="_idContainer286" class="IMG---Figure">
					<img src="image/B16069_03_22.jpg" alt="Figure 3.22 – The PROTEINS dataset embedding using GCNs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – The PROTEINS dataset embedding using GCNs</p>
			<p>This is just one of the possible methods for learning embeddings for graphs. More advanced solutions can be experimented with to better fit the problem of interest.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor063"/>Summary </h1>
			<p>In this chapter, we have learned how unsupervised machine learning can be effectively applied to graphs to solve real problems, such as node and graph representation learning.</p>
			<p>In particular, we first analyzed shallow embedding methods, a set of algorithms that are able to learn and return only the embedding values for the learned input data.</p>
			<p>We then learned how autoencoder algorithms can be used to encode the input by preserving important information in a lower-dimensional space. We have also seen how this idea can be adapted to graphs, by learning about embeddings that allow us to reconstruct the pair-wise node/graph similarity.</p>
			<p>Finally, we introduced the main concepts behind GNNs. We have seen how well-known concepts, such as convolution, can be applied to graphs.</p>
			<p>In the next chapter, we will revise these concepts in a supervised setting. There, a target label is provided and the objective is to learn a mapping between the input and the output.</p>
		</div>
	</body></html>