- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating Model Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When only the wealthy could afford education, tests and exams were not used
    to evaluate students. Instead, tests evaluated the teachers for parents who wanted
    to know whether their children learned enough to justify the instructors’ wages.
    Obviously, this is different today. Now, such evaluations are used to distinguish
    between high-achieving and low-achieving students, filtering them into careers
    and other opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: Given the significance of this process, a great deal of effort is invested in
    developing accurate student assessments. Fair assessments have a large number
    of questions that cover a wide breadth of topics and reward true knowledge over
    lucky guesses. A good assessment also requires students to think about problems
    they have never faced before. Correct responses, therefore, reflect an ability
    to generalize knowledge more broadly.
  prefs: []
  type: TYPE_NORMAL
- en: The process of evaluating machine learning algorithms is very similar to the
    process of evaluating students. Since algorithms have varying strengths and weaknesses,
    tests should distinguish between learners. It is also important to understand
    how a learner will perform on future data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter provides the information needed to assess machine learners, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: The reasons why predictive accuracy is not sufficient to measure performance,
    and the performance measures you might use instead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods to ensure that the performance measures reasonably reflect a model’s
    ability to predict or forecast unseen cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use R to apply these more useful measures and methods to the predictive
    models covered in previous chapters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as the best way to learn a topic is to attempt to teach it to someone else,
    the process of teaching and evaluating machine learners will provide you with
    greater insight into the methods you’ve learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we measured classifier accuracy by dividing the number
    of correct predictions by the total number of predictions. This finds the proportion
    of cases in which the learner is correct, and the proportion of incorrect cases
    follows directly. For example, suppose that a classifier correctly predicted whether
    newborn babies were a carrier of a treatable but potentially fatal genetic defect
    in 99,990 out of 100,000 cases. This would imply an accuracy of 99.99 percent
    and an error rate of only 0.01 percent.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this appears to be an extremely valuable classifier. However,
    it would be wise to collect additional information before trusting a child’s life
    to the test. What if the genetic defect is found in only 10 out of every 100,000
    babies? A test that invariably predicts no defect will be correct for 99.99 percent
    of all cases, but incorrect for 100 percent of the cases that matter most. In
    other words, even though the classifier is extremely accurate, it is not very
    useful for preventing treatable birth defects.
  prefs: []
  type: TYPE_NORMAL
- en: This is one consequence of the **class imbalance problem**, which refers to
    the trouble associated with data having a large majority of records belonging
    to a single class.
  prefs: []
  type: TYPE_NORMAL
- en: Though there are many ways to measure a classifier’s performance, the best measure
    is always that which captures whether the classifier is successful at its intended
    purpose. It is crucial to define performance measures in terms of utility rather
    than raw accuracy. To this end, we will explore a variety of alternative performance
    measures derived from the confusion matrix. Before we get started, however, we
    need to consider how to prepare a classifier for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a classifier’s predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of evaluating a classification model is to better understand how its
    performance will extrapolate to future cases. Since it is usually infeasible to
    test an unproven model in a live environment, we typically simulate future conditions
    by asking the model to classify cases in a dataset made of cases that resemble
    what it will be asked to do in the future. By observing the learner’s responses
    to this examination, we can learn about its strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though we’ve evaluated classifiers in prior chapters, it’s worth reflecting
    on the types of data at our disposal:'
  prefs: []
  type: TYPE_NORMAL
- en: Actual class values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicted class values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimated probability of the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The actual and predicted class values may be self-evident, but they are the
    key to the evaluation. Just like a teacher uses an answer key—a list of correct
    answers—to assess the student’s answers, we need to know the correct answer for
    a machine learner’s predictions. The goal is to maintain two vectors of data:
    one holding the correct or actual class values, and the other holding the predicted
    class values. Both vectors must have the same number of values stored in the same
    order. The predicted and actual values may be stored as separate R vectors or
    as columns in a single R data frame.'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining this data is easy. The actual class values come directly from the
    target in the test dataset. Predicted class values are obtained from the classifier
    built upon the training data, which is then applied to the test data. For most
    machine learning packages, this involves applying the `predict()` function to
    a model object and a data frame of test data, such as `predictions <- predict(model,
    test_data)`.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have only examined classification predictions using these two
    vectors of data, but most models can supply another piece of useful information.
    Even though the classifier makes a single prediction about each example, it may
    be more confident about some decisions than others.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a classifier may be 99 percent certain that an SMS with the words
    “free” and “ringtones” is spam, but only 51 percent certain that an SMS with the
    word “tonight” is spam. In both cases, the classifier classifies the message as
    spam, but it is far more certain about one decision than the other.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17290_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Learners may differ in their prediction confidence even when trained
    on the same data'
  prefs: []
  type: TYPE_NORMAL
- en: Studying these internal prediction probabilities provides useful data for evaluating
    a model’s performance. If two models make the same number of mistakes, but one
    is more able to accurately assess its uncertainty, then it is a smarter model.
    It’s ideal to find a learner that is extremely confident when making a correct
    prediction, but timid in the face of doubt. The balance between confidence and
    caution is a key part of model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: The function call to obtain the internal prediction probabilities varies across
    R packages. For most classifiers, the `predict()` function allows an additional
    parameter to specify the desired type of prediction. To obtain a single predicted
    class, such as spam or ham, you typically set the `type = "class"` parameter.
    To obtain the prediction probability, the `type` parameter should be set to one
    of `"prob"`, `"posterior"`, `"raw"`, or `"probability"`, depending on the classifier
    used.
  prefs: []
  type: TYPE_NORMAL
- en: All classifiers presented in this book can provide prediction probabilities.
    The correct setting for the `type` parameter is included in the syntax box introducing
    each model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to output the predicted probabilities for the C5.0 classifier
    built in *Chapter 5*, *Divide and Conquer – Classification Using Decision Trees
    and Rules*, use the `predict()` function with `type = "prob"` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To output the Naive Bayes predicted probabilities for the SMS spam classification
    model developed in *Chapter 4*, *Probabilistic Learning – Classification Using
    Naive Bayes*, use `predict()` with `type = "raw"` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In most cases, the `predict()` function returns a probability for each category
    of the outcome. For example, in the case of a two-outcome model like the SMS classifier,
    the predicted probabilities might be stored in a matrix or data frame, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each line in this output shows the classifier’s predicted probability of spam
    and ham. According to probability rules, the sum of the probabilities across each
    row is 1 because these are mutually exclusive and exhaustive outcomes. For convenience,
    during the evaluation process, it can be helpful to construct a data frame collecting
    the predicted class, the actual class, and the predicted probability of the class
    level (or levels) of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sms_results.csv` file available in the GitHub repository for this chapter
    is an example of a data frame in exactly this format and is built from the predictions
    of the SMS classifier built in *Chapter 4*, *Probabilistic Learning – Classification
    Using Naive Bayes*. The steps required to construct this evaluation dataset have
    been omitted for brevity, so to follow along with the example here, simply download
    the file and load it into a data frame using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `sms_results` data frame is simple. It contains four vectors
    of 1,390 values. One column contains values indicating the actual type of SMS
    message (spam or ham), another indicates the Naive Bayes model’s predicted message
    type, and the third and fourth columns indicate the probability that the message
    was spam or ham, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For these six test cases, the predicted and actual SMS message types agree;
    the model predicted their statuses correctly. Furthermore, the prediction probabilities
    suggest that the model was extremely confident about these predictions because
    they all fall close to or are exactly 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens when the predicted and actual values are further from 0 and 1?
    Using the `subset()` function, we can identify a few of these records. The following
    output shows test cases where the model estimated the probability of spam as being
    between 40 and 60 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'By the model’s own estimation, these were cases in which a correct prediction
    was virtually a coin flip. Yet all three predictions were wrong—an unlucky result.
    Let’s look at a few more cases where the model was wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These cases illustrate the important fact that a model can be extremely confident
    and yet it can still be extremely wrong. All six of these test cases were spam
    messages that the classifier believed to have no less than a 98 percent chance
    of being ham.
  prefs: []
  type: TYPE_NORMAL
- en: Despite such mistakes, is the model still useful? We can answer this question
    by applying various error metrics to this evaluation data. In fact, many such
    metrics are based on a tool we’ve already used extensively in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at confusion matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **confusion matrix** is a table that categorizes predictions according to
    whether they match the actual value. One of the table’s dimensions indicates the
    possible categories of predicted values, while the other dimension indicates the
    same thing for actual values. Although we have mostly worked with 2x2 confusion
    matrices so far, a matrix can be created for models that predict any number of
    class values. The following figure depicts the familiar confusion matrix for a
    two-class binary model, as well as the 3x3 confusion matrix for a three-class
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the predicted value is the same as the actual value, this is a correct
    classification. Correct predictions fall on the diagonal in the confusion matrix
    (denoted by **O**). The off-diagonal matrix cells (denoted by **X**) indicate
    the cases where the predicted value differs from the actual value. These are incorrect
    predictions. Performance measures for classification models are based on the counts
    of predictions falling on and off the diagonal in these tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Confusion matrices count cases where the predicted class agrees
    or disagrees with the actual value'
  prefs: []
  type: TYPE_NORMAL
- en: The most common performance measures consider the model’s ability to discern
    one class versus all others. The class of interest is known as the **positive**
    class, while all others are known as **negative**.
  prefs: []
  type: TYPE_NORMAL
- en: The use of the terms positive and negative is not intended to imply any value
    judgment (that is, good versus bad), nor does it necessarily suggest that the
    outcome is present or absent (such as there being a birth defect versus there
    not being one). The choice of the positive outcome can even be arbitrary, as in
    cases where a model is predicting categories such as sunny versus rainy, or dog
    versus cat.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between positive class and negative class predictions can
    be depicted as a 2x2 confusion matrix that tabulates whether predictions fall
    into one of four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive** (**TP**): Correctly classified as the class of interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative** (**TN**): Correctly classified as not the class of interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive** (**FP**): Incorrectly classified as the class of interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative** (**FN**): Incorrectly classified as not the class of interest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the spam classifier, the positive class is spam, as this is the outcome
    we hope to detect. We then can imagine the confusion matrix as shown in *Figure
    10.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Distinguishing between positive and negative classes adds detail
    to the confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix presented in this way is the basis for many of the most
    important measures of model performance. In the next section, we’ll use this matrix
    to better understand exactly what is meant by accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Using confusion matrices to measure performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the 2x2 confusion matrix, we can formalize our definition of **prediction
    accuracy** (sometimes called the **success rate**) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_001.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, the terms *TP*, *TN*, *FP*, and *FN* refer to the number of
    times the model’s predictions fell into each of these categories. The accuracy
    is therefore a proportion that represents the number of true positives and true
    negatives divided by the total number of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **error rate**, or the proportion of incorrectly classified examples, is
    specified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the error rate can be calculated as 1 minus the accuracy. Intuitively,
    this makes sense; a model that is correct 95 percent of the time is incorrect
    five percent of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to tabulate a classifier’s predictions into a confusion matrix
    is to use R’s `table()` function. The command for creating a confusion matrix
    for the SMS data is shown as follows. The counts in this table could then be used
    to calculate accuracy and other statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to create a confusion matrix with more informative output,
    the `CrossTable()` function in the `gmodels` package offers a customizable solution.
    If you recall, we first used this function in *Chapter 2*, *Managing and Understanding
    Data*. If you didn’t install the package at that time, you will need to do so
    using the `install.packages("gmodels")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the `CrossTable()` output includes proportions in each cell that
    indicate the cell count as a percentage of the table’s row, column, and overall
    total counts. The output also includes row and column totals. As shown in the
    following code, the syntax is similar to the `table()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a confusion matrix with a wealth of additional detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We’ve used `CrossTable()` in several previous chapters, so by now, you should
    be familiar with its output. If you ever forget how to interpret the output, simply
    refer to the key (labeled `Cell Contents`), which provides the definition of each
    number in the table cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the confusion matrix to obtain the accuracy and error rate. Since
    accuracy is (TP + TN) / (TP + TN + FP + FN), we can calculate it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate the error rate (FP + FN) / (TP + TN + FP + FN) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same as 1 minus accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Although these calculations may seem simple, it is important to practice thinking
    about how the components of the confusion matrix relate to one another. In the
    next section, you will see how these same pieces can be combined in different
    ways to create a variety of additional performance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond accuracy – other measures of performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Countless performance measures have been developed and used for specific purposes
    in disciplines as diverse as medicine, information retrieval, marketing, and signal
    detection theory, among others. To cover all of them could fill hundreds of pages,
    which makes a comprehensive description infeasible here. Instead, we’ll consider
    only some of the most useful and most cited measures in machine learning literature.
  prefs: []
  type: TYPE_NORMAL
- en: The `caret` package by Max Kuhn includes functions for computing many such performance
    measures. This package provides tools for preparing, training, evaluating, and
    visualizing machine learning models and data; the name “caret” is a simplification
    of “classification and regression training.” Because it is also valuable for tuning
    models, in addition to its use here, we will also employ the `caret` package extensively
    in *Chapter 14*, *Building Better Learners*. Before proceeding, you will need
    to install the package using the `install.packages("caret")` command.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on `caret`, refer to *Building Predictive Models in R Using
    the caret Package, Kuhn, M, Journal of Statistical Software, 2008, Vol. 28* or
    the package’s very informative documentation pages at [http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `caret` package adds yet another function for creating a confusion matrix.
    As shown in the following command, the syntax is similar to `table()`, but with
    a minor difference. Because `caret` computes the measures of model performance
    that reflect the ability to classify the positive class, a `positive` parameter
    should be specified. In this case, since the SMS classifier is intended to detect
    spam, we will set `positive = "spam"` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: At the top of the output is a confusion matrix much like the one produced by
    the `table()` function, but transposed. The output also includes a set of performance
    measures. Some of these, like accuracy, are familiar, while many others are new.
    Let’s look at some of the most important metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The kappa statistic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **kappa statistic** (labeled `Kappa` in the previous output) adjusts the
    accuracy by accounting for the possibility of a correct prediction by chance alone.
    This is especially important for datasets with a severe class imbalance because
    a classifier can obtain high accuracy simply by always guessing the most frequent
    class. The kappa statistic will only reward the classifier if it is correct more
    often than this simplistic strategy.
  prefs: []
  type: TYPE_NORMAL
- en: There is more than one way to define the kappa statistic. The most common method,
    described here, uses **Cohen’s kappa coefficient**, as described in the paper
    *A coefficient of agreement for nominal scales, Cohen, J, Education and Psychological
    Measurement, 1960, Vol. 20, pp. 37-46*.
  prefs: []
  type: TYPE_NORMAL
- en: Kappa values typically range from 0 to a maximum of 1, with higher values reflecting
    stronger agreement between the model’s predictions and the true values. It is
    possible to observe values less than 0 if the predictions are consistently in
    the wrong direction—that is, the predictions disagree with the actual values or
    are wrong more often than would be expected by random guessing. This rarely occurs
    for machine learning models and usually reflects a coding issue, which can be
    fixed by simply reversing the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how a model is to be used, the interpretation of the kappa statistic
    might vary. One common interpretation is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Poor agreement = less than 0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fair agreement = 0.2 to 0.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moderate agreement = 0.4 to 0.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good agreement = 0.6 to 0.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very good agreement = 0.8 to 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that these categories are subjective. While “good agreement”
    may be more than adequate for predicting someone’s favorite ice cream flavor,
    “very good agreement” may not suffice if your goal is to identify birth defects.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the previous scale, refer to *The measurement of observer
    agreement for categorical data, Landis, JR, Koch, GG. Biometrics, 1997, Vol. 33,
    pp. 159-174*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the formula for calculating the kappa statistic. In this formula,
    Pr(*a*) refers to the proportion of actual agreement and Pr(*e*) refers to the
    expected agreement between the classifier and the true values, under the assumption
    that they were chosen at random:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These proportions are easy to obtain from a confusion matrix once you know
    where to look. Let’s consider the confusion matrix for the SMS classification
    model created with the `CrossTable()` function, which is repeated here for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the bottom value in each cell indicates the proportion of all
    instances falling into that cell. Therefore, to calculate the observed agreement
    Pr(*a*), we simply add the proportion of all instances where the predicted type
    and actual SMS type agree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can calculate Pr(*a*) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: For this classifier, the observed and actual values agree 97.4 percent of the
    time—you will note that this is the same as the accuracy. The kappa statistic
    adjusts the accuracy relative to the expected agreement, Pr(*e*), which is the
    probability that chance alone would lead the predicted and actual values to match,
    under the assumption that both are selected randomly according to the observed
    proportions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find these observed proportions, we can use the probability rules we learned
    in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*. Assuming
    two events are independent (meaning one does not affect the other), probability
    rules note that the probability of both occurring is equal to the product of the
    probabilities of each one occurring. For instance, we know that the probability
    of both choosing ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: Pr(*actual_type is ham*) * Pr(*predicted_type is ham*)
  prefs: []
  type: TYPE_NORMAL
- en: 'And the probability of both choosing spam is:'
  prefs: []
  type: TYPE_NORMAL
- en: Pr(*actual_type is spam*) * Pr(*predicted_type is spam*)
  prefs: []
  type: TYPE_NORMAL
- en: The probability that the predicted or actual type is spam or ham can be obtained
    from the row or column totals. For instance, Pr(*actual_type is ham*) = 0.868
    and Pr(*predicted type is ham*) = 0.888.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pr(*e*) can be calculated as the sum of the probabilities that the predicted
    and actual values agree that the message is either spam or ham. Recall that for
    mutually exclusive events (events that cannot happen simultaneously), the probability
    of either occurring is equal to the sum of their probabilities. Therefore, to
    obtain the final Pr(*e*), we simply add both products, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Since Pr(*e*) is 0.786, by chance alone, we would expect the observed and actual
    values to agree about 78.6 percent of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that we now have all the information needed to complete the kappa
    formula. Plugging the Pr(*a*) and Pr(*e*) values into the kappa formula, we find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The kappa is about 0.88, which agrees with the previous `confusionMatrix()`
    output from `caret` (the small difference is due to rounding). Using the suggested
    interpretation, we note that there is very good agreement between the classifier’s
    predictions and the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of R functions for calculating kappa automatically. The
    `Kappa()` function (be sure to note the capital “K”) in the **Visualizing Categorical
    Data** (**VCD**) package uses a confusion matrix of predicted and actual values.
    After installing the package by typing `install.packages("vcd")`, the following
    commands can be used to obtain kappa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We’re interested in the unweighted kappa. The value of 0.88 matches what we
    computed manually.
  prefs: []
  type: TYPE_NORMAL
- en: The weighted kappa is used when there are varying degrees of agreement. For
    example, using a scale of cold, cool, warm, and hot, the value of warm agrees
    more with hot than it does with the value of cold. In the case of a two-outcome
    event, such as spam and ham, the weighted and unweighted kappa statistics will
    be identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kappa2()` function in the **Interrater Reliability** (`irr`) package can
    be used to calculate kappa from vectors of predicted and actual values stored
    in a data frame. After installing the package using `install.packages("irr")`,
    the following commands can be used to obtain kappa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `Kappa()` and `kappa2()` functions report the same kappa statistic, so use
    whichever option you are more comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful not to use the built-in `kappa()` function. It is completely unrelated
    to the kappa statistic reported previously!
  prefs: []
  type: TYPE_NORMAL
- en: The Matthews correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although accuracy and kappa have been popular measures of performance for many
    years, a third option has quickly become a de facto standard in the field of machine
    learning. Like both prior metrics, the **Matthews correlation coefficient** (**MCC**)
    is a single statistic intended to reflect the overall performance of a classification
    model. Additionally, the MCC is like kappa in that it is useful even in the case
    that the dataset is severely imbalanced—the types of situations in which the traditional
    accuracy measure can be very misleading.
  prefs: []
  type: TYPE_NORMAL
- en: The MCC has grown in popularity due to its ease of interpretation, as well as
    a growing body of evidence suggesting that it performs better in a wider variety
    of circumstances than kappa. Recent empirical research has indicated that the
    MCC may be the best single metric for describing the real-world performance of
    a binary classification model. Other studies have identified potential circumstances
    in which the kappa statistic provides a misleading or incorrect depiction of model
    performance. In these cases, when the MCC and kappa disagree, the MCC metric tends
    to provide a more reasonable assessment of the model’s true capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the relative advantages of the Matthews correlation
    coefficient versus kappa, see *The Matthews correlation coefficient (MCC) is more
    informative than Cohen’s kappa and brier score in binary classification assessment,
    Chicco D, Warrens MJ, Jurman G, IEEE Access, 2021, Vol. 9, pp. 78368-78381*. Alternatively,
    refer to *Why Cohen’s Kappa should be avoided as performance measure in classification,
    Delgado R, Tibau XA, PLoS One, 2019, Vol. 14(9):e0222916*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Values of the MCC are interpreted on the same scale as Pearson’s correlation
    coefficient, which was introduced in *Chapter 6*, *Forecasting Numeric Data –
    Regression Methods*. This ranges from -1 to +1, which indicate perfectly inaccurate
    and perfectly accurate predictions, respectively. A value of 0 indicates a model
    that performs no better than random guessing. As most MCC scores fall somewhere
    in the range of values between 0 and 1, some subjectivity is involved in knowing
    what is a “good” score. Much like the scale used for Pearson correlations, one
    potential interpretation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Perfectly incorrect = -1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly incorrect = -0.5 to -1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moderately incorrect = -0.3 to -0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weakly incorrect = -0.1 to 0.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly correct = -0.1 to 0.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weakly correct = 0.1 to 0.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moderately correct = 0.3 to 0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly correct = 0.5 to 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perfectly correct = 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the worst-performing models fall in the middle of the scale. In other
    words, a model on the negative side of the scale (from perfectly to weakly incorrect)
    still performs better than a model predicting at random. For instance, even though
    the accuracy of a strongly incorrect model is poor, the predictions can simply
    be reversed to obtain the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: As with all such scales, these should be used only as rough guidelines. Furthermore,
    the key benefit of a metric like the MCC is not to understand a model’s performance
    in isolation, but rather, to facilitate performance comparisons across several
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MCC can be computed from the confusion matrix for a binary classifier as
    shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the confusion matrix for the SMS spam classification model, we obtain
    the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: TN = 1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP = 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FN = 31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TP = 152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MCC can then be computed manually in R as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mltools` package by Ben Gorman provides an `mcc()` function which can
    perform the MCC calculation using vectors of predicted and actual values. After
    installing the package, the following R code produces the same result as the calculation
    done by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, for a binary classifier where the positive class is coded as
    1 and the negative class is coded as 0, the MCC is identical to the Pearson correlation
    between the predicted and actual values. We can demonstrate this using the `cor()`
    function in R, after using `ifelse()` to convert the categorical (`"spam"` or
    `"ham"`) values into binary (`1` or `0`) values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The fact that such an obvious classification performance metric was hiding in
    plain sight, as a simple adaptation of Pearson’s correlation introduced in the
    late 1800s, makes it quite remarkable that the MCC has only become popular in
    recent decades! Biochemist Brian W. Matthews is responsible for popularizing this
    metric in 1975 for use in two-outcome classification problems and thus receives
    naming credit for this specific application. However, it seems quite likely that
    the metric was already used widely, even if it had not garnered much attention
    until much later. Today, it is used across industry, academic research, and even
    as a benchmark for machine learning competitions. There may be no single metric
    that better captures the overall performance of a binary classification model.
    However, as you will soon see, a more in-depth understanding of model performance
    can be obtained using combinations of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the MCC is defined here for binary classification, it is unclear whether
    it is the best metric for multi-class outcomes. For a discussion of this and other
    alternatives, see *A comparison of MCC and CEN error measures in multi-class prediction,
    Jurman G, Riccadonna S, Furlanello C, 2012, PLOS One 7(8): e41882*.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finding a useful classifier often involves a balance between predictions that
    are overly conservative and overly aggressive. For example, an email filter could
    guarantee to eliminate every spam message by aggressively filtering nearly every
    ham message. On the other hand, to guarantee that no ham messages will be inadvertently
    filtered might require us to allow an unacceptable amount of spam to pass through
    the filter. A pair of performance measures captures this tradeoff: sensitivity
    and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **sensitivity** of a model (also called the **true positive rate**) measures
    the proportion of positive examples that were correctly classified. Therefore,
    as shown in the following formula, it is calculated as the number of true positives
    divided by the total number of positives, both those correctly classified (the
    true positives) and those incorrectly classified (the false negatives):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_005.png)'
  prefs: []
  type: TYPE_IMG
- en: The **specificity** of a model (also called the **true negative rate**) measures
    the proportion of negative examples that were correctly classified. As with sensitivity,
    this is computed as the number of true negatives divided by the total number of
    negatives—the true negatives plus the false positives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given the confusion matrix for the SMS classifier, we can easily calculate
    these measures by hand. Assuming that spam is a positive class, we can confirm
    that the numbers in the `confusionMatrix()` output are correct. For example, the
    calculation for sensitivity is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for specificity, we can calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `caret` package provides functions for calculating sensitivity and specificity
    directly from vectors of predicted and actual values. Be careful to specify the
    `positive` or `negative` parameter appropriately, as shown in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Sensitivity and specificity range from 0 to 1, with values close to 1 being
    more desirable. Of course, it is important to find an appropriate balance between
    the two—a task that is often quite context-specific.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in this case, the sensitivity of 0.831 implies that 83.1 percent
    of the spam messages were correctly classified. Similarly, the specificity of
    0.997 implies that 99.7 percent of non-spam messages were correctly classified,
    or alternatively, 0.3 percent of valid messages were rejected as spam. The idea
    of rejecting 0.3 percent of valid SMS messages may be unacceptable, or it may
    be a reasonable tradeoff given the reduction in spam.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity provide tools for thinking about such tradeoffs.
    Typically, changes are made to the model and different models are tested until
    you find one that meets a desired sensitivity and specificity threshold. Visualizations,
    such as those discussed later in this chapter, can also assist with understanding
    the balance between sensitivity and specificity.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Closely related to sensitivity and specificity are two other performance measures
    related to compromises made in classification: precision and recall. Used primarily
    in the context of information retrieval, these statistics are intended to indicate
    how interesting and relevant a model’s results are, or whether the predictions
    are diluted by meaningless noise.'
  prefs: []
  type: TYPE_NORMAL
- en: The **precision** (also known as the **positive predictive value**) is defined
    as the proportion of positive predictions that are truly positive; in other words,
    when a model predicts the positive class, how often is it correct? A precise model
    will only predict the positive class in cases very likely to be positive. It will
    be very trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Consider what would happen if the model was very imprecise. Over time, the results
    would be less likely to be trusted. In the context of information retrieval, this
    would be analogous to a search engine like Google returning irrelevant results.
    Eventually, users would switch to a competitor like Bing. In the case of the SMS
    spam filter, high precision means that the model is able to carefully target only
    the spam while avoiding false positives in the ham.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **recall** is a measure of how complete the results are.
    As shown in the following formula, this is defined as the number of true positives
    over the total number of positives. You may have already recognized this as the
    same as sensitivity; however, the interpretation differs slightly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_008.png)'
  prefs: []
  type: TYPE_IMG
- en: A model with high recall captures a large portion of the positive examples,
    meaning that it has a wide breadth. For example, a search engine with high recall
    returns a large number of documents pertinent to the search query. Similarly,
    the SMS spam filter has high recall if the majority of spam messages are correctly
    identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate precision and recall from the confusion matrix. Again, assuming
    that spam is a positive class, the precision is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And the recall is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `caret` package can be used to compute either of these measures from vectors
    of predicted and actual classes. Precision uses the `posPredValue()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall uses the `sensitivity()` function that we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Like the tradeoff between sensitivity and specificity, for most real-world problems,
    it is difficult to build a model with both high precision and high recall. It
    is easy to be precise if you target only the low-hanging fruit—the easiest-to-classify
    examples. Similarly, it is easy for a model to have a high recall by casting a
    very wide net, meaning that the model is overly aggressive at identifying positive
    cases. In contrast, having both high precision and recall at the same time is
    very challenging. It is therefore important to test a variety of models in order
    to find the combination of precision and recall that meets the needs of your project.
  prefs: []
  type: TYPE_NORMAL
- en: The F-measure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A measure of model performance that combines precision and recall into a single
    number is known as the **F-measure** (also sometimes called the **F**[1] **score**
    or the **F-score**). The F-measure combines precision and recall using the **harmonic
    mean**, a type of average that is used for rates of change. The harmonic mean
    is used rather than the more common arithmetic mean since both precision and recall
    are expressed as proportions between 0 and 1, which can be interpreted as rates.
    The following is the formula for the F-measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the F-measure, use the precision and recall values computed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This comes out exactly the same as using the counts from the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Since the F-measure describes model performance in a single number, it provides
    a convenient, quantitative metric to compare several models directly. Indeed,
    the F-measure was once virtually a gold standard measure of a model of performance,
    but today, it seems to be much less widely used than it was previously. One potential
    explanation is that it assumes that equal weight should be assigned to precision
    and recall, an assumption that is not always valid, depending on the real-world
    costs of false positives and false negatives. Of course, it is possible to calculate
    F-scores using different weights for precision and recall, but choosing the weights
    can be tricky at best and arbitrary at worst. This being said, perhaps a more
    important reason why this metric has fallen out of favor is the adoption of methods
    that visually depict a model’s performance on different subsets of data, such
    as those described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing performance tradeoffs with ROC curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizations are helpful for understanding the performance of machine learning
    algorithms in greater detail. Where statistics such as sensitivity and specificity,
    or precision and recall, attempt to boil model performance down to a single number,
    visualizations depict how a learner performs across a wide range of conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Because learning algorithms have different biases, it is possible that two models
    with similar accuracy could have drastic differences in how they achieve their
    accuracy. Some models may struggle with certain predictions that others make with
    ease while breezing through cases that others struggle to get right. Visualizations
    provide a method for understanding these tradeoffs by comparing learners side
    by side in a single chart.
  prefs: []
  type: TYPE_NORMAL
- en: The **receiver operating characteristic** (**ROC**) curve is commonly used to
    examine the tradeoff between the detection of true positives while avoiding false
    positives. As you might suspect from the name, ROC curves were developed by engineers
    in the field of communications. Around the time of World War II, radar and radio
    operators used ROC curves to measure a receiver’s ability to discriminate between
    true signals and false alarms. The same technique is useful today for visualizing
    the efficacy of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: For more reading on ROC curves, see *An introduction to ROC analysis, Fawcett
    T, Pattern Recognition Letters, 2006, Vol. 27, pp. 861–874*.
  prefs: []
  type: TYPE_NORMAL
- en: The characteristics of a typical ROC diagram are depicted in *Figure 10.4*.
    The ROC curve is drawn using the proportion of true positives on the vertical
    axis and the proportion of false positives on the horizontal axis. Because these
    values are equivalent to sensitivity and (1 – specificity), respectively, the
    diagram is also known as a **sensitivity/specificity plot**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17290_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: The ROC curve depicts classifier shapes relative to perfect and
    useless classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: The points comprising ROC curves indicate the true positive rate at varying
    false positive thresholds. To illustrate this concept, three hypothetical classifiers
    are contrasted in the previous plot. First, the *perfect classifier* has a curve
    that passes through the point at a 100 percent true positive rate and a 0 percent
    false positive rate. It is able to correctly identify all of the true positives
    before it incorrectly classifies any negative result. Next, the diagonal line
    from the bottom-left to the top-right corner of the diagram represents a *classifier
    with no predictive value*. This type of classifier detects true positives and
    false positives at exactly the same rate, implying that the classifier cannot
    discriminate between the two. This is the baseline by which other classifiers
    may be judged. ROC curves falling close to this line indicate models that are
    not very useful. Lastly, most real-world classifiers are like the *test classifier*,
    in that they fall somewhere in the zone between perfect and useless.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand how the ROC curve is constructed is to create one
    by hand. The values in the table depicted in *Figure 10.5* indicate predictions
    of a hypothetical spam model applied to a test set containing 20 examples, of
    which six are the positive class (spam) and 14 are the negative class (ham).
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: To construct the ROC curve, the estimated probability values for
    the positive class are sorted in descending order, then compared to the actual
    class value'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the curves, the classifier’s predictions are sorted by the model’s
    estimated probability of the positive class, in descending order, with the largest
    values first, as shown in the table. Then, beginning at the plot’s origin, each
    prediction’s impact on the true positive rate and false positive rate results
    in a curve tracing vertically for each positive example and horizontally for each
    negative example. This process can be performed by hand on a piece of graph paper,
    as depicted in *Figure 10.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17290_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The ROC curve can be created by hand on graph paper by plotting
    the number of positive examples versus the number of negative examples'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the ROC curve is not complete at this point, because the axes are
    skewed due to the presence of more than twice as many negative examples as positive
    examples in the test set. A simple fix for this is to scale the plot proportionally
    so that the two axes are equivalent in size, as depicted in *Figure 10.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Scaling the plot’s axes creates a proportionate comparison, regardless
    of the initial balance of positive and negative examples'
  prefs: []
  type: TYPE_NORMAL
- en: If we imagine that both the *x* and *y* axes now range from 0 to 1, we can interpret
    each axis as a percentage. The *y* axis is the number of positives and originally
    ranged from 0 to 6; after shrinking it to a scale from 0 to 1, each increment
    becomes 1/6\. On this scale, we can think of the vertical coordinate of the ROC
    curve as the number of true positives divided by the total number of positives,
    which is the true positive rate, or sensitivity. Similarly, the *x* axis measures
    the number of negatives; by dividing by the total number of negatives (14 in this
    example), we obtain the true negative rate, or specificity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table in *Figure 10.8* depicts these calculations for all 20 examples in
    the hypothetical test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: The ROC curve traces what happens to the model’s true positive
    rate versus the false positive rate, for increasingly large sets of examples'
  prefs: []
  type: TYPE_NORMAL
- en: An important property of ROC curves is that they are not affected by the class
    imbalance problem in which one of the two outcomes—typically the positive class—is
    much rarer than the other. Many performance metrics, such as accuracy, can be
    misleading for imbalanced data. This is not the case for ROC curves, because both
    dimensions of the plot are based solely on rates *within* the positive and negative
    values, and thus the ratio *across* positives and negatives does not affect the
    result. Because many of the most important machine learning tasks involve severely
    imbalanced outcomes, ROC curves are a very useful tool for understanding the overall
    quality of a model.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ROC curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If ROC curves are helpful for evaluating a single model, it is unsurprising
    that they are also useful for comparing across models. Intuitively, we know that
    curves closer to the top-left of the plot area are better. In practice, the comparison
    is often more challenging than this, as differences between curves are often subtle
    rather than obvious, and the interpretation is nuanced and specific to how the
    model is to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the nuances, let’s begin by considering what causes two models
    to trace different curves on the ROC plot. Beginning at the origin, the curve’s
    length is extended as additional test set examples are predicted to be positive.
    Because the *y* axis represents the true positive rate and the *x* axis represents
    the false positive rate, a steeper upward trajectory is an implicit ratio, implying
    that the model is better at identifying the positive examples without making as
    many mistakes. This is illustrated in *Figure 10.9*, which depicts the start of
    ROC curves for two imaginary models. For the same number of predictions—indicated
    by the equal lengths of the vectors emerging from the origin—the first model has
    a higher true positive rate and a lower false positive rate, which implies it
    is the better performer of the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: For the same number of predictions, model 1 outperforms model
    2 because it has a higher true positive rate'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we continue to trace the ROC curves for each of these two models, evaluating
    the model’s predictions on the entire dataset. In this case, perhaps the first
    model continues to outperform the second at all points on the curve, as shown
    in *Figure 10.10*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all points on the curve, the first model has a higher true positive rate
    and a lower false positive rate, which means it is the better performer across
    the entire dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17290_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Model 1 consistently performs better than model 2, with a higher
    true positive rate and a lower false positive rate at all points on the curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the second model is clearly inferior in the prior example, choosing
    the better performer is not always so easy. *Figure 10.11* depicts intersecting
    ROC curves, which suggests that neither model is the best performer for all applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B17290_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Both model 1 and model 2 are the better performers for different
    subsets of the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The point of intersection between the two ROC curves splits the plot into two
    areas: one in which the first model has a higher true positive rate and the other
    in which the opposite is true. So, how do we know which model is “best” for any
    given use case?'
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, when comparing two curves, it helps to understand that
    both models are trying to sort the dataset in order of the highest to the lowest
    probability that each example is of the positive class. Models that are better
    able to sort the dataset in this way will have ROC curves closer to the top-left
    of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: The first model in *Figure 10.11* jumps to an early lead because it was able
    to sort a larger number of positive examples to the very front of the dataset,
    but after this initial surge, the second model was able to catch up and outperform
    the other by slowly and steadily sorting positive examples in front of negative
    examples over the remainder of the dataset. Although the second model may have
    better overall performance on the full dataset, we tend to prefer models that
    perform better early—the ones that take the so-called “low-hanging fruit” in the
    dataset. The justification for preferring these models is that many real-world
    models are used only for action on a subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a model used to identify customers that are most likely
    to respond to a direct mail advertising campaign. If we could afford to mail all
    potential customers, a model would be unnecessary. But because we don’t have the
    budget to send the advertisement to every address, the model is used to estimate
    the probability that the recipient will purchase the product after viewing the
    advertisement. A model that is better at sorting the true most likely purchasers
    to the front of the list will have a greater slope early in the ROC curve and
    will shrink the marketing budget needed to acquire purchasers. In *Figure 10.11*,
    the first model would be a better fit for this task.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to this approach, another consideration is the relative costs of
    various types of errors; false positives and false negatives often have a different
    impact in the real world. If we know that a spam filter or a cancer screening
    needs to target a specific true positive rate, such as 90 percent or 99 percent,
    we will favor the model that has the lower false positive rate at the desired
    levels. Although neither model would be very good due to the high false positive
    rate, *Figure 10.11* suggests that the second model would be slightly preferable
    for these applications.
  prefs: []
  type: TYPE_NORMAL
- en: As these examples demonstrate, ROC curves allow model performance comparisons
    that also consider how the models will be used. This flexibility is appreciated
    over simpler numeric metrics like accuracy or kappa, but it may also be desirable
    to quantify the ROC curve in a single metric that can be compared quantitatively,
    much like these statistics. The next section introduces exactly this type of measure.
  prefs: []
  type: TYPE_NORMAL
- en: The area under the ROC curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Comparing ROC curves can be somewhat subjective and context-specific, so metrics
    that reduce performance to a single numeric value are always in demand to simplify
    and bring objectivity into the comparison. While it may be difficult to say what
    makes a “good” ROC curve, in general, we know that the closer the ROC curve is
    to the top-left of the plot, the better it is at identifying positive values.
    This can be measured using a statistic known as the **area under the ROC curve**
    (**AUC**). The AUC treats the ROC diagram as a two-dimensional square and measures
    the total area under the ROC curve. The AUC ranges from 0.5 (for a classifier
    with no predictive value) to 1.0 (for a perfect classifier). A convention for
    interpreting AUC scores uses a system similar to academic letter grades:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A**: Outstanding = 0.9 to 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B**: Excellent/Good = 0.8 to 0.9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C**: Acceptable/Fair = 0.7 to 0.8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**D**: Poor = 0.6 to 0.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E**: No Discrimination = 0.5 to 0.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with most scales like this, the levels may work better for some tasks than
    others; the boundaries across categories are naturally somewhat fuzzy.
  prefs: []
  type: TYPE_NORMAL
- en: It is rare but possible for the ROC curve to fall below the diagonal, which
    causes the AUC to be less than 0.50\. This means the classifier performs worse
    than random. Usually, this is caused by a coding error, because a model that consistently
    makes the wrong prediction has obviously learned something useful about the data—it
    is merely applying the predictions in the wrong direction. To fix this issue,
    confirm that the positive cases are coded correctly, or simply reverse the predictions
    such that when the model predicts the negative class, choose the positive class
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the use of AUC started to become widespread, some treated it as a definitive
    measure of model performance, although unfortunately, this is not true in all
    cases. Generally speaking, higher AUC values reflect classifiers that are better
    at sorting a random positive example higher than a random negative example. However,
    *Figure 10.12* illustrates the important fact that two ROC curves may be shaped
    very differently, yet have an identical AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: ROC curves may have different performances despite having the
    same AUC'
  prefs: []
  type: TYPE_NORMAL
- en: Because the AUC is a simplification of the ROC curve, the AUC alone is insufficient
    to identify the “best” model for all use cases. The safest practice is to use
    the AUC in combination with a qualitative examination of the ROC curve, as described
    earlier in this chapter. If two models have an identical or similar AUC, it is
    usually preferable to choose the one that performs better early. Furthermore,
    even in the case that one model has a better overall AUC, a model that has a higher
    initial true positive rate may be preferred for applications that will use only
    a subset of the most confident predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating ROC curves and computing AUC in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pROC` package provides an easy-to-use set of functions for creating ROC
    curves and computing the AUC. The `pROC` website (at [https://web.expasy.org/pROC/](https://web.expasy.org/pROC/))
    includes a list of the full set of features, as well as several examples of visualization
    capabilities. Before continuing, be sure that you have installed the package using
    the `install.packages("pROC")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the `pROC` package, see *pROC: an open-source package
    for R and S+ to analyze and compare ROC curves, Robin, X, Turck, N, Hainard, A,
    Tiberti, N, Lisacek, F, Sanchez, JC, and Mueller M, BMC Bioinformatics, 2011,
    pp. 12-77*.'
  prefs: []
  type: TYPE_NORMAL
- en: To create visualizations with `pROC`, two vectors of data are needed. The first
    must contain the estimated probability of the positive class and the second must
    contain the predicted class values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the SMS classifier, we’ll supply the estimated spam probabilities and the
    actual class labels to the `roc()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `sms_roc` object, we can visualize the ROC curve with R’s `plot()`
    function. As shown in the following command, many of the standard parameters for
    adjusting the plot can be used, such as `main` (for adding a title), `col` (for
    changing the line color), and `lwd` (for adjusting the line width). The `grid`
    parameter adds faint gridlines to the plot to aid readability, and the `legacy.axes`
    parameter instructs `pROC` to label the *x* axis as 1 – specificity, which is
    a popular convention because it is equivalent to the false positive rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is an ROC curve for the Naive Bayes classifier and a diagonal reference
    line representing the baseline classifier with no predictive value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B17290_10_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: The ROC curve for the Naive Bayes SMS classifier'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitatively, we can see that this ROC curve appears to occupy the space in
    the top-left corner of the diagram, which suggests that it is closer to a perfect
    classifier than the dashed line representing a useless classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare this model’s performance to other models making predictions on the
    same dataset, we can add additional ROC curves to the same plot. Suppose that
    we had also trained a k-NN model on the SMS data using the `knn()` function described
    in *Chapter 3*, *Lazy Learning – Classification Using Nearest Neighbors*. Using
    this model, the predicted probabilities of spam were computed for each record
    in the test set and saved to a CSV file, which we can load here. After loading
    the file, we’ll apply the `roc()` function as before to compute the ROC curve,
    then use the `plot()` function with the parameter `add = TRUE` to add the curve
    to the previous plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting visualization has a second curve depicting the performance of
    the k-NN model making predictions on the same test set as the Naive Bayes model.
    The curve for k-NN is consistently lower, suggesting that it is a consistently
    worse model than the Naive Bayes approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B17290_10_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: ROC curves comparing the performance of Naive Bayes (topmost
    curve) and k-NN (bottom curve) on the SMS test set'
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm this quantitatively, we can use the `pROC` package to calculate
    the AUC. To do so, we simply apply the package’s `auc()` function to the `sms_roc`
    object for each model, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The AUC for the Naive Bayes SMS classifier is 0.98, which is extremely high
    and substantially better than the k-NN classifier’s AUC of 0.89\. But how do we
    know whether the model is just as likely to perform well on another dataset, or
    whether the difference is greater than expected by chance alone? In order to answer
    such questions, we need to better understand how far we can extrapolate a model’s
    predictions beyond the test data. Such methods are described in the sections that
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was mentioned before, but is worth repeating: the AUC value alone is often
    insufficient for identifying a “best” model. In this example, the AUC does identify
    the better model because the ROC curves do not intersect—the Naive Bayes model
    has a better true positive rate at all points on the ROC curve. When ROC curves
    *do* intersect, the “best” model will depend on how the model will be used. Additionally,
    it is possible to combine learners with intersecting ROC curves into even stronger
    models using the techniques covered in *Chapter 14*, *Building Better Learners*.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimating future performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some R machine learning packages present confusion matrices and performance
    measures during the model-building process. The purpose of these statistics is
    to provide insight into the model’s **resubstitution error**, which occurs when
    the target values of training examples are incorrectly predicted, despite the
    model being trained on this data. This can be used as a rough diagnostic to identify
    obviously poor performers. A model that cannot perform sufficiently well on the
    data it was trained on is unlikely to do well on future data.
  prefs: []
  type: TYPE_NORMAL
- en: The opposite is not true. In other words, a model that performs well on the
    training data cannot be assumed to perform well on future datasets. For example,
    a model that used rote memorization to perfectly classify every training instance
    with zero resubstitution error would be unable to generalize its predictions to
    data it has never seen before. For this reason, the error rate on the training
    data can be assumed to be optimistic about a model’s future performance.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of relying on resubstitution error, a better practice is to evaluate
    a model’s performance on data it has not yet seen. We used such a method in previous
    chapters when we split the available data into a set for training and a set for
    testing. In some cases, however, it is not always ideal to create training and
    test datasets. For instance, in a situation where you have only a small pool of
    data, you might not want to reduce the sample any further.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, as you will soon learn, there are other ways to estimate a model’s
    performance on unseen data. The `caret` package we used to calculate performance
    measures also offers functions to estimate future performance. If you are following
    along with the R code examples and haven’t already installed the `caret` package,
    please do so. You will also need to load the package to the R session using the
    `library(caret)` command.
  prefs: []
  type: TYPE_NORMAL
- en: The holdout method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The procedure of partitioning data into training and test datasets that we used
    in previous chapters is known as the **holdout method**. As shown in *Figure 10.15*,
    the **training dataset** is used to generate the model, which is then applied
    to the **test dataset** to generate predictions for evaluation. Typically, about
    one-third of the data is held out for testing and two-thirds is used for training,
    but this proportion can vary depending on the amount of available data or the
    complexity of the learning task. To ensure that the training and test datasets
    do not have systematic differences, their examples are randomly divided into two
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_10_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: The simplest holdout method divides the data into training and
    test sets'
  prefs: []
  type: TYPE_NORMAL
- en: For the holdout method to result in a truly accurate estimate of future performance,
    at no time should performance on the test dataset be allowed to influence the
    modeling process. In the words of Stanford professor and renowned machine learning
    expert Trevor Hastie, “*ideally the test set should be kept in a ‘vault,’ and
    be brought out only at the end of the data analysis*.” In other words, the test
    data should remain untouched aside from its one and only purpose, which is to
    evaluate a single, final model.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, see *The Elements of Statistical Learning (2nd edition),
    Hastie, Tibshirani, and Friedman (2009), p. 222*.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to unknowingly violate this rule and peek into the metaphorical “vault”
    when choosing one of several models or changing a single model based on the results
    of repeated testing. For example, suppose we built several models on the training
    data and selected the one with the highest accuracy on the test data. In this
    case, because we have used the test dataset to cherry-pick the best result, the
    test performance is not an unbiased measure of future performance on unseen data
  prefs: []
  type: TYPE_NORMAL
- en: A keen reader will note that holdout test data was used in previous chapters
    to both evaluate models and improve model performance. This was done for illustrative
    purposes but would indeed violate the rule stated previously. Consequently, the
    model performance statistics shown were not truly unbiased estimates of future
    performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this problem, it is better to divide the original data so that in addition
    to the training and test datasets, a **validation dataset** is available. The
    validation dataset can be used for iterating and refining the model or models
    chosen, leaving the test dataset to be used only once as a final step to report
    an estimated error rate for future predictions. A typical split between training,
    test, and validation would be 50 percent, 25 percent, and 25 percent, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_10_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: A validation dataset can be held out from training to select
    from multiple candidate models'
  prefs: []
  type: TYPE_NORMAL
- en: A simple method for creating holdout samples uses random number generators to
    assign records to partitions. This technique was first used in *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*, to create training
    and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to follow along with the following examples, download the `credit.csv`
    dataset from Packt Publishing’s website and load it into a data frame using the
    `credit <- read.csv("credit.csv", stringsAsFactors = TRUE)` command.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a data frame named `credit` with 1,000 rows of data. We can
    divide this into three partitions as follows. First, we create a vector of randomly
    ordered row IDs from 1 to 1,000 using the `runif()` function, which, by default,
    generates a specified number of random values between 0 and 1\. The `runif()`
    function gets its name from the random uniform distribution, which was discussed
    in *Chapter 2*, *Managing and Understanding Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `order()` function then returns a vector indicating the rank order of the
    1,000 random numbers. For instance, `order(c(0.5, 0.25, 0.75, 0.1))` returns the
    sequence `4 2 1 3` because the smallest number (0.1) appears fourth, the second
    smallest (0.25) appears second, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the random IDs are used to divide the credit data frame into 500, 250,
    and 250 records comprising the training, validation, and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: One problem with holdout sampling is that each partition may have a larger or
    smaller proportion of some classes. In cases where one (or more) class is a very
    small proportion of the dataset, this can lead to it being omitted from the training
    dataset—a significant problem because the model cannot then learn this class.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the chance of this occurring, a technique called **stratified random
    sampling** can be used. Although a random sample should generally contain roughly
    the same proportion of each class value as the full dataset, stratified random
    sampling guarantees that the random partitions have nearly the same proportion
    of each class as the full dataset, even when some classes are small.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `caret` package provides a `createDataPartition()` function, which creates
    partitions based on stratified holdout sampling. The steps for creating a stratified
    sample of training and test data for the `credit` dataset are shown in the following
    commands. To use the function, a vector of class values must be specified (here,
    `default` refers to whether a loan went into default), in addition to a parameter,
    `p`, which specifies the proportion of instances to be included in the partition.
    The `list = FALSE` parameter prevents the result from being stored as a list object—a
    capability that is needed for more complex sampling techniques, but is unnecessary
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `in_train` vector indicates the row numbers included in the training sample.
    We can use these row numbers to select examples for the `credit_train` data frame.
    Similarly, by using a negative symbol, we can use the rows not found in the `in_train`
    vector for the `credit_test` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Although it distributes the classes evenly, stratified sampling does not guarantee
    other types of representativeness. Some samples may have too many or too few difficult
    cases, easy-to-predict cases, or outliers. This is especially true for smaller
    datasets, which may not have a large enough portion of such cases to divide among
    training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to potentially biased samples, another problem with the holdout
    method is that substantial portions of data must be reserved for testing and validating
    the model. Since this data cannot be used to train the model until its performance
    has been measured, the performance estimates are likely to be overly conservative.
  prefs: []
  type: TYPE_NORMAL
- en: Since models trained on larger datasets generally perform better, a common practice
    is to retrain the model on the full set of data (that is, training plus test and
    validation) after a final model has been selected and evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: A technique called **repeated holdout** is sometimes used to mitigate the problems
    of randomly composed training datasets. The repeated holdout method is a special
    case of the holdout method that uses the average result from several random holdout
    samples to evaluate a model’s performance. As multiple holdout samples are used,
    it is less likely that the model is trained or tested on non-representative data.
    We’ll expand on this idea in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The repeated holdout is the basis of a technique known as **k-fold cross-validation**
    (**k-fold CV**), which has become the industry standard for estimating model performance.
    Rather than taking repeated random samples that could potentially use the same
    record more than once, k-fold CV randomly divides the data into *k* separate random
    partitions called **folds**.
  prefs: []
  type: TYPE_NORMAL
- en: Although *k* can be set to any number, by far the most common convention is
    to use a 10-fold CV. Why 10 folds? The reason is that empirical evidence suggests
    that there is little added benefit to using a greater number. For each of the
    10 folds (each comprising 10 percent of the total data), a machine learning model
    is built on the remaining 90 percent of the data. The fold’s 10 percent sample
    is then used for model evaluation. After the process of training and evaluating
    the model has occurred 10 times (with 10 different training/testing combinations),
    the average performance across all folds is reported.
  prefs: []
  type: TYPE_NORMAL
- en: An extreme case of k-fold CV is the **leave-one-out method**, which performs
    k-fold CV using a fold for each one of the data’s examples. This ensures that
    the greatest amount of data is used for training the model. Although this may
    seem useful, it is so computationally expensive that it is rarely used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Datasets for CV can be created using the `createFolds()` function in the `caret`
    package. Like stratified random holdout sampling, this function will attempt to
    maintain the same class balance in each of the folds as in the original dataset.
    The following is the command to create 10 folds, using `set.seed(123)` to ensure
    the results are reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the `createFolds()` function is a list of vectors storing the
    row numbers for each of the `k = 10` requested folds. We can peek at the contents
    using `str()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see that the first fold is named `Fold01` and stores 100 integers,
    indicating the 100 rows in the `credit` data frame for the first fold. To create
    training and test datasets to build and evaluate a model, an additional step is
    needed. The following commands show how to create data for the first fold. We’ll
    assign the selected 10 percent to the test dataset and use the negative symbol
    to assign the remaining 90 percent to the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: To perform the full 10-fold CV, this step would need to be repeated a total
    of 10 times, first building a model and then calculating the model’s performance
    each time. In the end, the performance measures would be averaged to obtain the
    overall performance. Thankfully, we can automate this task by applying several
    of the techniques we learned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the process, we’ll estimate the kappa statistic for a C5.0 decision
    tree model of the credit data using 10-fold CV. First, we need to load some R
    packages: `caret` (to create the folds), `C50` (to build the decision tree), and
    `irr` (to calculate kappa). The latter two packages were chosen for illustrative
    purposes; if you desire, you can use a different model or a different performance
    measure with the same series of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll create a list of 10 folds as we have done previously. As before,
    the `set.seed()` function is used here to ensure that the results are consistent
    if the same code is run again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will apply a series of identical steps to the list of folds using
    the `lapply()` function. As shown in the following code, because there is no existing
    function that does exactly what we need, we must define our own function to pass
    to `lapply()`. Our custom function divides the `credit` data frame into training
    and test data, builds a decision tree using the `C5.0()` function on the training
    data, generates a set of predictions from the test data, and compares the predicted
    and actual values using the `kappa2()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting kappa statistics are compiled into a list stored in the `cv_results`
    object, which we can examine using `str()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s just one more step remaining in the 10-fold-CV process: we must calculate
    the average of these 10 values. Although you will be tempted to type `mean(cv_results)`,
    because `cv_results` is not a numeric vector, the result would be an error. Instead,
    use the `unlist()` function, which eliminates the list structure and reduces `cv_results`
    to a numeric vector. From there, we can calculate the mean kappa as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: This kappa statistic is relatively low, corresponding to “fair” on the interpretation
    scale, which suggests that the credit scoring model performs only marginally better
    than random chance. In *Chapter 14*, *Building Better Learners*, we’ll examine
    automated methods based on a 10-fold CV, which can assist us with improving the
    performance of this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because CV provides a performance estimate from multiple test sets, we can
    also compute the variability in the estimate. For example, the standard deviation
    of the 10 iterations can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: After finding the average and standard deviation of the performance metric,
    it is possible to calculate a confidence interval or determine whether two models
    have a **statistically significant** difference in performance, which means that
    it is likely that the difference is real and not due to random variation.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, recent research has demonstrated that CV violates assumptions
    of such statistical tests, particularly the need for the data to be drawn from
    independent random samples, which is clearly not the case for CV folds, which
    are linked to one another by definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a discussion of the limitations of performance estimates taken from 10-fold
    CV, see *Cross-validation: what does it estimate and how well does it do it?,
    Bates S, Hastie T, and Tibshirani R, 2022, https://arxiv.org/abs/2104.00673*.'
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated variants of CV have been developed to improve the robustness
    of model performance estimates. One such technique is **repeated k-fold CV**,
    which involves repeatedly applying k-fold CV and averaging the results. A common
    strategy is to perform 10-fold CV 10 times. Although computationally intensive,
    this provides an even more robust performance estimate than a standard 10-fold
    CV, as the performance is averaged over many more trials. However, it too violates
    statistical assumptions, and thus statistical tests performed on the results may
    be slightly biased.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the current gold standard for estimating model performance is **nested
    cross-validation**, which literally performs k-fold CV within another k-fold CV
    process. This technique is described in *Chapter 11*, *Being Successful with Machine
    Learning*, and is not only extremely computationally expensive but is also more
    challenging to implement and interpret. The upside of nested k-fold CV is that
    it produces truly valid comparisons of model performance compared to standard
    k-fold CV, which is biased due to its violation of statistical assumptions. On
    the other hand, the bias caused by this issue seems to be less important for very
    large datasets, so it may still be reasonable—and remains a common practice—to
    use the confidence intervals or significance tests derived from the simpler CV
    approach to help identify the “best” model.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A slightly less popular, but very important, alternative to k-fold CV is known
    as **bootstrap sampling**, the **bootstrap**, or **bootstrapping** for short.
    Generally speaking, these refer to statistical methods that use random samples
    of data to estimate the properties of a larger set. When this principle is applied
    to machine learning model performance, it implies the creation of several randomly
    selected training and test datasets, which are then used to estimate performance
    statistics. The results from the various random datasets are then averaged to
    obtain a final estimate of future performance.
  prefs: []
  type: TYPE_NORMAL
- en: So, what makes this procedure different from k-fold CV? Whereas CV divides the
    data into separate partitions in which each example can appear only once, the
    bootstrap allows examples to be selected multiple times through a process of **sampling
    with replacement**. This means that from the original dataset of *n* examples,
    the bootstrap procedure will create one or more new training datasets that also
    contain *n* examples, some of which are repeated.
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding test datasets are then constructed from the set of examples
    that were not selected for the respective training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a bootstrapped dataset, the probability that any given instance is excluded
    from the training dataset is 36.8 percent. We can prove this mathematically by
    recognizing that each example has a 1/*n* chance of being sampled each time one
    of *n* rows is added to the training dataset. Therefore, to be in the test set,
    an example must *not* be selected *n* times. As the chance of being chosen is
    1/*n*, the chance of *not* being chosen is therefore 1 - 1/*n*, and the probability
    of going unselected *n* times is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this formula, if the dataset to be bootstrapped contains 1,000 rows,
    the probability of a random record being unselected is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for a dataset with 100,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'And as *n* approaches infinity, the formula reduces to 1/*e*, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Given that the probability of being unselected is 36.8 percent, the probability
    of any instance being selected for the training dataset is 100 - 36.8 = 63.2 percent.
    In other words, the training data represents only 63.2 percent of available examples,
    some of which are repeated. In contrast with 10-fold CV, which uses 90 percent
    of examples for training, the bootstrap sample is less representative of the full
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Because a model trained on only 63.2 percent of the training data is likely
    to perform worse than a model trained on a larger training set, the bootstrap’s
    performance estimates may be substantially lower than what will be obtained when
    the model is later trained on the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'A special case of bootstrapping, known as the **0.632 bootstrap**, accounts
    for this by calculating the final performance measure as a function of performance
    on both the training data (which is overly optimistic) and the test data (which
    is overly pessimistic). The final error rate is then estimated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_10_011.png)'
  prefs: []
  type: TYPE_IMG
- en: One advantage of bootstrap sampling over CV is that it tends to work better
    with very small datasets. Additionally, bootstrap sampling has applications beyond
    performance measurement. In particular, in *Chapter 14*, *Building Better Learners*,
    you will learn how the principles of bootstrap sampling can be used to improve
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter presented several of the most common measures and techniques for
    evaluating the performance of machine learning classification models. Although
    accuracy provides a simple method for examining how often a model is correct,
    this can be misleading in the case of rare events because the real-life importance
    of such events may be inversely proportional to how frequently they appear in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Some measures based on confusion matrices better capture a model’s performance
    as well as the balance between the costs of various types of errors. The kappa
    statistic and Matthews correlation coefficient are two more sophisticated measures
    of performance, which work well even for severely unbalanced datasets. Additionally,
    closely examining the tradeoffs between sensitivity and specificity, or precision
    and recall, can be a useful tool for thinking about the implications of errors
    in the real world. Visualizations such as the ROC curve are also helpful to this
    end.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth mentioning that, sometimes, the best measure of a model’s performance
    is to consider how well it meets, or doesn’t meet, other objectives. For instance,
    you may need to explain a model’s logic in simple language, which would eliminate
    some models from consideration. Additionally, even if it performs very well, a
    model that is too slow or difficult to scale to a production environment is completely
    useless.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead to the chapters that follow, an obvious extension of *measuring*
    performance is finding ways to *improve* performance. As you continue in this
    book, you will apply many of the principles in this chapter while strengthening
    your machine learning abilities and adding more advanced skills. CV techniques,
    ROC curves, bootstrapping, and the `caret` package will reappear regularly in
    the coming pages, as we build upon our work so far to investigate ways to make
    smarter models by systematically iterating, refining, and combining learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
