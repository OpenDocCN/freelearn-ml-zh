- en: 'Chapter 4: Data Preparation at Scale Using Amazon SageMaker Data Wrangler and
    Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've identified our dataset and explored both manual and automated
    labeling. Now it's time to turn our attention to preparing the data for training.
    Data scientists are familiar with the steps of feature engineering, such as **scaling
    numeric features**, **encoding categorical features**, and **dimensionality reduction**.
  prefs: []
  type: TYPE_NORMAL
- en: As motivation, let's consider our weather dataset. What if our input dataset
    is imbalanced or not really representative of the data we'll encounter in production?
    Our model will not be as accurate as we'd like, and the consequences can be profound.
    Some facial recognition systems have been trained on datasets weighted toward
    white faces, with distressing consequences ([https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9)).
  prefs: []
  type: TYPE_NORMAL
- en: We need to understand what input features are affecting the model. That's important
    from a business standpoint as well as a legal or regulatory standpoint. Consider
    a model that predicts operational outages for an application. Understanding why
    outages happen is perhaps more valuable than predicting when an outage will occur
    – is the problem in our application or due to some external factor such as a network
    hiccup? Then, in some industries such as financial services, we cannot use a model
    without being able to demonstrate that it doesn't violate regulations against
    discriminatory lending, say.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller version of our dataset (covering 1 month) is about 5 GB of data.
    We can analyze that dataset on a modern workstation without too much difficulty.
    But what about the full dataset, which is closer to 500 GB? If we want to prepare
    the full dataset, we need to work with horizontally scalable cluster computing
    frameworks. Furthermore, activities such as encoding categorical variables can
    take quite some time if we use inefficient processing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at the challenges involved in data preparation when
    processing a large dataset and examining the **SageMaker** features that help
    us with large-scale feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Visual data preparation with Data Wrangler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias detection and explainability with Data Wrangler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation at scale with SageMaker Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is in the `CH04` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Visual data preparation with Data Wrangler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start small with our 1-month dataset. Working with a small dataset is
    a good way to get familiar with the data before diving into more scalable techniques.
    SageMaker Data Wrangler gives us an easy way to construct a data flow, a series
    of data preparation steps powered by a visual interface.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this section, we'll use Data Wrangler to inspect and transform
    data, and then export the Data Wrangler steps into a reusable flow.
  prefs: []
  type: TYPE_NORMAL
- en: Data inspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started with Data Wrangler for data inspection, where we look at
    the properties of our data and determine how to prepare it for model training.
    Begin by adding a new flow in SageMaker Studio; go to the **File** menu, then
    **New**, then **Flow**. After the flow starts up and connects to Data Wrangler,
    we need to import our data. The following screenshot shows the data import step
    in Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Import data source in Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Import data source in Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: Because our dataset consists of multiple small JSON files scattered in date-partitioned
    folders, we'll use `PrepareData.ipynb` notebook walks you through creating a `Glue`
    database and table and registering the partitions in the section called `Glue
    Catalog`. Once that's done, click on **Athena** to start importing the small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next screen, specify the database you created in the notebook. Enter
    the following query to import 1 month''s worth of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the import step in Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Athena import into Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Athena import into Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: Run the query and click on **Import dataset**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we''re ready to perform some analysis and transformation. Click the **+**
    symbol next to the last box in the data flow and select **Add analysis**. You''ll
    now have a screen where you can choose one of the available analyses, as you can
    see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data analysis configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Data analysis configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with a **Table summary** step, which shows some statistical properties
    of numeric features, as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Table summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Table summary
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s try a scatter plot to help us visualize the distribution of the
    measurement values. Set the `y` axis to `value`, the `x` axis to `aggdate`, color
    by `country`, and facet by `parameter`. We can see in the following preview chart
    that the value for nitrogen dioxide is relatively steady over time, while the
    value for carbon monoxide shows more variability for some countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Scatter plot showing measurement values by date, color-coded
    by country, and faceted'
  prefs: []
  type: TYPE_NORMAL
- en: by parameter](img/B17249_04_05.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Scatter plot showing measurement values by date, color-coded by
    country, and faceted by parameter
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to add more scatter plots or try a histogram. We'll explore the bias
    report and quick mode in the *Bias detection and explainability with Data Wrangler
    and Clarify* section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've done some basic data inspection, we move on to data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will convert the data from the raw format into a format
    usable for model training. Recall the basic format of our raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll perform the following steps using Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale numeric values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode categorical values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add features related to the date (for example, day of the week, day in a month).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop unwanted columns (`source name`, `coordinates`, `averaging period`, `attribution`,
    `units`, and `location`). These columns are either redundant (for example, the
    important part of the location is in the city and country columns) or not usable
    as features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Back to the *Preparation* part of the flow, click the **+** symbol next to
    the last box in the data flow panel and select **Add Transform**. You''ll see
    a preview of the dataset and a list of the available transforms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Data transformations in Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Data transformations in Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: 'For our first transformation, select `sourcetype` as the column, set **Output
    Style** to **Columns**, and add a prefix for the new column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – One-hot encoding in Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – One-hot encoding in Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: When you're done setting up the transformation, click **Preview** and then **Add**
    to add the transform. You can now add additional transformations to drop the unwanted
    columns, scale the numeric columns, and featurize the date. You can also provide
    your own custom code if you like.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting the flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data Wrangler is very handy when we want to quickly explore a dataset. But we
    can also export the results of a flow into Amazon SageMaker **Feature Store**,
    generate a **SageMaker pipeline**, create a Data Wrangler job, or generate **Python**
    code. We will not use these capabilities now, but feel free to experiment with
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Bias detection and explainability with Data Wrangler and Clarify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've done some initial work in exploring and preparing our data, let's
    do a sanity check on our input data. While bias can mean many things, one particular
    symptom is a dataset that has many more samples of one type of data than another,
    which will affect our model's performance. We'll use Data Wrangler to see if our
    input data is imbalanced and understand which features are most important to our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, add an analysis to the flow. Choose `mobile` column as the label,
    with `1` as the predicted value. Choose `city` as the column to use for bias analysis,
    then click **Check for bias**. In this scenario, we want to determine whether
    our dataset is somehow imbalanced with respect to the city and whether the data
    was collected at a mobile station. If the quality of data from mobile sources
    is inferior to non-mobile sources, it'd be good to know if the mobile sources
    are unevenly distributed among cities.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll examine **feature importance**. Feature importance is one aspect
    of model explainability. We want to understand which parts of the dataset are
    most important to model behavior. Another aspect, which we'll visit in [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify*, in the *Monitor bias drift and
    feature importance drift using Amazon SageMaker Clarify* section, is understanding
    which features contributed to a specific inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add another analysis in the last step of the flow. Select `value` column (Data
    Wrangler will infer that this is a regression problem). Preview and create the
    analysis. You should see a screen that looks similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Feature importance generated by Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Feature importance generated by Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: This analysis generates a random forest model, evaluates performance using a
    test set with 30% of the data, and calculates a **Gini importance score** for
    each feature. As you can see in *Figure 4.8*, the city and day of the month are
    the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: So far we've used Data Wrangler for visual inspection and transformation. Now,
    we'll look at how to handle larger datasets using SageMaker Processing.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation at scale with SageMaker Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's turn our attention to preparing the entire dataset. At 500 GB, it's
    too large to process using `sklearn` on a single EC2 instance. We will write a
    SageMaker processing job that uses **Spark ML** for data preparation. (Alternatively,
    you can use **Dask**, but at the time of writing, SageMaker Processing does not
    provide a Dask container out of the box.)
  prefs: []
  type: TYPE_NORMAL
- en: The `Processing Job` part of this chapter's notebook walks you through launching
    the processing job. Note that we'll use a cluster of 15 EC2 instances to run the
    job (if you need limits raised, you can contact AWS support).
  prefs: []
  type: TYPE_NORMAL
- en: Also note that up until now, we've been working with the uncompressed JSON version
    of the data. This format containing thousands of small JSON files is not ideal
    for Spark processing as the `OpenAQ` dataset also includes a `gzip` is not a preferred
    compression format as it is not splittable; if you have a choice, use the Snappy
    compression format.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the gzipped Parquet version of our data for the larger data preparation
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define the `processor` class, using `7200` seconds (2 hours).
    Two hours is more than sufficient to process at least one of the 8 tables in the
    Parquet dataset. If you want to process all eight of them, change the timeout
    to 3 hours and make an adjustment in the `preprocess.py` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll set the Spark configuration, following the formulas defined in
    an EMR blog ([https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/](https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we''ll launch the job. We need to include a JSON `serde` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The processing script, `CH04/scripts/preprocess.py`, walks through several steps,
    which we'll explain in the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load one or more of the Parquet table sets from S3\. If you want to
    process more than one, modify the `get_tables` function to return more table names
    in the list as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The next step in the processing script is dropping unnecessary columns from
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Drop columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll repeat most of the steps we did in Data Wrangler using **PySpark**.
    We need to drop some columns that we don''t want, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Converting data types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll convert the `mobile` field to an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Scaling numeric fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the Spark ML standard scaler to transform the `value` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Featurizing the date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The date by itself isn''t that useful, so we''ll extract several new features
    from it indicating the day, month, quarter, and year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Simulating labels for air quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although we used ground truth in [*Chapter 3*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*,
    Data Labeling with Amazon SageMaker Ground Truth*, for labeling, for the sake
    of this demonstration we''ll use a simple heuristic to assign these labels instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Encoding categorical variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we''ll encode the categorical features. Most of these features have fairly
    high cardinality, so we''ll perform ordinal encoding here and learn embeddings
    later in our training process. We will only use one-hot encoding for the parameter,
    which only has seven possible choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Splitting and saving the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After some final cleanup of the dataset, we can split the dataset into train,
    validation, and test sets, and save them to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we saw how to use a SageMaker Processing job to perform data
    preparation on a larger dataset using Apache Spark. In the field, many datasets
    are large enough to require a distributed processing framework, and now you understand
    how to integrate a Spark job into your SageMaker workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we tackled feature engineering for a large (~ 500 GB) dataset.
    We looked at challenges including scalability, bias, and explainability. We saw
    how to use SageMaker Data Wrangler, Clarify, and Processing jobs to explore and
    prepare data.
  prefs: []
  type: TYPE_NORMAL
- en: While there are many ways to use these tools, we recommend using Data Wrangler
    for interactive exploration of small to mid-sized datasets. For processing large
    datasets in their entirety, switch to programmatic use of processing jobs using
    the Spark framework to take advantage of parallel processing. (At the time of
    writing, Data Wrangler does not support running on multiple instances, but you
    can run a processing job on multiple instances.) You can always export a Data
    Wrangler flow as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset is many terabytes, consider running a Spark job directly in
    **EMR** or Glue and invoking SageMaker using the SageMaker Spark SDK. EMR and
    Glue have optimized Spark runtimes and more efficient integration with S3 storage.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have our data ready for model training. In the next chapter,
    we'll explore using Amazon SageMaker Feature Store to help us manage prepared
    feature data.
  prefs: []
  type: TYPE_NORMAL
