- en: '*Chapter 3*: Identifying and Fixing Missing Values'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think I speak for many data scientists when I say that rarely is there something
    so seemingly small and trivial that is as of much consequence as the missing value.
    We spend a good deal of our time worrying about missing values because they can
    have a dramatic, and surprising, effect on our analysis. This is most likely to
    happen when missing values are not random â€“ that is, when they are correlated
    with a feature or target. For example, let's say we are doing a longitudinal study
    of earnings, but individuals with lower education are more likely to skip the
    earnings question each year. There is a decent chance that this will bias our
    parameter estimate for education.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, identifying missing values is not even half of the battle. We then
    need to decide how to handle them. Do we remove any observation with a missing
    value for one or more features? Do we impute a value based on a sample-wide statistic
    such as the mean? Or do we assign a value based on a more targeted statistic,
    such as the mean for those in a certain class? Do we think of this differently
    for time series or longitudinal data where the nearest temporal value may make
    the most sense? Or should we use a more complex multivariate technique for imputing
    values, perhaps based on linear regression or **k-nearest neighbors** (**KNN**)?
  prefs: []
  type: TYPE_NORMAL
- en: The answer to all of these questions is *yes*. At some point, we will want to
    use each of these techniques. We will want to be able to answer why or why not
    to all of these possibilities when making a final choice about missing value imputation.
    Each will make sense, depending on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look at techniques for identifying missing values for
    each feature or target, and for observations where values for a large number of
    the features are absent. Then, we will explore strategies for imputing values,
    such as setting values to the overall mean, to the mean for a given category,
    or forward filling. We will also examine multivariate techniques for imputing
    values for missing values and discuss when they are appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing values with regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using KNN imputation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using random forest for imputation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will rely heavily on the pandas and NumPy libraries, but you don't
    require any prior knowledge of these. If you have installed Python from a scientific
    distribution, such as Anaconda or WinPython, these libraries are probably already
    installed. We will also be using the `statsmodels` library for linear regression,
    and machine learning algorithms from `sklearn` and `missingpy`. If you need to
    install any of these packages, you can do so by running `pip install [package
    name]` from a terminal window or Windows PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since identifying missing values is such an important part of an analyst's workflow,
    any tool we use needs to make it easy to regularly check for such values. Fortunately,
    pandas makes it quite simple to identify missing values.
  prefs: []
  type: TYPE_NORMAL
- en: We will be working with the `weeksworked16` and `weeksworked17` for weeks worked
    in 2016 and 2017, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will also work with the COVID-19 data again. This dataset has one observation
    for each country that specifies the total COVID-19 cases and deaths, as well as
    some demographic data for each country.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to identify our missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading the NLS and COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we count the number of missing values for columns that we may use as
    features. We can use the `isnull` method to test whether each feature value is
    missing. It will return `True` if the value is missing and `False` if not. Then,
    we can use `sum` to count the number of `True` values since `sum` will treat each
    `True` value as 1 and each `False` value as 0\. We use `axis=0` to sum over the
    rows for each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, 33 of the 221 countries have null values for `aged_65_older`.
    We have `life_expectancy` for almost all countries.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want the number of missing values for each row, we can specify `axis=1`
    when summing. The following code creates a Series, `demovarsmisscnt`, with the
    number of missing values for the demographic features for each country. 181 countries
    have values for all of the features, 11 are missing values for four of the five
    features, and three are missing values for all of the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at a few of the countries with four or more missing values.
    There is very little demographic data available for these countries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also check missing values for total cases and deaths. 29 countries have
    missing values for cases per million in population, and 36 have missing deaths
    per million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also get a sense of which countries are missing both. 29 countries
    are missing both cases and deaths, and we only have both for 185 countries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes, we have logical missing values that we need to transform into actual
    missing values. This happens when the dataset designers use valid values as codes
    for missing values. These are often values such as 9, 99, or 999, based on the
    allowable number of digits for the variable. Or it might be a more complicated
    coding scheme where there are codes for different reasons for there being missings.
    For example, in the NLS dataset, the codes reveal why the respondent did not provide
    an answer for a question: `-3` is an invalid skip, `-4` is a valid skip, and `-5`
    is a non-interview.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last four columns in the NLS DataFrame have data at the highest grade completed
    for the respondent''s mother and father, parental income, and mother''s age when
    the respondent was born. Let''s examine logical missings for those columns, starting
    with the highest grade that was completed for the respondent''s mother:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are 523 invalid skips and 165 valid skips. Let''s look at a few individuals
    that have at least one of these non-response values for these four features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For our analysis, the reason why there is a non-response is not important.
    Let''s just count the number of non-responses for each of the features, regardless
    of the reason for the non-response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should set these values to `missing` before using these features in our
    analysis. We can use `replace` to set all the values between -5 and -1 to `missing`.
    When we check for actual missings, we get the expected counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This section demonstrated some very handy pandas techniques for identifying
    the number of missing values for each feature, as well as observations with a
    large number of missing values. We also learned how to find logical missing values
    and convert them into actual missings. Next, we'll take our first look at cleaning
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll go over some of the most straightforward approaches
    for handling missing values. This includes dropping observations where there are
    missing values; assigning a sample-wide summary statistic, such as the mean, to
    the missing values; and assigning values based on the mean value for an appropriate
    subset of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the NLS data and select some of the educational data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the techniques we explored in the previous section to identify missing
    values. `schoolrecord.isnull().sum(axis=0)` gives us the number of missing values
    for each feature. The overwhelming majority of observations have missing values
    for `satverbal`, with 7,578 out of 8,984\. Only 31 observations have missing values
    for `highestdegree`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can create a Series, `misscnt`, that specifies the number of missing features
    for each observation with `misscnt = schoolrecord.isnull().sum(axis=1)`. 946 observations
    have seven missing values for the educational data, while 11 are missing values
    for all eight features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also take a look at a few observations with seven or more missing values.
    It looks like `highestdegree` is often the one feature that is present, which
    is not surprising, given that we have already discovered that `highestdegree`
    is rarely missing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s drop observations that have missing values for seven or more features
    out of eight. We can accomplish this by setting the `thresh` parameter of `dropna`
    to `2`. This will drop observations that have fewer than two non-missing values;
    that is, 0 or 1 non-missing values. We get the expected number of observations
    after using `dropna`; that is, 8,984 - 946 - 11 = 8,027:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are a fair number of missing values for `gpaoverall` â€“ that is, 2,980
    â€“ though we have valid values for two-thirds of observations ((8,984 â€“ 2,980)/8,984).
    We might be able to salvage this as a feature if we do a good job of imputing
    the missing values. This is likely more desirable than just removing these observations.
    We do not want to lose that data if we can avoid it, particularly if individuals
    with missing `gpaoverall` are different from others in ways that will matter for
    our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward approach is to assign the overall mean for `gpaoverall`
    to the missing values. The following code uses the pandas Series `fillna` method
    to assign all missing values of `gpaoverall` to the mean value of the Series.
    The first argument to `fillna` is the value you want for all missing values â€“
    in this case, `schoolrecord.gpaoverall.mean()`. Note that we need to remember
    to set the `inplace` parameter to `True` to overwrite the existing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The mean has not changed. However, there is a substantial reduction in the standard
    deviation, from 61.6 to 53.3\. This is one of the disadvantages of using the dataset's
    mean for all missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLS data also has a fair number of missing values for `wageincome`. The
    following code shows that 3,893 observations have missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rather than assigning the mean value of `wageincome` to the missings, we could
    use another common technique for imputing values: we could assign the nearest
    non-missing value from a preceding observation. The `ffill` option of `fillna`
    will do this for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We could have done a backward fill instead by setting the `method` parameter
    of `fillna` to `bfill`. This sets missing values to the nearest following value.
    This produces the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If missing values are randomly distributed, then forward or backward filling
    has one advantage over using the mean: it is more likely to approximate the distribution
    of the non-missing values for the feature. Notice that the standard deviation
    did not drop substantially.'
  prefs: []
  type: TYPE_NORMAL
- en: There are times when it makes sense to base our imputation of values on the
    mean or median value for similar observations; say, those that have the same value
    for a related feature. If we are imputing values for feature X1, and X1 is correlated
    with X2, we can use the relationship between X1 and X2 to impute a value for X1
    that may make more sense than the dataset's mean. This is pretty straightforward
    when X2 is categorical. In this case, we can impute the mean value of X1 for the
    associated value of X2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the NLS DataFrame, weeks worked in 2017 correlates with the highest degree
    earned. The following code shows how the mean value of weeks worked changes with
    degree attainment. The mean for weeks worked is 39, but it is much lower for those
    without a degree (28.72) and much higher for those with a professional degree
    (47.20). In this case, it may be a better choice to assign 28.72 to the missing
    values for weeks worked for individuals who have not attained a degree, rather
    than 39:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code assigns the mean value of weeks worked across observations
    with the same degree attainment level, for those observations missing weeks worked.
    We do this by using `groupby` to create a groupby DataFrame, `groupby([''highestdegree''])[''weeksworked17'']`.
    Then, we use `fillna` within `apply` to fill those missing values with the mean
    for the highest degree group. Notice that we make sure to only do this imputation
    for observations where the highest degree is not missing, `~nls97.highestdegree.isnull()`.
    We will still have missing values for observations that are missing both the highest
    degree and weeks worked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These imputation strategies â€“ removing observations with missing values, assigning
    a dataset's mean or median, using forward or backward filling, or using a group
    mean for a correlated feature â€“ are fine for many predictive analytics projects.
    They work best when the missing values are not correlated with the target. When
    that is true, imputing values allows us to retain the other information from those
    observations without biasing our estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, however, that is not the case and more complicated imputation strategies
    are required. The next few sections will explore multivariate techniques for cleaning
    missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing values with regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We ended the previous section by assigning a group mean to the missing values
    rather than the overall sample mean. As we discussed, this is useful when the
    feature that determines the groups is correlated with the feature that has the
    missing values. Using regression to impute values is conceptually similar to this,
    but we typically use it when the imputation will be based on two or more features.
  prefs: []
  type: TYPE_NORMAL
- en: Regression imputation replaces a feature's missing values with values predicted
    by a regression model of correlated features. This particular kind of imputation
    is known as deterministic regression imputation since the imputed values all lie
    on the regression line, and no error or randomness is introduced.
  prefs: []
  type: TYPE_NORMAL
- en: One potential drawback of this approach is that it can substantially reduce
    the variance of the feature with missing values. We can use stochastic regression
    imputation to address this drawback. We will explore both approaches in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `wageincome` feature in the NLS dataset has several missing values. We
    can use linear regression to impute values. The wage income value is the reported
    earnings for 2016:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by loading the NLS data again and checking for missing values for
    `wageincome` and features that might be correlated with `wageincome`. We also
    load the `statsmodels` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `info` method tells us that we are missing values for `wageincome` for
    nearly 3,000 observations. There are fewer missing values for the other features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s convert the `highestdegree` feature into a numeric value. This will
    make the analysis we''ll be doing in the rest of this section easier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we''ve already discovered, we need to replace logical missing values for
    `parentincome` with actual missings. After that, we can run some correlations.
    Each of the features has some correlation with `wageincome`, particularly `hdegnum`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should check whether observations with missing values for wage income are
    different in some important way from those with non-missing values. The following
    code shows that these observations have significantly lower degree attainment
    levels, parental income, and weeks worked. This is a clear case where assigning
    the overall mean would not be the best choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s try regression imputation instead. Let''s start by cleaning up the data
    a little bit more. We can replace the missing `weeksworked16` and `parentincome`
    values with their means. We should also collapse `hdegnum` into those attaining
    less than a college degree, those with a college degree, and those with a post-graduate
    degree. We can set those up as dummy variables, with 0 or 1 values when they''re
    `False` or `True`, respectively. This is a tried and true method for treating
    categorical data in regression analysis as it allows us to estimate different
    *y* intercepts based on group membership:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scikit-learn has preprocessing features that can help us with tasks like these.
    We will cover some of them in the next chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we define a function, `getlm`, to run a linear model using the `statsmodels`
    module. This function has parameters for the name of the target or dependent variable,
    `ycolname`, and the names of the features or independent variables, `xcolnames`.
    Much of the work is done by the `fit` method of `statsmodels`; that is, `OLS(y,
    X).fit()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use the `getlm` function to get the parameter estimates and the
    model summary. All of the coefficients are positive and significant at the 95%
    level since they have `pvalues` less than 0.05\. As expected, wage income increases
    with the number of weeks worked and with parental income. Having a college degree
    gives a nearly $16K boost to earnings, compared with not having a college degree.
    A post-graduate degree bumps up the earnings prediction even more â€“ almost $37K
    more than for those with less than a college degree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use this model to impute values for wage income where they are missing.
    We need to add a constant for the predictions since our model included a constant.
    We can convert the predictions into a DataFrame and then join it with the rest
    of the NLS data. Then, we can create a new wage income feature, `wageincomeimp`,
    that gets the predicted value when wage income is missing, and the original wage
    income value otherwise. Let''s also take a look at some of the predictions to
    see whether they make sense:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should look at some summary statistics for our prediction and compare those
    with the actual wage income values. The mean for the imputed wage income feature
    is lower than the original wage income mean. This is not surprising since, as
    we have seen, individuals with missing wage income have lower values for positively
    correlated features. What is surprising is the sharp reduction in the standard
    deviation. This is one of the drawbacks of deterministic regression imputation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Stochastic regression imputation adds a normally distributed error to the predictions
    based on the residuals from our model. We want this error to have a mean of 0
    with the same standard deviation as our residuals. We can use NumPy's normal function
    for that with `np.random.normal(0, lm.resid.std(), nls97.shape[0])`. The `lm.resid.std()`
    parameter gets us the standard deviation of the residuals from our model. The
    final parameter value, `nls97.shape[0]`, indicates how many values to create;
    in this case, we want a value for every row in our data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can join those values with our data and then add the error, `randomadd`,
    to our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This should increase the variance but not have much of an effect on the mean.
    Let''s confirm this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That seemed to have worked. Our stochastic prediction has pretty much the same
    standard deviation as the original wage income feature.
  prefs: []
  type: TYPE_NORMAL
- en: Regression imputation is a good way to take advantage of all the data we have
    to impute values for a feature. It is often superior to the imputation methods
    we examined in the previous section, particularly when missing values are not
    random. If we use stochastic regression imputation, we will not artificially reduce
    our variance.
  prefs: []
  type: TYPE_NORMAL
- en: Before we started using machine learning for this work, this was our go-to multivariate
    approach for imputation. We now have the option of using algorithms such as KNN
    for this task, which has advantages over regression imputation in some cases.
    KNN imputation, unlike regression imputation, does not assume a linear relationship
    between features, or that those features are normally distributed. We will explore
    KNN imputation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using KNN imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNNÂ isÂ a popular machine learning technique because it is intuitive, easy to
    run, and yields good results when there are not a large number of features and
    observations. For the same reasons, it is often used to impute missing values.
    As its name suggests, KNN identifies theÂ kÂ observations whose features are most
    similar to each observation. When it's used to impute missing values, KNN uses
    the nearest neighbors to determine what fill values to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use KNN imputation to do the same imputation we did in the previous
    section on regression imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing `KNNImputer` from scikit-learn and loading the NLS
    data again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must prepare the features. We collapse degree attainment into three
    categories â€“ less than college, college, and post-college degree â€“ with each category
    represented by a different dummy variable. We must also convert the logical missing
    values for parent income into actual missings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a DataFrame that contains just the wage income and a few correlated
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are now ready to use the `fit_transform` method of the KNN imputer to get
    values for all the missing values in the passed DataFrame, `wagedata`. `fit_transform`
    returns a NumPy array that contains all the non-missing values from `wagedata`,
    plus the imputed ones. We can convert this array into a DataFrame using the same
    index as `wagedata`. This will make it easy to join the data in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will use this technique throughout this book when we're working with the
    NumPy arrays that are returned when we use scikit-learn's `transform` and `fit_transform`
    methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to specify the value to use for the number of nearest neighbors, for
    k. We use a general rule of thumb for determining k â€“ the square root of the number
    of observations divided by 2 (*sqrt(N)/2*). That gives us 47 for k in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must join the imputed wage income and weeks worked columns with the
    original NLS wage data and make a few observations. Notice that, with KNN imputation,
    we did not need to do any pre-imputation for missing values of correlated features
    (with regression imputation, we set weeks worked and parent income to their dataset
    means). That does mean, however, that KNN imputation will return an imputation,
    even when there is not a lot of information, such as with `101122` for `personid`
    in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the summary statistics for the original and imputed features.
    Not surprisingly, the imputed wage income''s mean is lower than the original mean.
    As we discovered in the previous section, observations with missing wage incomes
    have lower degree attainment, weeks worked, and parental income. We also lose
    some of the variance in wage income:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: KNN does imputations without making any assumptions about the distribution of
    the underlying data. With regression imputation, the standard assumptions for
    linear regression apply â€“ that is, that there is a linear relationship between
    features and that they are distributed normally. If this is not the case, KNN
    is likely a better approach for imputation.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these advantages, KNN imputation does have limitations. First, we must
    tune the model with an initial assumption about a good value for k, sometimes
    informed by little more than our knowledge of the size of the dataset. KNN is
    also computationally expensive and may be impractical for very large datasets.
    Finally, KNN imputation may not perform well when the correlation is weak between
    the feature to be imputed and the predictor features. An alternative to KNN for
    imputation, random forest imputation, can help us avoid the disadvantages of both
    KNN and regression imputation. We will explore random forest imputation in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is an ensemble learning method. It uses bootstrap aggregating,
    also known as bagging, to improve model accuracy. It makes predictions by repeatedly
    taking the mean of multiple trees, yielding progressively better estimates. We
    will use the `MissForest` algorithm in this section, which is an application of
    the random forest algorithm to find missing value imputation.
  prefs: []
  type: TYPE_NORMAL
- en: '`MissForest` starts by filling in the median or mode (for continuous or categorical
    features, respectively) for missing values, then uses random forest to predict
    values. Using this transformed dataset, with missing values replaced with initial
    predictions, `MissForest` generates new predictions, perhaps replacing the initial
    prediction with a better one. `MissForest` will typically go through at least
    four iterations of this process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `MissForest` is even easier than using the KNN imputer, which we used
    in the previous section. We will impute values for the same wage income data that
    we worked with previously:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the `MissForest` module and loading the NLS data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We need to address a conflict in the name of `sklearn.neighbors._base`, which
    can be either `sklearn.neighbors._base` or `sklearn.neighbors.base`, depending
    on your version of scikit-learn. At the time of writing, `MissForest` uses the
    older name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s do the same data cleaning that we did in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to run `MissForest`. Notice that this process is quite similar
    to our process of using the KNN imputer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at a few of our imputed values and some summary statistics.
    The imputed values have a lower mean. This is not surprising, given that we have
    already learned that the missing values are not distributed randomly, that individuals
    with lower degree attainment and weeks worked are more likely to have missing
    values for wage income:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`MissForest` uses the random forest algorithm to generate highly accurate predictions.
    Unlike KNN, it doesn''t need to be tuned with an initial value for k. It also
    is computationally less expensive than KNN. Perhaps most importantly, random forest
    imputation is less sensitive to low or very high correlation among features, though
    that was not an issue in this example.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the most popular approaches for missing value imputation
    and discussed the advantages and disadvantages of each approach. Assigning an
    overall sample mean is not usually a good approach, particularly when observations
    with missing values are different from other observations in important ways. We
    can also substantially reduce our variance. Forward or backward filling allows
    us to maintain the variance in our data, but it works best when the proximity
    of observations is meaningful, such as with time series or longitudinal data.
    In most non-trivial cases, we will want to use a multivariate technique, such
    as regression, KNN, or random forest imputation.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we haven't touched on the important issue of data leakage and how to
    create separate training and testing datasets. To avoid data leakage, we need
    to work with training data independently of the testing data as soon as we begin
    our feature engineering. We will look at feature engineering in more detail in
    the next chapter. There, we will encode, transform, and scale features, while
    also being careful to separate the training and testing data.
  prefs: []
  type: TYPE_NORMAL
