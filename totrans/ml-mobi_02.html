<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Supervised and Unsupervised Learning Algorithms</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we <span>got some insight into the various aspects of</span><span> machine learning and were introduced to the various ways in which machine learning algorithms could be categorized. In this chapter, we will go a step further into machine learning algorithms and try to understand supervised and unsupervised learning algorithms. This categorization is based on the learning mechanism of the algorithm, and is the most popular.</span></p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>An introduction to the supervised learning algorithm in the form of a detailed practical example to help understand it and its guiding principles</li>
<li>The key supervised learning algorithms and their application areas:
<ul>
<li>Naive Bayes</li>
<li>Decision trees</li>
<li>Linear regression</li>
<li>Logistic regression</li>
<li>Support vector machines</li>
<li>Random forest</li>
</ul>
</li>
<li>An introduction to the unsupervised learning algorithm in the form of a detailed practical example to help understand it</li>
<li>The key unsupervised learning algorithms and their application areas:
<ul>
<li>Clustering algorithms</li>
<li>Association rule mapping</li>
</ul>
</li>
<li>A broad overview of the different mobile SDKs and tools available to implement these algorithms in mobile devices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">Let's look at supervised learning for simple day-to-day activities. A parent asks their 15-year-old son to go to the store and get some vegetables. They give him a list of vegetables, <span>say beets, carrots, beans, and tomatoes, </span>that they want him to buy. He goes to the store and is able to identify the list of vegetables as per the list provided by his mother from all the other numerous varieties of vegetables present in the store and put them in his cart before going to the checkout. How was this possible?</p>
<p class="mce-root CDPAlignLeft CDPAlign">Simple. The parent had provided enough training to the son by providing instances of each and every vegetable, which equipped him with sufficient knowledge of the vegetables. The son used the knowledge he has gained to choose the correct vegetables. He used the various attributes of the vegetables to arrive at the correct class label of the vegetable, which, in this case, is the name of the vegetable. The following table gives us a few of the attributes of the vegetables present in the list, by means of which the son was able to recognize the class label, that is, the vegetable name:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Vegetable name =</strong></p>
<p><strong>class label</strong></p>
</td>
<td>
<p><strong>Carrots</strong></p>
</td>
<td>
<p><strong>Beets</strong></p>
</td>
<td>
<p><strong>Beans</strong></p>
</td>
<td>
<p><strong>Tomatoes</strong></p>
</td>
</tr>
<tr>
<td>
<p>Attribute 1 <span>=</span> Color</p>
</td>
<td>
<p>Orange</p>
</td>
<td>
<p>Pink</p>
</td>
<td>
<p>Green</p>
</td>
<td>
<p>Red</p>
</td>
</tr>
<tr>
<td>
<p>Attribute 2 <span>=</span> Shape</p>
</td>
<td>
<p>Cone</p>
</td>
<td>
<p>Round</p>
</td>
<td>
<p>Stick</p>
</td>
<td>
<p>Round</p>
</td>
</tr>
<tr>
<td>
<p>Attribute 3 <span>=</span> Texture</p>
</td>
<td>
<p>Hard</p>
</td>
<td>
<p>Hard</p>
</td>
<td>
<p>Soft</p>
</td>
<td>
<p>Soft and juicy</p>
</td>
</tr>
<tr>
<td>
<p>Attribute 4 <span>=</span> Size</p>
</td>
<td>
<p>10 cm in length</p>
</td>
<td>
<p>3 cm radius</p>
</td>
<td>
<p>10 cm in length</p>
</td>
<td>
<p>3 cm radius</p>
</td>
</tr>
<tr>
<td>
<p>Attribute 5 <span>=</span> Taste</p>
</td>
<td>
<p>Sweet</p>
</td>
<td>
<p>Sweet</p>
</td>
<td>
<p>Bland</p>
</td>
<td>
<p>Sweet and sour</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">We just got introduced to supervised learning. We will relate this activity to the key steps of machine learning:</p>
<ul>
<li><strong>Define the ML problem</strong>: Purchasing the correct classes of vegetables from all the classes of vegetables present in the store, based on the training and experience already gained on different attributes of the vegetables.</li>
<li><strong>Prepare/gather the data and train the model</strong>: The 15-year-old son has already been trained with sufficient knowledge of all the vegetables. This knowledge of all the different types of vegetables he has seen and eaten, and of their attributes and features, forms the historical training data for the problem, for the model—the 15-year-old son.</li>
</ul>
<ul>
<li><strong>Evaluate the model</strong>: The son is asked to purchase a few vegetables from the store. This is the test set provided to him to evaluate the model. The task of the model now is to identify the correct class label of the vegetables from the store based on the list provided.</li>
</ul>
<p>There may be errors in the identification and purchase of correct vegetables in some cases. For example, the son might purchase double beans (a variant of beans) instead of ordinary beans. This may be due to a lack of sufficient training given to him on the distinguishing features between the beans and the double beans. If there is such an error, the parent would retrain him with the new type of vegetable, so that next time, he won't make that mistake.</p>
<p>So, we saw the basic concepts and functions of the supervised machine learning problem. Let's now get into the details of supervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep dive into supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>Assume there are predictor attributes, <em>x1</em>, <em>x2</em>, .... <em>xn</em>, and also an objective attribute, <kbd>y</kbd>, for a given dataset. Then, the supervised learning<span> is the</span> <span>machine </span>learning<span> task of finding the prediction function that takes as input both the predictor attributes and the objective attribute from this dataset, and is capable of mapping the predictive attributes to the objective attribute for even unseen data currently not in the training dataset with minimal error.</span></p>
<p><span>The data in the dataset used for arriving at the prediction function is called the <strong>training data</strong> and it consists of a set</span><span> </span><span>of training examples where each example consists of an input object, <kbd>x</kbd> (typically a vector), </span><span>and a desired output value, <kbd>Y</kbd>. A supervised learning algorithm analyzes the training data and produces an inferred function that maps the input to output and could also be used for mapping new, unseen example data:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>Y = f(X) + error</em></p>
<p>The whole category of algorithms is called <strong>supervised learning</strong>, because here we consider both input and output variables for learning. So learning is supervised algorithm is by providing the input as well as the expected output in the training data for all the instances of training data.</p>
<div class="packt_tip">The supervised algorithms have both predictor attributes and an objective function. The predictor attributes in a set of data items are those items that are considered to predict the objective function. The objective function is the goal of machine learning. This usually takes in the predictor attributes, perhaps with some other compute functionality, and would usually output a single numeric value.</div>
<p>Once we have defined a proper machine learning problem that would require supervised learning, the next step is to choose the machine learning algorithm that would solve the problem. This is the toughest task, because there is a huge list of learning algorithms present, and selecting the most suitable from among them is a nightmare. </p>
<p>Professor Pedro Domingos has provided a simple reference <span>architecture</span> (<a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>), on which basis we could perform the algorithm selection using on three critical components that would be required for any machine learning algorithm, as follows:</p>
<ul>
<li><strong>Representation</strong>: The way the model is represented so that it can be understood by the computer. It can also be considered as the hypothesis space within which the model would act.</li>
<li><strong>Evaluation</strong>: For each algorithm or model, there needs to be an evaluation or scoring function to determine which one performs better. The scoring function would be different for each type of algorithm.</li>
<li><strong>Optimization</strong>: A method to search among the models in the language for the highest-scoring one. The choice of optimization technique is integral to the efficiency of the learner, and also helps determine the model produced if the evaluation function has more than one optimum.</li>
</ul>
<p>Supervised learning problems can be further grouped into regression and classification problems:</p>
<ul>
<li><strong>Classification</strong>: When the output variable is a category, such as green or red, or good or bad.</li>
<li><strong>Regression</strong>: When the output variable is a real value, such as dollars or weight.</li>
</ul>
<p>In this section, we will go through the following supervised learning algorithms with easy-to-understand examples:</p>
<ul>
<li>Naive Bayes</li>
<li>Decision trees</li>
<li>Linear regression</li>
</ul>
<ul>
<li>Logistic regression</li>
<li>Support vector machines</li>
<li>Random forest</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes</h1>
                </header>
            
            <article>
                
<p>Naive Bayes is a powerful classification algorithm, implemented on the principles of Bayes theorem. It assumes that there is non-dependence between the feature variables considered in the dataset.</p>
<p>Bayes theorem<span> </span><span>describes the </span>probability<span> of an </span>event<span>, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.</span></p>
<p><span>A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a vegetable may be considered to be a carrot if it is orange, cone-shaped, and about three inches in length. The algorithm is naive as it considers all of these properties independently to contribute to the probability that this vegetable is a carrot. Generally, features are not independent, but Naive Bayes considers them so for prediction.</span></p>
<p>Let's see a practical usage where the Naive Bayes algorithm is used. Let's assume we have several news feeds and we want to classify these feeds into cultural events and non-cultural. Let's consider the following sentences:</p>
<ul>
<li><em>Dramatic event went well—cultural event</em></li>
<li><em>This good public rally had a huge crowd—non-cultural event</em></li>
<li><em>Music show was good—cultural event</em></li>
<li><em>Dramatic event had a huge crowd—cultural event</em></li>
<li><em>The political debate was very informative—non-cultural event</em></li>
</ul>
<p>When we are using Bayes theorem, all we want to do is use probabilities to calculate whether the sentences fall under cultural or non-cultural events.</p>
<p>As in the case of the carrot, we had features of color, shape, and size, and we treated all of them as independent to determine whether the vegetable considered is a carrot.</p>
<p>Similarly, to determine whether a feed is related to a cultural event, we take a sentence and then, from the sentence, consider each word as an independent feature.</p>
<p>Bayes' theorem states that <em>p(A|B) = p(B|A). P(A)/ P(B)</em>, where <em>P(Cultural Event|Dramatic show good) = P(Dramatic show good|Cultural Event).P(Cultural event)/P(Dramatic show good)</em>.</p>
<p>We can discard the denominator here, as we are determining which tag has a higher probability in both cultural and non-cultural categories. The denominator for both cultural and non-cultural events is going to be the entire dataset and, hence, the same.</p>
<p><em>P(Dramatic show good)</em> cannot be found, as this sentence doesn't occur in training data. So this is where the naive Bayes theorem really helps:</p>
<p class="CDPAlignCenter CDPAlign"><em>P( Dramatic show good) = P(Dramatic).P(show).P(good)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Dramatic show good/Cultural event) = P(Dramatic|cultural event).P(Show|cultural event)|P(good|cultural event)</em></p>
<p>Now it is easy to calculate these and determine the probability of whether the new news feed will be a cultural news feed or a political news feed:</p>
<p class="CDPAlignCenter CDPAlign"><em>P( Cultural event ) = 3/5 ( 3 out of total 5 sentences)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Non-cultural event) = 2/5</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Dramatic/cultural event) = Counting how many times Dramatic appears in cultural event tags </em><em>= 2/13 ( 2 times dramatic appears in the total number of words of cultural event tags)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P( Show/cultural event) = 1/13</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(good/cultural event) =1/13</em></p>
<div class="packt_tip">There are various techniques, such as removing stop words, lemmatizing, n-grams, and TF-IDF, that can be used to make the feature identification of text classification more effective. We will be going through a few of them in the upcoming chapters.</div>
<p>Here is the final calculated summary:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Word</strong></td>
<td><strong>P(word|cultural event)</strong></td>
<td><strong>P(word|non-cultural event)</strong></td>
</tr>
<tr>
<td>Dramatic</td>
<td>2/13</td>
<td>0</td>
</tr>
<tr>
<td>Show</td>
<td>1/13</td>
<td>0</td>
</tr>
<tr>
<td>Good</td>
<td>1/13</td>
<td>1/13</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, we just multiply the probabilities and see which is bigger, and then fit the sentence into that category of tags.</p>
<p>So we know from the table that the tag is going to belong to the cultural event category, as that is what is going to result in a bigger product when the individual probabilities are multiplied.</p>
<p>These examples have given us a good introduction to the Naive Bayes theorem, which can be applied to the following areas:</p>
<ul>
<li>Text classification</li>
<li>Spam filtering</li>
<li>Document categorization</li>
<li>Sentiment analysis in social media</li>
<li>Classification of news articles based on genre</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>Decision tree algorithms are used for making decisions based on certain conditions. A decision tree is drawn upside down with its root at the top<em>.</em><span> </span></p>
<p><span>Let's take an organization's data where the feature set consists of certain software products along with their attributes—the time taken to build the product <em>T</em>, the effort taken to build the product <em>E</em>, and the cost taken to build the product <em>C</em>. It needs to be decided whether those products are to be built in the company or should be bought as products directly from outside the company.</span></p>
<p>Now, let's see how the decision tree could be created for this. <span>In the following diagram, the bold text in black represents a condition/</span>internal node<span>, based on which the tree splits into branches/</span>edges<span>. The end of the branch that doesn't split any more is the decision/</span>leaf.</p>
<p><span>Decision trees are used in program management, project management, and risk planning. Let's see a practical example. The following diagram shows the decision tree used by an organization for deciding which of its software needs to be built in-house or be purchased as products directly from outside. There are various decision points that need to be considered before making a decision and this can be represented in the form of a tree. The three features, cost, effort, and the schedule parameters, are considered to arrive at the decision as to <strong>Buy</strong> or <strong>Build</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-931 image-border" src="assets/bf8171b2-ea97-4fe9-9674-8a03b443ba32.png" style="width:40.75em;height:27.00em;"/></p>
<p><span>The preceding tree is called a </span><strong>classification tree</strong><span> as the aim is to classify a product nature as to buy or to build. </span><strong>Regression trees</strong><span> are represented in the same manner, only they predict continuous values, such as the price of a house. In general, decision tree algorithms are referred to as <strong>CART</strong> or <strong>Classification and Regression Trees</strong>.</span></p>
<p>Decision trees can be applied to the following areas:</p>
<ul>
<li>Risk identification</li>
<li>Loan processing</li>
<li>Election result prediction</li>
<li>Process optimization</li>
<li>Optional Pricing</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p><span>Regression analysis linear regression is a statistical analysis method that finds relationships between variables. It helps us to understand the relationship between input and output numerical variables.</span></p>
<p><span>In this method, it is important to determine the dependent variables. For example, the value of the house (dependent variable) varies based on the size of the house; that is, how many square feet its area is (independent variable). The value of the house varies based on its location. Linear regression techniques can be useful for prediction.</span></p>
<p>Linear regression is used when the response is a continuous variable. The following diagram clearly shows how the linear regression for one variable work. The price of the house varies according to its size and is depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4b7c59ff-b2f7-4f3c-aa38-352a0ea6e7c3.png" style="width:24.42em;height:17.17em;"/></p>
<p>Linear regression can be applied to the following areas:</p>
<ul>
<li>Marketing</li>
<li>Pricing</li>
<li>Promotions</li>
<li>Analyzing consumer behavior</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Logistic regression is a classification algorithm that is best suited to when the output to be predicted is a binary type—true or false, male or female, win or loss, and so on. Binary type means only two outcomes are possible.</p>
<p>The logistic regression is so called because of the sigmoid function used by the algorithm.</p>
<p>A<span> </span>logistic function<span> </span>or<span> </span>logistic curve<span> </span>is a common S shape (sigmoid curve), depicted by the following equation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b2784543-67ad-4ec9-b815-c399b9c1de8b.png" style="width:11.17em;height:2.92em;"/></p>
<p class="mce-root">In the preceding equation, the symbols have the following meanings:</p>
<ul>
<li><em>e</em>: The<span> </span>natural logarithm<span> </span>base (also known as<span> </span><strong>Euler's number</strong>)</li>
<li><em>x<sub>0</sub></em>: The<span> </span>x-value of the sigmoid's midpoint</li>
<li><em>L</em>: The curve's maximum value</li>
<li><em>k</em>: The steepness of the curve</li>
</ul>
<p class="mce-root">The standard logistic function is called a <strong>sigmoid function</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/db7523fe-adb9-4808-be3a-b5b1ef5f1379.png" style="width:8.75em;height:2.92em;"/></p>
<p>The sigmoid curve is depicted here. It's an S-shaped curve:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-932 image-border" src="assets/cdbef754-0c3f-4fda-89d5-7f37df1b3aaf.png" style="width:23.58em;height:15.83em;"/></p>
<p class="ui_qtext_para">This curve has a finite limit of the following:</p>
<ul>
<li class="ui_qtext_para"><em>0</em> as <em>x</em> approaches <em>−∞</em></li>
<li class="ui_qtext_para"><em>1</em> as <em>x</em> approaches <em>+∞</em></li>
</ul>
<p class="ui_qtext_para">The output of the sigmoid function when <em>x=0</em> is <em>0.5</em>.</p>
<p class="ui_qtext_para">Thus, if the output is more than <em>0.5</em>, we can classify the outcome as 1 (or <strong>YES</strong>), and, if it is less than <em>0.5</em>, we can classify it as 0 (or <strong>NO</strong>). For example: if the output is <em>0.65</em>, in probability terms, it can be interpreted as—<em>There is a 65 percent chance that it is going to rain today.</em></p>
<p class="ui_qtext_para">Thus, the output of the sigmoid function cannot just be used to classify yes/no; it can also be used to determine the probability of yes/no. It can be applied to the following areas:</p>
<ul>
<li>Image segmentation and categorization</li>
<li>Geographic image processing</li>
<li>Handwriting recognition</li>
<li>Healthcare, for disease prediction and gene analytics</li>
<li>Prediction in various areas where a binary outcome is expected</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p><span>A <strong>support vector machine</strong> (<strong>SVM</strong>) is a supervised machine learning algorithm that can be used for both classification and regression. SVMs are more commonly used for classification.</span></p>
<p><span>Given some data points, each belonging to one of the two binary classes, the goal is to decide which class a new data point will be in. We need to visualize the data point as a </span><span><span class="katex"><span class="katex-mathml">p</span></span></span><span>-dimensional vector, and we need to determine whether we can separate two such data points with a (</span><span><span class="katex"><span class="katex-mathml">p-1</span></span></span><span>) dimensional hyperplane.</span></p>
<p>There may be many hyper planes that separate such data points, and this algorithm will help us to arrive at the best hyperplane that provides the largest separation. This hyperplane is called the <span><strong>maximum-margin hyperplane</strong>, and the classifier is called the <strong>maximum-margin classifier</strong>. </span><span>We can extend the concept of a separating hyperplane to develop a hyperplane that </span>almost <span>separates the classes, using a so-called </span><strong>soft margin</strong><span>. The generalization of the maximal margin classifier to the non-separable case is known as the </span><strong>support vector</strong><span> <strong>classifier</strong>.</span></p>
<p>Let's take the first example. In this, there is one hyperplane that separates the red dots and the blue dots:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-946 image-border" src="assets/df1c4793-a07d-4227-b3c7-7a8da1056fb3.png" style="width:25.00em;height:20.50em;"/></p>
<p>But imagine that the points were distributed as follows—how will we identify the hyperplane that separates the red dots and the blue dots:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-947 image-border" src="assets/4d12a5c3-2f11-4e4e-bc58-0b28b38566e3.png" style="width:25.42em;height:21.25em;"/></p>
<p>The solution is to identify the hyperplane with SVM. It can execute transformations to identify the hyperplane that separates the two for classification. It will introduce a new feature, <em>z</em>, which is <span><em>z=x^2+y^2</em>. Let's plot the graph with the <em>x</em> and <em>z</em> axes, and identify the hyperplane for classification:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-948 image-border" src="assets/8c4da293-facf-4976-bfa8-d8d5187bb495.png" style="width:26.42em;height:22.08em;"/></p>
<p>Now that we understand the basics of SVM, let's look at the areas where it can be applied:</p>
<ul>
<li>Face detection</li>
<li>Image classification</li>
<li>Bioinformatics</li>
<li>Geological and environmental sciences</li>
<li>Genetics</li>
<li>Protein studies</li>
<li>Handwriting recognition</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest</h1>
                </header>
            
            <article>
                
<p>We have already seen what a decision tree is. Having understood decision trees, let's take a look at random forests. A random forest combines many decision trees into a single model. Individually, predictions made by decision trees (or humans) may not be accurate, but combined together, the predictions will be closer to the mark, on average.</p>
<p>The following diagram shows us a random forest, where there are multiple trees and each is making a prediction:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-933 image-border" src="assets/dddb6689-4e22-41f6-93b7-20556116d586.png" style="width:34.92em;height:17.17em;"/></p>
<p>Random forest is a combination of many decision trees and, hence, there is a greater probability of having many views from all trees in the forest to arrive at the final desired outcome/prediction. <span>If only a single decision tree is taken into consideration for prediction, there is less information considered for prediction. But in random forest, when there are many trees involved, the source of information is diverse and extensive. </span><span>Unlike decision trees, random forests are not biased, since they are not dependent on one source. </span></p>
<p>The following diagram demonstrates the concept of random forests:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-935 image-border" src="assets/2c788ab0-cf09-4c26-ad32-dc464266e75e.png" style="width:15.58em;height:21.08em;"/></p>
<p>Random forests can be applied to the following areas:</p>
<ul>
<li>Risk identification</li>
<li>Loan processing</li>
<li>Election result prediction</li>
<li>Process optimization</li>
<li>Optional pricing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to unsupervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>Consider a scenario where a child is given a bag full of beads of different sizes, colors, shapes, and made of various materials. We just leave to the child do whatever they want with the whole bag of beads. </p>
<p>There are various things the child could do, based on their interests:</p>
<ul>
<li>Separate the beads into categories based on size</li>
<li>Separate the beads into categories based on shape</li>
</ul>
<ul>
<li>Separate the beads into categories based on a combination of color and shape</li>
<li>Separate the beads into categories based on a combination of material, color, and shape</li>
</ul>
<p>The possibilities are endless. However, the child without any prior teaching is able to go through the beads and uncover patterns of which it doesn't need any any prior knowledge at all. They are discovering the patterns purely on the basis of going through the beads at hand, that is, the data at hand. We just got introduced to unsupervised machine learning!</p>
<p class="mce-root">We will relate the preceding activity to the key steps of machine learning:</p>
<ol>
<li><strong>Define the ML problem</strong>: Uncover hidden patterns of beads from the given bag of beads.</li>
<li><strong>Prepare/gather the data and train the model</strong>: The child opens the bagful of beads and understands what the bag contains. They discover the attributes of the different beads present:
<ul>
<li>Color</li>
<li>Shape</li>
<li>Size</li>
<li>Material</li>
</ul>
</li>
<li><strong>Evaluate the model</strong>: If a new set of beads is given to the child, how will they cluster these beads based on their previous experience of clustering beads?</li>
</ol>
<p>There may be errors in grouping the beads <span><span>that </span></span>need to be corrected/fixed so that they don't recur in future.</p>
<p>So, now that we have seen the basic concepts and functions of the unsupervised machine learning problem, let's get into the details of unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep dive into unsupervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>Unsupervised machine learning<span> deals with learning</span><span> unlabeled data—that is, data that has not been classified or categorized, and arriving at conclusions/patterns in relation to them.</span></p>
<p class="mce-root">These categories learn from test data that has not been labeled, classified, or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.</p>
<p><span>The input given to the learning algorithm is unlabeled and, hence, there is no straightforward way to evaluate the accuracy of the structure that is produced as output by the algorithm. This is one feature that distinguishes unsupervised learning from </span>supervised learning<span>. </span></p>
<div class="packt_tip"><span>The unsupervised algorithms have predictor attributes but <strong>NO</strong> objective function.</span></div>
<p>What does it mean to learn without an objective? Consider the following:</p>
<ul>
<li>Explore the data for natural groupings.</li>
<li>Learn association rules, and later examine whether they can be of any use.</li>
</ul>
<p>Here are some classic examples:</p>
<ul>
<li>Performing market basket analysis and then optimizing shelf allocation and placement</li>
<li>Cascaded or correlated mechanical faults</li>
<li>Demographic grouping beyond known classes</li>
<li>Planning product bundling offers</li>
</ul>
<p>In this section, we will go through the following unsupervised learning algorithms with easy-to-understand examples:</p>
<ul>
<li>Clustering algorithms</li>
<li>Association rule mapping</li>
</ul>
<div class="packt_infobox"><strong>Principal component analysis</strong> (<strong>PCA</strong>) and <span><strong>singular value decomposition</strong> (<strong>SVD</strong>) may also be of interest if you want to deep dive into those concepts</span><span>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">Clustering the dataset into useful groups is what clustering algorithms do. <span>The goal of clustering is to create groups of data points, such that points in different clusters are dissimilar, while points within a cluster are similar.</span></p>
<p>There are two essential elements for clustering algorithms to work:</p>
<ul>
<li><strong>Similarity function</strong>: This determines how we decide that two points are similar.</li>
<li><strong>Clustering method</strong>: This is the method observed in order to arrive at clusters.</li>
</ul>
<p>There needs to be a mechanism to determine similarity between points, on which basis they could be categorized as similar or dissimilar. There are various similarity measures. Here are a few:</p>
<ul>
<li><strong>Euclidean</strong>:</li>
</ul>
<p style="padding-left: 180px"><img src="assets/447a4ef5-4329-41b4-b2ea-e6c7841b00ac.png" style="width:23.92em;height:5.67em;"/></p>
<ul>
<li><strong><span>Cosine</span></strong><span>:</span></li>
</ul>
<p style="padding-left: 210px"><img src="assets/7763f67b-7e38-4083-9787-9bb1fb188549.png" style="width:15.42em;height:5.67em;"/></p>
<ul>
<li><strong>KL-divergence</strong>:</li>
</ul>
<p style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="assets/13af89d4-c884-4b40-b37f-cd26372c34c1.png" style="width:21.92em;height:4.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering methods</h1>
                </header>
            
            <article>
                
<p>Once we know the similarity measure, we next need to choose the clustering method. We will go through two clustering methods:</p>
<ul>
<li>Hierarchical <span>agglomerative </span>clustering methods</li>
<li>K-means clustering</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical agglomerative clustering methods</h1>
                </header>
            
            <article>
                
<p>Agglomerative hierarchical clustering is a classical clustering algorithm from the statistics domain. It involves iterative merging of the two most similar groups, which, in the first instance, contain single elements. The name of the algorithm refers to its way of working, as it creates hierarchical results in an agglomerative or bottom-up way, that is, by merging smaller groups into larger ones.</p>
<p>Here is the high-level algorithm for this method of clustering used in document clustering.</p>
<ol>
<li>Generic agglomerative process (Salton, G: <em>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</em>, <em>Addison-Wesley</em>, 1989) result in nested clusters via iterations.</li>
<li>Compute all pairwise document-document similarity coefficients</li>
<li>Place each of the <em>n</em> documents into a class of its own</li>
<li>Merge the two most similar clusters into one:
<ul>
<li>Replace the two clusters with the new cluster</li>
<li>Recompute inter-cluster similarity scores with regard to the new cluster</li>
<li>If the cluster radius is greater than maxsize, block further merging</li>
</ul>
</li>
<li>Repeat the preceding step until there are only <em>k</em> clusters left (note: <em>k</em> could equal <em>1</em>)</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means clustering</h1>
                </header>
            
            <article>
                
<p><span>The goal of this K-means clustering algorithm is to find K groups in the data, with each group having similar data points.</span><span> The algorithm works iteratively to assign each data point to one of </span><em>K</em><span> groups based on the features that are provided. Data points are clustered based on feature similarity.</span></p>
<p>The K value is assigned randomly at the beginning of the algorithm and different variations of results could be obtained by altering the K value. Once the algorithm sequence of activities is initiated after the selection of K, as depicted in the following points, we find that there are two major steps that keep repeating, until there is no further scope for changes in the clusters.</p>
<p>The two major steps that get repeated are <em>Step 2</em> and <em>Step 3</em>, depicted as follows:</p>
<ul>
<li><strong>Step 2</strong>: Assigning the data point from the dataset to any of the K clusters. This is done by calculating the distance of the data point from the cluster centroid. As specified, any one of the distance functions that we discussed already could be used for this calculation.</li>
<li><strong>Step 3</strong>: Here again, recalibration of the centroid occurs. <span>This is done by taking the mean of all data points assigned to that centroid cluster.</span></li>
</ul>
<p>The final output of the algorithm is K clusters that have similar data points:</p>
<ol>
<li>Select <em>k-seeds d(k<sub>i</sub>,kj) &gt; d<sub>min</sub></em></li>
<li>Assign points to clusters according to minimum distance: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/226eeaa2-a75e-4049-8e39-175067e5597c.png" style="width:53.33em;height:3.25em;"/></p>
<ol start="3">
<li>Compute new cluster centroids:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c5c1f8a6-f9f6-4bc7-bfc6-466d617ba5d1.png" style="width:11.75em;height:4.67em;"/></p>
<ol start="4">
<li>Reassign points to the cluster (as in <em>Step 2</em>)</li>
<li>Iterate until no points change the cluster. </li>
</ol>
<p>Here are some areas where clustering algorithms are used:</p>
<ul>
<li>City planning</li>
<li>Earthquake studies</li>
<li>Insurance</li>
<li>Marketing</li>
<li>Medicine, for the <span>analysis of antimicrobial activity and medical imaging</span> </li>
<li>Crime analysis</li>
<li>Robotics, for anomaly detection and natural language processing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Association rule learning algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">Association rule mining is more useful for categorical non-numeric data. <span>Association rule mining is primarily focused on finding frequent co-occurring associations among a collection of items. It is sometimes also called <strong>market-basket analysis</strong>.</span></p>
<p><span>In a shopper's basket, the goal is to determine what items occur together frequently. This shows co-relations that are very hard to find from a random sampling method. </span><span>The classic example of this is the famous Beer and Diapers association, which is often mentioned in data mining books. The scenario is this: men who go to the store to buy diapers will also tend to buy beer. This scenario is very hard to intuit or determine through random sampling.</span></p>
<p><span>Another example was discovered by Walmart in 2004, when a series of hurricanes crossed Florida. Walmart wanted to know what shoppers usually buy before a hurricane strikes. They found one particular item that increased in sales by a factor of seven over normal shopping days; that item was not bottled water, batteries, beer, flashlights, generators, or any of the usual things that we might imagine. The item was <strong>strawberry pop tarts</strong>! It is possible to conceive a multitude  of reasons as to why this was the most desired product prior to the arrival of a hurricane–pop tarts do not require refrigeration, they do not need to be cooked, they come in individually wrapped portions, they have a long shelf life, they are a snack food, they are a breakfast food, kids love them, we love them, the list goes on. Despite these obvious reasons, it was still a huge surprise! </span></p>
<p>When mining for associations, the following could be useful:</p>
<ul>
<li><span>Search for rare and unusual co-occurring associations of non-numeric items.</span></li>
<li><span>If the data is time-based data, consider the effects of introducing a time lag in data mining experiments to see whether the strength of the correlation reaches its peak at a later time.</span></li>
</ul>
<p>Market-basket analysis can be applied to the following areas:</p>
<ul>
<li>Retail management</li>
<li>Store management</li>
<li>Inventory management</li>
<li>NASA and environmental studies</li>
<li>Medical diagnoses</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about <span>what supervised learning is through a naive example and d</span><span>eep dived into concepts of supervised learning. We w</span><span>ent through various supervised learning algorithms with practical examples and their application areas and t</span><span>hen we started going through unsupervised learning with naive examples. We also covered the </span><span>concepts of unsupervised learning and t</span><span>hen we went through various unsupervised learning algorithms with practical examples and their application areas.</span></p>
<p>In the subsequent chapters, we will be solving mobile machine learning problems by using some of the supervised and unsupervised machine learning algorithms that we have gone through in this chapter. W<span>e will also be exposing you to mobile machine learning SDKs, which will be used to implement mobile machine learning solutions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Dr. Pedro Domingo's paper—<a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>,<a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"> </a>summarizes twelve key lessons that machine learning researchers and practitioners have learned, including pitfalls to avoid, important issues to focus on, and answers to common questions in this area.</li>
</ul>


            </article>

            
        </section>
    </body></html>