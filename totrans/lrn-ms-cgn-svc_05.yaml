- en: Chapter 5. Speaking with Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to discover and understand the intent
    of a user, based on utterances. In this chapter, we will learn how to add audio
    capabilities to our applications, convert text to speech and speech to text, and
    learn how to identify the person speaking. Throughout this chapter, we will learn
    how you can utilize spoken audio to verify a person. Finally, we will briefly
    touch on how to customize speech recognition to make it unique for your application's
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we will have covered the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting spoken audio to text and text to spoken audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing intent from spoken audio by utilizing LUIS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying that the speaker is who they claim to be
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the speaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailoring the Speaker Recognition API to recognize custom speaking styles and
    environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting text to audio and vice versa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive
    Services"), *Getting Started with Microsoft Cognitive Services*, we utilized a
    part of the Bing Speech API. We gave the example application the ability to say
    sentences to us. We will use the code that we created in that example now, but
    we will dive a bit deeper into the details.
  prefs: []
  type: TYPE_NORMAL
- en: We will also go through the other feature of Bing Speech API, that is, converting
    spoken audio to text. The idea is that we can speak to the smart-house application,
    which will recognize what we are saying. Using the textual output, the application
    will use LUIS to gather the intent of our sentence. If LUIS needs more information,
    the application will politely ask us for more via audio.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, we want to modify the build definition of the smart-house application.
    We need to specify whether we are running it on a 32-bit or 64-bit OS. To utilize
    speech-to-text conversion, we want to install the Bing Speech NuGet client package.
    Search for `Microsoft.ProjectOxford.SpeechRecognition` and install either the
    32-bit version or the 64-bit version, depending on your system.
  prefs: []
  type: TYPE_NORMAL
- en: Further on, we need to add references to `System.Runtime.Serialization` and
    `System.Web`. These are needed so that we are able to make web requests and deserialize
    response data from the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking to the application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Add a new file to the `Model` folder, called `SpeechToText.cs`. Beneath the
    automatically created `SpeechToText` class, we want to add an `enum` type variable
    called `SttStatus`. It should have two values, `Success` and `Error`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we want to define an `EventArgs` class for events that we will
    raise during execution. Add the following class at the bottom of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `event` argument will hold the operation status, a message
    of any kind, and a list of strings. This will be a list with potential speech-to-text
    conversions.
  prefs: []
  type: TYPE_NORMAL
- en: The `SpeechToText` class needs to implement `IDisposable`. This is done so that
    we can clean up the resources used for recording spoken audio and shut down the
    application properly. We will add the details presently, so for now, just make
    sure to add the `Dispose` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to define a few private members in the class, as well as an event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `OnSttStatusUpdated` event will be triggered whenever we have a new operation
    status. `DataRecognitionClient` and `MicrophoneRecognitionClient` are the two
    objects that we can use to call the Bing Speech API. We will look at how they
    are created presently.
  prefs: []
  type: TYPE_NORMAL
- en: We define `SpeechRecognitionMode` as `ShortPhrase`. This means that we do not
    expect any spoken sentences longer than 15 seconds. The alternative is `LongDictation`,
    which means that we can convert spoken sentences to be up to 2 minutes long.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we specify the language to be English, and define a `bool` type variable,
    which indicates whether or not we are currently recording anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our constructor, we accept the Bing Speech API key as a parameter. We will
    use this in the creation of our API clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we create both `_dataRecClient` and `_micRecClient` by calling
    `SpeechRecognitionServiceFactory`. For the first client, we state that we want
    to use intent recognition as well. The parameters required are the language, Bing
    API key, the LUIS app ID, and the LUIS API key. By using a `DataRecognitionClient`
    object, we can upload audio files with speech.
  prefs: []
  type: TYPE_NORMAL
- en: By using `MicrophoneRecognitionClient`, we can use a microphone for real-time
    conversion. For this, we do not want intent detection, so we call `CreateMicrophoneClient`.
    In this case, we only need to specify the speech mode, the language, and the Bing
    Speech API key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before leaving the constructor, we call the `Initialize` function. In this,
    we subscribe to certain events on each of the clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are quite a few similarities between the two clients.
    The two differences are that `_dataRecClient` will get intents through the `OnIntent`
    event, and `_micRecClient` will get the microphone status through the `OnMicrophoneStatus`
    event.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not really care about partial responses. However, they may be useful
    in some cases, as they will continuously give the currently completed conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For our application, we will choose to output it to the debug console window.
    In this case, `PartialResult` is a string with the partially converted text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We do not care about the current microphone status, either. Again, we output
    the status to the debug console window.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, add a helper function, called `RaiseSttStatusUpdated`. This
    should raise `OnSttStatusUpdated` when called.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we are calling `_dataRecClient`, we may recognize intents from LUIS. In
    these cases, we want to raise an event, where we output the recognized intent.
    This is done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We choose to print out intent information and the `Payload`. This is a string
    containing recognized entities, intents, and actions that are triggered from LUIS.
  prefs: []
  type: TYPE_NORMAL
- en: 'If any errors occur during the conversion, there are several things we will
    want to do. First and foremost, we want to stop any microphone recordings that
    may be running. There is really no point in trying to convert more in the current
    operation if it has failed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will create `StopMicRecording` presently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we want to notify any subscribers that the conversion failed.
    In such cases, we want to give details about error codes and error messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `OnConversationError` event does, fortunately, provide us with detailed
    information about any errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the `StopMicRecording` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple function that calls `EndMicAndRecognition` on the `_micRecClient
    MicrophoneRecognitionClient` object. When this is called, we stop the client from
    recording.
  prefs: []
  type: TYPE_NORMAL
- en: The final event handler that we need to create is the `OnResponseReceived` handler.
    This will be triggered whenever we receive a complete, converted response from
    the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we want to make sure we do not record any more if we are currently recording:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SpeechResponseEventArgs` argument contains a `PhraseResponse` object.
    This contains an array of `RecognizedPhrase`, which we want to access. Each item
    in this array contains the confidence of correct conversion. It also contains
    the converted phrases as `DisplayText`. This uses inverse text normalization,
    proper capitalization, and punctuation, and it masks profanities with asterisks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We may also get the converted phrases in other formats, as described in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `LexicalForm` | This is the raw, unprocessed recognition result. |'
  prefs: []
  type: TYPE_TB
- en: '| `InverseTextNormalizationResult` | This displays phrases such as *one two
    three four* as *1234*, so it is ideal for usages such as *go to second street*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MaskedInverseTextNormalizationResult` | Inverse text normalization and the
    profanity mask. No capitalization or punctuation is applied. |'
  prefs: []
  type: TYPE_TB
- en: 'For our use, we are just interested in the `DisplayText`. With a populated
    list of recognized phrases, we raise the status update event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to use this class, we need a couple of public functions so that
    we can start speech recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `StartMicToText` method will call the `StartMicAndRecognition` method on
    the `_micRecClient` object. This will allow us to use the microphone to convert
    spoken audio. This function will be our main way of accessing this API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function will require a filename for the audio file, with the audio
    we want to convert. We open the file, with read access, and are ready to read
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As long as we have data available, we read from the file. We will fill up the
    `buffer`, and call the `SendAudio` method. This will then trigger a recognition
    operation in the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'If any exceptions occur, we make sure to output the exception message to a
    debug window. Finally, we need to call the `EndAudio` method so that the service
    does not wait for any more data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before leaving this class, we need to dispose of our API clients. Add the following
    in the `Dispose` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We stop microphone recording, unsubscribe from all events, and dispose and clear
    the client objects.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that the application compiles before moving on. We will look at how
    to use this class presently.
  prefs: []
  type: TYPE_NORMAL
- en: Letting the application speak back
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen how to make the application speak back to us. We are going
    to use the same classes we created in [Chapter 1](ch01.html "Chapter 1. Getting
    Started with Microsoft Cognitive Services"), *Getting Started with Microsoft Cognitive
    Services*. Copy `Authentication.cs` and `TextToSpeech.cs` from the example project
    from [Chapter 1](ch01.html "Chapter 1. Getting Started with Microsoft Cognitive
    Services"), *Getting Started with Microsoft Cognitive Services,* into the `Model`
    folder. Make sure that the namespaces are changed accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: As we have been through the code already, we will not go through it again. We
    will instead look at some of the details left out in [Chapter 1](ch01.html "Chapter 1. Getting
    Started with Microsoft Cognitive Services"), *Getting Started with Microsoft Cognitive
    Services*.
  prefs: []
  type: TYPE_NORMAL
- en: Audio output format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The audio output format can be one of the following formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '`raw-8khz-8bit-mono-mulaw`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`raw-16khz-16bit-mono-pcm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`riff-8khz-8bit-mono-mulaw`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`riff-16khz-16bit-mono-pcm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are four possible error codes that can occur in calls to the API. These
    are described in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Code | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `400 / BadRequest` | A required parameter is missing, empty, or null. Alternatively,
    a parameter is invalid. An example may be a string that''s longer than the allowed
    length. |'
  prefs: []
  type: TYPE_TB
- en: '| `401 / Unauthorized` | The request is not authorized. |'
  prefs: []
  type: TYPE_TB
- en: '| `413 / RequestEntityTooLarge` | The SSML input is larger than what''s supported.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `502 / BadGateway` | A network-related or server-related issue. |'
  prefs: []
  type: TYPE_TB
- en: Supported languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following languages are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: English (Australia), English (United Kingdom), English (United States), English
    (Canada), English (India), Spanish, Mexican Spanish, German, Arabic (Egypt), French,
    Canadian French, Italian, Japanese, Portuguese, Russian, Chinese (S), Chinese
    (Hong Kong), and Chinese (T).
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing LUIS based on spoken commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To utilize the features that we have just added, we are going to modify `LuisView`
    and `LuisViewModel`. Add a new `Button` in the View, which will make sure that
    we record commands. Add a corresponding `ICommand` in the ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to add a few more members to the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first two will be used to convert between spoken audio and text. The third
    is the API key for the Bing Speech API.
  prefs: []
  type: TYPE_NORMAL
- en: Make the ViewModel implement `IDisposable`, and explicitly dispose the `SpeechToText`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the objects by adding the following in the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create the client objects and subscribe to the required events. Finally,
    it will call a function to generate authentication tokens for the REST API calls.
    This function should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we receive any errors from `_ttsClient`, we want to output it to the debug
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We do not need to output this to the UI, as this is a nice-to-have feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have audio available, we want to make sure that we play it. We do so
    by creating a `SoundPlayer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Using the audio stream we got from the event arguments, we can play the audio
    to the user.
  prefs: []
  type: TYPE_NORMAL
- en: If we have a status update from `_sttClient`, we want to display this in the
    textbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have successfully recognized spoken audio, we want to show the `Message`
    string if it is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to show all recognized phrases. Using the first available phrase,
    we make a call to LUIS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If the recognition failed, we print out any error messages that we may have.
    Finally, we make sure that the `ResultText` is updated with the new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The newly created `ICommand` needs to have a function to start the recognition
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The function starts the microphone recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to make some modifications to `OnLuisUtteranceResultUpdated`.
    Make the following modifications, where we output any `DialogResponse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This will play the `DialogResponse` if it exists. The application will ask you
    for more information if required. It will then start the recording, so we can
    answer without clicking any buttons.
  prefs: []
  type: TYPE_NORMAL
- en: If no `DialogResponse` exists, we simply make the application say the summary
    to us. This will contain data on intents, entities, and actions from LUIS.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing who is speaking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the **Speaker Recognition** API, we can identify who is speaking. By defining
    one or more speaker profiles with corresponding samples, we can identify whether
    any of them are speaking at any time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to utilize this feature, we need to go through a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to add one or more speaker profiles to the service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each speaker profile enrolls several spoken samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We call the service to identify a speaker based on audio input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have not already done so, sign up for an API key for the Speaker Recognition
    API at [https://portal.azure.com](https://portal.azure.com).
  prefs: []
  type: TYPE_NORMAL
- en: Start by adding a new NuGet package to your smart-house application. Search
    for and add `Microsoft.ProjectOxford.SpeakerRecognition`.
  prefs: []
  type: TYPE_NORMAL
- en: Add a new class called `SpeakerIdentification` to the `Model` folder of your
    project. This class will hold all of the functionality related to speaker identification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beneath the class, we will add another class, containing `EventArgs` for status
    updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The two first properties should be self-explanatory. The last one, `IdentificationProfile`,
    will hold the results of a successful identification process. We will look at
    what information this contains presently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also want to send events for errors, so let''s add an `EventArgs` class
    for the required information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Again, the property should be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `SpeakerIdentification` class, add two events and one private member
    at the top of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The events will be triggered if we have any status updates, a successful identification,
    or errors. The `ISpeakerIdentificationServiceClient` object is the access point
    for the Speaker Recognition API. Inject this object through the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to raise events, add two helper functions, one for each event.
    Call these `RaiseOnIdentificationStatusUpdated` and `RaiseOnIdentificationError`.
    They should accept the corresponding `EventArgs` object as a parameter and trigger
    the corresponding event.
  prefs: []
  type: TYPE_NORMAL
- en: Adding speaker profiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be able to identify speakers, we need to add profiles. Each profile can be
    seen as a unique person who we can identify later.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, each subscription allows for 1,000 speaker profiles
    to be created. This also includes profiles that are created for verification,
    which we will look at presently.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate creating profiles, we need to add some elements to our `AdministrationView`
    and `AdministrationViewModel` properties, so open these files.
  prefs: []
  type: TYPE_NORMAL
- en: In the View, add a new button for adding speaker profiles. Also, add a list
    box, which will show all of our profiles. How you lay out the UI is up to you.
  prefs: []
  type: TYPE_NORMAL
- en: The ViewModel will need a new `ICommand` property for the button. It will also
    need an `ObservableObject` property for our profile list; make sure it is of type
    `Guid`. We will also need to be able to select a profile, so add a `Guid` property
    for the selected profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we need to add a new member to the ViewModel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the reference to the class we created earlier. Create this object in
    the constructor, passing on an `ISpeakerIdentificationServiceClient` object, which
    you inject via the ViewModel''s constructor. In the constructor, you should also
    subscribe to the events we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Basically, we want both event handles to update the status text with the message
    they carry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is for `OnSpeakerIdentificationStatusUpdated`. The same should
    be used for `OnSpeakerIdentificationError`, but set `StatusText` to be `e.ErrorMessage`
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the function created for our `ICommand` property, we do the following to
    create a new profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We make a call to our `_speakerIdentification` object''s `CreateSpeakerProfile`
    function. This function will return a `Guid`, which is the unique ID of that speaker.
    In our example, we do not do anything further with this. In a real-life application,
    I would recommend mapping this ID to a name in some way. As you will see presently,
    identifying people through GUIDs is for machines, not people:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We finish this function by calling a `GetSpeakerProfile` function, which we
    will create next. This will fetch a list of all the profiles we have created so
    that we can use these throught the further process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In our `GetSpeakerProfiles` function, we call `ListSpeakerProfiles` on our
    `_speakerIdentification` object. This will, as we will see presently, fetch a
    list of GUIDs, containing the profile IDs. If this list is null, there is no point
    in moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If the list does contain anything, we add these IDs to our `SpeakerProfiles`,
    which is the `ObservableCollection` property. This will show all of our profiles
    in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: This function should also be called from the `Initialize` function, so we populate
    the list when we start the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the `SpeakerIdentification` class, create a new function called `CreateSpeakerProfile`.
    This should have the return type `Task<Guid>` and be marked as `async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We will then make a call to `CreateProfileAsync` on the API object. We need
    to specify the locale, which is used for the speaker profile. At the time of writing,
    `en-US` is the only valid option.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the call is successful, we get a `CreateProfileResponse` object in response.
    This contains the ID of the newly created speaker profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: If the `response` is null, we raise an error event. If it contains data, we
    return the `ProfileId` to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Add the corresponding `catch` clause to finish the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new function called `ListSpeakerProfile`. This should return `Task<List<Guid>>`
    and be marked as `async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create a list of type `Guid`, which is the list of speaker profiles
    we will return. Then, we call the `GetProfilesAsync` method on our `_speakerIdentificationClient`
    object. This will get us an array of type `Profile`, which contains information
    on each profile. This is information such as creation time, enrollment status,
    last modified, and so on. We are interested in the IDs of each profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: If any profiles are returned, we loop through the array and add each `profileId`
    to the previously created list. This list is then returned to the caller, which
    in our case will be the ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: End the function with the corresponding `catch` clause. Make sure that the code
    compiles and executes as expected before continuing. This means that you should
    now be able to add speaker profiles to the service and get the created profiles
    displayed in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delete a speaker profile, we will need to add a new function to `SpeakerIdentification`.
    Call this function `DeleteSpeakerProfile`, and let it accept a `Guid` as its parameter.
    This will be the ID of the given profile we want to delete. Mark the function
    as `async`. The function should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the call to the `DeleteProfileAsync` method expects a `Guid`
    type, `profileId`. There is no return value and, as such, when we call this function,
    we need to call the `GetSpeakerProfile` method in our ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate the deletion of speaker profiles, add a new button to the UI and
    a corresponding `ICommand` property in the ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: Enrolling a profile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a speaker profile in place, we need to associate spoken audio with the
    profile. We do this through a process called **enrolling**. For speaker identification,
    enrolling is text-independent. This means that you can use whatever sentence you
    want for enrollment. Once the voice is recorded, a number of features will be
    extracted to form a unique voice-print.
  prefs: []
  type: TYPE_NORMAL
- en: When enrolling, the audio file you are using must be 5 seconds at least and
    5 minutes at most. Best practice states that you should accumulate at least 30
    seconds of speech. This is 30 seconds *after* silence has been removed, so several
    audio files may be required. This recommendation can be avoided by specifying
    an extra parameter, as we will see presently.
  prefs: []
  type: TYPE_NORMAL
- en: How you choose to upload the audio file is up to you. In the smart-house application,
    we will use a microphone to record live audio. To do so, we will need to add a
    new NuGet package called **NAudio**. This is an audio library for .NET, which
    simplifies audio work.
  prefs: []
  type: TYPE_NORMAL
- en: We will also need a class to deal with recording, which is out of the scope
    of this book. As such, I recommend you copy the `Recording.cs` file, which can
    be found in the sample project in the `Model` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `AdministrationViewModel` ViewModel, add a private member for the newly
    copied class. Create the class and subscribe to the events defined in the `Initialize`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We have an event for errors and one for available audio stream. Let `OnRecordingError`
    print the `ErrorMessage` to the status text field.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `OnAudioStreamAvailable`, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Here, we call `CreateSpeakerEnrollment` on the `_speakerIdentification` object.
    We will cover this function presently. The parameters we pass on are the `AudioStream`,
    from the recording, as well as the ID of the selected profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to get audio files for enrollment, we need to start and stop the
    recording. This can be done by simply adding two new buttons, one for start and
    one for stop. They will then need to execute one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in the `SpeakerIdentification.cs` file, we need to create a new function,
    `CreateSpeakerEnrollment`. This should accept `Stream` and `Guid` as parameters,
    and be marked as `async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we call the `EnrollAsync` function on `_speakerIdentificationClient`.
    This function requires both the `audioStream` and `profileId` as parameters. An
    optional third parameter is a `bool` type variable, which lets you decide whether
    or not you would like to use the recommended speech length or not. The default
    is `false`, meaning that you use the recommended setting of at least 30 seconds
    of speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the call is successful, we get an `OperationLocation` object back. This
    holds a URL that we can query for the enrollment status, which is precisely what
    we will do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: First, we make sure that we have the `location` data. Without it, there is no
    point in moving on. If we do have the `location` data, we call a function, `GetEnrollmentOperationStatus`,
    specifying the `location` as the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Add the corresponding `catch` clause to finish the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `GetEnrollmentOperationStatus` method accepts `OperationLocation` as a
    parameter. When we enter the function, we move into a `while` loop, which will
    run until the operation completes. We call `CheckEnrollmentStatusAsync`, specifying
    the `location` as the parameter. If this call is successful, it will return an
    `EnrollmentOperation` object, which contains data such as status, enrollment speech
    time, and an estimation of the time of enrollment left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'When we have retrieved the result, we check to see if the status is running
    or not. If it isn''t, the operation has either failed, succeeded, or not started.
    In any case, we do not want to check any further, so we send an update with the
    status and break out of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: If the status is still running, we update the status and wait for 1 second before
    trying again.
  prefs: []
  type: TYPE_NORMAL
- en: 'With enrollment completed, there may be times when we need to reset the enrollment
    for a given profile. We can do so by creating a new function in `SpeakerIdentification`.
    Name it `ResetEnrollments`, and let it accept a `Guid` as a parameter. This should
    be the profile ID of the speaker profile to reset. Execute the following inside
    a `try` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This will delete all audio files associated with the given profile and also
    reset the enrollment status. To call this function, add a new button to the UI
    and the corresponding `ICommand` property in the ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you compile and run the application, you may get a result similar to the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Enrolling a profile](img/B12373_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Identifying the speaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step is to identify the speaker, which we will do in the `HomeView`
    and corresponding `HomeViewModel`. We do not need to modify the UI much, but we
    do need to add two buttons in order to start and stop the recording. Alternatively,
    if you are not using a microphone, you can get away with one button for browsing
    an audio file. Either way, add the corresponding `ICommand` properties in the
    ViewModel.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to add private members for the `Recording` and `SpeakerIdentification`
    classes. Both should be created in the constructor, where we should inject `ISpeakerIdentificationServiceClient`
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Initialize` function, subscribe to the required events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: For both of the error event handlers, `OnSpeakerRecordingError` and `OnSpeakerIdentificationError`,
    we do not wish to print the error message here. For simplicity, we just output
    it to the debug console window.
  prefs: []
  type: TYPE_NORMAL
- en: The `OnSpeakerRecordingAvailable` event will be triggered when we have recorded
    some audio. This is the event handler that will trigger an attempt to identify
    the person speaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is get a list of speaker profile IDs. We do so
    by calling `ListSpeakerProfiles`, which we looked at earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'With the list of speaker profiles, we call the `IdentifySpeaker` method on
    the `_speakerIdentification` object. We pass on the recorded audio stream and
    the profile list, as an array, as parameters to the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Finish the event handler by adding the corresponding `catch` clause.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the `SpeakerIdentification.cs` file, we add the new function, `IdentifySpeaker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The function should be marked as `async` and accept a `Stream` and an array
    of `Guid` as parameters. To identify a speaker, we make a call to the `IdentifyAsync`
    function on the `_speakerIdentificationClient` object. This requires an audio
    file, in the form of a `Stream`, as well as an array of profile IDs. An optional
    third parameter is a `bool`, which you can use to indicate whether or not you
    want to deviate from the recommended speech length.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the call succeeds, we get an `OperationLocation` object back. This contains
    a URL that we can use to retrieve the status of the current identification process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'If the resulting data contains nothing, we do not want to bother doing anything
    else. If it does contain data, we pass it on as a parameter to the `GetIdentificationOperationStatus`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This function is quite similar to `GetEnrollmentOperationStatus`. We go into
    a `while` loop, which will run until the operation completes. We call `CheckIdentificationStatusAsync`,
    passing on the `location` as a parameter, getting `IdentificationOperation` as
    a result. This will contain data, such as a status, the identified profiles ID,
    and the confidence of a correct result.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the operation is not running, we raise the event with the status message
    and the `ProcessingResult`. If the operation is still running, we update the status
    and wait for 1 second before trying again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Add the corresponding `catch` clause before heading back to the `HomeViewModel`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece in the puzzle is to create `OnSpeakerIdentificationStatusReceived`.
    Add the following code inside HomeViewModel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We need to check to see whether or not we have an identified profile. If we
    do not, we leave the function. If we have an identified profile, we give a response
    to the screen, stating who it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the administrative side of the application, this is a place where it
    would be convenient to have name-to-profile ID mapping. As you can see from the
    following resulting screenshot, recognizing one `GUID` among many is not that
    easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying the speaker](img/B12373_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Verifying a person through speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of verifying if a person is who they claim to be is quite similar
    to the identification process. To show how it is done, we will create a new example
    project, as we do not need this functionality in our smart-house application.
  prefs: []
  type: TYPE_NORMAL
- en: Add the `Microsoft.ProjectOxford.SpeakerRecognition` and `NAudio` NuGet packages
    to the project. We will need the `Recording` class that we used earlier, so copy
    this from the smart-house application's `Model` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `MainView.xaml` file. We need a few elements in the UI for the example
    to work. Add a `Button` element to add speaker profiles. Add two `Listbox` elements.
    One will hold available verification phrases while the other will list our speaker
    profiles.
  prefs: []
  type: TYPE_NORMAL
- en: Add `Button` elements for deleting a profile, starting and stopping enrollment
    recording, resetting enrollment, and starting/stopping verification recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ViewModel, you will need to add two `ObservableCollection` properties:
    one of type `string`, the other of type `Guid`. One will contain the available
    verification phrases, while the other will contain the list of speaker profiles.
    You will also need a property for the selected speaker profile, and we also want
    a string property to show the status.'
  prefs: []
  type: TYPE_NORMAL
- en: The ViewModel will also need seven `ICommand` properties, one for each of our
    buttons.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new class in the `Model` folder and call this `SpeakerVerification`.
    Add two new classes beneath this one, in the same file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is the event arguments that we will pass on when we raise a status
    update event. The `Verification` property will, if set, hold the verification
    result, which we will see presently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The next class is a generic event argument, which is used when we raise an
    error event. In `SpeakerVerification` itself, add the following events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'For our convenience, add helper functions to raise these. Call them `RaiseOnVerificationStatusUpdated`
    and `RaiseOnVerificationError`. Raise the correct event in each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We also need to add a private member called `ISpeakerVerificationServiceClient`.
    This will be in charge of calling the API. We inject this through the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following functions to the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CreateSpeakerProfile`: No parameters, the `async` function, and the return
    type `Task<Guid>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ListSpeakerProfile`: No parameters, the `async` function, and the return type
    `Task<List<Guid>>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DeleteSpeakerProfile`: `Guid` as the required parameter, the `async` function,
    no returned values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResetEnrollments`: `Guid` as the required parameter, the `async` function,
    no returned values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The contents of these functions can be copied from the corresponding functions
    in the smart-house application, as they are exactly the same. The only difference
    is that you need to change the API call from `_speakerIdentificationClient` to
    `_speakerVerificationClient`. Also, raising the events will require the newly
    created event arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need a function to list verification phrases. These are phrases that
    are supported for use with verification. When enrolling a profile, you are required
    to say one of the sentences in this list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function named `GetVerificationPhrase`. Have it return `Task<List<string>>`,
    and mark it as `async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We will make a call to `GetPhrasesAsync`, specifying the language we want the
    phrases to be in. At the time of writing, English is the only possible choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this call is successful, we will get an array of `VerificationPhrases` in
    return. Each element in this array contains a string with the following phrase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We loop through the array and add the phrases to our list, which we will return
    to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have created a profile and we have the list of possible verification
    phrases. Now, we need to do the enrollment. To enroll, the service requires at
    least three enrollments from each speaker. This means that you choose a phrase
    and enroll it at least three times.
  prefs: []
  type: TYPE_NORMAL
- en: When you do the enrollment, it is highly recommended to use the same recording
    device that you will use for verification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new function called `CreateSpeakerEnrollment`. This should require
    a `Stream` and a `Guid`. The first parameter is the audio to use for enrollment.
    The latter is the ID of the profile we are enrolling. The function should be marked
    as `async`, and have no return value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: When we call `EnrollAsync`, we pass on the `audioStream` and `profileId` parameters.
    If the call is successful, we get an `Enrollment` object back. This contains the
    current status of enrollment and specifies the number of enrollments you need
    to add before completing the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the `enrollmentStatus` is null, we exit the function and notify any subscribers.
    If we do have status data, we raise the event to notify it that there is a status
    update, specifying the current status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Add the corresponding `catch` clause to finish up the function.
  prefs: []
  type: TYPE_NORMAL
- en: The last function we need in this class is a function for verification. To verify
    a speaker, you need to send in an audio file. This file must be at least 1 second
    and at most 15 seconds long. You will need to record the same phrase that you
    used for enrollment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `VerifySpeaker` function and make it require a `Stream` and `Guid`.
    The stream is the audio file we will use for verification. The `Guid` is the ID
    of the profile we wish to verify. The function should be `async` and have no return
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We will make a call to `VerifyAsync` from `_speakerVerificationClient`. The
    required parameters are `audioStream` and `speakerProfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful API call will result in a `Verification` object in response. This
    will contain the verification results, as well as the confidence of the results
    being correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: If we do have a verification result, we raise the status update event. Add the
    corresponding `catch` clause to complete the function.
  prefs: []
  type: TYPE_NORMAL
- en: Back in the ViewModel, we need to wire up the commands and event handlers. This
    is done in a similar manner as for speaker identification, and as such we will
    not cover the code in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the code compiling and running, the result may look similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verifying a person through speech](img/B12373_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that we have created a speaker profile. We have also completed
    the enrollment and are ready to verify the speaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verifying the speaker profile may result in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verifying a person through speech](img/B12373_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the verification was accepted with high confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to verify this using a different phrase or let someone else try to
    verify as a particular speaker profile, we may end up with the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verifying a person through speech](img/B12373_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the verification has been rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing speech recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we use speech recognition systems, there are several components that are
    working together. Two of the more important components are acoustic and language
    models. The first one labels short fragments of audio into sound units. The second
    helps the system decide the words, based on the likelihood of a given word appearing
    in certain sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Although Microsoft has done a great job of creating comprehensive acoustic and
    language models, there may still be times when you need to customize these models.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you have an application that is supposed to be used in a factory
    environment. Using speech recognition will require acoustic training of that environment
    so that the recognition can separate it from usual factory noises.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is if your application is used by a specific group of people,
    say, an application for search, where programming is the main topic. You would
    typically use words such as *object-oriented*, *dot net*, or *debugging*. This
    can be recognized by customizing language models.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom acoustic model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create custom acoustic models, you will need audio files and transcripts.
    Each audio file must be stored as a WAV and be between 100 ms and 1 minute in
    length. It is recommended that there is at least 100 ms of silence at the start
    and end of the file. Typically, this will be between 500 ms and 1 second. With
    a lot of background noise, it is recommended to have silences in-between content.
  prefs: []
  type: TYPE_NORMAL
- en: Each file should contain one sentence or utterance. Files should be uniquely
    named, and an entire set of files can be up to 2 GB. This translates to about
    17 to 34 hours of audio, depending on the sampling rate. All files in one set
    should be placed in a zipped folder, which then can be uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: Accompanying the audio files is a single file with the transcript. This should
    name the file and have the sentence next to the name. The filename and sentence
    should be separated by a tab.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the audio files and transcript will make CRIS process it. When this
    process is done, you will get a report stating which sentences have failed or
    succeeded. If anything fails, you will get the reason for the failure.
  prefs: []
  type: TYPE_NORMAL
- en: When the dataset has been uploaded, you can create the acoustic model. This
    will be associated with the dataset you select. When the model has been created,
    you can start the process to train it. Once the training is completed, you can
    deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating custom language models will also require a dataset. This set is a single
    plain text file containing sentences or utterances unique to your model. Each
    new line marks a new utterance. The maximum file size is 2 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the file will make CRIS process it. Once the processing is done, you
    will get a report, which will print any errors, with the reason of failure.
  prefs: []
  type: TYPE_NORMAL
- en: With the processing done, you can create a custom language model. Each model
    will be associated with a given dataset of your selection. Once created, you can
    train the model, and when the training complete, you can deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To deploy and use the custom models, you will need to create a deployment. Here,
    you will name and describe the application. You can select acoustic models and
    language models. Be aware that you can only select one of each per deployed application.
  prefs: []
  type: TYPE_NORMAL
- en: Once created, the deployment will start. This process can take up to 30 minutes
    to complete, so be patient. When the deployment completes, you can get the required
    information by clicking on the application name. You will be given URLs you can
    use, as well as subscription keys to use.
  prefs: []
  type: TYPE_NORMAL
- en: To use the custom models with the Bing Speech API, you can overload `CreateDataClientWithIntent`
    and `CreateMicrophoneClient`. The overloads you will want to use specify both
    the primary and secondary API keys. You need to use the ones supplied by CRIS.
    Additionally, you need to specify the supplied URL as the last parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Once this is done, you are able to use customized recognition models.
  prefs: []
  type: TYPE_NORMAL
- en: Translating speech on the fly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the **Translator Speech** API, you can add automatic end-to-end translation
    for speech. Utilizing this API, one can submit an audio stream of speech and retrieve
    a textual and audio version of translated text. It uses silent detection to detect
    when speech has ended. Results will be streamed back once the pause is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a comprehensive list of supported languages, please visit the following
    site: [https://www.microsoft.com/en-us/translator/business/languages/](https://www.microsoft.com/en-us/translator/business/languages/).'
  prefs: []
  type: TYPE_NORMAL
- en: The result recieved from the API, will contain a stream of audio- and text-based
    results. The results contain the source text in its original language and the
    translation in the target language.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a thorough example on how to use the **Translator Speech** API, please
    visit the following sample at GitHub: [https://github.com/MicrosoftTranslator/SpeechTranslator](https://github.com/MicrosoftTranslator/SpeechTranslator).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have focused on speech. We started by looking at
    how we can convert spoken audio to text and text to spoken audio. Using this,
    we modified our LUIS implementation so that we can say commands and have conversations
    with the smart-house application. From there, we moved on to see how we can identify
    a person speaking using the Speaker Recognition API. Using the same API, we also
    learned how to verify that a person is who they claim to be. We briefly looked
    at the core functionality of the Custom Speech Service. Finally, we briefly covered
    an introduction to the Translator Speech API.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will move back to textual APIs, where we will learn
    how to explore and analyze text in different ways.
  prefs: []
  type: TYPE_NORMAL
