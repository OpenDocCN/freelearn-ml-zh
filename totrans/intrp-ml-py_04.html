<html><head></head><body>
  <div id="_idContainer129" class="Basic-Text-Frame">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-103" class="chapterTitle">Global Model-Agnostic Interpretation Methods</h1>
    <p class="normal">In the first part of this book, we introduced the concepts, challenges, and purpose of machine learning interpretation. This chapter kicks off the second part, which dives into a vast array of methods that are used to diagnose models and understand their underlying data. One of the biggest questions answered by interpretation methods is: <em class="italic">What matters most to the model and how does it matter?</em> Interpretation methods can shed light on the overall importance of features and how they—individually or combined—impact a model’s outcome. This chapter will provide a theoretical and practical foundation to approach these questions.</p>
    <p class="normal">Initially, we will explore the notion of feature importance by examining the model’s inherent parameters. Following that, we will study how to employ <strong class="keyWord">permutation feature importance</strong> in a model-agnostic manner to effectively, reliably, and autonomously rank features. Finally, we will outline how <strong class="keyWord">SHapley Additive exPlanations</strong> (<strong class="keyWord">SHAP</strong>) can rectify some of the shortcomings of permutation feature importance.</p>
    <p class="normal">This chapter will look at several ways to visualize global explanations, such as SHAP’s bar and beeswarm plots, and then dive into feature-specific visualizations like <strong class="keyWord">Partial Dependence Plots</strong> (<strong class="keyWord">PDP</strong>) and <strong class="keyWord">Accumulated Local Effect</strong> (<strong class="keyWord">ALE</strong>) plots. Lastly, feature interactions can enrich explanations because features often team up, so we will discuss 2-dimensional PDP and ALE plots.</p>
    <p class="normal">The following are the main topics we are going to cover in this chapter:</p>
    <ul>
      <li class="bulletList">What is feature importance?</li>
      <li class="bulletList">Gauging feature importance with model-agnostic methods</li>
      <li class="bulletList">Using SHAP, PDP, and ALE plots to visualize:<ul>
          <li class="bulletList">Global explanations</li>
          <li class="bulletList">Feature summary explanations</li>
          <li class="bulletList">Feature interactions</li>
        </ul>
      </li>
    </ul>
    <h1 id="_idParaDest-104" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">catboost</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">shap</code>, <code class="inlineCode">pdpbox</code>, and <code class="inlineCode">pyale</code> libraries. Instructions on how to install all of these libraries are in the GitHub repository <code class="inlineCode">README.md</code> file. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/Ty0Ev"><span class="url">https://packt.link/Ty0Ev</span></a></p>
    </div>
    <h1 id="_idParaDest-105" class="heading-1">The mission</h1>
    <p class="normal">The used car market in the United States is a thriving and substantial industry with significant economic impact. In recent years, approximately 40 million used light vehicles have been sold yearly, representing over two-thirds of the overall yearly sales in the automotive sector. In addition, the market has witnessed consistent growth, driven by the rising cost of new vehicles, longer-lasting cars, and an increasing consumer preference for pre-owned vehicles due to the perception of value for money. As a result, this market segment has become increasingly important for businesses and consumers.</p>
    <p class="normal">Given the market opportunity, a tech startup is currently working on a machine-learning-driven, two-sided marketplace for used car sales. It plans to work much like the e-commerce site eBay, except it’s focused on cars. For example, sellers can list their cars at a fixed price or auction them, and buyers can either pay the higher fixed price or participate in the auction, but how do you come up with a starting point for the price? Typically, sellers define the price, but the site can generate an optimal price that maximizes the overall value for all participants, ensuring that the platform remains attractive and sustainable in the long run.</p>
    <p class="normal">Optimal pricing is not a simple solution since it has to maintain a healthy balance between the number of buyers and sellers on the platform while being perceived as fair by both buyers and sellers. It remains competitive with other platforms while being profitable. However, it all starts with a price prediction model that can estimate the fair value, and then, from there, it can incorporate other models and constraints to adjust it. To that end, one of the startup’s data scientists has obtained a dataset of used car listings from Craigslist and merged it with other sources, such as the U.S. Census Bureau for demographic data and<a id="_idIndexMarker405"/> the <strong class="keyWord">Environmental Protection Agency</strong> (<strong class="keyWord">EPA</strong>) for emissions data. The idea is to train a model with this dataset, but we are unsure what features are helpful. And that’s where you come in!</p>
    <p class="normal">You have been hired to explain which features are useful to the machine learning model and why. This is critical because the startup only wants to ask sellers to provide the bare minimum of details about their car before they get a price estimate. Of course, there are details like the car’s make and model and even the color that another machine learning model can automatically guess from the pictures. Still, some features like the transmission or cylinders may vary in the car model, and the seller may not know or be willing to disclose them. Limiting the questions asked produces the least friction and thus will lead to more sellers completing their listings successfully.</p>
    <h1 id="_idParaDest-106" class="heading-1">The approach</h1>
    <p class="normal">You have decided to take the following steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Train a couple of models.</li>
      <li class="numberedList">Evaluate them.</li>
      <li class="numberedList">Create feature importance values using several methods, both model-specific and model-agnostic.</li>
      <li class="numberedList">Plot global summaries, feature summaries, and feature interaction plots to understand how these features relate to the outcome and each other.</li>
    </ol>
    <p class="normal">The plots will help you communicate findings to the tech startup executives and your data science colleagues.</p>
    <h1 id="_idParaDest-107" class="heading-1">The preparations</h1>
    <p class="normal">You will find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/04/UsedCars.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/04/UsedCars.ipynb</span></a></p>
    <h2 id="_idParaDest-108" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this example, you <a id="_idIndexMarker406"/>need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn) and <code class="inlineCode">catboost</code> to load and configure the model</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">shap</code>, <code class="inlineCode">pdpbox</code>, and <code class="inlineCode">pyale</code> to generate and visualize the model interpretations</li>
    </ul>
    <p class="normal">You should load all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os, random
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics, ensemble, tree, inspection,\
                    model_selection
<span class="hljs-keyword">import</span> catboost <span class="hljs-keyword">as</span> cb
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> shap
<span class="hljs-keyword">from</span> pdpbox <span class="hljs-keyword">import</span> pdp, info_plots
<span class="hljs-keyword">from</span> PyALE <span class="hljs-keyword">import</span> ale
<span class="hljs-keyword">from</span> lime.lime_tabular <span class="hljs-keyword">import</span> LimeTabularExplainer
</code></pre>
    <p class="normal">The following snippet of code will load the <code class="inlineCode">usedcars</code> dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">usedcars_df = mldatasets.load(<span class="hljs-string">"usedcars"</span>, prepare=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">You can inspect it using the <code class="inlineCode">usedcars_df.info()</code> function and verify that there are indeed over 256,000 rows and 29 columns with no nulls. Some of them are <code class="inlineCode">object</code> data types because they are categorical.</p>
    <p class="normal">The data dictionary for the dataset is as follows:</p>
    <ul>
      <li class="bulletList">Variables related to the listing:<ul>
          <li class="bulletList"><code class="inlineCode">price</code>: target, continuous, the price posted for the vehicle</li>
          <li class="bulletList"><code class="inlineCode">region</code>: categorical, the region for the listing—usually this is a city, metropolitan area, or (for more rural areas) a portion of a state or a least populated state (out of 402)</li>
          <li class="bulletList"><code class="inlineCode">posting_date</code>: datetime, the date and time of the posting (all postings are for a single month period in 2021 so you cannot observe seasonal patterns with it)</li>
          <li class="bulletList"><code class="inlineCode">lat</code>: continuous, the latitude in decimal format</li>
          <li class="bulletList"><code class="inlineCode">long</code>: continuous, the longitude in decimal format</li>
          <li class="bulletList"><code class="inlineCode">state</code>: categorical, the two-letter state code (out of 51 including [DC])</li>
          <li class="bulletList"><code class="inlineCode">city</code>: categorical, the name of the city (out of over 6,700)</li>
          <li class="bulletList"><code class="inlineCode">zip</code>: nominal, the zip code (out of over 13,100)</li>
        </ul>
      </li>
      <li class="bulletList">Variables <a id="_idIndexMarker407"/>related to the vehicle:<ul>
          <li class="bulletList"><code class="inlineCode">make</code>: categorical, the brand or manufacturer of the vehicle (out of 37)</li>
          <li class="bulletList"><code class="inlineCode">make_cat</code>: categorical, the category for the make (out of 5). It’s [obsolete] for makes no longer produced, such as “Saturn,” [luxury sports] for brands like “Ferrari,” and [luxury] for brands like “Mercedes-Benz.” Everything else is [regular] or [premium]. The only difference is that [premium] include brands like “Cadillac” and “Acura,” which are the high-end brands of car manufacturers in the [regular] category.</li>
          <li class="bulletList"><code class="inlineCode">make_pop</code>: continuous, the relative popularity of the make in percentiles (0–1)</li>
          <li class="bulletList"><code class="inlineCode">model</code>: categorical, the model (out of over 17,000)</li>
          <li class="bulletList"><code class="inlineCode">model_premier</code>: binary, whether the model is a luxury version/trim of a model (if the model itself is not already high-end such as those in the luxury, luxury sports, or premium categories)</li>
          <li class="bulletList"><code class="inlineCode">year</code>: ordinal, the year of the model (from 1984–2022)</li>
          <li class="bulletList"><code class="inlineCode">make_yr_pop</code>: continuous, the relative popularity of the make for the year it was made for in percentiles (0–1) </li>
          <li class="bulletList"><code class="inlineCode">model_yr_pop</code>: continuous, the relative popularity of the model for the year it was made in percentiles (0–1) </li>
          <li class="bulletList"><code class="inlineCode">odometer</code>: continuous, the reading in the vehicle’s odometer</li>
          <li class="bulletList"><code class="inlineCode">auto_trans</code>: binary, whether the car has automatic transmission—otherwise, it is manual</li>
          <li class="bulletList"><code class="inlineCode">fuel</code>: categorical, the type of fuel used (out of 5: [gas], [diesel], [hybrid], [electric] and [other])</li>
          <li class="bulletList"><code class="inlineCode">model_type</code>: categorical (out of 13: [sedan], [SUV], and [pickup] are the three most popular, by far)</li>
          <li class="bulletList"><code class="inlineCode">drive</code>: categorical, whether it’s four-wheel, front-wheel, or rear-wheel drive (out of 3: [4wd], [fwd] and [rwd])</li>
          <li class="bulletList"><code class="inlineCode">cylinders</code>: nominal, the number of cylinders of the engine (from 2–16). Generally, the more cylinders, the higher the horsepower</li>
          <li class="bulletList"><code class="inlineCode">title_status</code>: categorical, what the title says about the status of the vehicle (out of 7 like [clean], [rebuilt], [unknown], [salvage], and [lien])</li>
          <li class="bulletList"><code class="inlineCode">condition</code>: categorical, what the owner reported the condition of the vehicle to be (out of 7 like [good], [unknown], [excellent] and [like new])</li>
        </ul>
      </li>
      <li class="bulletList">Variables<a id="_idIndexMarker408"/> related to the emissions of the vehicle:<ul>
          <li class="bulletList"><code class="inlineCode">epa_co2</code>: continuous, tailpipe CO2 in grams/mile. For models after 2013, it is based on EPA tests. For previous years, CO2 is estimated using an EPA emission factor (<code class="inlineCode">-1</code> = not vvailable)</li>
          <li class="bulletList"><code class="inlineCode">epa_displ</code>: continuous, the engine displacement (in liters 0.6–8.4)</li>
        </ul>
      </li>
      <li class="bulletList">Variables related to the ZIP code of the listing:<ul>
          <li class="bulletList"><code class="inlineCode">zip_population</code>: continuous, the population</li>
          <li class="bulletList"><code class="inlineCode">zip_density</code>: continuous, the density (residents per sq. mile)</li>
          <li class="bulletList"><code class="inlineCode">est_households_medianincome_usd</code>: continuous, the median household income</li>
        </ul>
      </li>
    </ul>
    <h2 id="_idParaDest-109" class="heading-2">Data preparation</h2>
    <p class="normal">We should<a id="_idIndexMarker409"/> apply categorical encoding to them so that there’s one column per category, but only if the category has at least 500 records. We can do this with the <code class="inlineCode">make_dummies_with_limits</code> utility function. But first, let’s back up the original dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">usedcars_orig_df = usedcars_df.copy()
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'fuel'</span>,
    min_recs=<span class="hljs-number">500</span>,
    defcatname=<span class="hljs-string">'other'</span>
)
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'make_cat'</span>
)
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'model_type'</span>,
    min_recs=<span class="hljs-number">500</span>,
    defcatname=<span class="hljs-string">'other'</span>
)
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'condition'</span>,
    min_recs=<span class="hljs-number">200</span>,
    defcatname=<span class="hljs-string">'other'</span>
)
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'drive'</span>
)
usedcars_df = mldatasets.make_dummies_with_limits(
    usedcars_df,
    <span class="hljs-string">'title_status'</span>,
    min_recs=<span class="hljs-number">500</span>,
    defcatname=<span class="hljs-string">'other'</span>
)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker410"/> above snippet got rid of many <code class="inlineCode">object</code> columns by turning them into a series of binary columns (of the <code class="inlineCode">uint8</code> type). However, there are still a few <code class="inlineCode">object</code> columns left. We can find out which ones like this:</p>
    <pre class="programlisting code"><code class="hljs-code">usedcars_df.dtypes[<span class="hljs-keyword">lambda</span> x: x == <span class="hljs-built_in">object</span>].index.tolist()
</code></pre>
    <p class="normal">We don’t need any of these columns since we have <code class="inlineCode">latitude</code>, <code class="inlineCode">longitude</code>, and some of the demographic features, which provide the model with some idea of where the car was being sold. As for the <code class="inlineCode">make</code> and <code class="inlineCode">model</code>, we have the <code class="inlineCode">make</code> and <code class="inlineCode">model</code> popularity and category features. We can remove the non-numerical features by simply only selecting the numerical ones like this:</p>
    <pre class="programlisting code"><code class="hljs-code">usedcars_df = usedcars_df.select_dtypes(
    include=(<span class="hljs-built_in">int</span>,<span class="hljs-built_in">float</span>,np.uint8)
)
</code></pre>
    <p class="normal">The final<a id="_idIndexMarker411"/> data preparation steps are to:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Define our random seed (<code class="inlineCode">rand</code>) to ensure reproducibility.</li>
      <li class="numberedList">Split data into <code class="inlineCode">X</code> (features) and <code class="inlineCode">y</code> (labels). The former has all columns except for the target variable (<code class="inlineCode">target_col</code>). The latter is only the target.</li>
      <li class="numberedList">Lastly, divide both <code class="inlineCode">X</code> and <code class="inlineCode">y</code> into train and test components randomly using scikit-learn’s <code class="inlineCode">train_test_split</code> function:</li>
    </ol>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">42</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>]=<span class="hljs-built_in">str</span>(rand)
np.random.seed(rand)
random.seed(rand)
target_col = <span class="hljs-string">'price'</span>
X = usedcars_df.drop([target_col], axis=<span class="hljs-number">1</span>)
y = usedcars_df[target_col]
X_train, X_test, y_train, y_test = model_selection.train_test_split(
    X, y, test_size=<span class="hljs-number">0.25</span>, random_state=rand
)
</code></pre>
    <p class="normal">Now we have everything to move forward so we will go ahead with some model training!</p>
    <h1 id="_idParaDest-110" class="heading-1">Model training and evaluation</h1>
    <p class="normal">The <a id="_idIndexMarker412"/>following code snippet will train<a id="_idIndexMarker413"/> two classifiers, <strong class="keyWord">CatBoost</strong> and <strong class="keyWord">Random Forest</strong>:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_mdl = cb.CatBoostRegressor(
    depth=<span class="hljs-number">7</span>, learning_rate=<span class="hljs-number">0.2</span>, random_state=rand, verbose=<span class="hljs-literal">False</span>
)
cb_mdl = cb_mdl.fit(X_train, y_train)
rf_mdl =ensemble.RandomForestRegressor(n_jobs=-<span class="hljs-number">1</span>,random_state=rand)
rf_mdl = rf_mdl.fit(X_train.to_numpy(), y_train.to_numpy())
</code></pre>
    <p class="normal">Next, we can evaluate the <a id="_idIndexMarker414"/>CatBoost model using a <strong class="keyWord">regression plot</strong>, and a <a id="_idIndexMarker415"/>few metrics. Run the following code, which will output <em class="italic">Figure 4.1</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">mdl = cb_mdl
y_train_pred, y_test_pred = mldatasets.evaluate_reg_mdl(
    mdl, X_train, X_test, y_train, y_test
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker416"/>CatBoost model <a id="_idIndexMarker417"/>produced a high R-squared of 0.94 and a test RMSE of nearly 3,100. The regression plot in <em class="italic">Figure 4.1</em> tells us that although there are quite a few cases that have an extremely high error, the vast majority of the 64,000 test samples were predicted fairly well. You can confirm this by running the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">thresh = <span class="hljs-number">4000</span>
pct_under = np.where(
    np.<span class="hljs-built_in">abs</span>(y_test - y_test_pred) &lt; thresh, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>
).<span class="hljs-built_in">sum</span>() / <span class="hljs-built_in">len</span>(y_test)
<span class="hljs-built_in">print</span>(f"Percentage of test samples under ${thresh:,<span class="hljs-number">.0</span>f} <span class="hljs-keyword">in</span> absolute
      error {pct_under:<span class="hljs-number">.1</span>%}")
</code></pre>
    <p class="normal">It says that the percentage of test samples with an absolute error in the $4,000 range is nearly 90%. Granted it’s a large margin of error for cars that cost a few thousand, but we just want to get a sense of how accurate the model is. We will likely need to improve it for production, but for now, it will do for the exercise requested by the tech startup:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_01.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.1: CatBoost model predictive performance</p>
    <p class="normal">So now let’s <a id="_idIndexMarker418"/>repeat the same <a id="_idIndexMarker419"/>evaluation for the Random Forest model (<code class="inlineCode">rf_mdl</code>). To do this, we must run the same snippet but simply replace <code class="inlineCode">cb_mdl</code> with <code class="inlineCode">rf_mdl</code> in the first line.</p>
    <p class="normal">The Random Forest performs just as well for the test samples, and its metrics are remarkably similar to CatBoost, except it overfits much more in this case. This matters because we will now compare some feature importance methods on both and don’t want differences in predictive performance to be a reason to doubt any findings.</p>
    <h1 id="_idParaDest-111" class="heading-1">What is feature importance?</h1>
    <p class="normal">Feature importance <a id="_idIndexMarker420"/>refers to the extent to which each feature contributes to the final output of a model. For linear models, it’s easier to determine the <a id="_idIndexMarker421"/>importance since coefficients clearly indicate the contributions of each feature. However, this isn’t always the case for non-linear models.</p>
    <p class="normal">To simplify the concept, let’s compare model classes to various team sports. In some sports, it’s easy to identify the players who have the greatest impact on the outcome, while in others, it isn’t. Let’s consider two sports as examples:</p>
    <ul>
      <li class="bulletList"><em class="italic">Relay race</em>: In this sport, each runner covers equal distances, and the race’s outcome largely depends on the speed at which they complete their part. Thus, it’s easy to separate and quantify each racer’s contributions. A relay race is similar to a linear model since the race’s outcome is a linear combination of independent components.</li>
      <li class="bulletList"><em class="italic">Basketball</em>: In this game, players have distinct roles, and comparing them using the same metrics is not possible. Moreover, the varying game conditions and player interactions can significantly affect their contributions to the outcome. So, how can we measure and rank each player’s contributions?</li>
    </ul>
    <p class="normal">Models possess inherent parameters that can occasionally aid in unraveling the contributions of each feature. We’ve trained two models. How have their intrinsic parameters been used to calculate feature importance?</p>
    <p class="normal">Let’s begin with Random Forest. If you plot one of its estimators with the following code, it will generate <em class="italic">Figure 4.2</em>. Because each estimator is up to six levels deep, we will plot only up to the second level (<code class="inlineCode">max_depth=2</code>) because, otherwise, the text would be too small. But feel free to increase the <code class="inlineCode">max_depth</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">tree.plot_tree(rf_mdl.estimators_[<span class="hljs-number">0</span>], filled=<span class="hljs-literal">True</span>, max_depth=<span class="hljs-number">2</span>,\
              feature_names=X_train.columns)
</code></pre>
    <p class="normal">Notice the <code class="inlineCode">squared_error</code> and <code class="inlineCode">samples</code> in each node of the estimator in <em class="italic">Figure 4.2</em>. By dividing these numbers, you can calculate<a id="_idIndexMarker422"/> the <strong class="keyWord">Mean Squared Error</strong> (<strong class="keyWord">MSE</strong>). Although for classifiers, it’s<a id="_idIndexMarker423"/> the <strong class="keyWord">Gini coefficient</strong>, for regressors, MSE is the impurity measure. It’s expected to decrease as you go deeper in the tree, so the sum of these weighted impurity decreases is calculated for each feature throughout the tree. How much a feature decreases node impurity indicates how much it contributes to the model’s outcome. This would be a <strong class="keyWord">model-specific</strong> method since it can’t be used with linear models<a id="_idIndexMarker424"/> or neural networks, but this is precisely how tree-based models compute feature importance. Random Forest is no different. However, it’s an <a id="_idIndexMarker425"/>ensemble, so it has a collection of estimators, so the feature importance is the mean decrease in impurity across all estimators.</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_02.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.2: First level for the first estimator of the Random Forest model</p>
    <p class="normal">We can obtain and print the feature importance values for the Random Forest model with the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">rf_feat_imp = rf_mdl.feature_importances_
<span class="hljs-built_in">print</span>(rf_feat_imp)
</code></pre>
    <p class="normal">Note that since they have been normalized, they add up to 1 (<code class="inlineCode">sum(rf_feat_imp)</code>).</p>
    <p class="normal">CatBoost uses a different method, by default, to compute feature importance, called <code class="inlineCode">PredictionValuesChange</code>. It shows how much, on average, the model outcome changes if the feature value changes. It traverses the tree performing a weighted average of feature contributions according to the number of nodes in each branch (left or right). If it encounters a combination of features in a node, it evenly assigns contributions to each one. As a result, it can yield misleading feature importance values for features that usually interact with one another.</p>
    <p class="normal">You can also<a id="_idIndexMarker426"/> retrieve the CatBoost feature importances with <code class="inlineCode">feature_importances_</code> like this, and unlike Random Forest, they add up to 100 and not 1:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_feat_imp = cb_mdl.feature_importances_
<span class="hljs-built_in">print</span>(cb_feat_imp)
</code></pre>
    <p class="normal">Now, let’s <a id="_idIndexMarker427"/>put them side by side in a <code class="inlineCode">pandas</code> <code class="inlineCode">DataFrame</code>. We will multiply the Random Forest feature importances (<code class="inlineCode">rf_feat_imp</code>) times 100 to be on the same scale as CatBoost’s. Then we will sort by <code class="inlineCode">cb_feat_imp</code> and format so that there’s a nice bar chart embedded in our DataFrame. The result of the following snippet is in <em class="italic">Figure 4.3</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">feat_imp_df = pd.DataFrame(
    {
        <span class="hljs-string">'feature'</span>:X_train.columns,
        <span class="hljs-string">'cb_feat_imp'</span>:cb_feat_imp,
        <span class="hljs-string">'rf_feat_imp'</span>:rf_feat_imp*<span class="hljs-number">100</span>
    }
)
feat_imp_df = feat_imp_df.sort_values(<span class="hljs-string">'cb_feat_imp'</span>, ascending=<span class="hljs-literal">False</span>)
feat_imp_df.style.<span class="hljs-built_in">format</span>(
    <span class="hljs-string">'{:.2f}%'</span>, subset=[<span class="hljs-string">'</span><span class="hljs-string">cb_feat_imp'</span>, <span class="hljs-string">'rf_feat_imp'</span>])
    .bar(subset=[<span class="hljs-string">'cb_feat_imp'</span>, <span class="hljs-string">'rf_feat_imp'</span>], color=<span class="hljs-string">'#4EF'</span>, width=<span class="hljs-number">60</span>
)
</code></pre>
    <p class="normal">Notice that both models have the same most important feature in <em class="italic">Figure 4.3</em>. The top ten features are mostly the same but not in the same rank. In particular, <code class="inlineCode">odometer</code> appears to be much more important for CatBoost than for Random Forest. Also, all the other features don’t match up in ranking except for the least important ones, for which there’s a consensus that they are indeed last:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_03.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.3: Compare both models’ feature importance values</p>
    <p class="normal">How can we address these differences and employ a technique that consistently represents a feature’s importance? We will explore this using model-agnostic approaches.</p>
    <h1 id="_idParaDest-112" class="heading-1">Assessing feature importance with model-agnostic methods</h1>
    <p class="normal"><strong class="keyWord">Model-agnostic</strong> methods<a id="_idIndexMarker428"/> imply <a id="_idIndexMarker429"/>that we will not depend on intrinsic model parameters to compute feature importance. Instead, we will consider the model as a black box, with only the inputs and output visible. So, how can we determine which inputs made a difference?</p>
    <p class="normal">What if we altered the inputs randomly? Indeed, one of the most effective methods for evaluating feature importance is through simulations designed to measure a feature’s impact or lack thereof. In other words, let’s remove a random player from the game and observe the outcome! In this section, we will discuss two ways to achieve this: permutation feature importance and SHAP.</p>
    <h2 id="_idParaDest-113" class="heading-2">Permutation feature importance</h2>
    <p class="normal">Once we have<a id="_idIndexMarker430"/> a trained model, we cannot <a id="_idIndexMarker431"/>remove a feature to assess the impact of not using it. However, we can:</p>
    <ul>
      <li class="bulletList">Replace the feature with a static value, such as the mean or median, rendering it devoid of useful information.</li>
      <li class="bulletList">Shuffle (permute) the feature values to disrupt the relationship between the feature and the outcome.</li>
    </ul>
    <p class="normal">Permutation feature importance (<code class="inlineCode">permutation_importance</code>) uses the second strategy with a test dataset. Then it measures a change in the score (MSE, r2, f1, accuracy, etc.). In this case, a substantial decrease in the<a id="_idIndexMarker432"/> negative <strong class="keyWord">Mean Absolute Error</strong> (<strong class="keyWord">MAE</strong>) when the feature is shuffled would suggest that the feature has a high influence on the prediction. It would have to repeat the shuffling several times (<code class="inlineCode">n_repeats</code>) to arrive at conclusive results by averaging out the decreases in accuracy. Please note the default scorer for Random Forest regressors is R-squared, while it’s RMSE for CatBoost, so we are making sure they both use the same scorer by setting the <code class="inlineCode">scoring</code> parameter. The following code does all of this for both models:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_perm_imp = inspection.permutation_importance(
    cb_mdl, X_test, y_test, n_repeats=<span class="hljs-number">10</span>, random_state=rand,\
    scoring=<span class="hljs-string">'neg_mean_absolute_error'</span>
)
rf_perm_imp = inspection.permutation_importance(
    rf_mdl, X_test.to_numpy(), y_test.to_numpy(), n_repeats=<span class="hljs-number">10</span>,\
    random_state=rand, scoring=<span class="hljs-string">'neg_mean_absolute_error'</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker433"/>method outputs a mean score (<code class="inlineCode">importances_mean</code>) and a standard deviation (<code class="inlineCode">importances_std</code>) across all repeats for <a id="_idIndexMarker434"/>each model, which we can place in a <code class="inlineCode">pandas</code> DataFrame, sort, and format, as we did before with feature importance. The following code generates the table in <em class="italic">Figure 4.4</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">perm_imp_df = pd.DataFrame(
    {
        <span class="hljs-string">'feature'</span>:X_train.columns,
        <span class="hljs-string">'</span><span class="hljs-string">cb_perm_mean'</span>:cb_perm_imp.importances_mean,
        <span class="hljs-string">'cb_perm_std'</span>:cb_perm_imp.importances_std,
        <span class="hljs-string">'rf_perm_mean'</span>:rf_perm_imp.importances_mean,
        <span class="hljs-string">'rf_perm_std'</span>:rf_perm_imp.importances_std
    }
)
perm_imp_df = perm_imp_df.sort_values(
    <span class="hljs-string">'cb_perm_mean'</span>, ascending=<span class="hljs-literal">False</span>
)
perm_imp_df.style.<span class="hljs-built_in">format</span>(
    <span class="hljs-string">'{:.4f}'</span>, subset=[<span class="hljs-string">'cb_perm_mean'</span>, <span class="hljs-string">'cb_perm_std'</span>, <span class="hljs-string">'</span><span class="hljs-string">rf_perm_mean'</span>,
    <span class="hljs-string">'rf_perm_std'</span>]).bar(subset=[<span class="hljs-string">'cb_perm_mean'</span>, <span class="hljs-string">'rf_perm_mean'</span>],\
    color=<span class="hljs-string">'#4EF'</span>, width=<span class="hljs-number">60</span>
)
</code></pre>
    <p class="normal">The first four features for both models in <em class="italic">Figure 4.4</em> are much more consistent than <em class="italic">Figure 4.3</em>—not that they have to be because they are different models! However, it makes sense considering they were derived from the same method. There are considerable differences too. Random Forest seems to rely much more heavily on fewer features, but these features might not be as necessary if they achieve a very similar predictive performance as CatBoost:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_04.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.4: Compare both models’ permutation feature importance values</p>
    <p class="normal">Permutation feature importance can be understood as the average increase in model error when a feature is made irrelevant, along with its interactions with other features. It is relatively fast to compute since models don’t need to be retrained, but its specific value <a id="_idIndexMarker435"/>should be taken with a grain of salt <a id="_idIndexMarker436"/>because there are some drawbacks to this shuffling technique:</p>
    <ul>
      <li class="bulletList">Shuffling a highly correlated feature with another unshuffled feature may not significantly affect predictive performance, as the unshuffled feature retains some information from the shuffled one. This means it cannot accurately assess the importance of multicollinear features.</li>
      <li class="bulletList">Shuffling can lead to unrealistic observations, like predicting vehicle traffic with weather and ending up with winter temperatures during the summer. This will result in a higher predictive error for a model that has never encountered such examples, overstating the importance scores beyond their actual significance.</li>
    </ul>
    <p class="normal">Therefore, these importance values are only useful to rank the features and gauge their relative importance against other features in a particular model. We will now explore how Shapley values can help address these issues.</p>
    <h2 id="_idParaDest-114" class="heading-2">SHAP values</h2>
    <p class="normal">Before delving into SHAP values, we should <a id="_idIndexMarker437"/>discuss <strong class="keyWord">Shapley values</strong>. SHAP is an implementation of Shapley values that takes some liberties but maintains many of the same properties.</p>
    <p class="normal">It’s appropriate that we’ve been discussing feature importance within the context of games, as Shapley values are <a id="_idIndexMarker438"/>rooted in <strong class="keyWord">cooperative game theory</strong>. In this context, players form different sets <a id="_idIndexMarker439"/>called <strong class="keyWord">coalitions</strong>, and when they play, they get varying scores known<a id="_idIndexMarker440"/> as <strong class="keyWord">marginal contributions</strong>. The Shapley value is the average of these contributions across multiple simulations. In terms of feature importance, players represent features, sets of players symbolize sets of features, and marginal contribution is related to a decrease in predictive error.</p>
    <p class="normal">This approach may seem similar to permutation feature importance, but its focus on feature combinations rather than individual features helps tackle the multicollinear issue. Moreover, the values obtained through this method satisfy several favorable mathematical <a id="_idIndexMarker441"/>properties, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Additivity</strong>: the sum of the parts adds to the total value</li>
      <li class="bulletList"><strong class="keyWord">Symmetry</strong>: consistent values for equal contributions</li>
      <li class="bulletList"><strong class="keyWord">Efficiency</strong>: equal to the difference between the prediction and expected value</li>
      <li class="bulletList"><strong class="keyWord">Dummy</strong>: a value of zero for features with no impact on the outcome</li>
    </ul>
    <p class="normal">In practice, this method demands substantial computational resources. For instance, five features yield <img src="../Images/B18406_04_001.png" alt="" role="presentation"/> possible coalitions, while 15 features result in 32,768. Therefore, most Shapley implementations use shortcuts like Monte Carlo sampling or leveraging the model’s intrinsic parameters (which makes them model-specific). The SHAP library employs various strategies to reduce computational load without sacrificing Shapley properties too much.</p>
    <h3 id="_idParaDest-115" class="heading-3">Comprehensive explanations with KernelExplainer</h3>
    <p class="normal">Within SHAP, the <a id="_idIndexMarker442"/>most prevalent <a id="_idIndexMarker443"/>model-agnostic approach is <code class="inlineCode">KernelExplainer</code>, which is based on <strong class="keyWord">Local Interpretable Model-agnostic Explanations</strong> (<strong class="keyWord">LIME</strong>). Don’t fret if you don’t understand the specifics since we will cover it in detail in <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>. To cut down on computation, it employs sample coalitions. In addition, it follows the same procedures as LIME, such as fitting <a id="_idIndexMarker444"/>weighted linear models, but employs Shapley sample coalitions and a different kernel that returns SHAP values as coefficients.</p>
    <p class="normal"><code class="inlineCode">KernelExplainer</code> can be initialized with a <code class="inlineCode">kmeans</code> background sample of the training dataset (<code class="inlineCode">X_train_summary</code>), which helps it define the kernels. It can be still slow. Therefore, it’s better not to use large datasets to compute the <code class="inlineCode">shap_values</code>. Instead, in the following code, we are using only 2% of the test dataset (<code class="inlineCode">X_test_sample</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">rf_fn = lambda x: rf_mdl.predict(x)
X_train_summary = shap.kmeans(X_train.to_numpy(), <span class="hljs-number">50</span>)
X_test_sample = X_test.sample(frac=<span class="hljs-number">0.02</span>)
rf_kernel_explainer = shap.KernelExplainer(rf_fn, X_train_summary)
rf_shap_values = rf_kernel_explainer.shap_values(
    X_test_sample.to_numpy()
)
</code></pre>
    <p class="normal">It might take a while to run the whole thing. Should it take too long, feel free to reduce the sample size from <code class="inlineCode">0.02</code> to <code class="inlineCode">0.005</code>. SHAP values will be less reliable but it’s just an example so you can get a taste of <code class="inlineCode">KernelExplainer</code>.</p>
    <p class="normal">Once it <a id="_idIndexMarker445"/>completes, please<a id="_idIndexMarker446"/> run <code class="inlineCode">print(rf_shap_values.shape)</code> to get an idea of the dimensions we’ll be dealing with. Note that it’s two dimensions! There’s one SHAP value per observation x feature. For this reason, SHAP values can be used for both global and local interpretation. Put a pin in that! We will cover the local interpretation in the next chapter. For now, we will look at another SHAP explainer.</p>
    <h3 id="_idParaDest-116" class="heading-3">Faster explanations with TreeExplainer</h3>
    <p class="normal"><code class="inlineCode">TreeExplainer</code> was <a id="_idIndexMarker447"/>designed to efficiently estimate SHAP values for tree-based <a id="_idIndexMarker448"/>models such as XGBoost, Random Forest, and CART decision trees. It can allocate non-zero values to non-influential features because it employs the conditional expectation value function rather than marginal expectation, violating the Shapley dummy property. This can have consequences when features are collinear, potentially making explanations less reliable. However, it adheres to other properties.</p>
    <p class="normal">You can obtain the SHAP values with <code class="inlineCode">TreeExplainer</code> like this:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_tree_explainer = shap.TreeExplainer(cb_mdl)
cb_shap_values = cb_tree_explainer.shap_values(X_test)
</code></pre>
    <p class="normal">As you can tell, it’s easier and much quicker. It also outputs a two-dimensional array like <code class="inlineCode">KernelExplainer</code>. You can check with <code class="inlineCode">print(cb_shap_values.shape)</code>.</p>
    <p class="normal">For feature importance values, we can collapse two dimensions into one. All we need to do is average the absolute value per feature like this:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_shap_imp = np.mean(np.<span class="hljs-built_in">abs</span>(cb_shap_values),<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">For Random Forest, just replace the <code class="inlineCode">cb_</code> for <code class="inlineCode">rf_</code> with the same code.</p>
    <p class="normal">We can now compare both SHAP feature importances side-by-side using a formatted and sorted <code class="inlineCode">pandas</code> DataFrame. The following code will generate the table in <em class="italic">Figure 4.5</em>.</p>
    <pre class="programlisting code"><code class="hljs-code">shap_imp_df = pd.DataFrame(
    {
        <span class="hljs-string">'feature'</span>:X_train.columns,
        <span class="hljs-string">'cb_shap_imp'</span>:cb_shap_imp,
        <span class="hljs-string">'rf_shap_imp'</span>:rf_shap_imp
    }
)
shap_imp_df = shap_imp_df.sort_values(<span class="hljs-string">'cb_shap_imp'</span>, ascending=<span class="hljs-literal">False</span>)
shap_imp_df.style.<span class="hljs-built_in">format</span>(
    <span class="hljs-string">'{:.4f}'</span>, subset=[<span class="hljs-string">'cb_shap_imp'</span>, <span class="hljs-string">'rf_shap_imp'</span>]).bar(
    subset=[<span class="hljs-string">'cb_shap_imp'</span>, <span class="hljs-string">'rf_shap_imp'</span>], color=<span class="hljs-string">'#4EF'</span>, width=<span class="hljs-number">60</span>
)
</code></pre>
    <p class="normal"><em class="italic">Figure 4.5</em> not<a id="_idIndexMarker449"/> only compares feature importance for two different models but also for two different SHAP explainers. They aren’t necessarily perfect depictions, but they are both to be trusted more than permutation feature importance:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_05.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.5: Compare both models’ SHAP feature importance values</p>
    <p class="normal">SHAP for both<a id="_idIndexMarker450"/> models suggest that <code class="inlineCode">loan_to_value_ratio</code> and <code class="inlineCode">make_cat_va</code> importances were previously deflated. This makes sense because <code class="inlineCode">loan_to_value_ratio</code> is highly correlated with several of the top features and <code class="inlineCode">make_cat_va</code> with all the other product-type features.</p>
    <h1 id="_idParaDest-117" class="heading-1">Visualize global explanations</h1>
    <p class="normal">Previously, we <a id="_idIndexMarker451"/>covered the concept of global explanations and SHAP values. But we didn’t demonstrate the many ways we can visualize them. As you will learn, SHAP values are very versatile and can be used to examine much more than feature importance!</p>
    <p class="normal">But first, we must initialize a SHAP explainer. In the previous chapter, we generated the SHAP values using <code class="inlineCode">shap.TreeExplainer</code> and <code class="inlineCode">shap.KernelExplainer</code>. This time, we will use SHAP’s newer interface, which simplifies the process by saving SHAP values and corresponding data in a single object and much more! Instead of explicitly defining the type of explainer, you initialize it with <code class="inlineCode">shap.Explainer(model)</code>, which returns the callable object. Then, you load your test dataset (<code class="inlineCode">X_test</code>) into the callable <code class="inlineCode">Explainer</code>, and it returns an <code class="inlineCode">Explanation</code> object:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_explainer = shap.Explainer(cb_mdl)
cb_shap = cb_explainer(X_test)
</code></pre>
    <p class="normal">In case you are wondering, how did it know what kind of explainer to create? Glad you asked! There’s an optional parameter called <code class="inlineCode">algorithm</code> in the initialization function, and you can explicitly define <code class="inlineCode">tree</code>, <code class="inlineCode">linear</code>, <code class="inlineCode">additive</code>, <code class="inlineCode">kernel</code>, and others. But by default, it is set to <code class="inlineCode">auto</code>, which means it will guess which kind of explainer is needed for the model. In this case, CatBoost is a tree ensemble, so <code class="inlineCode">tree</code> is what makes sense. We can easily check that SHAP chose the right explainer with <code class="inlineCode">cb_explainer.[dict]{custom-style="P - Italics"}[ or ]print(type(cb_explainer))</code>. It will return <code class="inlineCode">&lt;class 'shap.explainers._tree.Tree'&gt;</code>, which is correct! As for the explanation stored in <code class="inlineCode">cb_shap</code>, what is it exactly? It’s an object that contains pretty much everything that is needed to plot explanations, such as the SHAP values (<code class="inlineCode">cb_shap.values</code>) and corresponding dataset (<code class="inlineCode">cb_shap.data</code>). They should be exactly the same dimensions because there’s one SHAP value for every data point. It’s easy to verify this by checking their <a id="_idIndexMarker452"/>dimensions with the <code class="inlineCode">shape</code> property:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Values dimensions: %s"</span> % (cb_shap.values.shape,))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Data dimensions: %s"</span> % (cb_shap.data.shape,))
</code></pre>
    <p class="normal">Now, let’s put these values to some use!</p>
    <h2 id="_idParaDest-118" class="heading-2">SHAP bar plot</h2>
    <p class="normal">Let’s start <a id="_idIndexMarker453"/>with the most straightforward<a id="_idIndexMarker454"/> global explanation visualizations we can do, which is feature importance. You can do this with a bar chart (<code class="inlineCode">shap.plots.bar</code>). All it needs is the explanation object (<code class="inlineCode">cb_shap</code>), but by default, it will only display 10 bars. Fortunately, we can change this with <code class="inlineCode">max_display</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.bar(cb_shap, max_display=<span class="hljs-number">15</span>)
</code></pre>
    <p class="normal">The preceding snippet will generate <em class="italic">Figure 4.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_06.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.6: CatBoost model’s SHAP feature importance</p>
    <p class="normal"><em class="italic">Figure 4.6</em> should <a id="_idIndexMarker455"/>look very familiar if you read the previous chapter. In fact, it should match the <code class="inlineCode">cb_shap_imp</code> column in <em class="italic">Figure 4.5</em>.</p>
    <p class="normal">SHAP feature<a id="_idIndexMarker456"/> importance offers considerable flexibility since it is simply the average of the absolute value of SHAP values for each feature. With the granularity of SHAP values, you can dissect them in the same way as the test dataset to obtain insights across various dimensions. This reveals more about feature importance than a single average for each feature.</p>
    <p class="normal">For example, you can compare feature importance across different groups. Suppose we want to explore how feature importance differs between <code class="inlineCode">year</code> cohorts. First, we need a threshold to define. Let’s use 2014 because it’s the median <code class="inlineCode">year</code> in our dataset. Values above that can<a id="_idIndexMarker457"/> be set in the “Newer Car” cohort and pre-2014 values set to “Older Car.” Then, use <code class="inlineCode">np.where</code> to create an array<a id="_idIndexMarker458"/> assigning the cohorts to each observation. To create the bar chart, repeat the previous process but use the cohorts function to split the explanation, applying the absolute value (<code class="inlineCode">abs</code>) and <code class="inlineCode">mean</code> operations to each cohort.</p>
    <pre class="programlisting code"><code class="hljs-code">yr_cohort = np.where(
    X_test.year &gt; <span class="hljs-number">2014</span>, <span class="hljs-string">"Newer Car"</span>, <span class="hljs-string">"Older Car"</span>
)
shap.plots.bar(cb_shap.cohorts(yr_cohort).<span class="hljs-built_in">abs</span>.mean(<span class="hljs-number">0</span>))
</code></pre>
    <p class="normal">This code snippet produces <em class="italic">Figure 4.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_07.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.7: CatBoost model’s SHAP feature importance split by property value cohorts</p>
    <p class="normal">As you <a id="_idIndexMarker459"/>can see in <em class="italic">Figure 4.7</em>, all the top features <a id="_idIndexMarker460"/>matter less for “Older cars.” One of the biggest differences is with <code class="inlineCode">year</code> itself. When a car is older, how much older doesn’t matter as much as when it’s in the “Newer Car” cohort.</p>
    <h2 id="_idParaDest-119" class="heading-2">SHAP beeswarm plot</h2>
    <p class="normal">Bar charts can <a id="_idIndexMarker461"/>obscure some aspects of <a id="_idIndexMarker462"/>how features affect model outcomes. Not only do different feature values have varying impacts, but their distribution across all observations can also exhibit considerable variation. The beeswarm plot aims to provide more insight by using dots to represent each observation for every individual feature, even though features are ordered<a id="_idIndexMarker463"/> by global feature importance:</p>
    <ul>
      <li class="bulletList">Dots are color-coded based on their position in the range of low to high values for each feature.</li>
      <li class="bulletList">Dots are arranged horizontally according to their impact on the outcome, centered on the line where SHAP value = 0, with negative impacts on the left and positive impacts on the right.</li>
      <li class="bulletList">Dots accumulate vertically, creating a histogram-like visualization to display the number of observations influencing the outcome at every impact level for each feature.</li>
    </ul>
    <p class="normal">To better <a id="_idIndexMarker464"/>understand this, we’ll create a beeswarm plot. Generating the plot is easy with the <code class="inlineCode">shap.plots.beeswarm</code> function. It only requires the explanation object (<code class="inlineCode">cb_shap</code>), and, as with the bar plot, we’ll override the default <code class="inlineCode">max_display</code> to show only 15 features:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.beeswarm(cb_shap, max_display=<span class="hljs-number">15</span>)
</code></pre>
    <p class="normal">The result of the preceding code is in <em class="italic">Figure 4.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_08.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.8: The CatBoost model’s SHAP beeswarm plot</p>
    <p class="normal"><em class="italic">Figure 4.8</em> can<a id="_idIndexMarker465"/> be<a id="_idIndexMarker466"/> read as follows:</p>
    <ul>
      <li class="bulletList">The most important feature (of the 15) from top to bottom is <code class="inlineCode">year</code> and the least is longitude (<code class="inlineCode">long</code>). It should match the same order in the bar chart or if you take the average absolute values of the SHAP values across every feature.</li>
      <li class="bulletList">Low values for <code class="inlineCode">year</code> negatively impact the model outcome, while high values impact it positively. There’s a nice clean gradient in between, suggesting an increasing monotonic relationship between <code class="inlineCode">year</code> and the predicted <code class="inlineCode">price</code>. That is, the higher the <code class="inlineCode">year</code>, the higher the <code class="inlineCode">price</code> according to the CatBoost model.</li>
      <li class="bulletList"><code class="inlineCode">odometer</code> has a negative or negligible impact on a sizable portion of the observations. However, it has a long tail of observations for which it has a sizable impact. You can tell this by looking at the density depicted vertically.</li>
      <li class="bulletList">If you<a id="_idIndexMarker467"/> scan the rest of the plot looking for other continuous features, you won’t find such a clean gradient anywhere else as you did for <code class="inlineCode">year</code> and <code class="inlineCode">odometer</code> but you’ll find some trends like higher values of <code class="inlineCode">make_yr_pop</code> and <code class="inlineCode">model_yr_pop</code> having mostly a negative impact.</li>
      <li class="bulletList">For binary features, it is easy to tell because you only have two colors, and they are sometimes neatly split, as with <code class="inlineCode">model_premier</code>, <code class="inlineCode">model_type_pickup</code>, <code class="inlineCode">drive_fwd</code>, <code class="inlineCode">make_cat_regular</code>, and <code class="inlineCode">fuel_diesel</code>, demonstrating how a certain kind of vehicle might be a tell-tale sign for the model whether it is high priced or not. In this case, a pick-up model increases the price, whereas a vehicle with a regular make—that is, a brand that is not luxury—decreases the price.</li>
    </ul>
    <p class="normal">While the<a id="_idIndexMarker468"/> beeswarm plot offers an excellent summary of many findings, it can sometimes be challenging to interpret, and it doesn’t capture everything. The color-coding is useful for illustrating the relationship between feature values and model output, but what if you want more detail? That’s where the partial dependence plot comes in. It’s among the many feature summary explanations that provide a global interpretation method specific to features.</p>
    <h1 id="_idParaDest-120" class="heading-1">Feature summary explanations</h1>
    <p class="normal">This section will <a id="_idIndexMarker469"/>cover a number of methods used to visualize how an individual feature impacts the outcome.</p>
    <h2 id="_idParaDest-121" class="heading-2">Partial dependence plots </h2>
    <p class="normal"><strong class="keyWord">Partial Dependence Plots</strong> (<strong class="keyWord">PDPs</strong>) display<a id="_idIndexMarker470"/> a feature’s relationship with the outcome according to the model. In essence, the PDP illustrates the marginal effect of a feature on the model’s predicted output across all possible values of that feature.</p>
    <p class="normal">The calculation involves two steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Initially, conduct a simulation where the feature value for each observation is altered to a range of different values, and predict the model using those values. For example, if the <code class="inlineCode">year</code> varies between 1984 and 2022, create copies of each observation with <code class="inlineCode">year</code> values ranging between these two numbers. Then, run the model using these values. This first step can be plotted as <a id="_idIndexMarker471"/>the <strong class="keyWord">Individual Conditional Expectation</strong> (<strong class="keyWord">ICE</strong>) plot, with simulated values for <code class="inlineCode">year</code> on the X-axis and the model output on the Y-axis, and one line per simulated observation.</li>
      <li class="numberedList">In the second step, simply average the ICE lines to obtain a general trend line. This line represents the PDP!</li>
    </ol>
    <p class="normal">PDPs can<a id="_idIndexMarker472"/> be created using scikit-learn but these only work well with scikit-learn models. They also can be generated with the SHAP library, as well as another library called PDPBox. Each has its advantages and disadvantages, which we will cover in this section.</p>
    <p class="normal">SHAP’s <code class="inlineCode">partial_dependence</code> plot function takes the name of a feature (<code class="inlineCode">year</code>), a <code class="inlineCode">predict</code> function (<code class="inlineCode">cb_mdl.predict</code>), and a dataset (<code class="inlineCode">X_test</code>). There are some optional parameters such as whether to show the ICE lines (<code class="inlineCode">ice</code>), a horizontal model expected value line (<code class="inlineCode">model_expected_value</code>), and a vertical feature expected value line (<code class="inlineCode">feature_expected_value</code>). It shows the <code class="inlineCode">ice</code> lines by default, but there are so many observations in the test dataset it would take a long time to generate the plot, and would be “too busy” to appreciate the trends. The SHAP PDP can also incorporate SHAP values (<code class="inlineCode">shap_values=True</code>) but it would take a very long time considering the size of the dataset. It’s best to sample your dataset to make it more plot-friendly:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.partial_dependence(
    <span class="hljs-string">"year"</span>, cb_mdl.predict, X_test, ice=<span class="hljs-literal">False</span>, model_expected_value=<span class="hljs-literal">True</span>,\
    feature_expected_value=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The preceding code will produce the plot in <em class="italic">Figure 4.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_09.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.9: SHAP’s PDP for year</p>
    <p class="normal">As you <a id="_idIndexMarker473"/>can appreciate in <em class="italic">Figure 4.9</em>, there’s an upward trend for <code class="inlineCode">year</code>. This finding shouldn’t be surprising considering the neat gradient in <em class="italic">Figure 4.8</em> for <em class="italic">year</em>. With the histogram, you can tell the bulk of observations have values for <code class="inlineCode">year</code> over 2011, which is where it starts to have an above-verage impact on the model. This makes sense once you compare the location of the histogram to the location of the bulge in the beeswarm plot (<em class="italic">Figure 4.8</em>).</p>
    <p class="normal">With PDPBox, we will make several variations of PDP plots. This library separates the potentially time-consuming process of making the simulations with the <code class="inlineCode">PDPIsolate</code> function from the plotting with the <code class="inlineCode">plot</code> function. We will only have to run <code class="inlineCode">PDPIsolate</code> once but <code class="inlineCode">plot</code> three times.</p>
    <p class="normal">For the first plot, we use <code class="inlineCode">plot_pts_dist=True</code> to display a rug. The rug is a more concise way of conveying the distribution than the histogram.</p>
    <p class="normal">For the second one, we use <code class="inlineCode">plot_lines=True</code> to plot the ICE lines, but we can only plot a fraction of them, so <code class="inlineCode">frac_to_plot=0.01</code> randomly selects 1% of them.</p>
    <p class="normal">For the third one, instead of showing a rug, we can construct the X-axis with quantiles (<code class="inlineCode">to_bins=True</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">pdp_single_feature = pdp.PDPIsolate(
    model=cb_mdl, df=X_test, model_features=X_test.columns,\
    feature=<span class="hljs-string">'year'</span>, feature_name=<span class="hljs-string">'year'</span>, n_classes=<span class="hljs-number">0</span>,\
    n_jobs=-<span class="hljs-number">1</span>)
fig, axes = pdp_single_feature.plot(plot_pts_dist=<span class="hljs-literal">True</span>)
fig.show()
    fig, axes = pdp_single_feature.plot(
    plot_pts_dist=<span class="hljs-literal">True</span>, plot_lines=<span class="hljs-literal">True</span>, frac_to_plot=<span class="hljs-number">0.01</span>
)
fig.show() 
    fig, axes = pdp_single_feature.plot(
    to_bins=<span class="hljs-literal">True</span>, plot_lines=<span class="hljs-literal">True</span>, frac_to_plot=<span class="hljs-number">0.01</span>
)
fig.show()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker474"/>preceding snippet will output the three plots in <em class="italic">Figure 4.10</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_10.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.10: Three variations of PDPBox’s PDPs for year</p>
    <p class="normal">The ICE lines enrich the PDP plots by showing the potential for variance. The last plot in <em class="italic">Figure 4.10</em> also shows how, even though rugs or histograms are useful guides, organizing the<a id="_idIndexMarker475"/> axis in quantiles better helps visualize distribution. In this case, two-thirds of <code class="inlineCode">year</code> is distributed before 2017. And the two decades between 1984 and 2005 only account for 11%. It’s only fair they get a corresponding portion of the plot.</p>
    <p class="normal">We can now create a few lists we will use to iterate across different kinds of features, whether continuous (<code class="inlineCode">cont_feature_l</code>), binary (<code class="inlineCode">bin_feature_l</code>), or categorical (<code class="inlineCode">cat_feature_l</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">cont_feature_l = [
    <span class="hljs-string">'year'</span>, <span class="hljs-string">'odometer'</span>, <span class="hljs-string">'cylinders'</span>, <span class="hljs-string">'epa_displ'</span>, <span class="hljs-string">'make_pop'</span>,\
    <span class="hljs-string">'make_yr_pop'</span>, <span class="hljs-string">'model_yr_pop'</span>
]
make_cat_feature_l = [
    <span class="hljs-string">'make_cat_obsolete'</span>, <span class="hljs-string">'make_cat_regular'</span>, <span class="hljs-string">'make_cat_premium'</span>,\
    <span class="hljs-string">'make_cat_luxury'</span>, <span class="hljs-string">'make_cat_luxury_sports'</span>
]
bin_feature_l = [<span class="hljs-string">'model_premier'</span>, <span class="hljs-string">'auto_trans'</span>]
cat_feature_l = make_cat_feature_l + bin_feature_l
</code></pre>
    <p class="normal">To quickly get a sense of what PDPs look like for each feature, we can iterate across one of the lists of features producing PDP plots for each one. We will do continuous features (<code class="inlineCode">cont_feature_l</code>) because it’s easiest to visualize, but you can try one of the other lists as well:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> cont_feature_l:
    pdp_single_feature = pdp.PDPIsolate(
        model=cb_mdl, df=X_test, model_features=X_test.columns,\
        feature=feature, feature_name=feature, n_classes=<span class="hljs-number">0</span>,\
        n_jobs=-<span class="hljs-number">1</span>
    )
    fig, axes = pdp_single_feature.plot(
        to_bins=<span class="hljs-literal">True</span>, plot_lines=<span class="hljs-literal">True</span>, frac_to_plot=<span class="hljs-number">0.01</span>,\
        show_percentile=<span class="hljs-literal">True</span>, engine=<span class="hljs-string">'matplotlib'</span>
    )
</code></pre>
    <p class="normal">The preceding code will output eight plots, including the one in <em class="italic">Figure 4.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_11.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.11: PDPBox’s PDP for odometer</p>
    <p class="normal">The <a id="_idIndexMarker476"/>beeswarm plot in <em class="italic">Figure 4.8</em> shows that a low value of <code class="inlineCode">odometer</code> correlates with the model outputting a higher price. In <em class="italic">Figure 4.11</em>, it depicts how it’s mostly monotonically decreasing except in the extremes. It’s interesting that there are <code class="inlineCode">odometer</code> values of zero and ten million in the extremes. While it makes sense that the model learned that <code class="inlineCode">odometer</code> made no difference in prices when it’s zero, a value of ten million is an anomaly, and for this reason, you can tell that the ICE lines are going in different directions because the model is not sure what to make of such a value. We must also keep in mind that ICE plots, and thus PDPs, are generated with simulations that may create examples that wouldn’t exist in real life, such as a brand-new car with an extremely high <code class="inlineCode">odometer</code> value.</p>
    <p class="normal">If you replace <code class="inlineCode">cont_feature_l</code> with <code class="inlineCode">make_cat_feature_l</code> and re-run the previous snippet, you will realize that some make categories correlate positively while others negatively with the outcome. This finding shouldn’t be surprising considering some make categories, such as “luxury” are indicative of a high price and others of a lower one. That being said, <code class="inlineCode">make_cat</code> is a categorical feature that was one-hot encoded, so it’s not natural to create PDP plots as if each category were an independent binary feature.</p>
    <p class="normal">Fortunately, you <a id="_idIndexMarker477"/>can create a PDP with each one-hot encoded category side by side. All you need to do is plug in the list of product-type features in the <code class="inlineCode">feature</code> attribute. PDPBox also has a “predict plot,” which can help provide context by showing prediction distribution across feature values. <code class="inlineCode">PredictPlot</code> is easy to plot, having many of the same attributes as the <code class="inlineCode">plot</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">pdp_multi_feature = pdp.PDPIsolate(
    model=cb_mdl, df=X_test, model_features=X_test.columns,\
    feature=make_cat_feature_l, feature_name=<span class="hljs-string">"make_cat"</span>, n_classes=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>
)
fig, axes = pdp_multi_feature.plot(
    plot_lines=<span class="hljs-literal">True</span>, frac_to_plot=<span class="hljs-number">0.01</span>, show_percentile=<span class="hljs-literal">True</span>
)
fig.show()
predict_plot = info_plots.PredictPlot(
    model=cb_mdl, df=X_test, feature_name=<span class="hljs-string">"make_cat"</span>, n_classes=<span class="hljs-number">0</span>,
    model_features=X_test.columns, feature=make_cat_feature_l
)
fig, _, _ = predict_plot.plot()
fig.show()
</code></pre>
    <p class="normal">The preceding snippet should create the two plots in <em class="italic">Figure 4.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_12.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.12: PDPBox’s actual plot for make categories</p>
    <p class="normal">The PDP for <code class="inlineCode">make_cat</code> in <em class="italic">Figure 4.12</em> shows the tendency of “luxury” and “premium” categories to lead to higher prices but only by about 3,000 dollars on average with a lot of variance depicted in the ICE lines. However, remember what was said about simulations not being necessarily representative of real-life scenarios?</p>
    <p class="normal">If we<a id="_idIndexMarker478"/> compare the PDP to the box and whiskers in the actual predictions in <em class="italic">Figure 4.12</em>, we can tell that the difference in average prediction between regular and any of the “luxury” and “premium” categories is at least seven thousand dollars. Of course, the average doesn’t tell the whole story because even a premium car with too much mileage or that’s very old might cost less than a regular vehicle. The price depends on many factors besides the reputation of the make. Of course, just judging by how the boxes and whiskers align in <em class="italic">Figure 4.12</em>, “luxury sports” and “obsolete” categories are stronger indications of higher and lower prices, respectively.</p>
    <p class="normal">PDPs are often straightforward to interpret and relatively quick to generate. However, the simulation strategy it employs doesn’t account for feature distribution and heavily relies on the assumption of feature independence, which can lead to counterintuitive examples. We will now explore two alternatives.</p>
    <h2 id="_idParaDest-122" class="heading-2">SHAP scatter plot</h2>
    <p class="normal">SHAP values are <a id="_idIndexMarker479"/>available for every data point, enabling you to plot them against feature values, resulting in a PDP-like visualization with model impact (SHAP values) on the <em class="italic">y</em>-axis and feature values on the <em class="italic">x</em>-axis. Being a similar concept, the SHAP library initially called this a <code class="inlineCode">dependence_plot</code>, but now it’s referred to as a scatter plot. Despite the similarities, PDP and SHAP values are calculated differently.</p>
    <p class="normal">Creating a SHAP scatter plot is simple, requiring only the explanation object. Optionally, you can color-code the dots according to another feature to understand potential interactions. You can also clip outliers from the <em class="italic">x</em>-axis using percentiles with <code class="inlineCode">xmin</code> and <code class="inlineCode">xmax</code> attributes and make dots 20% opaque (<code class="inlineCode">alpha</code>) to identify sparser areas more easily:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.scatter(
    cb_shap[:,<span class="hljs-string">"odometer"</span>], color=cb_shap[:,<span class="hljs-string">"year"</span>], xmin=<span class="hljs-string">"percentile(1)"</span>,\
    xmax=<span class="hljs-string">"</span><span class="hljs-string">percentile(99)"</span>, alpha=<span class="hljs-number">0.2</span>
)
shap.plots.scatter(
    cb_shap[:,<span class="hljs-string">"long"</span>], color=cb_shap[:,<span class="hljs-string">"epa_displ"</span>],\
    xmin=<span class="hljs-string">"percentile(0.5)"</span>, xmax=<span class="hljs-string">"percentile(99.5)"</span>, alpha=<span class="hljs-number">0.2</span>, x_jitter=<span class="hljs-number">0.5</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker480"/>above snippet creates <em class="italic">Figure 4.13</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_13.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.13: SHAP’s scatter plots for odometer and long, color-coded for year and epa_displ respectively</p>
    <p class="normal">The<a id="_idIndexMarker481"/> first plot in <em class="italic">Figure 4.13</em> depicts how a higher <code class="inlineCode">odometer</code> reading negatively impacts the model outcome. Also, the color coding shows that the SHAP values for higher years are even lower when the odometer reading is over ninety thousand. In other words, an old car with a high odometer reading is expected, but with a new car, it’s a red flag!</p>
    <p class="normal">The second plot in <em class="italic">Figure 4.13</em> is very interesting; it shows how the west coast of the country (at about -120°) correlates with higher SHAP values and the further east it goes, the lower the SHAP values. Hawaii, Anchorage, and Alaska (at about -150°) are also higher than the east coast of the United States (at around -75°). The color coding shows how the more liters displaced of fuel mostly leads to higher SHAP values but it’s not as stark of a difference the further east you go.</p>
    <p class="normal">The <code class="inlineCode">scatter</code> plot works well for continuous features, but can you use it for discrete ones? Yes! Let’s create one for <code class="inlineCode">make_cat_luxury</code>. Since there are only two possible values on the <em class="italic">x</em>-axis, 0 and 1, what makes sense is to jitter them so that all the dots don’t get plotted on top of each other. For instance, <code class="inlineCode">x_jitter=0.4</code> means they will be jittered horizontally up to 0.4, or 0.2 on each side of the original value. We can combine this with <code class="inlineCode">alpha</code> to ensure that we can appreciate the density:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.scatter(
    cb_shap[:,<span class="hljs-string">"make_cat_luxury"</span>], color=cb_shap[:,<span class="hljs-string">"year"</span>], x_jitter=<span class="hljs-number">0.4</span>,\
    alpha=<span class="hljs-number">0.2</span>, hist=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">The snippet above produces <em class="italic">Figure 4.14</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_14.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.14: SHAP’s scatter plot for make_cat_luxury color-coded for year</p>
    <p class="normal"><em class="italic">Figure 4.14</em> shows<a id="_idIndexMarker482"/> that according to the SHAP values, <code class="inlineCode">make_cat_fha=1</code> positively impacts the model outcome, whereas <code class="inlineCode">make_cat_fha=1</code> has a mildly negative effect. The color coding suggests that lower years tempers the impact, making it smaller. This makes sense because older luxury cars have depreciated.</p>
    <p class="normal">While SHAP scatter plots may be an improvement from PDP plots, SHAP’s tree explainers trade fidelity for speed, causing features that have no influence on the model to potentially have a SHAP value above zero. Fidelity refers to the accuracy of an explanation in representing the behavior of the model. To get higher fidelity, you need to use a method that makes fewer shortcuts in understanding what the model is doing, and even then, some adjustments to parameters like using a greater sample size will increase fidelity too because you are using more data to create the explanations. In this case, the solution is to use <code class="inlineCode">KernelExplainer</code> instead because, as we previously discussed, it is more comprehensive, but it has issues with feature dependence. So there’s no free lunch! Next, we will cover ALE plots as a partial solution to these problems.</p>
    <h2 id="_idParaDest-123" class="heading-2">ALE plots</h2>
    <p class="normal">ALE plots <a id="_idIndexMarker483"/>are advantageous over PDPs because they are unbiased and faster. ALE accounts for data distributions when calculating feature effects, resulting in an unbiased representation. The algorithm divides the feature space into equally sized windows and computes how predictions change within these windows, resulting in the <em class="italic">local effects</em>. Summing the effects across all windows makes them <em class="italic">accumulated</em>.</p>
    <p class="normal">You can easily generate the ALE plot with the <code class="inlineCode">ale</code> function. All you need is some data (<code class="inlineCode">X_test_no_outliers</code>), the model (<code class="inlineCode">cb_mdl</code>), and the feature(s) and feature type(s) to be plotted. Optionally, you can enter the <code class="inlineCode">grid_size</code>, which will define the window size for local effects calculations. It is <code class="inlineCode">20</code> by default, but we can increase it for higher fidelity, provided you have enough data. As mentioned previously, some adjustments to parameters can affect fidelity. For window size, it will break the data into smaller bins to compute values and thus make them more granular. Also, confidence intervals are shown by default. Incidentally, it’s best to remove outliers in this case because it’s hard to appreciate the plot when the maximum loan nearly reaches $8 million. You can try using <code class="inlineCode">X_test</code> instead of <code class="inlineCode">X_test_no_outliers</code> to see what I mean.</p>
    <p class="normal">Confidence intervals are shown by default. It’s preferable to exclude outliers in this case, as it’s difficult to appreciate the plot when only very few vehicles represent years before 1994 and after 2021 and have very low and very high odometer readings. You can use <code class="inlineCode">X_test</code> instead of <code class="inlineCode">X_test_no_outliers</code> in the <code class="inlineCode">ale</code> function to see the difference:</p>
    <pre class="programlisting code"><code class="hljs-code">X_test_no_outliers = X_test[
    (X_test.year.quantile(<span class="hljs-number">.01</span>) &lt;= X_test.year) &amp;\
    (X_test.year &lt;= X_test.year.quantile(<span class="hljs-number">.99</span>)) &amp;\
    (X_test.odometer.quantile(<span class="hljs-number">.01</span>) &lt;= X_test.odometer) &amp;\
    (X_test.odometer &lt;= X_test.odometer.quantile(<span class="hljs-number">.99</span>))
]
ale_effect = ale(
    X=X_test_no_outliers, model=cb_mdl, feature=[<span class="hljs-string">'odometer'</span>],\
    feature_type=<span class="hljs-string">'continuous'</span>, grid_size=<span class="hljs-number">80</span>
)
plt.show()
ale_effect = ale(
    X=X_test_no_outliers, model=cb_mdl,\
    feature=[<span class="hljs-string">'make_cat_luxury'</span>],feature_type=<span class="hljs-string">'discrete'</span>
)
plt.show()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker484"/>above snippet produces the two plots in <em class="italic">Figure 4.15</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_15.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.15: ALE plots for odometer and make_cat_luxury</p>
    <p class="normal">The first plot in <em class="italic">Figure 4.15</em> is the ALE plot for <code class="inlineCode">odometer</code>. As portrayed in <em class="italic">Figure 4.13</em>, the effect on <a id="_idIndexMarker485"/>the model goes from positive to negative as the odometer reading increases. However, unlike SHAP, in the ALE plot, it goes negative long before the odometer reaches 90,000. Please note the confidence interval is so thin it’s only visible somewhere under 10,000. The second ALE plot shows a significant positive impact of the “luxury” category on the outcome and that luxury vehicles are not as represented as other ones.</p>
    <p class="normal">So far, we’ve only discussed single-feature explanations. But we also can observe the interaction between features, which we will cover next.</p>
    <h1 id="_idParaDest-124" class="heading-1">Feature interactions</h1>
    <p class="normal">Features may not <a id="_idIndexMarker486"/>influence predictions independently. For example, as discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Key Concepts of Interpretability</em>, determining obesity based solely on weight isn’t possible. A person’s height or body fat, muscle, and other percentages are needed. Models understand data through correlations, and features are often correlated because they are naturally related, even if they are not linearly related. Interactions are what the model may do with correlated features. For instance, a decision tree may put them in the same branch, or a neural network may arrange its parameters in such a way that it creates interaction effects. This also occurs in our case. Let’s explore this through several feature interaction visualizations.</p>
    <h2 id="_idParaDest-125" class="heading-2">SHAP bar plot with clustering</h2>
    <p class="normal">SHAP comes <a id="_idIndexMarker487"/>with a hierarchical clustering method (<code class="inlineCode">shap.utils.hclust</code>) that allows for the grouping of training features based on the “redundancy” between any given pair of features. This refers to the degree to which they depend on each other, on a scale from complete redundancy (0) to total independence (1). We won’t use the entire dataset for this task because it would take a very long time, so we will use a 10% <code class="inlineCode">sample</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">X_samp = X.sample(frac=<span class="hljs-number">0.1</span>)
y_samp = y.loc[X_samp.index]
clustering = shap.utils.hclust(X_samp, y_samp)
</code></pre>
    <p class="normal">We can employ the same bar chart as in <em class="italic">Figure 4.6</em>, but this time, we input the <code class="inlineCode">clustering</code> and clustering cutoff and voilá! We can visualize which features are most redundant. Our aim is to pinpoint relationships that are less than 0.7 independent (<code class="inlineCode">clustering_cutoff</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">shap.plots.bar(cb_shap, clustering=clustering, clustering_cutoff=<span class="hljs-number">0.7</span>)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker488"/>preceding snippet generates the bar plot in <em class="italic">Figure 4.16</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_16.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.16: SHAP bar plot with clustering</p>
    <p class="normal">The dendrogram on the right in <em class="italic">Figure 4.16</em> reveals which features rely on each other the most and at three tiers. For instance, <code class="inlineCode">year</code> is dependent on <code class="inlineCode">odometer</code>, and when combined, they<a id="_idIndexMarker489"/> both depend on <code class="inlineCode">epa_co2</code>, which in turn depends indirectly on a number of features, including the fuel displacement (<code class="inlineCode">epa_displ</code>) and <code class="inlineCode">cylinders</code> features. Also, please note that all make category features depend on each other but also on the make’s relative popularity (<code class="inlineCode">make_pop</code>). This finding makes sense because some categories are overall more popular than others. These insights can serve as a guide for subsequent investigations.</p>
    <h2 id="_idParaDest-126" class="heading-2">2D ALE plots</h2>
    <p class="normal">The optimal <a id="_idIndexMarker490"/>way to visually inspect the impact of two variables on predictions is through 2D ALE plots, primarily because it’s unbiased when dealing with correlated features.</p>
    <p class="normal">Let’s scrutinize the top features, <code class="inlineCode">year</code> and <code class="inlineCode">odometer</code>, which also have a clear dependency. We’ll utilize the test dataset minus the outliers so that the plot focuses on the core of the data—that is, the part containing approximately 98% of the data points. This time, instead of a single feature, we’ll insert a list of two features:</p>
    <pre class="programlisting code"><code class="hljs-code">features_l = [<span class="hljs-string">"year"</span>, <span class="hljs-string">"odometer"</span>]
ale_effect = ale(
    X=X_test_no_outliers, model=cb_mdl, feature=features_l,\
    feature_type=<span class="hljs-string">'continuous'</span>, grid_size=<span class="hljs-number">50</span>, include_CI=<span class="hljs-literal">False</span>
)
plt.show()
</code></pre>
    <p class="normal">The above code produces the ALE plot in <em class="italic">Figure 4.17</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_17.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.17: 2D ALE plot for odometer and year</p>
    <p class="normal">As<a id="_idIndexMarker491"/> seen in <em class="italic">Figure 4.17</em>, except for a slim range of extremely high <code class="inlineCode">odometer</code> readings and areas where older cars and low <code class="inlineCode">odometer</code> readings overlap, there’s a modest effect for most of the plot. Mostly, they seem to only have a larger negative effect in the corners.</p>
    <p class="normal">An important point to remember is that SHAP’s clustering distance ranges from redundancy to independence. The issue with highly correlated features is that, at a certain point, they cease depending on each other and become entirely redundant. So, let’s examine how close to redundancy these two features are with the <code class="inlineCode">clustering</code> array we created with <code class="inlineCode">shap.utils.hclust</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">np.set_printoptions(suppress=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(clustering)
</code></pre>
    <p class="normal">The preceding snippet should output the below array:</p>
    <pre class="programlisting con"><code class="hljs-con">[[  2.     21.      0.      2.   ]
 [ 22.     52.      0.      3.   ]
 [ 23.     53.      0.      4.   ]
 [ 24.     54.      0.      5.   ]
 [ 20.     55.      0.      6.   ]
 [ 12.     14.      0.235   2.   ]
 [  0.     57.      0.253   3.   ]
 [  1.     58.      0.262   4.   ]
 [ 13.     59.      0.289   5.   ]
 [  4.      7.      0.383   2.   ]
 [  9.     11.      0.394   2.   ]
 [ 43.     44.      0.429   2.   ]
 [ 15.     17.      0.462   2.   ]
 [  5.     56.      0.477   7.   ]
 [ 10.     61.      0.605   3.   ]
 [ 46.     49.      0.647   2.   ]
 [ 62.     66.      0.654   5.   ]
 [ 51.     67.      0.665   3.   ]
 [  6.     68.      0.67    6.   ]
 [ 31.     70.      0.673   7.   ]
 [ 65.     71.      0.694  14.   ]
 [ 25.     32.      0.717   2.   ]
[ 41.    101.      0.997  52.   ]]
</code></pre>
    <p class="normal">The array<a id="_idIndexMarker492"/> consists of rows of edges. The columns represent node number 1, node number 2, their distance, and parent node number. At the top, there are a few pairs that are completely redundant such as <code class="inlineCode">make_pop</code> (feature 2) and <code class="inlineCode">make_cat_luxury_sports</code> (feature 21). Not a strange outcome considering luxury sports cars, like a Ferrari, are the least popular vehicles in the dataset because they aren’t sold as often as, say, a Ford:</p>
    <pre class="programlisting code"><code class="hljs-code">features_l = [<span class="hljs-string">"cylinders"</span>, <span class="hljs-string">"epa_displ"</span>]
ale_effect = ale(X=X_test, model=cb_mdl, feature=features_l,\
    feature_type=<span class="hljs-string">'discrete'</span>, grid_size=<span class="hljs-number">50</span>
)
</code></pre>
    <p class="normal">The preceding snippet outputs <em class="italic">Figure 4.18</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_18.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.18: 2D ALE plot for epa_displ and cylinders</p>
    <p class="normal"><em class="italic">Figure 4.18</em> distinctly<a id="_idIndexMarker493"/> illustrates higher interaction effects where <code class="inlineCode">cylinders</code> is larger than 8 and where <code class="inlineCode">epa_displ</code> is under 4. This finding is counterintuitive considering vehicles with a lot of cylinders are unlikely to have low amounts of engine displacement. On the other hand, the higher interaction effects when there are over 4 cylinders and an engine displacement of at least 6 liters make more sense. Please note that there are other factors such as <code class="inlineCode">year</code> and <code class="inlineCode">model_type</code> that correlate with these two features, but ALE is great at separating the effects of <code class="inlineCode">cylinders</code> and <code class="inlineCode">epa_displ</code> from other highly correlated features.</p>
    <h2 id="_idParaDest-127" class="heading-2">PDP interactions plots</h2>
    <p class="normal">I’m <a id="_idIndexMarker494"/>sure you’re wondering: Given the numerous limitations of PDP, when and where should we contemplate using 2D PDP?</p>
    <p class="normal">Only when a proven relationship exists between two features, but they are not entirely redundant or independent, and ideally, when they perfectly complement each other. Even then, an ALE plot is advisable as it teases out higher-order effects.</p>
    <p class="normal">Nonetheless, for illustrative purposes, we will generate a 2D PDP with <code class="inlineCode">long</code> and <code class="inlineCode">lat</code>, which are indirectly connected. The code for 2D is very similar to 1D, with the exception that we use <code class="inlineCode">PDPInteract</code> instead of <code class="inlineCode">PDPIsolate</code>, and then for the plot function, designate the <code class="inlineCode">plot_type</code> as <code class="inlineCode">contour</code>, but <code class="inlineCode">grid</code> could also be used:</p>
    <pre class="programlisting code"><code class="hljs-code">features_l = ["long", "lat"]
pdp_interaction_feature = pdp.PDPInteract(
    model=cb_mdl, df=X_test, model_features=X_test.columns,\
    features=features_l, feature_names=features_l, n_classes=0,\
    num_grid_points=15, n_jobs=-1
)
fig, _ = pdp_interaction_feature.plot(
    plot_type='contour', plot_pdp=False
)
fig.show()
</code></pre>
    <p class="normal">The preceding snippet outputs <em class="italic">Figure 4.19</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_19.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.19: PDP interaction contour plot for long and lat</p>
    <p class="normal"><em class="italic">Figure 4.19</em> serves <a id="_idIndexMarker495"/>as proof of how the features are related to the outcome: <code class="inlineCode">long</code> seems to be responsible for most of the effect, but in some areas, <code class="inlineCode">lat</code> seems to make a difference, especially in Alaska in the top-left corner. We can check the reliability of this result with a 2D predict plot (<code class="inlineCode">InteractPredictPlot</code>). As with <em class="italic">Figure 4.12</em>, the objective is to display the distribution of the data and the predicted scores for that data, but this time, it features a grid of bins that are color-coded for average scores and size-coded for the number of test observations. We can contrast this with the corresponding 2D target plot (<code class="inlineCode">InteractTargetPlot</code>), which does the same but for the labels (<code class="inlineCode">target</code>) and not the predicted score:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, axes, summary_df = info_plots.InteractPredictPlot(
    model=cb_mdl, X=X_test, features=features_l,\
    feature_names=features_l, num_grid_points=(<span class="hljs-number">15</span>,<span class="hljs-number">12</span>)
)
fig, axes, summary_df = info_plots.InteractTargetPlot(
    df=X_train.join(y_train), target=<span class="hljs-string">'price'</span>, features=features_l,\
    feature_names=features_l, num_grid_points=(<span class="hljs-number">15</span>,<span class="hljs-number">12</span>)
)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker496"/> preceding snippet creates both plots in <em class="italic">Figure 4.20</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_04_20.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 4.20: PDP actual plot for long and lat</p>
    <p class="normal">From the<a id="_idIndexMarker497"/> initial plot, it’s evident that the median predictions are greatest on the western half (on the right side) and especially toward the southwest (on the bottom right), but not by much. The second plot affirms that the labels are indeed more likely to be high-priced in these sections of the plot. Therefore, it’s no surprise that the model learned this distribution to its degree of accuracy. These plots aid in affirming a relationship between both features.</p>
    <p class="normal">In the following chapter, we’ll delve deeper into local explanations!</p>
    <h2 id="_idParaDest-128" class="heading-2">Mission accomplished</h2>
    <p class="normal">We set out to uncover what features helped predict the used car price for the two-sided marketplace. Using the intrinsic parameters of the decision trees, permutation feature importance, and SHAP, we realized that at least 15 features have a negligible impact on either model. Also, about an equal amount of features holds the lion’s share of impact. Some of the most critical features, like engine displacement (<code class="inlineCode">epa_displ</code>) and cylinders, are technical and can vary for the same make and model, so the user would have to know and enter them.</p>
    <p class="normal">We also found interesting and perfectly valid relationships between different features such as <code class="inlineCode">year</code> and <code class="inlineCode">odometer</code>, which help us understand how they interact in the model. We can share all of these findings with the tech startup.</p>
    <h1 id="_idParaDest-129" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should understand what model-specific methods to compute feature importance are and their shortcomings. Then, you should have learned how model-agnostic methods’ permutation feature importance and SHAP values are calculated and interpreted. You also learned the most common ways to visualize model explanations. You should know your way around global explanation methods like global summaries, feature summaries, and feature interaction plots and their advantages and disadvantages. </p>
    <p class="normal">In the next chapter, we will delve into local explanations.</p>
    <h1 id="_idParaDest-130" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Shapley, Lloyd S., 1953, <em class="italic">A value for n-person Games</em>. In Kuhn, H. W.; Tucker, A. W. (eds.). <em class="italic">Contributions to the Theory of Games. Annals of Mathematical Studies</em>. 28. Princeton University Press. pp. 307–317: <a href="https://doi.org/10.1515/9781400881970-018"><span class="url">https://doi.org/10.1515/9781400881970-018</span></a> </li>
      <li class="bulletList">Lundberg, S., and Lee, S., 2017, <em class="italic">A Unified Approach to Interpreting Model Predictions</em>. Advances in Neural Information Processing Systems: <a href="https://arxiv.org/abs/1705.07874"><span class="url">https://arxiv.org/abs/1705.07874</span></a> (documentation for SHAP: <a href="https://github.com/slundberg/shap"><span class="url">https://github.com/slundberg/shap</span></a>) </li>
      <li class="bulletList">Lundberg, S.M., Erion, G., and Lee, S., 2018, <em class="italic">Consistent Individualized Feature Attribution for Tree Ensembles</em>. ICML Workshop: <a href="https://arxiv.org/abs/1802.03888"><span class="url">https://arxiv.org/abs/1802.03888</span></a></li>
      <li class="bulletList">Molnar, C., 2019, <em class="italic">Interpretable Machine Learning. A Guide for Making Black Box Models Explainable</em>: <a href="https://christophm.github.io/interpretable-ml-book/"><span class="url">https://christophm.github.io/interpretable-ml-book/</span></a></li>
      <li class="bulletList">Documentation for SHAP plots: <a href="https://shap.readthedocs.io/en/latest/api.html#plots"><span class="url">https://shap.readthedocs.io/en/latest/api.html#plots</span></a></li>
      <li class="bulletList">Original paper for PDP method: Friedman, J.H., 2001, <em class="italic">Greedy function approximation: A gradient boosting machine</em>. Annals of Statistics, 29, 1189-1232: <a href="https://doi.org/10.1214/aos/1013203451"><span class="url">https://doi.org/10.1214/aos/1013203451</span></a></li>
      <li class="bulletList">Documentation for PDPBox: <a href="https://pdpbox.readthedocs.io/en/latest/"><span class="url">https://pdpbox.readthedocs.io/en/latest/</span></a></li>
      <li class="bulletList">Original paper for ALE method: Apley, D.W., &amp; Zhu, J., 2020, <em class="italic">Visualizing the effects of predictor variables in black box supervised learning models</em>. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82. <a href="https://arxiv.org/abs/1612.08468"><span class="url">https://arxiv.org/abs/1612.08468</span></a></li>
      <li class="bulletList">Repository for PyALE: <a href="https://github.com/DanaJomar/PyALE"><span class="url">https://github.com/DanaJomar/PyALE</span></a></li>
      <li class="bulletList">Original paper for LIME method: Ribeiro, M., Singh, S., and Guestrin, C., 2016, <em class="italic">“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</em>. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. <a href="https://arxiv.org/abs/1602.04938"><span class="url">https://arxiv.org/abs/1602.04938</span></a></li>
      <li class="bulletList">Documentation for LIME: <a href="https://lime-ml.readthedocs.io/en/latest/"><span class="url">https://lime-ml.readthedocs.io/en/latest/</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_4.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>