["```py\ntwitter.get_user_timeline(screen_name='GOP', tweet_mode='extended', count=500)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy import sparse\n\nSRC_PATH = '/home/ec2-user/SageMaker/mastering-ml-on-aws/chapter2/'\nstop_words = [word.strip() for word in open(SRC_PATH + 'stop_words.txt').readlines()]\nwith open(SRC_PATH + 'dem.txt', 'r') as file:\n   dem_text = [line.strip('\\n') for line in file]\nwith open(SRC_PATH + 'gop.txt', 'r') as file:\n   gop_text = [line.strip('\\n') for line in file]\n```", "```py\nvectorizer = CountVectorizer(input=dem_text + gop_text,\n                             stop_words=stop_words,\n                             max_features=1200)\n```", "```py\ndem_bow = vectorizer.fit_transform(dem_text)\ngop_bow = vectorizer.fit_transform(gop_text)\n```", "```py\n>>> gop_bow.toarray()\n\narray([[0, 0, 1, ..., 0, 1, 0],\n      [0, 0, 0, ..., 0, 0, 1],\n      [0, 1, 0, ..., 0, 0, 0],\n      ...,\n      [0, 0, 0, ..., 0, 0, 0],\n      [0, 1, 0, ..., 0, 0, 0],\n      [0, 0, 0, ..., 0, 1, 0]], dtype=int64)\n```", "```py\nx = sparse.vstack((dem_bow, gop_bow))\n```", "```py\nones = np.ones(200)\nzeros = np.zeros(200)\ny = np.hstack((ones, zeros))\n```", "```py\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n```", "```py\nfrom sklearn.naive_bayes import BernoulliNB\nnaive_bayes = BernoulliNB()\nmodel = naive_bayes.fit(x_train, y_train)\n```", "```py\ny_predictions = model.predict(x_test)\n```", "```py\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predictions)\n```", "```py\nfrom sklearn.pipeline import Pipeline\nx_train, x_test, y_train, y_test = train_test_split(dem_text + gop_text, y, test_size=0.25, random_state=5)\npipeline = Pipeline([('vect', vectorizer), ('nb', naive_bayes)])\npipeline_model = pipeline.fit(x_train, y_train)\ny_predictions = pipeline_model.predict(x_test)\naccuracy_score(y_test, y_predictions)\n```", "```py\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SQLContext\n\nsc = SparkContext('local', 'test')\nsql = SQLContext(sc)\n```", "```py\nfrom pyspark.sql.functions import lit\n\ndems_df = sql.read.text(\"file://\" + SRC_PATH + 'dem.txt')\ngop_df = sql.read.text(\"file://\" + SRC_PATH + 'gop.txt')\ncorpus_df = dems_df.select(\"value\", lit(1).alias(\"label\")).union(gop_df.select(\"value\", lit(0).alias(\"label\")))\n```", "```py\n>>> corpus_df.select(\"*\").limit(2).show()\n\n+--------------------+-----+\n|               value|label|\n+--------------------+-----+\n|This ruling is th...| 1 . |\n|No president shou...| 1 . |\n+--------------------+-----+\n```", "```py\ntrain_df, test_df = corpus_df.randomSplit([0.75, 0.25])\n```", "```py\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover\ntokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\nstop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_cleaned\")\nvectorizer = CountVectorizer(inputCol=\"words_cleaned\", outputCol=\"features\")\ncleaning_pipeline = Pipeline(stages = [tokenizer, stop_words_remover, vectorizer])\ncleaning_pipeline_model = cleaning_pipeline.fit(corpus_df)\ncleaned_training_df = cleaning_pipeline_model.transform(train_df)\ncleaned_testing_df = cleaning_pipeline_model.transform(test_df)\n```", "```py\n>>> cleaned_training_df.show(n=3)\n\n+-----------+------------------+-------------+--------------------+\n| value     |label| . words .  |words_cleaned| features           |\n+-----------+------------------+-------------+--------------------+\n|#Tuesday...| 1 . |[#tuesday...|[#tuesday... |(3025,[63,1398,18...|\n|#WorldAI...| 1 . |[#worlda....|[#worldai... |(3025,[37,75,155,...|\n|@Tony4W....| 1 . |[.@tony4w...|[.@tony4w... |(3025,[41,131,160...|\n+-----------------+------------+-------------+--------------------+\n```", "```py\nfrom pyspark.ml.classification import NaiveBayes\nnaive_bayes = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n```", "```py\nnaive_bayes_model = naive_bayes.fit(cleaned_training_df)\npredictions_df = naive_bayes_model.transform(cleaned_testing_df)\n\n>>> predictions_df.select(\"features\", \"label\", \"prediction\").limit(3).show()\n+--------------------+-----+----------+\n| features           |label|prediction|\n+--------------------+-----+----------+\n|(3025,[1303,1858,...| 1 . | 1.0      |\n|(3025,[1,20,91,13...| 1 . | 1.0      |\n|(3025,[16,145,157...| 1 . | 1.0      |\n+--------------------+-----+----------+\n```", "```py\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(\n   labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator.evaluate(predictions_df)\n```", "```py\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport json\nimport boto3\n\nsess = sagemaker.Session()\nrole = get_execution_role()\nbucket = \"mastering-ml-aws\"\nprefix = \"chapter2/blazingtext\"\n```", "```py\n__label__1 We are forever g..\n __label__0 RT @AFLCIO: Scott Walker.\n __label__0 Democrats will hold this\n __label__1 Congratulations to hundreds of thousands ...\n```", "```py\nwith open(SRC_PATH + 'dem.txt', 'r') as file:\n    dem_text = [\"__label__0 \" + line.strip('\\n') for line in file]\n\nwith open(SRC_PATH + 'gop.txt', 'r') as file:\n    gop_text = [\"__label__1 \" + line.strip('\\n') for line in file]\n\ncorpus = dem_text + gop_text\n```", "```py\nfrom sklearn.model_selection import train_test_split\ncorpus_train, corpus_test = train_test_split(corpus, test_size=0.25, random_state=42) \n\ncorpus_train_txt = \"\\n\".join(corpus_train)\ncorpus_test_txt = \"\\n\".join(corpus_test)\n\nwith open('tweets.train', 'w') as file:\n    file.write(corpus_train_txt) \nwith open('tweets.test', 'w') as file:\n    file.write(corpus_test_txt)\n```", "```py\ntrain_path = prefix + '/train'\nvalidation_path = prefix + '/validation'\n\nsess.upload_data(path='tweets.train', bucket=bucket, key_prefix=train_path)\nsess.upload_data(path='tweets.test', bucket=bucket, key_prefix=validation_path)\n\ns3_train_data = 's3://{}/{}'.format(bucket, train_path)\ns3_validation_data = 's3://{}/{}'.format(bucket, validation_path)\n```", "```py\ncontainer = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', \"blazingtext\", \"latest\")\n\ns3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\nbt_model = sagemaker.estimator.Estimator(container,\n                                         role, \n                                         train_instance_count=1, \n                                         train_instance_type='ml.c4.4xlarge',\n                                         train_volume_size = 30,\n                                         train_max_run = 360000,\n                                         input_mode= 'File',\n                                         output_path=s3_output_location,\n                                         sagemaker_session=sess)\n```", "```py\nbt_model.set_hyperparameters(mode=\"supervised\", epochs=10, min_count=3, learning_rate=0.05, vector_dim=10, early_stopping=False, patience=5, min_epochs=5, word_ngrams=2) train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix') validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix') data_channels = {'train': train_data, 'validation': validation_data}\nbt_model.fit(inputs=data_channels, logs=True)\n\n```", "```py\npredictor = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n```", "```py\ncorpus_test_no_labels = [x[11:] for x in corpus_test]\npayload = {\"instances\" : corpus_test_no_labels}\nresponse = predictor.predict(json.dumps(payload))\npredictions = json.loads(response)\nprint(json.dumps(predictions, indent=2))\n```", "```py\n[ { \"prob\": [ 0.5003 ], \"label\": [ \"__label__0\" ] }, { \"prob\": [ 0.5009 ], \"label\": [ \"__label__1\" ] }...\n```", "```py\npredicted_labels = [prediction['label'][0] for prediction in predictions]\npredicted_labels[:4]\n```", "```py\n['__label__0', '__label__1', '__label__0', '__label__0']\n```", "```py\nactual_labels = [x[:10] for x in corpus_test]\nactual_labels[:4]\n```", "```py\n['__label__1', '__label__1', '__label__0', '__label__1']\n```", "```py\nmatches = [(actual_label == predicted_label) for (actual_label, predicted_label) in zip(actual_labels, predicted_labels)]\nmatches[:4]\n```", "```py\n[False, True, True, False]\n```", "```py\nmatches.count(True) / len(matches)\n```", "```py\n0.61\n```"]