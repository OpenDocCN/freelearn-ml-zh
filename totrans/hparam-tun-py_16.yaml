- en: '*Chapter 13*: Tracking Hyperparameter Tuning Experiments'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with a lot of experiments can sometimes be overwhelming. Many iterations
    of experiments will need to be done. It will become even more complicated when
    we are experimenting with many ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the importance of tracking hyperparameter
    tuning experiments, along with the usual practices. You will also be introduced
    to several open source packages that are available and learn how to utilize each
    of them in practice.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to utilize your favorite package
    to track your hyperparameter tuning experiment. Being able to track your hyperparameter
    tuning experiment will boost the effectiveness of your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the usual practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Neptune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Scikit-Optimize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Optuna
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Microsoft NNI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring MLflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to track hyperparameter tuning experiments
    with various packages. To ensure that you can reproduce the code examples in this
    chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Python 3 (version 3.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pandas` package (version 1.3.4 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `NumPy` package (version 1.21.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `scikit-learn` package (version 1.0.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `matplotlib` package (version 3.5.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Plotly` package (version 4.0.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Neptune-client` package (version 0.16.3 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Neptune-optuna` package (version 0.9.14 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Scikit-Optimize package (version 0.9.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TensorFlow` package (version 2.4.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Optuna` package (version 2.10.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `MLflow` package (version 1.27.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the usual practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conducting hyperparameter tuning experiments in a small-scale project may seem
    straightforward. We can easily do several iterations of experiments and write
    all the results in a separate document. We can log the details of the best set
    of hyperparameter values (or the tested set of hyperparameters if we perform a
    manual search method, as shown in [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*,
    Exhaustive Search*), along with the evaluation metric, in each experiment iteration.
    By having an experiment log, we can learn from the history and define a better
    hyperparameter space in the next iteration of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: When we adopt the automated hyperparameter tuning method (all the methods we’ve
    discussed so far besides the manual search method), we can get the final best
    set of hyperparameter values directly. However, this is not the case when we adopt
    the manual search method. We need to test numerous sets of hyperparameters manually.
    Several practices are adopted by the community when performing manual searches.
    Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Using a built-in Python dictionary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most straightforward approach since we just need to create a Python
    dictionary that stores all the hyperparameter values that need to be tested. Although
    this practice is very simple, it has drawbacks. For example, we may not notice
    if we overwrite some of the hyperparameter values and forget to log the correct
    set of hyperparameter values. The following example of utilizing a built-in Python
    dictionary to store all of the hyperparameter values needs to be tested in a particular
    manual search iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s look at configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Using a configuration file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether it is a JSON, YAML, or CFG file, configuration files are another option.
    We can put all the hyperparameter details within this configuration file, along
    with other additional information, including (but not limited to) project name,
    author name, and data pre-processing pipeline methods. Once you have created the
    configuration file, you can load it into your Python script or Jupyter notebook,
    and treat it like a standard Python dictionary. The main advantage of using a
    configuration file is that all the important parameters are located within a single
    file, so it will be very easy to reuse the previously saved configuration files
    and increase the readability of your code. However, utilizing configuration files
    when working with a big project or huge code base can sometimes confuse us since
    we have to maintain several configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Using additional modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `argparse` and `Click` modules come in handy if you want to specify the
    hyperparameter values or any other training arguments via the **Command Line Interface**
    (**CLI**). These modules can be utilized when we write our code in a Python script,
    not in a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Using argparse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code shows how to utilize `argparse` in a Python script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how to access the values from the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run the Python script with specified parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It is worth noting that the default values of the hyperparameters will be used
    if you don’t specify them when calling the Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Using click
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code shows how to utilize `click` in a Python script. Note that
    `click` is very similar to `argparse` with a simpler implementation. We just need
    to add decorations on top of a particular function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to `argparse`, you can run the Python script with specified parameters,
    as shown here. The default hyperparameter values will be used if you don’t specify
    them when calling the Python script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: While experimenting with either `argparse` or `click` is very easy to do, it
    is worth noting that neither saves values anywhere. Hence, it requires extra effort
    to log all of the experimented hyperparameter values in each trial.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether we are adopting manual search or other automated hyperparameter
    tuning methods, it will require a lot of effort if we have to log the resulting
    experiment’s details manually. It can be overwhelming, especially when we are
    working with larger-scale experiments where we have to test several different
    ML models, data pre-processing pipelines, and other experiment setups. That’s
    why, in the coming sections, you will be introduced to several packages that can
    help you track your hyperparameter tuning experiments so that you have a more
    effective workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Neptune
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neptune** is a Python (and R) package that acts as a metadata store for MLOps.
    This package supports a lot of features for working with the model-building metadata.
    We can utilize Neptune for tracking our experiments, not only hyperparameter tuning
    experiments but also other model-building-related experiments. We can log, visualize,
    organize, and manage our experiments just by using a single package. Furthermore,
    it also supports model registry and live monitors our ML jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing Neptune is very easy – you can just use `pip install neptune-client`
    or `conda install -c conda-forge neptune-client`. Once it has been installed,
    you need to sign up for an account to get the API token. Neptune is free for an
    individual plan within the quota limit, but you need to pay if you want to utilize
    Neptune for commercial team usage. Further information about registering yourself
    for Neptune can be found on their official website: https://neptune.ai/register.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Neptune to help track your hyperparameter tuning experiments is straightforward,
    as shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new project from your Neptune account’s home page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Creating a new Neptune project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – Creating a new Neptune project
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter a name and description for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Entering the project’s details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Entering the project’s details
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the hyperparameter tuning experiment script. Neptune provides several
    boilerplate code options based on the framework you want to use, including (but
    not limited to) Optuna, PyTorch, Keras, TensorFlow, scikit-learn, and XGBoost.
    You can just copy the provided boilerplate code and customize it based on your
    needs. For example, let’s use the provided boilerplate code for Optuna (see *Figure
    13.3*) and save the training script as `train_optuna.py`. Please see the full
    code in this book’s GitHub repository, which was provided in the *Technical requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Creating the hyperparameter tuning experiment script'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – Creating the hyperparameter tuning experiment script
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the hyperparameter tuning script (`python train_optuna.py`) and look at
    the metadata of the experiments on your Neptune project page. Every run will be
    stored as a new experiment ID in Neptune, so you don’t have to worry about the
    experiment versioning since Neptune will handle it automatically for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Neptune’s experiment runs table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Neptune’s experiment runs table
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also see all the metadata for each of the experiment runs, including
    (but not limited to) the tested hyperparameters, source code, CPU/GPU usage, metric
    charts, artifacts (data, model, or any other related files), and figures (for
    example, confusion matrices), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Metadata stored in Neptune'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – Metadata stored in Neptune
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyze the experiment results. Neptune can not only help you log all of the
    metadata for each experiment run, but it can also compare several different runs
    using several types of comparison strategies. You can see the hyperparameter values
    comparison via parallel plot or line charts. You can also compare all of the experiment
    details via a **Side-by-side** comparison strategy (see *Figure 13.6*). Furthermore,
    Neptune also enables us to compare the logged images or artifacts between each
    run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Comparing the experiment runs and their results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 – Comparing the experiment runs and their results
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information regarding what you can log and display in Neptune, please
    refer to the official documentation page: https://docs.neptune.ai/you-should-know/what-can-you-log-and-display.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrations in Neptune
  prefs: []
  type: TYPE_NORMAL
- en: 'Neptune provides numerous integrations for ML-related experiments in general
    and also for specific hyperparameter tuning-related tasks. Three integrations
    are supported by Neptune for hyperparameter tuning tasks: Optuna, Keras, and Scikit-Optimize.
    For more information, please refer to the official documentation page: https://docs.neptune.ai/integrations-and-supported-tools/intro.'
  prefs: []
  type: TYPE_NORMAL
- en: More examples
  prefs: []
  type: TYPE_NORMAL
- en: 'Neptune is a very powerful package that can be utilized for other ML experiment-related
    tasks, too. For more examples of how to utilize Neptune in general, please refer
    to the official documentation page: https://docs.neptune.ai/getting-started/examples.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you were introduced to Neptune and how to utilize it to help
    you track your hyperparameter tuning experiments. In the next section, you will
    learn how to utilize the famous Scikit-Optimize package for hyperparameter tuning
    experiment tracking purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring scikit-optimize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You were introduced to the **Scikit-Optimize** package in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062),
    *Hyperparameter Tuning via Scikit*, to conduct a hyperparameter tuning experiment.
    In this section, we will learn how to utilize this package to track all hyperparameter
    tuning experiments conducted using this package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Optimize provides very nice visualization plots that summarize the tested
    hyperparameter values, the objective function scores, and the relationship between
    them. Three plots are available in this package, as shown here. Please see the
    full code in this book’s GitHub repository for more details. The following plots
    were generated based on the same experimental setup that was provided in [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062), *Hyperparameter Tuning via Scikit*,
    for the BOGP hyperparameter tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_convergence`: This is used to visualize the hyperparameter tuning optimization
    progress for each iteration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Convergence plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.7 – Convergence plot
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_evaluations`: This is used to visualize the optimization evolution process
    history. In other words, it shows the order in which hyperparameter values were
    sampled during the optimization process. For each hyperparameter, a histogram
    of explored hyperparameter values is generated. For each pair of hyperparameters,
    the scatter plot of tested hyperparameter values is visualized and equipped with
    colors to act as the legend of the evolution history (from blue to yellow):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Evaluation plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.8 – Evaluation plot
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_objective`: This is used to visualize the pairwise dependence plot of
    the objective function. This visualization helps us gain information regarding
    the relationship between the tested hyperparameter values and the objective function
    scores. From this plot, you can see which subspace needs more attention and which
    subspace, or even which hyperparameter, needs to be removed from the original
    space in the next trial:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Pairwise dependence plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.9 – Pairwise dependence plot
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Neptune
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Optimize provides very informative visualization modules. However, it
    does not support any experiment versioning capabilities, unlike the Neptune package.
    To get the best of both worlds, we can integrate Scikit-Optimize with Neptune
    via its integration module. For more information about this, please refer to the
    official documentation page: https://docs-legacy.neptune.ai/integrations/skopt.html.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to utilize the Scikit-Optimize package to help
    you track your hyperparameter tuning experiments. In the next section, you will
    learn how to utilize the Optuna package for hyperparameter tuning experiment tracking
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Optuna
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Optuna** is a hyperparameter tuning package in Python that provides several
    hyperparameter tuning methods. We discussed how to utilize Optuna to conduct a
    hyperparameter tuning experiment in [*Chapter 9*](B18753_09_ePub.xhtml#_idTextAnchor082),
    *Hyperparameter Tuning via Optuna*. Here, we will discuss how to utilize this
    package to track those experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Scikit-Optimize, Optuna provides very nice visualization modules
    to help us track the hyperparameter tuning experiments and as a guide for us to
    decide which subspace to search in the next trial. Four visualization modules
    can be utilized, as shown here. All of them expect the `study` object (see *Chapter
    9*, *Hyperparameter Tuning via Optuna*) as input. Please see the full code in
    this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_contour`: This is used to visualize the relationship between hyperparameters
    (as well as the objective function scores) in the form of contour plots:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Contour plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.10 – Contour plot
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_optimization_history`: This is used to visualize the hyperparameter tuning
    optimization progress for each iteration:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Optimization history plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.11 – Optimization history plot
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_parallel_coordinate`: This is used to visualize the relationship between
    hyperparameters (as well as the objective function scores) in the form of a parallel
    coordinate plot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Parallel coordinate plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.12 – Parallel coordinate plot
  prefs: []
  type: TYPE_NORMAL
- en: '`plot_slice`: This is used to visualize the hyperparameter tuning method’s
    search evolution. You can see what hyperparameter values have been tested in the
    experiment and which subspace is getting more attention during the search process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Slice plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.13 – Slice plot
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about all the visualization modules in Optuna is that they are
    all interactive charts since they are created using the `Plotly` visualization
    package. You can zoom in on a specific area in the charts and use other interactive
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Neptune
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Scikit-Optimize, Optuna provides very informative visualization
    modules. However, it does not support any experiment versioning capability, unlike
    the Neptune package. We can integrate Optuna with Neptune via its integration
    module. For more information about this, please refer to the official documentation
    page: https://docs-legacy.neptune.ai/integrations/optuna.html.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to utilize the Optuna package to track your
    hyperparameter tuning experiments. In the next section, you will learn how to
    utilize the Microsoft NNI package for hyperparameter tuning experiment tracking
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Microsoft NNI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural Network Intelligence** (**NNI**) is a package that is developed by
    Microsoft and can be utilized not only for hyperparameter tuning tasks but also
    for neural architecture search, model compression, and feature engineering. We
    discussed how to utilize NNI to conduct hyperparameter tuning experiments in [*Chapter
    10*](B18753_10_ePub.xhtml#_idTextAnchor092), *Advanced Hyperparameter Tuning with
    DEAP and Microsoft NNI*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss how to utilize this package to track those
    experiments. All of the experiment tracking modules provided by NNI are located
    in the *web portal*. You learned about the web portal in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092),
    *Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*. However, we haven’t
    discussed it in depth and there are many useful features you should know about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web portal can be utilized to visualize all of the hyperparameter tuning
    experiment’s metadata, including (but not limited to) tuning and training progress,
    evaluation metrics, and error logs. It can also be utilized to update the experiment’s
    concurrency and duration, and retry the failed trials. The following is a list
    of all the important modules in the NNI web portal that can be utilized to help
    us track our hyperparameter tuning experiments. The following plots have been
    generated based on the same experimental setup that was stated in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092),
    *Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*, for the Random Search
    method. Please see the full code in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Overview** page shows an overview of our hyperparameter tuning experiment,
    including its name and ID, status, start and end time, best metric, elapsed duration,
    number of trials faceted by the status, as well as the experiment path, training
    platform, and tuner details. Here, you can also change the maximum duration, the
    maximum number of trials, and the experiment’s concurrency. There is also a dedicated
    module that shows the top-performing trials:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.14 – The Overview page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.14 – The Overview page
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Trials detail** page shows every detail regarding the experiment’s trials,
    including a visualization of all the metrics (see *Figure 13.15*), a hyperparameter
    values parallel plot (see *Figure 13.16*), a bar chart of the duration of all
    the trials (see *Figure 13.17*), and a line chart of all intermediate results
    that shows the trend of each trial during the intermediate steps. We can also
    see the details of each trial via the **Trial jobs** module, including (but not
    limited to) the trial’s ID, duration, status, metric, hyperparameter value details,
    and log files (see *Figure 13.18*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.15 – The Trials detail page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.15 – The Trials detail page
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a parallel plot that shows different hyperparameter values
    that had been tested in the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.16 – Hyperparameter values parallel plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.16 – Hyperparameter values parallel plot
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a bar chart containing information about the duration of all
    the trials in the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.17 – Trials duration bar chart'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.17 – Trials duration bar chart
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there’s the **Trial jobs** module:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.18 – The Trial jobs module'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.18 – The Trial jobs module
  prefs: []
  type: TYPE_NORMAL
- en: 'The Trial jobs module includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sidebar**: We can access all the information related to the search space,
    config, and log files in the sidebar:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.19 – Sidebar'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.19 – Sidebar
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Auto refresh** button: We can also change the refresh interval of the
    web portal by using the **Auto refresh** button:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.20 – The Auto refresh button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.20 – The Auto refresh button
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Experiment summary** button: By clicking this button, you can view all
    the summaries for the current experiment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.21 – The Experiment summary button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_13_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.21 – The Experiment summary button
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to utilize the Microsoft NNI package to track
    your hyperparameter tuning experiments. In the next section, you will learn how
    to utilize the MLflow package for hyperparameter-tuning experiment tracking purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MLflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pip install mlflow` command.'
  prefs: []
  type: TYPE_NORMAL
- en: To track our hyperparameter tuning experiments with MLflow, we simply need to
    add several logging functions to our code base. Once we’ve added the required
    logging function, we can go to the provided UI by simply entering the `mlflow
    ui` command in the command line and opening it at [http://localhost:5000](http://localhost:5000).
    Many logging functions are provided by MLflow, and the following are some of the
    main important logging functions you need to be aware of. Please see the full
    example c
  prefs: []
  type: TYPE_NORMAL
- en: 'ode in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create_experiment()`: This function is used to create a new experiment. You
    can specify the name of the experiment, tags, and the path to store the experiment
    artifacts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_experiment()`: This function is used to set the given experiment name
    or ID as the current active experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_run()`: This function is used to start a new MLflow run under the current
    active experiment. It is suggested to use this function as a context manager within
    a `with` block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_metric()`: This function is used to log a single metric within the currently
    active run. If you want to do bulk logging, you can also use the `log_metrics()`
    function by passing a dictionary of metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_param()`: This function is used to log a parameter or hyperparameter within
    the currently active run. If you want to do bulk logging, you can also use the
    `log_params()` function by passing a dictionary of metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_artifact()`: This function is used to log a file or directory as an artifact
    of the currently active run. If you want to log all the contents of a local directory,
    you can also use the `log_artifacts()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_tag()`: This function is used to set a tag for the currently active run.
    You must provide the key and value of the tag. For example, you can set the key
    as `“release_version”` and the value as `“1.0.0”`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_figure()`: This function is used to log a figure as an artifact of the
    currently active run. This function supports the `matplotlib` and `pyplot` figure
    object types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_image()`: This function is used to log an image as an artifact of the
    currently active run. This function supports the `numpy.ndarray` and `PIL.image.image`
    object types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow Logging Functions
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information regarding all the available logging functions in MLfLow,
    please refer to the official documentation page: https://www.mlflow.org/docs/latest/tracking.html#logging-functions.'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Integrations
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow also supports integrations with many well-known open source packages,
    including (but not limited to) scikit-learn, TensorFlow, XGBoost, PyTorch, and
    Spark. You can do automatic logging by utilizing the provided integrations. For
    more information, please refer to the official documentation page: https://www.mlflow.org/docs/latest/tracking.html#automatic-logging.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Hyperparameter Tuning Use Cases
  prefs: []
  type: TYPE_NORMAL
- en: 'The author of MLflow has provided example code for hyperparameter tuning use
    cases. For more information, please refer to the official GitHub repository: https://github.com/mlflow/mlflow/tree/master/examples/hyperparam.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to utilize the MLflow package to track your
    hyperparameter tuning experiments. You can start exploring this package by yourself
    to get a better understanding of how this package works and how powerful it is.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the importance of tracking hyperparameter tuning
    experiments, along with the usual practices. You were also introduced to several
    open source packages that are available and learned how to utilize each of them
    in practice, including Neptune, Scikit-Optimize, Optuna, Microsoft NNI, and MLflow.
    At this point, you should be able to utilize your favorite package to track your
    hyperparameter tuning experiment, which will boost the effectiveness of your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll conclude all the topics we have discussed throughout
    this book. We’ll also discuss the next steps you can take to expand your hyperparameter
    tuning knowledge.
  prefs: []
  type: TYPE_NORMAL
