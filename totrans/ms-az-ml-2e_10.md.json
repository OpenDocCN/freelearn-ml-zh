["```py\n    from azureml.core import Workspace\n    ws = Workspace.from_config()\n    ```", "```py\n    # Create or get training cluster\n    aml_cluster = get_aml_cluster(\n      ws, cluster_name=\"cpu-cluster\")\n    aml_cluster.wait_for_completion(show_output=True)\n    ```", "```py\n    run_conf = get_run_config(['numpy', 'pandas',\n      'scikit-learn', 'tensorflow'])\n    ```", "```py\n    from azureml.pipeline.steps import PythonScriptStep\n    step = PythonScriptStep(name='Preprocessing',\n                            script_name=\"preprocess.py\",\n                            source_directory=\"code\",\n                            runconfig=run_conf,\n                            compute_target=aml_cluster)\n    ```", "```py\n    from azureml.pipeline.core import Pipeline\n    pipeline = Pipeline(ws, steps=[step])\n    ```", "```py\npipeline.validate()\n```", "```py\n    from azureml.core import Experiment\n    exp = Experiment(ws, \"azureml-pipeline\")\n    run = exp.submit(pipeline)\n    ```", "```py\nfrom azureml.core.dataset import Dataset\ndataset = Dataset.get_by_name(ws, 'titanic')\n```", "```py\npath ='https://...windows.net/demo/Titanic.csv'\ndataset = Dataset.Tabular.from_delimited_files(path)\n```", "```py\nfrom azureml.core.dataset import Dataset\ndataset = Dataset.get_by_name(ws, 'titanic')\ndata_in = dataset.as_named_input('titanic')\n```", "```py\nfrom azureml.pipeline.steps import PythonScriptStep\nstep = PythonScriptStep(name='Preprocessing',\n                        script_name=\"preprocess_input.py\",\n                        source_directory=\"code\",\n                        arguments=[\"--input\", data_in],\n                        inputs=[data_in],\n                        runconfig=run_conf,\n                        compute_target=aml_cluster)\n```", "```py\nimport argparse\nfrom azureml.core import Run, Dataset\nrun = Run.get_context()\nws = run.experiment.workspace\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input\", type=str)\nargs = parser.parse_args()\ndataset = Dataset.get_by_id(ws, id=args.input)\ndf = dataset.to_pandas_dataframe()\n```", "```py\nfrom azureml.core import Datastore\nfrom azureml.pipeline.core import PipelineData\ndatastore = Datastore.get(ws, datastore_name=\"mldata\")\ndata_train = PipelineData('train', datastore=datastore)\ndata_test = PipelineData('test', datastore=datastore)\n```", "```py\nfrom azureml.pipeline.steps import PythonScriptStep\nstep_1 = PythonScriptStep(name='Preprocessing',\n                          script_name= \\\n                            \"preprocess_output.py\",\n                          source_directory=\"code\",\n                          arguments=[\n                              \"--input\", data_in,\n                              \"--out-train\", data_train,\n                              \"--out-test\", data_test],\n                          inputs=[data_in],\n                          outputs=[data_train, data_test],\n                          runconfig=run_conf,\n                          compute_target=aml_cluster)\n```", "```py\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input\", type=str)\nparser.add_argument(\"--out-train\", type=str)\nparser.add_argument(\"--out-test\", type=str)\nargs = parser.parse_args()\n```", "```py\nimport os\nout_train = args.out_train\nos.makedirs(os.path.dirname(out_train), exist_ok=True)\nout_test = args.out_test\nos.makedirs(os.path.dirname(out_test), exist_ok=True)\ndf_train, df_test = preprocess(...)\ndf_train.to_csv(out_train)\ndf_test.to_csv(out_test)\n```", "```py\nfrom azureml.pipeline.steps import PythonScriptStep\nstep_2 = PythonScriptStep(name='Training',\n                          script_name=\"train.py\",\n                          source_directory=\"code\",\n                          arguments=[\n                              \"--in-train\", data_train,\n                              \"--in-test\", data_test],\n                          inputs=[data_train, data_test],\n                          runconfig=run_conf,\n                          compute_target=aml_cluster)\n```", "```py\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--in-train\", type=str)\nparser.add_argument(\"--in-test\", type=str)\nargs = parser.parse_args()\n...\ndf_train = pd.read_csv(args.in_train)\ndf_test = pd.read_csv(args.in_test)\n```", "```py\nfrom azureml.pipeline.core import Pipeline\npipeline = Pipeline(ws, steps=[step_1, step_2])\n```", "```py\nfrom azureml.data import OutputFileDatasetConfig\ndata_out = OutputFileDatasetConfig(name=\"predictions\", \n  destination=(datastore, 'titanic/predictions')) \n```", "```py\nfrom azureml.pipeline.core.graph import PipelineParameter\nlr_param = PipelineParameter(name=\"lr_arg\",\n                             default_value=0.01)\n```", "```py\ndata = mnist_dataset.as_named_input('mnist').as_mount()\nargs = [\"--in-train\", data, \"--learning-rate\", lr_param]\nstep = PythonScriptStep(name='Training',\n\tscript_name=\"train.py\",\n\tsource_directory=\"code\",\n\targuments=args,\n\tinputs=[data_train],\n\trunconfig=run_conf,\n\tcompute_target=aml_cluster)\n                     arguments=args ,\n                     estimator=estimator,\n                     compute_target=cpu_cluster)\n```", "```py\nparser = argparse.ArgumentParser()\nparser.add_argument('--learning-rate', type=float, \n  dest='lr')\nargs = parser.parse_args()\n# print learning rate \nprint(args.lr)\n```", "```py\npipeline = Pipeline(ws, steps=[step])\nservice = pipeline.publish(name=\"CNN_Train_Service\",\n                           version=\"1.0\")\nservice_id = service.id\nservice_endpoint = service.endpoint\n```", "```py\nfrom azureml.pipeline.core import PipelineEndpoint\napplication = PipelineEndpoint.publish(ws,\n  pipeline=service,\n  name=\"CNN_Train_Endpoint\")\nservice_id = application.id\nservice_endpoint = application.endpoint\n```", "```py\nfrom azureml.core.authentication import AzureCliAuthentication\ncli_auth = AzureCliAuthentication()\naad_token = cli_auth.get_authentication_header()\n```", "```py\nimport requests\nresponse = requests.post(service_endpoint,\n  headers=aad_token,\n  json={\"ExperimentName\": \"mnist-train\",\n        \"ParameterAssignments\": {\"lr_arg\": 0.05}})\n```", "```py\nfrom azureml.pipeline.core import PublishedPipeline\nfor pipeline in PublishedPipeline.list(ws):\n  print(\"name: %s, id: %s\" % (pipeline.name, pipeline.id))\n```", "```py\nfrom azureml.pipeline.core.schedule import \\\n  ScheduleRecurrence, Schedule\nrecurrence = ScheduleRecurrence(frequency=\"Minute\", \n                                interval=15)\nschedule = Schedule.create(ws, \n                           name=\"CNN_Train_Schedule\", \n                           pipeline_id=pipeline_id,\n                           experiment_name=\"mnist-train\", \n                           recurrence=recurrence, \n                           pipeline_parameters={})\n```", "```py\nfrom azureml.core.datastore import Datastore\n# use default datastore 'ws.get_default_datastore()'\n# or load a custom registered datastore\ndatastore = Datastore.get(workspace, 'mldemodatastore')\n# 5 min polling interval\npolling_interval = 5\nschedule = Schedule.create(\n    ws, name=\"CNN_Train_OnChange\", \n    pipeline_id=pipeline_id,\n    experiment_name=\"mnist-train\",\n    datastore=datastore,\n    data_path_parameter_name=\"mnist\"\n    polling_interval=polling_interval,\n    pipeline_parameters={})\n```", "```py\nfor schedule in Schedule.list(ws):\n  print(schedule.id)\n```", "```py\nschedule.disable(wait_for_provisioning=True)\n```", "```py\npipeline = Pipeline(ws, steps=[step1, step2, step3, step4])\n```", "```py\nstep3.run_after(step2)\nstep4.run_after(step3)\n```", "```py\nfrom azureml.pipeline.core import PipelineParameter\nfrom azureml.pipeline.steps import ParallelRunConfig\nparallel_run_config = ParallelRunConfig(\n  entry_script='score.py',\n  mini_batch_size=PipelineParameter(\n    name=\"batch_size\", \n    default_value=\"10\"),\n  output_action=\"append_row\",\n  append_row_file_name=\"parallel_run_step.txt\",\n  environment=batch_env,\n  compute_target=cpu_cluster,\n  process_count_per_node=2,\n  node_count=2)\n```", "```py\nfrom azureml.pipeline.steps import ParallelRunStep\nfrom azureml.core.dataset import Dataset\nparallelrun_step = ParallelRunStep(\n  name=\"ScoreParallel\",\n  parallel_run_config=parallel_run_config,\n  inputs=[Dataset.get_by_name(ws, 'mnist')],\n  output=PipelineData('mnist_results', \n                      datastore=datastore),\n  allow_reuse=True)\n```", "```py\nfrom azureml.pipeline.core import Pipeline\npipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\nrun = exp.submit(pipeline)\n```", "```py\nfrom azureml.pipeline.core.module import Module\nmodule = Module.create(ws,\n                       name=\"TopItemRecommender\",\n                       description=\"Recommend top 5 items\")\n```", "```py\nfrom azureml.pipeline.core.graph import \\\n  InputPortDef, OutputPortDef\nin1 = InputPortDef(name=\"in1\",\n                   default_datastore_mode=\"mount\", \n                   default_data_reference_name = \\\n                       datastore.name,\n                   label=\"Ratings\")\nout1 = OutputPortDef(name=\"out1\",\n                     default_datastore_mode=\"mount\", \n                     default_datastore_name=datastore.name,\n                     label=\"Recommendation\")\n```", "```py\nmodule.publish_python_script(\"train.py\",\n                             source_directory=\"./rec\",\n                             params={\"numTraits\": 5},\n                             inputs=[in1],\n                             outputs=[out1],\n                             version=\"1\",\n                             is_default=True)\n```", "```py\nfrom azureml.pipeline.core.module import Module\nmodule = Module.get(ws, name=\"TopItemRecommender\")\n```", "```py\nfrom azureml.pipeline.core import PipelineData\nin1 = PipelineData(\"in1\",\n                   datastore=datastore, \n                   output_mode=\"mount\", \n                   is_directory=False)\nout1 = PipelineData(\"out1\",\n                    datastore=datastore, \n                    output_mode=\"mount\", \n                    is_directory=False)\ninput_wiring = {\"in1\": in1}\noutput_wiring = {\"out1\": out1}\n```", "```py\nfrom azureml.pipeline.core import PipelineParameter\nnum_traits = PipelineParameter(name=\"numTraits\",\n                               default_value=5)\n```", "```py\nfrom azureml.core import RunConfiguration\nfrom azureml.pipeline.steps import ModuleStep\nstep = ModuleStep(module= module,\n                  version=\"1\",\n                  runconfig=RunConfiguration(),\n                  compute_target=aml_compute,\n                  inputs_map=input_wiring,\n                  outputs_map=output_wiring,\n                  arguments=[\n                    \"--output_sum\", first_sum,\n                    \"--output_product\", first_prod,\n                    \"--num-traits\", num_traits])\n```", "```py\nfrom azureml.core import Experiment\nfrom azureml.pipeline.core import Pipeline\npipeline = Pipeline(ws, steps=[step])\nexp = Experiment(ws, \"item-recommendation\")\nrun = exp.submit(pipeline)\n```", "```py\n{\n    \"name\": \"Machine Learning Execute Pipeline\",\n    \"type\": \"AzureMLExecutePipeline\",\n    \"linkedServiceName\": {\n        \"referenceName\": \"AzureMLService\",\n        \"type\": \"LinkedServiceReference\"\n    },\n    \"typeProperties\": {\n        \"mlPipelineId\": \"<insert pipeline id>\",\n        \"experimentName\": \"data-factory-pipeline\",\n        \"mlPipelineParameters\": {\n            \"batch_size\": \"10\"\n        }\n    }\n}\n```", "```py\ndef get_pipeline(workspace, pipeline_id):\n  for pipeline in PublishedPipeline.list(workspace):\n    if pipeline.id == pipeline_id:\n      return pipeline\n  return None\n```", "```py\nws = Workspace.get(\n  name=os.environ.get(\"WORKSPACE_NAME\"),\n  subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n  resource_group=os.environ.get(\"RESOURCE_GROUP\"))\npipeline = get_pipeline(args.pipeline_id)\npipeline_parameters = args.pipeline_parameters\nexp = Experiment(ws, name=args.experiment_name)\nrun = exp.submit(pipeline,\n                 pipeline_parameters=pipeline_parameters)\nprint(\"Pipeline run initiated %s\" % run.id)\n```"]