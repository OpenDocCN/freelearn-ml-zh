<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Facial and Motion Detection – Imaging Filters</h1>
                </header>
            
            <article>
                
<p>You have seen and heard of it everywhere. Facial recognition, motion detection. We have motion sensors in our homes as part of our security systems. Everybody is doing facial recognition—there are security cameras on our streets, in our airports, and probably even in our homes. And if we think about everything that an autonomous vehicle must do, wow! There is a recent link (at the time of writing) on how facial recognition technology identified a suspect amongst a crowd of 50,000 faces! <a href="https://www.digitaltrends.com/cool-tech/facial-recognition-china-50000/" target="_blank">https://www.digitaltrends.com/cool-tech/facial-recognition-china-50000/</a></p>
<p>But what exactly does that mean? How does it do what it does? What happens behind the scenes? And how can I use it in my applications? In this chapter, we are going to show two separate examples, one for facial detection and the other for motion detection. We'll show you exactly what goes on and just how fast you can add these capabilities into your application.</p>
<p>In this chapter we will cover:</p>
<ul>
<li>Facial detection</li>
<li>Motion detection
<ul>
<li>Adding detection to your application</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facial detection</h1>
                </header>
            
            <article>
                
<p>Let's start out with facial detection. In our example, I'm going to use my friendly little French Bulldog, Frenchie, as our assistant. Tried to get my beautiful wife to do the honors but makeup, hair; well, I'm sure you know that story! Frenchie the Bulldog, however, had no complaints.</p>
<p>Before I start, please re-read the chapter title. No matter how many times you read it, you'll probably miss the key point here, and it's so very important. Notice it says <strong>facial detection</strong> and not <strong>facial recognition</strong>. This is so very important that I needed to stop and re-stress it. We are not trying to identify Joe, Bob, or Sally. We are trying to identify that, out of everything we see via our camera, we can detect a face. We are not concerned with whose face it is, just the fact that it is a face! It is so important that we understand this before moving on! Otherwise, your expectations will be so incorrectly biased (another buzzword for your checklist) that you'll make yourself confused and upset, and we don't want that!</p>
<p>Facial detection, as I will stress again later, is the first part of facial recognition, a much more complicated beast. If you can't identify <span>that there are one or more faces </span>out of all the things on the screen, then you'll never be able to recognize whose face <span>that </span>is!</p>
<p>Let's start things off by taking a quick look at our application:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/76680c1c-486b-4f1a-b1c6-dff6bb74f804.png" style=""/></div>
<p>As you can see, we have a very simple screen. In our case, the laptop camera is our video capture device. Frenchie is kindly posing in front of the camera for us, just standing there enjoying life. But, as soon as we enable facial tracking, watch what happens:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/40008629-0a3a-459a-bd61-60d66bd3a4d9.png" style=""/></div>
<p>The facial features of Frenchie are now being tracked. What you see surrounding Frenchie are the tracking containers (white boxes), which tell us we know that there is a face and where it is, and our angle detector (red line), which provides some insight into the horizontal aspect of our face .</p>
<p>As we move Frenchie around, the tracking container and angle detector will track him. That's all well and good, but what happens if we enable facial tracking on a real human face? As you see here, the tracking containers and angles are tracking the facial figures of our guest poser, just like they did for Frenchie:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/857c656a-62f6-4f80-a86b-5eead320ed31.png" style=""/></div>
<p>As our poser moves his head from side to side, the camera tracks this and you can see the angle detectors adjusting to what it recognizes as the horizontal angle of the face. In this case, you will notice that the <span class="packt_screen">Color space</span> is in black and white and not color. This is a histogram back projection and is an option that you can change:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/88668c56-7e07-4141-b90e-cde7763e75a9.png" style=""/></div>
<p>Even as we move farther away from the camera, where other objects come into view, the facial detector can keep track of our face among the noise, as shown in the following screenshot. This is exactly how the facial recognition systems you see in movies work, albeit more advanced; and, using the code and samples we'll show you shortly, you too can be up and running with your own facial recognition application in minutes! We'll provide the detection; you provide the recognition:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c941ebd9-8f3e-42d6-9a83-bdf0ae6cbd84.png" style=""/></div>
<p>Now that we've seen how our application appears from the outside, let's look under the hood at what is going on.</p>
<p>Let's start by asking ourselves exactly what the problem is that we are trying to solve here. As we mentioned in previous sections, we are trying to detect (notice again I did not say recognize) facial images. While this is easy for a human, a computer needs very detailed instruction sets to accomplish this feat.</p>
<p>Luckily, there is a very famous algorithm called the Viola-Jones algorithm that will do the heavy lifting for us. Why did we pick this algorithm?</p>
<ol>
<li>It has very high detection rates and very low false positives.</li>
<li>It is very good at real-time processing.</li>
<li>It is very good at detecting faces from non-faces. Detecting faces is the first step in facial recognition!</li>
</ol>
<p>This algorithm requires that the camera has a full frontal, upright view of the face. To be detected, the face will need to point straight towards the camera, not tilted and not looking up or down. Remember again; for the moment, we are just interested in facial detection!</p>
<p>To delve into the technical side of things, our algorithm will require four stages to accomplish its job. They are:</p>
<ul>
<li>Haar feature selection</li>
<li>Creating an integral image</li>
<li>AdaBoost training</li>
<li>Cascading classifiers</li>
</ul>
<p>Let's think about what facial detection actually accomplished. All faces, be it human, animal, or otherwise, share some similar properties. For example, the eye is darker than the upper cheeks, the nose bridge is brighter than the eyes, and your forehead may be lighter than the rest of your face. Our algorithm matches these intuitions up by using what is known as <strong>Haar features</strong>. We can come up with matchable facial features by looking at the location and size of the eyes, mouth, bridge of the nose, and so forth. However, we do have an obstacle.</p>
<p>In a 24x24 pixel window, there are a total of 162,336 possible features. Obviously, to try and evaluate them all would be prohibitively expensive in both time and computation, if it works at all. So, we are going to work with a technique known as <strong>adaptive boosting</strong>, or more commonly, <strong>AdaBoost</strong>. It's another one for your buzzword-compliant list. If you have delved into or researched machine learning, I'm sure you've heard about a technique called <strong>boosting.</strong> That's exactly what AdaBoost is. Our learning algorithm will use AdaBoost to select the best features and train classifiers to use them.</p>
<p>AdaBoost can be used with many types of learning algorithms and is considered the best out-of-the-box algorithm for many tasks where boosting is required. You usually won't notice how good and fast it is until you switch to a different algorithm and benchmark it. I have done this countless number of times, and I can tell you that the difference is very noticeable.</p>
<p>Let's give a little more definition to boosting before we continue.</p>
<p><strong>Boosting</strong> takes the output from other <strong>weak-learning</strong> algorithms and combines them with a weighted sum that is the final output of the boosted classifier. The adaptive part of AdaBoost comes from the fact that subsequent learners are tweaked in favor of those instances that have been incorrectly classified by previous classifiers. We must be careful with our data preparation though, as AdaBoost is sensitive to noisy data and outliers (remember how we stressed those in <a href="7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml" target="_blank">Chapter 1</a>, <em>Machine Learning Basics</em>). The algorithm tends to <strong>overfit</strong> the data more than other algorithms, which is why, in our earlier chapters, we stressed on data preparation for missing data and outliers. In the end, if weak learning algorithms are better than random guessing, AdaBoost can be a valuable addition to our process.</p>
<p>With that brief description behind us, let's look under the covers at what's happening. For this example, we will again use the <strong>Accord framework</strong> and we will work with the vision face tracking sample.</p>
<p>We start by creating a <kbd>FaceHaarCascade</kbd> object. This object holds a collection of Haar-like features' weak classification stages. There will be many stages provided, each containing a set of classifier trees which will be used in the decision-making process. We are now technically working with a decision tree. The beauty of the Accord framework is that  <kbd>FaceHaarCascade</kbd> automatically creates all these stages and trees for us without exposing us to the details.</p>
<p>Let's see what a particular stage might look like:</p>
<pre><strong><br/> </strong>List&lt;HaarCascadeStage&gt; stages = new List&lt;HaarCascadeStage&gt;();<br/> List&lt;HaarFeatureNode[]&gt; nodes;<br/> HaarCascadeStage stage;<br/> stage = new HaarCascadeStage(0.822689414024353);<br/> nodes = new List&lt;HaarFeatureNode[]&gt;();<br/> nodes.Add(new[] { new HaarFeatureNode(0.004014195874333382, <br/>   0.0337941907346249, 0.8378106951713562, <br/>   new int[] { 3, 7, 14, 4, -1 },<br/>   new int[] { 3, 9, 14, 2, 2 }) });<br/> nodes.Add(new[] { new HaarFeatureNode(0.0151513395830989,<br/>   0.1514132022857666, 0.7488812208175659, <br/>   new int[] { 1, 2, 18, 4, -1 }, <br/>   new int[] { 7, 2, 6, 4, 3 }) });<br/> nodes.Add(new[] { new HaarFeatureNode(0.004210993181914091,<br/>   0.0900492817163467, 0.6374819874763489, <br/>   new int[] { 1, 7, 15, 9, -1 }, <br/>   new int[] { 1, 10, 15, 3, 3 })<br/>  });<br/> stage.Trees = nodes.ToArray(); stages.Add(stage);</pre>
<p>Now don't let that scare you off. As you can see, we are building a decision tree underneath the hood by providing the nodes for each stage with the numeric values for each feature.</p>
<p>Once created, we can use our cascade object to create our <kbd>HaarObjectDetector</kbd>, which is what we will use for our detection. It takes:</p>
<ol>
<li>Our facial cascade objects</li>
<li>The minimum window size to use when searching for objects</li>
<li>Our search mode, given that we are searching for only a single object</li>
<li>The re-scaling factor to use when re-scaling our search window during the search</li>
</ol>
<pre>HaarCascade cascade = new FaceHaarCascade();<br/>detector = new HaarObjectDetector(cascade, 25, <br/>  ObjectDetectorSearchMode.<strong>Single</strong>, 1.2f,<br/>  ObjectDetectorScalingMode.<strong>GreaterToSmaller</strong>);</pre>
<p>Now we are ready to tackle the topic of our video collection source. In our examples, we will simply use the local camera to capture all images. However, the Accord.NET framework makes it easy to use other sources for image capture, such as a <kbd>.avi</kbd> files, animated <kbd>.jpg</kbd> files, and so forth.</p>
<p>We connect to the camera, select the resolution, and are then ready to go:</p>
<pre>VideoCaptureDevice videoSource = new <br/>  VideoCaptureDevice(form.VideoDevice);<br/>foreach (var cap in device.VideoCapabilities)<br/>{<br/><br/>  if (cap.FrameSize.Height == 240)<br/>  return cap;<br/>  if (cap.FrameSize.Width == 320)<br/>  return cap;<br/>}<br/>return device.VideoCapabilities.Last();</pre>
<p>With the application now running and our video source selected, our application will look like this. Once again, enter Frenchie the Bulldog! Please excuse the mess; Frenchie is not the tidiest of assistants and he even left his empty cup of coffee on my table!</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d88366e3-4ffc-46f6-8270-91654ffd5c7d.png" style=""/></div>
<p>For this demonstration, you will notice that Frenchie is facing the camera, and in the background, we have two 55" monitors as well as many other items my wife likes to refer to as junk. I myself prefer to think of it as stochastic noise! This is done to show how the face detection algorithm can distinguish Frenchie's face amongst everything else. If our detector cannot handle this, it is going to get lost in the noise and be of little use to us.</p>
<p>With our video source now coming in, we need to be notified when a new frame is received so that we can process it, apply our markers, and so on. We do this by attaching to the <kbd>NewFrameReceived</kbd> event handler of the video source player. As a C# developer I am assuming that you are familiar with events such as this:</p>
<pre>this.videoSourcePlayer.NewFrameReceived += new<br/>  Accord.Video.NewFrameEventHandler<br/>  (this.videoSourcePlayer_NewFrame);</pre>
<p>Now that we have a video source and video coming in, let's look at what happens each time we are notified that a new video frame is available.</p>
<p>The first thing that we need to do is <kbd>downsample</kbd> the image to make it easier to work with:</p>
<pre>ResizeNearestNeighbor resize = new ResizeNearestNeighbor(160, 120);<br/>UnmanagedImage downsample = resize.Apply(im);</pre>
<p>With the image in a more manageable size, we will process the frame. If we have not found a facial region, we will stay in tracking mode waiting for a frame that has a detectable face. Once we have found a facial region, we will reset our tracker, locate the face, reduce its size in order to flush away any background noise, initialize the tracker, and apply the marker window to the image. All of this is accomplished with the following code:</p>
<pre>Rectangle[] regions = detector?.ProcessFrame(downsample);<br/>if (regions != null &amp;&amp; regions.Length &gt; 0)<br/>{<br/>  tracker?.Reset();<br/>  // Will track the first face found<br/>  Rectangle face = regions[0];<br/>  // Reduce the face size to avoid tracking background<br/>  Rectangle window = new Rectangle(<br/>    (int)((regions[0].X + regions[0].Width / 2f) * xscale),<br/>    (int)((regions[0].Y + regions[0].Height / 2f) * <br/>    yscale), 1, 1);<br/>  window.Inflate((int)(0.2f * regions[0].Width * xscale),<br/>    (int)(0.4f * regions[0].Height * yscale));<br/>  // Initialize tracker<br/>  if (tracker != null)<br/>  {<br/>    tracker.SearchWindow = window;<br/>    tracker.ProcessFrame(im);<br/>  }<br/>marker = new RectanglesMarker(window);<br/>marker.ApplyInPlace(im);<br/>args.Frame = im.ToManagedImage();<br/>tracking = true;<br/>}<br/>  else<br/>  {<br/>    detecting = true;<br/>  }</pre>
<p>Once a face is detected, our image frame looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0b15633-e9ba-4100-8815-3634799816a6.png" style=""/></div>
<p>If Frenchie tilts his head to the side, our image now looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f1e9e136-2b6e-4d68-bb99-3722d5328748.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motion detection</h1>
                </header>
            
            <article>
                
<p>You can already see that we are not only doing facial detection, but motion detection as well. So let's turn our focus to be a bit wider scale and detect any motion, not just faces. Again, we'll use Accord.NET for this and use the motion detection sample. As with facial detection, you will see just how simple it is to add this capability to your applications and instantly become a hero at work!</p>
<p>With motion detection, <span>we will highlight </span>anything that moves on the screen in red. The amount of movement is indicated by the thickness of red in any one area. So, using the <span>following </span>image, you can see that the fingers are moving but that everything else is motionless:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3e8ac5b7-d62c-4b78-b197-dc58104d1f08.png" style=""/></div>
<p>As the movement of the hand increases, you can see increased movement in the entire hand:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fa75855d-ffc6-4101-90df-0c00238cdaf5.png" style=""/></div>
<p>Once the entire hand starts to move, you can <span>see </span>not only more red but also the total amount of red increasing relative to the movement:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6b01b4ad-3a56-45f1-a8fa-0787d465a1ed.png" style=""/></div>
<p>If we do not wish to process the entire screen area for motion, we can define <span class="packt_screen">Motion Regions</span>; motion detection will occur only in those regions. In the following image, you can see that I've defined a <span class="packt_screen">Motion Regions</span>. You will notice in upcoming images that this is the only area that motion will be processed from:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/35721f38-ec53-4718-9b7b-aef3520b9aa9.png" style=""/></div>
<p>Now, if we create some motion for the camera (fingers moving), we will see that only motion from our defined region is being processed:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b428b005-8bf3-4f60-94e7-eb929444dd42.png" style=""/></div>
<p>You can also see that, with a <span class="packt_screen">Motion Regions</span> defined and Peter the meditating Gnome in front of the region, we are still able to detect motion behind him while filtering out non-interesting items; but his face is not a part of the recognition. You could, of course, combine both processes to have the best of both worlds:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ce20e943-3941-4e92-9a21-906edc8af75b.png" style=""/></div>
<p>Another option that we can use is <span class="packt_screen">grid motion highlighting</span>. This makes the motion detected region highlighted in red squares based on a defined grid. Basically, the motion area is now a red box, as you can see here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5a5657c4-837e-423e-ad74-62f4877516fa.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding detection to your application</h1>
                </header>
            
            <article>
                
<p>Here's a simple example of all you need to do to add video recognition to your application. As you can see, it couldn't be any easier!</p>
<pre>// create motion detector<br/>MotionDetector detector = new MotionDetector(<br/>  new SimpleBackgroundModelingDetector(),<br/>  new MotionAreaHighlighting());<br/>// continuously feed video frames to motion detector<br/>while ()<br/>{<br/>  // process new video frame and check motion level<br/>  if (detector.ProcessFrame(videoFrame) &gt; 0.02)<br/>  {<br/>   // ring alarm or do somethng else<br/>  }<br/>}<br/>Opening our video source<br/>videoSourcePlayer.VideoSource = new AsyncVideoSource(source);</pre>
<p>When we receive a new video frame, that's when all the magic happens. Here's all the code it takes to make processing a new video frame a success:</p>
<pre>private void videoSourcePlayer_NewFrame(object sender,  <br/>  NewFrameEventArgs args)<br/>{<br/>  lock (this)<br/>{<br/>  if (detector != null)<br/>{<br/>  float motionLevel = detector.ProcessFrame(args.Frame);<br/>  if (motionLevel &gt; motionAlarmLevel)<br/>  {<br/>    // flash for 2 seconds<br/>    flash = (int)(2 * (1000 / alarmTimer.Interval));<br/>  }<br/>// check objects' count<br/>if (detector.MotionProcessingAlgorithm is BlobCountingObjectsProcessing)<br/>{<br/>  BlobCountingObjectsProcessing countingDetector = <br/>    (BlobCountingObjectsProcessing)<br/>    detector.MotionProcessingAlgorithm;<br/>  detectedObjectsCount = countingDetector.ObjectsCount;<br/>}<br/>else<br/>{<br/>detectedObjectsCount = -1;<br/>}<br/>// accumulate history<br/>motionHistory.Add(motionLevel);<br/>if (motionHistory.Count &gt; 300)<br/>{<br/>  motionHistory.RemoveAt(0);<br/>}<br/>if (showMotionHistoryToolStripMenuItem.Checked)<br/>  DrawMotionHistory(args.Frame);<br/>}<br/>}</pre>
<p>The key here is detecting the amount of motion that is happening in the frame, which is done with the following code. For this example, we are using a motion alarm level of  <kbd>2</kbd>, but you can use whatever you like. Once this threshold has been passed, you can implement your desired logic, such as sending an alert email, a text, and starting video capture, and so forth:</p>
<pre>float motionLevel = detector.ProcessFrame(args.Frame);<br/>if (motionLevel &gt; motionAlarmLevel)<br/>{<br/>// flash for 2 seconds<br/>flash = (int)(2 * (1000 / alarmTimer.Interval));<br/>}<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about facial and motion detection. We discovered various algorithms you can employ in your current environment to easily integrate this functionality into your applications. We also showed some easy and simple code you can use to quickly add such capabilities. In the next chapter, we are going to step into the world of artificial neural networks and tackle some very exciting problems!</p>


            </article>

            
        </section>
    </body></html>