["```py\ndfx = pd.read_csv(\"../../datasets/single_cell_rna/nestorowa_corrected_log2_transformed_counts.txt\", sep=' ',  )\n```", "```py\ndfx['annotation'] = dfx.index.str[:4]\ny = dfx[\"annotation\"].values.ravel()\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(dfx.drop(columns = [\"annotation\"]))\n```", "```py\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=15, svd_solver='full')\npca.fit(X_scaled)\ndata_pca = pca.fit_transform(X_scaled)\n```", "```py\npca = PCA(n_components=900, svd_solver='full')\npca.fit(X_scaled)\ndata_pca = pca.fit_transform(X_scaled)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_pca, y, test_size=0.33)\n```", "```py\n    from sklearn.neighbors import KNeighborsClassifier\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbours=5)\n    knn.fit(X_train, y_train)\n    ```", "```py\n    y_pred = knn.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    ```", "```py\n    for i in range(1,10):\n        knn = KNeighborsClassifier(n_neighbors=i)\n        knn.fit(X_train, y_train)\n        y_pred = knn.predict(X_test)\n        print(\"n =\", i, \"acc =\", accuracy_score(y_test, y_pred))\n    ```", "```py\ndf = pd.read_csv(\"../datasets/dataset_enrollment_sd.csv\")\n```", "```py\ndftmp = df[(df[\"enrollment_cat\"] != \"Likely\")]\n```", "```py\n     plt.figure(figsize=(15, 6))\n     xfit = np.linspace(-90, 130)\n         sns.scatterplot(dftmp[\"Feature1\"], \n                         dftmp[\"Feature2\"], \n                         hue=dftmp[\"enrollment_cat\"].values, \n                         s=50)\n         for m, b in [(1, -45),]:\n             plt.plot(xfit, m * xfit + b, '-k')\n         plt.xlim(-120, 150);\n         plt.ylim(-100, 60);\n```", "```py\n         plt.figure(figsize=(15, 6))\n         xfit = np.linspace(-110, 180)\n         sns.scatterplot(dftmp[\"Feature1\"], \n                         dftmp[\"Feature2\"], \n                         hue=dftmp[\"enrollment_cat\"].values, \n                         s=50)\n         for m, b, d in [(1, -45, 60),]:\n      yfit = m * xfit + b\n      plt.plot(xfit, yfit, '-k')\n         plt.fill_between(xfit, yfit - d, \n                     yfit + d, edgecolor='none',\n                              color='#AAAAAA', alpha=0.4)\n         plt.xlim(-120, 150);\n         plt.ylim(-100, 60);\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =  \n                train_test_split(dftmp[[\"Feature1\",\"Feature2\"]], \n                                  dftmp[\"enrollment_cat\"].values,\n                                  test_size = 0.25)\nfrom sklearn.svm import SVC\nmodel = SVC (kernel='linear', C=1E10, random_state = 42)\nmodel.fit(X_train, y_train)\n```", "```py\n     plt.figure(figsize=(15, 6))\n     sns.scatterplot(dftmp[\"Feature1\"], \n                     dftmp[\"Feature2\"], \n                     hue=dftmp[\"enrollment_cat\"].values, s=50)\n     plot_svc_decision_function(model);\n     for j, k in model.support_vectors_:\n         plt.plot([j], [k], lw=0, ='o', color='red', \n                  markeredgewidth=2, markersize=20, \n                  fillstyle='none')\n```", "```py\nfrom sklearn.svm import SVC\nsvc = SVC(kernel=\"linear\")\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_depth=4)\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\npip install xgboost\n```", "```py\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=10000)\n```", "```py\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom google.cloud import bigquery\nimport missingno as msno\nfrom sklearn.metrics import classification_report\nimport ast\nimport autosklearn.classification\n```", "```py\nclient = bigquery.Client(location=\"US\")\nprint(\"Client creating using default project: {}\".format(client.project))\n```", "```py\nquery = \"\"\"\n    SELECT *\n    FROM `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups`\n\"\"\"\nquery_job = client.query(\n    query,\n    location=\"US\",\n)\ndf = query_job.to_dataframe()\nprint(df.shape)\n```", "```py\ndf.classification.nunique()\n```", "```py\ndfx = df.drop_duplicates([\"structureId\"])\n```", "```py\ndfx.classification.value_counts()[:10].sort_values().plot(kind = 'barh')\n```", "```py\ndfx = dfx[[\"classification\", \"residueCount\", \"resolution\", \"resolution\", \"crystallizationTempK\", \"densityMatthews\", \"densityPercentSol\", \"phValue\"]]\nmsno.matrix(dfx)\n```", "```py\ndf2 = dfx.groupby(\"classification\").filter(lambda x: len(x) > 14000)\ndf2.classification.value_counts()\n```", "```py\nquery = \"\"\"\n    SELECT DISTINCT\n        dups.*\n    FROM (\n        SELECT classification, count(residueCount) AS classCount\n        FROM `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups`\n        GROUP BY classification\n    ) AS sub\n    INNER JOIN `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups` AS dups\n        ON sub.classification = dups.classification\n    WHERE sub.classCount > 14000\n\"\"\"\nquery_job = client.query(\n    query,\n    location=\"US\",\n)\ndf2 = query_job.to_dataframe()\n```", "```py\ndf2 = df2.dropna()\ndf2.shape\n```", "```py\nimport pandas_gbq\npandas_gbq.to_gbq(df2, 'protein_structure_sequence.dataset_pdb_no_dups_cleaned', project_id ='biotech-project-321515', if_exists='replace')\n```", "```py\nX = df2.drop(columns=[\"classification\"])\ny = df2.classification.values.ravel()\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)\n```", "```py\npip install autosklearn\n```", "```py\nimport autosklearn.classification\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='/tmp/autosklearn_protein_tmp5',\n)\n```", "```py\nautoml.fit(X_train, y_train, dataset_name='dataset_pdb_no_dups')\n```", "```py\nprint(automl.leaderboard())\n```", "```py\nautoml.get_models_with_weights()[0]\n```", "```py\npredictions = automl.predict(X_test)\nprint(\"classification_report:\", classification_report(y_test, predictions))\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(color_codes=True)\n```", "```py\ndf = pd.read_csv(\"../../datasets/dataset_toxicity_sd.csv\")\ndf = df.dropna()\n```", "```py\nX = df[[\"Heteroatoms\", \"MolWt\", \"HeavyAtoms\", \"NHOH\", \"HAcceptors\", \"HDonors\"]]\ny = df.TPSA.values.ravel()\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)\n```", "```py\nimport seaborn as sns\nfig = sns.pairplot(data=df[[\"Heteroatoms\", \"MolWt\", \"HeavyAtoms\"]])\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n```", "```py\np = sns.jointplot(x=y_test, y=y_pred, kind=\"reg\")\np.fig.suptitle(f\"Linear Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}\")\np.fig.subplots_adjust(top=0.90)\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\np = sns.jointplot(x=y_test, y=y_pred, kind=\"reg\")\np.fig.suptitle(f\"RandomForestRegressor Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}\")\n# p.ax_joint.collections[0].set_alpha(0)\n# p.fig.tight_layout()\np.fig.subplots_adjust(top=0.90)\n```", "```py\nfor i in range(1,10):\n    reg = RandomForestRegressor(max_depth=i)\n                       .fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    print(\"depth =\", i, \n          \"score=\", r2_score(y_test, y_pred), \n          \"mse = \", mean_squared_error(y_test, y_pred))\n```", "```py\nfeatures = X.columns\nimportances = reg.feature_importances_\nindices = np.argsort(importances)[-9:]\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices],  \n                           color='royalblue', \n                           align='center')\nplt.yticks(range(len(indices)),[features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n```", "```py\nimport xgboost as xg\nreg = xg.XGBRegressor(objective ='reg:linear',n_estimators = 1000).fit(X_train, y_train)\ny_pred = reg.predict(X_test)\np = sns.jointplot(x=y_test, y=y_pred, kind=\"reg\")\np.fig.suptitle(f\"xgboost Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}\")\np.fig.subplots_adjust(top=0.90)\n```", "```py\n    pip install mysql-connector pymysql\n    ```", "```py\n    import pandas as pd\n    import mysql.connector\n    from sqlalchemy import create_engine\n    import sys\n    import seaborn as sns\n    ```", "```py\n    ENDPOINT=\"toxicitydataset.xxxxxx.us-east-2.rds.amazonaws.com\"\n    PORT=\"3306\"\n    USR=\"admin\"\n    DBNAME=\"toxicity_db_tutorial\"\n    PASSWORD = \"xxxxxxxxxxxxxxxxxx\"\n    ```", "```py\n    db_connection_str = 'mysql+pymysql://{USR}:{PASSWORD}@{ENDPOINT}:{PORT}/{DBNAME}'.format(USR=USR, PASSWORD=PASSWORD, ENDPOINT=ENDPOINT, PORT=PORT, DBNAME=DBNAME)\n    db_connection = create_engine(db_connection_str)\n    ```", "```py\n    df = pd.read_sql('SELECT * FROM dataset_toxicity_sd', con=db_connection)\n    ```", "```py\n    pip install auto-sklearn\n    ```", "```py\n    X = df[[\"Heteroatoms\", \"MolWt\", \"HeavyAtoms\", \"NHOH\", \"HAcceptors\", \"HDonors\"]]\n    y = df.TPSA.values.ravel()\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)\n    ```", "```py\n    import autosklearn.regression\n    automl = autosklearn.regression.AutoSklearnRegressor(\n        time_left_for_this_task=120,\n        per_run_time_limit=30,\n        tmp_folder='/tmp/autosklearn_regression_example_tmp2')\n    automl.fit(X_train, y_train, dataset_name='dataset_toxicity')\n    ```", "```py\n    automl.get_models_with_weights()[0]\n    ```", "```py\n    from sklearn.metrics import r2_score, mean_squared_error\n    predictions = automl.predict(X_test)\n    p = sns.jointplot(x=y_test, y=predictions, kind=\"reg\")\n    p.fig.suptitle(f\"automl, R2 = {round(r2_score(y_test, predictions), 3)}, MSE = {round(mean_squared_error(y_test, predictions), 2)}\")\n    # p.ax_joint.collections[0].set_alpha(0)\n    # p.fig.tight_layout()\n    p.fig.subplots_adjust(top=0.90)\n    ```"]