- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probabilistic Learning – Classification Using Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a meteorologist provides a weather forecast, precipitation is typically
    described with phrases like “70 percent chance of rain.” Such forecasts are known
    as probability of precipitation reports. Have you ever considered how they are
    calculated? It is a puzzling question because, in reality, it will either rain
    or not with absolute certainty.
  prefs: []
  type: TYPE_NORMAL
- en: Weather estimates are based on probabilistic methods, which are those concerned
    with describing uncertainty. They use data on past events to extrapolate future
    events. In the case of the weather, the chance of rain describes the proportion
    of prior days with similar atmospheric conditions on which precipitation occurred.
    A 70 percent chance of rain implies that in 7 out of 10 past cases with similar
    conditions, precipitation occurred somewhere in the area.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the Naive Bayes algorithm, which uses probabilities in
    much the same way as a weather forecast. While studying this method, you will
    learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic principles of probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specialized methods and data structures needed to analyze text data with
    R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to employ Naive Bayes to build a **Short Message Service** (**SMS**) junk
    message filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’ve taken a statistics class before, some of the material in this chapter
    may be a review. Even so, it may be helpful to refresh your knowledge of probability.
    You will find out that these principles are the basis of how Naive Bayes got such
    a strange name.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic statistical ideas necessary to understand the Naive Bayes algorithm
    have existed for centuries. The technique descended from the work of the 18th-century
    mathematician Thomas Bayes, who developed foundational principles for describing
    the probability of events and how these probabilities should be revised in light
    of additional information. These principles formed the foundation for what are
    now known as **Bayesian methods**.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover these methods in greater detail later. For now, it suffices to
    say that a probability is a number between zero and one (or from 0 to 100 percent)
    that captures the chance that an event will occur in light of the available evidence.
    The lower the probability, the less likely the event is to occur. A probability
    of zero indicates that the event will definitely not occur, while a probability
    of one indicates that the event will occur with absolute certainty. Life’s most
    interesting events tend to be those with uncertain probability; estimating the
    chance that they will occur helps us make better decisions by revealing the most
    likely outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classifiers based on Bayesian methods utilize training data to calculate the
    probability of each outcome based on the evidence provided by feature values.
    When the classifier is later applied to unlabeled data, it uses these calculated
    probabilities to predict the most likely class for the new example. It’s a simple
    idea, but it results in a method that can have results on par with more sophisticated
    algorithms. In fact, Bayesian classifiers have been used for:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification, such as junk email (spam) filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intrusion or anomaly detection in computer networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosing medical conditions given a set of observed symptoms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, Bayesian classifiers are best applied to problems for which the information
    from numerous attributes should be considered simultaneously to estimate the overall
    probability of an outcome. While many machine learning algorithms ignore features
    that have weak effects, Bayesian methods utilize all available evidence to subtly
    change the predictions. This implies that even if a large portion of features
    have relatively minor effects, their combined impact in a Bayesian model could
    be quite large.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of Bayesian methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping into the Naive Bayes algorithm, it’s worth spending some time
    defining the concepts that are used across Bayesian methods. Summarized in a single
    sentence, Bayesian probability theory is rooted in the idea that the estimated
    likelihood of an **event**, or potential outcome, should be based on the evidence
    at hand across multiple **trials**, or opportunities for the event to occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates events and trials for several real-world outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Event** | **Trial** |'
  prefs: []
  type: TYPE_TB
- en: '| Heads result | A coin flip |'
  prefs: []
  type: TYPE_TB
- en: '| Rainy weather | A single day (or another time period) |'
  prefs: []
  type: TYPE_TB
- en: '| Message is spam | An incoming email message |'
  prefs: []
  type: TYPE_TB
- en: '| Candidate becomes president | A presidential election |'
  prefs: []
  type: TYPE_TB
- en: '| Mortality | A hospital patient |'
  prefs: []
  type: TYPE_TB
- en: '| Winning the lottery | A lottery ticket |'
  prefs: []
  type: TYPE_TB
- en: Bayesian methods provide insights into how the probability of these events can
    be estimated from observed data. To see how, we’ll need to formalize our understanding
    of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probability of an event is estimated from observed data by dividing the
    number of trials in which the event occurred by the total number of trials. For
    instance, if it rained 3 out of 10 days with similar conditions as today, the
    probability of rain today can be estimated as *3 / 10 = 0.30* or 30 percent. Similarly,
    if 10 out of 50 prior email messages were spam, then the probability of any incoming
    message being spam can be estimated as *10 / 50 = 0.20* or 20 percent.
  prefs: []
  type: TYPE_NORMAL
- en: To denote these probabilities, we use notation in the form *P(A)*, which signifies
    the probability of event *A*. For example, *P(rain) = 0.30* to indicate a 30 percent
    chance of rain or *P(spam) = 0.20* to describe a 20 percent probability of an
    incoming message being spam.
  prefs: []
  type: TYPE_NORMAL
- en: Because a trial always results in some outcome happening, the probability of
    all possible outcomes of a trial must always sum to one. Thus, if the trial has
    exactly two outcomes and the outcomes cannot occur simultaneously, then knowing
    the probability of either outcome reveals the probability of the other. This is
    the case for many outcomes, such as heads or tails coin flips, or spam versus
    legitimate email messages, also known as “ham.” Using this principle, knowing
    that *P(spam) = 0.20* allows us to calculate *P(ham) = 1 – 0.20 = 0.80*. This
    only works because spam and ham are **mutually exclusive and exhaustive events**,
    which implies that they cannot occur at the same time and are the only possible
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: A single event cannot happen and not happen simultaneously. This means an event
    is always mutually exclusive and exhaustive with its **complement**, or the event
    comprising all other outcomes in which the event of interest does not happen.
    The complement of event *A* is typically denoted *A*^c or *A’*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the shorthand notation *P(A*^c*)* or *P(¬A)* can be used to denote
    the probability of event *A* not occurring. For example, the notation *P(¬spam)
    = 0.80* suggests that the probability of a message not being spam is 80%.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate events and their complements, it is often helpful to imagine
    a two-dimensional space that is partitioned into probabilities for each event.
    In the following diagram, the rectangle represents the possible outcomes for an
    email message. The circle represents the 20 percent probability that the message
    is spam. The remaining 80 percent represents the complement *P(¬spam)*, or the
    messages that are not spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: The probability space for all emails can be visualized as partitions
    of spam and ham'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding joint probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Often, we are interested in monitoring several non-mutually exclusive events
    in the same trial. If certain events occur concurrently with the event of interest,
    we may be able to use them to make predictions. Consider, for instance, a second
    event based on the outcome that an email message contains the word *Viagra*. The
    preceding diagram, updated for this second event, might appear as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Non-mutually exclusive events are depicted as overlapping partitions'
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the diagram that the Viagra circle overlaps with both the spam and
    ham areas of the diagram and the spam circle includes an area not covered by the
    Viagra circle. This implies that not all spam messages contain the term Viagra
    and some messages with the term Viagra are ham. However, because this word appears
    very rarely outside spam, its presence in a new incoming message would be strong
    evidence that the message is spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'To zoom in for a closer look at the overlap between these circles, we’ll employ
    a visualization known as a **Venn diagram**. First used in the late 19th century
    by mathematician John Venn, the diagram uses circles to illustrate the overlap
    between sets of items. As in most Venn diagrams, the size and degree of overlap
    of the circles in the depiction is not meaningful. Instead, it is used as a reminder
    to allocate probability to all combinations of events. A Venn diagram for spam
    and Viagra might be depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: A Venn diagram illustrates the overlap of the spam and Viagra events'
  prefs: []
  type: TYPE_NORMAL
- en: We know that 20 percent of all messages were spam (the left circle), and 5 percent
    of all messages contained the word *Viagra* (the right circle). We would like
    to quantify the degree of overlap between these two proportions. In other words,
    we hope to estimate the probability that both *P(spam)* and *P(Viagra)* occur,
    which can be written as *P(spam ![](img/B17290_04_001.png) Viagra)*. The *![](img/B17290_04_001.png)*
    symbol signifies the intersection of the two events; the notation *A ![](img/B17290_04_001.png)
    B* refers to the event in which both *A* and *B* occur.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating *P(spam ![](img/B17290_04_001.png) Viagra)* depends on the **joint
    probability** of the two events, or how the probability of one event is related
    to the probability of the other. If the two events are totally unrelated, they
    are called **independent events**. This is not to say that independent events
    cannot occur at the same time; event independence simply implies that knowing
    the outcome of one event does not provide any information about the outcome of
    the other. For instance, the outcome of a heads result on a coin flip is independent
    of whether the weather is rainy or sunny on any given day.
  prefs: []
  type: TYPE_NORMAL
- en: If all events were independent, it would be impossible to predict one event
    by observing another. In other words, **dependent events** are the basis of predictive
    modeling. Just as the presence of clouds is predictive of a rainy day, the appearance
    of the word *Viagra* is predictive of a spam email.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Dependent events are required for machines to learn how to identify
    useful patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the probability of dependent events is a bit more complex than for
    independent events. If *P(spam)* and *P(Viagra)* were independent, we could easily
    calculate *P(spam ![](img/B17290_04_001.png) Viagra)*, the probability of both
    events happening at the same time. Because 20 percent of all messages are spam,
    and 5 percent of all emails contain the word *Viagra*, we could assume that 1
    percent of all messages with the term *Viagra* are spam. This is because *0.05
    * 0.20 = 0.01*. More generally, for independent events *A* and *B*, the probability
    of both happening can be computed as *P(A ![](img/B17290_04_001.png) B) = P(A)
    * P(B)*.
  prefs: []
  type: TYPE_NORMAL
- en: That said, we know that *P(spam)* and *P(Viagra)* are likely to be highly dependent,
    which means that this calculation is incorrect. To obtain a more reasonable estimate,
    we need to use a more careful formulation of the relationship between these two
    events, which is based on more advanced Bayesian methods.
  prefs: []
  type: TYPE_NORMAL
- en: Computing conditional probability with Bayes’ theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The relationships between dependent events can be described using **Bayes’
    theorem**, which provides a way of thinking about how to revise an estimate of
    the probability of one event in light of the evidence provided by another. One
    formulation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_008.png)'
  prefs: []
  type: TYPE_IMG
- en: The notation *P(A|B)* is read as the probability of event *A* given that event
    *B* occurred. This is known as **conditional probability** since the probability
    of *A* is dependent (that is, conditional) on what happened with event *B*.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ theorem states that the best estimate of *P(A|B)* is the proportion of
    trials in which *A* occurred with *B*, out of all the trials in which *B* occurred.
    This implies that the probability of event *A* is higher if *A* and *B* often
    occur together each time *B* is observed. Note that this formula adjusts *P(A
    ![](img/B17290_04_001.png) B)* for the probability of *B* occurring. If *B* is
    extremely rare, *P(B)* and *P(A ![](img/B17290_04_001.png) B)* will always be
    small; however, if *A* almost always happens together with *B*, *P(A|B)* will
    still be high in spite of *B*’s rarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, *P(A ![](img/B17290_04_001.png) B) = P(A|B) * P(B)*, a fact
    that can be easily derived by applying a bit of algebra to the previous formula.
    Rearranging this formula once more with the knowledge that *P(A ![](img/B17290_04_001.png)
    B) = P(B ![](img/B17290_04_001.png) A)* results in the conclusion that *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, which we can then use in the following formulation of Bayes’
    theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_016.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, this is the traditional formulation of Bayes’ theorem for reasons that
    will become clear as we apply it to machine learning. First, to better understand
    how Bayes’ theorem works in practice, let’s revisit our hypothetical spam filter.
  prefs: []
  type: TYPE_NORMAL
- en: Without knowledge of an incoming message’s content, the best estimate of its
    spam status would be *P(spam)*, the probability that any prior message was spam.
    This estimate is known as the **prior probability**. We found this previously
    to be 20 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you obtained additional evidence by looking more carefully at the
    set of previously received messages and examining the frequency with which the
    term *Viagra* appeared. The probability that the word *Viagra* was used in previous
    spam messages, or *P(Viagra|spam)*, is called the **likelihood**. The probability
    that *Viagra* appeared in any message at all, or *P(Viagra)*, is known as the
    **marginal likelihood**.
  prefs: []
  type: TYPE_NORMAL
- en: 'By applying Bayes’ theorem to this evidence, we can compute a **posterior probability**
    that measures how likely a message is to be spam. If the posterior probability
    is greater than 50 percent, the message is more likely to be spam than ham, and
    it should perhaps be filtered. The following formula shows how Bayes’ theorem
    is applied to the evidence provided by previous email messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape  Description automatically generated with medium confidence](img/B17290_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Bayes’ theorem acting on previously received emails'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the components of Bayes’ theorem, it helps to construct a **frequency
    table** (shown on the left in the tables that follow) recording the number of
    times *Viagra* appeared in spam and ham messages. Just like a two-way cross-tabulation,
    one dimension of the table indicates levels of the class variable (spam or ham),
    while the other dimension indicates levels for features (Viagra: yes or no). The
    cells then indicate the number of instances that have the specified combination
    of the class value and feature value.'
  prefs: []
  type: TYPE_NORMAL
- en: The frequency table can then be used to construct a **likelihood table**, as
    shown on the right in the following tables. The rows of the likelihood table indicate
    the conditional probabilities for *Viagra* (yes/no), given that an email was either
    spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Frequency and likelihood tables are the basis for computing the
    posterior probability of spam'
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood table reveals that *P(Viagra=Yes|spam) = 4 / 20 = 0.20*, indicating
    that there is a 20 percent probability that a message contains the term *Viagra*
    given that the message is spam. Additionally, since *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, we can calculate *P(spam ![](img/B17290_04_001.png) Viagra)*
    as *P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04*. The same result
    can be found in the frequency table, which notes that 4 out of 100 messages were
    spam and contained the term *Viagra*. Either way, this is four times greater than
    the previous estimate of 0.01 we calculated as *P(A ![](img/B17290_04_001.png)
    B) = P(A) * P(B)* under the false assumption of independence. This, of course,
    illustrates the importance of Bayes’ theorem for estimating joint probability.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the posterior probability, *P(spam|Viagra)*, we simply take *P(Viagra|spam)
    * P(spam) / P(Viagra)*, or *(4 / 20) * (20 / 100) / (5 / 100) = 0.80*. Therefore,
    the probability that a message is spam is 80 percent given that it contains the
    word *Viagra*. In light of this finding, any message containing this term should
    probably be filtered.
  prefs: []
  type: TYPE_NORMAL
- en: This is very much how commercial spam filters work, although they consider a
    much larger number of words simultaneously when computing the frequency and likelihood
    tables. In the next section, we’ll see how this method can be adapted to accommodate
    cases when additional features are involved.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Naive Bayes** algorithm defines a simple method for applying Bayes’ theorem
    to classification problems. Although it is not the only machine learning method
    that utilizes Bayesian methods, it is the most common. It grew in popularity due
    to its successes in text classification, where it was once the de facto standard.
    The strengths and weaknesses of this algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strengths** | **Weaknesses** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Simple, fast, and very effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does well with noisy and missing data and large numbers of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires relatively few examples for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to obtain the estimated probability for a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Relies on an often-faulty assumption of equally important and independent features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not ideal for datasets with many numeric features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimated probabilities are less reliable than the predicted classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm is named as such because it makes some so-called “naive”
    assumptions about the data. In particular, Naive Bayes assumes that all of the
    features in the dataset are **equally important and independent**. These assumptions
    are rarely true in most real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when attempting to identify spam by monitoring email messages,
    it is almost certainly true that some features will be more important than others.
    For example, the email sender may be a more important indicator of spam than the
    message text. Additionally, the words in the message body are not independent
    of one another, since the appearance of some words is a very good indication that
    other words are also likely to appear. A message with the word *Viagra* will probably
    also contain the word *prescription* or *drugs*.
  prefs: []
  type: TYPE_NORMAL
- en: However, in most cases, even when these assumptions are violated, Naive Bayes
    still performs surprisingly well. This is true even in circumstances where strong
    dependencies are found among the features. Due to the algorithm’s versatility
    and accuracy across many types of conditions, particularly with smaller training
    datasets, Naive Bayes is often a reasonable baseline candidate for classification
    learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The exact reason why Naive Bayes works well in spite of its faulty assumptions
    has been the subject of much speculation. One explanation is that it is not important
    to obtain a precise estimate of probability so long as the predictions are accurate.
    For instance, if a spam filter correctly identifies spam, does it matter whether
    the predicted probability of spam was 51 percent or 99 percent? For one discussion
    of this topic, refer to *On the Optimality of the Simple Bayesian Classifier under
    Zero-One Loss, Domingos, P. and Pazzani, M., Machine Learning, 1997, Vol. 29,
    pp. 103-130*.
  prefs: []
  type: TYPE_NORMAL
- en: Classification with Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s extend our spam filter by adding a few additional terms to be monitored
    in addition to the term *Viagra*: *money*, *groceries*, and *unsubscribe*. The
    Naive Bayes learner is trained by constructing a likelihood table for the appearance
    of these four words (labeled *W*¹, *W*², *W*³, and *W*⁴), as shown in the following
    diagram for 100 emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: An expanded table adds likelihoods for additional terms in spam
    and ham messages'
  prefs: []
  type: TYPE_NORMAL
- en: As new messages are received, we need to calculate the posterior probability
    to determine whether they are more likely spam or ham, given the likelihood of
    the words being found in the message text. For example, suppose that a message
    contains the terms *Viagra* and *unsubscribe* but does not contain either *money*
    or *groceries*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes’ theorem, we can define the problem as shown in the following formula.
    This computes the probability that a message is spam given that *Viagra = Yes*,
    *Money = No*, *Groceries = No*, and *Unsubscribe = Yes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_020.png)'
  prefs: []
  type: TYPE_IMG
- en: For two reasons, this formula is computationally difficult to solve. First,
    as additional features are added, tremendous amounts of memory are needed to store
    the probabilities for all possible intersecting events. Imagine the complexity
    of a Venn diagram for the events for four words, let alone for hundreds or more.
    Second, many of these potential intersections will never have been observed in
    past data, which would lead to a joint probability of zero and problems that will
    become clear later.
  prefs: []
  type: TYPE_NORMAL
- en: The computation becomes more reasonable if we exploit the fact that Naive Bayes
    makes the naive assumption of independence among events. Specifically, it assumes
    **class-conditional independence**, which means that events are independent so
    long as they are conditioned on the same class value. The conditional independence
    assumption allows us to use the probability rule for independent events, which
    states that *P(A ![](img/B17290_04_001.png) B) = P(A) * P(B)*. This simplifies
    the numerator by allowing us to multiply the individual conditional probabilities
    rather than computing a complex conditional joint probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, because the denominator does not depend on the target class (spam or
    ham), it is treated as a constant value and can be ignored for the time being.
    This means that the conditional probability of spam can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the probability that the message is ham can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_023.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the equals symbol has been replaced by the proportional-to symbol
    (similar to a sideways, open-ended “8”) to indicate the fact that the denominator
    has been omitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values in the likelihood table, we can start filling in numbers in
    these equations. The overall likelihood of spam is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012*'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(1 / 80) * (66 / 80) * (71 / 80) * (23 / 80) * (80 / 100) = 0.002*'
  prefs: []
  type: TYPE_NORMAL
- en: Because *0.012 / 0.002 = 6*, we can say that this message is 6 times more likely
    to be spam than ham. However, to convert these numbers into probabilities, we
    need one last step to reintroduce the denominator that has been excluded. Essentially,
    we must re-scale the likelihood of each outcome by dividing it by the total likelihood
    across all possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, the probability of spam is equal to the likelihood that the message
    is spam divided by the likelihood that the message is either spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.012 / (0.012 + 0.002) = 0.857*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the probability of ham is equal to the likelihood that the message
    is ham divided by the likelihood that the message is either spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.002 / (0.012 + 0.002) = 0.143*'
  prefs: []
  type: TYPE_NORMAL
- en: Given the pattern of words found in this message, we expect that the message
    is spam with an 85.7 percent probability, and ham with a 14.3 percent probability.
    Because these are mutually exclusive and exhaustive events, the probabilities
    sum up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Naive Bayes classification algorithm used in the preceding example can
    be summarized by the following formula. The probability of level *L* for class
    *C*, given the evidence provided by features *F*[1] through *F*[n], is equal to
    the product of the probabilities of each piece of evidence conditioned on the
    class level, the prior probability of the class level, and a scaling factor *1
    / Z*, which converts the likelihood values into probabilities. This is formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_024.png)'
  prefs: []
  type: TYPE_IMG
- en: Although this equation seems intimidating, as the spam filtering example illustrated,
    the series of steps is fairly straightforward. Begin by building a frequency table,
    use this to build a likelihood table, and multiply out the conditional probabilities
    with the naive assumption of independence. Finally, divide by the total likelihood
    to transform each class likelihood into a probability. After attempting this calculation
    a few times by hand, it will become second nature.
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we employ Naive Bayes on more complex problems, there are some nuances
    to consider. Suppose we received another message, this time containing all four
    terms: *Viagra*, *groceries*, *money*, and *unsubscribe*. Using the Naive Bayes
    algorithm as before, we can compute the likelihood of spam as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(4 / 20) * (10 / 20) * (0 / 20) * (12 / 20) * (20 / 100) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(1 / 80) * (14 / 80) * (8 / 80) * (23 / 80) * (80 / 100) = 0.00005*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the probability of spam is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0 / (0 + 0.00005) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the probability of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.00005 / (0 + 0\. 0.00005) = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: These results suggest that the message is spam with 0 percent probability and
    ham with 100 percent probability. Does this prediction make sense? Probably not.
    The message contains several words usually associated with spam, including *Viagra*,
    which is rarely used in legitimate messages. It is therefore very likely that
    the message has been incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: This problem arises if an event never occurs for one or more levels of the class
    and therefore the resulting likelihoods are zero. For instance, the term *groceries*
    had never previously appeared in a spam message. Consequently, *P(groceries|spam)
    = 0%*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, because probabilities in the Naive Bayes formula are multiplied in a chain,
    this zero-percent value causes the posterior probability of spam to be zero, giving
    the word *groceries* the ability to effectively nullify and overrule all of the
    other evidence. Even if the email was otherwise overwhelmingly expected to be
    spam, the absence of the word *groceries* in spam will always veto the other evidence
    and result in the probability of spam being zero.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem involves using something called the **Laplace estimator**,
    which is named after the French mathematician Pierre-Simon Laplace. The Laplace
    estimator adds a small number to each of the counts in the frequency table, which
    ensures that each feature has a non-zero probability of occurring with each class.
    Typically, the Laplace estimator is set to one, which ensures that each class-feature
    combination is found in the data at least once.
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace estimator can be set to any value and does not necessarily even
    have to be the same for each of the features. If you were a devoted Bayesian,
    you could use a Laplace estimator to reflect a presumed prior probability of how
    a feature relates to a class. In practice, given a large enough training dataset,
    this is excessive. Consequently, the value of one is almost always used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this affects our prediction for this message. Using a Laplace
    value of 1, we add 1 to each numerator in the likelihood function. Then, we need
    to add 4 to each conditional probability denominator to compensate for the 4 additional
    values added to the numerator. The likelihood of spam is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(5 / 24) * (11 / 24) * (1 / 24) * (13 / 24) * (20 / 100) = 0.0004*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And the likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(2 / 84) * (15 / 84) * (9 / 84) * (24 / 84) * (80 / 100) = 0.0001*'
  prefs: []
  type: TYPE_NORMAL
- en: By computing *0.0004 / (0.0004 + 0.0001)*, we find that the probability of spam
    is 80 percent and therefore the probability of ham is about 20 percent. This is
    a more plausible result than the *P(spam) = 0* computed when the term *groceries*
    alone determined the result.
  prefs: []
  type: TYPE_NORMAL
- en: Although the Laplace estimator was added to the numerator and denominator of
    the likelihoods, it was not added to the prior probabilities—the values of 20/100
    and 80/100\. This is because our best estimate of the overall probability of spam
    and ham remains at 20% and 80% respectively given what was observed in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Using numeric features with Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naive Bayes uses frequency tables for learning the data, which means that each
    feature must be categorical in order to create the combinations of class and feature
    values comprising the matrix. Since numeric features do not have categories of
    values, the preceding algorithm does not work directly with numeric data. There
    are, however, ways that this can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: One easy and effective solution is to **discretize** numeric features, which
    simply means that the numbers are put into categories known as **bins**. For this
    reason, discretization is also sometimes called **binning**. This method works
    best when there are large amounts of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different ways to discretize a numeric feature. Perhaps the
    most common is to explore the data for natural categories or **cut points** in
    the distribution. For example, suppose that you added a feature to the spam dataset
    that recorded the time of day or night the email was sent, from 0 to 24 hours
    past midnight. Depicted using a histogram, the time data might look something
    like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: A histogram visualizing the distribution of the time emails were
    received'
  prefs: []
  type: TYPE_NORMAL
- en: In the early hours of the morning, message frequency is low. Activity picks
    up during business hours and tapers off in the evening. This creates four natural
    bins of activity, as partitioned by the dashed lines. These indicate places where
    the numeric data could be divided into levels to create a new categorical feature,
    which could then be used with Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of four bins was based on the natural distribution of data and a
    hunch about how the proportion of spam might change throughout the day. We might
    expect that spammers operate in the late hours of the night, or they may operate
    during the day when people are likely to check their email. That said, to capture
    these trends, we could have just as easily used three bins or twelve.
  prefs: []
  type: TYPE_NORMAL
- en: If there are no obvious cut points, one option is to discretize the feature
    using quantiles, which were introduced in *Chapter 2*, *Managing and Understanding
    Data*. You could divide the data into three bins with tertiles, four bins with
    quartiles, or five bins with quintiles.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that discretizing a numeric feature always results
    in a reduction of information, as the feature’s original granularity is reduced
    to a smaller number of categories. It is important to strike a balance. Too few
    bins can result in important trends being obscured. Too many bins can result in
    small counts in the Naive Bayes frequency table, which can increase the algorithm’s
    sensitivity to noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: Example – filtering mobile phone spam with the Naive Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the worldwide use of mobile phones has grown, a new avenue for electronic
    junk mail has opened for disreputable marketers. These advertisers utilize SMS
    text messages to target potential consumers with unwanted advertising known as
    SMS spam. This type of spam is troublesome because, unlike email spam, an SMS
    message is particularly disruptive due to the omnipresence of one’s mobile phone.
    Developing a classification algorithm that could filter SMS spam would provide
    a useful tool for cellular phone providers.
  prefs: []
  type: TYPE_NORMAL
- en: Since Naive Bayes has been used successfully for email spam filtering, it seems
    likely that it could also be applied to SMS spam. However, relative to email spam,
    SMS spam poses additional challenges for automated filters. SMS messages are often
    limited to 160 characters, reducing the amount of text that can be used to identify
    whether a message is junk. The limit, combined with small mobile phone keyboards,
    has led many to adopt a form of SMS shorthand lingo, which further blurs the line
    between legitimate messages and spam. Let’s see how a simple Naive Bayes classifier
    handles these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To develop the Naive Bayes classifier, we will use data adapted from the SMS
    Spam Collection at [https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
  prefs: []
  type: TYPE_NORMAL
- en: To read more about how the SMS Spam Collection was developed, refer to *On the
    Validity of a New SMS Spam Collection, Gómez, J. M., Almeida, T. A., and Yamakami,
    A., Proceedings of the 11th IEEE International Conference on Machine Learning
    and Applications, 2012*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset includes the text of SMS messages, along with a label indicating
    whether the message is unwanted. Junk messages are labeled spam, while legitimate
    messages are labeled ham. Some examples of spam and ham are shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample SMS ham** | **Sample SMS spam** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel
    bleh. But at least its not writhing pain kind of bleh.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If he started searching he will get job in few days. he have great potential
    and talent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I got another job! The one at the hospital doing data analysis or something,
    starts on monday! Not sure when my thesis will got finished
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry
    2 100 wkly draw txt MUSIC to 87066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: December only! Had your mobile 11mths+? You are entitled to update to the latest
    colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valentines Day Special! Win over £1000 in our quiz and take your partner on
    the trip of a lifetime! Send GO to 83600 now. 150p/msg rcvd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding messages, do you notice any distinguishing characteristics
    of spam? One notable characteristic is that two of the three spam messages use
    the word *free*, yet this word does not appear in any of the ham messages. On
    the other hand, two of the ham messages cite specific days of the week, as compared
    to zero in spam messages.
  prefs: []
  type: TYPE_NORMAL
- en: Our Naive Bayes classifier will take advantage of such patterns in the word
    frequency to determine whether the SMS messages seem to better fit the profile
    of spam or ham. While it’s not inconceivable that the word *free* would appear
    outside of a spam SMS, a legitimate message is likely to provide additional words
    giving context.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a ham message might ask, “Are you free on Sunday?” whereas a spam
    message might use the phrase “free ringtones.” The classifier will compute the
    probability of spam and ham given the evidence provided by all the words in the
    message.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step toward constructing our classifier involves processing the raw
    data for analysis. Text data is challenging to prepare because it is necessary
    to transform the words and sentences into a form that a computer can understand.
    We will transform our SMS data into a representation known as **bag-of-words**,
    which provides a binary feature indicating whether each word appears in the given
    example while ignoring word order or the context in which the word appears. Although
    this is a relatively simple representation, as we will soon demonstrate, it performs
    well enough for many classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used here has been modified slightly from the original to make it
    easier to work with in R. If you plan on following along with the example, download
    the `sms_spam.csv` file from the Packt website and save it to your R working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by importing the CSV data and saving it to a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `str()` function, we see that the `sms_raw` data frame includes 5,559
    total SMS messages with two features: `type` and `text`. The SMS type has been
    coded as either ham or spam. The `text` element stores the full raw SMS message
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `type` element is currently a character vector. Since this is a categorical
    variable, it would be better to convert it into a factor, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining this with the `str()` and `table()` functions, we see that `type`
    has now been appropriately recoded as a factor. Additionally, we see that 747
    (about 13 percent) of SMS messages in our data were labeled as `spam`, while the
    others were labeled as `ham`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For now, we will leave the message text alone. As you will learn in the next
    section, processing raw SMS messages will require the use of a new set of powerful
    tools designed specifically for processing text data.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – cleaning and standardizing text data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SMS messages are strings of text composed of words, spaces, numbers, and punctuation.
    Handling this type of complex data takes a large amount of thought and effort.
    One needs to consider how to remove numbers and punctuation; handle uninteresting
    words, such as *and*, *but*, and *or*; and break apart sentences into individual
    words. Thankfully, this functionality has been provided by members of the R community
    in a text-mining package titled `tm`.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package was originally created by Ingo Feinerer as a dissertation project
    at the Vienna University of Economics and Business. To learn more, see *Text Mining
    Infrastructure in R, Feinerer, I., Hornik, K., and Meyer, D., Journal of Statistical
    Software, 2008, Vol. 25, pp. 1-54*.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package can be installed via the `install.packages("tm")` command and
    loaded with the `library(tm)` command. Even if you already have it installed,
    it may be worth redoing the installation to ensure that your version is up to
    date, as the `tm` package is still under active development. This occasionally
    results in changes to its functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was tested using `tm` version 0.7-11, which was current as of May
    2023\. If you see differences in the output or if the code does not work, you
    may be using a different version. The Packt support page for this book, as well
    as its GitHub repository, will post solutions for future `tm` package versions
    if significant changes are noted.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in processing text data involves creating a **corpus**, which
    is a collection of text documents. The documents can be short or long, from individual
    news articles, pages in a book, pages from the web, or even entire books. In our
    case, the corpus will be a collection of SMS messages.
  prefs: []
  type: TYPE_NORMAL
- en: To create a corpus, we’ll use the `VCorpus()` function in the `tm` package,
    which refers to a volatile corpus—the term “volatile” meaning that it is stored
    in memory as opposed to being stored on disk (the `PCorpus()` function is used
    to access a permanent corpus stored in a database). This function requires us
    to specify the source of documents for the corpus, which could be a computer’s
    filesystem, a database, the web, or elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already loaded the SMS message text into R, we’ll use the `VectorSource()`
    reader function to create a source object from the existing `sms_raw$text` vector,
    which can then be supplied to `VCorpus()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The resulting corpus object is saved with the name `sms_corpus`.
  prefs: []
  type: TYPE_NORMAL
- en: By specifying an optional `readerControl` parameter, the `VCorpus()` function
    can be used to import text from sources such as PDFs and Microsoft Word files.
    To learn more, examine the *Data Import* section in the `tm` package vignette
    using the `vignette("tm")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'By printing the corpus, we see that it contains documents for each of the 5,559
    SMS messages in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, because the `tm` corpus is essentially a complex list, we can use list
    operations to select documents in the corpus. The `inspect()` function shows a
    summary of the result. For example, the following command will view a summary
    of the first and second SMS messages in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the actual message text, the `as.character()` function must be applied
    to the desired messages. To view one message, use the `as.character()` function
    on a single list element, noting that the double-bracket notation is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To view multiple documents, we’ll need to apply `as.character()` to several
    items in the `sms_corpus` object. For this, we’ll use the `lapply()` function,
    which is part of a family of R functions that applies a procedure to each element
    of an R data structure. These functions, which include `apply()` and `sapply()`
    among others, are one of the key idioms of the R language. Experienced R coders
    use these much like the way `for` or `while` loops are used in other programming
    languages, as they result in more readable (and sometimes more efficient) code.
    The `lapply()` function for applying `as.character()` to a subset of corpus elements
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As noted earlier, the corpus contains the raw text of 5,559 text messages. To
    perform our analysis, we need to divide these messages into individual words.
    First, we need to clean the text to standardize the words and remove punctuation
    characters that clutter the result. For example, we would like the strings *Hello!*,
    *HELLO*, and *hello* to be counted as instances of the same word.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm_map()` function provides a method to apply a transformation (also known
    as a mapping) to a `tm` corpus. We will use this function to clean up our corpus
    using a series of transformations and save the result in a new object called `corpus_clean`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first transformation will standardize the messages to use only lowercase
    characters. To this end, R provides a `tolower()` function that returns a lowercase
    version of text strings. In order to apply this function to the corpus, we need
    to use the `tm` wrapper function `content_transformer()` to treat `tolower()`
    as a transformation function that can be used to access the corpus. The full command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the command worked as expected, let’s inspect the first message
    in the original corpus and compare it to the same in the transformed corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As expected, uppercase letters in the clean corpus have been replaced with lowercase
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: The `content_transformer()` function can be used to apply more sophisticated
    text processing and cleanup processes like `grep` pattern matching and replacement.
    Simply write a custom function and wrap it before applying the `tm_map()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue our cleanup by removing numbers from the SMS messages. Although
    some numbers may provide useful information, the majority are likely to be unique
    to individual senders and thus will not provide useful patterns across all messages.
    With this in mind, we’ll strip all numbers from the corpus as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding code did not use the `content_transformer()` function.
    This is because `removeNumbers()` is included with `tm` along with several other
    mapping functions that do not need to be wrapped. To see the other built-in transformations,
    simply type `getTransformations()`.
  prefs: []
  type: TYPE_NORMAL
- en: Our next task is to remove filler words such as *to*, *and*, *but*, and *or*
    from the SMS messages. These terms are known as **stop words** and are typically
    removed prior to text mining. This is due to the fact that although they appear
    very frequently, they do not provide much useful information for our model as
    they are unlikely to distinguish between spam and ham.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than define a list of stop words ourselves, we’ll use the `stopwords()`
    function provided by the `tm` package. This function allows us to access sets
    of stop words from various languages. By default, common English language stop
    words are used. To see the default list, type `stopwords()` at the R command prompt.
    To see the other languages and options available, type `?stopwords` for the documentation
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Even within a single language, there is no single definitive list of stop words.
    For example, the default English list in `tm` includes about 174 words, while
    another option includes 571 words. You can even specify your own list of stop
    words. Regardless of the list you choose, keep in mind the goal of this transformation,
    which is to eliminate useless data while keeping as much useful information as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining the stop words alone is not a transformation. What we need is a way
    to remove any words that appear in the stop words list. The solution lies in the
    `removeWords()` function, which is a transformation included with the `tm` package.
    As we have done before, we’ll use the `tm_map()` function to apply this mapping
    to the data, providing the `stopwords()` function as a parameter to indicate the
    words we would like to remove. The full command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Since `stopwords()` simply returns a vector of stop words, if we had so chosen,
    we could have replaced this function call with our own vector of words to remove.
    In this way, we could expand or reduce the list of stop words to our liking or
    remove a different set of words entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing our cleanup process, we can also eliminate any punctuation from
    the text messages using the built-in `removePunctuation()` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `removePunctuation()` transformation completely strips punctuation characters
    from the text, which can lead to unintended consequences. For example, consider
    what happens when it is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As shown, the lack of a blank space after the ellipses caused the words *hello*
    and *world* to be joined as a single word. While this is not a substantial problem
    right now, it is worth noting for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work around the default behavior of `removePunctuation()`, it is possible
    to create a custom function that replaces rather than removes punctuation characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This uses R’s `gsub()` function to substitute any punctuation characters in
    `x` with a blank space. This `replacePunctuation()` function can then be used
    with `tm_map()` as with other transformations. The odd syntax of the `gsub()`
    command here is due to the use of a **regular expression**, which specifies a
    pattern that matches text characters. Regular expressions are covered in more
    detail in *Chapter 12*, *Advanced Data Preparation*.
  prefs: []
  type: TYPE_NORMAL
- en: Another common standardization for text data involves reducing words to their
    root form in a process called **stemming**. The stemming process takes words like
    *learned*, *learning*, and *learns* and strips the suffix in order to transform
    them into the base form, *learn*. This allows machine learning algorithms to treat
    the related terms as a single concept rather than attempting to learn a pattern
    for each variant.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package provides stemming functionality via integration with the `SnowballC`
    package. At the time of writing, `SnowballC` is not installed by default with
    `tm`, so do so with `install.packages("SnowballC")` if you have not done so already.
  prefs: []
  type: TYPE_NORMAL
- en: The `SnowballC` package is maintained by Milan Bouchet-Valat and provides an
    R interface for the C-based `libstemmer` library, itself based on M. F. Porter’s
    “Snowball” word-stemming algorithm, a widely used open-source stemming method.
    For more details, see [http://snowballstem.org](http://snowballstem.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SnowballC` package provides a `wordStem()` function, which for a character
    vector returns the same vector of terms in its root form. For example, the function
    correctly stems the variants of the word *learn* as described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the `wordStem()` function to an entire corpus of text documents, the
    `tm` package includes a `stemDocument()` transformation. We apply this to our
    corpus with the `tm_map()` function exactly as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If you receive an error message when applying the `stemDocument()` transformation,
    please confirm that you have the `SnowballC` package installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'After removing numbers, stop words, and punctuation, then also performing stemming,
    the text messages are left with the blank spaces that once separated the now-missing
    pieces. Therefore, the final step in our text cleanup process is to remove additional
    whitespace using the built-in `stripWhitespace()` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the first three messages in the SMS corpus before
    and after the cleaning process. The messages have been limited to the most interesting
    words, and punctuation and capitalization have been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **SMS messages before cleaning** | **SMS messages after cleaning** |'
  prefs: []
  type: TYPE_TB
- en: '| `> as.character(sms_corpus[1:3])``[[1]] Hope you are having a good``week.
    Just checking in``[[2]] K..give back my thanks.``[[3]] Am also doing in cbe only.``But
    have to pay.` | `> as.character(sms_corpus_clean[1:3])``[[1]] hope good week just
    check``[[2]] kgive back thank``[[3]] also cbe pay` |'
  prefs: []
  type: TYPE_TB
- en: Data preparation – splitting text documents into words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the data is processed to our liking, the final step is to split the
    messages into individual terms through a process called **tokenization**. A token
    is a single element of a text string; in this case, the tokens are words.
  prefs: []
  type: TYPE_NORMAL
- en: As you might assume, the `tm` package provides functionality to tokenize the
    SMS message corpus. The `DocumentTermMatrix()` function takes a corpus and creates
    a data structure called a **document-term matrix** (**DTM**) in which rows indicate
    documents (SMS messages) and columns indicate terms (words).
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package also provides a data structure for a **term-document matrix**
    (**TDM**), which is simply a transposed DTM in which the rows are terms and the
    columns are documents. Why the need for both? Sometimes, it is more convenient
    to work with one or the other. For example, if the number of documents is small,
    while the word list is large, it may make sense to use a TDM because it is usually
    easier to display many rows than to display many columns. That said, machine learning
    algorithms will generally require a DTM, as the columns are the features and the
    rows are the examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell in the matrix stores a number indicating a count of the times the
    word represented by the column appears in the document represented by the row.
    The following figure depicts only a small portion of the DTM for the SMS corpus,
    as the complete matrix has 5,559 rows and over 7,000 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: The DTM for the SMS messages is filled with mostly zeros'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that each cell in the table is zero implies that none of the words
    listed at the top of the columns appear in any of the first five messages in the
    corpus. This highlights the reason why this data structure is called a **sparse
    matrix**; the vast majority of cells in the matrix are filled with zeros. Stated
    in real-world terms, although each message must contain at least one word, the
    probability of any one word appearing in a given message is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a DTM sparse matrix from a *tm* corpus involves a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This will create an `sms_dtm` object that contains the tokenized corpus using
    the default settings, which apply minimal additional processing. The default settings
    are appropriate because we have already prepared the corpus manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we hadn’t already performed the preprocessing, we could
    do so here by providing a list of `control` parameter options to override the
    defaults. For example, to create a DTM directly from the raw, unprocessed SMS
    corpus, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This applies the same preprocessing steps to the SMS corpus in the same order
    as done earlier. However, comparing `sms_dtm` to `sms_dtm2`, we see a slight difference
    in the number of terms in the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this discrepancy has to do with a minor difference in the ordering
    of the preprocessing steps. The `DocumentTermMatrix()` function applies its cleanup
    functions to the text strings only after they have been split apart into words.
    Thus, it uses a slightly different stop word removal function. Consequently, some
    words are split differently than when they are cleaned before tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To force the two prior DTMs to be identical, we can override the default stop
    words function with our own that uses the original replacement function. Simply
    replace `stopwords = TRUE` with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The code file for this chapter includes the full set of steps to create an identical
    DTM using a single function call.
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences between these bring up an important principle of cleaning text
    data: the order of operations matters. With this in mind, it is very important
    to think through how early steps in the process are going to affect later ones.
    The order presented here will work in many cases, but when the process is tailored
    more carefully to specific datasets and use cases, it may require rethinking.
    For example, if there are certain terms you hope to exclude from the matrix, consider
    whether to search for them before or after stemming. Also, consider how the removal
    of punctuation—and whether the punctuation is eliminated or replaced by blank
    space—affects these steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating training and test datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our data prepared for analysis, we now need to split the data into training
    and test datasets so that after our spam classifier is built, it can be evaluated
    on data it has not previously seen. However, even though we need to keep the classifier
    blinded as to the contents of the test dataset, it is important that the split
    occurs after the data has been cleaned and processed. We need exactly the same
    preparation steps to have occurred on both the training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll divide the data into two portions: 75 percent for training and 25 percent
    for testing. Since the SMS messages are sorted in a random order, we can simply
    take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully,
    the DTM object acts very much like a data frame and can be split using the standard
    `[row, col]` operations. As our DTM stores SMS messages as rows and words as columns,
    we must request a specific range of rows and all columns for each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience later, it is also helpful to save a pair of vectors with the
    labels for each of the rows in the training and testing matrices. These labels
    are not stored in the DTM, so we need to pull them from the original `sms_raw`
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the subsets are representative of the complete set of SMS data,
    let’s compare the proportion of spam in the training and test data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Both the training data and test data contain about 13 percent spam. This suggests
    that the spam messages were divided evenly between the two datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing text data – word clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **word cloud** is a way to visually depict the frequency at which words appear
    in text data. The cloud is composed of words scattered somewhat randomly around
    the figure. Words appearing more often in the text are shown in a larger font,
    while less common terms are shown in smaller fonts. This type of figure grew in
    popularity as a way to observe trending topics on social media websites.
  prefs: []
  type: TYPE_NORMAL
- en: The `wordcloud` package provides a simple R function to create this type of
    diagram. We’ll use it to visualize the words in SMS messages. Comparing the clouds
    for spam and ham messages will help us gauge whether our Naive Bayes spam filter
    is likely to be successful. If you haven’t already done so, install and load the
    package by typing `install.packages("wordcloud")` and `library(wordcloud)` at
    the R command line.
  prefs: []
  type: TYPE_NORMAL
- en: The `wordcloud` package was written by Ian Fellows. For more information about
    this package, visit his blog at [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11).
  prefs: []
  type: TYPE_NORMAL
- en: 'A word cloud can be created directly from a *tm* corpus object using the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This will create a word cloud from our prepared SMS corpus. Since we specified
    `random.order = FALSE`, the cloud will be arranged in a non-random order, with
    higher-frequency words placed closer to the center. If we do not specify `random.order`,
    the cloud will be arranged randomly by default.
  prefs: []
  type: TYPE_NORMAL
- en: The `min.freq` parameter specifies the number of times a word must appear in
    the corpus before it will be displayed in the cloud. Since a frequency of 50 is
    about 1 percent of the corpus, this means that a word must be found in at least
    1 percent of the SMS messages to be included in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: You might get a warning message noting that R was unable to fit all the words
    in the figure. If so, try increasing the `min.freq` to reduce the number of words
    in the cloud. It might also help to use the `scale` parameter to reduce the font
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting word cloud should appear similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: A word cloud depicting words appearing in all SMS messages'
  prefs: []
  type: TYPE_NORMAL
- en: A perhaps more interesting visualization involves comparing the clouds for SMS
    spam and ham. Since we did not construct separate corpora for spam and ham, this
    is an appropriate time to note a very helpful feature of the `wordcloud()` function.
    Given a vector of raw text strings, it will automatically apply common text preparation
    processes before displaying the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use R’s `subset()` function to take a subset of the `sms_raw` data by
    the SMS type. First, we’ll create a subset where the `type` is `spam`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll do the same thing for the `ham` subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Be careful to note the double equals sign. Like many programming languages,
    R uses `==` to test equality. If you accidentally use a single equals sign, you’ll
    end up with a subset much larger than you expected!
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have two data frames, `spam` and `ham`, each with a `text` feature containing
    the raw text strings for SMS messages. Creating word clouds is as simple as before.
    This time, we’ll use the `max.words` parameter to look at the 40 most common words
    in each of the 2 sets. The `scale` parameter adjusts the maximum and minimum font
    sizes for words in the cloud. Feel free to change these parameters as you see
    fit. This is illustrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note that R provides warning messages when running this code that the “transformation
    drops documents.” The warnings are related to the `removePunctuation()` and `removeWords()`
    procedures that `wordcloud()` performs by default when given raw text data rather
    than a term matrix. Basically, there are some messages that are excluded from
    the result because there is no remaining message text after cleaning. For example,
    the ham message with the text *:)* representing the smiley emoji is removed from
    the set after cleaning. This is not a problem for the word clouds and the warnings
    can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting word clouds should appear similar to those that follow. Do you
    have a hunch on which one is the spam cloud, and which represents ham?
  prefs: []
  type: TYPE_NORMAL
- en: '![Text, letter  Description automatically generated](img/B17290_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Side-by-side word clouds depicting SMS spam and ham messages'
  prefs: []
  type: TYPE_NORMAL
- en: As you probably guessed, the spam cloud is on the left. Spam messages include
    words such as *call*, *free*, *mobile*, *claim*, and *stop*; these terms do not
    appear in the ham cloud at all. Instead, ham messages use words such as *can*,
    *sorry*, *love*, and *time*. These stark differences suggest that our Naive Bayes
    model will have some strong keywords to differentiate between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating indicator features for frequent words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in the data preparation process is to transform the sparse matrix
    into a data structure that can be used to train a Naive Bayes classifier. Currently,
    the sparse matrix includes over 6,500 features; this is a feature for every word
    that appears in at least one SMS message. It’s unlikely that all of these are
    useful for classification. To reduce the number of features, we’ll eliminate any
    word that appears in less than 5 messages, or in less than about 0.1 percent of
    records in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding frequent words requires the use of the `findFreqTerms()` function in
    the `tm` package. This function takes a DTM and returns a character vector containing
    words that appear at least a minimum number of times. For instance, the following
    command displays the words appearing at least five times in the `sms_dtm_train`
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the function is a character vector, so let’s save our frequent
    words for later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'A peek into the contents of the vector shows us that there are 1,139 terms
    appearing in at least 5 SMS messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to filter our DTM to include only the terms appearing in the frequent
    word vector. As before, we’ll use data frame-style `[row, col]` operations to
    request specific sections of the DTM, noting that the DTM column names are based
    on the words the DTM contains. We can take advantage of this fact to limit the
    DTM to specific words. Since we want all rows but only the columns representing
    the words in the `sms_freq_words` vector, our commands are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The training and test datasets now include 1,137 features, which correspond
    to words appearing in at least 5 messages.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is usually trained on data with categorical features.
    This poses a problem since the cells in the sparse matrix are numeric and measure
    the number of times a word appears in a message. We need to change this to a categorical
    variable that simply indicates yes or no, depending on whether the word appears
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following defines a `convert_counts()` function to convert counts into
    `Yes` or `No` strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: By now, some of the pieces of the preceding function should look familiar. The
    first line defines the function. The statement `ifelse(x > 0, "Yes", "No")` transforms
    the values in `x` such that if the value is greater than `0`, then it will be
    replaced with `"Yes"`; otherwise, it will be replaced with a `"No"` string. Lastly,
    the newly transformed vector `x` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: We now need to apply `convert_counts()` to each of the columns in our sparse
    matrix. You may be able to guess the name of the R function that does exactly
    this. The function is simply called `apply()` and is used much like `lapply()`
    was used previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apply()` function allows a function to be used on each of the rows or
    columns in a matrix. It uses a `MARGIN` parameter to specify either rows or columns.
    Here, we’ll use `MARGIN = 2` since we’re interested in the columns (`MARGIN =
    1` is used for rows). The commands to convert the training and test matrices are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The result will be two character-type matrices, each with cells indicating `"Yes"`
    or `"No"` for whether the word represented by the column appears at any point
    in the message represented by the row.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have transformed the raw SMS messages into a format that can be
    represented by a statistical model, it is time to apply the Naive Bayes algorithm.
    The algorithm will use the presence or absence of words to estimate the probability
    that a given SMS message is spam.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes implementation we will employ is in the `naivebayes` package.
    This package is maintained by Michal Majka and is a modern and efficient R implementation.
    If you have not done so already, be sure to install and load the package using
    the `install.packages("naivebayes")` and `library(naivebayes)` commands before
    continuing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many machine learning approaches are implemented in more than one R package,
    and Naive Bayes is no exception. Another option is `naiveBayes()` in the `e1071`
    package, which was used in older editions of this book but is otherwise nearly
    identical to `naive_bayes()` in usage. The `naivebayes` package used in this edition
    offers better performance and more advanced functionality, which is described
    at its website: [https://majkamichal.github.io/naivebayes/](https://majkamichal.github.io/naivebayes/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the k-NN algorithm we used for classification in the previous chapter,
    training a Naive Bayes learner and using it for classification occur in separate
    stages. Still, as shown in the following table, these steps are fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text, letter  Description automatically generated](img/B17290_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Naive Bayes classification syntax'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `sms_train` matrix, the following command trains a `naive_bayes`
    classifier object that can be used to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the previous command, you may notice the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This is nothing to be alarmed about for now; typing the `warnings()` command
    reveals the cause of this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: These warnings are caused by words that appeared in zero spam or zero ham messages
    and have veto power over the classification process due to their associated zero
    probabilities. For instance, because the word *accept* only appeared in ham messages
    in the training data, it does not mean that every future message with this word
    should be automatically classified as ham.
  prefs: []
  type: TYPE_NORMAL
- en: There is an easy solution to this problem using the Laplace estimator described
    earlier, but for now, we will evaluate this model using `laplace = 0`, which is
    the model’s default setting.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate the SMS classifier, we need to test its predictions on the unseen
    messages in the test data. Recall that the unseen message features are stored
    in a matrix named `sms_test`, while the class labels (spam or ham) are stored
    in a vector named `sms_test_labels`. The classifier that we trained has been named
    `sms_classifier`. We will use this classifier to generate predictions and then
    compare the predicted values to the true values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` function is used to make the predictions. We will store these
    in a vector named `sms_test_pred`. We simply supply this function with the names
    of our classifier and test dataset as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the predictions to the true values, we’ll use the `CrossTable()`
    function in the `gmodels` package, which we used in previous chapters. This time,
    we’ll add some additional parameters to eliminate unnecessary cell proportions,
    and use the `dnn` parameter (dimension names) to relabel the rows and columns
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the table, we can see that a total of only *6 + 30 = 36* of 1,390
    SMS messages were incorrectly classified (2.6 percent). Among the errors were
    6 out of 1,207 ham messages that were misidentified as spam and 30 of 183 spam
    messages that were incorrectly labeled as ham. Considering the little effort that
    we put into the project, this level of performance seems quite impressive. This
    case study exemplifies the reason why Naive Bayes is so often used for text classification:
    directly out of the box, it performs surprisingly well.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the six legitimate messages that were incorrectly classified
    as spam could cause significant problems for the deployment of our filtering algorithm
    because the filter could cause a person to miss an important text message. We
    should try to see whether we can slightly tweak the model to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may recall that we didn’t set a value for the Laplace estimator when training
    our model; in fact, it was hard to miss the message from R warning us about more
    than 50 features with zero probabilities! To address this issue, we’ll build a
    Naive Bayes model as before, but this time set `laplace = 1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll make predictions as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll compare the predicted classes to the actual classifications
    using cross-tabulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Adding a Laplace estimator by setting `laplace = 1` reduced the number of false
    positives (ham messages erroneously classified as spam) from 6 to 5, and the number
    of false negatives from 30 to 28\. Although this seems like a small change, it’s
    substantial considering that the model’s accuracy was already quite impressive.
    We’d need to be careful before tweaking the model too much more, as it is important
    to maintain a balance between being overly aggressive and overly passive when
    filtering spam. Users prefer that a small number of spam messages slip through
    the filter rather than the alternative, in which ham messages are filtered too
    aggressively.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about classification using Naive Bayes. This algorithm
    constructs tables of probabilities that are used to estimate the likelihood that
    new examples belong to various classes. The probabilities are calculated using
    a formula known as Bayes’ theorem, which specifies how dependent events are related.
    Although Bayes’ theorem can be computationally expensive, a simplified version
    that makes so-called “naive” assumptions about the independence of features is
    capable of handling much larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is often used for text classification. To illustrate
    its effectiveness, we employed Naive Bayes on a classification task involving
    spam SMS messages. Preparing the text data for analysis required the use of specialized
    R packages for text processing and visualization. Ultimately, the model was able
    to classify over 97 percent of all the SMS messages correctly as spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine two more machine learning methods. Each
    performs classification by partitioning data into groups of similar values. As
    you will discover shortly, these methods are quite useful on their own. Yet, looking
    further ahead, these basic algorithms also serve as an important foundation for
    some of the most powerful machine learning methods known today, which will be
    introduced later in *Chapter 14*, *Building Better Learners*.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  prefs: []
  type: TYPE_IMG
