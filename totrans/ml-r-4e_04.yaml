- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Probabilistic Learning – Classification Using Naive Bayes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率学习 - 使用朴素贝叶斯进行分类
- en: When a meteorologist provides a weather forecast, precipitation is typically
    described with phrases like “70 percent chance of rain.” Such forecasts are known
    as probability of precipitation reports. Have you ever considered how they are
    calculated? It is a puzzling question because, in reality, it will either rain
    or not with absolute certainty.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当气象学家提供天气预报时，降水通常用“70%降雨可能性”这样的短语来描述。这类预报被称为降水概率报告。你有没有考虑过它们是如何计算的？这是一个令人困惑的问题，因为在现实中，要么下雨，要么不下雨，这是绝对确定的。
- en: Weather estimates are based on probabilistic methods, which are those concerned
    with describing uncertainty. They use data on past events to extrapolate future
    events. In the case of the weather, the chance of rain describes the proportion
    of prior days with similar atmospheric conditions on which precipitation occurred.
    A 70 percent chance of rain implies that in 7 out of 10 past cases with similar
    conditions, precipitation occurred somewhere in the area.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 天气估计基于概率方法，这些方法涉及描述不确定性。它们使用过去事件的数据来预测未来事件。在天气的情况下，降雨的可能性描述了在相似大气条件下发生降水的先前天数所占的比例。70%的降雨可能性意味着在10个过去类似条件下，有7个地方发生了降水。
- en: 'This chapter covers the Naive Bayes algorithm, which uses probabilities in
    much the same way as a weather forecast. While studying this method, you will
    learn about:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了朴素贝叶斯算法，它使用概率的方式与天气预报非常相似。在研究这种方法时，你将了解：
- en: Basic principles of probability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率的基本原理
- en: The specialized methods and data structures needed to analyze text data with
    R
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R分析文本数据所需的专业方法和数据结构
- en: How to employ Naive Bayes to build a **Short Message Service** (**SMS**) junk
    message filter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用朴素贝叶斯构建**短信服务**（**SMS**）垃圾信息过滤器
- en: If you’ve taken a statistics class before, some of the material in this chapter
    may be a review. Even so, it may be helpful to refresh your knowledge of probability.
    You will find out that these principles are the basis of how Naive Bayes got such
    a strange name.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前上过统计学课程，本章的一些材料可能对你来说是复习。即便如此，刷新你对概率的了解可能也有帮助。你会发现这些原则是朴素贝叶斯获得这样一个奇怪名称的基础。
- en: Understanding Naive Bayes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯
- en: The basic statistical ideas necessary to understand the Naive Bayes algorithm
    have existed for centuries. The technique descended from the work of the 18th-century
    mathematician Thomas Bayes, who developed foundational principles for describing
    the probability of events and how these probabilities should be revised in light
    of additional information. These principles formed the foundation for what are
    now known as **Bayesian methods**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯算法所需的基本统计思想已经存在了几个世纪。这项技术源于18世纪数学家托马斯·贝叶斯的工作，他开发了描述事件概率及其在额外信息的基础上如何修订的基础原则。这些原则构成了现在被称为**贝叶斯方法**的基础。
- en: We will cover these methods in greater detail later. For now, it suffices to
    say that a probability is a number between zero and one (or from 0 to 100 percent)
    that captures the chance that an event will occur in light of the available evidence.
    The lower the probability, the less likely the event is to occur. A probability
    of zero indicates that the event will definitely not occur, while a probability
    of one indicates that the event will occur with absolute certainty. Life’s most
    interesting events tend to be those with uncertain probability; estimating the
    chance that they will occur helps us make better decisions by revealing the most
    likely outcomes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后更详细地介绍这些方法。现在，只需说一个概率是一个介于零和一之间的数字（或从0到100%）即可，它捕捉了在现有证据的基础上事件发生的可能性。概率越低，事件发生的可能性越小。零概率表示事件肯定不会发生，而一概率表示事件将以绝对确定性发生。生活中最有趣的事件往往具有不确定的概率；估计它们发生的可能性有助于我们通过揭示最可能的结果来做出更好的决策。
- en: 'Classifiers based on Bayesian methods utilize training data to calculate the
    probability of each outcome based on the evidence provided by feature values.
    When the classifier is later applied to unlabeled data, it uses these calculated
    probabilities to predict the most likely class for the new example. It’s a simple
    idea, but it results in a method that can have results on par with more sophisticated
    algorithms. In fact, Bayesian classifiers have been used for:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯方法的分类器利用训练数据根据特征值提供的证据计算每个结果的概率。当分类器后来应用于未标记的数据时，它使用这些计算出的概率来预测新示例最可能的类别。这是一个简单的想法，但结果是一个可以与更复杂算法相媲美的方法。事实上，贝叶斯分类器已被用于：
- en: Text classification, such as junk email (spam) filtering
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，如垃圾邮件（垃圾邮件过滤）
- en: Intrusion or anomaly detection in computer networks
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机网络中的入侵或异常检测
- en: Diagnosing medical conditions given a set of observed symptoms
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一组观察到的症状诊断医疗状况
- en: Typically, Bayesian classifiers are best applied to problems for which the information
    from numerous attributes should be considered simultaneously to estimate the overall
    probability of an outcome. While many machine learning algorithms ignore features
    that have weak effects, Bayesian methods utilize all available evidence to subtly
    change the predictions. This implies that even if a large portion of features
    have relatively minor effects, their combined impact in a Bayesian model could
    be quite large.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，贝叶斯分类器最适合应用于需要同时考虑多个属性信息以估计结果整体概率的问题。虽然许多机器学习算法忽略了具有较弱影响特征，但贝叶斯方法利用所有可用证据微妙地改变预测。这意味着即使大部分特征的影响相对较小，但在贝叶斯模型中它们的综合影响可能相当大。
- en: Basic concepts of Bayesian methods
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯方法的基本概念
- en: Before jumping into the Naive Bayes algorithm, it’s worth spending some time
    defining the concepts that are used across Bayesian methods. Summarized in a single
    sentence, Bayesian probability theory is rooted in the idea that the estimated
    likelihood of an **event**, or potential outcome, should be based on the evidence
    at hand across multiple **trials**, or opportunities for the event to occur.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究朴素贝叶斯算法之前，花些时间定义贝叶斯方法中使用的概念是值得的。用一句话总结，贝叶斯概率理论根植于这样一个观点：估计一个 **事件** 或潜在结果的似然性应该基于多个
    **试验** 或事件发生机会的证据。
- en: 'The following table illustrates events and trials for several real-world outcomes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了几个现实世界结果的事件和试验：
- en: '| **Event** | **Trial** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **事件** | **试验** |'
- en: '| Heads result | A coin flip |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 正面朝上 | 抛硬币 |'
- en: '| Rainy weather | A single day (or another time period) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 雨天 | 单日（或另一个时间段） |'
- en: '| Message is spam | An incoming email message |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 消息是垃圾邮件 | 一封 incoming 电子邮件 |'
- en: '| Candidate becomes president | A presidential election |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 候选人成为总统 | 总统选举 |'
- en: '| Mortality | A hospital patient |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 死亡率 | 医院病人 |'
- en: '| Winning the lottery | A lottery ticket |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 中奖 | 一张彩票 |'
- en: Bayesian methods provide insights into how the probability of these events can
    be estimated from observed data. To see how, we’ll need to formalize our understanding
    of probability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法提供了从观察数据中估计这些事件概率的见解。为了了解这一点，我们需要形式化我们对概率的理解。
- en: Understanding probability
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解概率
- en: The probability of an event is estimated from observed data by dividing the
    number of trials in which the event occurred by the total number of trials. For
    instance, if it rained 3 out of 10 days with similar conditions as today, the
    probability of rain today can be estimated as *3 / 10 = 0.30* or 30 percent. Similarly,
    if 10 out of 50 prior email messages were spam, then the probability of any incoming
    message being spam can be estimated as *10 / 50 = 0.20* or 20 percent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将事件发生的试验次数除以总试验次数来估计事件的概率。例如，如果今天有类似条件的 10 天中有 3 天下雨，那么今天下雨的概率可以估计为 *3 / 10
    = 0.30* 或 30%。同样，如果 50 封之前的电子邮件中有 10 封是垃圾邮件，那么任何新收到的邮件是垃圾邮件的概率可以估计为 *10 / 50 =
    0.20* 或 20%。
- en: To denote these probabilities, we use notation in the form *P(A)*, which signifies
    the probability of event *A*. For example, *P(rain) = 0.30* to indicate a 30 percent
    chance of rain or *P(spam) = 0.20* to describe a 20 percent probability of an
    incoming message being spam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这些概率，我们使用形式为 *P(A)* 的符号，它表示事件 *A* 的概率。例如，*P(rain) = 0.30* 表示有 30% 的降雨概率，或
    *P(spam) = 0.20* 描述一个新收到的消息有 20% 的概率是垃圾邮件。
- en: Because a trial always results in some outcome happening, the probability of
    all possible outcomes of a trial must always sum to one. Thus, if the trial has
    exactly two outcomes and the outcomes cannot occur simultaneously, then knowing
    the probability of either outcome reveals the probability of the other. This is
    the case for many outcomes, such as heads or tails coin flips, or spam versus
    legitimate email messages, also known as “ham.” Using this principle, knowing
    that *P(spam) = 0.20* allows us to calculate *P(ham) = 1 – 0.20 = 0.80*. This
    only works because spam and ham are **mutually exclusive and exhaustive events**,
    which implies that they cannot occur at the same time and are the only possible
    outcomes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于试验总是导致某些结果发生，因此试验所有可能结果的概率总和必须始终为1。因此，如果试验恰好有两个结果且这些结果不能同时发生，那么知道任意一个结果发生的概率就可以揭示另一个结果发生的概率。这种情况适用于许多结果，例如硬币的正反面，或垃圾邮件与合法电子邮件（也称为“ham”），使用这个原理，知道*P(spam)
    = 0.20*可以让我们计算出*P(ham) = 1 – 0.20 = 0.80*。这仅适用于垃圾邮件和ham是**互斥且穷尽的事件**，这意味着它们不能同时发生，并且是唯一的可能结果。
- en: A single event cannot happen and not happen simultaneously. This means an event
    is always mutually exclusive and exhaustive with its **complement**, or the event
    comprising all other outcomes in which the event of interest does not happen.
    The complement of event *A* is typically denoted *A*^c or *A’*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 单个事件不能同时发生和未发生。这意味着事件总是与其**补集**互斥且穷尽，或者包含所有其他结果的补集，其中感兴趣的事件未发生。事件*A*的补集通常表示为*A*^c或*A’*。
- en: Additionally, the shorthand notation *P(A*^c*)* or *P(¬A)* can be used to denote
    the probability of event *A* not occurring. For example, the notation *P(¬spam)
    = 0.80* suggests that the probability of a message not being spam is 80%.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用简写符号*P(A*^c*)*或*P(¬A)*来表示事件*A*不发生的概率。例如，符号*P(¬spam) = 0.80*表示消息不是垃圾邮件的概率为80%。
- en: 'To illustrate events and their complements, it is often helpful to imagine
    a two-dimensional space that is partitioned into probabilities for each event.
    In the following diagram, the rectangle represents the possible outcomes for an
    email message. The circle represents the 20 percent probability that the message
    is spam. The remaining 80 percent represents the complement *P(¬spam)*, or the
    messages that are not spam:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明事件及其补集，想象一个二维空间，该空间被划分为每个事件的概率，通常是有帮助的。在以下图中，矩形代表电子邮件消息的可能结果。圆圈代表消息是垃圾邮件的20%概率。剩余的80%代表补集*P(¬spam)*，或不是垃圾邮件的消息：
- en: '![Diagram  Description automatically generated](img/B17290_04_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_01.png)'
- en: 'Figure 4.1: The probability space for all emails can be visualized as partitions
    of spam and ham'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：所有电子邮件的概率空间可以表示为垃圾邮件和正常邮件的分区
- en: Understanding joint probability
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解联合概率
- en: 'Often, we are interested in monitoring several non-mutually exclusive events
    in the same trial. If certain events occur concurrently with the event of interest,
    we may be able to use them to make predictions. Consider, for instance, a second
    event based on the outcome that an email message contains the word *Viagra*. The
    preceding diagram, updated for this second event, might appear as shown in the
    following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在同一试验中会对几个非互斥事件进行监控。如果某些事件与感兴趣的事件同时发生，我们可能能够利用它们进行预测。例如，考虑一个基于电子邮件消息包含单词*Viagra*的结果的第二个事件。更新此第二个事件的先前列表可能如下所示：
- en: '![Diagram  Description automatically generated](img/B17290_04_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_02.png)'
- en: 'Figure 4.2: Non-mutually exclusive events are depicted as overlapping partitions'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：非互斥事件表示为重叠的分区
- en: Notice in the diagram that the Viagra circle overlaps with both the spam and
    ham areas of the diagram and the spam circle includes an area not covered by the
    Viagra circle. This implies that not all spam messages contain the term Viagra
    and some messages with the term Viagra are ham. However, because this word appears
    very rarely outside spam, its presence in a new incoming message would be strong
    evidence that the message is spam.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在图中，Viagra圆圈与图中的垃圾邮件和ham区域重叠，并且垃圾邮件圆圈包括Viagra圆圈未覆盖的区域。这表明并非所有垃圾邮件都包含Viagra这个词，并且一些包含Viagra的消息是ham。然而，由于这个词在垃圾邮件之外出现得非常少，它在新的传入消息中的出现将是该消息是垃圾邮件的强烈证据。
- en: 'To zoom in for a closer look at the overlap between these circles, we’ll employ
    a visualization known as a **Venn diagram**. First used in the late 19th century
    by mathematician John Venn, the diagram uses circles to illustrate the overlap
    between sets of items. As in most Venn diagrams, the size and degree of overlap
    of the circles in the depiction is not meaningful. Instead, it is used as a reminder
    to allocate probability to all combinations of events. A Venn diagram for spam
    and Viagra might be depicted as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地观察这两个圆之间的重叠，我们将使用一种称为**维恩图**的可视化方法。这种图最早在19世纪末由数学家约翰·文恩使用，它使用圆来表示项目集合的重叠。与大多数维恩图一样，图中圆的大小和重叠程度没有意义。相反，它被用作提醒，将概率分配给所有事件组合。垃圾邮件和Viagra的维恩图可能如下所示：
- en: '![Diagram  Description automatically generated](img/B17290_04_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_03.png)'
- en: 'Figure 4.3: A Venn diagram illustrates the overlap of the spam and Viagra events'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：一个维恩图展示了垃圾邮件和Viagra事件的交集
- en: We know that 20 percent of all messages were spam (the left circle), and 5 percent
    of all messages contained the word *Viagra* (the right circle). We would like
    to quantify the degree of overlap between these two proportions. In other words,
    we hope to estimate the probability that both *P(spam)* and *P(Viagra)* occur,
    which can be written as *P(spam ![](img/B17290_04_001.png) Viagra)*. The *![](img/B17290_04_001.png)*
    symbol signifies the intersection of the two events; the notation *A ![](img/B17290_04_001.png)
    B* refers to the event in which both *A* and *B* occur.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道所有消息中有20%是垃圾邮件（左边的圆），所有消息中有5%包含*Viagra*这个词（右边的圆）。我们希望量化这两个比例之间的重叠程度。换句话说，我们希望估计*P(spam)*和*P(Viagra)*同时发生的概率，这可以写成*P(spam
    ![图片](img/B17290_04_001.png) Viagra)*。*![图片](img/B17290_04_001.png)*符号表示两个事件的交集；*A
    ![图片](img/B17290_04_001.png) B*的表示法指的是*A*和*B*同时发生的事件。
- en: Calculating *P(spam ![](img/B17290_04_001.png) Viagra)* depends on the **joint
    probability** of the two events, or how the probability of one event is related
    to the probability of the other. If the two events are totally unrelated, they
    are called **independent events**. This is not to say that independent events
    cannot occur at the same time; event independence simply implies that knowing
    the outcome of one event does not provide any information about the outcome of
    the other. For instance, the outcome of a heads result on a coin flip is independent
    of whether the weather is rainy or sunny on any given day.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*P(spam ![图片](img/B17290_04_001.png) Viagra)*取决于两个事件的**联合概率**，即一个事件的概率如何与另一个事件的概率相关。如果两个事件完全无关，它们被称为**独立事件**。这并不是说独立事件不能同时发生；事件独立性仅仅意味着知道一个事件的结局不会提供任何关于另一个事件结局的信息。例如，抛硬币得到正面结果的结局与某一天是雨天还是晴天无关。
- en: If all events were independent, it would be impossible to predict one event
    by observing another. In other words, **dependent events** are the basis of predictive
    modeling. Just as the presence of clouds is predictive of a rainy day, the appearance
    of the word *Viagra* is predictive of a spam email.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有事件都是独立的，那么通过观察另一个事件来预测一个事件将是不可能的。换句话说，**相关事件**是预测建模的基础。就像云的存在预示着雨天一样，*Viagra*这个词的出现预示着垃圾邮件。
- en: '![](img/B17290_04_04.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_04.png)'
- en: 'Figure 4.4: Dependent events are required for machines to learn how to identify
    useful patterns'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：机器学习如何识别有用模式需要相关事件
- en: Calculating the probability of dependent events is a bit more complex than for
    independent events. If *P(spam)* and *P(Viagra)* were independent, we could easily
    calculate *P(spam ![](img/B17290_04_001.png) Viagra)*, the probability of both
    events happening at the same time. Because 20 percent of all messages are spam,
    and 5 percent of all emails contain the word *Viagra*, we could assume that 1
    percent of all messages with the term *Viagra* are spam. This is because *0.05
    * 0.20 = 0.01*. More generally, for independent events *A* and *B*, the probability
    of both happening can be computed as *P(A ![](img/B17290_04_001.png) B) = P(A)
    * P(B)*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相关事件的概率比独立事件要复杂一些。如果*P(spam)*和*P(Viagra)*是独立的，我们可以轻松地计算出*P(spam ![图片](img/B17290_04_001.png)
    Viagra)*，即两个事件同时发生的概率。因为所有消息中有20%是垃圾邮件，所有邮件中有5%包含*Viagra*这个词，我们可以假设所有包含*Viagra*这个词的消息中有1%是垃圾邮件。这是因为*0.05
    * 0.20 = 0.01*。更普遍地，对于独立事件*A*和*B*，两个事件同时发生的概率可以计算为*P(A ![图片](img/B17290_04_001.png)
    B) = P(A) * P(B)*。
- en: That said, we know that *P(spam)* and *P(Viagra)* are likely to be highly dependent,
    which means that this calculation is incorrect. To obtain a more reasonable estimate,
    we need to use a more careful formulation of the relationship between these two
    events, which is based on more advanced Bayesian methods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们知道*P(spam)*和*P(Viagra)*很可能高度相关，这意味着这个计算是不正确的。为了得到一个更合理的估计，我们需要使用这两个事件之间关系的更谨慎的公式，这个公式基于更先进的贝叶斯方法。
- en: Computing conditional probability with Bayes’ theorem
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理计算条件概率
- en: 'The relationships between dependent events can be described using **Bayes’
    theorem**, which provides a way of thinking about how to revise an estimate of
    the probability of one event in light of the evidence provided by another. One
    formulation is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用**贝叶斯定理**来描述相关事件之间的关系，它提供了一种思考如何根据另一个事件提供的证据来修订一个事件概率估计的方法。一种公式如下：
- en: '![](img/B17290_04_008.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_008.png)'
- en: The notation *P(A|B)* is read as the probability of event *A* given that event
    *B* occurred. This is known as **conditional probability** since the probability
    of *A* is dependent (that is, conditional) on what happened with event *B*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 符号*P(A|B)*读作事件*B*发生的情况下事件*A*的概率。这被称为**条件概率**，因为*A*的概率依赖于（即条件于）事件*B*的发生。
- en: Bayes’ theorem states that the best estimate of *P(A|B)* is the proportion of
    trials in which *A* occurred with *B*, out of all the trials in which *B* occurred.
    This implies that the probability of event *A* is higher if *A* and *B* often
    occur together each time *B* is observed. Note that this formula adjusts *P(A
    ![](img/B17290_04_001.png) B)* for the probability of *B* occurring. If *B* is
    extremely rare, *P(B)* and *P(A ![](img/B17290_04_001.png) B)* will always be
    small; however, if *A* almost always happens together with *B*, *P(A|B)* will
    still be high in spite of *B*’s rarity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理表明，*P(A|B)*的最佳估计是在所有发生事件*B*的试验中，事件*A*发生的试验比例。这意味着如果每次观察到*B*时*A*和*B*经常一起发生，事件*A*的概率就会更高。请注意，这个公式调整了*P(A
    ![图片](img/B17290_04_001.png) B)*以反映*B*发生的概率。如果*B*非常罕见，*P(B)*和*P(A ![图片](img/B17290_04_001.png)
    B)*将始终很小；然而，如果*A*几乎总是与*B*一起发生，尽管*B*很罕见，*P(A|B)*仍然会很高。
- en: 'By definition, *P(A ![](img/B17290_04_001.png) B) = P(A|B) * P(B)*, a fact
    that can be easily derived by applying a bit of algebra to the previous formula.
    Rearranging this formula once more with the knowledge that *P(A ![](img/B17290_04_001.png)
    B) = P(B ![](img/B17290_04_001.png) A)* results in the conclusion that *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, which we can then use in the following formulation of Bayes’
    theorem:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，*P(A ![图片](img/B17290_04_001.png) B) = P(A|B) * P(B)*，这是一个可以通过对先前公式应用一点代数轻松推导出的事实。利用*P(A
    ![图片](img/B17290_04_001.png) B) = P(B ![图片](img/B17290_04_001.png) A)*的知识重新排列这个公式，我们得出结论，*P(A
    ![图片](img/B17290_04_001.png) B) = P(B|A) * P(A)*，我们可以在贝叶斯定理的以下公式中使用它：
- en: '![](img/B17290_04_016.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_016.png)'
- en: In fact, this is the traditional formulation of Bayes’ theorem for reasons that
    will become clear as we apply it to machine learning. First, to better understand
    how Bayes’ theorem works in practice, let’s revisit our hypothetical spam filter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这是基于我们将它应用于机器学习时将变得清晰的原因的传统贝叶斯定理公式。首先，为了更好地理解贝叶斯定理在实际中的工作原理，让我们回顾一下我们的假设性垃圾邮件过滤器。
- en: Without knowledge of an incoming message’s content, the best estimate of its
    spam status would be *P(spam)*, the probability that any prior message was spam.
    This estimate is known as the **prior probability**. We found this previously
    to be 20 percent.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道收到的邮件内容的情况下，对其是否为垃圾邮件的最佳估计将是*P(spam)*，即任何先前邮件是垃圾邮件的概率。这个估计被称为**先验概率**。我们之前发现这个概率是20%。
- en: Suppose that you obtained additional evidence by looking more carefully at the
    set of previously received messages and examining the frequency with which the
    term *Viagra* appeared. The probability that the word *Viagra* was used in previous
    spam messages, or *P(Viagra|spam)*, is called the **likelihood**. The probability
    that *Viagra* appeared in any message at all, or *P(Viagra)*, is known as the
    **marginal likelihood**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你通过更仔细地查看先前收到的邮件集并检查“Viagra”一词出现的频率获得了额外的证据。单词*Viagra*在先前垃圾邮件中被使用的概率，或*P(Viagra|spam)*，被称为**似然性**。*Viagra*在任何邮件中出现的概率，或*P(Viagra)*，被称为**边缘似然性**。
- en: 'By applying Bayes’ theorem to this evidence, we can compute a **posterior probability**
    that measures how likely a message is to be spam. If the posterior probability
    is greater than 50 percent, the message is more likely to be spam than ham, and
    it should perhaps be filtered. The following formula shows how Bayes’ theorem
    is applied to the evidence provided by previous email messages:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将贝叶斯定理应用于这一证据，我们可以计算一个**后验概率**，该概率衡量一条消息是垃圾邮件的可能性。如果后验概率大于50%，则该消息更有可能是垃圾邮件而不是正常邮件，可能需要过滤。以下公式显示了贝叶斯定理是如何应用于先前电子邮件消息提供的证据的：
- en: '![Shape  Description automatically generated with medium confidence](img/B17290_04_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![形状描述自动生成，置信度中等](img/B17290_04_05.png)'
- en: 'Figure 4.5: Bayes’ theorem acting on previously received emails'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：贝叶斯定理作用于先前收到的电子邮件
- en: 'To calculate the components of Bayes’ theorem, it helps to construct a **frequency
    table** (shown on the left in the tables that follow) recording the number of
    times *Viagra* appeared in spam and ham messages. Just like a two-way cross-tabulation,
    one dimension of the table indicates levels of the class variable (spam or ham),
    while the other dimension indicates levels for features (Viagra: yes or no). The
    cells then indicate the number of instances that have the specified combination
    of the class value and feature value.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算贝叶斯定理的组成部分，构建一个**频率表**（如下表中左侧所示）记录*Viagra*在垃圾邮件和正常邮件中出现的次数是有帮助的。就像一个双向交叉表一样，表的其中一个维度表示类别变量（垃圾邮件或正常邮件）的水平，而另一个维度表示特征（Viagra：是或否）的水平。然后，单元格表示具有指定类别值和特征值的实例数量。
- en: The frequency table can then be used to construct a **likelihood table**, as
    shown on the right in the following tables. The rows of the likelihood table indicate
    the conditional probabilities for *Viagra* (yes/no), given that an email was either
    spam or ham.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用频率表来构建一个**似然表**，如下表中右侧所示。似然表的行表示在电子邮件是垃圾邮件或正常邮件的情况下，对于*Viagra*（是/否）的条件概率。
- en: '![Diagram  Description automatically generated](img/B17290_04_06.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17290_04_06.png)'
- en: 'Figure 4.6: Frequency and likelihood tables are the basis for computing the
    posterior probability of spam'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：频率和似然表是计算垃圾邮件后验概率的基础
- en: The likelihood table reveals that *P(Viagra=Yes|spam) = 4 / 20 = 0.20*, indicating
    that there is a 20 percent probability that a message contains the term *Viagra*
    given that the message is spam. Additionally, since *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, we can calculate *P(spam ![](img/B17290_04_001.png) Viagra)*
    as *P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04*. The same result
    can be found in the frequency table, which notes that 4 out of 100 messages were
    spam and contained the term *Viagra*. Either way, this is four times greater than
    the previous estimate of 0.01 we calculated as *P(A ![](img/B17290_04_001.png)
    B) = P(A) * P(B)* under the false assumption of independence. This, of course,
    illustrates the importance of Bayes’ theorem for estimating joint probability.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 似然表显示*P(Viagra=Yes|spam) = 4 / 20 = 0.20*，这表明如果一条消息是垃圾邮件，那么包含术语*Viagra*的概率是20%。此外，由于*P(A
    ![](img/B17290_04_001.png) B) = P(B|A) * P(A)*，我们可以计算*P(spam ![](img/B17290_04_001.png)
    Viagra)*为*P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04*。同样的结果可以在频率表中找到，该表指出100条消息中有4条是垃圾邮件并包含术语*Viagra*。无论如何，这比我们之前在错误假设独立性下计算的*P(A
    ![](img/B17290_04_001.png) B) = P(A) * P(B)*的估计值0.01高出四倍。这当然说明了贝叶斯定理在估计联合概率中的重要性。
- en: To compute the posterior probability, *P(spam|Viagra)*, we simply take *P(Viagra|spam)
    * P(spam) / P(Viagra)*, or *(4 / 20) * (20 / 100) / (5 / 100) = 0.80*. Therefore,
    the probability that a message is spam is 80 percent given that it contains the
    word *Viagra*. In light of this finding, any message containing this term should
    probably be filtered.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算后验概率，*P(spam|Viagra)*，我们只需取*P(Viagra|spam) * P(spam) / P(Viagra)*，或者*(4 /
    20) * (20 / 100) / (5 / 100) = 0.80*。因此，如果一条消息包含单词*Viagra*，那么这条消息是垃圾邮件的概率是80%。鉴于这一发现，任何包含此术语的消息可能应该被过滤。
- en: This is very much how commercial spam filters work, although they consider a
    much larger number of words simultaneously when computing the frequency and likelihood
    tables. In the next section, we’ll see how this method can be adapted to accommodate
    cases when additional features are involved.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是商业垃圾邮件过滤器的工作方式，尽管在计算频率和似然表时，它们会同时考虑更多的单词。在下一节中，我们将看到如何将这种方法适应涉及额外特征的情况。
- en: The Naive Bayes algorithm
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单贝叶斯算法
- en: 'The **Naive Bayes** algorithm defines a simple method for applying Bayes’ theorem
    to classification problems. Although it is not the only machine learning method
    that utilizes Bayesian methods, it is the most common. It grew in popularity due
    to its successes in text classification, where it was once the de facto standard.
    The strengths and weaknesses of this algorithm are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**算法定义了一种简单的方法，将贝叶斯定理应用于分类问题。尽管它不是唯一利用贝叶斯方法的机器学习方法，但它是最常见的。由于它在文本分类中的成功，朴素贝叶斯变得非常流行，一度成为事实上的标准。该算法的优点和缺点如下：'
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Simple, fast, and very effective
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单、快速且非常有效
- en: Does well with noisy and missing data and large numbers of features
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在有噪声和缺失数据以及大量特征的情况下表现良好
- en: Requires relatively few examples for training
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要相对较少的训练示例
- en: Easy to obtain the estimated probability for a prediction
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易获得预测的估计概率
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Relies on an often-faulty assumption of equally important and independent features
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于一个经常是错误的假设，即特征同等重要且相互独立
- en: Not ideal for datasets with many numeric features
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于具有许多数值特征的集合
- en: Estimated probabilities are less reliable than the predicted classes
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计的概率不如预测的类别可靠
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The Naive Bayes algorithm is named as such because it makes some so-called “naive”
    assumptions about the data. In particular, Naive Bayes assumes that all of the
    features in the dataset are **equally important and independent**. These assumptions
    are rarely true in most real-world applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法之所以被称为“朴素”，是因为它对数据做出了一些所谓的“朴素”假设。特别是，朴素贝叶斯假设数据集中的所有特征都是**同等重要且相互独立**的。在大多数实际应用中，这些假设很少是真实的。
- en: For example, when attempting to identify spam by monitoring email messages,
    it is almost certainly true that some features will be more important than others.
    For example, the email sender may be a more important indicator of spam than the
    message text. Additionally, the words in the message body are not independent
    of one another, since the appearance of some words is a very good indication that
    other words are also likely to appear. A message with the word *Viagra* will probably
    also contain the word *prescription* or *drugs*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当尝试通过监控电子邮件消息来识别垃圾邮件时，几乎可以肯定的是，某些特征将比其他特征更重要。例如，电子邮件发送者可能是比消息文本更重要的垃圾邮件指示器。此外，消息正文中的单词并不是相互独立的，因为某些单词的出现是其他单词也很可能出现的很好指示。包含“Viagra”一词的消息很可能也包含“prescription”或“drugs”一词。
- en: However, in most cases, even when these assumptions are violated, Naive Bayes
    still performs surprisingly well. This is true even in circumstances where strong
    dependencies are found among the features. Due to the algorithm’s versatility
    and accuracy across many types of conditions, particularly with smaller training
    datasets, Naive Bayes is often a reasonable baseline candidate for classification
    learning tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，即使这些假设被违反，朴素贝叶斯仍然表现出惊人的性能。即使在特征之间存在强依赖性的情况下，也是如此。由于该算法在各种条件下的灵活性和准确性，尤其是在较小的训练数据集上，朴素贝叶斯经常是分类学习任务的合理基线候选者。
- en: The exact reason why Naive Bayes works well in spite of its faulty assumptions
    has been the subject of much speculation. One explanation is that it is not important
    to obtain a precise estimate of probability so long as the predictions are accurate.
    For instance, if a spam filter correctly identifies spam, does it matter whether
    the predicted probability of spam was 51 percent or 99 percent? For one discussion
    of this topic, refer to *On the Optimality of the Simple Bayesian Classifier under
    Zero-One Loss, Domingos, P. and Pazzani, M., Machine Learning, 1997, Vol. 29,
    pp. 103-130*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管朴素贝叶斯存在错误的假设，但它为何表现良好的确切原因一直是许多猜测的对象。一种解释是，只要预测准确，获得概率的精确估计并不重要。例如，如果一个垃圾邮件过滤器正确地识别出垃圾邮件，那么预测垃圾邮件的概率是51%还是99%又有什么关系呢？关于这个话题的一个讨论，请参阅*《在零一损失下简单贝叶斯分类器的最优性》，作者：Domingos,
    P. 和 Pazzani, M.，机器学习，1997年，第29卷，第103-130页*。
- en: Classification with Naive Bayes
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: 'Let’s extend our spam filter by adding a few additional terms to be monitored
    in addition to the term *Viagra*: *money*, *groceries*, and *unsubscribe*. The
    Naive Bayes learner is trained by constructing a likelihood table for the appearance
    of these four words (labeled *W*¹, *W*², *W*³, and *W*⁴), as shown in the following
    diagram for 100 emails:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过添加一些额外的监控术语来扩展我们的垃圾邮件过滤器，除了术语*Viagra*之外，还包括*money*、*groceries*和*unsubscribe*。朴素贝叶斯学习器通过构建这四个词（标记为*W*¹、*W*²、*W*³和*W*⁴）出现的可能性表来训练，如下图中100封电子邮件所示：
- en: '![Table  Description automatically generated](img/B17290_04_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_04_07.png)'
- en: 'Figure 4.7: An expanded table adds likelihoods for additional terms in spam
    and ham messages'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：扩展的表格增加了垃圾邮件和正常邮件中额外术语的可能性
- en: As new messages are received, we need to calculate the posterior probability
    to determine whether they are more likely spam or ham, given the likelihood of
    the words being found in the message text. For example, suppose that a message
    contains the terms *Viagra* and *unsubscribe* but does not contain either *money*
    or *groceries*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到新消息时，我们需要计算后验概率，以确定它们更有可能是垃圾邮件还是正常邮件，给定在消息文本中找到这些词的可能性。例如，假设一条消息包含术语*Viagra*和*unsubscribe*，但不包含*money*或*groceries*。
- en: 'Using Bayes’ theorem, we can define the problem as shown in the following formula.
    This computes the probability that a message is spam given that *Viagra = Yes*,
    *Money = No*, *Groceries = No*, and *Unsubscribe = Yes*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们可以将问题定义为以下公式。这计算了在*Viagra = 是*、*Money = 否*、*Groceries = 否*和*Unsubscribe
    = 是*的条件下，一条消息是垃圾邮件的概率：
- en: '![](img/B17290_04_020.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_020.png)'
- en: For two reasons, this formula is computationally difficult to solve. First,
    as additional features are added, tremendous amounts of memory are needed to store
    the probabilities for all possible intersecting events. Imagine the complexity
    of a Venn diagram for the events for four words, let alone for hundreds or more.
    Second, many of these potential intersections will never have been observed in
    past data, which would lead to a joint probability of zero and problems that will
    become clear later.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个原因，这个公式在计算上很难解决。首先，随着附加特征的添加，需要大量的内存来存储所有可能交点事件的概率。想象一下四个词事件Venn图的复杂性，更不用说数百个或更多。其次，许多这些潜在的交点在过去的资料中从未被观察到，这会导致联合概率为零，并导致后面会变得明显的问题。
- en: The computation becomes more reasonable if we exploit the fact that Naive Bayes
    makes the naive assumption of independence among events. Specifically, it assumes
    **class-conditional independence**, which means that events are independent so
    long as they are conditioned on the same class value. The conditional independence
    assumption allows us to use the probability rule for independent events, which
    states that *P(A ![](img/B17290_04_001.png) B) = P(A) * P(B)*. This simplifies
    the numerator by allowing us to multiply the individual conditional probabilities
    rather than computing a complex conditional joint probability.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们利用朴素贝叶斯对事件之间独立性的朴素假设，计算将变得更加合理。具体来说，它假设**类条件独立性**，这意味着只要事件基于相同的类值，它们就是独立的。条件独立性假设允许我们使用独立事件的概率规则，该规则指出
    *P(A ![](img/B17290_04_001.png) B) = P(A) * P(B)*。这通过允许我们乘以单个条件概率而不是计算复杂的条件联合概率来简化分子。
- en: 'Lastly, because the denominator does not depend on the target class (spam or
    ham), it is treated as a constant value and can be ignored for the time being.
    This means that the conditional probability of spam can be expressed as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为分母不依赖于目标类（垃圾邮件或正常邮件），它被视为一个常数，暂时可以忽略。这意味着垃圾邮件的条件概率可以表示为：
- en: '![](img/B17290_04_022.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_022.png)'
- en: 'And the probability that the message is ham can be expressed as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以表示消息是正常邮件的概率为：
- en: '![](img/B17290_04_023.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_023.png)'
- en: Note that the equals symbol has been replaced by the proportional-to symbol
    (similar to a sideways, open-ended “8”) to indicate the fact that the denominator
    has been omitted.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，等号已被比例符号（类似于侧向的、开口的“8”）替换，以表明分母已被省略。
- en: 'Using the values in the likelihood table, we can start filling in numbers in
    these equations. The overall likelihood of spam is then:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可能性表中的值，我们可以开始填写这些方程中的数字。然后，垃圾邮件的整体可能性如下：
- en: '*(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012*'
- en: 'While the likelihood of ham is:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 而正常邮件的可能性是：
- en: '*(1 / 80) * (66 / 80) * (71 / 80) * (23 / 80) * (80 / 100) = 0.002*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1 / 80) * (66 / 80) * (71 / 80) * (23 / 80) * (80 / 100) = 0.002*'
- en: Because *0.012 / 0.002 = 6*, we can say that this message is 6 times more likely
    to be spam than ham. However, to convert these numbers into probabilities, we
    need one last step to reintroduce the denominator that has been excluded. Essentially,
    we must re-scale the likelihood of each outcome by dividing it by the total likelihood
    across all possible outcomes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*0.012 / 0.002 = 6*，我们可以说这条消息有6倍的可能性是垃圾邮件，而不是ham。然而，为了将这些数字转换为概率，我们需要最后一步重新引入之前排除的除数。本质上，我们必须通过除以所有可能结果的总似然值来重新缩放每个结果的似然值。
- en: 'In this way, the probability of spam is equal to the likelihood that the message
    is spam divided by the likelihood that the message is either spam or ham:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，垃圾邮件的概率等于消息是垃圾邮件的可能性除以消息是垃圾邮件或ham的可能性：
- en: '*0.012 / (0.012 + 0.002) = 0.857*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.012 / (0.012 + 0.002) = 0.857*'
- en: 'Similarly, the probability of ham is equal to the likelihood that the message
    is ham divided by the likelihood that the message is either spam or ham:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，ham的概率等于消息是ham的可能性除以消息是垃圾邮件或ham的可能性：
- en: '*0.002 / (0.012 + 0.002) = 0.143*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.002 / (0.012 + 0.002) = 0.143*'
- en: Given the pattern of words found in this message, we expect that the message
    is spam with an 85.7 percent probability, and ham with a 14.3 percent probability.
    Because these are mutually exclusive and exhaustive events, the probabilities
    sum up to 1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这条消息中找到的单词模式，我们预计这条消息有85.7%的概率是垃圾邮件，有14.3%的概率是ham。因为这些是互斥且穷尽的概率事件，所以它们的概率总和为1。
- en: 'The Naive Bayes classification algorithm used in the preceding example can
    be summarized by the following formula. The probability of level *L* for class
    *C*, given the evidence provided by features *F*[1] through *F*[n], is equal to
    the product of the probabilities of each piece of evidence conditioned on the
    class level, the prior probability of the class level, and a scaling factor *1
    / Z*, which converts the likelihood values into probabilities. This is formulated
    as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个例子中使用的朴素贝叶斯分类算法可以用以下公式总结。给定特征[F][1]到[F][n]提供的证据，对于类别*C*的*L*级概率等于每个证据在类别级条件下的概率乘积，类别级的先验概率，以及一个缩放因子*1
    / Z*，它将似然值转换为概率。这被公式化为：
- en: '![](img/B17290_04_024.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_024.png)'
- en: Although this equation seems intimidating, as the spam filtering example illustrated,
    the series of steps is fairly straightforward. Begin by building a frequency table,
    use this to build a likelihood table, and multiply out the conditional probabilities
    with the naive assumption of independence. Finally, divide by the total likelihood
    to transform each class likelihood into a probability. After attempting this calculation
    a few times by hand, it will become second nature.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个方程看起来令人畏惧，但正如垃圾邮件过滤示例所示，这一系列步骤相当简单。首先构建一个频率表，然后使用这个表构建一个似然表，并使用朴素假设的独立性乘出条件概率。最后，除以总似然值，将每个类别的似然值转换为概率。尝试手动计算几次后，这将成为第二本能。
- en: The Laplace estimator
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拉普拉斯估计器
- en: 'Before we employ Naive Bayes on more complex problems, there are some nuances
    to consider. Suppose we received another message, this time containing all four
    terms: *Viagra*, *groceries*, *money*, and *unsubscribe*. Using the Naive Bayes
    algorithm as before, we can compute the likelihood of spam as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用朴素贝叶斯解决更复杂的问题之前，有一些细微之处需要考虑。假设我们收到了另一条消息，这次包含所有四个术语：*Viagra*，*groceries*，*money*，和*unsubscribe*。使用之前的方法，我们可以计算垃圾邮件的可能性为：
- en: '*(4 / 20) * (10 / 20) * (0 / 20) * (12 / 20) * (20 / 100) = 0*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*(4 / 20) * (10 / 20) * (0 / 20) * (12 / 20) * (20 / 100) = 0*'
- en: 'And the likelihood of ham is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 并且ham的似然值为：
- en: '*(1 / 80) * (14 / 80) * (8 / 80) * (23 / 80) * (80 / 100) = 0.00005*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1 / 80) * (14 / 80) * (8 / 80) * (23 / 80) * (80 / 100) = 0.00005*'
- en: 'Therefore, the probability of spam is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，垃圾邮件的概率为：
- en: '*0 / (0 + 0.00005) = 0*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*0 / (0 + 0.00005) = 0*'
- en: 'And the probability of ham is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 并且ham的概率为：
- en: '*0.00005 / (0 + 0\. 0.00005) = 1*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.00005 / (0 + 0.00005) = 1*'
- en: These results suggest that the message is spam with 0 percent probability and
    ham with 100 percent probability. Does this prediction make sense? Probably not.
    The message contains several words usually associated with spam, including *Viagra*,
    which is rarely used in legitimate messages. It is therefore very likely that
    the message has been incorrectly classified.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，该消息有0%的概率是垃圾邮件，有100%的概率是正常邮件。这个预测有道理吗？可能没有。消息中包含了一些通常与垃圾邮件相关的词汇，包括*伟哥*，这在合法消息中很少使用。因此，该消息被错误分类的可能性非常大。
- en: This problem arises if an event never occurs for one or more levels of the class
    and therefore the resulting likelihoods are zero. For instance, the term *groceries*
    had never previously appeared in a spam message. Consequently, *P(groceries|spam)
    = 0%*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个事件在类的某个或多个级别上从未发生，那么由此产生的似然值将是零。例如，术语*杂货*以前从未出现在垃圾邮件中。因此，*P(groceries|spam)
    = 0%*。
- en: Now, because probabilities in the Naive Bayes formula are multiplied in a chain,
    this zero-percent value causes the posterior probability of spam to be zero, giving
    the word *groceries* the ability to effectively nullify and overrule all of the
    other evidence. Even if the email was otherwise overwhelmingly expected to be
    spam, the absence of the word *groceries* in spam will always veto the other evidence
    and result in the probability of spam being zero.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于朴素贝叶斯公式中的概率是链式相乘的，这个零值导致垃圾邮件的后验概率为零，使得单词*杂货*能够有效地抵消并推翻所有其他证据。即使电子邮件在其他方面几乎肯定会被认为是垃圾邮件，垃圾邮件中缺少单词*杂货*也会始终否决其他证据，导致垃圾邮件的概率为零。
- en: A solution to this problem involves using something called the **Laplace estimator**,
    which is named after the French mathematician Pierre-Simon Laplace. The Laplace
    estimator adds a small number to each of the counts in the frequency table, which
    ensures that each feature has a non-zero probability of occurring with each class.
    Typically, the Laplace estimator is set to one, which ensures that each class-feature
    combination is found in the data at least once.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法涉及使用一种称为**拉普拉斯估计器**的东西，该估计器是以法国数学家皮埃尔-西蒙·拉普拉斯的名字命名的。拉普拉斯估计器将一个小数加到频率表中的每个计数上，这确保了每个特征在每种类别中都有非零发生的概率。通常，拉普拉斯估计器设置为1，这确保每个类别-特征组合至少在数据中出现一次。
- en: The Laplace estimator can be set to any value and does not necessarily even
    have to be the same for each of the features. If you were a devoted Bayesian,
    you could use a Laplace estimator to reflect a presumed prior probability of how
    a feature relates to a class. In practice, given a large enough training dataset,
    this is excessive. Consequently, the value of one is almost always used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯估计器可以设置为任何值，并且不一定需要对每个特征都相同。如果你是一个虔诚的贝叶斯主义者，你可以使用拉普拉斯估计器来反映一个特征与类别相关的假设先验概率。在实践中，给定足够大的训练数据集，这通常是过度的。因此，值1几乎总是被使用。
- en: 'Let’s see how this affects our prediction for this message. Using a Laplace
    value of 1, we add 1 to each numerator in the likelihood function. Then, we need
    to add 4 to each conditional probability denominator to compensate for the 4 additional
    values added to the numerator. The likelihood of spam is therefore:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这如何影响我们对这条消息的预测。使用拉普拉斯值为1，我们在似然函数的每个分子上加上1。然后，我们需要在每个条件概率的分母上加上4，以补偿分子上增加的4个额外值。因此，垃圾邮件的可能性是：
- en: '*(5 / 24) * (11 / 24) * (1 / 24) * (13 / 24) * (20 / 100) = 0.0004*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*(5 / 24) * (11 / 24) * (1 / 24) * (13 / 24) * (20 / 100) = 0.0004*'
- en: 'And the likelihood of ham is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正常邮件的可能性是：
- en: '*(2 / 84) * (15 / 84) * (9 / 84) * (24 / 84) * (80 / 100) = 0.0001*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*(2 / 84) * (15 / 84) * (9 / 84) * (24 / 84) * (80 / 100) = 0.0001*'
- en: By computing *0.0004 / (0.0004 + 0.0001)*, we find that the probability of spam
    is 80 percent and therefore the probability of ham is about 20 percent. This is
    a more plausible result than the *P(spam) = 0* computed when the term *groceries*
    alone determined the result.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算*0.0004 / (0.0004 + 0.0001)*，我们发现垃圾邮件的概率是80%，因此正常邮件的概率大约是20%。这个结果比仅用术语*杂货*确定结果时计算的*P(spam)
    = 0*更合理。
- en: Although the Laplace estimator was added to the numerator and denominator of
    the likelihoods, it was not added to the prior probabilities—the values of 20/100
    and 80/100\. This is because our best estimate of the overall probability of spam
    and ham remains at 20% and 80% respectively given what was observed in the data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管拉普拉斯估计量被添加到了似然函数的分子和分母中，但它并没有被添加到先验概率中——即20/100和80/100的值。这是因为，根据数据中观察到的结果，我们对于垃圾邮件和正常邮件的整体概率的最佳估计仍然是20%和80%。
- en: Using numeric features with Naive Bayes
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用数值特征与简单贝叶斯
- en: Naive Bayes uses frequency tables for learning the data, which means that each
    feature must be categorical in order to create the combinations of class and feature
    values comprising the matrix. Since numeric features do not have categories of
    values, the preceding algorithm does not work directly with numeric data. There
    are, however, ways that this can be addressed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯使用频率表来学习数据，这意味着每个特征必须是分类的，以便创建由类和特征值组成的矩阵的组合。由于数值特征没有值类别，因此前面的算法不能直接处理数值数据。然而，有几种方法可以解决这个问题。
- en: One easy and effective solution is to **discretize** numeric features, which
    simply means that the numbers are put into categories known as **bins**. For this
    reason, discretization is also sometimes called **binning**. This method works
    best when there are large amounts of training data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单而有效的方法是将数值特征**离散化**，这仅仅意味着将数字放入称为**分组**的类别中。因此，离散化有时也被称为**分组**。这种方法在有大量训练数据时效果最佳。
- en: 'There are several different ways to discretize a numeric feature. Perhaps the
    most common is to explore the data for natural categories or **cut points** in
    the distribution. For example, suppose that you added a feature to the spam dataset
    that recorded the time of day or night the email was sent, from 0 to 24 hours
    past midnight. Depicted using a histogram, the time data might look something
    like the following diagram:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的方法可以对数值特征进行离散化。可能最常见的方法是探索数据中的自然类别或分布中的**切割点**。例如，假设你向垃圾邮件数据集中添加了一个特征，记录了邮件发送的白天或夜晚时间，从午夜过后的0到24小时。使用直方图表示，时间数据可能看起来像以下图表：
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_04_08.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图表，条形图，直方图 描述自动生成](img/B17290_04_08.png)'
- en: 'Figure 4.8: A histogram visualizing the distribution of the time emails were
    received'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：可视化接收电子邮件时间分布的直方图
- en: In the early hours of the morning, message frequency is low. Activity picks
    up during business hours and tapers off in the evening. This creates four natural
    bins of activity, as partitioned by the dashed lines. These indicate places where
    the numeric data could be divided into levels to create a new categorical feature,
    which could then be used with Naive Bayes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在清晨时分，消息频率较低。在办公时间活动增加，而在晚上逐渐减少。这形成了四个自然的活动分组，如图中虚线所示。这些表示可以将数值数据划分为不同级别以创建新的分类特征的地方，然后可以使用简单贝叶斯。
- en: The choice of four bins was based on the natural distribution of data and a
    hunch about how the proportion of spam might change throughout the day. We might
    expect that spammers operate in the late hours of the night, or they may operate
    during the day when people are likely to check their email. That said, to capture
    these trends, we could have just as easily used three bins or twelve.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 四个分组的选取是基于数据的自然分布以及对于一整天中垃圾邮件比例可能变化的直觉。我们可能预期垃圾邮件发送者会在深夜活动，或者他们可能在白天人们检查邮件时活动。尽管如此，为了捕捉这些趋势，我们同样可以使用三个或十二个分组。
- en: If there are no obvious cut points, one option is to discretize the feature
    using quantiles, which were introduced in *Chapter 2*, *Managing and Understanding
    Data*. You could divide the data into three bins with tertiles, four bins with
    quartiles, or five bins with quintiles.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有明显的切割点，一个选项是使用在*第2章*中介绍的量数对特征进行离散化。你可以用三分位数将数据分为三个分组，用四分位数分为四个分组，或者用五分位数分为五个分组。
- en: One thing to keep in mind is that discretizing a numeric feature always results
    in a reduction of information, as the feature’s original granularity is reduced
    to a smaller number of categories. It is important to strike a balance. Too few
    bins can result in important trends being obscured. Too many bins can result in
    small counts in the Naive Bayes frequency table, which can increase the algorithm’s
    sensitivity to noisy data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，将数值特征离散化总是会导致信息量的减少，因为特征的原有粒度被减少到更少的类别。重要的是要找到平衡点。分类太少可能导致重要趋势被掩盖。分类太多可能导致朴素贝叶斯频率表中的计数很小，这可能会增加算法对噪声数据的敏感性。
- en: Example – filtering mobile phone spam with the Naive Bayes algorithm
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用朴素贝叶斯算法过滤手机垃圾邮件
- en: As the worldwide use of mobile phones has grown, a new avenue for electronic
    junk mail has opened for disreputable marketers. These advertisers utilize SMS
    text messages to target potential consumers with unwanted advertising known as
    SMS spam. This type of spam is troublesome because, unlike email spam, an SMS
    message is particularly disruptive due to the omnipresence of one’s mobile phone.
    Developing a classification algorithm that could filter SMS spam would provide
    a useful tool for cellular phone providers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 随着全球手机使用的增长，不道德的营销人员开辟了一条新的电子垃圾邮件途径。这些广告商利用短信文本消息针对潜在消费者发送不受欢迎的广告，即短信垃圾邮件。这种垃圾邮件很麻烦，因为与电子邮件垃圾邮件不同，短信消息由于手机无处不在，特别具有破坏性。开发一个能够过滤短信垃圾邮件的分类算法将为手机服务提供商提供一个有用的工具。
- en: Since Naive Bayes has been used successfully for email spam filtering, it seems
    likely that it could also be applied to SMS spam. However, relative to email spam,
    SMS spam poses additional challenges for automated filters. SMS messages are often
    limited to 160 characters, reducing the amount of text that can be used to identify
    whether a message is junk. The limit, combined with small mobile phone keyboards,
    has led many to adopt a form of SMS shorthand lingo, which further blurs the line
    between legitimate messages and spam. Let’s see how a simple Naive Bayes classifier
    handles these challenges.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯分类器在电子邮件垃圾邮件过滤中已被成功应用，因此它似乎也可以应用于短信垃圾邮件。然而，相对于电子邮件垃圾邮件，短信垃圾邮件对自动过滤器提出了额外的挑战。短信消息通常限制在160个字符以内，这减少了用于识别消息是否为垃圾信息的文本量。这种限制，加上小型手机键盘，导致许多人采用一种短信缩写语，这进一步模糊了合法消息和垃圾邮件之间的界限。让我们看看简单的朴素贝叶斯分类器如何应对这些挑战。
- en: Step 1 – collecting data
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 收集数据
- en: To develop the Naive Bayes classifier, we will use data adapted from the SMS
    Spam Collection at [https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发朴素贝叶斯分类器，我们将使用从[https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)的短信垃圾邮件收集中改编的数据。
- en: To read more about how the SMS Spam Collection was developed, refer to *On the
    Validity of a New SMS Spam Collection, Gómez, J. M., Almeida, T. A., and Yamakami,
    A., Proceedings of the 11th IEEE International Conference on Machine Learning
    and Applications, 2012*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于短信垃圾邮件收集如何发展的信息，请参阅*关于新的短信垃圾邮件收集的有效性，Gómez, J. M.，Almeida, T. A.，和Yamakami,
    A.，2012年第11届IEEE国际机器学习与应用会议论文集*。
- en: 'This dataset includes the text of SMS messages, along with a label indicating
    whether the message is unwanted. Junk messages are labeled spam, while legitimate
    messages are labeled ham. Some examples of spam and ham are shown in the following
    table:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包括短信文本，以及一个标签，表示消息是否不受欢迎。垃圾消息被标记为垃圾邮件，而合法消息被标记为ham。以下表格显示了垃圾邮件和ham的一些示例：
- en: '| **Sample SMS ham** | **Sample SMS spam** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **样本短信垃圾信息** | **样本短信垃圾邮件** |'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel
    bleh. But at least its not writhing pain kind of bleh.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好。周五我补上了，昨天吃得像猪一样。现在感觉有点糟糕。但至少不是那种让人难以忍受的疼痛。
- en: If he started searching he will get job in few days. he have great potential
    and talent.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果他开始寻找，几天内就能找到工作。他有很大的潜力和才能。
- en: I got another job! The one at the hospital doing data analysis or something,
    starts on monday! Not sure when my thesis will got finished
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我又找到一份工作了！是在医院做数据分析之类的，周一开始！不确定我的论文什么时候能完成。
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry
    2 100 wkly draw txt MUSIC to 87066
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恭喜你，你获得了500元的CD代金券或125元的礼品保证，并且免费参加每周一次的抽奖活动，发送短信MUSIC到87066。
- en: December only! Had your mobile 11mths+? You are entitled to update to the latest
    colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 12月特惠！你的手机使用11个月以上了吗？你有权免费升级到最新的彩色摄像头手机！免费拨打08002986906至移动更新公司。
- en: Valentines Day Special! Win over £1000 in our quiz and take your partner on
    the trip of a lifetime! Send GO to 83600 now. 150p/msg rcvd.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情人节特别优惠！在我们的问答比赛中赢得1000英镑，并带你的伴侣去一次终身难忘的旅行！现在发送GO到83600。每条信息150便士。
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Looking at the preceding messages, do you notice any distinguishing characteristics
    of spam? One notable characteristic is that two of the three spam messages use
    the word *free*, yet this word does not appear in any of the ham messages. On
    the other hand, two of the ham messages cite specific days of the week, as compared
    to zero in spam messages.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的消息，你是否注意到垃圾邮件的任何显著特征？一个值得注意的特征是，三个垃圾邮件中有两个使用了单词*免费*，而这个词并没有出现在任何正常邮件中。另一方面，两条正常邮件提到了具体的星期几，而垃圾邮件中则没有。
- en: Our Naive Bayes classifier will take advantage of such patterns in the word
    frequency to determine whether the SMS messages seem to better fit the profile
    of spam or ham. While it’s not inconceivable that the word *free* would appear
    outside of a spam SMS, a legitimate message is likely to provide additional words
    giving context.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的朴素贝叶斯分类器将利用单词频率中的这些模式来确定短信消息似乎更适合垃圾邮件还是正常邮件。虽然不能排除单词*免费*出现在垃圾邮件之外的情况，但合法的消息更有可能提供额外的单词来提供上下文。
- en: For instance, a ham message might ask, “Are you free on Sunday?” whereas a spam
    message might use the phrase “free ringtones.” The classifier will compute the
    probability of spam and ham given the evidence provided by all the words in the
    message.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一条正常邮件可能会问：“你星期天有空吗？”而一条垃圾邮件可能会使用短语“免费铃声”。分类器将根据消息中所有单词提供的证据计算垃圾邮件和正常邮件的概率。
- en: Step 2 – exploring and preparing the data
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 - 探索和准备数据
- en: The first step toward constructing our classifier involves processing the raw
    data for analysis. Text data is challenging to prepare because it is necessary
    to transform the words and sentences into a form that a computer can understand.
    We will transform our SMS data into a representation known as **bag-of-words**,
    which provides a binary feature indicating whether each word appears in the given
    example while ignoring word order or the context in which the word appears. Although
    this is a relatively simple representation, as we will soon demonstrate, it performs
    well enough for many classification tasks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 构建我们的分类器第一步涉及对原始数据进行处理以进行分析。文本数据准备起来具有挑战性，因为需要将单词和句子转换成计算机可以理解的形式。我们将把我们的短信数据转换成一种称为**词袋**的表示形式，它提供了一种二元特征，表示每个单词是否出现在给定的示例中，同时忽略单词顺序或单词出现的上下文。尽管这是一种相对简单的表示形式，但正如我们很快将展示的，它对于许多分类任务来说已经足够好了。
- en: The dataset used here has been modified slightly from the original to make it
    easier to work with in R. If you plan on following along with the example, download
    the `sms_spam.csv` file from the Packt website and save it to your R working directory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的数据集已经从原始数据集稍作修改，以便更容易在R中使用。如果你打算跟随示例，请从Packt网站下载`sms_spam.csv`文件并将其保存到你的R工作目录中。
- en: 'We’ll begin by importing the CSV data and saving it to a data frame:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入CSV数据并将其保存到数据框中：
- en: '[PRE0]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the `str()` function, we see that the `sms_raw` data frame includes 5,559
    total SMS messages with two features: `type` and `text`. The SMS type has been
    coded as either ham or spam. The `text` element stores the full raw SMS message
    text:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`str()`函数，我们可以看到`sms_raw`数据框包括5559条总短信消息，具有两个特征：`type`和`text`。短信类型已被编码为ham或spam。`text`元素存储完整的原始短信消息文本：
- en: '[PRE1]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `type` element is currently a character vector. Since this is a categorical
    variable, it would be better to convert it into a factor, as shown in the following
    code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`type`元素目前是一个字符向量。由于这是一个分类变量，最好将其转换为因子，如下面的代码所示：'
- en: '[PRE3]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Examining this with the `str()` and `table()` functions, we see that `type`
    has now been appropriately recoded as a factor. Additionally, we see that 747
    (about 13 percent) of SMS messages in our data were labeled as `spam`, while the
    others were labeled as `ham`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`str()`和`table()`函数检查，我们发现`type`现在已被适当地重新编码为因子。此外，我们还可以看到在我们的数据中，有747条（大约13%）短信消息被标记为`spam`，其余的则被标记为`ham`：
- en: '[PRE4]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For now, we will leave the message text alone. As you will learn in the next
    section, processing raw SMS messages will require the use of a new set of powerful
    tools designed specifically for processing text data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将保持短信文本不变。您将在下一节中了解到，处理原始短信需要使用一套专门为处理文本数据设计的强大工具。
- en: Data preparation – cleaning and standardizing text data
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——清洗和标准化文本数据
- en: SMS messages are strings of text composed of words, spaces, numbers, and punctuation.
    Handling this type of complex data takes a large amount of thought and effort.
    One needs to consider how to remove numbers and punctuation; handle uninteresting
    words, such as *and*, *but*, and *or*; and break apart sentences into individual
    words. Thankfully, this functionality has been provided by members of the R community
    in a text-mining package titled `tm`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 短信消息是由单词、空格、数字和标点符号组成的文本字符串。处理这种复杂的数据类型需要大量的思考和努力。需要考虑如何删除数字和标点；处理无趣的词，例如*和*、*但*和*或*；以及将句子分解成单个单词。幸运的是，这种功能已经由R社区的成员在一个名为`tm`的文本挖掘包中提供。
- en: The `tm` package was originally created by Ingo Feinerer as a dissertation project
    at the Vienna University of Economics and Business. To learn more, see *Text Mining
    Infrastructure in R, Feinerer, I., Hornik, K., and Meyer, D., Journal of Statistical
    Software, 2008, Vol. 25, pp. 1-54*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包最初是由维也纳经济和商业大学的Ingo Feinerer作为学位论文项目创建的。要了解更多信息，请参阅*《R中的文本挖掘基础设施》，作者：Feinerer,
    I.，Hornik, K.和Meyer, D.，统计软件杂志，2008年，第25卷，第1-54页*。'
- en: The `tm` package can be installed via the `install.packages("tm")` command and
    loaded with the `library(tm)` command. Even if you already have it installed,
    it may be worth redoing the installation to ensure that your version is up to
    date, as the `tm` package is still under active development. This occasionally
    results in changes to its functionality.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`install.packages("tm")`命令安装`tm`包，并通过`library(tm)`命令加载。即使您已经安装了它，重新安装也可能值得，以确保您的版本是最新的，因为`tm`包仍在积极开发中。这偶尔会导致其功能发生变化。
- en: This chapter was tested using `tm` version 0.7-11, which was current as of May
    2023\. If you see differences in the output or if the code does not work, you
    may be using a different version. The Packt support page for this book, as well
    as its GitHub repository, will post solutions for future `tm` package versions
    if significant changes are noted.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用`tm`版本0.7-11进行了测试，截至2023年5月，这是当前版本。如果您看到输出有差异或代码无法工作，您可能使用的是不同版本。本书的Packt支持页面以及其GitHub仓库将发布针对未来`tm`包版本的解决方案，如果发现重大变化。
- en: The first step in processing text data involves creating a **corpus**, which
    is a collection of text documents. The documents can be short or long, from individual
    news articles, pages in a book, pages from the web, or even entire books. In our
    case, the corpus will be a collection of SMS messages.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据的第一个步骤是创建一个**语料库**，它是一组文本文档的集合。文档可以是短或长的，从单个新闻文章、书籍的页面、网页页面，甚至整本书。在我们的情况下，语料库将是一组短信。
- en: To create a corpus, we’ll use the `VCorpus()` function in the `tm` package,
    which refers to a volatile corpus—the term “volatile” meaning that it is stored
    in memory as opposed to being stored on disk (the `PCorpus()` function is used
    to access a permanent corpus stored in a database). This function requires us
    to specify the source of documents for the corpus, which could be a computer’s
    filesystem, a database, the web, or elsewhere.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个语料库，我们将使用`tm`包中的`VCorpus()`函数，它指的是一个易失性语料库——这里的“易失性”意味着它存储在内存中，而不是存储在磁盘上（使用`PCorpus()`函数来访问存储在数据库中的永久性语料库）。此函数要求我们指定语料库文档的来源，这可能是计算机的文件系统、数据库、网络或其他地方。
- en: 'Since we already loaded the SMS message text into R, we’ll use the `VectorSource()`
    reader function to create a source object from the existing `sms_raw$text` vector,
    which can then be supplied to `VCorpus()` as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在R中加载了短信文本，我们将使用`VectorSource()`读取器函数从现有的`sms_raw$text`向量创建一个源对象，然后可以将其如下提供给`VCorpus()`：
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resulting corpus object is saved with the name `sms_corpus`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的语料库对象以`sms_corpus`命名保存。
- en: By specifying an optional `readerControl` parameter, the `VCorpus()` function
    can be used to import text from sources such as PDFs and Microsoft Word files.
    To learn more, examine the *Data Import* section in the `tm` package vignette
    using the `vignette("tm")` command.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定可选的`readerControl`参数，`VCorpus()`函数可以用于从PDF和Microsoft Word文件等来源导入文本。要了解更多信息，请使用`vignette("tm")`命令检查`tm`包的*数据导入*部分。
- en: 'By printing the corpus, we see that it contains documents for each of the 5,559
    SMS messages in the training data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印语料库，我们看到它包含训练数据中5,559条短信消息的文档：
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, because the `tm` corpus is essentially a complex list, we can use list
    operations to select documents in the corpus. The `inspect()` function shows a
    summary of the result. For example, the following command will view a summary
    of the first and second SMS messages in the corpus:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为`tm`语料库本质上是一个复杂列表，我们可以使用列表操作来选择语料库中的文档。`inspect()`函数显示了结果的摘要。例如，以下命令将查看语料库中第一条和第二条短信消息的摘要：
- en: '[PRE11]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To view the actual message text, the `as.character()` function must be applied
    to the desired messages. To view one message, use the `as.character()` function
    on a single list element, noting that the double-bracket notation is required:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看实际的消息文本，必须对所需的消息应用`as.character()`函数。要查看一条消息，请在单个列表元素上使用`as.character()`函数，注意需要使用双括号表示法：
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To view multiple documents, we’ll need to apply `as.character()` to several
    items in the `sms_corpus` object. For this, we’ll use the `lapply()` function,
    which is part of a family of R functions that applies a procedure to each element
    of an R data structure. These functions, which include `apply()` and `sapply()`
    among others, are one of the key idioms of the R language. Experienced R coders
    use these much like the way `for` or `while` loops are used in other programming
    languages, as they result in more readable (and sometimes more efficient) code.
    The `lapply()` function for applying `as.character()` to a subset of corpus elements
    is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看多个文档，我们需要将`as.character()`应用于`sms_corpus`对象中的多个项。为此，我们将使用`lapply()`函数，它是R数据结构中应用程序的一个函数家族的一部分。这些函数，包括`apply()`和`sapply()`等，是R语言的关键习惯用法之一。经验丰富的R程序员使用这些函数的方式类似于在其他编程语言中使用`for`或`while`循环，因为它们会产生更易读（有时更高效）的代码。将`as.character()`应用于语料库元素子集的`lapply()`函数如下：
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As noted earlier, the corpus contains the raw text of 5,559 text messages. To
    perform our analysis, we need to divide these messages into individual words.
    First, we need to clean the text to standardize the words and remove punctuation
    characters that clutter the result. For example, we would like the strings *Hello!*,
    *HELLO*, and *hello* to be counted as instances of the same word.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，语料库包含5,559条文本消息的原始文本。为了进行我们的分析，我们需要将这些消息划分为单个单词。首先，我们需要清理文本以标准化单词并删除会干扰结果的标点符号。例如，我们希望字符串*Hello!*、*HELLO*和*hello*都被计为同一单词的实例。
- en: The `tm_map()` function provides a method to apply a transformation (also known
    as a mapping) to a `tm` corpus. We will use this function to clean up our corpus
    using a series of transformations and save the result in a new object called `corpus_clean`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm_map()`函数提供了一个将转换（也称为映射）应用于`tm`语料库的方法。我们将使用此函数通过一系列转换清理我们的语料库，并将结果保存在一个名为`corpus_clean`的新对象中。'
- en: 'Our first transformation will standardize the messages to use only lowercase
    characters. To this end, R provides a `tolower()` function that returns a lowercase
    version of text strings. In order to apply this function to the corpus, we need
    to use the `tm` wrapper function `content_transformer()` to treat `tolower()`
    as a transformation function that can be used to access the corpus. The full command
    is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步转换将标准化消息以仅使用小写字母。为此，R提供了一个`tolower()`函数，该函数返回文本字符串的小写版本。为了将此函数应用于语料库，我们需要使用`tm`包装函数`content_transformer()`将`tolower()`视为一个转换函数，该函数可以用于访问语料库。完整的命令如下：
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To check whether the command worked as expected, let’s inspect the first message
    in the original corpus and compare it to the same in the transformed corpus:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查命令是否按预期工作，让我们检查原始语料库中的第一条消息并将其与转换语料库中的相同消息进行比较：
- en: '[PRE18]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As expected, uppercase letters in the clean corpus have been replaced with lowercase
    versions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，清理后的语料库中的大写字母已被替换为小写版本。
- en: The `content_transformer()` function can be used to apply more sophisticated
    text processing and cleanup processes like `grep` pattern matching and replacement.
    Simply write a custom function and wrap it before applying the `tm_map()` function.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`content_transformer()` 函数可用于应用更复杂的文本处理和清理过程，例如 `grep` 模式匹配和替换。只需编写一个自定义函数，并在应用
    `tm_map()` 函数之前将其包装起来。'
- en: 'Let’s continue our cleanup by removing numbers from the SMS messages. Although
    some numbers may provide useful information, the majority are likely to be unique
    to individual senders and thus will not provide useful patterns across all messages.
    With this in mind, we’ll strip all numbers from the corpus as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续清理过程，通过从短信中移除数字。尽管一些数字可能提供有用信息，但大多数数字可能仅对个别发送者独特，因此不会在所有消息中提供有用的模式。考虑到这一点，我们将从语料库中移除所有数字，如下所示：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that the preceding code did not use the `content_transformer()` function.
    This is because `removeNumbers()` is included with `tm` along with several other
    mapping functions that do not need to be wrapped. To see the other built-in transformations,
    simply type `getTransformations()`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的代码没有使用 `content_transformer()` 函数。这是因为 `removeNumbers()` 与 `tm` 包一起提供，还包括其他不需要包装的映射函数。要查看其他内置转换，请简单地输入
    `getTransformations()`。
- en: Our next task is to remove filler words such as *to*, *and*, *but*, and *or*
    from the SMS messages. These terms are known as **stop words** and are typically
    removed prior to text mining. This is due to the fact that although they appear
    very frequently, they do not provide much useful information for our model as
    they are unlikely to distinguish between spam and ham.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的任务是移除短信中的填充词，如 *to*、*and*、*but* 和 *or*。这些术语被称为 **停用词**，通常在文本挖掘之前被移除。这是因为尽管它们出现频率很高，但它们对我们模型的有用信息不多，因为它们不太可能区分垃圾邮件和正常邮件。
- en: Rather than define a list of stop words ourselves, we’ll use the `stopwords()`
    function provided by the `tm` package. This function allows us to access sets
    of stop words from various languages. By default, common English language stop
    words are used. To see the default list, type `stopwords()` at the R command prompt.
    To see the other languages and options available, type `?stopwords` for the documentation
    page.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会自己定义停用词列表，而是使用 `tm` 包提供的 `stopwords()` 函数。此函数允许我们访问来自各种语言的停用词集合。默认情况下，使用的是常见的英语停用词。要在
    R 命令提示符中查看默认列表，请输入 `stopwords()`。要查看其他语言和选项，请输入 `?stopwords` 以获取文档页面。
- en: Even within a single language, there is no single definitive list of stop words.
    For example, the default English list in `tm` includes about 174 words, while
    another option includes 571 words. You can even specify your own list of stop
    words. Regardless of the list you choose, keep in mind the goal of this transformation,
    which is to eliminate useless data while keeping as much useful information as
    possible.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在单一语言中，也没有一个单一的、确定的停用词列表。例如，`tm` 中的默认英语列表包含大约 174 个单词，而另一个选项包含 571 个单词。你甚至可以指定自己的停用词列表。无论你选择哪个列表，都要记住这个转换的目标，即消除无用数据，同时尽可能保留有用信息。
- en: 'Defining the stop words alone is not a transformation. What we need is a way
    to remove any words that appear in the stop words list. The solution lies in the
    `removeWords()` function, which is a transformation included with the `tm` package.
    As we have done before, we’ll use the `tm_map()` function to apply this mapping
    to the data, providing the `stopwords()` function as a parameter to indicate the
    words we would like to remove. The full command is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 仅定义停用词本身不是一种转换。我们需要的是一种方法来移除任何出现在停用词列表中的单词。解决方案在于 `tm` 包中包含的 `removeWords()`
    函数。像之前一样，我们将使用 `tm_map()` 函数应用此映射到数据上，将 `stopwords()` 函数作为参数提供，以指示我们想要移除的单词。完整的命令如下：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Since `stopwords()` simply returns a vector of stop words, if we had so chosen,
    we could have replaced this function call with our own vector of words to remove.
    In this way, we could expand or reduce the list of stop words to our liking or
    remove a different set of words entirely.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `stopwords()` 仅返回一个停用词向量，如果我们选择这样做，我们可以用我们自己的单词移除向量替换此函数调用。这样，我们可以根据喜好扩展或减少停用词列表，或者完全移除不同的单词集。
- en: 'Continuing our cleanup process, we can also eliminate any punctuation from
    the text messages using the built-in `removePunctuation()` transformation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的清理过程，我们还可以使用内置的 `removePunctuation()` 转换从文本消息中消除任何标点符号：
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `removePunctuation()` transformation completely strips punctuation characters
    from the text, which can lead to unintended consequences. For example, consider
    what happens when it is applied as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`removePunctuation()` 转换会完全从文本中移除标点符号，这可能会导致意想不到的后果。例如，考虑以下应用方式会发生什么：'
- en: '[PRE25]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As shown, the lack of a blank space after the ellipses caused the words *hello*
    and *world* to be joined as a single word. While this is not a substantial problem
    right now, it is worth noting for the future.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，省略号后面的空白缺失导致单词 *hello* 和 *world* 被合并为一个单词。虽然这目前不是一个重大问题，但值得注意。
- en: 'To work around the default behavior of `removePunctuation()`, it is possible
    to create a custom function that replaces rather than removes punctuation characters:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过 `removePunctuation()` 的默认行为，可以创建一个自定义函数来替换而不是移除标点符号：
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This uses R’s `gsub()` function to substitute any punctuation characters in
    `x` with a blank space. This `replacePunctuation()` function can then be used
    with `tm_map()` as with other transformations. The odd syntax of the `gsub()`
    command here is due to the use of a **regular expression**, which specifies a
    pattern that matches text characters. Regular expressions are covered in more
    detail in *Chapter 12*, *Advanced Data Preparation*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用 R 的 `gsub()` 函数将 `x` 中的任何标点符号替换为一个空格。然后可以使用 `tm_map()` 与其他转换一样使用 `replacePunctuation()`
    函数。这里 `gsub()` 命令的奇怪语法是由于使用了 **正则表达式**，它指定了一个匹配文本字符的模式。正则表达式在 *第 12 章*，*高级数据准备*
    中有更详细的介绍。
- en: Another common standardization for text data involves reducing words to their
    root form in a process called **stemming**. The stemming process takes words like
    *learned*, *learning*, and *learns* and strips the suffix in order to transform
    them into the base form, *learn*. This allows machine learning algorithms to treat
    the related terms as a single concept rather than attempting to learn a pattern
    for each variant.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据的另一个常见标准化涉及通过称为 **词干提取** 的过程将单词还原到其根形式。词干提取过程将像 *learned*、*learning* 和 *learns*
    这样的单词去除后缀，以便将它们转换为基本形式，*learn*。这允许机器学习算法将相关术语视为单一概念，而不是尝试为每个变体学习一个模式。
- en: The `tm` package provides stemming functionality via integration with the `SnowballC`
    package. At the time of writing, `SnowballC` is not installed by default with
    `tm`, so do so with `install.packages("SnowballC")` if you have not done so already.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm` 包通过集成 `SnowballC` 包提供词干提取功能。在撰写本文时，`SnowballC` 并未默认与 `tm` 一起安装，因此如果您尚未安装，请使用
    `install.packages("SnowballC")` 进行安装。'
- en: The `SnowballC` package is maintained by Milan Bouchet-Valat and provides an
    R interface for the C-based `libstemmer` library, itself based on M. F. Porter’s
    “Snowball” word-stemming algorithm, a widely used open-source stemming method.
    For more details, see [http://snowballstem.org](http://snowballstem.org).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`SnowballC` 包由 Milan Bouchet-Valat 维护，并为基于 C 的 `libstemmer` 库提供 R 接口，该库本身基于
    M. F. Porter 的“Snowball”词干提取算法，这是一种广泛使用的开源词干提取方法。有关更多详细信息，请参阅 [http://snowballstem.org](http://snowballstem.org)。'
- en: 'The `SnowballC` package provides a `wordStem()` function, which for a character
    vector returns the same vector of terms in its root form. For example, the function
    correctly stems the variants of the word *learn* as described previously:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`SnowballC` 包提供了一个 `wordStem()` 函数，对于一个字符向量，它会返回其根形式的相同术语向量。例如，该函数正确地处理了之前描述的单词
    *learn* 的变体：'
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To apply the `wordStem()` function to an entire corpus of text documents, the
    `tm` package includes a `stemDocument()` transformation. We apply this to our
    corpus with the `tm_map()` function exactly as before:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `wordStem()` 函数应用于整个文本文档集合，`tm` 包包括一个 `stemDocument()` 转换。我们使用 `tm_map()`
    函数以与之前相同的方式应用此转换：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you receive an error message when applying the `stemDocument()` transformation,
    please confirm that you have the `SnowballC` package installed.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在应用 `stemDocument()` 转换时收到错误消息，请确认您已安装 `SnowballC` 包。
- en: 'After removing numbers, stop words, and punctuation, then also performing stemming,
    the text messages are left with the blank spaces that once separated the now-missing
    pieces. Therefore, the final step in our text cleanup process is to remove additional
    whitespace using the built-in `stripWhitespace()` transformation:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在移除数字、停用词和标点符号后，然后也进行词干提取，文本消息就只剩下之前分隔现在缺失部分的空白空间。因此，我们文本清理过程的最后一步是使用内置的 `stripWhitespace()`
    转换来移除额外的空白：
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following table shows the first three messages in the SMS corpus before
    and after the cleaning process. The messages have been limited to the most interesting
    words, and punctuation and capitalization have been removed:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了在清洗过程前后，短信语料库中的前三条消息。消息已被限制为最有趣的单词，并且已经移除了标点和大小写：
- en: '| **SMS messages before cleaning** | **SMS messages after cleaning** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **清洗前的短信消息** | **清洗后的短信消息** |'
- en: '| `> as.character(sms_corpus[1:3])``[[1]] Hope you are having a good``week.
    Just checking in``[[2]] K..give back my thanks.``[[3]] Am also doing in cbe only.``But
    have to pay.` | `> as.character(sms_corpus_clean[1:3])``[[1]] hope good week just
    check``[[2]] kgive back thank``[[3]] also cbe pay` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `> as.character(sms_corpus[1:3])``[[1]] Hope you are having a good``week.
    Just checking in``[[2]] K..give back my thanks.``[[3]] Am also doing in cbe only.``But
    have to pay.` | `> as.character(sms_corpus_clean[1:3])``[[1]] hope good week just
    check``[[2]] kgive back thank``[[3]] also cbe pay` |'
- en: Data preparation – splitting text documents into words
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 将文本文档分割成单词
- en: Now that the data is processed to our liking, the final step is to split the
    messages into individual terms through a process called **tokenization**. A token
    is a single element of a text string; in this case, the tokens are words.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经被处理成我们所期望的样子，最后一步是通过一个叫做**分词**的过程将消息分割成单个术语。术语是文本字符串的单个元素；在这种情况下，术语是单词。
- en: As you might assume, the `tm` package provides functionality to tokenize the
    SMS message corpus. The `DocumentTermMatrix()` function takes a corpus and creates
    a data structure called a **document-term matrix** (**DTM**) in which rows indicate
    documents (SMS messages) and columns indicate terms (words).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，`tm`包提供了对SMS消息语料库进行分词的功能。`DocumentTermMatrix()`函数接受一个语料库并创建一个称为**文档-术语矩阵**（**DTM**）的数据结构，其中行表示文档（短信消息），列表示术语（单词）。
- en: The `tm` package also provides a data structure for a **term-document matrix**
    (**TDM**), which is simply a transposed DTM in which the rows are terms and the
    columns are documents. Why the need for both? Sometimes, it is more convenient
    to work with one or the other. For example, if the number of documents is small,
    while the word list is large, it may make sense to use a TDM because it is usually
    easier to display many rows than to display many columns. That said, machine learning
    algorithms will generally require a DTM, as the columns are the features and the
    rows are the examples.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包还提供了一个**术语-文档矩阵**（**TDM**）的数据结构，它实际上是一个转置的文档-术语矩阵（**DTM**），其中行是术语，列是文档。为什么需要两者？有时，使用其中一个更方便。例如，如果文档数量很少，而单词列表很大，那么使用TDM可能是有意义的，因为通常显示多行比显示多列更容易。话虽如此，机器学习算法通常需要DTM，因为列是特征，行是示例。'
- en: 'Each cell in the matrix stores a number indicating a count of the times the
    word represented by the column appears in the document represented by the row.
    The following figure depicts only a small portion of the DTM for the SMS corpus,
    as the complete matrix has 5,559 rows and over 7,000 columns:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每个单元格存储一个数字，表示列中单词在行中文档中出现的次数。以下图仅显示了SMS语料库DTM的一小部分，因为完整的矩阵有5,559行和超过7,000列：
- en: '![Table  Description automatically generated](img/B17290_04_09.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_04_09.png)'
- en: 'Figure 4.9: The DTM for the SMS messages is filled with mostly zeros'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：SMS消息的DTM主要由零填充
- en: The fact that each cell in the table is zero implies that none of the words
    listed at the top of the columns appear in any of the first five messages in the
    corpus. This highlights the reason why this data structure is called a **sparse
    matrix**; the vast majority of cells in the matrix are filled with zeros. Stated
    in real-world terms, although each message must contain at least one word, the
    probability of any one word appearing in a given message is small.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中每个单元格为零的事实意味着列顶部的单词列表中的任何一个单词都没有出现在语料库的前五条消息中。这突出了为什么这种数据结构被称为**稀疏矩阵**的原因；矩阵中的绝大多数单元格都是零。用现实世界的术语来说，尽管每条消息必须至少包含一个单词，但任何单个单词出现在给定消息中的概率很小。
- en: 'Creating a DTM sparse matrix from a *tm* corpus involves a single command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从*tm*语料库创建DTM稀疏矩阵只需要一个命令：
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will create an `sms_dtm` object that contains the tokenized corpus using
    the default settings, which apply minimal additional processing. The default settings
    are appropriate because we have already prepared the corpus manually.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个`sms_dtm`对象，它使用默认设置包含分词后的语料库，这些设置应用了最小的额外处理。默认设置是合适的，因为我们已经手动准备了语料库。
- en: 'On the other hand, if we hadn’t already performed the preprocessing, we could
    do so here by providing a list of `control` parameter options to override the
    defaults. For example, to create a DTM directly from the raw, unprocessed SMS
    corpus, we can use the following command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们还没有执行预处理，我们可以通过提供一个`control`参数选项的列表来覆盖默认设置。例如，要从原始、未经处理的短信语料库直接创建DTM，我们可以使用以下命令：
- en: '[PRE33]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This applies the same preprocessing steps to the SMS corpus in the same order
    as done earlier. However, comparing `sms_dtm` to `sms_dtm2`, we see a slight difference
    in the number of terms in the matrix:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这将按照之前相同的顺序对SMS语料库进行相同的预处理步骤。然而，将`sms_dtm`与`sms_dtm2`比较，我们发现矩阵中的术语数量略有差异：
- en: '[PRE34]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The reason for this discrepancy has to do with a minor difference in the ordering
    of the preprocessing steps. The `DocumentTermMatrix()` function applies its cleanup
    functions to the text strings only after they have been split apart into words.
    Thus, it uses a slightly different stop word removal function. Consequently, some
    words are split differently than when they are cleaned before tokenization.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异的原因与预处理步骤顺序的微小差异有关。`DocumentTermMatrix()`函数在将文本字符串分割成单词之后，才对其应用清理函数。因此，它使用了一个稍微不同的停用词去除函数。因此，某些单词在清理之前分割的方式与清理后不同。
- en: 'To force the two prior DTMs to be identical, we can override the default stop
    words function with our own that uses the original replacement function. Simply
    replace `stopwords = TRUE` with the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要强制两个先前的DTM（文档-词矩阵）相同，我们可以用我们自己的使用原始替换函数的函数覆盖默认的停用词函数。只需将`stopwords = TRUE`替换为以下内容：
- en: '[PRE38]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The code file for this chapter includes the full set of steps to create an identical
    DTM using a single function call.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件包括创建一个相同的DTM所需的完整步骤，只需一个函数调用即可。
- en: 'The differences between these bring up an important principle of cleaning text
    data: the order of operations matters. With this in mind, it is very important
    to think through how early steps in the process are going to affect later ones.
    The order presented here will work in many cases, but when the process is tailored
    more carefully to specific datasets and use cases, it may require rethinking.
    For example, if there are certain terms you hope to exclude from the matrix, consider
    whether to search for them before or after stemming. Also, consider how the removal
    of punctuation—and whether the punctuation is eliminated or replaced by blank
    space—affects these steps.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异提出了清理文本数据的一个重要原则：操作的顺序很重要。考虑到这一点，思考过程早期步骤如何影响后续步骤非常重要。这里提供的顺序在许多情况下都会有效，但当过程更细致地针对特定数据集和用例定制时，可能需要重新思考。例如，如果你希望从矩阵中排除某些术语，考虑是否在词干提取之前或之后搜索它们。还要考虑移除标点符号（以及标点符号是否被替换为空白空间）如何影响这些步骤。
- en: Data preparation – creating training and test datasets
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 创建训练和测试数据集
- en: With our data prepared for analysis, we now need to split the data into training
    and test datasets so that after our spam classifier is built, it can be evaluated
    on data it has not previously seen. However, even though we need to keep the classifier
    blinded as to the contents of the test dataset, it is important that the split
    occurs after the data has been cleaned and processed. We need exactly the same
    preparation steps to have occurred on both the training and test datasets.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据准备好分析后，我们现在需要将数据分为训练和测试数据集，以便在构建垃圾邮件分类器之后，它可以在之前未见过的数据上评估。然而，尽管我们需要保持分类器对测试数据集的内容不知情，但在数据清理和加工之后进行分割很重要。我们需要确保训练和测试数据集上发生的准备步骤完全相同。
- en: 'We’ll divide the data into two portions: 75 percent for training and 25 percent
    for testing. Since the SMS messages are sorted in a random order, we can simply
    take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully,
    the DTM object acts very much like a data frame and can be split using the standard
    `[row, col]` operations. As our DTM stores SMS messages as rows and words as columns,
    we must request a specific range of rows and all columns for each:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分为两部分：75%用于训练，25%用于测试。由于短信消息是随机排序的，我们可以简单地取前4,169条用于训练，剩下的1,390条用于测试。幸运的是，DTM对象非常类似于数据框，可以使用标准的`[行，列]`操作进行分割。由于我们的DTM将短信消息存储为行，将单词存储为列，我们必须为每个请求特定的行范围和所有列：
- en: '[PRE39]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For convenience later, it is also helpful to save a pair of vectors with the
    labels for each of the rows in the training and testing matrices. These labels
    are not stored in the DTM, so we need to pull them from the original `sms_raw`
    data frame:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，保存一对向量，其中包含训练和测试矩阵中每行的标签也是有帮助的。这些标签没有存储在 DTM 中，因此我们需要从原始 `sms_raw` 数据框中提取它们：
- en: '[PRE40]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To confirm that the subsets are representative of the complete set of SMS data,
    let’s compare the proportion of spam in the training and test data frames:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认这些子集代表完整的短信数据集，让我们比较训练和测试数据框中垃圾邮件的比例：
- en: '[PRE41]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Both the training data and test data contain about 13 percent spam. This suggests
    that the spam messages were divided evenly between the two datasets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和测试数据中大约有13%的垃圾邮件。这表明垃圾邮件在这两个数据集中是平均分配的。
- en: Visualizing text data – word clouds
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化文本数据 – 单词云
- en: A **word cloud** is a way to visually depict the frequency at which words appear
    in text data. The cloud is composed of words scattered somewhat randomly around
    the figure. Words appearing more often in the text are shown in a larger font,
    while less common terms are shown in smaller fonts. This type of figure grew in
    popularity as a way to observe trending topics on social media websites.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**单词云**是一种以视觉方式表示文本数据中单词出现频率的方法。云由围绕图形随机散布的单词组成。在文本中出现频率更高的单词以较大的字体显示，而较少出现的术语则以较小的字体显示。这种类型的图表作为观察社交媒体网站上趋势话题的一种方式而越来越受欢迎。'
- en: The `wordcloud` package provides a simple R function to create this type of
    diagram. We’ll use it to visualize the words in SMS messages. Comparing the clouds
    for spam and ham messages will help us gauge whether our Naive Bayes spam filter
    is likely to be successful. If you haven’t already done so, install and load the
    package by typing `install.packages("wordcloud")` and `library(wordcloud)` at
    the R command line.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordcloud` 包提供了一个简单的 R 函数来创建此类图表。我们将使用它来可视化短信中的单词。比较垃圾邮件和正常邮件的云图将帮助我们判断我们的朴素贝叶斯垃圾邮件过滤器是否可能成功。如果您还没有这样做，请在
    R 命令行中输入 `install.packages("wordcloud")` 和 `library(wordcloud)` 来安装和加载此包。'
- en: The `wordcloud` package was written by Ian Fellows. For more information about
    this package, visit his blog at [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordcloud` 包是由 Ian Fellows 编写的。有关此包的更多信息，请访问他的博客[http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11)。'
- en: 'A word cloud can be created directly from a *tm* corpus object using the syntax:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用以下语法从 *tm* 语料库对象创建单词云：
- en: '[PRE45]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This will create a word cloud from our prepared SMS corpus. Since we specified
    `random.order = FALSE`, the cloud will be arranged in a non-random order, with
    higher-frequency words placed closer to the center. If we do not specify `random.order`,
    the cloud will be arranged randomly by default.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这将根据我们准备好的短信语料库创建单词云。由于我们指定了 `random.order = FALSE`，云将按非随机顺序排列，高频词将放置在中心附近。如果我们不指定
    `random.order`，则默认随机排列。
- en: The `min.freq` parameter specifies the number of times a word must appear in
    the corpus before it will be displayed in the cloud. Since a frequency of 50 is
    about 1 percent of the corpus, this means that a word must be found in at least
    1 percent of the SMS messages to be included in the cloud.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`min.freq` 参数指定单词在语料库中必须出现的次数，才能在云中显示。由于频率为50大约是语料库的1%，这意味着一个单词必须至少出现在1%的短信中才能包含在云中。'
- en: You might get a warning message noting that R was unable to fit all the words
    in the figure. If so, try increasing the `min.freq` to reduce the number of words
    in the cloud. It might also help to use the `scale` parameter to reduce the font
    size.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会收到一条警告消息，指出 R 无法将所有单词拟合到图中。如果是这样，请尝试增加 `min.freq` 以减少云中的单词数量。使用 `scale`
    参数减小字体大小也可能有所帮助。
- en: 'The resulting word cloud should appear similar to the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的单词云应类似于以下示例：
- en: '![Text  Description automatically generated](img/B17290_04_10.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17290_04_10.png)'
- en: 'Figure 4.10: A word cloud depicting words appearing in all SMS messages'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：展示所有短信中出现的单词的单词云
- en: A perhaps more interesting visualization involves comparing the clouds for SMS
    spam and ham. Since we did not construct separate corpora for spam and ham, this
    is an appropriate time to note a very helpful feature of the `wordcloud()` function.
    Given a vector of raw text strings, it will automatically apply common text preparation
    processes before displaying the cloud.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能更有趣的可视化是将垃圾邮件和正常短信的云进行比较。由于我们没有为垃圾邮件和正常短信构建单独的语料库，这是注意 `wordcloud()` 函数的一个非常有用的特性的合适时机。给定一个原始文本字符串向量，它将在显示云之前自动应用常见的文本准备过程。
- en: 'Let’s use R’s `subset()` function to take a subset of the `sms_raw` data by
    the SMS type. First, we’ll create a subset where the `type` is `spam`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 R 的 `subset()` 函数通过短信类型对 `sms_raw` 数据进行子集化。首先，我们将创建一个 `type` 为 `spam`
    的子集：
- en: '[PRE46]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we’ll do the same thing for the `ham` subset:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对 `ham` 子集做同样的事情：
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Be careful to note the double equals sign. Like many programming languages,
    R uses `==` to test equality. If you accidentally use a single equals sign, you’ll
    end up with a subset much larger than you expected!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意双等号的使用。像许多编程语言一样，R 使用 `==` 来测试相等性。如果你不小心使用了单个等号，你最终会得到一个比你预期的更大的子集！
- en: 'We now have two data frames, `spam` and `ham`, each with a `text` feature containing
    the raw text strings for SMS messages. Creating word clouds is as simple as before.
    This time, we’ll use the `max.words` parameter to look at the 40 most common words
    in each of the 2 sets. The `scale` parameter adjusts the maximum and minimum font
    sizes for words in the cloud. Feel free to change these parameters as you see
    fit. This is illustrated in the following code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个数据框，`spam` 和 `ham`，每个数据框都有一个包含短信原始文本字符串的 `text` 特征。创建词云就像以前一样简单。这次，我们将使用
    `max.words` 参数来查看每个集合中最常见的 40 个单词。`scale` 参数调整云中单词的最大和最小字体大小。你可以根据需要更改这些参数。以下代码展示了这一点：
- en: '[PRE48]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note that R provides warning messages when running this code that the “transformation
    drops documents.” The warnings are related to the `removePunctuation()` and `removeWords()`
    procedures that `wordcloud()` performs by default when given raw text data rather
    than a term matrix. Basically, there are some messages that are excluded from
    the result because there is no remaining message text after cleaning. For example,
    the ham message with the text *:)* representing the smiley emoji is removed from
    the set after cleaning. This is not a problem for the word clouds and the warnings
    can be ignored.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当运行此代码时，R 会提供警告信息，指出“转换丢弃文档”。这些警告与 `wordcloud()` 在接收到原始文本数据而不是术语矩阵时默认执行的
    `removePunctuation()` 和 `removeWords()` 程序有关。基本上，有一些消息被排除在结果之外，因为在清理后没有剩余的消息文本。例如，带有文本
    *:)* 表示笑脸表情的垃圾邮件消息在清理后被从集合中移除。这对词云没有问题，可以忽略这些警告。
- en: The resulting word clouds should appear similar to those that follow. Do you
    have a hunch on which one is the spam cloud, and which represents ham?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的词云应该看起来与下面的类似。你有没有一种预感，哪一个是垃圾邮件云，哪一个是正常短信云？
- en: '![Text, letter  Description automatically generated](img/B17290_04_11.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![文本，字母描述自动生成](img/B17290_04_11.png)'
- en: 'Figure 4.11: Side-by-side word clouds depicting SMS spam and ham messages'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：并排显示的词云，描绘了垃圾邮件和正常短信消息
- en: As you probably guessed, the spam cloud is on the left. Spam messages include
    words such as *call*, *free*, *mobile*, *claim*, and *stop*; these terms do not
    appear in the ham cloud at all. Instead, ham messages use words such as *can*,
    *sorry*, *love*, and *time*. These stark differences suggest that our Naive Bayes
    model will have some strong keywords to differentiate between the classes.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，垃圾邮件云在左边。垃圾邮件包括诸如 *call*、*free*、*mobile*、*claim* 和 *stop* 等单词；这些术语根本不会出现在垃圾邮件云中。相反，垃圾邮件使用诸如
    *can*、*sorry*、*love* 和 *time* 等单词。这些明显的差异表明，我们的朴素贝叶斯模型将有一些强有力的关键词来区分这两类。
- en: Data preparation – creating indicator features for frequent words
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 为频繁单词创建指示特征
- en: The final step in the data preparation process is to transform the sparse matrix
    into a data structure that can be used to train a Naive Bayes classifier. Currently,
    the sparse matrix includes over 6,500 features; this is a feature for every word
    that appears in at least one SMS message. It’s unlikely that all of these are
    useful for classification. To reduce the number of features, we’ll eliminate any
    word that appears in less than 5 messages, or in less than about 0.1 percent of
    records in the training data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备过程的最后一步是将稀疏矩阵转换成可以用于训练 Naive Bayes 分类器的数据结构。目前，稀疏矩阵包括超过 6,500 个特征；这是每个至少出现在一条短信中的单词的特征。不太可能所有这些都有助于分类。为了减少特征数量，我们将删除在不到
    5 条消息或训练数据中不到约 0.1% 的记录中出现的任何单词。
- en: 'Finding frequent words requires the use of the `findFreqTerms()` function in
    the `tm` package. This function takes a DTM and returns a character vector containing
    words that appear at least a minimum number of times. For instance, the following
    command displays the words appearing at least five times in the `sms_dtm_train`
    matrix:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找频繁单词需要使用 `tm` 包中的 `findFreqTerms()` 函数。此函数接受一个 DTM 并返回一个包含至少出现最小次数的单词的字符向量。例如，以下命令显示在
    `sms_dtm_train` 矩阵中至少出现五次的单词：
- en: '[PRE49]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result of the function is a character vector, so let’s save our frequent
    words for later:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的结果是一个字符向量，所以让我们将我们的频繁单词保存起来以备后用：
- en: '[PRE50]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A peek into the contents of the vector shows us that there are 1,139 terms
    appearing in at least 5 SMS messages:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 查看向量的内容显示，至少在 5 条短信中出现的术语有 1,139 个：
- en: '[PRE51]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We now need to filter our DTM to include only the terms appearing in the frequent
    word vector. As before, we’ll use data frame-style `[row, col]` operations to
    request specific sections of the DTM, noting that the DTM column names are based
    on the words the DTM contains. We can take advantage of this fact to limit the
    DTM to specific words. Since we want all rows but only the columns representing
    the words in the `sms_freq_words` vector, our commands are:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要过滤我们的 DTM，只包含频繁单词向量中出现的术语。像之前一样，我们将使用数据框风格的 `[row, col]` 操作来请求 DTM 的特定部分，注意
    DTM 的列名是基于 DTM 包含的单词。我们可以利用这个事实来限制 DTM 到特定的单词。由于我们想要所有行，但只有 `sms_freq_words` 向量中单词表示的列，我们的命令如下：
- en: '[PRE53]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The training and test datasets now include 1,137 features, which correspond
    to words appearing in at least 5 messages.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据集现在包括 1,137 个特征，这些特征对应于至少在 5 条消息中出现的单词。
- en: The Naive Bayes classifier is usually trained on data with categorical features.
    This poses a problem since the cells in the sparse matrix are numeric and measure
    the number of times a word appears in a message. We need to change this to a categorical
    variable that simply indicates yes or no, depending on whether the word appears
    at all.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 'Naive Bayes 分类器通常在具有分类特征的数据上训练。这带来一个问题，因为稀疏矩阵中的单元格是数值型的，并衡量一个单词在消息中出现的次数。我们需要将其转换为表示是否出现的分类变量，是或否。 '
- en: 'The following defines a `convert_counts()` function to convert counts into
    `Yes` or `No` strings:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定义了一个 `convert_counts()` 函数，用于将计数转换为 `Yes` 或 `No` 字符串：
- en: '[PRE54]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: By now, some of the pieces of the preceding function should look familiar. The
    first line defines the function. The statement `ifelse(x > 0, "Yes", "No")` transforms
    the values in `x` such that if the value is greater than `0`, then it will be
    replaced with `"Yes"`; otherwise, it will be replaced with a `"No"` string. Lastly,
    the newly transformed vector `x` is returned.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，前面函数的一些部分应该看起来很熟悉。第一行定义了函数。语句 `ifelse(x > 0, "Yes", "No")` 将 `x` 中的值进行转换，如果值大于
    `0`，则将其替换为 `"Yes"`；否则，将其替换为一个 `"No"` 字符串。最后，返回新转换的向量 `x`。
- en: We now need to apply `convert_counts()` to each of the columns in our sparse
    matrix. You may be able to guess the name of the R function that does exactly
    this. The function is simply called `apply()` and is used much like `lapply()`
    was used previously.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将 `convert_counts()` 应用到稀疏矩阵的每一列。你可能能够猜到执行此操作的 R 函数的名称。该函数简单地称为 `apply()`，其用法与之前使用的
    `lapply()` 类似。
- en: 'The `apply()` function allows a function to be used on each of the rows or
    columns in a matrix. It uses a `MARGIN` parameter to specify either rows or columns.
    Here, we’ll use `MARGIN = 2` since we’re interested in the columns (`MARGIN =
    1` is used for rows). The commands to convert the training and test matrices are
    as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply()`函数允许对矩阵中的每一行或每一列使用一个函数。它使用`MARGIN`参数来指定是行还是列。在这里，我们将使用`MARGIN = 2`，因为我们感兴趣的是列（`MARGIN
    = 1`用于行）。转换训练和测试矩阵的命令如下：'
- en: '[PRE55]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The result will be two character-type matrices, each with cells indicating `"Yes"`
    or `"No"` for whether the word represented by the column appears at any point
    in the message represented by the row.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是两个字符类型的矩阵，每个矩阵的单元格都表示列中单词是否在任何时刻出现在行中代表的邮件中，是“是”还是“否”。
- en: Step 3 – training a model on the data
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: Now that we have transformed the raw SMS messages into a format that can be
    represented by a statistical model, it is time to apply the Naive Bayes algorithm.
    The algorithm will use the presence or absence of words to estimate the probability
    that a given SMS message is spam.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将原始短信消息转换成了可以由统计模型表示的格式，是时候应用朴素贝叶斯算法了。该算法将使用单词的存在或不存在来估计给定短信消息是垃圾邮件的概率。
- en: The Naive Bayes implementation we will employ is in the `naivebayes` package.
    This package is maintained by Michal Majka and is a modern and efficient R implementation.
    If you have not done so already, be sure to install and load the package using
    the `install.packages("naivebayes")` and `library(naivebayes)` commands before
    continuing.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的朴素贝叶斯实现是在`naivebayes`包中。这个包由Michal Majka维护，是一个现代且高效的R实现。如果您还没有这样做，请确保在继续之前使用`install.packages("naivebayes")`和`library(naivebayes)`命令安装并加载该包。
- en: 'Many machine learning approaches are implemented in more than one R package,
    and Naive Bayes is no exception. Another option is `naiveBayes()` in the `e1071`
    package, which was used in older editions of this book but is otherwise nearly
    identical to `naive_bayes()` in usage. The `naivebayes` package used in this edition
    offers better performance and more advanced functionality, which is described
    at its website: [https://majkamichal.github.io/naivebayes/](https://majkamichal.github.io/naivebayes/).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法在多个R包中实现，朴素贝叶斯也不例外。另一个选项是`e1071`包中的`naiveBayes()`，这在本书的旧版本中使用过，但在使用上几乎与`naive_bayes()`相同。本版使用的`naivebayes`包提供了更好的性能和更高级的功能，详情请访问其网站：[https://majkamichal.github.io/naivebayes/](https://majkamichal.github.io/naivebayes/)。
- en: 'Unlike the k-NN algorithm we used for classification in the previous chapter,
    training a Naive Bayes learner and using it for classification occur in separate
    stages. Still, as shown in the following table, these steps are fairly straightforward:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中用于分类的k-NN算法不同，朴素贝叶斯学习器的训练和使用分类发生在不同的阶段。尽管如此，如以下表格所示，这些步骤相当直接：
- en: '![Text, letter  Description automatically generated](img/B17290_04_12.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![文本，字母  描述自动生成](img/B17290_04_12.png)'
- en: 'Figure 4.12: Naive Bayes classification syntax'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：朴素贝叶斯分类语法
- en: 'Using the `sms_train` matrix, the following command trains a `naive_bayes`
    classifier object that can be used to make predictions:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sms_train`矩阵，以下命令训练了一个`naive_bayes`分类器对象，可以用来进行预测：
- en: '[PRE56]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'After running the previous command, you may notice the following output:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令后，您可能会注意到以下输出：
- en: '[PRE57]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This is nothing to be alarmed about for now; typing the `warnings()` command
    reveals the cause of this issue:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这没有什么好担心的；输入`warnings()`命令可以揭示这个问题的原因：
- en: '[PRE58]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: These warnings are caused by words that appeared in zero spam or zero ham messages
    and have veto power over the classification process due to their associated zero
    probabilities. For instance, because the word *accept* only appeared in ham messages
    in the training data, it does not mean that every future message with this word
    should be automatically classified as ham.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这些警告是由那些在零垃圾邮件或零正常邮件中出现的单词引起的，由于它们关联的零概率，它们在分类过程中具有否决权。例如，因为单词*接受*仅在训练数据中的正常邮件中出现，这并不意味着每个包含这个单词的未来邮件都应该自动被分类为正常邮件。
- en: There is an easy solution to this problem using the Laplace estimator described
    earlier, but for now, we will evaluate this model using `laplace = 0`, which is
    the model’s default setting.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面描述的拉普拉斯估计器可以轻松解决这个问题，但到目前为止，我们将使用`laplace = 0`来评估这个模型，这是模型的默认设置。
- en: Step 4 – evaluating model performance
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: To evaluate the SMS classifier, we need to test its predictions on the unseen
    messages in the test data. Recall that the unseen message features are stored
    in a matrix named `sms_test`, while the class labels (spam or ham) are stored
    in a vector named `sms_test_labels`. The classifier that we trained has been named
    `sms_classifier`. We will use this classifier to generate predictions and then
    compare the predicted values to the true values.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 SMS 分类器，我们需要在测试数据中未见过的新消息上测试其预测结果。回想一下，未见过的新消息特征存储在一个名为 `sms_test` 的矩阵中，而类别标签（垃圾邮件或正常邮件）存储在一个名为
    `sms_test_labels` 的向量中。我们训练的分类器已被命名为 `sms_classifier`。我们将使用这个分类器来生成预测，然后比较预测值和真实值。
- en: 'The `predict()` function is used to make the predictions. We will store these
    in a vector named `sms_test_pred`. We simply supply this function with the names
    of our classifier and test dataset as shown:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `predict()` 函数进行预测。我们将这些预测存储在一个名为 `sms_test_pred` 的向量中。我们只需向这个函数提供我们分类器和测试数据集的名称，如下所示：
- en: '[PRE60]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To compare the predictions to the true values, we’ll use the `CrossTable()`
    function in the `gmodels` package, which we used in previous chapters. This time,
    we’ll add some additional parameters to eliminate unnecessary cell proportions,
    and use the `dnn` parameter (dimension names) to relabel the rows and columns
    as shown in the following code:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将预测值与真实值进行比较，我们将使用 `gmodels` 包中的 `CrossTable()` 函数，我们在前面的章节中使用过它。这次，我们将添加一些额外的参数来消除不必要的单元格比例，并使用
    `dnn` 参数（维度名称）来重新标记行和列，如下面的代码所示：
- en: '[PRE61]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This produces the following table:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下表格：
- en: '[PRE62]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Looking at the table, we can see that a total of only *6 + 30 = 36* of 1,390
    SMS messages were incorrectly classified (2.6 percent). Among the errors were
    6 out of 1,207 ham messages that were misidentified as spam and 30 of 183 spam
    messages that were incorrectly labeled as ham. Considering the little effort that
    we put into the project, this level of performance seems quite impressive. This
    case study exemplifies the reason why Naive Bayes is so often used for text classification:
    directly out of the box, it performs surprisingly well.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 观察表格，我们可以看到总共只有 *6 + 30 = 36* 条 1,390 条短信被错误分类（2.6%）。在这些错误中，有 6 条在 1,207 条正常邮件中被错误地识别为垃圾邮件，以及
    30 条在 183 条垃圾邮件中被错误地标记为正常邮件。考虑到我们在项目上投入的少量努力，这种性能水平似乎相当令人印象深刻。这个案例研究说明了为什么朴素贝叶斯在文本分类中如此经常被使用：直接使用，它表现出令人惊讶的良好性能。
- en: On the other hand, the six legitimate messages that were incorrectly classified
    as spam could cause significant problems for the deployment of our filtering algorithm
    because the filter could cause a person to miss an important text message. We
    should try to see whether we can slightly tweak the model to achieve better performance.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，被错误分类为垃圾邮件的六条合法消息可能会对我们过滤算法的部署造成重大问题，因为过滤器可能会使一个人错过一条重要的短信。我们应该尝试看看我们是否可以稍微调整模型以获得更好的性能。
- en: Step 5 – improving model performance
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步 – 提高模型性能
- en: 'You may recall that we didn’t set a value for the Laplace estimator when training
    our model; in fact, it was hard to miss the message from R warning us about more
    than 50 features with zero probabilities! To address this issue, we’ll build a
    Naive Bayes model as before, but this time set `laplace = 1`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们在训练模型时没有为拉普拉斯估计器设置值；事实上，很难错过 R 警告我们超过 50 个特征概率为零的信息！为了解决这个问题，我们将像以前一样构建一个朴素贝叶斯模型，但这次将
    `laplace = 1` 设置：
- en: '[PRE63]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we’ll make predictions as before:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将像以前一样进行预测：
- en: '[PRE64]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we’ll compare the predicted classes to the actual classifications
    using cross-tabulation:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用交叉表比较预测类别与实际分类：
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This produces the following table:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下表格：
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Adding a Laplace estimator by setting `laplace = 1` reduced the number of false
    positives (ham messages erroneously classified as spam) from 6 to 5, and the number
    of false negatives from 30 to 28\. Although this seems like a small change, it’s
    substantial considering that the model’s accuracy was already quite impressive.
    We’d need to be careful before tweaking the model too much more, as it is important
    to maintain a balance between being overly aggressive and overly passive when
    filtering spam. Users prefer that a small number of spam messages slip through
    the filter rather than the alternative, in which ham messages are filtered too
    aggressively.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`laplace = 1`添加拉普拉斯估计器，将误报（将正常邮件错误分类为垃圾邮件）的数量从6减少到5，将漏报（将垃圾邮件错误分类为正常邮件）的数量从30减少到28。尽管这似乎是一个微小的变化，但考虑到模型的准确率已经相当令人印象深刻，这是一个重大的改进。在进一步调整模型之前，我们需要谨慎，因为在过滤垃圾邮件时，保持过于激进和过于被动之间的平衡是很重要的。用户更希望少量垃圾邮件能够通过过滤器，而不是另一种情况，即正常邮件被过度过滤。
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about classification using Naive Bayes. This algorithm
    constructs tables of probabilities that are used to estimate the likelihood that
    new examples belong to various classes. The probabilities are calculated using
    a formula known as Bayes’ theorem, which specifies how dependent events are related.
    Although Bayes’ theorem can be computationally expensive, a simplified version
    that makes so-called “naive” assumptions about the independence of features is
    capable of handling much larger datasets.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了使用天真贝叶斯进行分类。该算法构建用于估计新示例属于各种类别的概率表。这些概率是通过称为贝叶斯定理的公式计算的，该公式指定了依赖事件之间的关系。尽管贝叶斯定理在计算上可能很昂贵，但一个简化的版本，即所谓的“天真”假设关于特征的独立性，能够处理更大的数据集。
- en: The Naive Bayes classifier is often used for text classification. To illustrate
    its effectiveness, we employed Naive Bayes on a classification task involving
    spam SMS messages. Preparing the text data for analysis required the use of specialized
    R packages for text processing and visualization. Ultimately, the model was able
    to classify over 97 percent of all the SMS messages correctly as spam or ham.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 天真贝叶斯分类器常用于文本分类。为了说明其有效性，我们在涉及垃圾短信的分类任务中使用了天真贝叶斯。准备文本数据进行分析需要使用专门的R包进行文本处理和可视化。最终，该模型能够正确地将97%以上的短信分类为垃圾邮件或正常邮件。
- en: In the next chapter, we will examine two more machine learning methods. Each
    performs classification by partitioning data into groups of similar values. As
    you will discover shortly, these methods are quite useful on their own. Yet, looking
    further ahead, these basic algorithms also serve as an important foundation for
    some of the most powerful machine learning methods known today, which will be
    introduced later in *Chapter 14*, *Building Better Learners*.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨两种更多的机器学习方法。每种方法通过将数据划分为相似值组来进行分类。正如你很快会发现的那样，这些方法本身非常有用。然而，展望未来，这些基本算法也成为了当今一些最强大的机器学习方法的重要基础，这些方法将在第14章“构建更好的学习者”中介绍。
- en: Join our book’s Discord space
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人相聚，并和超过4000人一起学习：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/r.jpg)'
