- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Probabilistic Learning – Classification Using Naive Bayes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率学习 - 使用朴素贝叶斯进行分类
- en: When a meteorologist provides a weather forecast, precipitation is typically
    described with phrases like “70 percent chance of rain.” Such forecasts are known
    as probability of precipitation reports. Have you ever considered how they are
    calculated? It is a puzzling question because, in reality, it will either rain
    or not with absolute certainty.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当气象学家提供天气预报时，降水通常用“70%降雨可能性”这样的短语来描述。这类预报被称为降水概率报告。你有没有考虑过它们是如何计算的？这是一个令人困惑的问题，因为在现实中，要么下雨，要么不下雨，这是绝对确定的。
- en: Weather estimates are based on probabilistic methods, which are those concerned
    with describing uncertainty. They use data on past events to extrapolate future
    events. In the case of the weather, the chance of rain describes the proportion
    of prior days with similar atmospheric conditions on which precipitation occurred.
    A 70 percent chance of rain implies that in 7 out of 10 past cases with similar
    conditions, precipitation occurred somewhere in the area.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 天气估计基于概率方法，这些方法涉及描述不确定性。它们使用过去事件的数据来预测未来事件。在天气的情况下，降雨的可能性描述了在相似大气条件下发生降水的先前天数所占的比例。70%的降雨可能性意味着在10个过去类似条件下，有7个地方发生了降水。
- en: 'This chapter covers the Naive Bayes algorithm, which uses probabilities in
    much the same way as a weather forecast. While studying this method, you will
    learn about:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了朴素贝叶斯算法，它使用概率的方式与天气预报非常相似。在研究这种方法时，你将了解：
- en: Basic principles of probability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率的基本原理
- en: The specialized methods and data structures needed to analyze text data with
    R
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R分析文本数据所需的专业方法和数据结构
- en: How to employ Naive Bayes to build a **Short Message Service** (**SMS**) junk
    message filter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用朴素贝叶斯构建**短信服务**（**SMS**）垃圾信息过滤器
- en: If you’ve taken a statistics class before, some of the material in this chapter
    may be a review. Even so, it may be helpful to refresh your knowledge of probability.
    You will find out that these principles are the basis of how Naive Bayes got such
    a strange name.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前上过统计学课程，本章的一些材料可能对你来说是复习。即便如此，刷新你对概率的了解可能也有帮助。你会发现这些原则是朴素贝叶斯获得这样一个奇怪名称的基础。
- en: Understanding Naive Bayes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯
- en: The basic statistical ideas necessary to understand the Naive Bayes algorithm
    have existed for centuries. The technique descended from the work of the 18th-century
    mathematician Thomas Bayes, who developed foundational principles for describing
    the probability of events and how these probabilities should be revised in light
    of additional information. These principles formed the foundation for what are
    now known as **Bayesian methods**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯算法所需的基本统计思想已经存在了几个世纪。这项技术源于18世纪数学家托马斯·贝叶斯的工作，他开发了描述事件概率及其在额外信息的基础上如何修订的基础原则。这些原则构成了现在被称为**贝叶斯方法**的基础。
- en: We will cover these methods in greater detail later. For now, it suffices to
    say that a probability is a number between zero and one (or from 0 to 100 percent)
    that captures the chance that an event will occur in light of the available evidence.
    The lower the probability, the less likely the event is to occur. A probability
    of zero indicates that the event will definitely not occur, while a probability
    of one indicates that the event will occur with absolute certainty. Life’s most
    interesting events tend to be those with uncertain probability; estimating the
    chance that they will occur helps us make better decisions by revealing the most
    likely outcomes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后更详细地介绍这些方法。现在，只需说一个概率是一个介于零和一之间的数字（或从0到100%）即可，它捕捉了在现有证据的基础上事件发生的可能性。概率越低，事件发生的可能性越小。零概率表示事件肯定不会发生，而一概率表示事件将以绝对确定性发生。生活中最有趣的事件往往具有不确定的概率；估计它们发生的可能性有助于我们通过揭示最可能的结果来做出更好的决策。
- en: 'Classifiers based on Bayesian methods utilize training data to calculate the
    probability of each outcome based on the evidence provided by feature values.
    When the classifier is later applied to unlabeled data, it uses these calculated
    probabilities to predict the most likely class for the new example. It’s a simple
    idea, but it results in a method that can have results on par with more sophisticated
    algorithms. In fact, Bayesian classifiers have been used for:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯方法的分类器利用训练数据根据特征值提供的证据计算每个结果的概率。当分类器后来应用于未标记的数据时，它使用这些计算出的概率来预测新示例最可能的类别。这是一个简单的想法，但结果是一个可以与更复杂算法相媲美的方法。事实上，贝叶斯分类器已被用于：
- en: Text classification, such as junk email (spam) filtering
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，如垃圾邮件（垃圾邮件过滤）
- en: Intrusion or anomaly detection in computer networks
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机网络中的入侵或异常检测
- en: Diagnosing medical conditions given a set of observed symptoms
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一组观察到的症状诊断医疗状况
- en: Typically, Bayesian classifiers are best applied to problems for which the information
    from numerous attributes should be considered simultaneously to estimate the overall
    probability of an outcome. While many machine learning algorithms ignore features
    that have weak effects, Bayesian methods utilize all available evidence to subtly
    change the predictions. This implies that even if a large portion of features
    have relatively minor effects, their combined impact in a Bayesian model could
    be quite large.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，贝叶斯分类器最适合应用于需要同时考虑多个属性信息以估计结果整体概率的问题。虽然许多机器学习算法忽略了具有较弱影响特征，但贝叶斯方法利用所有可用证据微妙地改变预测。这意味着即使大部分特征的影响相对较小，但在贝叶斯模型中它们的综合影响可能相当大。
- en: Basic concepts of Bayesian methods
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯方法的基本概念
- en: Before jumping into the Naive Bayes algorithm, it’s worth spending some time
    defining the concepts that are used across Bayesian methods. Summarized in a single
    sentence, Bayesian probability theory is rooted in the idea that the estimated
    likelihood of an **event**, or potential outcome, should be based on the evidence
    at hand across multiple **trials**, or opportunities for the event to occur.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究朴素贝叶斯算法之前，花些时间定义贝叶斯方法中使用的概念是值得的。用一句话总结，贝叶斯概率理论根植于这样一个观点：估计一个 **事件** 或潜在结果的似然性应该基于多个
    **试验** 或事件发生机会的证据。
- en: 'The following table illustrates events and trials for several real-world outcomes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了几个现实世界结果的事件和试验：
- en: '| **Event** | **Trial** |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **事件** | **试验** |'
- en: '| Heads result | A coin flip |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 正面朝上 | 抛硬币 |'
- en: '| Rainy weather | A single day (or another time period) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 雨天 | 单日（或另一个时间段） |'
- en: '| Message is spam | An incoming email message |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 消息是垃圾邮件 | 一封 incoming 电子邮件 |'
- en: '| Candidate becomes president | A presidential election |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 候选人成为总统 | 总统选举 |'
- en: '| Mortality | A hospital patient |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 死亡率 | 医院病人 |'
- en: '| Winning the lottery | A lottery ticket |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 中奖 | 一张彩票 |'
- en: Bayesian methods provide insights into how the probability of these events can
    be estimated from observed data. To see how, we’ll need to formalize our understanding
    of probability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法提供了从观察数据中估计这些事件概率的见解。为了了解这一点，我们需要形式化我们对概率的理解。
- en: Understanding probability
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解概率
- en: The probability of an event is estimated from observed data by dividing the
    number of trials in which the event occurred by the total number of trials. For
    instance, if it rained 3 out of 10 days with similar conditions as today, the
    probability of rain today can be estimated as *3 / 10 = 0.30* or 30 percent. Similarly,
    if 10 out of 50 prior email messages were spam, then the probability of any incoming
    message being spam can be estimated as *10 / 50 = 0.20* or 20 percent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将事件发生的试验次数除以总试验次数来估计事件的概率。例如，如果今天有类似条件的 10 天中有 3 天下雨，那么今天下雨的概率可以估计为 *3 / 10
    = 0.30* 或 30%。同样，如果 50 封之前的电子邮件中有 10 封是垃圾邮件，那么任何新收到的邮件是垃圾邮件的概率可以估计为 *10 / 50 =
    0.20* 或 20%。
- en: To denote these probabilities, we use notation in the form *P(A)*, which signifies
    the probability of event *A*. For example, *P(rain) = 0.30* to indicate a 30 percent
    chance of rain or *P(spam) = 0.20* to describe a 20 percent probability of an
    incoming message being spam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这些概率，我们使用形式为 *P(A)* 的符号，它表示事件 *A* 的概率。例如，*P(rain) = 0.30* 表示有 30% 的降雨概率，或
    *P(spam) = 0.20* 描述一个新收到的消息有 20% 的概率是垃圾邮件。
- en: Because a trial always results in some outcome happening, the probability of
    all possible outcomes of a trial must always sum to one. Thus, if the trial has
    exactly two outcomes and the outcomes cannot occur simultaneously, then knowing
    the probability of either outcome reveals the probability of the other. This is
    the case for many outcomes, such as heads or tails coin flips, or spam versus
    legitimate email messages, also known as “ham.” Using this principle, knowing
    that *P(spam) = 0.20* allows us to calculate *P(ham) = 1 – 0.20 = 0.80*. This
    only works because spam and ham are **mutually exclusive and exhaustive events**,
    which implies that they cannot occur at the same time and are the only possible
    outcomes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于试验总是导致某些结果发生，因此试验所有可能结果的概率总和必须始终为1。因此，如果试验恰好有两个结果且这些结果不能同时发生，那么知道任意一个结果发生的概率就可以揭示另一个结果发生的概率。这种情况适用于许多结果，例如硬币的正反面，或垃圾邮件与合法电子邮件（也称为“ham”），使用这个原理，知道*P(spam)
    = 0.20*可以让我们计算出*P(ham) = 1 – 0.20 = 0.80*。这仅适用于垃圾邮件和ham是**互斥且穷尽的事件**，这意味着它们不能同时发生，并且是唯一的可能结果。
- en: A single event cannot happen and not happen simultaneously. This means an event
    is always mutually exclusive and exhaustive with its **complement**, or the event
    comprising all other outcomes in which the event of interest does not happen.
    The complement of event *A* is typically denoted *A*^c or *A’*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 单个事件不能同时发生和未发生。这意味着事件总是与其**补集**互斥且穷尽，或者包含所有其他结果的补集，其中感兴趣的事件未发生。事件*A*的补集通常表示为*A*^c或*A’*。
- en: Additionally, the shorthand notation *P(A*^c*)* or *P(¬A)* can be used to denote
    the probability of event *A* not occurring. For example, the notation *P(¬spam)
    = 0.80* suggests that the probability of a message not being spam is 80%.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用简写符号*P(A*^c*)*或*P(¬A)*来表示事件*A*不发生的概率。例如，符号*P(¬spam) = 0.80*表示消息不是垃圾邮件的概率为80%。
- en: 'To illustrate events and their complements, it is often helpful to imagine
    a two-dimensional space that is partitioned into probabilities for each event.
    In the following diagram, the rectangle represents the possible outcomes for an
    email message. The circle represents the 20 percent probability that the message
    is spam. The remaining 80 percent represents the complement *P(¬spam)*, or the
    messages that are not spam:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明事件及其补集，想象一个二维空间，该空间被划分为每个事件的概率，通常是有帮助的。在以下图中，矩形代表电子邮件消息的可能结果。圆圈代表消息是垃圾邮件的20%概率。剩余的80%代表补集*P(¬spam)*，或不是垃圾邮件的消息：
- en: '![Diagram  Description automatically generated](img/B17290_04_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_01.png)'
- en: 'Figure 4.1: The probability space for all emails can be visualized as partitions
    of spam and ham'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：所有电子邮件的概率空间可以表示为垃圾邮件和正常邮件的分区
- en: Understanding joint probability
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解联合概率
- en: 'Often, we are interested in monitoring several non-mutually exclusive events
    in the same trial. If certain events occur concurrently with the event of interest,
    we may be able to use them to make predictions. Consider, for instance, a second
    event based on the outcome that an email message contains the word *Viagra*. The
    preceding diagram, updated for this second event, might appear as shown in the
    following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在同一试验中会对几个非互斥事件进行监控。如果某些事件与感兴趣的事件同时发生，我们可能能够利用它们进行预测。例如，考虑一个基于电子邮件消息包含单词*Viagra*的结果的第二个事件。更新此第二个事件的先前列表可能如下所示：
- en: '![Diagram  Description automatically generated](img/B17290_04_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_02.png)'
- en: 'Figure 4.2: Non-mutually exclusive events are depicted as overlapping partitions'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：非互斥事件表示为重叠的分区
- en: Notice in the diagram that the Viagra circle overlaps with both the spam and
    ham areas of the diagram and the spam circle includes an area not covered by the
    Viagra circle. This implies that not all spam messages contain the term Viagra
    and some messages with the term Viagra are ham. However, because this word appears
    very rarely outside spam, its presence in a new incoming message would be strong
    evidence that the message is spam.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在图中，Viagra圆圈与图中的垃圾邮件和ham区域重叠，并且垃圾邮件圆圈包括Viagra圆圈未覆盖的区域。这表明并非所有垃圾邮件都包含Viagra这个词，并且一些包含Viagra的消息是ham。然而，由于这个词在垃圾邮件之外出现得非常少，它在新的传入消息中的出现将是该消息是垃圾邮件的强烈证据。
- en: 'To zoom in for a closer look at the overlap between these circles, we’ll employ
    a visualization known as a **Venn diagram**. First used in the late 19th century
    by mathematician John Venn, the diagram uses circles to illustrate the overlap
    between sets of items. As in most Venn diagrams, the size and degree of overlap
    of the circles in the depiction is not meaningful. Instead, it is used as a reminder
    to allocate probability to all combinations of events. A Venn diagram for spam
    and Viagra might be depicted as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地观察这两个圆之间的重叠，我们将使用一种称为**维恩图**的可视化方法。这种图最早在19世纪末由数学家约翰·文恩使用，它使用圆来表示项目集合的重叠。与大多数维恩图一样，图中圆的大小和重叠程度没有意义。相反，它被用作提醒，将概率分配给所有事件组合。垃圾邮件和Viagra的维恩图可能如下所示：
- en: '![Diagram  Description automatically generated](img/B17290_04_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_04_03.png)'
- en: 'Figure 4.3: A Venn diagram illustrates the overlap of the spam and Viagra events'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：一个维恩图展示了垃圾邮件和Viagra事件的交集
- en: We know that 20 percent of all messages were spam (the left circle), and 5 percent
    of all messages contained the word *Viagra* (the right circle). We would like
    to quantify the degree of overlap between these two proportions. In other words,
    we hope to estimate the probability that both *P(spam)* and *P(Viagra)* occur,
    which can be written as *P(spam ![](img/B17290_04_001.png) Viagra)*. The *![](img/B17290_04_001.png)*
    symbol signifies the intersection of the two events; the notation *A ![](img/B17290_04_001.png)
    B* refers to the event in which both *A* and *B* occur.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道所有消息中有20%是垃圾邮件（左边的圆），所有消息中有5%包含*Viagra*这个词（右边的圆）。我们希望量化这两个比例之间的重叠程度。换句话说，我们希望估计*P(spam)*和*P(Viagra)*同时发生的概率，这可以写成*P(spam
    ![图片](img/B17290_04_001.png) Viagra)*。*![图片](img/B17290_04_001.png)*符号表示两个事件的交集；*A
    ![图片](img/B17290_04_001.png) B*的表示法指的是*A*和*B*同时发生的事件。
- en: Calculating *P(spam ![](img/B17290_04_001.png) Viagra)* depends on the **joint
    probability** of the two events, or how the probability of one event is related
    to the probability of the other. If the two events are totally unrelated, they
    are called **independent events**. This is not to say that independent events
    cannot occur at the same time; event independence simply implies that knowing
    the outcome of one event does not provide any information about the outcome of
    the other. For instance, the outcome of a heads result on a coin flip is independent
    of whether the weather is rainy or sunny on any given day.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*P(spam ![图片](img/B17290_04_001.png) Viagra)*取决于两个事件的**联合概率**，即一个事件的概率如何与另一个事件的概率相关。如果两个事件完全无关，它们被称为**独立事件**。这并不是说独立事件不能同时发生；事件独立性仅仅意味着知道一个事件的结局不会提供任何关于另一个事件结局的信息。例如，抛硬币得到正面结果的结局与某一天是雨天还是晴天无关。
- en: If all events were independent, it would be impossible to predict one event
    by observing another. In other words, **dependent events** are the basis of predictive
    modeling. Just as the presence of clouds is predictive of a rainy day, the appearance
    of the word *Viagra* is predictive of a spam email.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有事件都是独立的，那么通过观察另一个事件来预测一个事件将是不可能的。换句话说，**相关事件**是预测建模的基础。就像云的存在预示着雨天一样，*Viagra*这个词的出现预示着垃圾邮件。
- en: '![](img/B17290_04_04.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_04.png)'
- en: 'Figure 4.4: Dependent events are required for machines to learn how to identify
    useful patterns'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：机器学习如何识别有用模式需要相关事件
- en: Calculating the probability of dependent events is a bit more complex than for
    independent events. If *P(spam)* and *P(Viagra)* were independent, we could easily
    calculate *P(spam ![](img/B17290_04_001.png) Viagra)*, the probability of both
    events happening at the same time. Because 20 percent of all messages are spam,
    and 5 percent of all emails contain the word *Viagra*, we could assume that 1
    percent of all messages with the term *Viagra* are spam. This is because *0.05
    * 0.20 = 0.01*. More generally, for independent events *A* and *B*, the probability
    of both happening can be computed as *P(A ![](img/B17290_04_001.png) B) = P(A)
    * P(B)*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相关事件的概率比独立事件要复杂一些。如果*P(spam)*和*P(Viagra)*是独立的，我们可以轻松地计算出*P(spam ![图片](img/B17290_04_001.png)
    Viagra)*，即两个事件同时发生的概率。因为所有消息中有20%是垃圾邮件，所有邮件中有5%包含*Viagra*这个词，我们可以假设所有包含*Viagra*这个词的消息中有1%是垃圾邮件。这是因为*0.05
    * 0.20 = 0.01*。更普遍地，对于独立事件*A*和*B*，两个事件同时发生的概率可以计算为*P(A ![图片](img/B17290_04_001.png)
    B) = P(A) * P(B)*。
- en: That said, we know that *P(spam)* and *P(Viagra)* are likely to be highly dependent,
    which means that this calculation is incorrect. To obtain a more reasonable estimate,
    we need to use a more careful formulation of the relationship between these two
    events, which is based on more advanced Bayesian methods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们知道*P(spam)*和*P(Viagra)*很可能高度相关，这意味着这个计算是不正确的。为了得到一个更合理的估计，我们需要使用这两个事件之间关系的更谨慎的公式，这个公式基于更先进的贝叶斯方法。
- en: Computing conditional probability with Bayes’ theorem
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理计算条件概率
- en: 'The relationships between dependent events can be described using **Bayes’
    theorem**, which provides a way of thinking about how to revise an estimate of
    the probability of one event in light of the evidence provided by another. One
    formulation is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用**贝叶斯定理**来描述相关事件之间的关系，它提供了一种思考如何根据另一个事件提供的证据来修订一个事件概率估计的方法。一种公式如下：
- en: '![](img/B17290_04_008.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_008.png)'
- en: The notation *P(A|B)* is read as the probability of event *A* given that event
    *B* occurred. This is known as **conditional probability** since the probability
    of *A* is dependent (that is, conditional) on what happened with event *B*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 符号*P(A|B)*读作事件*B*发生的情况下事件*A*的概率。这被称为**条件概率**，因为*A*的概率依赖于（即条件于）事件*B*的发生。
- en: Bayes’ theorem states that the best estimate of *P(A|B)* is the proportion of
    trials in which *A* occurred with *B*, out of all the trials in which *B* occurred.
    This implies that the probability of event *A* is higher if *A* and *B* often
    occur together each time *B* is observed. Note that this formula adjusts *P(A
    ![](img/B17290_04_001.png) B)* for the probability of *B* occurring. If *B* is
    extremely rare, *P(B)* and *P(A ![](img/B17290_04_001.png) B)* will always be
    small; however, if *A* almost always happens together with *B*, *P(A|B)* will
    still be high in spite of *B*’s rarity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理表明，*P(A|B)*的最佳估计是在所有发生事件*B*的试验中，事件*A*发生的试验比例。这意味着如果每次观察到*B*时*A*和*B*经常一起发生，事件*A*的概率就会更高。请注意，这个公式调整了*P(A
    ![图片](img/B17290_04_001.png) B)*以反映*B*发生的概率。如果*B*非常罕见，*P(B)*和*P(A ![图片](img/B17290_04_001.png)
    B)*将始终很小；然而，如果*A*几乎总是与*B*一起发生，尽管*B*很罕见，*P(A|B)*仍然会很高。
- en: 'By definition, *P(A ![](img/B17290_04_001.png) B) = P(A|B) * P(B)*, a fact
    that can be easily derived by applying a bit of algebra to the previous formula.
    Rearranging this formula once more with the knowledge that *P(A ![](img/B17290_04_001.png)
    B) = P(B ![](img/B17290_04_001.png) A)* results in the conclusion that *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, which we can then use in the following formulation of Bayes’
    theorem:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，*P(A ![图片](img/B17290_04_001.png) B) = P(A|B) * P(B)*，这是一个可以通过对先前公式应用一点代数轻松推导出的事实。利用*P(A
    ![图片](img/B17290_04_001.png) B) = P(B ![图片](img/B17290_04_001.png) A)*的知识重新排列这个公式，我们得出结论，*P(A
    ![图片](img/B17290_04_001.png) B) = P(B|A) * P(A)*，我们可以在贝叶斯定理的以下公式中使用它：
- en: '![](img/B17290_04_016.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_04_016.png)'
- en: In fact, this is the traditional formulation of Bayes’ theorem for reasons that
    will become clear as we apply it to machine learning. First, to better understand
    how Bayes’ theorem works in practice, let’s revisit our hypothetical spam filter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这是基于我们将它应用于机器学习时将变得清晰的原因的传统贝叶斯定理公式。首先，为了更好地理解贝叶斯定理在实际中的工作原理，让我们回顾一下我们的假设性垃圾邮件过滤器。
- en: Without knowledge of an incoming message’s content, the best estimate of its
    spam status would be *P(spam)*, the probability that any prior message was spam.
    This estimate is known as the **prior probability**. We found this previously
    to be 20 percent.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道收到的邮件内容的情况下，对其是否为垃圾邮件的最佳估计将是*P(spam)*，即任何先前邮件是垃圾邮件的概率。这个估计被称为**先验概率**。我们之前发现这个概率是20%。
- en: Suppose that you obtained additional evidence by looking more carefully at the
    set of previously received messages and examining the frequency with which the
    term *Viagra* appeared. The probability that the word *Viagra* was used in previous
    spam messages, or *P(Viagra|spam)*, is called the **likelihood**. The probability
    that *Viagra* appeared in any message at all, or *P(Viagra)*, is known as the
    **marginal likelihood**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你通过更仔细地查看先前收到的邮件集并检查“Viagra”一词出现的频率获得了额外的证据。单词*Viagra*在先前垃圾邮件中被使用的概率，或*P(Viagra|spam)*，被称为**似然性**。*Viagra*在任何邮件中出现的概率，或*P(Viagra)*，被称为**边缘似然性**。
- en: 'By applying Bayes’ theorem to this evidence, we can compute a **posterior probability**
    that measures how likely a message is to be spam. If the posterior probability
    is greater than 50 percent, the message is more likely to be spam than ham, and
    it should perhaps be filtered. The following formula shows how Bayes’ theorem
    is applied to the evidence provided by previous email messages:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将贝叶斯定理应用于这一证据，我们可以计算一个**后验概率**，该概率衡量一条消息是垃圾邮件的可能性。如果后验概率大于50%，则该消息更有可能是垃圾邮件而不是正常邮件，可能需要过滤。以下公式显示了贝叶斯定理是如何应用于先前电子邮件消息提供的证据的：
- en: '![Shape  Description automatically generated with medium confidence](img/B17290_04_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![形状描述自动生成，置信度中等](img/B17290_04_05.png)'
- en: 'Figure 4.5: Bayes’ theorem acting on previously received emails'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：贝叶斯定理作用于先前收到的电子邮件
- en: 'To calculate the components of Bayes’ theorem, it helps to construct a **frequency
    table** (shown on the left in the tables that follow) recording the number of
    times *Viagra* appeared in spam and ham messages. Just like a two-way cross-tabulation,
    one dimension of the table indicates levels of the class variable (spam or ham),
    while the other dimension indicates levels for features (Viagra: yes or no). The
    cells then indicate the number of instances that have the specified combination
    of the class value and feature value.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算贝叶斯定理的组成部分，构建一个**频率表**（如下表中左侧所示）记录*Viagra*在垃圾邮件和正常邮件中出现的次数是有帮助的。就像一个双向交叉表一样，表的其中一个维度表示类别变量（垃圾邮件或正常邮件）的水平，而另一个维度表示特征（Viagra：是或否）的水平。然后，单元格表示具有指定类别值和特征值的实例数量。
- en: The frequency table can then be used to construct a **likelihood table**, as
    shown on the right in the following tables. The rows of the likelihood table indicate
    the conditional probabilities for *Viagra* (yes/no), given that an email was either
    spam or ham.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用频率表来构建一个**似然表**，如下表中右侧所示。似然表的行表示在电子邮件是垃圾邮件或正常邮件的情况下，对于*Viagra*（是/否）的条件概率。
- en: '![Diagram  Description automatically generated](img/B17290_04_06.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17290_04_06.png)'
- en: 'Figure 4.6: Frequency and likelihood tables are the basis for computing the
    posterior probability of spam'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：频率和似然表是计算垃圾邮件后验概率的基础
- en: The likelihood table reveals that *P(Viagra=Yes|spam) = 4 / 20 = 0.20*, indicating
    that there is a 20 percent probability that a message contains the term *Viagra*
    given that the message is spam. Additionally, since *P(A ![](img/B17290_04_001.png)
    B) = P(B|A) * P(A)*, we can calculate *P(spam ![](img/B17290_04_001.png) Viagra)*
    as *P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04*. The same result
    can be found in the frequency table, which notes that 4 out of 100 messages were
    spam and contained the term *Viagra*. Either way, this is four times greater than
    the previous estimate of 0.01 we calculated as *P(A ![](img/B17290_04_001.png)
    B) = P(A) * P(B)* under the false assumption of independence. This, of course,
    illustrates the importance of Bayes’ theorem for estimating joint probability.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 似然表显示*P(Viagra=Yes|spam) = 4 / 20 = 0.20*，这表明如果一条消息是垃圾邮件，那么包含术语*Viagra*的概率是20%。此外，由于*P(A
    ![](img/B17290_04_001.png) B) = P(B|A) * P(A)*，我们可以计算*P(spam ![](img/B17290_04_001.png)
    Viagra)*为*P(Viagra|spam) * P(spam) = (4 / 20) * (20 / 100) = 0.04*。同样的结果可以在频率表中找到，该表指出100条消息中有4条是垃圾邮件并包含术语*Viagra*。无论如何，这比我们之前在错误假设独立性下计算的*P(A
    ![](img/B17290_04_001.png) B) = P(A) * P(B)*的估计值0.01高出四倍。这当然说明了贝叶斯定理在估计联合概率中的重要性。
- en: To compute the posterior probability, *P(spam|Viagra)*, we simply take *P(Viagra|spam)
    * P(spam) / P(Viagra)*, or *(4 / 20) * (20 / 100) / (5 / 100) = 0.80*. Therefore,
    the probability that a message is spam is 80 percent given that it contains the
    word *Viagra*. In light of this finding, any message containing this term should
    probably be filtered.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算后验概率，*P(spam|Viagra)*，我们只需取*P(Viagra|spam) * P(spam) / P(Viagra)*，或者*(4 /
    20) * (20 / 100) / (5 / 100) = 0.80*。因此，如果一条消息包含单词*Viagra*，那么这条消息是垃圾邮件的概率是80%。鉴于这一发现，任何包含此术语的消息可能应该被过滤。
- en: This is very much how commercial spam filters work, although they consider a
    much larger number of words simultaneously when computing the frequency and likelihood
    tables. In the next section, we’ll see how this method can be adapted to accommodate
    cases when additional features are involved.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是商业垃圾邮件过滤器的工作方式，尽管在计算频率和似然表时，它们会同时考虑更多的单词。在下一节中，我们将看到如何将这种方法适应涉及额外特征的情况。
- en: The Naive Bayes algorithm
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单贝叶斯算法
- en: 'The **Naive Bayes** algorithm defines a simple method for applying Bayes’ theorem
    to classification problems. Although it is not the only machine learning method
    that utilizes Bayesian methods, it is the most common. It grew in popularity due
    to its successes in text classification, where it was once the de facto standard.
    The strengths and weaknesses of this algorithm are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**算法定义了一种简单的方法，将贝叶斯定理应用于分类问题。尽管它不是唯一利用贝叶斯方法的机器学习方法，但它是最常见的。由于它在文本分类中的成功，朴素贝叶斯变得非常流行，一度成为事实上的标准。该算法的优点和缺点如下：'
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Simple, fast, and very effective
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单、快速且非常有效
- en: Does well with noisy and missing data and large numbers of features
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在有噪声和缺失数据以及大量特征的情况下表现良好
- en: Requires relatively few examples for training
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要相对较少的训练示例
- en: Easy to obtain the estimated probability for a prediction
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易获得预测的估计概率
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Relies on an often-faulty assumption of equally important and independent features
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于一个经常是错误的假设，即特征同等重要且相互独立
- en: Not ideal for datasets with many numeric features
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于具有许多数值特征的集合
- en: Estimated probabilities are less reliable than the predicted classes
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计的概率不如预测的类别可靠
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The Naive Bayes algorithm is named as such because it makes some so-called “naive”
    assumptions about the data. In particular, Naive Bayes assumes that all of the
    features in the dataset are **equally important and independent**. These assumptions
    are rarely true in most real-world applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法之所以被称为“朴素”，是因为它对数据做出了一些所谓的“朴素”假设。特别是，朴素贝叶斯假设数据集中的所有特征都是**同等重要且相互独立**的。在大多数实际应用中，这些假设很少是真实的。
- en: For example, when attempting to identify spam by monitoring email messages,
    it is almost certainly true that some features will be more important than others.
    For example, the email sender may be a more important indicator of spam than the
    message text. Additionally, the words in the message body are not independent
    of one another, since the appearance of some words is a very good indication that
    other words are also likely to appear. A message with the word *Viagra* will probably
    also contain the word *prescription* or *drugs*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当尝试通过监控电子邮件消息来识别垃圾邮件时，几乎可以肯定的是，某些特征将比其他特征更重要。例如，电子邮件发送者可能是比消息文本更重要的垃圾邮件指示器。此外，消息正文中的单词并不是相互独立的，因为某些单词的出现是其他单词也很可能出现的很好指示。包含“Viagra”一词的消息很可能也包含“prescription”或“drugs”一词。
- en: However, in most cases, even when these assumptions are violated, Naive Bayes
    still performs surprisingly well. This is true even in circumstances where strong
    dependencies are found among the features. Due to the algorithm’s versatility
    and accuracy across many types of conditions, particularly with smaller training
    datasets, Naive Bayes is often a reasonable baseline candidate for classification
    learning tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，即使这些假设被违反，朴素贝叶斯仍然表现出惊人的性能。即使在特征之间存在强依赖性的情况下，也是如此。由于该算法在各种条件下的灵活性和准确性，尤其是在较小的训练数据集上，朴素贝叶斯经常是分类学习任务的合理基线候选者。
- en: The exact reason why Naive Bayes works well in spite of its faulty assumptions
    has been the subject of much speculation. One explanation is that it is not important
    to obtain a precise estimate of probability so long as the predictions are accurate.
    For instance, if a spam filter correctly identifies spam, does it matter whether
    the predicted probability of spam was 51 percent or 99 percent? For one discussion
    of this topic, refer to *On the Optimality of the Simple Bayesian Classifier under
    Zero-One Loss, Domingos, P. and Pazzani, M., Machine Learning, 1997, Vol. 29,
    pp. 103-130*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管朴素贝叶斯存在错误的假设，但它为何表现良好的确切原因一直是许多猜测的对象。一种解释是，只要预测准确，获得概率的精确估计并不重要。例如，如果一个垃圾邮件过滤器正确地识别出垃圾邮件，那么预测垃圾邮件的概率是51%还是99%又有什么关系呢？关于这个话题的一个讨论，请参阅*《在零一损失下简单贝叶斯分类器的最优性》，作者：Domingos,
    P. 和 Pazzani, M.，机器学习，1997年，第29卷，第103-130页*。
- en: Classification with Naive Bayes
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: 'Let’s extend our spam filter by adding a few additional terms to be monitored
    in addition to the term *Viagra*: *money*, *groceries*, and *unsubscribe*. The
    Naive Bayes learner is trained by constructing a likelihood table for the appearance
    of these four words (labeled *W*¹, *W*², *W*³, and *W*⁴), as shown in the following
    diagram for 100 emails:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过添加一些额外的监控术语来扩展我们的垃圾邮件过滤器，除了术语*Viagra*之外，还包括*money*、*groceries*和*unsubscribe*。朴素贝叶斯学习器通过构建这四个词（标记为*W*¹、*W*²、*W*³和*W*⁴）出现的可能性表来训练，如下图中100封电子邮件所示：
- en: '![Table  Description automatically generated](img/B17290_04_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_04_07.png)'
- en: 'Figure 4.7: An expanded table adds likelihoods for additional terms in spam
    and ham messages'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：扩展的表格增加了垃圾邮件和正常邮件中额外术语的可能性
- en: As new messages are received, we need to calculate the posterior probability
    to determine whether they are more likely spam or ham, given the likelihood of
    the words being found in the message text. For example, suppose that a message
    contains the terms *Viagra* and *unsubscribe* but does not contain either *money*
    or *groceries*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当收到新消息时，我们需要计算后验概率，以确定它们更有可能是垃圾邮件还是正常邮件，给定在消息文本中找到这些词的可能性。例如，假设一条消息包含术语*Viagra*和*unsubscribe*，但不包含*money*或*groceries*。
- en: 'Using Bayes’ theorem, we can define the problem as shown in the following formula.
    This computes the probability that a message is spam given that *Viagra = Yes*,
    *Money = No*, *Groceries = No*, and *Unsubscribe = Yes*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们可以将问题定义为以下公式。这计算了在*Viagra = 是*、*Money = 否*、*Groceries = 否*和*Unsubscribe
    = 是*的条件下，一条消息是垃圾邮件的概率：
- en: '![](img/B17290_04_020.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_020.png)'
- en: For two reasons, this formula is computationally difficult to solve. First,
    as additional features are added, tremendous amounts of memory are needed to store
    the probabilities for all possible intersecting events. Imagine the complexity
    of a Venn diagram for the events for four words, let alone for hundreds or more.
    Second, many of these potential intersections will never have been observed in
    past data, which would lead to a joint probability of zero and problems that will
    become clear later.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两个原因，这个公式在计算上很难解决。首先，随着附加特征的添加，需要大量的内存来存储所有可能交点事件的概率。想象一下四个词事件Venn图的复杂性，更不用说数百个或更多。其次，许多这些潜在的交点在过去的资料中从未被观察到，这会导致联合概率为零，并导致后面会变得明显的问题。
- en: The computation becomes more reasonable if we exploit the fact that Naive Bayes
    makes the naive assumption of independence among events. Specifically, it assumes
    **class-conditional independence**, which means that events are independent so
    long as they are conditioned on the same class value. The conditional independence
    assumption allows us to use the probability rule for independent events, which
    states that *P(A ![](img/B17290_04_001.png) B) = P(A) * P(B)*. This simplifies
    the numerator by allowing us to multiply the individual conditional probabilities
    rather than computing a complex conditional joint probability.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们利用朴素贝叶斯对事件之间独立性的朴素假设，计算将变得更加合理。具体来说，它假设**类条件独立性**，这意味着只要事件基于相同的类值，它们就是独立的。条件独立性假设允许我们使用独立事件的概率规则，该规则指出
    *P(A ![](img/B17290_04_001.png) B) = P(A) * P(B)*。这通过允许我们乘以单个条件概率而不是计算复杂的条件联合概率来简化分子。
- en: 'Lastly, because the denominator does not depend on the target class (spam or
    ham), it is treated as a constant value and can be ignored for the time being.
    This means that the conditional probability of spam can be expressed as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为分母不依赖于目标类（垃圾邮件或正常邮件），它被视为一个常数，暂时可以忽略。这意味着垃圾邮件的条件概率可以表示为：
- en: '![](img/B17290_04_022.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_022.png)'
- en: 'And the probability that the message is ham can be expressed as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以表示消息是正常邮件的概率为：
- en: '![](img/B17290_04_023.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_04_023.png)'
- en: Note that the equals symbol has been replaced by the proportional-to symbol
    (similar to a sideways, open-ended “8”) to indicate the fact that the denominator
    has been omitted.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，等号已被比例符号（类似于侧向的、开口的“8”）替换，以表明分母已被省略。
- en: 'Using the values in the likelihood table, we can start filling in numbers in
    these equations. The overall likelihood of spam is then:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可能性表中的值，我们可以开始填写这些方程中的数字。然后，垃圾邮件的整体可能性如下：
- en: '*(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*(4 / 20) * (10 / 20) * (20 / 20) * (12 / 20) * (20 / 100) = 0.012*'
- en: 'While the likelihood of ham is:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 而正常邮件的可能性是：
- en: '*(1 / 80) * (66 / 80) * (71 / 80) * (23 / 80) * (80 / 100) = 0.002*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Because *0.012 / 0.002 = 6*, we can say that this message is 6 times more likely
    to be spam than ham. However, to convert these numbers into probabilities, we
    need one last step to reintroduce the denominator that has been excluded. Essentially,
    we must re-scale the likelihood of each outcome by dividing it by the total likelihood
    across all possible outcomes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, the probability of spam is equal to the likelihood that the message
    is spam divided by the likelihood that the message is either spam or ham:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '*0.012 / (0.012 + 0.002) = 0.857*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the probability of ham is equal to the likelihood that the message
    is ham divided by the likelihood that the message is either spam or ham:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*0.002 / (0.012 + 0.002) = 0.143*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Given the pattern of words found in this message, we expect that the message
    is spam with an 85.7 percent probability, and ham with a 14.3 percent probability.
    Because these are mutually exclusive and exhaustive events, the probabilities
    sum up to 1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The Naive Bayes classification algorithm used in the preceding example can
    be summarized by the following formula. The probability of level *L* for class
    *C*, given the evidence provided by features *F*[1] through *F*[n], is equal to
    the product of the probabilities of each piece of evidence conditioned on the
    class level, the prior probability of the class level, and a scaling factor *1
    / Z*, which converts the likelihood values into probabilities. This is formulated
    as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_04_024.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Although this equation seems intimidating, as the spam filtering example illustrated,
    the series of steps is fairly straightforward. Begin by building a frequency table,
    use this to build a likelihood table, and multiply out the conditional probabilities
    with the naive assumption of independence. Finally, divide by the total likelihood
    to transform each class likelihood into a probability. After attempting this calculation
    a few times by hand, it will become second nature.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace estimator
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we employ Naive Bayes on more complex problems, there are some nuances
    to consider. Suppose we received another message, this time containing all four
    terms: *Viagra*, *groceries*, *money*, and *unsubscribe*. Using the Naive Bayes
    algorithm as before, we can compute the likelihood of spam as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*(4 / 20) * (10 / 20) * (0 / 20) * (12 / 20) * (20 / 100) = 0*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'And the likelihood of ham is:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '*(1 / 80) * (14 / 80) * (8 / 80) * (23 / 80) * (80 / 100) = 0.00005*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the probability of spam is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '*0 / (0 + 0.00005) = 0*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'And the probability of ham is:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*0.00005 / (0 + 0\. 0.00005) = 1*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: These results suggest that the message is spam with 0 percent probability and
    ham with 100 percent probability. Does this prediction make sense? Probably not.
    The message contains several words usually associated with spam, including *Viagra*,
    which is rarely used in legitimate messages. It is therefore very likely that
    the message has been incorrectly classified.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: This problem arises if an event never occurs for one or more levels of the class
    and therefore the resulting likelihoods are zero. For instance, the term *groceries*
    had never previously appeared in a spam message. Consequently, *P(groceries|spam)
    = 0%*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Now, because probabilities in the Naive Bayes formula are multiplied in a chain,
    this zero-percent value causes the posterior probability of spam to be zero, giving
    the word *groceries* the ability to effectively nullify and overrule all of the
    other evidence. Even if the email was otherwise overwhelmingly expected to be
    spam, the absence of the word *groceries* in spam will always veto the other evidence
    and result in the probability of spam being zero.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem involves using something called the **Laplace estimator**,
    which is named after the French mathematician Pierre-Simon Laplace. The Laplace
    estimator adds a small number to each of the counts in the frequency table, which
    ensures that each feature has a non-zero probability of occurring with each class.
    Typically, the Laplace estimator is set to one, which ensures that each class-feature
    combination is found in the data at least once.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace estimator can be set to any value and does not necessarily even
    have to be the same for each of the features. If you were a devoted Bayesian,
    you could use a Laplace estimator to reflect a presumed prior probability of how
    a feature relates to a class. In practice, given a large enough training dataset,
    this is excessive. Consequently, the value of one is almost always used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this affects our prediction for this message. Using a Laplace
    value of 1, we add 1 to each numerator in the likelihood function. Then, we need
    to add 4 to each conditional probability denominator to compensate for the 4 additional
    values added to the numerator. The likelihood of spam is therefore:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '*(5 / 24) * (11 / 24) * (1 / 24) * (13 / 24) * (20 / 100) = 0.0004*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'And the likelihood of ham is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*(2 / 84) * (15 / 84) * (9 / 84) * (24 / 84) * (80 / 100) = 0.0001*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: By computing *0.0004 / (0.0004 + 0.0001)*, we find that the probability of spam
    is 80 percent and therefore the probability of ham is about 20 percent. This is
    a more plausible result than the *P(spam) = 0* computed when the term *groceries*
    alone determined the result.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Although the Laplace estimator was added to the numerator and denominator of
    the likelihoods, it was not added to the prior probabilities—the values of 20/100
    and 80/100\. This is because our best estimate of the overall probability of spam
    and ham remains at 20% and 80% respectively given what was observed in the data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Using numeric features with Naive Bayes
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naive Bayes uses frequency tables for learning the data, which means that each
    feature must be categorical in order to create the combinations of class and feature
    values comprising the matrix. Since numeric features do not have categories of
    values, the preceding algorithm does not work directly with numeric data. There
    are, however, ways that this can be addressed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: One easy and effective solution is to **discretize** numeric features, which
    simply means that the numbers are put into categories known as **bins**. For this
    reason, discretization is also sometimes called **binning**. This method works
    best when there are large amounts of training data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different ways to discretize a numeric feature. Perhaps the
    most common is to explore the data for natural categories or **cut points** in
    the distribution. For example, suppose that you added a feature to the spam dataset
    that recorded the time of day or night the email was sent, from 0 to 24 hours
    past midnight. Depicted using a histogram, the time data might look something
    like the following diagram:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart, histogram  Description automatically generated](img/B17290_04_08.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: A histogram visualizing the distribution of the time emails were
    received'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In the early hours of the morning, message frequency is low. Activity picks
    up during business hours and tapers off in the evening. This creates four natural
    bins of activity, as partitioned by the dashed lines. These indicate places where
    the numeric data could be divided into levels to create a new categorical feature,
    which could then be used with Naive Bayes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The choice of four bins was based on the natural distribution of data and a
    hunch about how the proportion of spam might change throughout the day. We might
    expect that spammers operate in the late hours of the night, or they may operate
    during the day when people are likely to check their email. That said, to capture
    these trends, we could have just as easily used three bins or twelve.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: If there are no obvious cut points, one option is to discretize the feature
    using quantiles, which were introduced in *Chapter 2*, *Managing and Understanding
    Data*. You could divide the data into three bins with tertiles, four bins with
    quartiles, or five bins with quintiles.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that discretizing a numeric feature always results
    in a reduction of information, as the feature’s original granularity is reduced
    to a smaller number of categories. It is important to strike a balance. Too few
    bins can result in important trends being obscured. Too many bins can result in
    small counts in the Naive Bayes frequency table, which can increase the algorithm’s
    sensitivity to noisy data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Example – filtering mobile phone spam with the Naive Bayes algorithm
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the worldwide use of mobile phones has grown, a new avenue for electronic
    junk mail has opened for disreputable marketers. These advertisers utilize SMS
    text messages to target potential consumers with unwanted advertising known as
    SMS spam. This type of spam is troublesome because, unlike email spam, an SMS
    message is particularly disruptive due to the omnipresence of one’s mobile phone.
    Developing a classification algorithm that could filter SMS spam would provide
    a useful tool for cellular phone providers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Since Naive Bayes has been used successfully for email spam filtering, it seems
    likely that it could also be applied to SMS spam. However, relative to email spam,
    SMS spam poses additional challenges for automated filters. SMS messages are often
    limited to 160 characters, reducing the amount of text that can be used to identify
    whether a message is junk. The limit, combined with small mobile phone keyboards,
    has led many to adopt a form of SMS shorthand lingo, which further blurs the line
    between legitimate messages and spam. Let’s see how a simple Naive Bayes classifier
    handles these challenges.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To develop the Naive Bayes classifier, we will use data adapted from the SMS
    Spam Collection at [https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: To read more about how the SMS Spam Collection was developed, refer to *On the
    Validity of a New SMS Spam Collection, Gómez, J. M., Almeida, T. A., and Yamakami,
    A., Proceedings of the 11th IEEE International Conference on Machine Learning
    and Applications, 2012*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset includes the text of SMS messages, along with a label indicating
    whether the message is unwanted. Junk messages are labeled spam, while legitimate
    messages are labeled ham. Some examples of spam and ham are shown in the following
    table:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample SMS ham** | **Sample SMS spam** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel
    bleh. But at least its not writhing pain kind of bleh.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If he started searching he will get job in few days. he have great potential
    and talent.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I got another job! The one at the hospital doing data analysis or something,
    starts on monday! Not sure when my thesis will got finished
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations ur awarded 500 of CD vouchers or 125gift guaranteed & Free entry
    2 100 wkly draw txt MUSIC to 87066
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: December only! Had your mobile 11mths+? You are entitled to update to the latest
    colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valentines Day Special! Win over £1000 in our quiz and take your partner on
    the trip of a lifetime! Send GO to 83600 now. 150p/msg rcvd.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding messages, do you notice any distinguishing characteristics
    of spam? One notable characteristic is that two of the three spam messages use
    the word *free*, yet this word does not appear in any of the ham messages. On
    the other hand, two of the ham messages cite specific days of the week, as compared
    to zero in spam messages.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Our Naive Bayes classifier will take advantage of such patterns in the word
    frequency to determine whether the SMS messages seem to better fit the profile
    of spam or ham. While it’s not inconceivable that the word *free* would appear
    outside of a spam SMS, a legitimate message is likely to provide additional words
    giving context.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a ham message might ask, “Are you free on Sunday?” whereas a spam
    message might use the phrase “free ringtones.” The classifier will compute the
    probability of spam and ham given the evidence provided by all the words in the
    message.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step toward constructing our classifier involves processing the raw
    data for analysis. Text data is challenging to prepare because it is necessary
    to transform the words and sentences into a form that a computer can understand.
    We will transform our SMS data into a representation known as **bag-of-words**,
    which provides a binary feature indicating whether each word appears in the given
    example while ignoring word order or the context in which the word appears. Although
    this is a relatively simple representation, as we will soon demonstrate, it performs
    well enough for many classification tasks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used here has been modified slightly from the original to make it
    easier to work with in R. If you plan on following along with the example, download
    the `sms_spam.csv` file from the Packt website and save it to your R working directory.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by importing the CSV data and saving it to a data frame:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the `str()` function, we see that the `sms_raw` data frame includes 5,559
    total SMS messages with two features: `type` and `text`. The SMS type has been
    coded as either ham or spam. The `text` element stores the full raw SMS message
    text:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `type` element is currently a character vector. Since this is a categorical
    variable, it would be better to convert it into a factor, as shown in the following
    code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Examining this with the `str()` and `table()` functions, we see that `type`
    has now been appropriately recoded as a factor. Additionally, we see that 747
    (about 13 percent) of SMS messages in our data were labeled as `spam`, while the
    others were labeled as `ham`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For now, we will leave the message text alone. As you will learn in the next
    section, processing raw SMS messages will require the use of a new set of powerful
    tools designed specifically for processing text data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – cleaning and standardizing text data
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SMS messages are strings of text composed of words, spaces, numbers, and punctuation.
    Handling this type of complex data takes a large amount of thought and effort.
    One needs to consider how to remove numbers and punctuation; handle uninteresting
    words, such as *and*, *but*, and *or*; and break apart sentences into individual
    words. Thankfully, this functionality has been provided by members of the R community
    in a text-mining package titled `tm`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package was originally created by Ingo Feinerer as a dissertation project
    at the Vienna University of Economics and Business. To learn more, see *Text Mining
    Infrastructure in R, Feinerer, I., Hornik, K., and Meyer, D., Journal of Statistical
    Software, 2008, Vol. 25, pp. 1-54*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package can be installed via the `install.packages("tm")` command and
    loaded with the `library(tm)` command. Even if you already have it installed,
    it may be worth redoing the installation to ensure that your version is up to
    date, as the `tm` package is still under active development. This occasionally
    results in changes to its functionality.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was tested using `tm` version 0.7-11, which was current as of May
    2023\. If you see differences in the output or if the code does not work, you
    may be using a different version. The Packt support page for this book, as well
    as its GitHub repository, will post solutions for future `tm` package versions
    if significant changes are noted.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The first step in processing text data involves creating a **corpus**, which
    is a collection of text documents. The documents can be short or long, from individual
    news articles, pages in a book, pages from the web, or even entire books. In our
    case, the corpus will be a collection of SMS messages.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: To create a corpus, we’ll use the `VCorpus()` function in the `tm` package,
    which refers to a volatile corpus—the term “volatile” meaning that it is stored
    in memory as opposed to being stored on disk (the `PCorpus()` function is used
    to access a permanent corpus stored in a database). This function requires us
    to specify the source of documents for the corpus, which could be a computer’s
    filesystem, a database, the web, or elsewhere.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already loaded the SMS message text into R, we’ll use the `VectorSource()`
    reader function to create a source object from the existing `sms_raw$text` vector,
    which can then be supplied to `VCorpus()` as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The resulting corpus object is saved with the name `sms_corpus`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: By specifying an optional `readerControl` parameter, the `VCorpus()` function
    can be used to import text from sources such as PDFs and Microsoft Word files.
    To learn more, examine the *Data Import* section in the `tm` package vignette
    using the `vignette("tm")` command.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'By printing the corpus, we see that it contains documents for each of the 5,559
    SMS messages in the training data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, because the `tm` corpus is essentially a complex list, we can use list
    operations to select documents in the corpus. The `inspect()` function shows a
    summary of the result. For example, the following command will view a summary
    of the first and second SMS messages in the corpus:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To view the actual message text, the `as.character()` function must be applied
    to the desired messages. To view one message, use the `as.character()` function
    on a single list element, noting that the double-bracket notation is required:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To view multiple documents, we’ll need to apply `as.character()` to several
    items in the `sms_corpus` object. For this, we’ll use the `lapply()` function,
    which is part of a family of R functions that applies a procedure to each element
    of an R data structure. These functions, which include `apply()` and `sapply()`
    among others, are one of the key idioms of the R language. Experienced R coders
    use these much like the way `for` or `while` loops are used in other programming
    languages, as they result in more readable (and sometimes more efficient) code.
    The `lapply()` function for applying `as.character()` to a subset of corpus elements
    is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As noted earlier, the corpus contains the raw text of 5,559 text messages. To
    perform our analysis, we need to divide these messages into individual words.
    First, we need to clean the text to standardize the words and remove punctuation
    characters that clutter the result. For example, we would like the strings *Hello!*,
    *HELLO*, and *hello* to be counted as instances of the same word.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The `tm_map()` function provides a method to apply a transformation (also known
    as a mapping) to a `tm` corpus. We will use this function to clean up our corpus
    using a series of transformations and save the result in a new object called `corpus_clean`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first transformation will standardize the messages to use only lowercase
    characters. To this end, R provides a `tolower()` function that returns a lowercase
    version of text strings. In order to apply this function to the corpus, we need
    to use the `tm` wrapper function `content_transformer()` to treat `tolower()`
    as a transformation function that can be used to access the corpus. The full command
    is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To check whether the command worked as expected, let’s inspect the first message
    in the original corpus and compare it to the same in the transformed corpus:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As expected, uppercase letters in the clean corpus have been replaced with lowercase
    versions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The `content_transformer()` function can be used to apply more sophisticated
    text processing and cleanup processes like `grep` pattern matching and replacement.
    Simply write a custom function and wrap it before applying the `tm_map()` function.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue our cleanup by removing numbers from the SMS messages. Although
    some numbers may provide useful information, the majority are likely to be unique
    to individual senders and thus will not provide useful patterns across all messages.
    With this in mind, we’ll strip all numbers from the corpus as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that the preceding code did not use the `content_transformer()` function.
    This is because `removeNumbers()` is included with `tm` along with several other
    mapping functions that do not need to be wrapped. To see the other built-in transformations,
    simply type `getTransformations()`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Our next task is to remove filler words such as *to*, *and*, *but*, and *or*
    from the SMS messages. These terms are known as **stop words** and are typically
    removed prior to text mining. This is due to the fact that although they appear
    very frequently, they do not provide much useful information for our model as
    they are unlikely to distinguish between spam and ham.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Rather than define a list of stop words ourselves, we’ll use the `stopwords()`
    function provided by the `tm` package. This function allows us to access sets
    of stop words from various languages. By default, common English language stop
    words are used. To see the default list, type `stopwords()` at the R command prompt.
    To see the other languages and options available, type `?stopwords` for the documentation
    page.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Even within a single language, there is no single definitive list of stop words.
    For example, the default English list in `tm` includes about 174 words, while
    another option includes 571 words. You can even specify your own list of stop
    words. Regardless of the list you choose, keep in mind the goal of this transformation,
    which is to eliminate useless data while keeping as much useful information as
    possible.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining the stop words alone is not a transformation. What we need is a way
    to remove any words that appear in the stop words list. The solution lies in the
    `removeWords()` function, which is a transformation included with the `tm` package.
    As we have done before, we’ll use the `tm_map()` function to apply this mapping
    to the data, providing the `stopwords()` function as a parameter to indicate the
    words we would like to remove. The full command is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Since `stopwords()` simply returns a vector of stop words, if we had so chosen,
    we could have replaced this function call with our own vector of words to remove.
    In this way, we could expand or reduce the list of stop words to our liking or
    remove a different set of words entirely.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing our cleanup process, we can also eliminate any punctuation from
    the text messages using the built-in `removePunctuation()` transformation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `removePunctuation()` transformation completely strips punctuation characters
    from the text, which can lead to unintended consequences. For example, consider
    what happens when it is applied as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As shown, the lack of a blank space after the ellipses caused the words *hello*
    and *world* to be joined as a single word. While this is not a substantial problem
    right now, it is worth noting for the future.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'To work around the default behavior of `removePunctuation()`, it is possible
    to create a custom function that replaces rather than removes punctuation characters:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This uses R’s `gsub()` function to substitute any punctuation characters in
    `x` with a blank space. This `replacePunctuation()` function can then be used
    with `tm_map()` as with other transformations. The odd syntax of the `gsub()`
    command here is due to the use of a **regular expression**, which specifies a
    pattern that matches text characters. Regular expressions are covered in more
    detail in *Chapter 12*, *Advanced Data Preparation*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Another common standardization for text data involves reducing words to their
    root form in a process called **stemming**. The stemming process takes words like
    *learned*, *learning*, and *learns* and strips the suffix in order to transform
    them into the base form, *learn*. This allows machine learning algorithms to treat
    the related terms as a single concept rather than attempting to learn a pattern
    for each variant.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package provides stemming functionality via integration with the `SnowballC`
    package. At the time of writing, `SnowballC` is not installed by default with
    `tm`, so do so with `install.packages("SnowballC")` if you have not done so already.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The `SnowballC` package is maintained by Milan Bouchet-Valat and provides an
    R interface for the C-based `libstemmer` library, itself based on M. F. Porter’s
    “Snowball” word-stemming algorithm, a widely used open-source stemming method.
    For more details, see [http://snowballstem.org](http://snowballstem.org).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SnowballC` package provides a `wordStem()` function, which for a character
    vector returns the same vector of terms in its root form. For example, the function
    correctly stems the variants of the word *learn* as described previously:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To apply the `wordStem()` function to an entire corpus of text documents, the
    `tm` package includes a `stemDocument()` transformation. We apply this to our
    corpus with the `tm_map()` function exactly as before:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you receive an error message when applying the `stemDocument()` transformation,
    please confirm that you have the `SnowballC` package installed.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'After removing numbers, stop words, and punctuation, then also performing stemming,
    the text messages are left with the blank spaces that once separated the now-missing
    pieces. Therefore, the final step in our text cleanup process is to remove additional
    whitespace using the built-in `stripWhitespace()` transformation:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following table shows the first three messages in the SMS corpus before
    and after the cleaning process. The messages have been limited to the most interesting
    words, and punctuation and capitalization have been removed:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '| **SMS messages before cleaning** | **SMS messages after cleaning** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| `> as.character(sms_corpus[1:3])``[[1]] Hope you are having a good``week.
    Just checking in``[[2]] K..give back my thanks.``[[3]] Am also doing in cbe only.``But
    have to pay.` | `> as.character(sms_corpus_clean[1:3])``[[1]] hope good week just
    check``[[2]] kgive back thank``[[3]] also cbe pay` |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: Data preparation – splitting text documents into words
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the data is processed to our liking, the final step is to split the
    messages into individual terms through a process called **tokenization**. A token
    is a single element of a text string; in this case, the tokens are words.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: As you might assume, the `tm` package provides functionality to tokenize the
    SMS message corpus. The `DocumentTermMatrix()` function takes a corpus and creates
    a data structure called a **document-term matrix** (**DTM**) in which rows indicate
    documents (SMS messages) and columns indicate terms (words).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package also provides a data structure for a **term-document matrix**
    (**TDM**), which is simply a transposed DTM in which the rows are terms and the
    columns are documents. Why the need for both? Sometimes, it is more convenient
    to work with one or the other. For example, if the number of documents is small,
    while the word list is large, it may make sense to use a TDM because it is usually
    easier to display many rows than to display many columns. That said, machine learning
    algorithms will generally require a DTM, as the columns are the features and the
    rows are the examples.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell in the matrix stores a number indicating a count of the times the
    word represented by the column appears in the document represented by the row.
    The following figure depicts only a small portion of the DTM for the SMS corpus,
    as the complete matrix has 5,559 rows and over 7,000 columns:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17290_04_09.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: The DTM for the SMS messages is filled with mostly zeros'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The fact that each cell in the table is zero implies that none of the words
    listed at the top of the columns appear in any of the first five messages in the
    corpus. This highlights the reason why this data structure is called a **sparse
    matrix**; the vast majority of cells in the matrix are filled with zeros. Stated
    in real-world terms, although each message must contain at least one word, the
    probability of any one word appearing in a given message is small.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a DTM sparse matrix from a *tm* corpus involves a single command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This will create an `sms_dtm` object that contains the tokenized corpus using
    the default settings, which apply minimal additional processing. The default settings
    are appropriate because we have already prepared the corpus manually.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we hadn’t already performed the preprocessing, we could
    do so here by providing a list of `control` parameter options to override the
    defaults. For example, to create a DTM directly from the raw, unprocessed SMS
    corpus, we can use the following command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This applies the same preprocessing steps to the SMS corpus in the same order
    as done earlier. However, comparing `sms_dtm` to `sms_dtm2`, we see a slight difference
    in the number of terms in the matrix:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The reason for this discrepancy has to do with a minor difference in the ordering
    of the preprocessing steps. The `DocumentTermMatrix()` function applies its cleanup
    functions to the text strings only after they have been split apart into words.
    Thus, it uses a slightly different stop word removal function. Consequently, some
    words are split differently than when they are cleaned before tokenization.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'To force the two prior DTMs to be identical, we can override the default stop
    words function with our own that uses the original replacement function. Simply
    replace `stopwords = TRUE` with the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The code file for this chapter includes the full set of steps to create an identical
    DTM using a single function call.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences between these bring up an important principle of cleaning text
    data: the order of operations matters. With this in mind, it is very important
    to think through how early steps in the process are going to affect later ones.
    The order presented here will work in many cases, but when the process is tailored
    more carefully to specific datasets and use cases, it may require rethinking.
    For example, if there are certain terms you hope to exclude from the matrix, consider
    whether to search for them before or after stemming. Also, consider how the removal
    of punctuation—and whether the punctuation is eliminated or replaced by blank
    space—affects these steps.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating training and test datasets
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our data prepared for analysis, we now need to split the data into training
    and test datasets so that after our spam classifier is built, it can be evaluated
    on data it has not previously seen. However, even though we need to keep the classifier
    blinded as to the contents of the test dataset, it is important that the split
    occurs after the data has been cleaned and processed. We need exactly the same
    preparation steps to have occurred on both the training and test datasets.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll divide the data into two portions: 75 percent for training and 25 percent
    for testing. Since the SMS messages are sorted in a random order, we can simply
    take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully,
    the DTM object acts very much like a data frame and can be split using the standard
    `[row, col]` operations. As our DTM stores SMS messages as rows and words as columns,
    we must request a specific range of rows and all columns for each:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For convenience later, it is also helpful to save a pair of vectors with the
    labels for each of the rows in the training and testing matrices. These labels
    are not stored in the DTM, so we need to pull them from the original `sms_raw`
    data frame:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To confirm that the subsets are representative of the complete set of SMS data,
    let’s compare the proportion of spam in the training and test data frames:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Both the training data and test data contain about 13 percent spam. This suggests
    that the spam messages were divided evenly between the two datasets.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing text data – word clouds
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **word cloud** is a way to visually depict the frequency at which words appear
    in text data. The cloud is composed of words scattered somewhat randomly around
    the figure. Words appearing more often in the text are shown in a larger font,
    while less common terms are shown in smaller fonts. This type of figure grew in
    popularity as a way to observe trending topics on social media websites.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: The `wordcloud` package provides a simple R function to create this type of
    diagram. We’ll use it to visualize the words in SMS messages. Comparing the clouds
    for spam and ham messages will help us gauge whether our Naive Bayes spam filter
    is likely to be successful. If you haven’t already done so, install and load the
    package by typing `install.packages("wordcloud")` and `library(wordcloud)` at
    the R command line.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The `wordcloud` package was written by Ian Fellows. For more information about
    this package, visit his blog at [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'A word cloud can be created directly from a *tm* corpus object using the syntax:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This will create a word cloud from our prepared SMS corpus. Since we specified
    `random.order = FALSE`, the cloud will be arranged in a non-random order, with
    higher-frequency words placed closer to the center. If we do not specify `random.order`,
    the cloud will be arranged randomly by default.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The `min.freq` parameter specifies the number of times a word must appear in
    the corpus before it will be displayed in the cloud. Since a frequency of 50 is
    about 1 percent of the corpus, this means that a word must be found in at least
    1 percent of the SMS messages to be included in the cloud.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: You might get a warning message noting that R was unable to fit all the words
    in the figure. If so, try increasing the `min.freq` to reduce the number of words
    in the cloud. It might also help to use the `scale` parameter to reduce the font
    size.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting word cloud should appear similar to the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_04_10.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: A word cloud depicting words appearing in all SMS messages'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: A perhaps more interesting visualization involves comparing the clouds for SMS
    spam and ham. Since we did not construct separate corpora for spam and ham, this
    is an appropriate time to note a very helpful feature of the `wordcloud()` function.
    Given a vector of raw text strings, it will automatically apply common text preparation
    processes before displaying the cloud.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use R’s `subset()` function to take a subset of the `sms_raw` data by
    the SMS type. First, we’ll create a subset where the `type` is `spam`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we’ll do the same thing for the `ham` subset:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Be careful to note the double equals sign. Like many programming languages,
    R uses `==` to test equality. If you accidentally use a single equals sign, you’ll
    end up with a subset much larger than you expected!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have two data frames, `spam` and `ham`, each with a `text` feature containing
    the raw text strings for SMS messages. Creating word clouds is as simple as before.
    This time, we’ll use the `max.words` parameter to look at the 40 most common words
    in each of the 2 sets. The `scale` parameter adjusts the maximum and minimum font
    sizes for words in the cloud. Feel free to change these parameters as you see
    fit. This is illustrated in the following code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note that R provides warning messages when running this code that the “transformation
    drops documents.” The warnings are related to the `removePunctuation()` and `removeWords()`
    procedures that `wordcloud()` performs by default when given raw text data rather
    than a term matrix. Basically, there are some messages that are excluded from
    the result because there is no remaining message text after cleaning. For example,
    the ham message with the text *:)* representing the smiley emoji is removed from
    the set after cleaning. This is not a problem for the word clouds and the warnings
    can be ignored.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The resulting word clouds should appear similar to those that follow. Do you
    have a hunch on which one is the spam cloud, and which represents ham?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![Text, letter  Description automatically generated](img/B17290_04_11.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Side-by-side word clouds depicting SMS spam and ham messages'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: As you probably guessed, the spam cloud is on the left. Spam messages include
    words such as *call*, *free*, *mobile*, *claim*, and *stop*; these terms do not
    appear in the ham cloud at all. Instead, ham messages use words such as *can*,
    *sorry*, *love*, and *time*. These stark differences suggest that our Naive Bayes
    model will have some strong keywords to differentiate between the classes.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating indicator features for frequent words
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in the data preparation process is to transform the sparse matrix
    into a data structure that can be used to train a Naive Bayes classifier. Currently,
    the sparse matrix includes over 6,500 features; this is a feature for every word
    that appears in at least one SMS message. It’s unlikely that all of these are
    useful for classification. To reduce the number of features, we’ll eliminate any
    word that appears in less than 5 messages, or in less than about 0.1 percent of
    records in the training data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding frequent words requires the use of the `findFreqTerms()` function in
    the `tm` package. This function takes a DTM and returns a character vector containing
    words that appear at least a minimum number of times. For instance, the following
    command displays the words appearing at least five times in the `sms_dtm_train`
    matrix:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result of the function is a character vector, so let’s save our frequent
    words for later:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A peek into the contents of the vector shows us that there are 1,139 terms
    appearing in at least 5 SMS messages:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We now need to filter our DTM to include only the terms appearing in the frequent
    word vector. As before, we’ll use data frame-style `[row, col]` operations to
    request specific sections of the DTM, noting that the DTM column names are based
    on the words the DTM contains. We can take advantage of this fact to limit the
    DTM to specific words. Since we want all rows but only the columns representing
    the words in the `sms_freq_words` vector, our commands are:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The training and test datasets now include 1,137 features, which correspond
    to words appearing in at least 5 messages.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is usually trained on data with categorical features.
    This poses a problem since the cells in the sparse matrix are numeric and measure
    the number of times a word appears in a message. We need to change this to a categorical
    variable that simply indicates yes or no, depending on whether the word appears
    at all.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'The following defines a `convert_counts()` function to convert counts into
    `Yes` or `No` strings:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: By now, some of the pieces of the preceding function should look familiar. The
    first line defines the function. The statement `ifelse(x > 0, "Yes", "No")` transforms
    the values in `x` such that if the value is greater than `0`, then it will be
    replaced with `"Yes"`; otherwise, it will be replaced with a `"No"` string. Lastly,
    the newly transformed vector `x` is returned.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: We now need to apply `convert_counts()` to each of the columns in our sparse
    matrix. You may be able to guess the name of the R function that does exactly
    this. The function is simply called `apply()` and is used much like `lapply()`
    was used previously.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apply()` function allows a function to be used on each of the rows or
    columns in a matrix. It uses a `MARGIN` parameter to specify either rows or columns.
    Here, we’ll use `MARGIN = 2` since we’re interested in the columns (`MARGIN =
    1` is used for rows). The commands to convert the training and test matrices are
    as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The result will be two character-type matrices, each with cells indicating `"Yes"`
    or `"No"` for whether the word represented by the column appears at any point
    in the message represented by the row.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have transformed the raw SMS messages into a format that can be
    represented by a statistical model, it is time to apply the Naive Bayes algorithm.
    The algorithm will use the presence or absence of words to estimate the probability
    that a given SMS message is spam.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes implementation we will employ is in the `naivebayes` package.
    This package is maintained by Michal Majka and is a modern and efficient R implementation.
    If you have not done so already, be sure to install and load the package using
    the `install.packages("naivebayes")` and `library(naivebayes)` commands before
    continuing.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Many machine learning approaches are implemented in more than one R package,
    and Naive Bayes is no exception. Another option is `naiveBayes()` in the `e1071`
    package, which was used in older editions of this book but is otherwise nearly
    identical to `naive_bayes()` in usage. The `naivebayes` package used in this edition
    offers better performance and more advanced functionality, which is described
    at its website: [https://majkamichal.github.io/naivebayes/](https://majkamichal.github.io/naivebayes/).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the k-NN algorithm we used for classification in the previous chapter,
    training a Naive Bayes learner and using it for classification occur in separate
    stages. Still, as shown in the following table, these steps are fairly straightforward:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Text, letter  Description automatically generated](img/B17290_04_12.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Naive Bayes classification syntax'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `sms_train` matrix, the following command trains a `naive_bayes`
    classifier object that can be used to make predictions:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'After running the previous command, you may notice the following output:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This is nothing to be alarmed about for now; typing the `warnings()` command
    reveals the cause of this issue:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: These warnings are caused by words that appeared in zero spam or zero ham messages
    and have veto power over the classification process due to their associated zero
    probabilities. For instance, because the word *accept* only appeared in ham messages
    in the training data, it does not mean that every future message with this word
    should be automatically classified as ham.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: There is an easy solution to this problem using the Laplace estimator described
    earlier, but for now, we will evaluate this model using `laplace = 0`, which is
    the model’s default setting.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate the SMS classifier, we need to test its predictions on the unseen
    messages in the test data. Recall that the unseen message features are stored
    in a matrix named `sms_test`, while the class labels (spam or ham) are stored
    in a vector named `sms_test_labels`. The classifier that we trained has been named
    `sms_classifier`. We will use this classifier to generate predictions and then
    compare the predicted values to the true values.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` function is used to make the predictions. We will store these
    in a vector named `sms_test_pred`. We simply supply this function with the names
    of our classifier and test dataset as shown:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To compare the predictions to the true values, we’ll use the `CrossTable()`
    function in the `gmodels` package, which we used in previous chapters. This time,
    we’ll add some additional parameters to eliminate unnecessary cell proportions,
    and use the `dnn` parameter (dimension names) to relabel the rows and columns
    as shown in the following code:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This produces the following table:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Looking at the table, we can see that a total of only *6 + 30 = 36* of 1,390
    SMS messages were incorrectly classified (2.6 percent). Among the errors were
    6 out of 1,207 ham messages that were misidentified as spam and 30 of 183 spam
    messages that were incorrectly labeled as ham. Considering the little effort that
    we put into the project, this level of performance seems quite impressive. This
    case study exemplifies the reason why Naive Bayes is so often used for text classification:
    directly out of the box, it performs surprisingly well.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the six legitimate messages that were incorrectly classified
    as spam could cause significant problems for the deployment of our filtering algorithm
    because the filter could cause a person to miss an important text message. We
    should try to see whether we can slightly tweak the model to achieve better performance.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may recall that we didn’t set a value for the Laplace estimator when training
    our model; in fact, it was hard to miss the message from R warning us about more
    than 50 features with zero probabilities! To address this issue, we’ll build a
    Naive Bayes model as before, but this time set `laplace = 1`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we’ll make predictions as before:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we’ll compare the predicted classes to the actual classifications
    using cross-tabulation:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This produces the following table:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Adding a Laplace estimator by setting `laplace = 1` reduced the number of false
    positives (ham messages erroneously classified as spam) from 6 to 5, and the number
    of false negatives from 30 to 28\. Although this seems like a small change, it’s
    substantial considering that the model’s accuracy was already quite impressive.
    We’d need to be careful before tweaking the model too much more, as it is important
    to maintain a balance between being overly aggressive and overly passive when
    filtering spam. Users prefer that a small number of spam messages slip through
    the filter rather than the alternative, in which ham messages are filtered too
    aggressively.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about classification using Naive Bayes. This algorithm
    constructs tables of probabilities that are used to estimate the likelihood that
    new examples belong to various classes. The probabilities are calculated using
    a formula known as Bayes’ theorem, which specifies how dependent events are related.
    Although Bayes’ theorem can be computationally expensive, a simplified version
    that makes so-called “naive” assumptions about the independence of features is
    capable of handling much larger datasets.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is often used for text classification. To illustrate
    its effectiveness, we employed Naive Bayes on a classification task involving
    spam SMS messages. Preparing the text data for analysis required the use of specialized
    R packages for text processing and visualization. Ultimately, the model was able
    to classify over 97 percent of all the SMS messages correctly as spam or ham.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine two more machine learning methods. Each
    performs classification by partitioning data into groups of similar values. As
    you will discover shortly, these methods are quite useful on their own. Yet, looking
    further ahead, these basic algorithms also serve as an important foundation for
    some of the most powerful machine learning methods known today, which will be
    introduced later in *Chapter 14*, *Building Better Learners*.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
