- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: From Model to Model Factory
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从模型到模型工厂
- en: 'This chapter is all about one of the most important concepts in ML engineering:
    how do you take the difficult task of training and fine-tuning your models and
    make it something you can automate, reproduce, and scale for production systems?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章全部关于机器学习工程中最重要的概念之一：您如何将训练和微调模型的困难任务自动化、可重复和可扩展到生产系统？
- en: We will recap the main ideas behind training different ML models at a theoretical
    and practical level, before providing motivation for retraining, namely the idea
    that ML models will not perform well forever. This concept is also known as **drift**.
    Following this, we will cover some of the main concepts behind feature engineering,
    which is a key part of any ML task. Next, we will deep dive into how ML works
    and how it is, at heart, a series of optimization problems. We will explore how
    when setting out to tackle these optimization problems, you can do so with a variety
    of tools at various levels of abstraction. In particular, we will discuss how
    you can provide the direct definition of the model you want to train, which I
    term *hand cranking*, or how you can perform hyperparameter tuning or **automated
    ML** (**AutoML**). We will look at examples of using different libraries and tools
    that do all of these, before exploring how to implement them for later use in
    your training workflow. We will then build on the introductory work we did in
    *Chapter 2*, *The Machine Learning Development Process*, on MLflow by showing
    you how to interface with the different MLflow APIs to manage your models and
    update their status in MLflow’s Model Registry.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在理论和实践层面回顾训练不同机器学习模型的主要思想，然后提供重新训练的动机，即机器学习模型不会永远表现良好的观点。这个概念也被称为**漂移**。在此之后，我们将介绍特征工程背后的主要概念，这是任何机器学习任务的关键部分。接下来，我们将深入探讨机器学习是如何工作的，以及它本质上是一系列优化问题。我们将探讨在着手解决这些优化问题时，您可以使用各种工具在各个抽象级别上这样做。特别是，我们将讨论您如何提供您想要训练的模型的直接定义，我称之为*手动操作*，或者您如何执行超参数调整或**自动化机器学习**（**AutoML**）。我们将在查看使用不同库和工具的示例之后，探讨如何将它们实现以供后续在训练工作流程中使用。然后，我们将基于我们在*第2章*（机器学习开发过程）中进行的初步工作，通过向您展示如何与不同的MLflow
    API接口来管理模型并在MLflow模型注册表中更新它们的状态，来构建MLflow。
- en: We will end this chapter by discussing the utilities that allow you to chain
    all of your ML model training steps into single units known as **pipelines**,
    which can help act as more compact representations of all the steps we have discussed
    previously. The summary at the end will recap the key messages and also outline
    how what we have done here will be built upon further in *Chapter 4*, *Packaging
    Up*, and *Chapter 5*, *Deployment Patterns and Tools*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章的结尾放在讨论允许您将所有机器学习模型训练步骤链接成单个单元（称为**管道**）的实用工具上，这些单元可以帮助作为我们之前讨论的所有步骤的更紧凑表示。总结部分将回顾关键信息，并概述我们在这里所做的工作将在*第4章*（打包）、*第5章*（部署模式和工具）中进一步构建。
- en: 'In essence, this chapter will tell you *what* you need to stick together in
    your solution, while later chapters will tell you *how* to stick them together
    robustly. We will cover this in the following sections:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，本章将告诉您在您的解决方案中需要将哪些内容组合在一起，而后续章节将告诉您如何将这些内容稳健地组合在一起。我们将在以下章节中介绍这一点：
- en: Defining the model factory
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型工厂
- en: Learning about learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关于学习
- en: Engineering features for machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习构建特征
- en: Designing your training system
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计您的训练系统
- en: Retraining required
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要重新训练
- en: Persisting your models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化您的模型
- en: Building the model factory with pipelines
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道构建模型工厂
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'As in the previous chapters, the required packages for this chapter are contained
    within a conda environment `.yml` file in the repository folder for `Chapter03`,
    so to create the conda environment for this chapter, simply run the following
    command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，本章所需的软件包包含在`Chapter03`文件夹中的`.yml`文件中，因此要为本章创建conda环境，只需运行以下命令：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will install packages including MLflow, AutoKeras, Hyperopt Optuna, auto-sklearn,
    Alibi Detect, and Evidently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装包括MLflow、AutoKeras、Hyperopt Optuna、auto-sklearn、Alibi Detect和Evidently在内的软件包。
- en: 'Note that if you are running these examples on a Macbook with Apple Silicon,
    a straight `pip` or `conda` install of TensorFlow and `auto-sklearn` may not work
    out of the box. Instead, you will need to install the following packages to work
    with TensorFlow:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你在配备苹果硅的Macbook上运行这些示例，直接使用`pip`或`conda`安装TensorFlow和`auto-sklearn`可能不会成功。相反，你需要安装以下包来与TensorFlow一起工作：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And then
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To install `auto-sklearn`, you will need to run
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`auto-sklearn`，你需要运行
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Or install `swig` using whatever Mac package manager you use, then you can run
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用你使用的任何Mac包管理器安装`swig`，然后你可以运行
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the model factory
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型工厂
- en: 'If we want to develop solutions that move away from ad hoc, manual, and inconsistent
    execution and toward ML systems that can be automated, robust, and scalable, then
    we have to tackle the question of how we will create and curate the star of the
    show: the models themselves.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要开发从临时、手动和不一致的执行转向可以自动化、稳健和可扩展的机器学习系统的解决方案，那么我们必须解决如何创建和培育表演明星：模型本身的问题。
- en: In this section, we will discuss the key components that have to be brought
    together to move toward this vision and provide some examples of what these may
    look like in code. These examples are not the only way to implement these concepts,
    but they will enable us to start building up our ML solutions toward the level
    of sophistication we will need if we want to deploy in the *real world*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论必须组合在一起的关键组件，以实现这一愿景，并提供一些示例，说明这些组件在代码中可能的样子。这些示例不是实现这些概念的唯一方式，但它们将使我们能够开始构建我们的机器学习解决方案，以实现我们想要在*现实世界*中部署所需的复杂程度。
- en: 'The main components we are talking about here are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的主要组件如下：
- en: '**Training system**: A system for robustly training our models on the data
    we have in an automated way. This consists of all the code we have developed to
    train our ML models on data.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练系统**：一个用于以自动化方式在我们拥有的数据上稳健地训练我们模型的系统。这包括我们为在数据上训练我们的机器学习模型而开发的全部代码。'
- en: '**Model store**: A place to persist successfully trained models and a place
    to share production-ready models with components that will run the predictions.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型存储库**：一个用于持久化成功训练的模型的地方，以及一个与将运行预测的组件共享生产就绪模型的地方。'
- en: '**Drift detector**: A system for detecting changes in model performance to
    trigger training runs.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**漂移检测器**：一个用于检测模型性能变化的系统，以触发训练运行。'
- en: 'These components, combined with their interaction with the deployed prediction
    system, encompass the idea of a model factory. This is shown schematically in
    the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件及其与部署的预测系统的交互，构成了模型工厂的概念。以下图示展示了这一点：
- en: '![Figure 3.1 – The components of the model factory ](img/B19525_03_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 模型工厂的组件](img/B19525_03_01.png)'
- en: 'Figure 3.1: The components of the model factory.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：模型工厂的组件。
- en: For the rest of this chapter, we will explore the three components we mentioned
    previously in detail. **Prediction systems** will be the focus of later chapters,
    especially *Chapter 5*, *Deployment Patterns and Tools*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将详细探讨我们之前提到的三个组件。**预测系统**将是后续章节的重点，特别是*第五章*，*部署模式和工具*。
- en: First, let’s explore what it means to train an ML model and how we can build
    systems to do so.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探讨训练机器学习模型意味着什么，以及我们如何构建系统来完成这项工作。
- en: Learning about learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何学习
- en: 'At their heart, ML algorithms all contain one key feature: an optimization
    of some kind. The fact that these algorithms *learn* (meaning that they iteratively
    improve their performance concerning an appropriate metric upon exposure to more
    observations) is what makes them so powerful and exciting. This process of learning
    is what we refer to when we say *training*.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，机器学习算法都包含一个关键特性：某种形式的优化。这些算法能够“学习”（意味着它们在接触到更多观察时，会迭代地改善它们在适当指标上的性能），这使得它们如此强大和令人兴奋。当我们说“训练”时，我们指的就是这个过程。
- en: In this section, we will cover the key concepts underpinning training, the options
    we can select in our code, and what these mean for the potential performance and
    capabilities of our training system.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍支撑训练的关键概念，我们可以在代码中选择的选项，以及这些选项对我们训练系统潜在性能和能力的影响。
- en: Defining the target
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义目标
- en: We have just stated that training is an optimization, but what exactly are we
    optimizing? Let’s consider supervised learning. In training, we provide the labels
    or values that we would want to predict for the given feature so that the algorithms
    can learn the relationship between the features and the target. To optimize the
    internal parameters of the algorithm during training, it needs to know how *wrong*
    it would be with its current set of parameters. The optimization is then all about
    updating the parameters so that this measure of *wrongness* gets smaller and smaller.
    This is exactly what is captured by the concept of a loss function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到训练是一个优化过程，但我们到底在优化什么？让我们考虑监督学习。在训练过程中，我们提供我们希望预测给定特征的标签或值，以便算法可以学习特征与目标之间的关系。为了在训练过程中优化算法的内部参数，它需要知道其当前参数集会有多大的“错误”。优化就是通过更新参数，使这种“错误”的度量越来越小。这正是损失函数概念所捕捉的。
- en: Loss functions come in a variety of forms, and you can even define your own
    if you need to with a lot of packages, but there are some standard ones that it
    helps to be aware of. The names of some of these are mentioned here.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数有多种形式，如果你需要，你甚至可以使用很多包来定义自己的损失函数，但有一些标准损失函数是值得了解的。其中一些名称在此处提到。
- en: 'For regression problems, you can use the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，你可以使用以下方法：
- en: Mean squared error/L2 loss
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差/L2损失
- en: Mean absolute error/L1 loss
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差/L1损失
- en: 'For binary classification problems, you can use the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，你可以使用以下方法：
- en: Log loss/logistic loss/cross-entropy loss
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数损失/逻辑损失/交叉熵损失
- en: Hinge loss
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉链损失
- en: 'For multi-class classification problems, you can use the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类分类问题，你可以使用以下方法：
- en: Multi-class across entropy loss
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类熵损失
- en: Kullback Leibler divergence loss
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度损失
- en: In unsupervised learning, the concept of a loss function still applies but now
    the target is the correct distribution of the input data. After defining your
    loss function, you then need to optimize it. This is what we will look at in the
    next section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，损失函数的概念仍然适用，但现在目标是输入数据的正确分布。在定义你的损失函数之后，你需要对其进行优化。这就是我们将在下一节中探讨的内容。
- en: Cutting your losses
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪切损失
- en: At this point, we know that training is all about optimizing, and we know what
    to optimize, but we have not covered *how* to optimize yet.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道训练完全是关于优化的，我们也知道要优化什么，但我们还没有介绍如何优化。
- en: As usual, there are plenty of options to choose from. In this section, we will
    look at some of the main approaches.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有很多选项可以选择。在本节中，我们将探讨一些主要的方法。
- en: 'The following are the **constant learning rate** approaches:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些**恒定学习率**的方法：
- en: '**Gradient descent**: This algorithm works by calculating the derivative of
    our loss function regarding our parameters, and then uses this to construct an
    update that moves us in the direction of decreasing loss.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**：此算法通过计算我们的损失函数相对于参数的导数，然后使用这个导数来构建一个更新，使我们在减少损失的方向上移动。'
- en: '**Batch gradient descent**: The gradient that we use to make our move in the
    parameter space is found by taking the average of all the gradients found. It
    does this by looking at each data point in our training set and checking that
    the dataset is not too large and the loss function is relatively smooth and convex.
    This can pretty much reach the global minimum.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量梯度下降**：我们用来在参数空间中移动的梯度是通过取所有找到的梯度的平均值得到的。它是通过查看我们的训练集中的每个数据点，并检查数据集不是太大，损失函数相对平滑且凸来做到这一点的。这几乎可以达到全局最小值。'
- en: '**Stochastic gradient descent**: The gradient is calculated using one randomly
    selected data point at each iteration. This is faster at getting to the global
    minimum of the loss function, but it is more susceptible to sudden fluctuations
    in the loss after each optimization step.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**：在每次迭代中，使用一个随机选择的数据点来计算梯度。这有助于更快地达到损失函数的全局最小值，但它在每次优化步骤后对损失值的突然波动更敏感。'
- en: '**Mini-batch gradient descent**: This is a mixture of both the batch and stochastic
    cases. In this case, updates to the gradient for each update to the parameters
    use several points greater than one but smaller than the entire dataset. This
    means that the size of the batch is now a parameter that needs to be tuned. The
    larger the batch, the more we approach batch gradient descent, which provides
    a better gradient estimate but is slower. The smaller the batch, the more we approach
    stochastic gradient descent, which is faster but not as robust. Mini-batch allows
    us to decide where in between the two we want to be. Batch sizes may be selected
    with a variety of criteria in mind. These can take on a range of memory considerations.
    Batches processed in parallel and larger batches will consume more memory while
    providing improved generalization performance for smaller batches. See *Chapter
    8* of the book *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
    at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) for
    more details.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**：这是批量和随机两种情况的混合。在这种情况下，对于参数的每次更新，都会使用多个大于1但小于整个数据集的点来更新梯度。这意味着批量大小的现在是一个需要调整的参数。批量大时，我们更接近批梯度下降，这提供了更好的梯度估计，但速度较慢。批量小时，我们更接近随机梯度下降，这速度更快，但不够稳健。小批量允许我们决定在这两者之间想要处于哪个位置。可以根据各种标准选择批大小。这些可能涉及一系列的内存考虑。并行处理的大批量批次将消耗更多内存，同时为小批量提供更好的泛化性能。有关更多详细信息，请参阅Ian
    Goodfellow、Yoshua Bengio和Aaron Courville所著的《深度学习》一书的第8章，网址为[https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)。'
- en: 'Then, there are the **adaptive learning rate methods**. Some of the most common
    are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，还有**自适应学习率方法**。以下是一些最常见的：
- en: '**AdaGrad**: The learning rate parameters are dynamically updated based on
    the properties of the learning updates during the optimization process.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaGrad**：学习率参数根据优化过程中的学习更新属性动态更新。'
- en: '**AdaDelta**: This is an extension of `AdaGrad` that does not use all the previous
    gradient updates. Instead, it uses a rolling window on the updates.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaDelta**：这是`AdaGrad`的一个扩展，它不使用所有之前的梯度更新。相反，它使用一个滚动窗口来跟踪更新。'
- en: '**RMSprop**: This works by maintaining a moving average of the square of all
    the gradient steps. It then divides the latest gradient by the square root of
    this.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSprop**：它通过维护所有梯度步骤平方的移动平均值来工作。然后，它将最新的梯度除以这个值的平方根。'
- en: '**Adam**: This is an algorithm that is supposed to combine the benefits of
    `AdaGrad` and `RMSprop`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚当**：这是一个旨在结合`AdaGrad`和`RMSprop`优点的算法。'
- en: The limits and capabilities of all these optimization approaches are important
    for us, as ML engineers, because we want to ensure that our training systems use
    the right tool for the job and are optimal for the problem at hand. Just having
    the awareness that there are multiple options for your internal optimization will
    also help you focus your efforts and increase performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们这些机器学习工程师来说，所有这些优化方法的限制和能力都很重要，因为我们希望确保我们的训练系统使用正确的工具来完成工作，并且对当前问题是最优的。仅仅意识到有多个内部优化选项也会帮助你集中精力并提高性能。
- en: '![Figure 3.5 – Simple representation of training as the optimization of a loss
    function ](img/B19525_03_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 训练作为损失函数优化的简单表示](img/B19525_03_02.png)'
- en: 'Figure 3.2: Simple representation of training as the optimization of a loss
    function.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：训练作为损失函数优化的简单表示。
- en: Now, let’s discuss how we prepare the raw material that the model factory needs
    to do its work, the data, through the process of feature engineering.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何通过特征工程的过程准备模型工厂完成其工作所需的原始材料，即数据。
- en: Preparing the data
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Data can come in all varieties of types and quality. It can be tabular and from
    a relational database, unstructured text from a crawled website, a formatted response
    from a REST API, an image, an audio file, or any other form you can think of.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以以各种类型和质量出现。它可以来自关系型数据库的表格数据，也可以是从爬取的网站中获取的非结构化文本，或者是REST API的格式化响应，还可以是图像、音频文件，或者任何你能想到的其他形式。
- en: If you want to run machine learning algorithms on this data though, the first
    thing you have to do is make it readable by these algorithms. This process is
    known as *feature engineering*, and that is what the next few sections will discuss
    to give you some grounding in the main principles. There are many excellent resources
    on feature engineering that can go into a lot of depth, so we will only touch
    on some of the main concepts here. For more information, you could check out a
    book like *Feature Engineering Cookbook* by Soledad Galli, Packt, 2022.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在这组数据上运行机器学习算法，你必须做的第一件事是让它对这些算法来说是可读的。这个过程被称为*特征工程*，接下来的几节将讨论这一点，以为你提供主要原则的基础。关于特征工程有许多优秀的资源可以深入探讨，所以我们在这里只会触及一些主要概念。更多信息，你可以查阅索莱达·加利亚（Soledad
    Galli）所著的《特征工程食谱》（Feature Engineering Cookbook），Packt出版社，2022年版。
- en: Engineering features for machine learning
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习构建特征
- en: Before we feed any data into an ML model, it has to be transformed into a state
    that can be *understood* by our models. We also need to make sure we only do this
    on the data we deem useful for improving the performance of the model, as it is
    far too easy to explode the number of features and fall victim to the *curse of
    dimensionality*. This refers to a series of related observations where, in high-dimensional
    problems, data becomes increasingly sparse in the feature space, so achieving
    statistical significance can require exponentially more data. In this section,
    we will not cover the theoretical basis of feature engineering. Instead, we will
    focus on how we, as ML engineers, can help automate some of the steps in production.
    To this end, we will quickly recap the main types of feature preparation and feature
    engineering steps so that we have the necessary pieces to add to our pipelines
    later in this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将任何数据输入到机器学习模型之前，它必须被转换成我们模型能够*理解*的状态。我们还需要确保我们只对那些我们认为有助于提高模型性能的数据进行转换，因为这很容易导致特征数量激增，并成为*维度诅咒*的受害者。这指的是一系列相关观察，在高维问题中，数据在特征空间中变得越来越稀疏，因此要实现统计显著性可能需要指数级更多的数据。在本节中，我们不会涵盖特征工程的理论基础。相反，我们将关注作为机器学习工程师，我们如何帮助自动化生产中的某些步骤。为此，我们将快速回顾主要类型的特征准备和特征工程步骤，以便我们可以在本章后面的部分添加必要的组件。
- en: Engineering categorical features
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建类别特征
- en: Categorical features are those that form a non-numerical set of distinct objects,
    such as the day of the week or hair color. They can be distributed in a variety
    of ways throughout your data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分类别特征是指形成一组非数值的、不同的对象集合，例如星期几或发色。它们可以在你的数据中以多种方式分布。
- en: 'For an ML algorithm to be able to *digest* a categorical feature, we need to
    translate the feature into something numerical, while also ensuring that the numerical
    representation *does not produce bias or weigh our values inappropriately*. An
    example of this would be if we had a feature that contained different products
    sold in a supermarket:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让机器学习算法能够*消化*类别特征，我们需要将特征转换成某种数值形式，同时确保数值表示*不会产生偏差或不适当地影响我们的值*。一个例子是，如果我们有一个包含超市中不同产品销售的特征：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we can map each to a positive integer using `sklearn`''s `OrdinalEncoder`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以使用`sklearn`的`OrdinalEncoder`将每个类别映射到一个正整数：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is what is called **ordinal encoding**. We have mapped these features to
    numbers, so there’s a big tick there, but is the representation appropriate? Well,
    if you think about it for a second, not really. These numbers seem to suggest
    that cereal is to bleach as toilet roll is to cereal, and that the average of
    toilet roll and bleach is cereal. These statements don’t make sense (and I don’t
    want bleach and toilet roll for breakfast), so this suggests we should try a different
    approach. This representation would be appropriate, however, in cases where we
    wanted to maintain the notion of ordering in the categorical features. An excellent
    example would be if we had a survey and the participants were asked their opinion
    of the statement *breakfast is the most important meal of the day*. If the participants
    were then told to select one option from the list *Strongly Disagree*, *Disagree*,
    *Neither* *Disagree* *nor* *Agree*, *Agree*, and *Strongly Agree* and we ordinally
    encoded this data to map to the numerical list of *1*, *2*, *3*, *4*, and *5*,
    then we could more intuitively answer questions such as *Was the average response
    more in agreement or disagreement?* and *How widespread was the opinion on this
    statement?* Ordinal encoding would help here, but as we mentioned previously,
    it’s not necessarily correct in this case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的**序数编码**。我们已经将这些特征映射到数字上，所以这里有一个大勾，但这种表示合适吗？好吧，如果你稍微思考一下，其实并不合适。这些数字似乎暗示谷物对漂白剂就像卫生纸对谷物一样，而卫生纸和漂白剂的平均值是谷物。这些陈述没有意义（我也不想在早餐时吃漂白剂和卫生纸），所以这表明我们应该尝试不同的方法。然而，在需要保持分类特征中顺序概念的情况下，这种表示是合适的。一个很好的例子是，如果我们有一个调查，参与者被要求对陈述“早餐是一天中最重要的一餐”发表意见。如果参与者被告知从列表中选择一个选项，如“强烈不同意”，“不同意”，“既不同意也不反对”，“同意”，“强烈同意”，然后我们将这些数据序数编码以映射到数字列表*1*，*2*，*3*，*4*和*5*，那么我们可以更直观地回答诸如“平均反应是更同意还是不同意？”和“对这个陈述的意见有多普遍？”等问题。序数编码在这里会有帮助，但正如我们之前提到的，在这种情况下并不一定正确。
- en: 'What we could do is consider the list of items in this feature, and then provide
    a binary number to represent whether the value is or isn’t that particular value
    in the original list. So, here, we will decide to use `sklearn`''s `OneHotEncoder`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的事情是考虑这个特性中的项目列表，然后提供一个二进制数来表示原始列表中的值是否存在。所以，在这里，我们将决定使用`sklearn`的`OneHotEncoder`：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This representation is known as a **one-hot encoding**. There are a few benefits
    to this method of encoding, including the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示被称为**独热编码**。这种方法编码有几个优点，包括以下内容：
- en: There are no enforced orderings of the values.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有强制排序的值。
- en: All the feature vectors have unit norms (more on this later).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有特征向量都有单位范数（关于这一点稍后讨论）。
- en: Every unique feature is orthogonal to the others, so there are no weird averages
    or distance statements that are implicit in the representation.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个独特的特征都与其他特征正交，所以表示中没有隐含的奇怪平均值或距离陈述。
- en: One of the disadvantages of this approach is that if your categorical list contains
    a lot of instances, then the size of your feature vector will easily blow up,
    and we have to both store and work with extremely sparse vectors and matrices
    at the algorithmic level. This can very easily lead to issues in several implementations
    and is another manifestation of the dreaded curse of dimensionality.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是，如果你的分类列表包含大量实例，那么你的特征向量的大小将很容易膨胀，我们不得不在算法级别上存储和处理极其稀疏的向量和矩阵。这很容易导致几个实现中的问题，也是可怕的维度诅咒的另一种表现。
- en: In the next section, numerical features are discussed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，将讨论数值特征。
- en: Engineering numerical features
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工程数值特征
- en: Preparing numerical features is slightly easier since we already have numbers,
    but there are a few steps we still need to take to prepare for many algorithms.
    For most ML algorithms, the features must be all on similar scales; for example,
    they must have a magnitude between -1 and 1 or 0 and 1\. This is for the relatively
    obvious reason that some algorithms taking in a feature for house price values
    of up to a million dollars and another for the square footage of the house will
    automatically weigh the larger dollar values more. This also means that we lose
    the helpful notion of where specific values sit in their distributions. For example,
    some algorithms will benefit from scaling features so that the median dollar value
    and the median square footage value are both represented by 0.5 rather than 500,000
    and 350\. Or we may want all of our distributions to have the same meaning if
    they were normally distributed, which allows our algorithms to focus on the shape
    of the distributions rather than their locations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数值特征稍微容易一些，因为我们已经有了数字，但仍有几个步骤需要我们完成以准备许多算法。对于大多数机器学习算法，特征必须在相似的尺度上；例如，它们必须在-1和1或0和1之间具有幅度。这有一个相对明显的原因，即某些算法会自动将高达百万美元的房价特征和房屋面积的另一个特征赋予更大的权重。这也意味着我们失去了关于特定值在其分布中位置的有用概念。例如，一些算法会从将特征缩放到中值美元价值和中值面积价值都表示为0.5而不是500,000和350中受益。或者，我们可能希望所有分布都具有相同的含义，如果它们是正态分布的，这将允许我们的算法专注于分布的形状而不是它们的位置。
- en: 'So, what do we do? Well, as always, we are not starting from scratch and there
    are some standard techniques we can apply. Some very common ones are listed here,
    but there are far too many to include all of them:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该怎么办呢？嗯，就像往常一样，我们不是从零开始，我们可以应用一些标准技术。这里列出了其中一些非常常见的，但它们的数量太多，无法全部包括：
- en: '**Standardization**: This is a transformation of a numerical feature and assumes
    that the distribution of values is normal or Gaussian before scaling the variance
    to be 1 and the average to be 0\. If your data is indeed normal or Gaussian, then
    this is a good technique to use. The mathematical formula for standardization
    is very simple, so I’ve provided it here, where *z* represents the transformed
    value, *x* is the original value, and ![](img/B19525_03_001.png) and ![](img/B19525_03_002.png)
    are the average and standard deviation, respectively:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**：这是一种数值特征的转换，假设在缩放方差为1和平均值为0之前，值的分布是正态的或高斯分布。如果你的数据确实是正态的或高斯分布，那么这是一个很好的技术。标准化的数学公式非常简单，所以我在这里提供了它，其中
    *z* 代表变换后的值，*x* 是原始值，而 ![](img/B19525_03_001.png) 和 ![](img/B19525_03_002.png)
    分别是平均值和标准差：'
- en: '![](img/B19525_03_003.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_03_003.png)'
- en: '**Min-max normalization**: In this case, we want to scale the numerical features
    so that they’re always between 0 and 1, irrespective of the type of distribution
    that they follow.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大归一化**：在这种情况下，我们希望缩放数值特征，使它们始终在0和1之间，无论它们遵循的分布类型如何。'
- en: 'This is intuitively easy to do, as you just need to subtract the minimum of
    the distribution from any given value and then divide by the range of the data
    (maximum minus minimum). You can think of this first step as making sure that
    all the values are greater than or equal to 0\. The second step involves making
    sure that their maximum size is 1\. This can be written with a simple formula,
    where the transformed number, ![](img/B19525_03_004.png) is the original number,
    and ![](img/B19525_03_004.png) represents the entire distribution of that feature:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这在直观上很容易做到，因为你只需要从任何给定值中减去分布的最小值，然后除以数据的范围（最大值减去最小值）。你可以将这一步视为确保所有值都大于或等于0。第二步是确保它们的最大尺寸为1。这可以用一个简单的公式来表示，其中变换后的数字
    ![](img/B19525_03_004.png) 是原始数字，而 ![](img/B19525_03_004.png) 代表该特征的整个分布：
- en: '![](img/B19525_03_006.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_03_006.png)'
- en: '**Feature vector normalization**: Here, you scale every single sample in your
    dataset so that they have norms equal to 1\. This can be very important if you
    are using algorithms where the distance or cosine similarity between features
    is an important component, such as in clustering. It is also commonly used in
    text classification in combination with other feature engineering methods, such
    as the **TF-IDF** **statistic**. In this case, assuming your entire feature is
    numerical, you just calculate the appropriate norm for your feature vector and
    then divide every component by that value. For example, if we use the Euclidean
    or L2-norm of the feature vector, ![](img/B19525_03_007.png), then we would transform
    each component, ![](img/B19525_03_008.png) via the following formula:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量归一化**：在这里，您需要将数据集中的每个样本缩放，使它们的范数等于1。如果您使用的是距离或特征之间的余弦相似度是重要组成部分的算法，这可能会非常重要，例如在聚类中。它也常与**TF-IDF**
    **统计**等其他特征工程方法结合使用，在文本分类中。在这种情况下，假设您的整个特征是数值的，您只需计算特征向量的适当范数，然后将每个分量除以该值。例如，如果我们使用特征向量的欧几里得或L2范数，![](img/B19525_03_007.png)，那么我们将通过以下公式转换每个分量，![](img/B19525_03_008.png)：'
- en: '![](img/B19525_03_009.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_03_009.png)'
- en: 'To highlight the improvements these simple steps can make to your model’s performance,
    we will look at a simple example from the `sklearn` wine dataset. Here, we will
    be training a Ridge classifier on data that has not been standardized and then
    on data that has been standardized. Once we’ve done this, we will compare the
    results:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出这些简单步骤对模型性能的改进，我们将从`sklearn`葡萄酒数据集的一个简单示例中进行分析。在这里，我们将对未标准化的数据进行Ridge分类器的训练，然后对已标准化的数据进行训练。完成这些后，我们将比较结果：
- en: 'First, we must import the relevant libraries and set up our training and test
    data:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须导入相关库并设置我们的训练和测试数据：
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we must make a typical 70/30 train/test split:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须进行典型的70/30训练/测试分割：
- en: '[PRE9]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we must train a model without any standardization in the features and
    predict on the test set:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须在不进行特征标准化的情况下训练一个模型，并在测试集上进行预测：
- en: '[PRE10]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we must do the same but with a standardization step added in:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须做同样的事情，但添加一个标准化步骤：
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, if we print some performance metrics, we will see that without scaling,
    the accuracy of the predictions is at `0.76`, while the other metrics, such as
    the weighted averages of `precision`, `recall`, and `f1-score`, are `0.83`, `0.76`,
    and `0.68`, respectively:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们打印一些性能指标，我们会看到没有缩放的情况下，预测的准确率为`0.76`，而其他指标，如`precision`、`recall`和`f1-score`的加权平均值分别为`0.83`、`0.76`和`0.68`：
- en: '[PRE12]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This produces the following output:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE13]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the case where we standardized the data, the metrics are far better across
    the board, with the accuracy and weighted averages of the `precision`, `recall`,
    and `f1-score` all at `0.98`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据标准化的情况下，所有指标都非常好，准确率以及`precision`、`recall`和`f1-score`的加权平均值都达到了`0.98`：
- en: '[PRE14]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following output:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE15]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we can see a significant jump in performance, just by adding one simple
    step to our ML training process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只需在机器学习训练过程中添加一个简单的步骤，就能看到性能的显著提升。
- en: Now, let’s look at how training is designed and works at its core. This will
    help us make sensible choices for our algorithms and training approaches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看训练是如何设计和在其核心工作的。这将帮助我们为我们的算法和训练方法做出明智的选择。
- en: Designing your training system
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计您的训练系统
- en: 'Viewed at the highest level, ML models go through a life cycle with two stages:
    a **training** phase and an **output** phase. During the training phase, the model
    is fed data to learn from the dataset. In the prediction phase, the model, complete
    with its optimized parameters, is fed new data in order and returns the desired
    output.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从最高层次来看，机器学习模型经历一个生命周期，有两个阶段：**训练**阶段和**输出**阶段。在训练阶段，模型被喂给数据以从数据集中学习。在预测阶段，模型（包括其优化的参数）按顺序被喂给新数据，并返回所需的输出。
- en: These two phases have very different computational and processing requirements.
    In the training phase, we have to expose the model to as much data as we can to
    gain the best performance, all while ensuring subsets of data are kept aside for
    testing and validation. Model training is fundamentally an optimization problem,
    which requires several incremental steps to get to a solution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个阶段在计算和处理需求上非常不同。在训练阶段，我们必须让模型接触到尽可能多的数据以获得最佳性能，同时确保将数据集的子集保留用于测试和验证。模型训练本质上是一个优化问题，需要几个增量步骤才能得到解决方案。
- en: 'Therefore, this is computationally demanding, and in cases where the data is
    relatively large (or compute resources are relatively low), it can take a long
    time. Even if you had a small dataset and a lot of computational resources, training
    is still not a low-latency process. Also, it is a process that is often run in
    batches and where small additions to the dataset will not make that much difference
    to model performance (there are exceptions to this). Prediction, on the other
    hand, is a more straightforward process and can be thought of in the same way
    as running any calculation or function in your code: inputs go in, a calculation
    occurs, and the result comes out. This (in general) is not computationally demanding
    and is low latency.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, this means that, firstly, it makes sense to separate these two
    steps (training and prediction) both logically and in code. Secondly, it means
    we have to consider the different execution requirements for these two stages
    and build this into our solution designs. Finally, we need to make choices about
    our training regime, including whether we schedule training in batches, use incremental
    learning, or should trigger training based on model performance criteria. These
    are the key parts of your training system.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Training system design options
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we create any detailed designs of our training system, some general
    questions will always apply:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Is there infrastructure available that is appropriate to the problem?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the data and how will we feed it to the algorithm?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How am I testing the performance of the model?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of infrastructure, this can be very dependent on the model and data
    you are using for training. If you are going to train a linear regression on data
    with three features and your dataset contains only 10,000 tabular records, you
    can likely run this on laptop-scale hardware without much thought. This is not
    a lot of data, and your model does not have a lot of free parameters. If you are
    training on a far larger dataset, such as one that contains 100 million tabular
    records, then you could benefit from parallelization across something such as
    a Spark cluster. If, however, you are training a 100-layer deep convolutional
    neural network on 1,000 images, then you are likely going to want to use a GPU.
    There are plenty of options, but the key is choosing the right thing for the job.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the question of how we feed data to the algorithm, this can be non-trivial.
    Are we going to run a SQL query against a remotely hosted database? If so, how
    are we connecting to it? Does the machine we’re running the query on have enough
    RAM to store the data?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: If not, do we need to consider using an algorithm that can learn incrementally?
    For classic algorithmic performance testing, we need to employ the well-known
    tricks of the ML trade and perform train/test/validation splits on our data. We
    also need to decide what cross-validation strategies we may want to employ. We
    then need to select our model performance metric of choice and calculate it appropriately.
    As ML engineers, however, we will also be interested in *other* measures of performance,
    such as training time, efficient use of memory, latency, and (dare I say it) cost.
    We will need to understand how we can measure and then optimize these as well.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: So long as we bear these things in mind as we proceed, we will be in a good
    position. Now, onto the design.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction to this section, we have two fundamental
    pieces to consider: the training and output processes. There are two ways in which
    we can put these together for our solution. We will discuss this in the next section.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Train-run
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Option 1* is to perform training and prediction in the same process, with
    training occurring in either batch or incremental mode. This is shown schematically
    in the following diagram. This pattern is called *train-run*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The train-run process ](img/B19525_03_03.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The train-run process.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is the simpler of the two but also the least desirable for real-world
    problems since it does not embody the *separation of concerns* principle we mentioned
    previously. This does not mean it is an invalid pattern, and it does have the
    advantage of often being simpler to implement. Here, we run our entire training
    process before making our predictions, with no real *break* in between. Given
    our previous discussions, we can automatically rule out this approach if we have
    to serve prediction in a very low-latency fashion; for example, through an event-driven
    or streaming solution (more on these later).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Where this approach *could* be completely valid, though (and I’ve seen this
    a few times in practice), is either in cases where the algorithms you are applying
    are actually very lightweight to train and you need to keep using very recent
    data, or where you are running a large batch process relatively infrequently.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this is a simple approach and does not apply to all cases, it does
    have distinct advantages:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Since you are training as often as you predict, you are doing everything you
    can to protect against modern performance degradation, meaning that you are combatting
    *drift* (see later sections in this chapter).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are significantly reducing the complexity of your solution. Although you
    are tightly coupling two components, which should generally be avoided, the training
    and prediction stages may be so simple to code that if you just stick them together,
    you will save a lot of development time. This is a non-trivial point because *development
    time costs money*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at the other case.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Train-persist
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Option 2* is that training runs in batch, while prediction runs in whatever
    mode is deemed appropriate, with the prediction solution reading in the trained
    model from a store. We will call this design pattern *train-persist*. This is
    shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The train-persist process ](img/B19525_03_04.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The train-persist process.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are going to train our model and then persist the model so that it can
    be picked up later by a prediction process, then we need to ensure a few things
    are in place:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: What are our model storage options?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a clear mechanism for accessing our model store (writing to and reading
    from)?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often should we train versus how often will we predict?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, we will solve the first two questions by using MLflow, which we
    introduced in *Chapter 2*, *The Machine Learning Development Process*, but will
    revisit in later sections. There are also lots of other solutions available. The
    key point is that no matter what you use as a model store and *handover* point
    between your train and predict processes, it should be used in a way that is robust
    and accessible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The third point is trickier. You could potentially just decide at the outset
    that you want to train on a schedule, and you stick to that. Or you could be more
    sophisticated and develop trigger criteria that must be met before training occurs.
    Again, this is a choice that you, as an ML engineer, need to make with your team.
    Later in this chapter, we will discuss mechanisms for scheduling your training
    runs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore what you have to do if you want to trigger
    your training runs based on how your model’s performance could be degrading over
    time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Retraining required
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You wouldn’t expect that after finishing your education, you never read a paper
    or book or speak to anyone again, which would mean you wouldn’t be able to make
    informed decisions about what is happening in the world. So, you shouldn’t expect
    an ML model to be trained once and then be performant forever afterward.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea is intuitive, but it represents a formal problem for ML models known
    as **drift**. Drift is a term that covers a variety of reasons for your model’s
    performance dropping over time. It can be split into two main types:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept drift**: This happens when there is a change in the fundamental relationship
    between the features of your data and the outcome you are trying to predict. Sometimes,
    this is also known as *covariate drift*. An example could be that at the time
    of training, you only have a subsample of data that seems to show a linear relationship
    between the features and your outcome. If it turns out that, after gathering a
    lot more data post-deployment, the relationship is non-linear, then concept drift
    has occurred. The mitigation against this is retraining with data that is more
    representative of the correct relationship.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift**: This happens when there is a change in the statistical properties
    of the variables you are using as your features. For example, you could be using
    *age* as a feature in one of your models but at training time, you only have data
    for 16–24-year-olds.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model gets deployed and your system starts ingesting data for a wider
    age demographic, then you have data drift.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that drift is part of life as an ML engineer, so we will spend
    a good bit of time getting to know how to detect and mitigate against it. But
    why does it happen? As you would expect, there are a variety of reasons for drift
    that it is important to consider. Let us consider some examples. Say the mechanism
    you used for sampling your training data is not appropriate in some way; perhaps
    you have subsampled for a specific geographic region or demographic, but you want
    the model to be applied in more general circumstances.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: There may be seasonal effects in the problem domain in which we are operating,
    as can be expected in sales forecasting or weather prediction. Anomalies could
    be introduced by “black swan” or rare events, like geopolitical events or even
    the Covid-19 pandemic. The data-gathering process may at some point introduce
    errors, for example, if there is a bug in an upstream system or the process itself
    is not being followed or has changed. This last example can be particularly prevalent
    in processes where manual inputs of data are required. If a salesperson is to
    be trusted with correctly labeling the state of a sale in the **Customer Resource
    Management** (**CRM**) system, then salespeople with less training or experience
    may not label the data as accurately or in as timely a manner. Despite advances
    in so many areas of software development, this sort of data-gathering process
    is still very prevalent and so you must guard against this in your own machine
    learning system development. It can be mitigated slightly by trying to enforce
    more automation of data gathering or in providing guides to those entering data
    (think drop-down menus), but it is almost certain that a lot of data is still
    gathered in this way and will be for the foreseeable future.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'That drift is an important aspect of your system to consider should be clear
    now, but dealing with it is actually a multi-step process. We first need to detect
    the drift. Detecting drift in your deployed models is a key part of MLOps and
    should be at the forefront of your mind as an ML engineer. We then need to diagnose
    the source of the drift; this will usually involve some sort of offline investigation
    by those responsible for monitoring. The tools and techniques we will mention
    will help you to define workflows that start to automate this, though, so that
    any repeatable tasks are taken care of when an issue is detected. Finally, we
    need to implement some action to remediate the effects of the drift: this will
    often be retraining the model using an updated or corrected dataset but may require
    a redevelopment or rewrite of key components of your model. In general, if you
    can build your training systems so that retraining is triggered based on an informed
    understanding of the drift in your models, you will save a lot of computational
    resources by only training when required.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss some of the ways we can detect drift in our models.
    This will help us start building up a smart retraining strategy in our solution.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Detecting data drift
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have defined drift, and we know that detecting it is going to be
    important if we want to build sophisticated training systems. The next logical
    question is, *how do we do this?*
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The definitions of drift we gave in the previous section were very qualitative;
    we can start to make these statements a bit more quantitative as we explore the
    calculations and concepts that can help us detect drift.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will rely heavily on the `alibi-detect` Python package
    from Seldon, which, at the time of writing, is not available from **Anaconda.org**
    but is available on PyPI. To acquire this package, use the following commands:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It is very easy to use the `alibi-detect` package. In the following example,
    we will work with the `wine` dataset from `sklearn`, which will be used elsewhere
    in this chapter. In this first example, we will split the data 50/50 and call
    one set the *reference* set and the other the *test* set. We will then use the
    Kolmogorov-Smirnov test to show that there hasn’t been data drift between these
    two datasets, as expected, and then artificially add some drift to show that it
    has been successfully detected:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the `TabularDrift` detector from the `alibi-detect` package,
    as well as the relevant packages for loading and splitting the data:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we must get and split the data:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we must initialize our drift detector using the reference data and by
    providing the `p-value` we want to be used by the statistical significance tests.
    If you want to make your drift detector trigger when smaller differences occur
    in the data distribution, you must select a larger `p_val`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now check for drift in the test dataset against the reference dataset:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This returns `''Drift: No''`.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, we have not detected drift here, as expected (see the following *IMPORTANT
    NOTE* for more on this).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although there was no drift in this case, we can easily simulate a scenario
    where the chemical apparatus being used for measuring the chemical properties
    experienced a calibration error, and all the values are recorded as 10% higher
    than their true values. In this case, if we run drift detection again on the same
    reference dataset, we will get the following output:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This returns `''Drift: Yes''`, showing that the drift has been successfully
    detected.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: This example is very artificial but is useful for illustrating the point. In
    a standard dataset like this, there won’t be data drift between 50% of the randomly
    sampled data and the other 50% of the data. This is why we have to artificially
    *shift* some of the points to show that the detector does indeed work. In real-world
    scenarios, data drift can occur naturally due to everything from updates to sensors
    being used for measurements; to changes in consumer behavior; all the way through
    to changes in database software or schemas. So, be on guard as many drift cases
    won’t be as easy to spot as in this case!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how, with a few simple lines of Python, we can detect a change
    in our dataset, which means our ML model may start to degrade in performance if
    we do not retrain to take the new properties of the data into account. We can
    also use similar techniques to track when the performance metrics of our model,
    for example, accuracy or mean squared error, are drifting as well. In this case,
    we have to make sure we periodically calculate performance on new test or validation
    datasets.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The first drift detection example was very simple and showed us how to detect
    a basic case of one-off data drift, specifically feature drift. We will now show
    an example of detecting **label drift**, which is basically the same but now we
    simply use the labels as the reference and comparison dataset. We will ignore
    the first few steps as they are identical, and resume from the point where we
    have reference and test datasets available.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the example for the drift in the features, we can configure the tabular
    drift detector, but now we will use the initial label as our baseline dataset:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can now check for drift in the test labels against the reference dataset:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This returns `''Drift: No''`.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, we have not detected drift here, as expected. Note that this method can
    also be used as a good sanity check that training and test data labels follow
    similar distributions and our sampling of test data is representative.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As in the previous example, we can simulate some drift in the data, and then
    check that this is indeed detected:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will now move on to a far more complex scenario, which is detecting concept
    drift.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Detecting concept drift
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concept drift was described in this section, and there it was emphasized that
    this type of drift is really all about a change in the relationships between the
    variables in our model. This means by definition that it is far more likely that
    cases of this type will be complex and potentially quite hard to diagnose.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The most common way that you can catch concept drift is by monitoring the performance
    of your model through time. For example, if we are working with the `wine` classification
    problem again, we can look at metrics that tell us the model’s classification
    performance, plot these through time, and then build logic around the trends and
    outliers that we might see in these values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The `alibi_detect` package, which we have already been using, has several useful
    methods for online drift detection that can be used to find concept drift as it
    happens and impacts model performance. Online here refers to the fact that the
    drift detection takes place at the level of a single data point, so this can happen
    even if data comes in completely sequentially in production. Several of these
    methods assume that either PyTorch or TensorFlow are available as backends since
    the methods use **Untrained AutoEncoders** (**UAEs**) as out-of-the-box pre-processing
    methods.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let us walk through an example of creating and using one of these
    online detectors, the Online Maximum Mean Discrepancy method. The following example
    assumes that in addition to the reference dataset, `X_ref`, we have also defined
    variables for the expected run time, `ert`, and the window size, `window_size`.
    The expected run time is a variable that states the average number of data points
    the detector should run before it raises false positive detection. The idea here
    is that you want the expected run time to be larger but as it gets larger the
    detector becomes more insensitive to actual drift, so a balance must be struck.
    The `window_size` is the size of the sliding window of data used in order to calculate
    the appropriate drift test statistic. A smaller `window_size` means you are tuning
    the detector to find sharp changes in the data or performance in a small time-frame,
    whereas longer window sizes will mean you are tuning to look for more subtle drift
    effects over longer periods of time.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'First we import the method:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We then initialize the drift detector with some variable settings as discussed
    in the previous paragraph. We also include the number of bootstrapped simulations
    we want to apply in order for the method to calculate some thresholds for detecting
    the drift.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on your hardware settings for the deep learning library used and the
    size of the data, this may take some time.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can then simulate the drift detection in a production setting by taking
    the test data from the **Wine** dataset and feeding it in one feature vector at
    a time. If the feature vector for any given instance of data is given by `x`,
    we can then call the `predict` method of the drift detector and retrieve the `''is_drift''`
    value from the returned metadata like so:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Performing step 2 on all of the rows of the test data and plotting a vertical
    orange bar wherever we find drift detected gives the plot in *Figure 3.5*.![](img/B19525_03_05.png)
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.5: The features of the test set from the wine dataset, which we have
    used to run some simulated drift detection.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, we can see in the plots of the simulated data that the accuracy
    of the data has changed over time. If we want to automate the detection of behaviors
    like this though, we will need to not simply plot this data but start to analyze
    it in a systematic way that we can fold into our model monitoring processes running
    in production.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The test data for the **Wine** dataset was used in the drift example
    only as an example. In production, this drift detection will be running on data
    that has never been seen before, but the principle is the same.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know drift is happening, we’ll move on to discuss how you can start
    to decide which limits to set on your drift detectors and then cover some processes
    and techniques for helping you to diagnose the type and source of your drift.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Setting the limits
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the techniques we have been describing in this section on drift are
    very much aligned with standard techniques from statistics and machine learning.
    You can get very far using these techniques almost “out of the box” to diagnose
    a series of different types of issues, but we have not discussed how we can bring
    these together into a coherent set of drift detection mechanisms. One of the most
    important things to consider before setting out to do this is setting the boundaries
    of acceptable behavior of the data and the model so that you know when your system
    should raise an alarm or take some action. We will call this “setting the limits”
    for your drift detection system.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: So, where do you start? This is where things become a bit less technical and
    definitely more centered around operating within a business environment, but let’s
    cover some of the key points. First, it is important to understand what is important
    to alert on. Alerting on deviations in all of the metrics that you can think of
    might sound like a good idea, but it may just create a super noisy system where
    it is hard to find issues that are genuinely of concern. So, we have to be judicious
    in our selection of what we want to track and monitor. Next, we need to understand
    the timeliness required for detecting issues. This relates very strongly to the
    notion in software of **Service-Level Agreements** (**SLAs**), which write down
    the demanded and expected performance of the system in question. If your business
    is running real-time anomaly detection and predictive maintenance models on equipment
    used in hazardous conditions, it may be that the requirement for timeliness in
    alarms being raised and action being taken is quite high. However, if your machine
    learning system is performing a financial forecast once a week, then it could
    be that the timeliness constraints are a lot less severe. Finally, you need to
    set the limits. This means that you need to think carefully about the metrics
    you are tracking and think “What constitutes bad here?” or “What do we want to
    be notified of?” It may be that as part of your Discovery phase in the project,
    you know that the business is happy with a regression model that can have wide
    variability in the accuracy of its prediction, as long as it provides suitable
    confidence intervals.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In another scenario, it could be that the classification model you are building
    must have a recall that fluctuates only within a relatively tight band; otherwise,
    it will jeopardize the efficacy of processes downstream.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing the drift
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have discussed in another section how there can be a variety of
    reasons for drift in our model, when it comes down to it, we must remember that
    machine learning models only act on features to create predictions. This then
    means that if we want to diagnose the source of the drift, we need to look no
    further than our features.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: So, where do we start? The first thing we should consider is that any feature
    could realistically have drifted, but not all the features will be equally important
    in terms of the model. This means we need to understand how important the features
    are before prioritizing which ones need remedial action.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance can be calculated in ways that are either model dependent
    or model independent. The model-dependent methods refer specifically to tree-based
    models, such as decision trees or random forests. In these cases, feature importance
    can often be extracted from the model for inspection, depending on the package
    used for developing the model. As an example, if we take a random forest classifier
    trained in Scikit-Learn, we can extract its feature importances using syntax like
    that given below. In this example, we retrieve the default feature importances
    for the random forest model, which are calculated using **Mean Decrease in Impurity**
    (**MDI**), equivalently known as “Gini importance,” and put them in an ordered
    pandas series for later analysis:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Although this is extremely simple, it can sometimes give erroneous results for
    a couple of reasons. The feature importances here have been calculated using an
    impurity measure, which is a class of measures that can exhibit bias toward features
    with high cardinality (e.g., numerical) and are computed only on training set
    data, meaning they do not take into account any generalizability of the model
    onto unseen test data. This should always be kept in mind when using this sort
    of importance measure.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Another standard measure of feature importance, which is model agnostic and
    alleviates some of the issues for MDI or Gini importance, is the permutation importance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'This works by taking the feature we are interested in, shuffling it (i.e.,
    moving the values in the column of the feature matrix up, down, or via some other
    method of reorganization), and then recalculating the model accuracy or error.
    The change in the accuracy or error can then be used as a measure of the importance
    of this feature, as fewer importance features should mean less change in model
    performance upon shuffling. Below is an example of this method, again using Scikit-Learn,
    on the same model we used in the previous example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Finally, one other very popular method for determining feature importance is
    the calculation of **SHAP** (**SHapley Additive exPlanation**) values for the
    features. This uses ideas from game theory to consider how the features combine
    to inform the prediction. SHAP values are calculated by training the model on
    all permutations of features that include or exclude the considered feature and
    then calculating the marginal contribution to the predicted value of that feature.
    This is different from permutation importance because we are no longer simply
    permuting the feature values; we are now actually running through a series of
    different potential sets of features including or excluding the feature.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start calculating SHAP values on your models by installing the `shap`
    package:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And then we can execute syntax like the following, using the same random forest
    model in the previous examples to define a *shap explainer* object and calculate
    the SHAP values for the features in the test dataset. We assume here the `X_test`
    is a pandas DataFrame with the feature names as the column names:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that the calculation of the SHAP values can take some time due to running
    all the permutations. The `shap_values` themselves are not feature importances,
    but contain the SHAP values calculated for each feature in all the different feature
    combination experiments. In order to determine feature importances, you should
    take the average of the absolute magnitudes of the `shap_values` for each feature.
    This is done for you and the result plotted if you use the following command:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have now covered three different ways to calculate feature importances for
    your models, two of them completely model agnostic. Feature importance is extremely
    helpful to help you get to the root of drift very quickly. If you see the performance
    of your model drifting or breaching a threshold you have set, you can use the
    feature importances to focus your diagnostic efforts on where the most important
    features are and ignore drift in features that are not as critical.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered a useful way to help dig into the drift, we will now
    discuss how you can go about remediating it once you spot the feature or features
    that seem to be causing the most trouble.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Remediating the drift
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few ways we can take action against drift in order to maintain
    the performance of our system:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**Remove features and retrain**: If certain features are drifting or exhibiting
    degradation of some other kind, we can try removing them and retraining the model.
    This can become time consuming as our data scientists potentially need to re-run
    some analysis and testing to ensure that this approach still makes sense from
    a modeling point of view. We also have to take into account things like the importance
    of the features we are removing.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain with more data**: If we are seeing concept drift, we may simply be
    noticing that the model has become stale with respect to the distributions and
    the relationships between these distributions in the data. It could be that retraining
    the model and including more recent data may create an uptick in performance.
    There is also the option of retraining the model on some selected portion of more
    recent data. This can be especially useful if you are able to diagnose some dramatic
    event or shift in the data, for example, the introduction of Covid-19 lockdowns.
    This approach can be hard to automate though, so sometimes it is also an option
    to introduce a time-windowed approach, where you train on some preselected amount
    of data up to the present time.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Roll back the model**: We can replace the current model with a previous version
    or even a go-to baseline model. This can be a very good approach if your baseline
    model is more simple but also more predictable in terms of performance, because
    it applies some simple business logic, for example. The ability to roll back to
    previous versions of models requires that you have built up a good set of automated
    processes around your model registry. This is very reminiscent of rollbacks in
    general software engineering, a key component of building robust systems.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewrite or debug the solution**: It may be the case that the drift we are
    dealing with is so substantial that the model as it stands cannot cope with any
    of the above approaches. The idea of rewriting the model may seem drastic but
    this can be more common than you think. As an example, consider that initially
    you deploy a well-tuned LightGBM model that performs binary classification on
    a set of five features daily. After running the solution for months, it could
    be that after detecting drift in the model performance several times, you decide
    that it is better to perform an investigation to see if there is a better approach.
    This can be especially helpful in this scenario as now you know more about the
    data you will see in production. You may then discover that actually, a random
    forest classifier is not as performant on the same production data scenarios on
    average but that *it is* more stable, behaving more consistently and triggering
    drift alarms less often. You may then decide that actually, it is better for the
    business to deploy this different model into the same system as it will reduce
    operational overheads from dealing with the drift alarms and it will be something
    the business can trust more. It is important to note that if you need to write
    a new pipeline or model, it is often important to roll back to a previous model
    while the team does this work.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fix the data source**: Sometimes, the most challenging issues do not actually
    have anything to do with the underlying model but are more to do with changes
    in how data is collected and fed downstream to your system. There are so many
    business scenarios where the collection of data, the transformation of data, or
    the characteristics of data may be changed due to the introduction of new processes,
    updates to systems, or even due to changes in the personnel responsible for entering
    some source data. A great example from the author’s own experience is when it
    comes to **customer resource management** (**CRM**) systems, the quality of the
    data being input from the sales team can depend on so many factors that it can
    be reasonable to expect slow or sudden changes in data quality, consistency, and
    timeliness.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the right answer may not actually be an engineering one, but a
    process one, working with the appropriate teams and stakeholders to ensure that
    data quality is maintained, and standard processes are followed. This will benefit
    customers and the business, but it can still be a hard sell.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can start to build this into solutions that will automatically trigger
    our ML model being retrained, as shown in *Figure 3.6*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – An example of drift detection and the training system process
    ](img/B19525_03_06.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: An example of drift detection and the training system process.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Other tools for monitoring
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The examples in this chapter have mainly used the alibi-detect package, but
    we are now in somewhat of a golden age of open source **MLOps** tools. There are
    several different packages and solutions available that you can start using to
    build your monitoring solutions without spending a penny.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will quickly cover some of these tools and show some basic
    points on their syntax, so that if you want to develop monitoring pipelines, then
    you can just get started right away and know where is best to use these different
    tools.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: First, we will cover **Evidently AI** ([https://www.evidentlyai.com/](https://www.evidentlyai.com/)),
    which is a very easy-to-use Python package that allows users to not only monitor
    their models but also create customizable dashboards in a few lines of syntax.
    Below is an adaptation of the getting started guide from the documentation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install Evidently:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Import the `Report` functionality. The `Report` is an object that collects
    calculations across several metrics to allow for visualizations or outputs as
    a JSON object. We will show this latter behavior later:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, import what is known as a metric preset, in this case for data drift.
    We can think of this as a templated report object that we can later customize:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, assuming you have the data to hand, you can then run the data drift report.
    Let’s assume you have the **Wine** dataset from the previous examples to hand.
    If we split the wine data 50/50 using `scikit-learn`''s `train_test_split()` method,
    we will have two datasets, which we again use to simulate the reference dataset,
    `X_ref`, and the current dataset, `X_curr`:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Evidently then provides some really nice functionality for visualizing the
    results in the report. You can export or view these using a few different methods.
    You can export the report to JSON or HTML objects for consumption or to review
    downstream or in other applications. *Figures 3.7* and *3.8* show snippets of
    the results when you create these outputs with the following commands:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/B19525_03_07.png)'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.7: JSON output from the Evidently report on the 50/50 split Wine feature
    set.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_03_08.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: HTML version of the drift report generated by Evidently on the
    50/50 split Wine feature set.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: One of the nice things about the rendered HTML report is that you can dynamically
    drill down into some useful information. As an example, *Figure 3.9* shows that
    if you click down into any of the features, you are provided with a plot of data
    drift through time, and *Figure 3.10* shows that you can also get a plot of the
    distributions of the features in the same way.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_03_09.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: The automatically generated data drift plot when you drill down
    in the Evidently report for the Wine features.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_03_10.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: The automatically generated histogram showing the distribution
    of the feature when you drill down in the Evidently report for the Wine feature
    set.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: This has only scratched the surface of what you can do with Evidently. There
    is a lot of functionality available for generating your own model test suites
    and monitoring functionality as well as visualizing it all nicely like we have
    seen.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the concepts of model and data drift and how to detect
    them, we can now move on to a discussion about how we can take a lot of the concepts
    we covered earlier in the chapter and automate them.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The next couple of sections will provide deep dives into different aspects of
    the training process and, in particular, how this process can be automated using
    a variety of tools.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Automating training
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training process is an integral part of the model factory and one of the
    main differentiators between ML engineering and traditional software engineering.
    The next few sections will discuss in detail how we can start to use some excellent
    open source tooling to streamline, optimize, and, in some cases, fully automate
    elements of this process.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchies of automation
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main reasons that ML is now a common part of software development,
    as well as a major business and academic activity, is because of the plethora
    of tools available. All of the packages and libraries containing working and optimized
    implementations of sophisticated algorithms have allowed people to build on top
    of these, rather than have to reimplement the basics every time there is a problem
    to solve.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: This is a powerful expression of the idea of **abstraction** in software development,
    where lower-level units can be leveraged and engaged with at higher levels of
    implementation.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: This idea can be extended even further to the entire enterprise of training
    itself. At the lowest level of implementation (but still a very high level in
    the sense of the underlying algorithms), we can provide details about how we want
    the training process to go. We can manually define the exact set of hyperparameters
    (see the next section on *Optimizing hyperparameters*) to use in the training
    run in our code. I call this **hand cranking**. We can then move one level of
    abstraction up and supply ranges and bounds for our hyperparameters to tools designed
    to efficiently sample and test our model’s performance for each of these; for
    instance, *automated hyperparameter tuning*. Finally, there is one higher level
    of abstraction that has created a lot of media excitement over the past few years,
    where we optimize over which algorithm to run. This is known as **automated ML**
    or **AutoML**.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: There can be a lot of hype surrounding AutoML, with some people proclaiming
    the eventual automation of all ML development job roles. In my opinion, this is
    just not realistic, as selecting your model and hyperparameters is only one aspect
    of a hugely complex engineering challenge (hence this being a book and not a leaflet!).
    AutoML is, however, a very powerful tool that should be added to your arsenal
    of capabilities when you go into your next ML project.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize all of this quite handily as a *hierarchy of automation*;
    basically, how much control do you, as the ML engineer, want in the training process?
    I once heard this described in terms of gear control in a car (credit: *Databricks
    at Spark AI 2019*). Hand cranking is the equivalent of driving a manual car with
    full control over the gears: there’s more to think about, but it can be very efficient
    if you know what you’re doing. One level up, you have automatic cars: there’s
    less to worry about so that you can focus more on getting to your destination,
    traffic, and other challenges. This is a good option for a lot of people but still
    requires you to have sufficient knowledge, skills, and understanding. Finally,
    we have self-driving cars: sit back, relax, and don’t even worry about how to
    get where you’re going. You can focus on what you are going to do once you get
    there.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'This *hierarchy of automation* is shown in the following diagram:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – The hierarchy of automation of ML model optimization, with AutoML
    as the most automated possibility ](img/B19525_03_11.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: The hierarchy of automation of ML model optimization, with AutoML
    as the most automated possibility.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: That, in a nutshell, is how the different levels of training abstraction link
    together.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will discuss how to get started building out implementations
    of hyperparameter optimization and AutoML. We will not cover “hand cranking” as
    that is self-explanatory.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameters
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you fit some sort of mathematical function to data, some values are tuned
    during the fitting or training procedure: these are called **parameters**. For
    ML, there is a further level of abstraction where we have to define the values
    that tell the algorithms we are employing *how they should update the parameters*.
    These values are called **hyperparameters**, and their selection is one of the
    important *dark arts* of training ML algorithms.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tables list some hyperparameters that are used for common ML
    algorithms to show you the different forms they may take. These lists are not
    exhaustive but are there to highlight that hyperparameter optimization is not
    a trivial exercise:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Hyperparameters** | **What This Controls** |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| Decision Trees andRandom Forests |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: Tree depth.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min/max leaves.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: How many levels are in your trees.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much branching can occur at each level.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '| Support Vector Machines |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: C
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Penalty for misclassification.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The radius of influence of the training points for **Radial Basis Function**
    (**RBF**) kernels.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '| Neural Networks(numerous architectures) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Learning rate.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of hidden layers.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many more.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Update step sizes.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How deep your network is.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The firing conditions of your neurons.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '| Logistic Regression |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: Solver
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization type.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization prefactor.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: How to minimize the loss.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to prevent overfitting/make the problem well behaved.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strength of the regularization type.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3.1: Some hyperparameters and what they control for some supervised algorithms.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Further examples can be seen in the following table:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Hyperparameters** | **What This Controls** |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| K-Nearest Neighbors |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: K
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metric.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The number of clusters.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define the distance between points.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '| DBSCAN |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: Epsilon
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of samples.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metric.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The max distance to be considered neighbors.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many neighbors are required to be considered core.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define the distance between points.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3.2: Some hyperparameters and what they control for some unsupervised
    algorithms.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: All of these hyperparameters have their own specific set of values they can
    take. This range of hyperparameter values for the different potential algorithms
    you want to apply to your ML solution means that there are a lot of ways to define
    a *working* model (meaning one that doesn’t break the implementation you are using),
    but how do you find the *optimal* model?
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: This is where hyperparameter search comes in. The concept is that for a finite
    number of hyperparameter value combinations, we want to find the set that gives
    the best model performance. This is another optimization problem that’s similar
    to that of training in the first place!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss two very popular hyperparameter optimization
    libraries and show you how to implement them in a few lines of Python.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand which algorithms are being used for optimization
    in these hyperparameter libraries, as you may want to use a couple of different
    implementations from each to compare different approaches and assess performance.
    If you didn’t look at how they were working under the hood, you could easily make
    unfair comparisons – or worse, you could be comparing almost the same thing without
    knowing it! If you have some deeper knowledge of how these solutions work, you
    will also be able to make better judgment calls as to when they will be beneficial
    and when they will be overkill. Aim to have a working knowledge of a few of these
    algorithms and approaches, since this will help you design more holistic training
    systems with algorithm-tuning approaches that complement one another.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Hyperopt
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hyperopt** is an open source Python package that bills itself as being *for
    serial and parallel optimization over awkward search spaces, which may include
    real-valued, discrete, and conditional dimensions*. Check out the following link
    for more information: [https://github.com/Hyperopt/Hyperopt](https://github.com/Hyperopt/Hyperopt).
    At the time of writing, version 0.2.5 comes packaged with three algorithms for
    performing optimization over user-provided search spaces:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '**Random search**: This algorithm essentially selects random numbers within
    your provided ranges of parameter values and tries them. It then evaluates which
    sets of numbers provide the best performance according to your chosen objective
    function.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree of Parzen Estimators** (**TPE**): This is a Bayesian optimization approach
    that models distributions of hyperparameters below and above a threshold for the
    objective function (roughly *good* and *bad* scorers), and then aims to draw more
    values from the *good* hyperparameter distribution.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive TPE**: This is a modified version of TPE that allows for some optimization
    of the search, as well as the ability to create an ML model to help guide the
    optimization process.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Hyperopt repository and documentation contain several nice and detailed
    worked examples. We will not go through these here. Instead, we will learn how
    to use this for a simple classification model, such as the one we defined in *Chapter
    1*, *Introduction to ML Engineering*. Let’s get started:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'In Hyperopt, we must define the hyperparameters that we want to optimize across.
    For example, for a typical logistic regression problem, we could define the space
    of hyperparameters to cover, whether we want to reuse parameters that were learned
    from the previous model runs each time (`warm_start`), whether we want the model
    to include a bias in the decision function (`fit_intercept`), the tolerance set
    for deciding when to stop the optimization (`tol`), the regularization parameter
    (`C`), which `solver` we want to try, and the maximum number of iterations, `max_iter`,
    in any training run:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we have to define an objective function to optimize. In the case of our
    classification algorithm, we can simply define the `loss` function we want to
    minimize as 1 minus the `f1-score`. Note that Hyperopt allows your objective function
    to supply run statistics and metadata via your return statement if you are using
    the `fmin` functionality. The only requirement if you do this is that you return
    a value labeled `loss` and a valid status value from the list of `Hyperopt.STATUS_STRING`
    (`ok` by default and `fail` if there is an issue in the calculation that you want
    to call out as a failure):'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we must optimize using the `fmin` method with the **TPE** algorithm:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The content of `best` is a dictionary containing all the best hyperparameters
    in the search space you defined. So, in this case, we have the following:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can then use these hyperparameters to define your model for training on
    the data.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Optuna
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Optuna** is a software package that has an extensive series of capabilities
    based on some core design principles, such as its **define-by-run** API and modular
    architecture. *Define-by-run* here refers to the fact that, when using Optuna,
    the user does not have to define the full set of parameters to test, which is
    *define-and-run*. Instead, they can provide some initial values and ask Optuna
    to suggest its own set of experiments to run. This saves the user time and reduces
    the code footprint (two big pluses for me!).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Optuna contains four basic search algorithms: **grid search**, **random search**,
    **TPE**, and the **Covariance Matrix Adaptation Evolution Strategy** (**CMA-ES**)
    algorithm. We covered the first three previously, but CMA-ES is an important addition
    to the mix. As its name suggests, this is based on an evolutionary algorithm and
    draws samples of hyperparameters from a multivariate Gaussian distribution. Then,
    it uses the rankings of the evaluated scores for the given objective function
    to dynamically update the parameters of the Gaussian distribution (the covariance
    matrix being one set of these) to help find an optimum over the search space quickly
    and robustly.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: The key thing that makes Optuna’s optimization process different from Hyperopt,
    however, is in its application of **pruning** or **automated early stopping**.
    During optimization, if Optuna detects evidence that a trial of a set of hyperparameters
    will not lead to a better overall trained algorithm, it terminates that trial.
    The developers of the package suggest that this leads to overall efficiency gains
    in the hyperparameter optimization process by reducing unnecessary computation.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’re looking at the same example we looked at previously, but we are
    now using Optuna instead of Hyperopt:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'First, when using Optuna, we can work using an object known as `Study`, which
    provides us with a convenient way to fold our search space into our `objective`
    function:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we must set up the data in the same way as we did in the Hyperopt example:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we can define this `Study` object that we mentioned and tell it how we
    wish to optimize the value that’s returned by our `objective` function, complete
    with guidance on how many trials to run in the `study`. Here, we will use the
    TPE sampling algorithm again:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can access the best parameters via the `study.best_trial.params` variable,
    which gives us the following values for the best case:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As you can see, Optuna is also very simple to use and very powerful. Now, let’s
    look at the final level of the hierarchy of automation: AutoML.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that these values are different from the ones returned by Hyperopt.
    This is because we have only run 16 trials in each case, so we are not effectively
    subsampling the space. If you run either of the Hyperopt or Optuna samples a few
    times in a row, you can get quite different results for the same reason. The example
    given here is just to show the syntax, but if you are keen, you can set the number
    of iterations to be very high (or create smaller spaces to sample), and the results
    of the two approaches should roughly converge.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: AutoML
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final level of our hierarchy is the one where we, as the engineer, have
    the least direct control over the training process, but where we also potentially
    get a good answer for very little effort!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: The development time that’s required to search through many hyperparameters
    and algorithms for your problem can be large, even when you code up reasonable-looking
    search parameters and loops.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Given this, the past few years have seen the deployment of several **AutoML**
    libraries and tools in a variety of languages and software ecosystems. The hype
    surrounding these techniques has meant they have had a lot of airtime, which has
    led to several data scientists questioning when their jobs will be automated away.
    As we mentioned previously in this chapter, in my opinion, declaring the death
    of data science is extremely premature and also dangerous from an organizational
    and business performance standpoint. These tools have been given such a pseudo-mythical
    status that many companies could believe that simply using them a few times will
    solve all their data science and ML problems.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: They are wrong, but they are also right.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: These tools and techniques *are* very powerful and *can* help make some things
    better, but they are not a magical *plug-and-play* panacea. Let’s explore these
    tools and start to think about how to incorporate them into our ML engineering
    workflow and solutions.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: auto-sklearn
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of our favorite libraries, good old Scikit-Learn, was always going to be
    one of the first targets for building a popular AutoML library. One of the very
    powerful features of auto-sklearn is that its API has been designed so that the
    main objects that optimize and section models and hyperparameters can be swapped
    seamlessly into your code.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, an example will show this more clearly. In the following example,
    we will assume that the `Wine` dataset (a favorite for this chapter) has already
    been retrieved and split into train and test samples in line with other examples,
    such as the one in the *Detecting drift* section:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'First, since this is a classification problem, the main thing we need to get
    from `auto-sklearn` is the `autosklearn.classification` object:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We must then define our `auto-sklearn` object. This provides several parameters
    that help us define how the model and hyperparameter tuning process will proceed.
    In this example, we will provide an upper time limit in seconds for running the
    overall optimization and an upper time limit in seconds for any single call to
    the ML model:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, just like we would fit a normal `sklearn` classifier, we can fit the
    `auto-sklearn` object. As we mentioned previously, the `auto-sklearn` API has
    been designed so that this looks familiar:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that we’ve fit the object, we can start to dissect what has been achieved
    by the object during its optimization run.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we can see which models were tried and which were kept in the object
    as part of the final ensemble:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can then get a readout of the main statistics from the run:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we can predict some text features, as expected:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we can check how well we did by using our favorite metric calculators
    – in this case, the `sklearn metrics` module:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As you can see, it is very straightforward to start using this powerful library,
    especially if you are already comfortable working with `sklearn`.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s discuss how we extend this concept to neural networks, which have
    an extra layer of complexity due to their different potential model architectures.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: AutoKeras
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A particular area where AutoML has been a big hit is neural networks. This is
    because for a neural network, the question of *what is the best model?* is a very
    complicated one. For our typical classifiers, we can usually think of a relatively
    short, finite list of algorithms to try. For a neural network, we don’t have this
    finite list. Instead, we have an essentially infinite set of possible neural network
    *architectures*; for instance, for organizing the neurons into layers and the
    connections between them. Searching for the optimal neural network architecture
    is a problem in which powerful optimization can make your life, as an ML engineer
    or data scientist, a whole lot easier.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, we are going to explore an AutoML solution built on top of
    the very popular neural network API library known as Keras. Unbelievably, the
    name of this package is – you guessed it – AutoKeras!
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will, once again, assume that the `Wine` dataset has been
    loaded so that we can focus on the details of the implementation. Let’s get started:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the `autokeras` library:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, it’s time for the fun and, for `autokeras`, the extremely simple bit!
    Since our data is structured (tabular with a defined schema), we can use the `StructuredDataClassifier`
    object, which wraps the underlying mechanisms for automated neural network architecture
    and hyperparameter search:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, all we have to do is fit this classifier object, noticing its similarity
    to the `sklearn` API. Remember that we assume that the training and test data
    exist in `pandas DataFrames`, as in the other examples in this chapter:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The training objects in AutoKeras have a convenient evaluation method wrapped
    within them. Let’s use this to see how accurate our solution was:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: With that, we have successfully performed a neural network architecture and
    hyperparameter search in a few lines of Python. As always, read the solution documentation
    for more information on the parameters you can provide to the different methods.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve covered how to create performant models, in the next section,
    we will learn how to persist these models so that they can be used in other programs.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Persisting your models
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced some of the basics of model version control
    using MLflow. In particular, we discussed how to log metrics for your ML experiments
    using the MLflow Tracking API. We are now going to build on this knowledge and
    consider the touchpoints our training systems should have with model control systems
    in general.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s recap what we’re trying to do with the training system. We want
    to automate (as far as possible) a lot of the work that was done by the data scientists
    in finding the first working model, so that we can continually update and create
    new model versions that still solve the problem in the future. We would also like
    to have a simple mechanism that allows the results of the training process to
    be shared with the part of the solution that will carry out the prediction when
    in production. We can think of our model version control system as a bridge between
    the different stages of the ML development process we discussed in *Chapter 2*,
    *The Machine Learning Development Process*. In particular, we can see that the
    ability to track experiment results allows us to keep the results of the **Play**
    phase and build on these during the **Develop** phase. We can also track more
    experiments, test runs, and hyperparameter optimization results in the same place
    during the **Develop** phase. Then, we can start to tag the performant models
    as ones that are good candidates for deployment, thus bridging the gap between
    the **Develop** and **Deploy** development phases.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'If we focus on MLflow for now (though plenty of other solutions are available
    that fulfill the need for a model version control system), then MLflow’s Tracking
    and Model Registry functionalities nicely slot into these bridging roles. This
    is represented schematically in the following diagram:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – How the MLflow Tracking and Model Registry functionalities can
    help us progress through the different stages of the ML development process ](img/B19525_03_12.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: How the MLflow Tracking and Model Registry functionalities can
    help us progress through the different stages of the ML development process.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 2*, *The Machine Learning Development Process*, we only explored
    the basics of the MLflow Tracking API for storing experimental model run metadata.
    Now, we will briefly dive into how to store production-ready models in a very
    organized way so that you can start to perform model staging. This is the process
    whereby models can be progressed through stages of readiness, and you can swap
    models in and out of production if you wish to. This is an extremely important
    part of any training system that supplies models and will run as part of a deployed
    solution, which is what this book is all about!
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: As alluded to previously, the functionality that we need in MLflow is called
    **Model Registry**, which enables you to manage the staging of models across your
    development life cycle. Here, we will walk through examples of how to take a logged
    model and push it to the registry, how to update information such as the model
    version number in the registry, and then how to progress your model through different
    life cycle stages. We will finish this section by learning how to retrieve a given
    model from the registry in other programs – a key point if we are to share our
    models between separate training and prediction services.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the Python code for interacting with Model Registry, we
    have one important piece of setup to perform. The registry only works if a database
    is being used to store the model metadata and parameters. This is different from
    the basic Tracking API, which works with just a file backend store. This means
    that before pushing models to Model Registry, we have to fire up an MLflow server
    with a database backend. You can do this with a **SQLite** database running locally
    by executing the following command in your terminal.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'You will have to run this before the code snippets in the rest of this section
    (this command is stored in a short Bash script in this book’s GitHub repository,
    under [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python/blob/main/Chapter03/mlflow-advanced/start-mlflow-server.sh](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python/blob/main/Chapter03/mlflow-advanced/start-mlflow-server.sh)):'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now that the backend database is up and running, we can use it as part of our
    model workflow. Let’s get started:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by logging some metrics and parameters for one of the models we
    trained earlier in this chapter:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Inside the same code block, we can now log the model to Model Registry, providing
    a name for the model to reference later:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, let’s assume we are running a prediction service and we want to retrieve
    the model and predict using it. Here, we have to write the following:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'By default, newly registered models in Model Registry are assigned the `''Staging''`
    stage value. Therefore, if we want to retrieve the model based on knowing the
    stage but not the model version, we could execute the following code:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Based on all of our discussions in this chapter, the result of our training
    system must be able to produce a model we are happy to deploy to production. The
    following piece of code promotes the model to a different stage, called `"Production"`:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: These are the most important ways to interact with Model Registry and we have
    covered the basics of how to register, update, promote, and retrieve your models
    in your training (and prediction) systems.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will learn how to chain our main training steps together into single
    units called **pipelines**. We will cover some of the standard ways of doing this
    inside single scripts, which will allow us to build our first training pipelines.
    In *Chapter 5*, *Deployment Patterns and Tools*, we will cover tools for building
    more generic software pipelines for your ML solution (of which your training pipeline
    may be a single component).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Building the model factory with pipelines
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of a software pipeline is intuitive enough. If you have a series
    of steps chained together in your code, so that the next step consumes or uses
    the output of the previous step or steps, then you have a pipeline.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, when we refer to a pipeline, we will be specifically dealing
    with steps that contain processing or calculations that are appropriate to ML.
    For example, the following diagram shows how this concept may apply to some of
    the steps the marketing classifier mentioned in *Chapter 1*, *Introduction to
    ML Engineering*:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – The main stages of any training pipeline and how this maps
    to a specific case from Chapter 1, Introduction to ML Engineering ](img/B19525_03_13.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: The main stages of any training pipeline and how this maps to
    a specific case from Chapter 1, Introduction to ML Engineering.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss some of the standard tools for building up your ML pipelines in
    code.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn pipelines
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our old friend **Scikit-Learn** comes packaged with some nice pipelining functionality.
    The API is extremely easy to use, as you would expect from **Scikit-Learn**, but
    has some concepts we should understand before proceeding:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '**The pipeline object**: This is the object that will bring together all the
    steps we require, in particular, `sklearn` demands that instantiated pipeline
    objects are composed of sequences of transformers and estimators, with all intermediate
    objects having the `.fit()` and `.transform()` methods and the last step being
    an estimator with at least the `.fit()` method. We will explain these terms in
    the next two points. The reason for this condition is that the `pipeline` object
    will inherit the methods from the last item in the sequence provided, so we must
    make sure to have `.fit()` present in the last object.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimators**: The estimator class is the base object in `scikit-learn` and
    anything in the package that can be fit on data and then predict on data, therefore
    the `.fit()` and `.predict()` methods, is a subclass of the estimator class.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers**: In **Scikit-Learn**, transformers are any estimators that
    have a `.transform()` or `.fit_transform()` method and, as you can guess, are
    mainly focused on transforming datasets from one form to another rather than performing
    predictions.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of the `pipeline` object really helps facilitate the simplification
    of your code, as rather than writing several different fitting, transforming,
    and predicting steps as their own function calls with datasets and then managing
    the flow of that data, you can simply compose them all in one object that manages
    this for you and uses the same simple API.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'There are new transformers and features being added to Scikit-Learn all the
    time, which means that it become possible to build more and more useful pipelines.
    For example, at the time of writing, Scikit-Learn versions greater than 0.20 also
    contain the `ColumnTransformer` object, which allows you to build pipelines that
    perform different actions on specific columns. This is exactly what we want to
    do with the logistic regression marketing model example we were discussing previously,
    where we want to standardize our numerical values and one-hot encode our categorical
    variables. Let’s get started:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'To create this pipeline, you need to import the `ColumnTransformer` and `Pipeline`
    objects:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'To show you how to chain steps inside the transformers that make up the pipeline,
    we will add some imputation later. For this, we need to import the `SimpleImputer`
    object:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now, we must define the numerical transformer sub-pipeline, which contains
    the two steps for imputation and scaling. We must also define the names of the
    numerical columns this will apply to so that we can use them later:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we must perform similar steps for the categorical variables, but here,
    we only have one transformation step to define for the `one-hot` encoder:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We must bring all of these preprocessing steps together into a single object,
    called `preprocessor`, using the `ColumnTransformer` object. This will apply our
    `transformers` to the appropriate columns of our DataFrame:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, we want to add the ML model step at the end of the previous steps
    and finalize the pipeline. We will call this `clf_pipeline`:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This is our first ML training pipeline. The beauty of the `scikit-learn` API
    is that the `clf_pipeline` object can now be called as if it were a standard algorithm
    from the rest of the library. So, this means we can write the following:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This will run the `fit` methods of all of the pipeline steps in turn.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: The previous example was relatively basic, but there are a few ways you can
    make this sort of pipeline more sophisticated if your use case requires it. One
    of the simplest and most extensible is the ability in Scikit-Learn to create custom
    transformer objects that inherit from the base classes. You can do this for a
    class transformer by inheriting from the `BaseEstimator` and `TransformerMixIn`
    classes and defining your own transformation logic. As a simple example, let’s
    build a transformer that takes in the specified columns and adds a float. This
    is just a simple schematic to show you how it’s done; I can’t imagine that adding
    a single float to your columns will be that helpful in most cases!
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'You could then add this transformer to your `pipeline`:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This example of adding a number is actually not the best use case for using
    this class-based transformer definition, as this operation is stateless. Since
    there is no training or complex manipulation of the values being fed in that requires
    the class to retain and update its state, we have actually just wrapped a function.
    The second way of adding your own custom steps takes advantage of this and uses
    the `FunctionTransformer` class to wrap any function you provide:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: By building on these examples, you can start to create complex pipelines that
    can perform any feature engineering task you want.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, we can clearly see that the ability to abstract the
    steps that are performing feature engineering and training your model into a single
    object is very powerful, as it means you can reuse this object in various places
    and build even more complex workflows with it without constantly recoding the
    details of the implementation. Abstraction is a good thing!
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: We will now turn to another way of writing pipelines, using Spark ML.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML pipelines
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is another toolset we have been using throughout this book that will
    be particularly important when we discuss scaling up our solutions: Apache Spark
    and its ML ecosystem. We will see that building a similar pipeline with Spark
    ML requires a slightly different set of syntax, but the key concepts look very
    similar to the Scikit-Learn case.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: There are a few important points to mention about PySpark pipelines. Firstly,
    in line with good programming practices in Scala, which Spark is written in, objects
    are treated as **immutable**, so transformations do not occur *in place*. Instead,
    new objects are created. This means that the output of any transformation will
    require new columns to be created in your original DataFrame (or indeed new columns
    in a new DataFrame).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the Spark ML estimators (that is, the ML algorithms) all require the
    features to be assembled into one tuple-like object in a single column. This contrasts
    with Scikit-Learn, where you can keep all the features in their columns in your
    data object. This means that you need to become comfortable with the use of **assemblers**,
    which are utilities for pulling disparate feature columns together, especially
    when you are working with mixed categorical and numerical features that must be
    transformed in different ways before being invested by the algorithm.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, Spark has many functions that use **lazy evaluation**, meaning that
    they are only executed when they’re triggered by specific actions. This means
    that you can build up your entire ML pipeline and not have to transform any data.
    The reason for lazy evaluation is that the computational steps in Spark are stored
    in a **Directed Acyclic Graph** (**DAG**) so that the execution plan can be optimized
    before you perform the computational steps, making Spark very efficient.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Finally – and this is a small point – it is commonplace to write PySpark variables
    using *camel case* rather than the common *snake case*, which is often used for
    Python variables (for instance, **variableName** versus `variable_name`). This
    is done to keep the code in line with the PySpark functions that inherit this
    convention from the underlying **Scala** code behind Spark.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: The Spark ML pipelines API utilizes concepts of Transformer and Estimator in
    a similar way to how the Scikit-Learn pipeline API did, with some important differences.
    The first difference is that Transformers in Spark ML implement `.transform()`
    but not the `.fit_transform()` method. Secondly, the Transformer and Estimator
    objects in Spark ML are stateless, so once you have trained them they do not change
    and they only contain model metadata. They don’t store anything about the original
    input data. One similarity is that pipelines are treated as Estimators in Spark
    ML as well.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: We will now build a basic example to show how to build a training pipeline using
    the Spark ML API.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must one-hot encode the categorical features for the previous example
    using the following syntax:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'For the numerical columns, we must perform imputation:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Then, we must perform standardization. Here, we need to be a bit clever about
    how we apply `StandardScaler` as it only applies to one column at a time. Therefore,
    we need to create a scaler for each numerical feature after pulling our numerically
    imputed features into a single feature vector:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Then, we have to assemble the numerical and categorical transformed features
    into one feature column:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Finally, we can define our model step, add this to the `pipeline`, and then
    train on and transform:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'You can then persist the model pipeline as you would any `Spark` object, for
    example, by using:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Where `path` is the path to your target destination. You would then read this
    pipeline into memory by using:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: And that is how we can build a training pipeline in PySpark using **Spark ML**.
    This example shows you enough to get started with the API and build out your own,
    more sophisticated pipelines.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: We will now conclude this chapter with a brief summary of everything we have
    covered.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the important topic of how to build up our
    solutions for training and staging the ML models that we want to run in production.
    We split the components of such a solution into pieces that tackled training the
    models, the persistence of the models, serving the models, and triggering retraining
    for the models. I termed this the “Model Factory.”
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: We got into the more technical details of some important concepts with a deep
    dive into what training an ML model really means, which we framed as learning
    about how ML models learn. Some time was then spent on the key concepts of feature
    engineering, or how you transform your data into something that a ML model can
    understand during this process. This was followed by sections on how to think
    about the different modes your training system can run in, which I termed “train-persist”
    and “train-run.”
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed how you can perform drift detection on your models and the
    data they are consuming using a variety of techniques. This included some examples
    of performing drift detection using the Alibi Detect and Evidently packages and
    a discussion of how to calculate feature importances.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: We then covered the concept of how the training process can be automated at
    various levels of abstraction, before explaining how to programmatically manage
    the staging of your models with MLflow Model Registry. The final section covered
    how to define training pipelines in the Scikit-Learn and Spark ML packages.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will find out how to package up some of these concepts
    in a Pythonic way so that they can be deployed and reused seamlessly in other
    projects.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
