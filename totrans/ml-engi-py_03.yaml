- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From Model to Model Factory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is all about one of the most important concepts in ML engineering:
    how do you take the difficult task of training and fine-tuning your models and
    make it something you can automate, reproduce, and scale for production systems?'
  prefs: []
  type: TYPE_NORMAL
- en: We will recap the main ideas behind training different ML models at a theoretical
    and practical level, before providing motivation for retraining, namely the idea
    that ML models will not perform well forever. This concept is also known as **drift**.
    Following this, we will cover some of the main concepts behind feature engineering,
    which is a key part of any ML task. Next, we will deep dive into how ML works
    and how it is, at heart, a series of optimization problems. We will explore how
    when setting out to tackle these optimization problems, you can do so with a variety
    of tools at various levels of abstraction. In particular, we will discuss how
    you can provide the direct definition of the model you want to train, which I
    term *hand cranking*, or how you can perform hyperparameter tuning or **automated
    ML** (**AutoML**). We will look at examples of using different libraries and tools
    that do all of these, before exploring how to implement them for later use in
    your training workflow. We will then build on the introductory work we did in
    *Chapter 2*, *The Machine Learning Development Process*, on MLflow by showing
    you how to interface with the different MLflow APIs to manage your models and
    update their status in MLflow’s Model Registry.
  prefs: []
  type: TYPE_NORMAL
- en: We will end this chapter by discussing the utilities that allow you to chain
    all of your ML model training steps into single units known as **pipelines**,
    which can help act as more compact representations of all the steps we have discussed
    previously. The summary at the end will recap the key messages and also outline
    how what we have done here will be built upon further in *Chapter 4*, *Packaging
    Up*, and *Chapter 5*, *Deployment Patterns and Tools*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, this chapter will tell you *what* you need to stick together in
    your solution, while later chapters will tell you *how* to stick them together
    robustly. We will cover this in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model factory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering features for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing your training system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting your models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the model factory with pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in the previous chapters, the required packages for this chapter are contained
    within a conda environment `.yml` file in the repository folder for `Chapter03`,
    so to create the conda environment for this chapter, simply run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will install packages including MLflow, AutoKeras, Hyperopt Optuna, auto-sklearn,
    Alibi Detect, and Evidently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if you are running these examples on a Macbook with Apple Silicon,
    a straight `pip` or `conda` install of TensorFlow and `auto-sklearn` may not work
    out of the box. Instead, you will need to install the following packages to work
    with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And then
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To install `auto-sklearn`, you will need to run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Or install `swig` using whatever Mac package manager you use, then you can run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Defining the model factory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to develop solutions that move away from ad hoc, manual, and inconsistent
    execution and toward ML systems that can be automated, robust, and scalable, then
    we have to tackle the question of how we will create and curate the star of the
    show: the models themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the key components that have to be brought
    together to move toward this vision and provide some examples of what these may
    look like in code. These examples are not the only way to implement these concepts,
    but they will enable us to start building up our ML solutions toward the level
    of sophistication we will need if we want to deploy in the *real world*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components we are talking about here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training system**: A system for robustly training our models on the data
    we have in an automated way. This consists of all the code we have developed to
    train our ML models on data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model store**: A place to persist successfully trained models and a place
    to share production-ready models with components that will run the predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drift detector**: A system for detecting changes in model performance to
    trigger training runs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These components, combined with their interaction with the deployed prediction
    system, encompass the idea of a model factory. This is shown schematically in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The components of the model factory ](img/B19525_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: The components of the model factory.'
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this chapter, we will explore the three components we mentioned
    previously in detail. **Prediction systems** will be the focus of later chapters,
    especially *Chapter 5*, *Deployment Patterns and Tools*.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s explore what it means to train an ML model and how we can build
    systems to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At their heart, ML algorithms all contain one key feature: an optimization
    of some kind. The fact that these algorithms *learn* (meaning that they iteratively
    improve their performance concerning an appropriate metric upon exposure to more
    observations) is what makes them so powerful and exciting. This process of learning
    is what we refer to when we say *training*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover the key concepts underpinning training, the options
    we can select in our code, and what these mean for the potential performance and
    capabilities of our training system.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just stated that training is an optimization, but what exactly are we
    optimizing? Let’s consider supervised learning. In training, we provide the labels
    or values that we would want to predict for the given feature so that the algorithms
    can learn the relationship between the features and the target. To optimize the
    internal parameters of the algorithm during training, it needs to know how *wrong*
    it would be with its current set of parameters. The optimization is then all about
    updating the parameters so that this measure of *wrongness* gets smaller and smaller.
    This is exactly what is captured by the concept of a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions come in a variety of forms, and you can even define your own
    if you need to with a lot of packages, but there are some standard ones that it
    helps to be aware of. The names of some of these are mentioned here.
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression problems, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error/L2 loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean absolute error/L1 loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For binary classification problems, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Log loss/logistic loss/cross-entropy loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinge loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For multi-class classification problems, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class across entropy loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullback Leibler divergence loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In unsupervised learning, the concept of a loss function still applies but now
    the target is the correct distribution of the input data. After defining your
    loss function, you then need to optimize it. This is what we will look at in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cutting your losses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we know that training is all about optimizing, and we know what
    to optimize, but we have not covered *how* to optimize yet.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, there are plenty of options to choose from. In this section, we will
    look at some of the main approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the **constant learning rate** approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent**: This algorithm works by calculating the derivative of
    our loss function regarding our parameters, and then uses this to construct an
    update that moves us in the direction of decreasing loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch gradient descent**: The gradient that we use to make our move in the
    parameter space is found by taking the average of all the gradients found. It
    does this by looking at each data point in our training set and checking that
    the dataset is not too large and the loss function is relatively smooth and convex.
    This can pretty much reach the global minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent**: The gradient is calculated using one randomly
    selected data point at each iteration. This is faster at getting to the global
    minimum of the loss function, but it is more susceptible to sudden fluctuations
    in the loss after each optimization step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch gradient descent**: This is a mixture of both the batch and stochastic
    cases. In this case, updates to the gradient for each update to the parameters
    use several points greater than one but smaller than the entire dataset. This
    means that the size of the batch is now a parameter that needs to be tuned. The
    larger the batch, the more we approach batch gradient descent, which provides
    a better gradient estimate but is slower. The smaller the batch, the more we approach
    stochastic gradient descent, which is faster but not as robust. Mini-batch allows
    us to decide where in between the two we want to be. Batch sizes may be selected
    with a variety of criteria in mind. These can take on a range of memory considerations.
    Batches processed in parallel and larger batches will consume more memory while
    providing improved generalization performance for smaller batches. See *Chapter
    8* of the book *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
    at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) for
    more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, there are the **adaptive learning rate methods**. Some of the most common
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaGrad**: The learning rate parameters are dynamically updated based on
    the properties of the learning updates during the optimization process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AdaDelta**: This is an extension of `AdaGrad` that does not use all the previous
    gradient updates. Instead, it uses a rolling window on the updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSprop**: This works by maintaining a moving average of the square of all
    the gradient steps. It then divides the latest gradient by the square root of
    this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam**: This is an algorithm that is supposed to combine the benefits of
    `AdaGrad` and `RMSprop`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limits and capabilities of all these optimization approaches are important
    for us, as ML engineers, because we want to ensure that our training systems use
    the right tool for the job and are optimal for the problem at hand. Just having
    the awareness that there are multiple options for your internal optimization will
    also help you focus your efforts and increase performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Simple representation of training as the optimization of a loss
    function ](img/B19525_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Simple representation of training as the optimization of a loss
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how we prepare the raw material that the model factory needs
    to do its work, the data, through the process of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data can come in all varieties of types and quality. It can be tabular and from
    a relational database, unstructured text from a crawled website, a formatted response
    from a REST API, an image, an audio file, or any other form you can think of.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to run machine learning algorithms on this data though, the first
    thing you have to do is make it readable by these algorithms. This process is
    known as *feature engineering*, and that is what the next few sections will discuss
    to give you some grounding in the main principles. There are many excellent resources
    on feature engineering that can go into a lot of depth, so we will only touch
    on some of the main concepts here. For more information, you could check out a
    book like *Feature Engineering Cookbook* by Soledad Galli, Packt, 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering features for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we feed any data into an ML model, it has to be transformed into a state
    that can be *understood* by our models. We also need to make sure we only do this
    on the data we deem useful for improving the performance of the model, as it is
    far too easy to explode the number of features and fall victim to the *curse of
    dimensionality*. This refers to a series of related observations where, in high-dimensional
    problems, data becomes increasingly sparse in the feature space, so achieving
    statistical significance can require exponentially more data. In this section,
    we will not cover the theoretical basis of feature engineering. Instead, we will
    focus on how we, as ML engineers, can help automate some of the steps in production.
    To this end, we will quickly recap the main types of feature preparation and feature
    engineering steps so that we have the necessary pieces to add to our pipelines
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering categorical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Categorical features are those that form a non-numerical set of distinct objects,
    such as the day of the week or hair color. They can be distributed in a variety
    of ways throughout your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an ML algorithm to be able to *digest* a categorical feature, we need to
    translate the feature into something numerical, while also ensuring that the numerical
    representation *does not produce bias or weigh our values inappropriately*. An
    example of this would be if we had a feature that contained different products
    sold in a supermarket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can map each to a positive integer using `sklearn`''s `OrdinalEncoder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is what is called **ordinal encoding**. We have mapped these features to
    numbers, so there’s a big tick there, but is the representation appropriate? Well,
    if you think about it for a second, not really. These numbers seem to suggest
    that cereal is to bleach as toilet roll is to cereal, and that the average of
    toilet roll and bleach is cereal. These statements don’t make sense (and I don’t
    want bleach and toilet roll for breakfast), so this suggests we should try a different
    approach. This representation would be appropriate, however, in cases where we
    wanted to maintain the notion of ordering in the categorical features. An excellent
    example would be if we had a survey and the participants were asked their opinion
    of the statement *breakfast is the most important meal of the day*. If the participants
    were then told to select one option from the list *Strongly Disagree*, *Disagree*,
    *Neither* *Disagree* *nor* *Agree*, *Agree*, and *Strongly Agree* and we ordinally
    encoded this data to map to the numerical list of *1*, *2*, *3*, *4*, and *5*,
    then we could more intuitively answer questions such as *Was the average response
    more in agreement or disagreement?* and *How widespread was the opinion on this
    statement?* Ordinal encoding would help here, but as we mentioned previously,
    it’s not necessarily correct in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we could do is consider the list of items in this feature, and then provide
    a binary number to represent whether the value is or isn’t that particular value
    in the original list. So, here, we will decide to use `sklearn`''s `OneHotEncoder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This representation is known as a **one-hot encoding**. There are a few benefits
    to this method of encoding, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no enforced orderings of the values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the feature vectors have unit norms (more on this later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every unique feature is orthogonal to the others, so there are no weird averages
    or distance statements that are implicit in the representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the disadvantages of this approach is that if your categorical list contains
    a lot of instances, then the size of your feature vector will easily blow up,
    and we have to both store and work with extremely sparse vectors and matrices
    at the algorithmic level. This can very easily lead to issues in several implementations
    and is another manifestation of the dreaded curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, numerical features are discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering numerical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Preparing numerical features is slightly easier since we already have numbers,
    but there are a few steps we still need to take to prepare for many algorithms.
    For most ML algorithms, the features must be all on similar scales; for example,
    they must have a magnitude between -1 and 1 or 0 and 1\. This is for the relatively
    obvious reason that some algorithms taking in a feature for house price values
    of up to a million dollars and another for the square footage of the house will
    automatically weigh the larger dollar values more. This also means that we lose
    the helpful notion of where specific values sit in their distributions. For example,
    some algorithms will benefit from scaling features so that the median dollar value
    and the median square footage value are both represented by 0.5 rather than 500,000
    and 350\. Or we may want all of our distributions to have the same meaning if
    they were normally distributed, which allows our algorithms to focus on the shape
    of the distributions rather than their locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what do we do? Well, as always, we are not starting from scratch and there
    are some standard techniques we can apply. Some very common ones are listed here,
    but there are far too many to include all of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardization**: This is a transformation of a numerical feature and assumes
    that the distribution of values is normal or Gaussian before scaling the variance
    to be 1 and the average to be 0\. If your data is indeed normal or Gaussian, then
    this is a good technique to use. The mathematical formula for standardization
    is very simple, so I’ve provided it here, where *z* represents the transformed
    value, *x* is the original value, and ![](img/B19525_03_001.png) and ![](img/B19525_03_002.png)
    are the average and standard deviation, respectively:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19525_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Min-max normalization**: In this case, we want to scale the numerical features
    so that they’re always between 0 and 1, irrespective of the type of distribution
    that they follow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is intuitively easy to do, as you just need to subtract the minimum of
    the distribution from any given value and then divide by the range of the data
    (maximum minus minimum). You can think of this first step as making sure that
    all the values are greater than or equal to 0\. The second step involves making
    sure that their maximum size is 1\. This can be written with a simple formula,
    where the transformed number, ![](img/B19525_03_004.png) is the original number,
    and ![](img/B19525_03_004.png) represents the entire distribution of that feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Feature vector normalization**: Here, you scale every single sample in your
    dataset so that they have norms equal to 1\. This can be very important if you
    are using algorithms where the distance or cosine similarity between features
    is an important component, such as in clustering. It is also commonly used in
    text classification in combination with other feature engineering methods, such
    as the **TF-IDF** **statistic**. In this case, assuming your entire feature is
    numerical, you just calculate the appropriate norm for your feature vector and
    then divide every component by that value. For example, if we use the Euclidean
    or L2-norm of the feature vector, ![](img/B19525_03_007.png), then we would transform
    each component, ![](img/B19525_03_008.png) via the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19525_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To highlight the improvements these simple steps can make to your model’s performance,
    we will look at a simple example from the `sklearn` wine dataset. Here, we will
    be training a Ridge classifier on data that has not been standardized and then
    on data that has been standardized. Once we’ve done this, we will compare the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the relevant libraries and set up our training and test
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must make a typical 70/30 train/test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must train a model without any standardization in the features and
    predict on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must do the same but with a standardization step added in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, if we print some performance metrics, we will see that without scaling,
    the accuracy of the predictions is at `0.76`, while the other metrics, such as
    the weighted averages of `precision`, `recall`, and `f1-score`, are `0.83`, `0.76`,
    and `0.68`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the case where we standardized the data, the metrics are far better across
    the board, with the accuracy and weighted averages of the `precision`, `recall`,
    and `f1-score` all at `0.98`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can see a significant jump in performance, just by adding one simple
    step to our ML training process.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at how training is designed and works at its core. This will
    help us make sensible choices for our algorithms and training approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Designing your training system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Viewed at the highest level, ML models go through a life cycle with two stages:
    a **training** phase and an **output** phase. During the training phase, the model
    is fed data to learn from the dataset. In the prediction phase, the model, complete
    with its optimized parameters, is fed new data in order and returns the desired
    output.'
  prefs: []
  type: TYPE_NORMAL
- en: These two phases have very different computational and processing requirements.
    In the training phase, we have to expose the model to as much data as we can to
    gain the best performance, all while ensuring subsets of data are kept aside for
    testing and validation. Model training is fundamentally an optimization problem,
    which requires several incremental steps to get to a solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, this is computationally demanding, and in cases where the data is
    relatively large (or compute resources are relatively low), it can take a long
    time. Even if you had a small dataset and a lot of computational resources, training
    is still not a low-latency process. Also, it is a process that is often run in
    batches and where small additions to the dataset will not make that much difference
    to model performance (there are exceptions to this). Prediction, on the other
    hand, is a more straightforward process and can be thought of in the same way
    as running any calculation or function in your code: inputs go in, a calculation
    occurs, and the result comes out. This (in general) is not computationally demanding
    and is low latency.'
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, this means that, firstly, it makes sense to separate these two
    steps (training and prediction) both logically and in code. Secondly, it means
    we have to consider the different execution requirements for these two stages
    and build this into our solution designs. Finally, we need to make choices about
    our training regime, including whether we schedule training in batches, use incremental
    learning, or should trigger training based on model performance criteria. These
    are the key parts of your training system.
  prefs: []
  type: TYPE_NORMAL
- en: Training system design options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we create any detailed designs of our training system, some general
    questions will always apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Is there infrastructure available that is appropriate to the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the data and how will we feed it to the algorithm?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How am I testing the performance of the model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of infrastructure, this can be very dependent on the model and data
    you are using for training. If you are going to train a linear regression on data
    with three features and your dataset contains only 10,000 tabular records, you
    can likely run this on laptop-scale hardware without much thought. This is not
    a lot of data, and your model does not have a lot of free parameters. If you are
    training on a far larger dataset, such as one that contains 100 million tabular
    records, then you could benefit from parallelization across something such as
    a Spark cluster. If, however, you are training a 100-layer deep convolutional
    neural network on 1,000 images, then you are likely going to want to use a GPU.
    There are plenty of options, but the key is choosing the right thing for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the question of how we feed data to the algorithm, this can be non-trivial.
    Are we going to run a SQL query against a remotely hosted database? If so, how
    are we connecting to it? Does the machine we’re running the query on have enough
    RAM to store the data?
  prefs: []
  type: TYPE_NORMAL
- en: If not, do we need to consider using an algorithm that can learn incrementally?
    For classic algorithmic performance testing, we need to employ the well-known
    tricks of the ML trade and perform train/test/validation splits on our data. We
    also need to decide what cross-validation strategies we may want to employ. We
    then need to select our model performance metric of choice and calculate it appropriately.
    As ML engineers, however, we will also be interested in *other* measures of performance,
    such as training time, efficient use of memory, latency, and (dare I say it) cost.
    We will need to understand how we can measure and then optimize these as well.
  prefs: []
  type: TYPE_NORMAL
- en: So long as we bear these things in mind as we proceed, we will be in a good
    position. Now, onto the design.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction to this section, we have two fundamental
    pieces to consider: the training and output processes. There are two ways in which
    we can put these together for our solution. We will discuss this in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Train-run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Option 1* is to perform training and prediction in the same process, with
    training occurring in either batch or incremental mode. This is shown schematically
    in the following diagram. This pattern is called *train-run*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The train-run process ](img/B19525_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The train-run process.'
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is the simpler of the two but also the least desirable for real-world
    problems since it does not embody the *separation of concerns* principle we mentioned
    previously. This does not mean it is an invalid pattern, and it does have the
    advantage of often being simpler to implement. Here, we run our entire training
    process before making our predictions, with no real *break* in between. Given
    our previous discussions, we can automatically rule out this approach if we have
    to serve prediction in a very low-latency fashion; for example, through an event-driven
    or streaming solution (more on these later).
  prefs: []
  type: TYPE_NORMAL
- en: Where this approach *could* be completely valid, though (and I’ve seen this
    a few times in practice), is either in cases where the algorithms you are applying
    are actually very lightweight to train and you need to keep using very recent
    data, or where you are running a large batch process relatively infrequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this is a simple approach and does not apply to all cases, it does
    have distinct advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Since you are training as often as you predict, you are doing everything you
    can to protect against modern performance degradation, meaning that you are combatting
    *drift* (see later sections in this chapter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are significantly reducing the complexity of your solution. Although you
    are tightly coupling two components, which should generally be avoided, the training
    and prediction stages may be so simple to code that if you just stick them together,
    you will save a lot of development time. This is a non-trivial point because *development
    time costs money*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at the other case.
  prefs: []
  type: TYPE_NORMAL
- en: Train-persist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Option 2* is that training runs in batch, while prediction runs in whatever
    mode is deemed appropriate, with the prediction solution reading in the trained
    model from a store. We will call this design pattern *train-persist*. This is
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The train-persist process ](img/B19525_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The train-persist process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are going to train our model and then persist the model so that it can
    be picked up later by a prediction process, then we need to ensure a few things
    are in place:'
  prefs: []
  type: TYPE_NORMAL
- en: What are our model storage options?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a clear mechanism for accessing our model store (writing to and reading
    from)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often should we train versus how often will we predict?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, we will solve the first two questions by using MLflow, which we
    introduced in *Chapter 2*, *The Machine Learning Development Process*, but will
    revisit in later sections. There are also lots of other solutions available. The
    key point is that no matter what you use as a model store and *handover* point
    between your train and predict processes, it should be used in a way that is robust
    and accessible.
  prefs: []
  type: TYPE_NORMAL
- en: The third point is trickier. You could potentially just decide at the outset
    that you want to train on a schedule, and you stick to that. Or you could be more
    sophisticated and develop trigger criteria that must be met before training occurs.
    Again, this is a choice that you, as an ML engineer, need to make with your team.
    Later in this chapter, we will discuss mechanisms for scheduling your training
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore what you have to do if you want to trigger
    your training runs based on how your model’s performance could be degrading over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining required
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You wouldn’t expect that after finishing your education, you never read a paper
    or book or speak to anyone again, which would mean you wouldn’t be able to make
    informed decisions about what is happening in the world. So, you shouldn’t expect
    an ML model to be trained once and then be performant forever afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'This idea is intuitive, but it represents a formal problem for ML models known
    as **drift**. Drift is a term that covers a variety of reasons for your model’s
    performance dropping over time. It can be split into two main types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Concept drift**: This happens when there is a change in the fundamental relationship
    between the features of your data and the outcome you are trying to predict. Sometimes,
    this is also known as *covariate drift*. An example could be that at the time
    of training, you only have a subsample of data that seems to show a linear relationship
    between the features and your outcome. If it turns out that, after gathering a
    lot more data post-deployment, the relationship is non-linear, then concept drift
    has occurred. The mitigation against this is retraining with data that is more
    representative of the correct relationship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data drift**: This happens when there is a change in the statistical properties
    of the variables you are using as your features. For example, you could be using
    *age* as a feature in one of your models but at training time, you only have data
    for 16–24-year-olds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model gets deployed and your system starts ingesting data for a wider
    age demographic, then you have data drift.
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that drift is part of life as an ML engineer, so we will spend
    a good bit of time getting to know how to detect and mitigate against it. But
    why does it happen? As you would expect, there are a variety of reasons for drift
    that it is important to consider. Let us consider some examples. Say the mechanism
    you used for sampling your training data is not appropriate in some way; perhaps
    you have subsampled for a specific geographic region or demographic, but you want
    the model to be applied in more general circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: There may be seasonal effects in the problem domain in which we are operating,
    as can be expected in sales forecasting or weather prediction. Anomalies could
    be introduced by “black swan” or rare events, like geopolitical events or even
    the Covid-19 pandemic. The data-gathering process may at some point introduce
    errors, for example, if there is a bug in an upstream system or the process itself
    is not being followed or has changed. This last example can be particularly prevalent
    in processes where manual inputs of data are required. If a salesperson is to
    be trusted with correctly labeling the state of a sale in the **Customer Resource
    Management** (**CRM**) system, then salespeople with less training or experience
    may not label the data as accurately or in as timely a manner. Despite advances
    in so many areas of software development, this sort of data-gathering process
    is still very prevalent and so you must guard against this in your own machine
    learning system development. It can be mitigated slightly by trying to enforce
    more automation of data gathering or in providing guides to those entering data
    (think drop-down menus), but it is almost certain that a lot of data is still
    gathered in this way and will be for the foreseeable future.
  prefs: []
  type: TYPE_NORMAL
- en: 'That drift is an important aspect of your system to consider should be clear
    now, but dealing with it is actually a multi-step process. We first need to detect
    the drift. Detecting drift in your deployed models is a key part of MLOps and
    should be at the forefront of your mind as an ML engineer. We then need to diagnose
    the source of the drift; this will usually involve some sort of offline investigation
    by those responsible for monitoring. The tools and techniques we will mention
    will help you to define workflows that start to automate this, though, so that
    any repeatable tasks are taken care of when an issue is detected. Finally, we
    need to implement some action to remediate the effects of the drift: this will
    often be retraining the model using an updated or corrected dataset but may require
    a redevelopment or rewrite of key components of your model. In general, if you
    can build your training systems so that retraining is triggered based on an informed
    understanding of the drift in your models, you will save a lot of computational
    resources by only training when required.'
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss some of the ways we can detect drift in our models.
    This will help us start building up a smart retraining strategy in our solution.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting data drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have defined drift, and we know that detecting it is going to be
    important if we want to build sophisticated training systems. The next logical
    question is, *how do we do this?*
  prefs: []
  type: TYPE_NORMAL
- en: The definitions of drift we gave in the previous section were very qualitative;
    we can start to make these statements a bit more quantitative as we explore the
    calculations and concepts that can help us detect drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will rely heavily on the `alibi-detect` Python package
    from Seldon, which, at the time of writing, is not available from **Anaconda.org**
    but is available on PyPI. To acquire this package, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It is very easy to use the `alibi-detect` package. In the following example,
    we will work with the `wine` dataset from `sklearn`, which will be used elsewhere
    in this chapter. In this first example, we will split the data 50/50 and call
    one set the *reference* set and the other the *test* set. We will then use the
    Kolmogorov-Smirnov test to show that there hasn’t been data drift between these
    two datasets, as expected, and then artificially add some drift to show that it
    has been successfully detected:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the `TabularDrift` detector from the `alibi-detect` package,
    as well as the relevant packages for loading and splitting the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must get and split the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must initialize our drift detector using the reference data and by
    providing the `p-value` we want to be used by the statistical significance tests.
    If you want to make your drift detector trigger when smaller differences occur
    in the data distribution, you must select a larger `p_val`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now check for drift in the test dataset against the reference dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns `''Drift: No''`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, we have not detected drift here, as expected (see the following *IMPORTANT
    NOTE* for more on this).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although there was no drift in this case, we can easily simulate a scenario
    where the chemical apparatus being used for measuring the chemical properties
    experienced a calibration error, and all the values are recorded as 10% higher
    than their true values. In this case, if we run drift detection again on the same
    reference dataset, we will get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns `''Drift: Yes''`, showing that the drift has been successfully
    detected.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: This example is very artificial but is useful for illustrating the point. In
    a standard dataset like this, there won’t be data drift between 50% of the randomly
    sampled data and the other 50% of the data. This is why we have to artificially
    *shift* some of the points to show that the detector does indeed work. In real-world
    scenarios, data drift can occur naturally due to everything from updates to sensors
    being used for measurements; to changes in consumer behavior; all the way through
    to changes in database software or schemas. So, be on guard as many drift cases
    won’t be as easy to spot as in this case!
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how, with a few simple lines of Python, we can detect a change
    in our dataset, which means our ML model may start to degrade in performance if
    we do not retrain to take the new properties of the data into account. We can
    also use similar techniques to track when the performance metrics of our model,
    for example, accuracy or mean squared error, are drifting as well. In this case,
    we have to make sure we periodically calculate performance on new test or validation
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The first drift detection example was very simple and showed us how to detect
    a basic case of one-off data drift, specifically feature drift. We will now show
    an example of detecting **label drift**, which is basically the same but now we
    simply use the labels as the reference and comparison dataset. We will ignore
    the first few steps as they are identical, and resume from the point where we
    have reference and test datasets available.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the example for the drift in the features, we can configure the tabular
    drift detector, but now we will use the initial label as our baseline dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now check for drift in the test labels against the reference dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns `''Drift: No''`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, we have not detected drift here, as expected. Note that this method can
    also be used as a good sanity check that training and test data labels follow
    similar distributions and our sampling of test data is representative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As in the previous example, we can simulate some drift in the data, and then
    check that this is indeed detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now move on to a far more complex scenario, which is detecting concept
    drift.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting concept drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concept drift was described in this section, and there it was emphasized that
    this type of drift is really all about a change in the relationships between the
    variables in our model. This means by definition that it is far more likely that
    cases of this type will be complex and potentially quite hard to diagnose.
  prefs: []
  type: TYPE_NORMAL
- en: The most common way that you can catch concept drift is by monitoring the performance
    of your model through time. For example, if we are working with the `wine` classification
    problem again, we can look at metrics that tell us the model’s classification
    performance, plot these through time, and then build logic around the trends and
    outliers that we might see in these values.
  prefs: []
  type: TYPE_NORMAL
- en: The `alibi_detect` package, which we have already been using, has several useful
    methods for online drift detection that can be used to find concept drift as it
    happens and impacts model performance. Online here refers to the fact that the
    drift detection takes place at the level of a single data point, so this can happen
    even if data comes in completely sequentially in production. Several of these
    methods assume that either PyTorch or TensorFlow are available as backends since
    the methods use **Untrained AutoEncoders** (**UAEs**) as out-of-the-box pre-processing
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let us walk through an example of creating and using one of these
    online detectors, the Online Maximum Mean Discrepancy method. The following example
    assumes that in addition to the reference dataset, `X_ref`, we have also defined
    variables for the expected run time, `ert`, and the window size, `window_size`.
    The expected run time is a variable that states the average number of data points
    the detector should run before it raises false positive detection. The idea here
    is that you want the expected run time to be larger but as it gets larger the
    detector becomes more insensitive to actual drift, so a balance must be struck.
    The `window_size` is the size of the sliding window of data used in order to calculate
    the appropriate drift test statistic. A smaller `window_size` means you are tuning
    the detector to find sharp changes in the data or performance in a small time-frame,
    whereas longer window sizes will mean you are tuning to look for more subtle drift
    effects over longer periods of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we import the method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We then initialize the drift detector with some variable settings as discussed
    in the previous paragraph. We also include the number of bootstrapped simulations
    we want to apply in order for the method to calculate some thresholds for detecting
    the drift.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on your hardware settings for the deep learning library used and the
    size of the data, this may take some time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then simulate the drift detection in a production setting by taking
    the test data from the **Wine** dataset and feeding it in one feature vector at
    a time. If the feature vector for any given instance of data is given by `x`,
    we can then call the `predict` method of the drift detector and retrieve the `''is_drift''`
    value from the returned metadata like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing step 2 on all of the rows of the test data and plotting a vertical
    orange bar wherever we find drift detected gives the plot in *Figure 3.5*.![](img/B19525_03_05.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.5: The features of the test set from the wine dataset, which we have
    used to run some simulated drift detection.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, we can see in the plots of the simulated data that the accuracy
    of the data has changed over time. If we want to automate the detection of behaviors
    like this though, we will need to not simply plot this data but start to analyze
    it in a systematic way that we can fold into our model monitoring processes running
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The test data for the **Wine** dataset was used in the drift example
    only as an example. In production, this drift detection will be running on data
    that has never been seen before, but the principle is the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know drift is happening, we’ll move on to discuss how you can start
    to decide which limits to set on your drift detectors and then cover some processes
    and techniques for helping you to diagnose the type and source of your drift.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the techniques we have been describing in this section on drift are
    very much aligned with standard techniques from statistics and machine learning.
    You can get very far using these techniques almost “out of the box” to diagnose
    a series of different types of issues, but we have not discussed how we can bring
    these together into a coherent set of drift detection mechanisms. One of the most
    important things to consider before setting out to do this is setting the boundaries
    of acceptable behavior of the data and the model so that you know when your system
    should raise an alarm or take some action. We will call this “setting the limits”
    for your drift detection system.
  prefs: []
  type: TYPE_NORMAL
- en: So, where do you start? This is where things become a bit less technical and
    definitely more centered around operating within a business environment, but let’s
    cover some of the key points. First, it is important to understand what is important
    to alert on. Alerting on deviations in all of the metrics that you can think of
    might sound like a good idea, but it may just create a super noisy system where
    it is hard to find issues that are genuinely of concern. So, we have to be judicious
    in our selection of what we want to track and monitor. Next, we need to understand
    the timeliness required for detecting issues. This relates very strongly to the
    notion in software of **Service-Level Agreements** (**SLAs**), which write down
    the demanded and expected performance of the system in question. If your business
    is running real-time anomaly detection and predictive maintenance models on equipment
    used in hazardous conditions, it may be that the requirement for timeliness in
    alarms being raised and action being taken is quite high. However, if your machine
    learning system is performing a financial forecast once a week, then it could
    be that the timeliness constraints are a lot less severe. Finally, you need to
    set the limits. This means that you need to think carefully about the metrics
    you are tracking and think “What constitutes bad here?” or “What do we want to
    be notified of?” It may be that as part of your Discovery phase in the project,
    you know that the business is happy with a regression model that can have wide
    variability in the accuracy of its prediction, as long as it provides suitable
    confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: In another scenario, it could be that the classification model you are building
    must have a recall that fluctuates only within a relatively tight band; otherwise,
    it will jeopardize the efficacy of processes downstream.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing the drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have discussed in another section how there can be a variety of
    reasons for drift in our model, when it comes down to it, we must remember that
    machine learning models only act on features to create predictions. This then
    means that if we want to diagnose the source of the drift, we need to look no
    further than our features.
  prefs: []
  type: TYPE_NORMAL
- en: So, where do we start? The first thing we should consider is that any feature
    could realistically have drifted, but not all the features will be equally important
    in terms of the model. This means we need to understand how important the features
    are before prioritizing which ones need remedial action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance can be calculated in ways that are either model dependent
    or model independent. The model-dependent methods refer specifically to tree-based
    models, such as decision trees or random forests. In these cases, feature importance
    can often be extracted from the model for inspection, depending on the package
    used for developing the model. As an example, if we take a random forest classifier
    trained in Scikit-Learn, we can extract its feature importances using syntax like
    that given below. In this example, we retrieve the default feature importances
    for the random forest model, which are calculated using **Mean Decrease in Impurity**
    (**MDI**), equivalently known as “Gini importance,” and put them in an ordered
    pandas series for later analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Although this is extremely simple, it can sometimes give erroneous results for
    a couple of reasons. The feature importances here have been calculated using an
    impurity measure, which is a class of measures that can exhibit bias toward features
    with high cardinality (e.g., numerical) and are computed only on training set
    data, meaning they do not take into account any generalizability of the model
    onto unseen test data. This should always be kept in mind when using this sort
    of importance measure.
  prefs: []
  type: TYPE_NORMAL
- en: Another standard measure of feature importance, which is model agnostic and
    alleviates some of the issues for MDI or Gini importance, is the permutation importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This works by taking the feature we are interested in, shuffling it (i.e.,
    moving the values in the column of the feature matrix up, down, or via some other
    method of reorganization), and then recalculating the model accuracy or error.
    The change in the accuracy or error can then be used as a measure of the importance
    of this feature, as fewer importance features should mean less change in model
    performance upon shuffling. Below is an example of this method, again using Scikit-Learn,
    on the same model we used in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Finally, one other very popular method for determining feature importance is
    the calculation of **SHAP** (**SHapley Additive exPlanation**) values for the
    features. This uses ideas from game theory to consider how the features combine
    to inform the prediction. SHAP values are calculated by training the model on
    all permutations of features that include or exclude the considered feature and
    then calculating the marginal contribution to the predicted value of that feature.
    This is different from permutation importance because we are no longer simply
    permuting the feature values; we are now actually running through a series of
    different potential sets of features including or excluding the feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start calculating SHAP values on your models by installing the `shap`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can execute syntax like the following, using the same random forest
    model in the previous examples to define a *shap explainer* object and calculate
    the SHAP values for the features in the test dataset. We assume here the `X_test`
    is a pandas DataFrame with the feature names as the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the calculation of the SHAP values can take some time due to running
    all the permutations. The `shap_values` themselves are not feature importances,
    but contain the SHAP values calculated for each feature in all the different feature
    combination experiments. In order to determine feature importances, you should
    take the average of the absolute magnitudes of the `shap_values` for each feature.
    This is done for you and the result plotted if you use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We have now covered three different ways to calculate feature importances for
    your models, two of them completely model agnostic. Feature importance is extremely
    helpful to help you get to the root of drift very quickly. If you see the performance
    of your model drifting or breaching a threshold you have set, you can use the
    feature importances to focus your diagnostic efforts on where the most important
    features are and ignore drift in features that are not as critical.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered a useful way to help dig into the drift, we will now
    discuss how you can go about remediating it once you spot the feature or features
    that seem to be causing the most trouble.
  prefs: []
  type: TYPE_NORMAL
- en: Remediating the drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few ways we can take action against drift in order to maintain
    the performance of our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Remove features and retrain**: If certain features are drifting or exhibiting
    degradation of some other kind, we can try removing them and retraining the model.
    This can become time consuming as our data scientists potentially need to re-run
    some analysis and testing to ensure that this approach still makes sense from
    a modeling point of view. We also have to take into account things like the importance
    of the features we are removing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain with more data**: If we are seeing concept drift, we may simply be
    noticing that the model has become stale with respect to the distributions and
    the relationships between these distributions in the data. It could be that retraining
    the model and including more recent data may create an uptick in performance.
    There is also the option of retraining the model on some selected portion of more
    recent data. This can be especially useful if you are able to diagnose some dramatic
    event or shift in the data, for example, the introduction of Covid-19 lockdowns.
    This approach can be hard to automate though, so sometimes it is also an option
    to introduce a time-windowed approach, where you train on some preselected amount
    of data up to the present time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Roll back the model**: We can replace the current model with a previous version
    or even a go-to baseline model. This can be a very good approach if your baseline
    model is more simple but also more predictable in terms of performance, because
    it applies some simple business logic, for example. The ability to roll back to
    previous versions of models requires that you have built up a good set of automated
    processes around your model registry. This is very reminiscent of rollbacks in
    general software engineering, a key component of building robust systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewrite or debug the solution**: It may be the case that the drift we are
    dealing with is so substantial that the model as it stands cannot cope with any
    of the above approaches. The idea of rewriting the model may seem drastic but
    this can be more common than you think. As an example, consider that initially
    you deploy a well-tuned LightGBM model that performs binary classification on
    a set of five features daily. After running the solution for months, it could
    be that after detecting drift in the model performance several times, you decide
    that it is better to perform an investigation to see if there is a better approach.
    This can be especially helpful in this scenario as now you know more about the
    data you will see in production. You may then discover that actually, a random
    forest classifier is not as performant on the same production data scenarios on
    average but that *it is* more stable, behaving more consistently and triggering
    drift alarms less often. You may then decide that actually, it is better for the
    business to deploy this different model into the same system as it will reduce
    operational overheads from dealing with the drift alarms and it will be something
    the business can trust more. It is important to note that if you need to write
    a new pipeline or model, it is often important to roll back to a previous model
    while the team does this work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fix the data source**: Sometimes, the most challenging issues do not actually
    have anything to do with the underlying model but are more to do with changes
    in how data is collected and fed downstream to your system. There are so many
    business scenarios where the collection of data, the transformation of data, or
    the characteristics of data may be changed due to the introduction of new processes,
    updates to systems, or even due to changes in the personnel responsible for entering
    some source data. A great example from the author’s own experience is when it
    comes to **customer resource management** (**CRM**) systems, the quality of the
    data being input from the sales team can depend on so many factors that it can
    be reasonable to expect slow or sudden changes in data quality, consistency, and
    timeliness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the right answer may not actually be an engineering one, but a
    process one, working with the appropriate teams and stakeholders to ensure that
    data quality is maintained, and standard processes are followed. This will benefit
    customers and the business, but it can still be a hard sell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can start to build this into solutions that will automatically trigger
    our ML model being retrained, as shown in *Figure 3.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – An example of drift detection and the training system process
    ](img/B19525_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: An example of drift detection and the training system process.'
  prefs: []
  type: TYPE_NORMAL
- en: Other tools for monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The examples in this chapter have mainly used the alibi-detect package, but
    we are now in somewhat of a golden age of open source **MLOps** tools. There are
    several different packages and solutions available that you can start using to
    build your monitoring solutions without spending a penny.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will quickly cover some of these tools and show some basic
    points on their syntax, so that if you want to develop monitoring pipelines, then
    you can just get started right away and know where is best to use these different
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will cover **Evidently AI** ([https://www.evidentlyai.com/](https://www.evidentlyai.com/)),
    which is a very easy-to-use Python package that allows users to not only monitor
    their models but also create customizable dashboards in a few lines of syntax.
    Below is an adaptation of the getting started guide from the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install Evidently:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `Report` functionality. The `Report` is an object that collects
    calculations across several metrics to allow for visualizations or outputs as
    a JSON object. We will show this latter behavior later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import what is known as a metric preset, in this case for data drift.
    We can think of this as a templated report object that we can later customize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, assuming you have the data to hand, you can then run the data drift report.
    Let’s assume you have the **Wine** dataset from the previous examples to hand.
    If we split the wine data 50/50 using `scikit-learn`''s `train_test_split()` method,
    we will have two datasets, which we again use to simulate the reference dataset,
    `X_ref`, and the current dataset, `X_curr`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evidently then provides some really nice functionality for visualizing the
    results in the report. You can export or view these using a few different methods.
    You can export the report to JSON or HTML objects for consumption or to review
    downstream or in other applications. *Figures 3.7* and *3.8* show snippets of
    the results when you create these outputs with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B19525_03_07.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.7: JSON output from the Evidently report on the 50/50 split Wine feature
    set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: HTML version of the drift report generated by Evidently on the
    50/50 split Wine feature set.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the nice things about the rendered HTML report is that you can dynamically
    drill down into some useful information. As an example, *Figure 3.9* shows that
    if you click down into any of the features, you are provided with a plot of data
    drift through time, and *Figure 3.10* shows that you can also get a plot of the
    distributions of the features in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: The automatically generated data drift plot when you drill down
    in the Evidently report for the Wine features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: The automatically generated histogram showing the distribution
    of the feature when you drill down in the Evidently report for the Wine feature
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: This has only scratched the surface of what you can do with Evidently. There
    is a lot of functionality available for generating your own model test suites
    and monitoring functionality as well as visualizing it all nicely like we have
    seen.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the concepts of model and data drift and how to detect
    them, we can now move on to a discussion about how we can take a lot of the concepts
    we covered earlier in the chapter and automate them.
  prefs: []
  type: TYPE_NORMAL
- en: The next couple of sections will provide deep dives into different aspects of
    the training process and, in particular, how this process can be automated using
    a variety of tools.
  prefs: []
  type: TYPE_NORMAL
- en: Automating training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training process is an integral part of the model factory and one of the
    main differentiators between ML engineering and traditional software engineering.
    The next few sections will discuss in detail how we can start to use some excellent
    open source tooling to streamline, optimize, and, in some cases, fully automate
    elements of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchies of automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main reasons that ML is now a common part of software development,
    as well as a major business and academic activity, is because of the plethora
    of tools available. All of the packages and libraries containing working and optimized
    implementations of sophisticated algorithms have allowed people to build on top
    of these, rather than have to reimplement the basics every time there is a problem
    to solve.
  prefs: []
  type: TYPE_NORMAL
- en: This is a powerful expression of the idea of **abstraction** in software development,
    where lower-level units can be leveraged and engaged with at higher levels of
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: This idea can be extended even further to the entire enterprise of training
    itself. At the lowest level of implementation (but still a very high level in
    the sense of the underlying algorithms), we can provide details about how we want
    the training process to go. We can manually define the exact set of hyperparameters
    (see the next section on *Optimizing hyperparameters*) to use in the training
    run in our code. I call this **hand cranking**. We can then move one level of
    abstraction up and supply ranges and bounds for our hyperparameters to tools designed
    to efficiently sample and test our model’s performance for each of these; for
    instance, *automated hyperparameter tuning*. Finally, there is one higher level
    of abstraction that has created a lot of media excitement over the past few years,
    where we optimize over which algorithm to run. This is known as **automated ML**
    or **AutoML**.
  prefs: []
  type: TYPE_NORMAL
- en: There can be a lot of hype surrounding AutoML, with some people proclaiming
    the eventual automation of all ML development job roles. In my opinion, this is
    just not realistic, as selecting your model and hyperparameters is only one aspect
    of a hugely complex engineering challenge (hence this being a book and not a leaflet!).
    AutoML is, however, a very powerful tool that should be added to your arsenal
    of capabilities when you go into your next ML project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize all of this quite handily as a *hierarchy of automation*;
    basically, how much control do you, as the ML engineer, want in the training process?
    I once heard this described in terms of gear control in a car (credit: *Databricks
    at Spark AI 2019*). Hand cranking is the equivalent of driving a manual car with
    full control over the gears: there’s more to think about, but it can be very efficient
    if you know what you’re doing. One level up, you have automatic cars: there’s
    less to worry about so that you can focus more on getting to your destination,
    traffic, and other challenges. This is a good option for a lot of people but still
    requires you to have sufficient knowledge, skills, and understanding. Finally,
    we have self-driving cars: sit back, relax, and don’t even worry about how to
    get where you’re going. You can focus on what you are going to do once you get
    there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This *hierarchy of automation* is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – The hierarchy of automation of ML model optimization, with AutoML
    as the most automated possibility ](img/B19525_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: The hierarchy of automation of ML model optimization, with AutoML
    as the most automated possibility.'
  prefs: []
  type: TYPE_NORMAL
- en: That, in a nutshell, is how the different levels of training abstraction link
    together.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will discuss how to get started building out implementations
    of hyperparameter optimization and AutoML. We will not cover “hand cranking” as
    that is self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you fit some sort of mathematical function to data, some values are tuned
    during the fitting or training procedure: these are called **parameters**. For
    ML, there is a further level of abstraction where we have to define the values
    that tell the algorithms we are employing *how they should update the parameters*.
    These values are called **hyperparameters**, and their selection is one of the
    important *dark arts* of training ML algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tables list some hyperparameters that are used for common ML
    algorithms to show you the different forms they may take. These lists are not
    exhaustive but are there to highlight that hyperparameter optimization is not
    a trivial exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Hyperparameters** | **What This Controls** |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Trees andRandom Forests |'
  prefs: []
  type: TYPE_TB
- en: Tree depth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Min/max leaves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: How many levels are in your trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much branching can occur at each level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Support Vector Machines |'
  prefs: []
  type: TYPE_TB
- en: C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Penalty for misclassification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The radius of influence of the training points for **Radial Basis Function**
    (**RBF**) kernels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Neural Networks(numerous architectures) |'
  prefs: []
  type: TYPE_TB
- en: Learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of hidden layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Update step sizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How deep your network is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The firing conditions of your neurons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Logistic Regression |'
  prefs: []
  type: TYPE_TB
- en: Solver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization prefactor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: How to minimize the loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to prevent overfitting/make the problem well behaved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strength of the regularization type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3.1: Some hyperparameters and what they control for some supervised algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further examples can be seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Hyperparameters** | **What This Controls** |'
  prefs: []
  type: TYPE_TB
- en: '| K-Nearest Neighbors |'
  prefs: []
  type: TYPE_TB
- en: K
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The number of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define the distance between points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DBSCAN |'
  prefs: []
  type: TYPE_TB
- en: Epsilon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum number of samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The max distance to be considered neighbors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many neighbors are required to be considered core.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define the distance between points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3.2: Some hyperparameters and what they control for some unsupervised
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: All of these hyperparameters have their own specific set of values they can
    take. This range of hyperparameter values for the different potential algorithms
    you want to apply to your ML solution means that there are a lot of ways to define
    a *working* model (meaning one that doesn’t break the implementation you are using),
    but how do you find the *optimal* model?
  prefs: []
  type: TYPE_NORMAL
- en: This is where hyperparameter search comes in. The concept is that for a finite
    number of hyperparameter value combinations, we want to find the set that gives
    the best model performance. This is another optimization problem that’s similar
    to that of training in the first place!
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss two very popular hyperparameter optimization
    libraries and show you how to implement them in a few lines of Python.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand which algorithms are being used for optimization
    in these hyperparameter libraries, as you may want to use a couple of different
    implementations from each to compare different approaches and assess performance.
    If you didn’t look at how they were working under the hood, you could easily make
    unfair comparisons – or worse, you could be comparing almost the same thing without
    knowing it! If you have some deeper knowledge of how these solutions work, you
    will also be able to make better judgment calls as to when they will be beneficial
    and when they will be overkill. Aim to have a working knowledge of a few of these
    algorithms and approaches, since this will help you design more holistic training
    systems with algorithm-tuning approaches that complement one another.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperopt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hyperopt** is an open source Python package that bills itself as being *for
    serial and parallel optimization over awkward search spaces, which may include
    real-valued, discrete, and conditional dimensions*. Check out the following link
    for more information: [https://github.com/Hyperopt/Hyperopt](https://github.com/Hyperopt/Hyperopt).
    At the time of writing, version 0.2.5 comes packaged with three algorithms for
    performing optimization over user-provided search spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random search**: This algorithm essentially selects random numbers within
    your provided ranges of parameter values and tries them. It then evaluates which
    sets of numbers provide the best performance according to your chosen objective
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree of Parzen Estimators** (**TPE**): This is a Bayesian optimization approach
    that models distributions of hyperparameters below and above a threshold for the
    objective function (roughly *good* and *bad* scorers), and then aims to draw more
    values from the *good* hyperparameter distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive TPE**: This is a modified version of TPE that allows for some optimization
    of the search, as well as the ability to create an ML model to help guide the
    optimization process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Hyperopt repository and documentation contain several nice and detailed
    worked examples. We will not go through these here. Instead, we will learn how
    to use this for a simple classification model, such as the one we defined in *Chapter
    1*, *Introduction to ML Engineering*. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Hyperopt, we must define the hyperparameters that we want to optimize across.
    For example, for a typical logistic regression problem, we could define the space
    of hyperparameters to cover, whether we want to reuse parameters that were learned
    from the previous model runs each time (`warm_start`), whether we want the model
    to include a bias in the decision function (`fit_intercept`), the tolerance set
    for deciding when to stop the optimization (`tol`), the regularization parameter
    (`C`), which `solver` we want to try, and the maximum number of iterations, `max_iter`,
    in any training run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we have to define an objective function to optimize. In the case of our
    classification algorithm, we can simply define the `loss` function we want to
    minimize as 1 minus the `f1-score`. Note that Hyperopt allows your objective function
    to supply run statistics and metadata via your return statement if you are using
    the `fmin` functionality. The only requirement if you do this is that you return
    a value labeled `loss` and a valid status value from the list of `Hyperopt.STATUS_STRING`
    (`ok` by default and `fail` if there is an issue in the calculation that you want
    to call out as a failure):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must optimize using the `fmin` method with the **TPE** algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The content of `best` is a dictionary containing all the best hyperparameters
    in the search space you defined. So, in this case, we have the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can then use these hyperparameters to define your model for training on
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Optuna
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Optuna** is a software package that has an extensive series of capabilities
    based on some core design principles, such as its **define-by-run** API and modular
    architecture. *Define-by-run* here refers to the fact that, when using Optuna,
    the user does not have to define the full set of parameters to test, which is
    *define-and-run*. Instead, they can provide some initial values and ask Optuna
    to suggest its own set of experiments to run. This saves the user time and reduces
    the code footprint (two big pluses for me!).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optuna contains four basic search algorithms: **grid search**, **random search**,
    **TPE**, and the **Covariance Matrix Adaptation Evolution Strategy** (**CMA-ES**)
    algorithm. We covered the first three previously, but CMA-ES is an important addition
    to the mix. As its name suggests, this is based on an evolutionary algorithm and
    draws samples of hyperparameters from a multivariate Gaussian distribution. Then,
    it uses the rankings of the evaluated scores for the given objective function
    to dynamically update the parameters of the Gaussian distribution (the covariance
    matrix being one set of these) to help find an optimum over the search space quickly
    and robustly.'
  prefs: []
  type: TYPE_NORMAL
- en: The key thing that makes Optuna’s optimization process different from Hyperopt,
    however, is in its application of **pruning** or **automated early stopping**.
    During optimization, if Optuna detects evidence that a trial of a set of hyperparameters
    will not lead to a better overall trained algorithm, it terminates that trial.
    The developers of the package suggest that this leads to overall efficiency gains
    in the hyperparameter optimization process by reducing unnecessary computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’re looking at the same example we looked at previously, but we are
    now using Optuna instead of Hyperopt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, when using Optuna, we can work using an object known as `Study`, which
    provides us with a convenient way to fold our search space into our `objective`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must set up the data in the same way as we did in the Hyperopt example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can define this `Study` object that we mentioned and tell it how we
    wish to optimize the value that’s returned by our `objective` function, complete
    with guidance on how many trials to run in the `study`. Here, we will use the
    TPE sampling algorithm again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can access the best parameters via the `study.best_trial.params` variable,
    which gives us the following values for the best case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, Optuna is also very simple to use and very powerful. Now, let’s
    look at the final level of the hierarchy of automation: AutoML.'
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that these values are different from the ones returned by Hyperopt.
    This is because we have only run 16 trials in each case, so we are not effectively
    subsampling the space. If you run either of the Hyperopt or Optuna samples a few
    times in a row, you can get quite different results for the same reason. The example
    given here is just to show the syntax, but if you are keen, you can set the number
    of iterations to be very high (or create smaller spaces to sample), and the results
    of the two approaches should roughly converge.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final level of our hierarchy is the one where we, as the engineer, have
    the least direct control over the training process, but where we also potentially
    get a good answer for very little effort!
  prefs: []
  type: TYPE_NORMAL
- en: The development time that’s required to search through many hyperparameters
    and algorithms for your problem can be large, even when you code up reasonable-looking
    search parameters and loops.
  prefs: []
  type: TYPE_NORMAL
- en: Given this, the past few years have seen the deployment of several **AutoML**
    libraries and tools in a variety of languages and software ecosystems. The hype
    surrounding these techniques has meant they have had a lot of airtime, which has
    led to several data scientists questioning when their jobs will be automated away.
    As we mentioned previously in this chapter, in my opinion, declaring the death
    of data science is extremely premature and also dangerous from an organizational
    and business performance standpoint. These tools have been given such a pseudo-mythical
    status that many companies could believe that simply using them a few times will
    solve all their data science and ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: They are wrong, but they are also right.
  prefs: []
  type: TYPE_NORMAL
- en: These tools and techniques *are* very powerful and *can* help make some things
    better, but they are not a magical *plug-and-play* panacea. Let’s explore these
    tools and start to think about how to incorporate them into our ML engineering
    workflow and solutions.
  prefs: []
  type: TYPE_NORMAL
- en: auto-sklearn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of our favorite libraries, good old Scikit-Learn, was always going to be
    one of the first targets for building a popular AutoML library. One of the very
    powerful features of auto-sklearn is that its API has been designed so that the
    main objects that optimize and section models and hyperparameters can be swapped
    seamlessly into your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, an example will show this more clearly. In the following example,
    we will assume that the `Wine` dataset (a favorite for this chapter) has already
    been retrieved and split into train and test samples in line with other examples,
    such as the one in the *Detecting drift* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, since this is a classification problem, the main thing we need to get
    from `auto-sklearn` is the `autosklearn.classification` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We must then define our `auto-sklearn` object. This provides several parameters
    that help us define how the model and hyperparameter tuning process will proceed.
    In this example, we will provide an upper time limit in seconds for running the
    overall optimization and an upper time limit in seconds for any single call to
    the ML model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, just like we would fit a normal `sklearn` classifier, we can fit the
    `auto-sklearn` object. As we mentioned previously, the `auto-sklearn` API has
    been designed so that this looks familiar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve fit the object, we can start to dissect what has been achieved
    by the object during its optimization run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we can see which models were tried and which were kept in the object
    as part of the final ensemble:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then get a readout of the main statistics from the run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can predict some text features, as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can check how well we did by using our favorite metric calculators
    – in this case, the `sklearn metrics` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, it is very straightforward to start using this powerful library,
    especially if you are already comfortable working with `sklearn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let’s discuss how we extend this concept to neural networks, which have
    an extra layer of complexity due to their different potential model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: AutoKeras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A particular area where AutoML has been a big hit is neural networks. This is
    because for a neural network, the question of *what is the best model?* is a very
    complicated one. For our typical classifiers, we can usually think of a relatively
    short, finite list of algorithms to try. For a neural network, we don’t have this
    finite list. Instead, we have an essentially infinite set of possible neural network
    *architectures*; for instance, for organizing the neurons into layers and the
    connections between them. Searching for the optimal neural network architecture
    is a problem in which powerful optimization can make your life, as an ML engineer
    or data scientist, a whole lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, we are going to explore an AutoML solution built on top of
    the very popular neural network API library known as Keras. Unbelievably, the
    name of this package is – you guessed it – AutoKeras!
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will, once again, assume that the `Wine` dataset has been
    loaded so that we can focus on the details of the implementation. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the `autokeras` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it’s time for the fun and, for `autokeras`, the extremely simple bit!
    Since our data is structured (tabular with a defined schema), we can use the `StructuredDataClassifier`
    object, which wraps the underlying mechanisms for automated neural network architecture
    and hyperparameter search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, all we have to do is fit this classifier object, noticing its similarity
    to the `sklearn` API. Remember that we assume that the training and test data
    exist in `pandas DataFrames`, as in the other examples in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The training objects in AutoKeras have a convenient evaluation method wrapped
    within them. Let’s use this to see how accurate our solution was:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we have successfully performed a neural network architecture and
    hyperparameter search in a few lines of Python. As always, read the solution documentation
    for more information on the parameters you can provide to the different methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve covered how to create performant models, in the next section,
    we will learn how to persist these models so that they can be used in other programs.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting your models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced some of the basics of model version control
    using MLflow. In particular, we discussed how to log metrics for your ML experiments
    using the MLflow Tracking API. We are now going to build on this knowledge and
    consider the touchpoints our training systems should have with model control systems
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s recap what we’re trying to do with the training system. We want
    to automate (as far as possible) a lot of the work that was done by the data scientists
    in finding the first working model, so that we can continually update and create
    new model versions that still solve the problem in the future. We would also like
    to have a simple mechanism that allows the results of the training process to
    be shared with the part of the solution that will carry out the prediction when
    in production. We can think of our model version control system as a bridge between
    the different stages of the ML development process we discussed in *Chapter 2*,
    *The Machine Learning Development Process*. In particular, we can see that the
    ability to track experiment results allows us to keep the results of the **Play**
    phase and build on these during the **Develop** phase. We can also track more
    experiments, test runs, and hyperparameter optimization results in the same place
    during the **Develop** phase. Then, we can start to tag the performant models
    as ones that are good candidates for deployment, thus bridging the gap between
    the **Develop** and **Deploy** development phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we focus on MLflow for now (though plenty of other solutions are available
    that fulfill the need for a model version control system), then MLflow’s Tracking
    and Model Registry functionalities nicely slot into these bridging roles. This
    is represented schematically in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – How the MLflow Tracking and Model Registry functionalities can
    help us progress through the different stages of the ML development process ](img/B19525_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: How the MLflow Tracking and Model Registry functionalities can
    help us progress through the different stages of the ML development process.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 2*, *The Machine Learning Development Process*, we only explored
    the basics of the MLflow Tracking API for storing experimental model run metadata.
    Now, we will briefly dive into how to store production-ready models in a very
    organized way so that you can start to perform model staging. This is the process
    whereby models can be progressed through stages of readiness, and you can swap
    models in and out of production if you wish to. This is an extremely important
    part of any training system that supplies models and will run as part of a deployed
    solution, which is what this book is all about!
  prefs: []
  type: TYPE_NORMAL
- en: As alluded to previously, the functionality that we need in MLflow is called
    **Model Registry**, which enables you to manage the staging of models across your
    development life cycle. Here, we will walk through examples of how to take a logged
    model and push it to the registry, how to update information such as the model
    version number in the registry, and then how to progress your model through different
    life cycle stages. We will finish this section by learning how to retrieve a given
    model from the registry in other programs – a key point if we are to share our
    models between separate training and prediction services.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the Python code for interacting with Model Registry, we
    have one important piece of setup to perform. The registry only works if a database
    is being used to store the model metadata and parameters. This is different from
    the basic Tracking API, which works with just a file backend store. This means
    that before pushing models to Model Registry, we have to fire up an MLflow server
    with a database backend. You can do this with a **SQLite** database running locally
    by executing the following command in your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will have to run this before the code snippets in the rest of this section
    (this command is stored in a short Bash script in this book’s GitHub repository,
    under [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python/blob/main/Chapter03/mlflow-advanced/start-mlflow-server.sh](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python/blob/main/Chapter03/mlflow-advanced/start-mlflow-server.sh)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the backend database is up and running, we can use it as part of our
    model workflow. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by logging some metrics and parameters for one of the models we
    trained earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the same code block, we can now log the model to Model Registry, providing
    a name for the model to reference later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s assume we are running a prediction service and we want to retrieve
    the model and predict using it. Here, we have to write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By default, newly registered models in Model Registry are assigned the `''Staging''`
    stage value. Therefore, if we want to retrieve the model based on knowing the
    stage but not the model version, we could execute the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on all of our discussions in this chapter, the result of our training
    system must be able to produce a model we are happy to deploy to production. The
    following piece of code promotes the model to a different stage, called `"Production"`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These are the most important ways to interact with Model Registry and we have
    covered the basics of how to register, update, promote, and retrieve your models
    in your training (and prediction) systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will learn how to chain our main training steps together into single
    units called **pipelines**. We will cover some of the standard ways of doing this
    inside single scripts, which will allow us to build our first training pipelines.
    In *Chapter 5*, *Deployment Patterns and Tools*, we will cover tools for building
    more generic software pipelines for your ML solution (of which your training pipeline
    may be a single component).
  prefs: []
  type: TYPE_NORMAL
- en: Building the model factory with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of a software pipeline is intuitive enough. If you have a series
    of steps chained together in your code, so that the next step consumes or uses
    the output of the previous step or steps, then you have a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, when we refer to a pipeline, we will be specifically dealing
    with steps that contain processing or calculations that are appropriate to ML.
    For example, the following diagram shows how this concept may apply to some of
    the steps the marketing classifier mentioned in *Chapter 1*, *Introduction to
    ML Engineering*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – The main stages of any training pipeline and how this maps
    to a specific case from Chapter 1, Introduction to ML Engineering ](img/B19525_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: The main stages of any training pipeline and how this maps to
    a specific case from Chapter 1, Introduction to ML Engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss some of the standard tools for building up your ML pipelines in
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our old friend **Scikit-Learn** comes packaged with some nice pipelining functionality.
    The API is extremely easy to use, as you would expect from **Scikit-Learn**, but
    has some concepts we should understand before proceeding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The pipeline object**: This is the object that will bring together all the
    steps we require, in particular, `sklearn` demands that instantiated pipeline
    objects are composed of sequences of transformers and estimators, with all intermediate
    objects having the `.fit()` and `.transform()` methods and the last step being
    an estimator with at least the `.fit()` method. We will explain these terms in
    the next two points. The reason for this condition is that the `pipeline` object
    will inherit the methods from the last item in the sequence provided, so we must
    make sure to have `.fit()` present in the last object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimators**: The estimator class is the base object in `scikit-learn` and
    anything in the package that can be fit on data and then predict on data, therefore
    the `.fit()` and `.predict()` methods, is a subclass of the estimator class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers**: In **Scikit-Learn**, transformers are any estimators that
    have a `.transform()` or `.fit_transform()` method and, as you can guess, are
    mainly focused on transforming datasets from one form to another rather than performing
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of the `pipeline` object really helps facilitate the simplification
    of your code, as rather than writing several different fitting, transforming,
    and predicting steps as their own function calls with datasets and then managing
    the flow of that data, you can simply compose them all in one object that manages
    this for you and uses the same simple API.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are new transformers and features being added to Scikit-Learn all the
    time, which means that it become possible to build more and more useful pipelines.
    For example, at the time of writing, Scikit-Learn versions greater than 0.20 also
    contain the `ColumnTransformer` object, which allows you to build pipelines that
    perform different actions on specific columns. This is exactly what we want to
    do with the logistic regression marketing model example we were discussing previously,
    where we want to standardize our numerical values and one-hot encode our categorical
    variables. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create this pipeline, you need to import the `ColumnTransformer` and `Pipeline`
    objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To show you how to chain steps inside the transformers that make up the pipeline,
    we will add some imputation later. For this, we need to import the `SimpleImputer`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must define the numerical transformer sub-pipeline, which contains
    the two steps for imputation and scaling. We must also define the names of the
    numerical columns this will apply to so that we can use them later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must perform similar steps for the categorical variables, but here,
    we only have one transformation step to define for the `one-hot` encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We must bring all of these preprocessing steps together into a single object,
    called `preprocessor`, using the `ColumnTransformer` object. This will apply our
    `transformers` to the appropriate columns of our DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we want to add the ML model step at the end of the previous steps
    and finalize the pipeline. We will call this `clf_pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is our first ML training pipeline. The beauty of the `scikit-learn` API
    is that the `clf_pipeline` object can now be called as if it were a standard algorithm
    from the rest of the library. So, this means we can write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will run the `fit` methods of all of the pipeline steps in turn.
  prefs: []
  type: TYPE_NORMAL
- en: The previous example was relatively basic, but there are a few ways you can
    make this sort of pipeline more sophisticated if your use case requires it. One
    of the simplest and most extensible is the ability in Scikit-Learn to create custom
    transformer objects that inherit from the base classes. You can do this for a
    class transformer by inheriting from the `BaseEstimator` and `TransformerMixIn`
    classes and defining your own transformation logic. As a simple example, let’s
    build a transformer that takes in the specified columns and adds a float. This
    is just a simple schematic to show you how it’s done; I can’t imagine that adding
    a single float to your columns will be that helpful in most cases!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'You could then add this transformer to your `pipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This example of adding a number is actually not the best use case for using
    this class-based transformer definition, as this operation is stateless. Since
    there is no training or complex manipulation of the values being fed in that requires
    the class to retain and update its state, we have actually just wrapped a function.
    The second way of adding your own custom steps takes advantage of this and uses
    the `FunctionTransformer` class to wrap any function you provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: By building on these examples, you can start to create complex pipelines that
    can perform any feature engineering task you want.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, we can clearly see that the ability to abstract the
    steps that are performing feature engineering and training your model into a single
    object is very powerful, as it means you can reuse this object in various places
    and build even more complex workflows with it without constantly recoding the
    details of the implementation. Abstraction is a good thing!
  prefs: []
  type: TYPE_NORMAL
- en: We will now turn to another way of writing pipelines, using Spark ML.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is another toolset we have been using throughout this book that will
    be particularly important when we discuss scaling up our solutions: Apache Spark
    and its ML ecosystem. We will see that building a similar pipeline with Spark
    ML requires a slightly different set of syntax, but the key concepts look very
    similar to the Scikit-Learn case.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few important points to mention about PySpark pipelines. Firstly,
    in line with good programming practices in Scala, which Spark is written in, objects
    are treated as **immutable**, so transformations do not occur *in place*. Instead,
    new objects are created. This means that the output of any transformation will
    require new columns to be created in your original DataFrame (or indeed new columns
    in a new DataFrame).
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the Spark ML estimators (that is, the ML algorithms) all require the
    features to be assembled into one tuple-like object in a single column. This contrasts
    with Scikit-Learn, where you can keep all the features in their columns in your
    data object. This means that you need to become comfortable with the use of **assemblers**,
    which are utilities for pulling disparate feature columns together, especially
    when you are working with mixed categorical and numerical features that must be
    transformed in different ways before being invested by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, Spark has many functions that use **lazy evaluation**, meaning that
    they are only executed when they’re triggered by specific actions. This means
    that you can build up your entire ML pipeline and not have to transform any data.
    The reason for lazy evaluation is that the computational steps in Spark are stored
    in a **Directed Acyclic Graph** (**DAG**) so that the execution plan can be optimized
    before you perform the computational steps, making Spark very efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Finally – and this is a small point – it is commonplace to write PySpark variables
    using *camel case* rather than the common *snake case*, which is often used for
    Python variables (for instance, **variableName** versus `variable_name`). This
    is done to keep the code in line with the PySpark functions that inherit this
    convention from the underlying **Scala** code behind Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark ML pipelines API utilizes concepts of Transformer and Estimator in
    a similar way to how the Scikit-Learn pipeline API did, with some important differences.
    The first difference is that Transformers in Spark ML implement `.transform()`
    but not the `.fit_transform()` method. Secondly, the Transformer and Estimator
    objects in Spark ML are stateless, so once you have trained them they do not change
    and they only contain model metadata. They don’t store anything about the original
    input data. One similarity is that pipelines are treated as Estimators in Spark
    ML as well.
  prefs: []
  type: TYPE_NORMAL
- en: We will now build a basic example to show how to build a training pipeline using
    the Spark ML API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must one-hot encode the categorical features for the previous example
    using the following syntax:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the numerical columns, we must perform imputation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must perform standardization. Here, we need to be a bit clever about
    how we apply `StandardScaler` as it only applies to one column at a time. Therefore,
    we need to create a scaler for each numerical feature after pulling our numerically
    imputed features into a single feature vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we have to assemble the numerical and categorical transformed features
    into one feature column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can define our model step, add this to the `pipeline`, and then
    train on and transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then persist the model pipeline as you would any `Spark` object, for
    example, by using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Where `path` is the path to your target destination. You would then read this
    pipeline into memory by using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: And that is how we can build a training pipeline in PySpark using **Spark ML**.
    This example shows you enough to get started with the API and build out your own,
    more sophisticated pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: We will now conclude this chapter with a brief summary of everything we have
    covered.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the important topic of how to build up our
    solutions for training and staging the ML models that we want to run in production.
    We split the components of such a solution into pieces that tackled training the
    models, the persistence of the models, serving the models, and triggering retraining
    for the models. I termed this the “Model Factory.”
  prefs: []
  type: TYPE_NORMAL
- en: We got into the more technical details of some important concepts with a deep
    dive into what training an ML model really means, which we framed as learning
    about how ML models learn. Some time was then spent on the key concepts of feature
    engineering, or how you transform your data into something that a ML model can
    understand during this process. This was followed by sections on how to think
    about the different modes your training system can run in, which I termed “train-persist”
    and “train-run.”
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed how you can perform drift detection on your models and the
    data they are consuming using a variety of techniques. This included some examples
    of performing drift detection using the Alibi Detect and Evidently packages and
    a discussion of how to calculate feature importances.
  prefs: []
  type: TYPE_NORMAL
- en: We then covered the concept of how the training process can be automated at
    various levels of abstraction, before explaining how to programmatically manage
    the staging of your models with MLflow Model Registry. The final section covered
    how to define training pipelines in the Scikit-Learn and Spark ML packages.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will find out how to package up some of these concepts
    in a Pythonic way so that they can be deployed and reused seamlessly in other
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  prefs: []
  type: TYPE_IMG
