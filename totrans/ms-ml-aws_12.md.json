["```py\nimport boto3\nimport re\nfrom sagemaker import get_execution_role\nfrom sagemaker.amazon.amazon_estimator import get_image_uri\n\nrole = get_execution_role()\n\nbucket='mastering-ml-aws'\n\ntraining_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n```", "```py\n# Define Parameters\n\nnum_layers = \"18\" \nimage_shape = \"3,224,224\"\nnum_training_samples = \"15420\"\nnum_classes = \"257\"\nmini_batch_size =  \"64\"\nepochs = \"2\"\nlearning_rate = \"0.01\"\n```", "```py\nimport time\nimport boto3\nfrom time import gmtime, strftime\n\n# caltech-256\ns3_train_key = \"image-classification-full-training/train\"\ns3_validation_key = \"image-classification-full-training/validation\"\ns3_train = 's3://{}/{}/'.format(bucket, s3_train_key)\ns3_validation = 's3://{}/{}/'.format(bucket, s3_validation_key)\n\ns3 = boto3.client('s3')\n```", "```py\n# create unique job name \njob_name_prefix = 'example-imageclassification'\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\njob_name = job_name_prefix + timestamp\ntraining_params = \\\n{\n    # specify the training docker image\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image,\n        \"TrainingInputMode\": \"File\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,\n        \"InstanceType\": \"ml.p2.xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"image_shape\": image_shape,\n        \"num_layers\": str(num_layers),\n        \"num_training_samples\": str(num_training_samples),\n        \"num_classes\": str(num_classes),\n        \"mini_batch_size\": str(mini_batch_size),\n        \"epochs\": str(epochs),\n        \"learning_rate\": str(learning_rate)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 360000\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": s3_train,\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            },\n            \"ContentType\": \"application/x-recordio\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": s3_validation,\n                    \"S3DataDistributionType\": \"FullyReplicated\"\n                }\n            },\n            \"ContentType\": \"application/x-recordio\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n```", "```py\n# create the Amazon SageMaker training job\n\nsagemaker = boto3.client(service_name='sagemaker')\nsagemaker.create_training_job(**training_params)\n```", "```py\nimport boto3\nfrom time import gmtime, strftime\n\nsage = boto3.Session().client(service_name='sagemaker') \n\nmodel_name=\"example-full-image-classification-model\"\n\ninfo = sage.describe_training_job(TrainingJobName=job_name)\nmodel_data = info['ModelArtifacts']['S3ModelArtifacts']\n\nhosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n\nprimary_container = {\n    'Image': hosting_image,\n    'ModelDataUrl': model_data,\n}\n\ncreate_model_response = sage.create_model(\n    ModelName = model_name,\n    ExecutionRoleArn = role,\n    PrimaryContainer = primary_container)\n```", "```py\n!wget -r -np -nH --cut-dirs=2 -P /tmp/ -R \"index.html*\" http://www.vision.caltech.edu/Image_Datasets/Caltech256/images/008.bathtub/\n\nbatch_input = 's3://{}/image-classification-full-training/test/'.format(bucket)\ntest_images = '/tmp/images/008.bathtub'\n\n!aws s3 cp $test_images $batch_input --recursive --quiet\n```", "```py\ntimestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\nbatch_job_name = \"image-classification-model\" + timestamp\nrequest = \\\n{\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": model_name,\n    \"MaxConcurrentTransforms\": 16,\n    \"MaxPayloadInMB\": 6,\n    \"BatchStrategy\": \"SingleRecord\",\n    \"TransformOutput\": {\n        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, batch_job_name)\n    },\n    \"TransformInput\": {\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"S3Prefix\",\n                \"S3Uri\": batch_input\n            }\n        },\n        \"ContentType\": \"application/x-image\",\n        \"SplitType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    \"TransformResources\": {\n            \"InstanceType\": \"ml.p2.xlarge\",\n            \"InstanceCount\": 1\n    }\n}\n\nprint('Transform job name: {}'.format(batch_job_name))\nprint('\\nInput Data Location: {}'.format(s3_validation))\n```", "```py\nsagemaker = boto3.client('sagemaker')\nsagemaker.create_transform_job(**request)\n```", "```py\n{\n  \"prediction\": [\n    0.0002778972266241908,\n    0.05520012229681015,\n...\n    ]\n}\n```"]