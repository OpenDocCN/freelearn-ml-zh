- en: '*Chapter 14*: Naïve Bayes Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：朴素贝叶斯分类'
- en: In this chapter, we will examine situations where naïve Bayes might be a more
    efficient classifier than the ones we have examined so far. Naïve Bayes is a very
    intuitive and easy-to-implement classifier. Assuming our features are independent,
    we may even get improved performance over logistic regession, particularly if
    we are not using regularization with the latter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将探讨在哪些情况下朴素贝叶斯可能比我们迄今为止考察的某些分类器更有效率。朴素贝叶斯是一个非常直观且易于实现的分类器。假设我们的特征是独立的，我们甚至可能比逻辑回归得到更好的性能，尤其是如果我们不使用后者进行正则化的话。
- en: In this chapter, we will discuss the fundamental assumptions of naïve Bayes
    and how the algorithm is used to tackle some of the modeling challenges we have
    already explored, as well as some new ones, such as text classification. We will
    consider when naïve Bayes is a good option and when it is not. We will also examine
    the interpretation of naïve Bayes models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将讨论朴素贝叶斯的基本假设以及算法如何用于解决我们已探索的一些建模挑战，以及一些新的挑战，如文本分类。我们将考虑何时朴素贝叶斯是一个好的选择，何时不是。我们还将检查朴素贝叶斯模型的解释。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Key concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键概念
- en: Naïve Bayes classification models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类模型
- en: Naïve Bayes for text classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯用于文本分类
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter. The only exception is the imbalanced-learn library, which can be installed
    with `pip install imbalanced-learn`. All the code in this chapter was tested with
    scikit-learn versions 0.24.2 and 1.0.2.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将主要使用 pandas、NumPy 和 scikit-learn 库。唯一的例外是 imbalanced-learn 库，可以使用 `pip
    install imbalanced-learn` 安装。本章中的所有代码都使用 scikit-learn 版本 0.24.2 和 1.0.2 进行了测试。
- en: Key concepts
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键概念
- en: The naïve Bayes classifier uses Bayes’ theorem to predict class membership.
    Bayes’ theorem describes the relationship between the probability of an event
    and the probability of an event given new, relevant data. The probability of an
    event given new data is called the **posterior probability**. The probability
    of an event occurring before the new data is appropriately referred to as the
    **prior probability**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器使用贝叶斯定理来预测类别成员资格。贝叶斯定理描述了事件发生的概率与给定新、相关数据的事件发生概率之间的关系。给定新数据的事件的概率称为**后验概率**。在新的数据之前发生事件的概率适当地称为**先验概率**。
- en: 'Bayes’ theorem gives us the following equation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理给出了以下方程：
- en: '![](img/B17978_14_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_14_001.jpg)'
- en: The posterior probability (the probability of an event given new data) is equal
    to the probability of the data given the event, times the prior probability of
    the event, divided by the probability of the new data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 后验概率（给定新数据的事件的概率）等于数据给定事件的概率，乘以事件的先验概率，除以新数据的概率。
- en: 'Somewhat less colloquially, this is typically written as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微不那么口语化地，这通常如下所示：
- en: '![](img/B17978_14_002.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_14_002.jpg)'
- en: 'Here, *A* is an event, such as class membership, and *B* is new information.
    When applied to classification, we get the following equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*A* 是一个事件，例如类别成员资格，而 *B* 是新信息。当应用于分类时，我们得到以下方程：
- en: '![](img/B17978_14_003.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_14_003.jpg)'
- en: Here, ![](img/B17978_14_004.png) is the probability of class membership for
    an instance given the features for the instance, and ![](img/B17978_14_005.png)
    is the probability of features given class membership. *P(y)* is the probability
    of class membership, while ![](img/B17978_14_006.png) is the probability of feature
    values. Therefore, the posterior probability, ![](img/B17978_14_007.png), is equal
    to the probability of the feature values given class membership, times the probability
    of class membership, divided by the probability of the feature values.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_14_004.png) 是给定实例的特征后实例属于该类别的概率，而 ![](img/B17978_14_005.png)
    是给定类别成员资格的特征概率。*P(y)* 是类别成员资格的概率，而 ![](img/B17978_14_006.png) 是特征值的概率。因此，后验概率，![](img/B17978_14_007.png)，等于给定类别成员资格的特征值概率，乘以类别成员资格的概率，除以特征值的概率。
- en: The assumption here is that the features are independent of one another. This
    is what gives this method the *naïve* adjective. As a practical matter, though,
    feature independence is not necessary to get good results with naïve Bayes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的假设是特征之间相互独立。这就是给这个方法带来*朴素*这个形容词的原因。然而，作为一个实际问题，特征独立性并不是使用朴素贝叶斯获得良好结果所必需的。
- en: Naïve Bayes can work with numeric or categorical features. We typically use
    Gaussian naïve Bayes when we have mainly numeric features. As its name implies,
    Gaussian naïve Bayes assumes that the conditional probability of feature values,
    ![](img/B17978_14_008.png), follows a normal distribution. ![](img/B17978_14_009.png)
    can then be calculated relatively straightforwardly using the standard deviations
    and means of features within each class.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯可以处理数值或分类特征。当我们主要拥有数值特征时，我们通常使用高斯贝叶斯。正如其名所示，高斯贝叶斯假设特征值的条件概率![](img/B17978_14_008.png)遵循正态分布。然后可以使用每个类中特征的标准差和均值相对简单地计算出![](img/B17978_14_009.png)。
- en: 'When our features are discrete or are counts, we can use multinomial naïve
    Bayes instead. More generally, it works well when the conditional probability
    of feature values follows a multinomial distribution. A common application of
    multinomial naïve Bayes is with text classification using the **bag-of-words**
    approach. With bag-of-words, the features are the counts of each word in a document.
    We can apply Bayes’ theorem to estimate the probability of class membership:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的特征是离散的或计数时，我们可以使用多项式贝叶斯。更普遍地说，当特征值的条件概率遵循多项式分布时，它效果很好。多项式贝叶斯的一个常见应用是与使用**词袋**方法的文本分类。在词袋中，特征是文档中每个词的计数。我们可以应用贝叶斯定理来估计类成员的概率：
- en: '![](img/B17978_14_010.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_14_010.jpg)'
- en: Here, ![](img/B17978_14_011.png) is the probability of membership in the *k*
    class, given a vector of word counts, *W*. We will put this to good use in the
    last section of this chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_14_011.png)是给定一个词频向量*W*时属于*k*类的概率。我们将在本章的最后部分充分利用这一点。
- en: There is a fairly wide range of text classification tasks for which naïve Bayes
    is applicable. It is used in sentiment analysis, spam detection, and news story
    categorization, to name just a few examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯适用于相当广泛的文本分类任务。它在情感分析、垃圾邮件检测和新闻故事分类等方面都有应用，仅举几个例子。
- en: Naïve Bayes is an efficient algorithm for both training and prediction and often
    performs very well. It is quite scalable, working well with large numbers of instances
    and features. It is also very easy to interpret. The algorithm does best when
    model complexity is not necessary for good predictions. Even when naïve Bayes
    is not likely to be the approach that will yield the least bias, it is often useful
    for diagnostic purposes, or to check the results from a different algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯是一种既适用于训练又适用于预测的高效算法，并且通常表现良好。它相当可扩展，可以很好地处理大量实例和特征。它也非常容易解释。当模型复杂性对于良好的预测不是必需的时候，算法表现最佳。即使简单贝叶斯不太可能是产生最少偏差的方法，它也经常用于诊断目的，或者检查不同算法的结果。
- en: The NBA data we worked with in the previous chapter might be a good candidate
    for modeling with naïve Bayes. We will explore that in the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中使用过的NBA数据可能是用简单贝叶斯建模的好候选。我们将在下一节中探讨这一点。
- en: Naïve Bayes classification models
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单贝叶斯分类模型
- en: One of the attractions of naïve Bayes is that you can get decent results quickly,
    even when you have lots of data. Both fitting and predicting are fairly easy on
    system resources. Another advantage is that relatively complex relationships can
    be captured without having to transform the feature space or doing much hyperparameter
    tuning. We can demonstrate this with the NBA data we worked with in the previous
    chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯的一个吸引力在于，即使数据量很大，你也能快速获得不错的结果。在系统资源上，拟合和预测都相对容易。另一个优点是，可以捕捉相对复杂的关系，而无需转换特征空间或进行大量的超参数调整。我们可以用我们在上一章中使用的NBA数据来证明这一点。
- en: We will work with data on **National Basketball Association** (**NBA**) games
    in this section. The dataset contains statistics from each NBA game from the 2017/2018
    season through the 2020/2021 season. This includes the home team; whether the
    home team won; the visiting team; shooting percentages for visiting and home teams;
    turnovers, rebounds, and assists by both teams; and several other measures.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用关于**国家篮球协会**（**NBA**）比赛的数据。该数据集包含了从2017/2018赛季到2020/2021赛季每场NBA比赛的统计数据。这包括主队；主队是否获胜；客队；客队和主队的投篮命中率；两队的失误、篮板和助攻；以及其他一些指标。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The NBA game data can be downloaded by the public at [https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball).
    This dataset contains game data starting with the 1946/1947 NBA season. It uses
    `nba_api` to pull stats from `nba.com`. That API is available at [https://github.com/swar/nba_api](https://github.com/swar/nba_api).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: NBA比赛数据可以通过以下链接由公众下载：[https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball)。这个数据集包含从1946/1947赛季开始的比赛数据。它使用`nba_api`从`nba.com`获取统计数据。该API可在[https://github.com/swar/nba_api](https://github.com/swar/nba_api)找到。
- en: 'Let’s build a classification model using naïve Bayes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用朴素贝叶斯构建一个分类模型：
- en: 'We will load the same libraries we have been using in the last few chapters:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载我们在过去几章中使用过的相同库：
- en: '[PRE0]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will load the NBA games data. We need to do a little data cleaning
    here. A handful of observations have missing values for whether the home team
    won or not, `WL_HOME`. We will remove those as that will be our target. We will
    also convert `WL_HOME` into an integer. Notice that there is not so much class
    imbalance that we need to take aggressive steps to deal with it:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载NBA比赛数据。在这里我们需要进行一些数据清理。一些观测值在主队是否获胜（`WL_HOME`）方面有缺失值。我们将删除这些值，因为那将是我们的目标。我们还将`WL_HOME`转换为整数。请注意，没有太多的类别不平衡，我们不需要采取激进的措施来处理它：
- en: '[PRE1]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s create training and testing DataFrames, organizing them by numeric
    and categorical features. We should also generate some descriptive statistics.
    Since we did that in the previous chapter, we will not repeat that here; however,
    it might be helpful to go back and take a look at those numbers to get ready for
    the modeling stage:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建训练和测试数据框，按数值和分类特征组织它们。我们还应该生成一些描述性统计。由于我们在上一章中已经做了，所以这里不再重复；然而，回顾那些数字可能有助于为建模阶段做好准备：
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we need to set up the column transformations. We will deal with some outliers
    for the numeric features, assigning those values and any missing values to the
    median. Then, we will use the standard scaler. We will set up one-hot encoding
    for the categorical features:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要设置列转换。我们将处理数值特征的异常值，将这些值和任何缺失值分配给中位数。然后，我们将使用标准缩放器。我们将为分类特征设置独热编码：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to run a naïve Bayes classifier. We will add a Gaussian naïve
    Bayes instance to a pipeline that will be run after the column transformation
    and some recursive feature elimination:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行一个朴素贝叶斯分类器。我们将在列转换和一些递归特征消除之后，将高斯朴素贝叶斯实例添加到一个管道中：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s evaluate this model with K-fold cross-validation. We get okay scores,
    though not as good as those with support vector classification in the previous
    chapter:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用K折交叉验证来评估这个模型。我们得到了不错的分数，虽然不如上一章中支持向量分类的分数好：
- en: '[PRE5]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With Gaussian naïve Bayes, there is only one hyperparameter we must worry about
    tuning. We can determine how much smoothing to use with the `var_smoothing` hyperparameter.
    We can do a randomized grid search to figure out the best value.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于高斯朴素贝叶斯，我们只有一个超参数需要担心调整。我们可以通过`var_smoothing`超参数确定使用多少平滑度。我们可以进行随机网格搜索以找出最佳值。
- en: 'The `var_smoothing` hyperparameter determines how much is added to variances,
    which will cause models to be less dependent on instances close to mean values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`var_smoothing`超参数决定了添加到方差中的量，这将导致模型对接近平均值实例的依赖性降低：'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get somewhat better accuracy:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到了更好的准确性：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We should also take a look at the results from the different iterations. As
    we can see, larger smoothing values do better:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看不同迭代的结果。正如我们所见，较大的平滑值表现更好：
- en: '[PRE8]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can also look at the average fit and score times for each iteration:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以查看每次迭代的平均拟合和评分时间：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s look at the predictions for the best model. In addition to improving
    accuracy, there is an improvement in sensitivity, from `0.83` to `0.92`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看最佳模型的预测结果。除了提高准确性外，敏感性也有所提高，从`0.83`提升到`0.92`：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It is a good idea to also look at a confusion matrix to get a better sense
    of how the model does:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时查看一个混淆矩阵来更好地了解模型的表现也是一个好主意：
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This produces the following plot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve
    Bayes model ](img/B17978_14_0011.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 基于高斯朴素贝叶斯模型的主队获胜混淆矩阵](img/B17978_14_0011.jpg)'
- en: Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve
    Bayes model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 基于高斯朴素贝叶斯模型的主队获胜混淆矩阵
- en: This is not bad, though still not as good as our support vector model in the
    previous chapter. In particular, we would like to do a little better at predicting
    losses. This is also reflected in the relatively low specificity score of **0.79**
    that we saw in the previous step. Recall that specificity is the rate at which
    we correctly predict negative values out of the actual negatives.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这还不错，但仍然不如我们之前章节中的支持向量模型好。特别是，我们希望在预测损失方面做得更好。这也在我们之前步骤中看到的相对较低的**0.79**特异性得分中得到了反映。记住，特异性是我们正确预测实际负值的负值比率。
- en: On the other hand, the fitting and scoring ran quite swiftly. We also did not
    need to do much hyperparameter tuning. Naïve Bayes can often be a good place to
    start when modeling either a binary or multiclass target.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，拟合和评分运行得相当快。我们也不需要做太多的超参数调整。朴素贝叶斯在建模二元或多类目标时通常是一个好的起点。
- en: Naïve Bayes has turned out to be an even more popular option for text classification.
    We will use it for that purpose in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯已经成为文本分类中更受欢迎的选项。我们将在下一节中使用它。
- en: Naïve Bayes for text classification
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类的朴素贝叶斯
- en: It is perhaps surprising that an algorithm based on calculating conditional
    probabilities could be useful for text classification. But this follows fairly
    straightforwardly with a key simplifying assumption. Let’s assume that our documents
    can be well represented by the counts of each word in the document, without regard
    for word order or grammar. This is known as a bag-of-words. The relationship that
    a bag-of-words has to a categorical target – say, spam/not spam or positive/negative
    – can be modeled successfully with multinomial naïve Bayes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 也许令人惊讶的是，一个基于计算条件概率的算法对文本分类是有用的。但这与一个关键简化假设相当直接。让我们假设我们的文档可以通过文档中每个单词的计数很好地表示，不考虑单词顺序或语法。这被称为词袋。词袋与分类目标之间的关系——比如说，垃圾邮件/非垃圾邮件或正面/负面——可以用多项式朴素贝叶斯成功建模。
- en: We will work with text message data in this section. The dataset we will use
    contains labels for spam and not spam messages.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用短信数据。我们将使用的数据集包含垃圾邮件和非垃圾邮件的标签。
- en: Note
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset on text messages can be downloaded by the public at [https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification).
    It contains two columns: the text message and the spam or not spam (ham) label.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该短信数据集可以通过[https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification)供公众下载。它包含两列：短信文本和垃圾邮件或非垃圾邮件（ham）标签。
- en: 'Let’s do some text classification with naïve Bayes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用朴素贝叶斯进行一些文本分类：
- en: 'We will need a couple of modules that we have not used so far in this book.
    We will import `MultinomialNB`, which we will need to construct a multinomial
    naïve Bayes model. We will also need `CountVectorizer` to create a bag-of-words.
    We will import the `SMOTE` module to handle class imbalance. Note that we will
    use an *imbalanced-learn* pipeline rather than a *scikit-learn* one. This is because
    we will be using `SMOTE` in our pipeline:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要几个我们在这本书中尚未使用的模块。我们将导入`MultinomialNB`，这是我们构建多项式朴素贝叶斯模型所需的。我们还需要`CountVectorizer`来创建词袋。我们将导入`SMOTE`模块来处理类别不平衡。请注意，我们将使用一个*imbalanced-learn*管道而不是一个*scikit-learn*管道。这是因为我们将在我们的管道中使用`SMOTE`：
- en: '[PRE12]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We are using `SMOTE` in this section to do oversampling; that is, we will be
    duplicating instances in underrepresented classes. Oversampling can be a good
    option when we are concerned that our model is doing a poor job of capturing variation
    in a class because we have too few instances of that class, relative to one or
    more other classes. Oversampling duplicates instances of that class.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本节中，我们使用`SMOTE`进行过采样；也就是说，我们将复制代表性不足的类别的实例。当我们担心我们的模型在捕捉一个类别的变化方面做得不好，因为我们相对于一个或多个其他类别的实例太少时，过采样可以是一个好的选择。过采样会复制该类别的实例。
- en: 'Next, we will load the text message dataset. We will convert our target into
    an integer variable and confirm that it worked as expected. Note the significant
    class imbalance. Let’s look at the first few rows to get a better feel for the
    data:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载短信数据集。我们将把我们的目标转换为整数变量，并确认它按预期工作。注意显著的类别不平衡。让我们查看前几行，以更好地了解数据：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we create training and testing DataFrames. We will use the `stratify` parameter
    to ensure equal distributions of target values in the training and testing data.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建训练和测试数据框。我们将使用`stratify`参数来确保训练和测试数据中目标值的分布相等。
- en: 'We will also instantiate a `CountVectorizer` object to create our bag-of-words
    later. We indicate that we want some words to be ignored because they do not provide
    useful information. We could have created a stop word list, but here, we will
    take advantage of scikit-learn’s list of stop words in English:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实例化一个`CountVectorizer`对象来创建我们后面的词袋。我们指出我们想要忽略一些单词，因为它们不提供有用的信息。我们本可以创建一个停用词列表，但在这里，我们将利用scikit-learn的英文停用词列表：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s look at how the vectorizer works with a couple of observations from our
    data. To make it easier to view, we will only pull from messages that contain
    fewer than 50 characters.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看向量器是如何与我们的数据中的几个观察结果一起工作的。为了便于查看，我们只会从包含少于50个字符的消息中提取信息。
- en: Using the vectorizer, we get counts for all non stop words used for each observation.
    For example, `like` is used once in the first message and not at all in the second.
    This gives `like` a value of `1` for the first observation in the transformed
    data and a value of `0` for the second observation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量器，我们为每个观察结果中使用的所有非停用词获取计数。例如，`like`在第一条消息中使用了一次，而在第二条消息中一次也没有使用。这给`like`在转换数据中的第一个观察结果赋予了一个值为`1`，而在第二个观察结果中赋予了一个值为`0`。
- en: 'We won’t use anything from this step in our model. We are only doing this for
    illustrative purposes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在我们的模型中使用这一步骤中的任何内容。我们这样做只是为了说明目的：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s instantiate a `MultinomialNB` object and add it to a pipeline. We
    will add oversampling using `SMOTE` to handle the class imbalance:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实例化一个`MultinomialNB`对象并将其添加到管道中。我们将使用`SMOTE`进行过采样以处理类别不平衡：
- en: '[PRE16]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s look at some predictions. We get an impressive **0.97** accuracy
    rate and equally good specificity. This excellent specificity suggests that we
    do not have many false positives. The somewhat lower sensitivity indicates that
    we are not catching some of the positives (the spam messages), though we are still
    doing quite well:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些预测结果。我们得到了令人印象深刻的**0.97**准确率和同样好的特异性。这种出色的特异性表明我们没有许多误报。相对较低的反应性表明我们没有捕捉到一些正例（垃圾邮件），尽管我们仍然做得相当不错：
- en: '[PRE17]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It is helpful to visualize our model’s performance with a confusion matrix:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵可视化我们模型的表现是有帮助的：
- en: '[PRE18]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This produces the following plot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 14.2 – Spam prediction using multinomial naïve Bayes ](img/B17978_14_0021.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 使用多项式朴素贝叶斯进行垃圾邮件预测](img/B17978_14_0021.jpg)'
- en: Figure 14.2 – Spam prediction using multinomial naïve Bayes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 使用多项式朴素贝叶斯进行垃圾邮件预测
- en: Naïve Bayes can yield excellent results when constructing a text classification
    model. The metrics are often quite good and quite efficient. This was a very straightforward
    binary classification problem. However, naïve Bayes can also be effective with
    multiclass text classification problems. The algorithm can be applied in pretty
    much the same way as we did here with multiclass targets.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯在构建文本分类模型时可以产生很好的结果。指标通常非常好且非常高效。这是一个非常直接的二元分类问题。然而，朴素贝叶斯也可以在多类文本分类问题中有效。该算法可以以与我们在这里使用多类目标相同的方式应用。
- en: Summary
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Naïve Bayes is a great algorithm to add to our regular toolkit for solving classification
    problems. It is not often the approach that will produce predictions with the
    least bias. However, the flip side is also true. There is less risk of overfitting,
    particularly when working with continuous features. It is also quite efficient,
    scaling well to a large number of observations and a large feature space.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一个很好的算法，可以添加到我们的常规工具包中，用于解决分类问题。它并不总是产生最少偏差的预测方法。然而，另一方面也是真的。过拟合的风险较小，尤其是在处理连续特征时。它也非常高效，能够很好地扩展到大量观察和大量特征空间。
- en: The next two chapters of this book will explore unsupervised learning algorithms
    – those where we do not have a target to predict. In the next chapter, we will
    examine principal component analysis, and then K-means clustering in the chapter
    after that.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本书接下来的两章将探讨无监督学习算法——那些我们没有预测目标的情况。在下一章中，我们将研究主成分分析，然后在下一章中研究K-means聚类。
