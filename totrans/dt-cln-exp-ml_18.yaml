- en: '*Chapter 14*: Naïve Bayes Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will examine situations where naïve Bayes might be a more
    efficient classifier than the ones we have examined so far. Naïve Bayes is a very
    intuitive and easy-to-implement classifier. Assuming our features are independent,
    we may even get improved performance over logistic regession, particularly if
    we are not using regularization with the latter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the fundamental assumptions of naïve Bayes
    and how the algorithm is used to tackle some of the modeling challenges we have
    already explored, as well as some new ones, such as text classification. We will
    consider when naïve Bayes is a good option and when it is not. We will also examine
    the interpretation of naïve Bayes models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes for text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter. The only exception is the imbalanced-learn library, which can be installed
    with `pip install imbalanced-learn`. All the code in this chapter was tested with
    scikit-learn versions 0.24.2 and 1.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The naïve Bayes classifier uses Bayes’ theorem to predict class membership.
    Bayes’ theorem describes the relationship between the probability of an event
    and the probability of an event given new, relevant data. The probability of an
    event given new data is called the **posterior probability**. The probability
    of an event occurring before the new data is appropriately referred to as the
    **prior probability**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes’ theorem gives us the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_14_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The posterior probability (the probability of an event given new data) is equal
    to the probability of the data given the event, times the prior probability of
    the event, divided by the probability of the new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Somewhat less colloquially, this is typically written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *A* is an event, such as class membership, and *B* is new information.
    When applied to classification, we get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_14_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_14_004.png) is the probability of class membership for
    an instance given the features for the instance, and ![](img/B17978_14_005.png)
    is the probability of features given class membership. *P(y)* is the probability
    of class membership, while ![](img/B17978_14_006.png) is the probability of feature
    values. Therefore, the posterior probability, ![](img/B17978_14_007.png), is equal
    to the probability of the feature values given class membership, times the probability
    of class membership, divided by the probability of the feature values.
  prefs: []
  type: TYPE_NORMAL
- en: The assumption here is that the features are independent of one another. This
    is what gives this method the *naïve* adjective. As a practical matter, though,
    feature independence is not necessary to get good results with naïve Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes can work with numeric or categorical features. We typically use
    Gaussian naïve Bayes when we have mainly numeric features. As its name implies,
    Gaussian naïve Bayes assumes that the conditional probability of feature values,
    ![](img/B17978_14_008.png), follows a normal distribution. ![](img/B17978_14_009.png)
    can then be calculated relatively straightforwardly using the standard deviations
    and means of features within each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'When our features are discrete or are counts, we can use multinomial naïve
    Bayes instead. More generally, it works well when the conditional probability
    of feature values follows a multinomial distribution. A common application of
    multinomial naïve Bayes is with text classification using the **bag-of-words**
    approach. With bag-of-words, the features are the counts of each word in a document.
    We can apply Bayes’ theorem to estimate the probability of class membership:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_14_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_14_011.png) is the probability of membership in the *k*
    class, given a vector of word counts, *W*. We will put this to good use in the
    last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There is a fairly wide range of text classification tasks for which naïve Bayes
    is applicable. It is used in sentiment analysis, spam detection, and news story
    categorization, to name just a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes is an efficient algorithm for both training and prediction and often
    performs very well. It is quite scalable, working well with large numbers of instances
    and features. It is also very easy to interpret. The algorithm does best when
    model complexity is not necessary for good predictions. Even when naïve Bayes
    is not likely to be the approach that will yield the least bias, it is often useful
    for diagnostic purposes, or to check the results from a different algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The NBA data we worked with in the previous chapter might be a good candidate
    for modeling with naïve Bayes. We will explore that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the attractions of naïve Bayes is that you can get decent results quickly,
    even when you have lots of data. Both fitting and predicting are fairly easy on
    system resources. Another advantage is that relatively complex relationships can
    be captured without having to transform the feature space or doing much hyperparameter
    tuning. We can demonstrate this with the NBA data we worked with in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with data on **National Basketball Association** (**NBA**) games
    in this section. The dataset contains statistics from each NBA game from the 2017/2018
    season through the 2020/2021 season. This includes the home team; whether the
    home team won; the visiting team; shooting percentages for visiting and home teams;
    turnovers, rebounds, and assists by both teams; and several other measures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The NBA game data can be downloaded by the public at [https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball).
    This dataset contains game data starting with the 1946/1947 NBA season. It uses
    `nba_api` to pull stats from `nba.com`. That API is available at [https://github.com/swar/nba_api](https://github.com/swar/nba_api).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a classification model using naïve Bayes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the same libraries we have been using in the last few chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the NBA games data. We need to do a little data cleaning
    here. A handful of observations have missing values for whether the home team
    won or not, `WL_HOME`. We will remove those as that will be our target. We will
    also convert `WL_HOME` into an integer. Notice that there is not so much class
    imbalance that we need to take aggressive steps to deal with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create training and testing DataFrames, organizing them by numeric
    and categorical features. We should also generate some descriptive statistics.
    Since we did that in the previous chapter, we will not repeat that here; however,
    it might be helpful to go back and take a look at those numbers to get ready for
    the modeling stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to set up the column transformations. We will deal with some outliers
    for the numeric features, assigning those values and any missing values to the
    median. Then, we will use the standard scaler. We will set up one-hot encoding
    for the categorical features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to run a naïve Bayes classifier. We will add a Gaussian naïve
    Bayes instance to a pipeline that will be run after the column transformation
    and some recursive feature elimination:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s evaluate this model with K-fold cross-validation. We get okay scores,
    though not as good as those with support vector classification in the previous
    chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With Gaussian naïve Bayes, there is only one hyperparameter we must worry about
    tuning. We can determine how much smoothing to use with the `var_smoothing` hyperparameter.
    We can do a randomized grid search to figure out the best value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `var_smoothing` hyperparameter determines how much is added to variances,
    which will cause models to be less dependent on instances close to mean values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get somewhat better accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also take a look at the results from the different iterations. As
    we can see, larger smoothing values do better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also look at the average fit and score times for each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the predictions for the best model. In addition to improving
    accuracy, there is an improvement in sensitivity, from `0.83` to `0.92`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is a good idea to also look at a confusion matrix to get a better sense
    of how the model does:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve
    Bayes model ](img/B17978_14_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve
    Bayes model
  prefs: []
  type: TYPE_NORMAL
- en: This is not bad, though still not as good as our support vector model in the
    previous chapter. In particular, we would like to do a little better at predicting
    losses. This is also reflected in the relatively low specificity score of **0.79**
    that we saw in the previous step. Recall that specificity is the rate at which
    we correctly predict negative values out of the actual negatives.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the fitting and scoring ran quite swiftly. We also did not
    need to do much hyperparameter tuning. Naïve Bayes can often be a good place to
    start when modeling either a binary or multiclass target.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes has turned out to be an even more popular option for text classification.
    We will use it for that purpose in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes for text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is perhaps surprising that an algorithm based on calculating conditional
    probabilities could be useful for text classification. But this follows fairly
    straightforwardly with a key simplifying assumption. Let’s assume that our documents
    can be well represented by the counts of each word in the document, without regard
    for word order or grammar. This is known as a bag-of-words. The relationship that
    a bag-of-words has to a categorical target – say, spam/not spam or positive/negative
    – can be modeled successfully with multinomial naïve Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with text message data in this section. The dataset we will use
    contains labels for spam and not spam messages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset on text messages can be downloaded by the public at [https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification).
    It contains two columns: the text message and the spam or not spam (ham) label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do some text classification with naïve Bayes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need a couple of modules that we have not used so far in this book.
    We will import `MultinomialNB`, which we will need to construct a multinomial
    naïve Bayes model. We will also need `CountVectorizer` to create a bag-of-words.
    We will import the `SMOTE` module to handle class imbalance. Note that we will
    use an *imbalanced-learn* pipeline rather than a *scikit-learn* one. This is because
    we will be using `SMOTE` in our pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are using `SMOTE` in this section to do oversampling; that is, we will be
    duplicating instances in underrepresented classes. Oversampling can be a good
    option when we are concerned that our model is doing a poor job of capturing variation
    in a class because we have too few instances of that class, relative to one or
    more other classes. Oversampling duplicates instances of that class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will load the text message dataset. We will convert our target into
    an integer variable and confirm that it worked as expected. Note the significant
    class imbalance. Let’s look at the first few rows to get a better feel for the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we create training and testing DataFrames. We will use the `stratify` parameter
    to ensure equal distributions of target values in the training and testing data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will also instantiate a `CountVectorizer` object to create our bag-of-words
    later. We indicate that we want some words to be ignored because they do not provide
    useful information. We could have created a stop word list, but here, we will
    take advantage of scikit-learn’s list of stop words in English:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at how the vectorizer works with a couple of observations from our
    data. To make it easier to view, we will only pull from messages that contain
    fewer than 50 characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the vectorizer, we get counts for all non stop words used for each observation.
    For example, `like` is used once in the first message and not at all in the second.
    This gives `like` a value of `1` for the first observation in the transformed
    data and a value of `0` for the second observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won’t use anything from this step in our model. We are only doing this for
    illustrative purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s instantiate a `MultinomialNB` object and add it to a pipeline. We
    will add oversampling using `SMOTE` to handle the class imbalance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s look at some predictions. We get an impressive **0.97** accuracy
    rate and equally good specificity. This excellent specificity suggests that we
    do not have many false positives. The somewhat lower sensitivity indicates that
    we are not catching some of the positives (the spam messages), though we are still
    doing quite well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is helpful to visualize our model’s performance with a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Spam prediction using multinomial naïve Bayes ](img/B17978_14_0021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Spam prediction using multinomial naïve Bayes
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes can yield excellent results when constructing a text classification
    model. The metrics are often quite good and quite efficient. This was a very straightforward
    binary classification problem. However, naïve Bayes can also be effective with
    multiclass text classification problems. The algorithm can be applied in pretty
    much the same way as we did here with multiclass targets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes is a great algorithm to add to our regular toolkit for solving classification
    problems. It is not often the approach that will produce predictions with the
    least bias. However, the flip side is also true. There is less risk of overfitting,
    particularly when working with continuous features. It is also quite efficient,
    scaling well to a large number of observations and a large feature space.
  prefs: []
  type: TYPE_NORMAL
- en: The next two chapters of this book will explore unsupervised learning algorithms
    – those where we do not have a target to predict. In the next chapter, we will
    examine principal component analysis, and then K-means clustering in the chapter
    after that.
  prefs: []
  type: TYPE_NORMAL
