- en: Time Series and Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the models that we have discussed up to this point predict a property
    about something based on other properties related to that something. For example,
    we predicted the species of a flower based on measurements of the flower. We also
    tried to predict the progression of the disease diabetes in a patient based on
    medical attributes about that patient.
  prefs: []
  type: TYPE_NORMAL
- en: The premise of time series modeling is different from these types of property
    prediction problems. Simply put, time series modeling helps us predict the future
    based on attributes about the past. For example, we may want to predict future
    stock prices based on previous values of that stock price, or we may want to predict
    how many users will be on our website at a certain time based on data about how
    many users were on our website at previous times. This is sometimes called **forecasting**.
  prefs: []
  type: TYPE_NORMAL
- en: The data utilized in time series modeling is typically different from the data
    utilized in classification, regression, or clustering. A time series model operates
    on one or more **time series**, as one might expect. This series is a sequential
    set of properties, attributes, or other numbers paired with their corresponding
    date and time or a corresponding proxy for a date and time (measurement index
    or day number, for example). For stock prices, this series would consist of a
    bunch of (date and time, stock price) pairings.
  prefs: []
  type: TYPE_NORMAL
- en: This time series data is found everywhere in industry and in academia. It is
    also becoming increasingly important as we explore and develop the **Internet
    of Things** (**IoT**). Fitness trackers, *smart* devices such as refrigerators,
    thermostats, cameras, drones, and many other new devices, are producing a staggering
    amount of time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you are not restricted to predicting the future with this type of
    data. There are many other useful things that you can do with time series data
    including anomaly detection, which will be covered later in this chapter. Anomaly
    detection attempts to detect unexpected or out-of-the-ordinary events in a time
    series. These events might correspond to catastrophic weather events, infrastructure
    failures, viral social media behavior, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Representing time series data in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are purpose-built systems to store and work with time series data. Some
    of these are even written in Go, including Prometheus and InfluxDB. However, some
    of the tooling that we have already utilized in the book is also suitable to handle
    time series. Specifically, `github.com/kniren/gota/dataframe`, `gonum.org/v1/gonum/floats`,
    and `gonum.org/v1/gonum/mat` can help us as we are working with time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, a dataset that includes a time series representing the number
    of international air passengers during the years 1949-1960 (available for download
    at [https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv](https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `time` column includes a series of times represented by a year along
    with a decimal, and the `AirPassengers` column includes the number of international
    air passengers at that `time`. In other words, this is a time series with a pairing
    of (time, number of passengers).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just tabular data and we can represent it perfectly well using a dataframe
    or matrix. Let''s utilize a dataframe for simplicity, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We could represent the series similarly with `gonum.org/v1/gonum/mat` and,
    when/if needed, we can convert the dataframe to slices of floats for use with
    `gonum.org/v1/gonum/floats`. If we wanted to plot the time series, for example,
    we could convert the columns to floats and produce a plot with `gonum.org/v1/plot`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this program produces the following plot of our time
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8613e7d-2182-4772-93e7-b564d8e2771a.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the number of international air passengers increases over time
    as more and more people begin to travel via airplanes. We can also see that there
    appear to be bumps or spikes that are repeating over time. We will dive more into
    these features shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding time series jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are probably noticing by this time in the book that each set of machine
    learning techniques has an associated set of jargon, and time series is no different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an explanation of some of this jargon that will be utilized throughout
    the rest of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time**, **datetime**, or **timestamp**: This property is the temporal element
    of each pairing in our time series. This could be simply a time or it could be
    a combination of date and time (sometimes referred to as datetime or timestamp).
    It might also include time zone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observation**, **measurement**, **signal**, or **random variable**: This
    is the property that we are trying to forecast and/or otherwise analyze as a function
    of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality**: A time series, such as the time series of air passenger data,
    may exhibit changes that correspond to seasons (weeks, months, years, and so on).
    Time series that behave in this manner are said to exhibit some seasonality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trends**: Time series that gradually increase or decrease over time (separate
    from seasonal effects) are said to exhibit a trend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stationary**: A time series that exhibits the same patterns over time, without
    trends or other gradual changes (such as changes in variance or covariance), is
    said to be stationary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time period**: The amount of time between successive observations in the
    time series, or the difference between one timestamp and the previously occurring
    timestamp in the series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-regressive model**: This is a model that tries to model a time series
    process by one or more delayed, or lagged, versions of the same process. For example,
    an auto-regressive model of stock prices would try to model stock prices by the
    value of the stock price at previous time intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving average model**: This is a model that tries to model a times series
    based on the current and various past values of an imperfectly predictable term,
    commonly referred to as **error**. For example, this imperfectly predictable term
    may be some white noise in the time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics related to time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to certain jargon associated with time series, there is an important
    set of statistics related to time series that we will be relying on as we perform
    forecasting and anomaly detection. These statistics are mainly related to how
    values in times series are related to other values in the same time series.
  prefs: []
  type: TYPE_NORMAL
- en: The statistics will help us as we profile our data, which is an important part
    of any time series modeling project, as it is with all of the other types of modeling
    that we have covered. Gaining intuition about the behavior of your time series
    over time, seasonality, and trends is crucial for ensuring that you apply appropriate
    models and perform mental checks of your results.
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autocorrelation** is a measure of how correlated a signal is with a delayed
    version of itself. For example, one or more previous observation of a stock price
    may be correlated (or change together) with the next observation of the stock
    price. If this was the case, we would say that the stock price was influenced
    by itself according to some lag or delay. We could then model the future stock
    price by a delayed version of itself at the specific lags indicated as highly
    correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the autocorrelation of a variable *x[t]* with a delayed version
    of itself (or a version with a lag) *x[s]*, we can utilize the **autocorrelation
    function** (**ACF**), defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c5965b3-e042-4979-9db1-815be4227eef.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *s* could represent any lagged version of *x*. Thus, we can calculate
    the autocorrelation between *x* and a version of *x* that has a lag of a single
    time period (*x[t-1]*), between *x* and a version of *x* that has a lag of two
    time periods (*x[t-2]*), and so on. Doing this gives us information about which
    delayed versions of *x* are most correlated with *x*, and thus helps us determine
    which delayed versions of *x* might be good candidates for use in modeling future
    versions of *x.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try calculating the first few autocorrelations of our airline passenger
    time series with itself. To do this, we first need to create a function that will
    calculate an autocorrelation in our time series for a specific time period lag.
    Here is an example implementation of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then loop over a few lags and utilize the `acf()` function to calculate
    the various autocorrelations. This process is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the autocorrelations with lags further back in the series tend
    to be smaller (although, this is not the case for every lag). However, this information
    can be a little bit hard to absorb in its numerical form. Let''s plot these values
    as a function of the lag to better visualize the correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following plot of the ACF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03bf60a0-2e99-416b-a1e9-c104396d33cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the autocorrelations are decreasing generally, but they are staying
    rather large (well above 0.5) even out to lags of 20 time periods. This is an
    indication that our time series is not stationary. Indeed, if we look at the previous
    plot of our time series, it is obviously trending upward. We will deal with this
    non-stationary behavior later in the chapter, but for now, suffice it to say that
    the ACF plot is indicating to us that the lagged versions of the number of air
    passengers are correlated with their non-delayed counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, the ACF will allow us to determine which type of time series
    we are modeling. For a process that could be modeled well by an auto-regressive
    model, we should see that the `acf` function decreases somewhat quickly, but not
    immediately, as you move to further lags. For a process that could be modeled
    well by a so-called moving average model, we would see a significant ACF term
    at the first lag, but then the ACF would die off after that first lag.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on interpreting ACF plots, see [https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/](https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/).
    This post gives some great details, some of which we are not able to cover here.
  prefs: []
  type: TYPE_NORMAL
- en: Partial autocorrelation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you might expect from the name, partial autocorrelation is related to autocorrelation,
    but there are some subtle differences. Partial means that this is a conditional
    sort of correlation. In essence, partial autocorrelation measures the correlation
    of a series with itself at a certain lag after subtracting off any autocorrelations
    at intermediate lags. You could think of this as the leftover autocorrelation
    after intermediate correlations have been removed.
  prefs: []
  type: TYPE_NORMAL
- en: The reason that we might want something like this is that we need more than
    just the ACF to determine the order of our time series model, assuming that it
    can be modeled by an auto-regressive model. Let's suppose that, using the ACF,
    we have determined that we can model our series by an auto-regressive model, because
    the ACF decays exponentially with the lags. How are we to know if we should model
    this time series by a version of itself lagged by one time period, or both a version
    of itself lagged by one time period and two time periods, and so on?
  prefs: []
  type: TYPE_NORMAL
- en: By subtracting out intermediate correlations, we are able to quickly determine
    any leftover correlation that could be modeled using an auto-regressive model
    with more terms. If the partial autocorrelation dies off after a first lag, we
    know that we should be able to model our series based on a single lagged version
    of itself (lagged at one time period). However, if the partial autocorrelation
    does not die off after a first lag, we know that we will need to employ multiple
    lagged versions of the time series in our auto-regressive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we imagine linearly modeling a value in our time series (*x[t])* by values
    at successively larger lags in the series (*x[t-1]*, *x[t-2],* and so on), our
    equation would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32e2bed0-9723-47dd-9fd2-cfae67fc46e5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The various coefficients *m[1]*, *m[2]*, and so on, are the partial autocorrelations
    for a lag of one time period, the partial autocorrelation for the lag at two time
    periods, and so on, respectively. Thus, all we need to do to calculate the partial
    autocorrelation for a certain lag is to estimate the linear regression formula
    that will give us the corresponding coefficient. A function that performs this
    calculation is called the **partial autocorrelation function** (**PACF**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our favorite linear regression package, `github.com/sajari/regression`,
    we can create a Go function that implements the PACF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use this `pacf` function to calculate a few values of partial
    autocorrelation, corresponding to lags for which we previously calculated autocorrelation.
    This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives the following values of `Partial Autocorrelation`
    in our air passengers times series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the partial autocorrelation dies off quickly after about the
    second lag. This indicates that there is not much remaining relationship-wise
    in the time series after factoring in the relationships between the time series
    and its first and second lags. The partial autocorrelation does not go to *0.0*
    exactly, but that is expected due to some noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us better visualize the PACF, let''s create another plot. We can do
    this the exact same way as we did with the ACF, just substituting the `pacf()`
    function for the `acf()` function. The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea6ac50a-56db-457f-be30-be7aee2d3c55.png)'
  prefs: []
  type: TYPE_IMG
- en: Auto-regressive models for forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first category of models that we are going to use to try and forecast our
    time series are called **auto-regressive** (**AR**) models. As already mentioned,
    we try to model a data point in our time series based on one or more previous
    points in the series. We are, thus, modeling the time series using the time series
    itself. This use of the series itself is what distinguishes AR methods from the
    more general regression methods discussed in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive model overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will often see AR models referred to as AR(1), AR(2), and so on. These numbers
    correspond to the **order** of the AR model or process you are using to model
    the time series, and it is this order that you can determine by performing autocorrelation
    and partial autocorrelation analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'An AR(1) model attempts to model an observation in your series based on the
    observation in the same series at a one time period delay:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e12f1bd0-9f16-45a3-8ff5-a0047b6eb370.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An AR(2) model would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1679ea40-6d0e-4b16-8261-29ccbc3edaf6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An AR(3) model would add another term and so on, all following this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: These formulas might remind you of linear regression, and we will actually use
    many of the same methods here that we used when creating linear regression models.
    However, the unique aspects of time series modeling should not be ignored here.
    It is important to have an intuition of the time-related elements of your data
    (seasonality, trends, autocorrelations, and so on) and how they influence the
    AR models.
  prefs: []
  type: TYPE_NORMAL
- en: The order of AR model that you use to model your time series can be determined
    best by looking at a graph of the PACF. In the graph, you will see the PACF values
    decay to and then hover around zero. Look at how many lags it takes for the PACF
    to start hovering around zero, and then utilize an AR order corresponding to that
    many lags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that some packages to plot the PACF and ACF include horizontal lines indicating
    the statistical significance of the various lagged terms. I have not included
    these here, but if you want to quantitatively determine the order for your AR
    models, you might consider calculating these as further discussed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm](http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm)
    and [](http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm) [http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4463.htm](http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4463.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive model assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main assumptions of auto-regressive models are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationarity**: AR models assume that your time series is stationary. We
    should not see any trends in the data if we plan on using AR models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ergodicity**: This fancy term basically means that the statistical properties
    of the time series, like mean and variance, should not vary or drift over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whatever time series that we are modeling with AR methods should meet these
    assumptions. However, even when some data (like our air passenger data) that does
    not meet these assumptions, we can play some differencing tricks to still take
    advantage of AR models.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive model example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will try to model our air passenger data using an auto-regressive model.
    Now, we already know that we are breaking one of the assumptions of AR models
    in that our data is not stationary. However, we can apply a common trick to make
    our series stationary, called **differencing**.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming to a stationary series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make our time series stationary, we will create a proxy time series where
    the observation at time period *t* is the observation at time period *t* from
    our original time series minus the previous observation. Let''s difference each
    observation in this manner and then plot the results to see if it gets rid of
    the trends in our data. We are also going to output this differenced time series
    to a new `*.csv` file, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8be8954a-2996-4c3b-bbff-648fa7c4c7e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that we have basically removed all signs of the upward trend
    that was in the original time series. However, there still appears to be a problem
    related to variance. The differenced time series appears to have an increasing
    variance about the mean as time gets larger, which breaks our ergodicity assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with the increase in variance, we can further transform our time series
    using a log or power transformation that penalizes the larger values later in
    the time series. Let''s add this log transform, replot the log of the differenced
    series, and then save the resulting data to a file called `log_diff_series.csv`.
    The code to accomplish this is the same as the previous code snippet, except we
    use `math.Log()` to transform each value, so we will spare the details. The following
    is the resulting plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99844e43-29c8-4f78-95ec-55d733dbad0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Awesome! Now we have what looks to be a stationary series that we can utilize
    in our AR models. Note, we are looking at this qualitatively for the most part
    in this example, but there are quantitative tests for stationarity (the Dickey–Fuller
    test, for example).
  prefs: []
  type: TYPE_NORMAL
- en: We have transformed our data here with both a difference and a log transformation.
    This allowed us to fit within the assumptions of the AR model, but it also made
    our data and our eventual model a little less interpretable. It's harder to think
    about the log of a differenced time series than the time series itself. We had
    a justification for this trade-off here, but the trade-off should be noted, and
    the hope is to avoid such obfuscations where we can.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the ACF and choosing an AR order
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a stationary series that fits within the assumptions of our
    model, let's revisit our ACF and PACF plots to see what has changed. We can utilize
    the same code that we used to plot the ACF and PACF previously, but this time
    we will use our transformed series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the resulting ACF plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8756cb7f-fb4e-4a2b-a9ce-64eb930f59c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the resulting PACF plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d3896c7-b4e8-402c-b6b9-6dbc589e7f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: The first thing we notice is that the ACF plot no longer has a slow decay from
    *1.0* as the lags get larger and larger. The ACF plot decays and fluctuates around
    *0.0*. We will come back to the ACF in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can see that the PACF also decays down to *0.0* and fluctuates around
    0.0 thereafter. To choose the order of our AR model, we want to examine where
    the PACF plot appears to cross the zero line for the first time. In our case,
    this appears to be after the second lag period, and thus, we might want to consider
    using an AR(2) model to model this time series auto-regressively.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting and evaluating an AR(2) model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen that PACF gives us the coefficients for various orders
    in our AR model. Taking advantage of this, we can get the coefficients for our
    first and second lag terms along with the intercept (or error term) in our model
    using a slightly modified version of the `pacf()` function shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then call this on our log differenced series to get our trained AR(2)
    model coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this training gives us the following AR(2) formula for
    the log of the differenced passenger counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To evaluate this AR(2) model, we can calculate the **Mean Absolute Error** (**MAE**),
    similar to how we calculated it for linear regression models. Specifically, we
    will compute predicted passenger count values paired with our observed passenger
    count values, and then we will calculate the error and accumulate the MAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s calculate our transformed (log and differenced) predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in order to calculate our MAE, we need to transform these predictions
    back to normal passenger counts (such that we can compare them directly to our
    original time series). The reverse transform of our log and differenced data involves
    calculating cumulative sums in the transformed series, adding them back to the
    base series values, and then taking an exponential. This reverse transform, the
    accumulation of the MAE, and the aggregation of points to plot our observations
    and predictions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s output the MAE to stand out and save a line plot of the observed
    and predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling this and running it results in the following `MAE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember from our original visualization of this series, the passenger
    counts range from just above zero to just above 600\. Thus, an MAE of approximately
    355 is not super great. To get a more complete view of how our predictions and
    observations line up, however, let''s look at the plot generated by the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/770fe545-b8a4-4fa3-8517-a582b320c31e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our model overpredicts the number of air passengers, especially
    as time goes on. The model does exhibit some of the structure that is seen in
    the original data and it produces a similar trend. Yet, it appears that we may
    need a slightly more sophisticated model to more realistically represent this
    series.
  prefs: []
  type: TYPE_NORMAL
- en: No model is perfect and we have tried a relatively simple time series model
    here. It's good that we tried to stick with a simple and interpretable model,
    but our evaluation results would probably motivate us to refactor our model in
    a real-world scenario. Refactoring is good! It means that we learned something.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive moving averages and other time series models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model that we tried earlier was a relatively simple pure auto-regressive
    model. However, we are not stuck with using auto-regression or pure auto-regression
    alone in our time series models. As with other classes of machine learning models
    covered in this book, there is a whole zoo of time series techniques, and we cannot
    cover them all here. However, we did want to mention a few notable techniques
    that you could explore as you follow up on this material.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive models are often combined with models called **moving average
    models**. When these are combined, they are often referred to as **auto-regressive
    moving average** (**ARMA**) or **auto-regressive integrated moving average** (**ARIMA**)
    models. The moving average part of ARMA/ARIMA models allows you to capture the
    effects of things like white noise or other error terms in your time series, which
    would actually improve our AR(2) model for air passengers.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, at the time of writing this content, no out of the box package
    exists to perform ARIMA in Go. As mentioned earlier, the auto-regressive part
    is relatively easy, but the moving average fitting is slightly more complicated.
    This is another great place to jump in with contributions!
  prefs: []
  type: TYPE_NORMAL
- en: There are also time series models that are outside of the realm of ARIMA models.
    For example, the **Holt-Winters method** attempts to capture seasonality in time
    series data via a forecast equation and three smoothing equations. There are preliminary
    implementations of the Holt-Winters method in `github.com/datastream/holtwinters`
    and `github.com/dgryski/go-holtwinters`. These likely need to be further maintained
    and productionized, but they serve as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the introduction to this chapter, we might not always be interested
    in forecasting a time series. We might want to detect anomalous behavior in a
    time series. For example, we might want to know when out of the ordinary bursts
    of traffic come across our network, or we may want an alert when out of the ordinary
    numbers of users are attempting certain things inside of our application. These
    events could be tied to security concerns or may just be used to adjust our infrastructure
    or application settings.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, due to Go's history of usage in monitoring and infrastructure, there
    are a variety of Go-based options to detect anomalies in time series data. This
    tooling has been used in production to detect anomalous behavior while monitoring
    infrastructure and applications and, although there are more tools than can be
    mentioned here, I will highlight a couple.
  prefs: []
  type: TYPE_NORMAL
- en: First, the InfluxDB ([https://www.influxdata.com/](https://www.influxdata.com/))
    and Prometheus ([https://prometheus.io/](https://prometheus.io/)) ecosystems have
    a variety of options for anomaly detection. Both InfluxDB and Prometheus offer
    open source, Go-based time series databases and related tooling. They are useful
    to monitor infrastructure and applications, and they have widespread use both
    in the Go community and outside of the Go community. For example, if you are interested
    in using InfluxDB, you can use `github.com/nathanielc/morgoth` for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: This package implements the **Lossy Counting Algorithm** (**LCA**). On the Prometheus
    side, you could utilize a query-based approach as further discussed on [https://prometheus.io/blog/2015/06/18/practical-anomaly-detection/](https://prometheus.io/blog/2015/06/18/practical-anomaly-detection/).
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of standalone Go packages for anomaly detection as well,
    including `github.com/lytics/anomalyzer` and `github.com/sec51/goanomaly`. More
    specifically, `github.com/lytics/anomalyzer` implements a variety of tests to
    determine if an observation in your series is anomalous, including tests based
    on the cumulative distribution functions, bootstrap permutations, permuted rank-sums,
    relative magnitudes, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'To detect anomalies with `github.com/lytics/anomalyzer`, we need to create
    some configurations and an `anomalyzer.Anomalyzer` value. Once we have done this,
    detecting an anomaly is as simple as calling the `Push()` method on the `anomalyzer.Anomalyzer`
    value, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this anomaly detection yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Time series statistics (ACF and PACF):'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to use ACF: [https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/](https://coolstatsblog.com/2013/08/07/how-to-use-the-autocorreation-function-acf/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identifying the number of AR or MA terms in an ARIMA model: [https://people.duke.edu/~rnau/411arim3.htm](https://people.duke.edu/~rnau/411arim3.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Auto-regressive models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more mathematical introduction to AR models: [https://onlinecourses.science.psu.edu/stat501/node/358](https://onlinecourses.science.psu.edu/stat501/node/358)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/sajari/regression` docs: [https://godoc.org/github.com/sajari/regression](https://godoc.org/github.com/sajari/regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARMA/ARIMA models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction to ARIMA: [https://people.duke.edu/~rnau/411arim.htm](https://people.duke.edu/~rnau/411arim.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anomaly detection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'InfluxDB: [https://www.influxdata.com/](https://www.influxdata.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prometheus: [https://prometheus.io/](https://prometheus.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/lytics/anomalyzer` docs: [https://godoc.org/github.com/lytics/anomalyzer](https://godoc.org/github.com/lytics/anomalyzer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/sec51/goanomaly` docs: [https://godoc.org/github.com/sec51/goanomaly](https://godoc.org/github.com/sec51/goanomaly)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that was timely! We now know what time series data is, how to represent
    it in Go, how to make some forecasts, and how to detect anomalies in our time
    series data. These skills will come in useful anytime you are working with data
    that is changing with time, whether its data related to stock prices, or monitoring
    data related to your infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will level up our Go-based machine learning by looking
    at a few advanced techniques, including neural networks and deep learning.
  prefs: []
  type: TYPE_NORMAL
