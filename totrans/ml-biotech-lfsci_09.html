<html><head></head><body>
		<div id="_idContainer253">
			<h1 id="_idParaDest-100"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.1.1">Chapter 7: Supervised Machine Learning</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">As you begin to progress your career and skill set in the field of data science, you will encounter many different types of models that fall into one of the two categories of either supervised or unsupervised learning. </span><span class="koboSpan" id="kobo.2.2">Recall that in applications of unsupervised learning, models are generally trained to either cluster or transform data in order to group or reshape data to extract insights when labels are not available for the given dataset. </span><span class="koboSpan" id="kobo.2.3">Within this chapter, we will now discuss the applications of </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">supervised learning</span></strong><span class="koboSpan" id="kobo.4.1"> as they apply to the areas of classification and regression to develop powerful predictive models to make educated guesses about a dataset's labels.</span></p>
			<p><span class="koboSpan" id="kobo.5.1">Over the course of this chapter, we will discuss the following topics:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.6.1">Understanding supervised learning</span></li>
				<li><span class="koboSpan" id="kobo.7.1">Measuring success in supervised machine learning</span></li>
				<li><span class="koboSpan" id="kobo.8.1">Understanding classification in supervised machine learning</span></li>
				<li><span class="koboSpan" id="kobo.9.1">Understanding regression in supervised machine learning</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.10.1">With our objectives in mind, let's now go ahead and get started.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.11.1">Understanding supervised learning</span></h1>
			<p><span class="koboSpan" id="kobo.12.1">As you begin </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.13.1">to explore data science either on your own or within an organization, you will often be asked the question, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">What exactly does supervised machine learning mean?</span></em><span class="koboSpan" id="kobo.15.1"> Let's go ahead and come up with a definition. </span><span class="koboSpan" id="kobo.15.2">We can define supervised learning as a general subset of machine learning in which data, like its associated labels, is used to train models that can learn or generalize from the data to make predictions, preferably with a high degree of certainty. </span><span class="koboSpan" id="kobo.15.3">Thinking back to </span><a href="B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082"><em class="italic"><span class="koboSpan" id="kobo.16.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.17.1">, </span><em class="italic"><span class="koboSpan" id="kobo.18.1">Introduction to Machine Learning</span></em><span class="koboSpan" id="kobo.19.1">, we can recall the example we completed concerning the breast cancer dataset in which we classified tumors as being either malignant or benign. </span><span class="koboSpan" id="kobo.19.2">This example, alongside the definition we created, is an excellent way to learn and understand the meaning behind supervised learning. </span></p>
			<p><span class="koboSpan" id="kobo.20.1">With the definition of supervised machine learning now in our minds, let's go ahead and talk about its different subtypes, namely, classification and regression. </span><span class="koboSpan" id="kobo.20.2">If you recall, </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">classification</span></strong><span class="koboSpan" id="kobo.22.1"> within </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.23.1">the scope of machine learning is the act of predicting a </span><strong class="bold"><span class="koboSpan" id="kobo.24.1">category</span></strong><span class="koboSpan" id="kobo.25.1"> for a given set of data, such as classifying a tumor as malignant or benign, an email as spam or not spam, or even a protein </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.26.1">as alpha or beta. </span><span class="koboSpan" id="kobo.26.2">In each of these cases, the model will output a </span><strong class="bold"><span class="koboSpan" id="kobo.27.1">discrete</span></strong><span class="koboSpan" id="kobo.28.1"> value. </span><span class="koboSpan" id="kobo.28.2">On the </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.29.1">other hand, </span><strong class="bold"><span class="koboSpan" id="kobo.30.1">regression</span></strong><span class="koboSpan" id="kobo.31.1"> is the prediction of an </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">exact value</span></strong><span class="koboSpan" id="kobo.33.1"> using a given set of data, such as the lipophilicity </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.34.1">of a small molecule, the isoelectric point of a </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">Monoclonal Antibody</span></strong><span class="koboSpan" id="kobo.36.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.37.1">mAb</span></strong><span class="koboSpan" id="kobo.38.1">), or the LCAP of an LCMS peak. </span><span class="koboSpan" id="kobo.38.2">In each </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.39.1">of these cases, the model will output a </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">continuous</span></strong><span class="koboSpan" id="kobo.41.1"> value.</span></p>
			<p><span class="koboSpan" id="kobo.42.1">Many different </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.43.1">models exist within the two categories of supervised learning. </span><span class="koboSpan" id="kobo.43.2">Within the scope of this book, we will focus on four main </span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.44.1">models for each of these two categories. </span><span class="koboSpan" id="kobo.44.2">When it </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.45.1">comes to classification, we </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.46.1">will discuss </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">K-Nearest Neighbor</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">KNN</span></strong><span class="koboSpan" id="kobo.50.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.51.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.52.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.53.1">SVMs</span></strong><span class="koboSpan" id="kobo.54.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">d</span></strong><strong class="bold"><span class="koboSpan" id="kobo.56.1">ecision trees</span></strong><span class="koboSpan" id="kobo.57.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">random forests</span></strong><span class="koboSpan" id="kobo.59.1">, as well as </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.60.1">XGBoost </span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.61.1">classification. </span><span class="koboSpan" id="kobo.61.2">When </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.62.1">it comes </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.63.1">to regression, we </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.64.1">will discuss </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">linear regression</span></strong><span class="koboSpan" id="kobo.66.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">logistic regression</span></strong><span class="koboSpan" id="kobo.68.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">random forest regression</span></strong><span class="koboSpan" id="kobo.70.1">, and </span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.71.1">even </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">gradient boosting regression</span></strong><span class="koboSpan" id="kobo.73.1">. </span><span class="koboSpan" id="kobo.73.2">We can see these depicted in </span><em class="italic"><span class="koboSpan" id="kobo.74.1">Figure 7.1</span></em><span class="koboSpan" id="kobo.75.1">:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<span class="koboSpan" id="kobo.76.1"><img src="image/B17761_07_001.jpg" alt="Figure 7.1 – The two areas of supervised machine learning "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.77.1">Figure 7.1 – The two areas of supervised machine learning</span></p>
			<p><span class="koboSpan" id="kobo.78.1">Our main objective in each of these models is to train a new instance of that model for </span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.79.1">a particular dataset. </span><span class="koboSpan" id="kobo.79.2">We will </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">fit</span></strong><span class="koboSpan" id="kobo.81.1"> our model with the data, and </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">tune</span></strong><span class="koboSpan" id="kobo.83.1"> or adjust the parameters to give us the best outcomes. </span><span class="koboSpan" id="kobo.83.2">To determine what the best outcomes should be, we will need to know how to measure success within our models. </span><span class="koboSpan" id="kobo.83.3">We will learn about that in the following section.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.84.1">Measuring success in supervised machine learning</span></h1>
			<p><span class="koboSpan" id="kobo.85.1">As we begin to train our supervised classifiers and regressors, we will need to implement a few ways to determine which models are performing better, thus allowing us </span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.86.1">to effectively tune the model's parameters and maximize its performance. </span><span class="koboSpan" id="kobo.86.2">The best way to achieve this is to understand what success looks like ahead of time before diving into the model development process. </span><span class="koboSpan" id="kobo.86.3">There are many different methods for measuring success depending on the situation. </span><span class="koboSpan" id="kobo.86.4">For example, accuracy can be a good metric for classifiers, but not regressors. </span><span class="koboSpan" id="kobo.86.5">Similarly, a business case for a classifier may not necessarily require accuracy to be the primary metric of interest. </span><span class="koboSpan" id="kobo.86.6">It simply depends on the situation at hand. </span><span class="koboSpan" id="kobo.86.7">Let's take a look at some of the most common metrics used for each of the fields of </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">classification</span></strong><span class="koboSpan" id="kobo.88.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">regression</span></strong><span class="koboSpan" id="kobo.90.1">.</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<span class="koboSpan" id="kobo.91.1"><img src="image/B17761_07_002.jpg" alt="Figure 7.2 – Common success metrics for regression and classification "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.92.1">Figure 7.2 – Common success metrics for regression and classification</span></p>
			<p><span class="koboSpan" id="kobo.93.1">Although </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.94.1">there are many other metrics </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.95.1">you can use for a given scenario, the eight listed in </span><em class="italic"><span class="koboSpan" id="kobo.96.1">Figure 7.2</span></em><span class="koboSpan" id="kobo.97.1"> are some of the most common ones you will likely encounter. </span><span class="koboSpan" id="kobo.97.2">Selecting a given metric can be difficult as it should always align with the given use case. </span><span class="koboSpan" id="kobo.97.3">Let's go ahead and explore this when it comes to classification.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.98.1">Measuring success with classifiers</span></h2>
			<p><span class="koboSpan" id="kobo.99.1">Take, for example, the tumor dataset we have worked with thus far. </span><span class="koboSpan" id="kobo.99.2">We defined our success </span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.100.1">metric as accuracy, and therefore maximizing accuracy was our model's main training objective. </span><span class="koboSpan" id="kobo.100.2">This, however, is not always the case, and the success metric you choose to use will almost always be dependent on both the model and the business problem at hand. </span><span class="koboSpan" id="kobo.100.3">Let's go ahead and take a closer look at some metrics commonly used in the data science space and define them:</span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.101.1">Accuracy</span></strong><span class="koboSpan" id="kobo.102.1">: A measurement </span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.103.1">that agrees closely with the accepted value</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.104.1">Precision</span></strong><span class="koboSpan" id="kobo.105.1">: A measurement </span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.106.1">that agrees with other measurements in the sense that they are similar to one another</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.107.1">An easier way to think about accuracy and precision is by picturing the results displayed using a bullseye depiction. </span><span class="koboSpan" id="kobo.107.2">The difference between precision and accuracy is in the sense </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.108.1">of both how close the results are to one another, and how close the results are to their true or actual values, respectively. </span><span class="koboSpan" id="kobo.108.2">We can see a visual depiction of this in </span><em class="italic"><span class="koboSpan" id="kobo.109.1">Figure 7.3</span></em><span class="koboSpan" id="kobo.110.1">:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<span class="koboSpan" id="kobo.111.1"><img src="image/B17761_07_003.jpg" alt="Figure 7.3 – Graphical illustration of the difference between accuracy and precision "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.112.1">Figure 7.3 – Graphical illustration of the difference between accuracy and precision</span></p>
			<p><span class="koboSpan" id="kobo.113.1">In addition to the visual depiction, we can also think of precision as a calculation representing the results as subsets relative to a total population. </span><span class="koboSpan" id="kobo.113.2">In this case, we will also need to define </span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.114.1">a new metric known as </span><strong class="bold"><span class="koboSpan" id="kobo.115.1">recall</span></strong><span class="koboSpan" id="kobo.116.1">. </span><span class="koboSpan" id="kobo.116.2">We can think of recall and precision mathematically in the context of positive and negative results through </span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.117.1">what is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">confusion matrix</span></strong><span class="koboSpan" id="kobo.119.1">. </span><span class="koboSpan" id="kobo.119.2">When comparing the results of a prediction relative to the actual results, we can get a good sense of the model's performance by comparing a few of these values. </span><span class="koboSpan" id="kobo.119.3">We can see a visual depiction of this in </span><em class="italic"><span class="koboSpan" id="kobo.120.1">Figure 7.4</span></em><span class="koboSpan" id="kobo.121.1">:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<span class="koboSpan" id="kobo.122.1"><img src="image/B17761_07_004.jpg" alt="Figure 7.4 – Graphical illustration of a confusion matrix "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.123.1">Figure 7.4 – Graphical illustration of a confusion matrix</span></p>
			<p><span class="koboSpan" id="kobo.124.1">With </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.125.1">this table in mind, we can define </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">recall</span></strong><span class="koboSpan" id="kobo.127.1"> as the fraction of fraudulent cases that a given model identifies, or, from a mathematical perspective, we can define it as follows:</span></p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<span class="koboSpan" id="kobo.128.1"><img src="image/B17761_Formula_07_001.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.129.1">Whereas we can define </span><strong class="bold"><span class="koboSpan" id="kobo.130.1">precision</span></strong><span class="koboSpan" id="kobo.131.1"> in the same context as follows:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<span class="koboSpan" id="kobo.132.1"><img src="image/B17761_Formula_07_002.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.133.1">We can visualize accuracy, precision, and recall in a similar manner in the following diagram, in which each metric represents a specific calculation of the overall results:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<span class="koboSpan" id="kobo.134.1"><img src="image/B17761_07_005.jpg" alt="Figure 7.5 – Graphical illustration explaining accuracy, precision, and recall "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.135.1">Figure 7.5 – Graphical illustration explaining accuracy, precision, and recall</span></p>
			<p><span class="koboSpan" id="kobo.136.1">Finally, there is one last commonly used metric that is usually considered to be a loose </span><em class="italic"><span class="koboSpan" id="kobo.137.1">combination</span></em><span class="koboSpan" id="kobo.138.1"> of precision and recall known as the </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">F1 score</span></strong><span class="koboSpan" id="kobo.140.1">. </span><span class="koboSpan" id="kobo.140.2">We can define the </span><em class="italic"><span class="koboSpan" id="kobo.141.1">F1 score</span></em><span class="koboSpan" id="kobo.142.1"> as follows:</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<span class="koboSpan" id="kobo.143.1"><img src="image/B17761_Formula_07_003.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.144.1">So </span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.145.1">how do you determine which metric to use? </span><span class="koboSpan" id="kobo.145.2">There is no </span><em class="italic"><span class="koboSpan" id="kobo.146.1">best</span></em><span class="koboSpan" id="kobo.147.1"> metric that you should always use as it is highly dependent on each situation. </span><span class="koboSpan" id="kobo.147.2">When determining the best metric, you should always ask yourself, </span><em class="italic"><span class="koboSpan" id="kobo.148.1">What is the main objective for the model, as well as the business?</span></em><span class="koboSpan" id="kobo.149.1"> In the eyes of the model, accuracy may be the best metric. </span><span class="koboSpan" id="kobo.149.2">On the other hand, in the eyes of the business, recall may be the best metric. </span></p>
			<p><span class="koboSpan" id="kobo.150.1">Ultimately, recall could be considered more useful when overlooked cases (defined above as false negatives) are more important. </span><span class="koboSpan" id="kobo.150.2">Consider, for example, a model that is predicting a patient's diagnosis – we would likely care more about false </span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.151.1">negatives than false positives. </span><span class="koboSpan" id="kobo.151.2">On the other hand, precision can be more important when false positives are more costly to us. </span><span class="koboSpan" id="kobo.151.3">It all depends on the given business case and requirements. </span><span class="koboSpan" id="kobo.151.4">So far, we have investigated success as it relates to classification, so let's now investigate these ideas as they relate to regression.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.152.1">Measuring success with regressors</span></h2>
			<p><span class="koboSpan" id="kobo.153.1">Although we have not yet taken a deep dive into the field of regression, we have defined </span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.154.1">the main idea as the development of a model whose output is a continuous numerical value. </span><span class="koboSpan" id="kobo.154.2">Take, for example, the molecular toxicity dataset containing many columns of data whose values are all continuous floats. </span><span class="koboSpan" id="kobo.154.3">Hypothetically, you could use </span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.155.1">this dataset to make predictions on the </span><strong class="bold"><span class="koboSpan" id="kobo.156.1">Total Polar Surface Area</span></strong><span class="koboSpan" id="kobo.157.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.158.1">TPSA</span></strong><span class="koboSpan" id="kobo.159.1">). </span><span class="koboSpan" id="kobo.159.2">In this case, the metrics of accuracy, precision, and recall would not be the most useful to us to best understand the performance of our models. </span><span class="koboSpan" id="kobo.159.3">Alternatively, we will need some metrics better catered to continuous values.</span></p>
			<p><span class="koboSpan" id="kobo.160.1">One of the most common metrics for defining success in many models (not necessarily machine learning) is the </span><strong class="bold"><span class="koboSpan" id="kobo.161.1">Pearson correlation coefficient</span></strong><span class="koboSpan" id="kobo.162.1">, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.163.1">R2</span></strong><span class="koboSpan" id="kobo.164.1">. </span><span class="koboSpan" id="kobo.164.2">This calculation </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.165.1">is a common method used to measure the linearity of data, as it represents the proportion of variance in the dependent variable. </span><span class="koboSpan" id="kobo.165.2">We can define </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">R2</span></strong><span class="koboSpan" id="kobo.167.1"> as follows:</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<span class="koboSpan" id="kobo.168.1"><img src="image/B17761_Formula_07_004.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.169.1">In this equation, </span><span class="koboSpan" id="kobo.170.1"><img src="image/B17761_Formula_07_005.png" alt=""/></span><span class="koboSpan" id="kobo.171.1"> is the predicted value, and </span><span class="koboSpan" id="kobo.172.1"><img src="image/B17761_Formula_07_006.png" alt=""/></span><span class="koboSpan" id="kobo.173.1"> is the mean value.</span></p>
			<p><span class="koboSpan" id="kobo.174.1">Take, for example, a dataset in which experimental (actual) values were known, and predicted values were calculated. </span><span class="koboSpan" id="kobo.174.2">We could plot the graphs of these values against </span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.175.1">one another and measure the correlation. </span><span class="koboSpan" id="kobo.175.2">In theory, a perfect model would have an ideal correlation (as close to a value of 1.00 as possible). </span><span class="koboSpan" id="kobo.175.3">We can see a depiction of high and low correlation in </span><em class="italic"><span class="koboSpan" id="kobo.176.1">Figure 7.6</span></em><span class="koboSpan" id="kobo.177.1">:</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<span class="koboSpan" id="kobo.178.1"><img src="image/B17761_07_006.jpg" alt="Figure 7.6 – Difference between high and low correlation in scatter plots "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.179.1">Figure 7.6 – Difference between high and low correlation in scatter plots</span></p>
			<p><span class="koboSpan" id="kobo.180.1">Although this metric can give you a good estimate of a model's performance, there are a few others that can give you a better sense of the model's error: </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">Mean Absolute Error</span></strong><span class="koboSpan" id="kobo.182.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.183.1">MAE</span></strong><span class="koboSpan" id="kobo.184.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">Mean Squared Error</span></strong><span class="koboSpan" id="kobo.186.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.187.1">MSE</span></strong><span class="koboSpan" id="kobo.188.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.189.1">Root Mean Squared Error</span></strong><span class="koboSpan" id="kobo.190.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.191.1">RMSE</span></strong><span class="koboSpan" id="kobo.192.1">). </span><span class="koboSpan" id="kobo.192.2">Let's go ahead and define these:</span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.193.1">MAE</span></strong><span class="koboSpan" id="kobo.194.1">: The average of the absolute differences between </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.195.1">the actual and predicted values in a given dataset. </span><span class="koboSpan" id="kobo.195.2">This measure tends to be more robust when handling datasets with outliers:</span><div id="_idContainer160" class="IMG---Figure"><span class="koboSpan" id="kobo.196.1"><img src="image/B17761_Formula_07_007.jpg" alt=""/></span></div><p><span class="koboSpan" id="kobo.197.1">In which </span><span class="koboSpan" id="kobo.198.1"><img src="image/B17761_Formula_07_008.png" alt=""/></span><span class="koboSpan" id="kobo.199.1"> is the predicted value, and </span><span class="koboSpan" id="kobo.200.1"><img src="image/B17761_Formula_07_009.png" alt=""/></span><span class="koboSpan" id="kobo.201.1"> is the mean value.</span></p></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.202.1">MSE</span></strong><span class="koboSpan" id="kobo.203.1">: The average of the squared differences between </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.204.1">the actual and predicted values in a given dataset:</span><div id="_idContainer163" class="IMG---Figure"><span class="koboSpan" id="kobo.205.1"><img src="image/B17761_Formula_07_010.jpg" alt=""/></span></div></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.206.1">RMSE</span></strong><span class="koboSpan" id="kobo.207.1">: Square root of the MSE to measure the </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.208.1">standard deviation of the values. </span><span class="koboSpan" id="kobo.208.2">This metric is commonly used to compare regression models against each other:</span><div id="_idContainer164" class="IMG---Figure"><span class="koboSpan" id="kobo.209.1"><img src="image/B17761_Formula_07_011.jpg" alt=""/></span></div></li>
			</ul>
			<p><span class="koboSpan" id="kobo.210.1">When it comes to regression, there are many different metrics you can use depending on the given situation. </span><span class="koboSpan" id="kobo.210.2">In most regression models, RMSE is generally used </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.211.1">to compare the performance of multiple models as it is quite simple to calculate and differentiable. </span><span class="koboSpan" id="kobo.211.2">On the other hand, datasets with outliers are generally compared to one another using MSE and MAE. </span><span class="koboSpan" id="kobo.211.3">Now that we have gained a better sense of measuring success through various metrics, let's now go ahead and explore the area of classification.</span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.212.1">Understanding classification in supervised machine learning</span></h1>
			<p><span class="koboSpan" id="kobo.213.1">Classification </span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.214.1">models in the </span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.215.1">context of machine learning are supervised models whose objectives are to classify or categorize items based on previously learned examples. </span><span class="koboSpan" id="kobo.215.2">You will encounter classification models in many forms as they tend to be some of the most common models used in the field of data science. </span><span class="koboSpan" id="kobo.215.3">There are three main types of classifiers that we can develop based on the outputs of the model.</span></p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<span class="koboSpan" id="kobo.216.1"><img src="image/B17761_07_007.jpg" alt="Figure 7.7 – The three types of supervised classification "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.217.1">Figure 7.7 – The three types of supervised classification</span></p>
			<p><span class="koboSpan" id="kobo.218.1">The first </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.219.1">type is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.220.1">binary classifier</span></strong><span class="koboSpan" id="kobo.221.1">. </span><span class="koboSpan" id="kobo.221.2">As the name suggests, this is a classifier </span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.222.1">that predicts in a binary fashion in the sense that an output is one of two options, such as emails being spam or not spam, or molecules being toxic or not toxic. </span><span class="koboSpan" id="kobo.222.2">There is no third option in either of these cases, rendering the model a binary classifier.</span></p>
			<p><span class="koboSpan" id="kobo.223.1">The second </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.224.1">type of classifier is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">multiclass classifier</span></strong><span class="koboSpan" id="kobo.226.1">. </span><span class="koboSpan" id="kobo.226.2">This type of classifier is trained on more than two different outputs. </span><span class="koboSpan" id="kobo.226.3">For </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.227.1">example, many types of proteins can be classified based on structure and function. </span><span class="koboSpan" id="kobo.227.2">Some of these examples include structural proteins, enzymes, hormones, storage proteins, and toxins. </span><span class="koboSpan" id="kobo.227.3">Developing a model that would predict the type of protein based on some of the protein's characteristics would be regarded as a multiclass classifier in the sense that each row of data could have only one possible class or output.</span></p>
			<p><span class="koboSpan" id="kobo.228.1">Finally, we </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.229.1">also have </span><strong class="bold"><span class="koboSpan" id="kobo.230.1">multilabel classifiers</span></strong><span class="koboSpan" id="kobo.231.1">. </span><span class="koboSpan" id="kobo.231.2">These classifiers, unlike their multiclass </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.232.1">counterparts, are able to predict multiple outputs for a given row of data. </span><span class="koboSpan" id="kobo.232.2">For example, when screening patients for clinical trials, you may want to build patient profiles using many different types of labels, such as gender, age, diabetic status, and smoker status. </span><span class="koboSpan" id="kobo.232.3">When trying to predict what a certain group of patients might look like, we need to be able to predict all of these labels.</span></p>
			<p><span class="koboSpan" id="kobo.233.1">Now that </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.234.1">we have </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.235.1">broken down classification into a few different types, you are likely thinking about the many different areas in projects you are working on where a classifier may be of great value. </span><span class="koboSpan" id="kobo.235.2">The good news here is that many of the standard or popular classification models we are about to explore can be easily recycled and fitted with new data. </span><span class="koboSpan" id="kobo.235.3">As we begin to explore the many different models in the following section, think about the projects that you are working on and the datasets you have available, and which models they may fit the best.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.236.1">Exploring different classification models</span></h2>
			<p><span class="koboSpan" id="kobo.237.1">As we explore </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.238.1">a number of machine learning models, we will test out their performances on a new dataset concerning </span><em class="italic"><span class="koboSpan" id="kobo.239.1">single-cell RNA sequences</span></em><span class="koboSpan" id="kobo.240.1">, published by </span><em class="italic"><span class="koboSpan" id="kobo.241.1">Nestorowa et al.</span></em><span class="koboSpan" id="kobo.242.1"> in 2016. </span><span class="koboSpan" id="kobo.242.2">We will focus on using this structured dataset in order to develop a number of different classifiers. </span><span class="koboSpan" id="kobo.242.3">Let's go ahead and import the data and prepare it for the classification models. </span><span class="koboSpan" id="kobo.242.4">First, we will go ahead and import our dataset of interest using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">read_csv()</span></strong><span class="koboSpan" id="kobo.244.1"> function in </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">pandas</span></strong><span class="koboSpan" id="kobo.246.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.247.1">dfx = pd.read_csv("../../datasets/single_cell_rna/nestorowa_corrected_log2_transformed_counts.txt", sep=' ',  )</span></p>
			<p><span class="koboSpan" id="kobo.248.1">Next, we will use the index to isolate our labels (classes) for each of the rows, using the first four characters of each row:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.249.1">dfx['annotation'] = dfx.index.str[:4]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.250.1">y = dfx["annotation"].values.ravel()</span></p>
			<p><span class="koboSpan" id="kobo.251.1">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">head()</span></strong><span class="koboSpan" id="kobo.253.1"> function to take a look at the data. </span><span class="koboSpan" id="kobo.253.2">What we will notice is that there is more than 3,992 columns' worth of data. </span><span class="koboSpan" id="kobo.253.3">As any good data scientist knows, developing models with too many columns will lead to many inefficiencies, and therefore it would be best to reduce these down using an unsupervised </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.254.1">learning technique, such as </span><strong class="bold"><span class="koboSpan" id="kobo.255.1">PCA</span></strong><span class="koboSpan" id="kobo.256.1">. </span><span class="koboSpan" id="kobo.256.2">Prior </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.257.1">to applying PCA, we will need to scale or normalize our dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.259.1"> class in </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">sklearn</span></strong><span class="koboSpan" id="kobo.261.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.262.1">from sklearn.preprocessing import StandardScaler</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.263.1">scaler = StandardScaler()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.264.1">X_scaled = scaler.fit_transform(dfx.drop(columns = ["annotation"]))</span></p>
			<p><span class="koboSpan" id="kobo.265.1">Next, we can apply PCA to reduce our dataset from 3,992 columns down to 15:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.266.1">from sklearn.decomposition import PCA</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.267.1">pca = PCA(n_components=15, svd_solver='full')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.268.1">pca.fit(X_scaled)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.269.1">data_pca = pca.fit_transform(X_scaled)</span></p>
			<p><span class="koboSpan" id="kobo.270.1">With the data now in a much more reduced state, we can check the </span><strong class="bold"><span class="koboSpan" id="kobo.271.1">explained variance ratio</span></strong><span class="koboSpan" id="kobo.272.1"> to see how this compares with the original dataset. </span><span class="koboSpan" id="kobo.272.2">We will see that the sum of all columns totals 0.17, which is relatively low. </span><span class="koboSpan" id="kobo.272.3">We will want to aim for a value around 0.8, so let's go ahead and increase the total number of columns in order to increase the percentage of variance:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.273.1">pca = PCA(n_components=900, svd_solver='full')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.274.1">pca.fit(X_scaled)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.275.1">data_pca = pca.fit_transform(X_scaled)</span></p>
			<p><span class="koboSpan" id="kobo.276.1">With the PCA model applied, we managed to reduce the total number of columns by roughly 77%.</span></p>
			<p><span class="koboSpan" id="kobo.277.1">With this completed, we are now prepared to split our dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.279.1"> class:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.280.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.281.1">X_train, X_test, y_train, y_test = train_test_split(data_pca, y, test_size=0.33)</span></p>
			<p><span class="koboSpan" id="kobo.282.1">With the </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.283.1">dataset now split into training and test sets, we are now ready to begin the classification model development process!</span></p>
			<h3><span class="koboSpan" id="kobo.284.1">K-Nearest Neighbors</span></h3>
			<p><span class="koboSpan" id="kobo.285.1">One of the </span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.286.1">classic, easy-to-develop, and </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.287.1">most commonly discussed classification models is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.288.1">KNN</span></strong><span class="koboSpan" id="kobo.289.1"> model, first developed by Evelyn Fix and Joseph Hodges in 1951. </span><span class="koboSpan" id="kobo.289.2">The main idea behind this model is determining class membership based on proximity to the closest neighbors. </span><span class="koboSpan" id="kobo.289.3">Take, for </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.290.1">example, a 2D </span><strong class="bold"><span class="koboSpan" id="kobo.291.1">binary</span></strong><span class="koboSpan" id="kobo.292.1"> dataset in which items are classified as either A or B. </span><span class="koboSpan" id="kobo.292.2">As a new dataset is added, the model will determine </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.293.1">its membership or class based on its proximity (usually </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">Euclidean</span></strong><span class="koboSpan" id="kobo.295.1">) to other items in the same dataset.</span></p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<span class="koboSpan" id="kobo.296.1"><img src="image/B17761_07_008.jpg" alt="Figure 7.8 – Graphical representation of the KNN model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.297.1">Figure 7.8 – Graphical representation of the KNN model</span></p>
			<p><span class="koboSpan" id="kobo.298.1">KNN is regarded as one of the easiest machine learning models to develop and implement given its simple nature and clever design. </span><span class="koboSpan" id="kobo.298.2">The model, although simple in application, does require some tuning in order to be fully effective. </span><span class="koboSpan" id="kobo.298.3">Let's go ahead and explore the use of this model for the single-cell RNA classification dataset:</span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.299.1">We can begin by importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">KNeighborsClassifier</span></strong><span class="koboSpan" id="kobo.301.1"> model from </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">sklearn</span></strong><span class="koboSpan" id="kobo.303.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.304.1">from sklearn.neighbors import KNeighborsClassifier</span></p></li>
				<li><span class="koboSpan" id="kobo.305.1">Next, we can instantiate a new instance of this model in Python, the number of neighbors to a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">5</span></strong><span class="koboSpan" id="kobo.307.1">, and fit the model to our training data:</span><p class="source-code"><span class="koboSpan" id="kobo.308.1">knn = KNeighborsClassifier(n_neighbours=5)</span></p><p class="source-code"><span class="koboSpan" id="kobo.309.1">knn.fit(X_train, y_train)</span></p></li>
				<li><span class="koboSpan" id="kobo.310.1">With </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.311.1">the model fit, we </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.312.1">can now go ahead and predict the outcomes of the model and set those to a variable we will call </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">y_pred</span></strong><span class="koboSpan" id="kobo.314.1">, and finally use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">classification_report</span></strong><span class="koboSpan" id="kobo.316.1"> function to see the results:</span><p class="source-code"><span class="koboSpan" id="kobo.317.1">y_pred = knn.predict(X_test)</span></p><p class="source-code"><span class="koboSpan" id="kobo.318.1">print(classification_report(y_test, y_pred))</span></p><p><span class="koboSpan" id="kobo.319.1">Using the classification report function, we can get a sense of the precision, recall, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">F1</span></strong><span class="koboSpan" id="kobo.321.1"> scores for each of the three classes. </span><span class="koboSpan" id="kobo.321.2">We can see that the precision was relatively high for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.322.1">LT.H</span></strong><span class="koboSpan" id="kobo.323.1"> class, but slightly lower for the other two. </span><span class="koboSpan" id="kobo.323.2">Alternatively, </span><strong class="source-inline"><span class="koboSpan" id="kobo.324.1">recall</span></strong><span class="koboSpan" id="kobo.325.1"> was very low for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.326.1">LT.H</span></strong><span class="koboSpan" id="kobo.327.1"> class, but quite high for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">Prog</span></strong><span class="koboSpan" id="kobo.329.1"> class. </span><span class="koboSpan" id="kobo.329.2">In total, an average precision of </span><strong class="source-inline"><span class="koboSpan" id="kobo.330.1">0.63</span></strong><span class="koboSpan" id="kobo.331.1"> was calculated for this model:</span></p><div id="_idContainer167" class="IMG---Figure"><span class="koboSpan" id="kobo.332.1"><img src="image/B17761_07_009.jpg" alt="Figure 7.9 – Results of the KNN model "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.333.1">Figure 7.9 – Results of the KNN model</span></p><p><span class="koboSpan" id="kobo.334.1">With these results in mind, let's go ahead and tune one of the parameters, namely, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">n_neighbours</span></strong><span class="koboSpan" id="kobo.336.1"> parameter in the range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.337.1">1</span></strong><span class="koboSpan" id="kobo.338.1">-</span><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">10</span></strong><span class="koboSpan" id="kobo.340.1">.</span></p></li>
				<li><span class="koboSpan" id="kobo.341.1">We can use a simple </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">for</span></strong><span class="koboSpan" id="kobo.343.1"> loop to accomplish this:</span><p class="source-code"><span class="koboSpan" id="kobo.344.1">for i in range(1,10):</span></p><p class="source-code"><span class="koboSpan" id="kobo.345.1">    knn = KNeighborsClassifier(n_neighbors=i)</span></p><p class="source-code"><span class="koboSpan" id="kobo.346.1">    knn.fit(X_train, y_train)</span></p><p class="source-code"><span class="koboSpan" id="kobo.347.1">    y_pred = knn.predict(X_test)</span></p><p class="source-code"><span class="koboSpan" id="kobo.348.1">    print("n =", i, "acc =", accuracy_score(y_test, y_pred))</span></p><p><span class="koboSpan" id="kobo.349.1">If we take a look at the results, we can see the number of </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">neighbors</span></strong><span class="koboSpan" id="kobo.351.1"> as well </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.352.1">as the overall </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.353.1">model accuracy. </span><span class="koboSpan" id="kobo.353.2">Immediately, we notice that the option value based on this metric alone is </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">n=2</span></strong><span class="koboSpan" id="kobo.355.1">, giving an accuracy of approximately 60%.</span></p></li>
			</ol>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<span class="koboSpan" id="kobo.356.1"><img src="image/B17761_07_010.jpg" alt="Figure 7.10 – Results of the KNN model at different neighbors "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.357.1">Figure 7.10 – Results of the KNN model at different neighbors</span></p>
			<p><span class="koboSpan" id="kobo.358.1">KNN is one of the simplest and fastest models for the development of classifiers; however, it is not always the best model for a complex dataset such as this one. </span><span class="koboSpan" id="kobo.358.2">You will notice that the results varied heavily from class to class, indicating the model was not able to effectively distinguish between them based on their proximity </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.359.1">to other members </span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.360.1">alone. </span><span class="koboSpan" id="kobo.360.2">Let's go ahead and explore another model known as an SVM, which tries to classify items in a slightly different way.</span></p>
			<h3><span class="koboSpan" id="kobo.361.1">Support Vector Machines</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.362.1">SVMs</span></strong><span class="koboSpan" id="kobo.363.1"> are a class of supervised machine learning models </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.364.1">commonly used for both classification and regression, first developed in 1992 by AT&amp;T Bell Laboratories. </span><span class="koboSpan" id="kobo.364.2">The main </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.365.1">idea behind SVMs is the </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.366.1">ability to separate classes using a </span><strong class="bold"><span class="koboSpan" id="kobo.367.1">hyperplane</span></strong><span class="koboSpan" id="kobo.368.1">. </span><span class="koboSpan" id="kobo.368.2">There are three main types of SVMs that you will likely hear about in discussions or encounter in the data science field: linear SVMs, polynomial SVMs, and RBF SVMs.</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<span class="koboSpan" id="kobo.369.1"><img src="image/B17761_07_011.jpg" alt="Figure 7.11 – Visual explanation of the different SVMs "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.370.1">Figure 7.11 – Visual explanation of the different SVMs</span></p>
			<p><span class="koboSpan" id="kobo.371.1">The main idea behind the three models lies in how the classes are separated. </span><span class="koboSpan" id="kobo.371.2">For example, in </span><strong class="bold"><span class="koboSpan" id="kobo.372.1">linear</span></strong><span class="koboSpan" id="kobo.373.1"> models, the </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.374.1">hyperplane is a linear line separating the two classes from each other. </span><span class="koboSpan" id="kobo.374.2">Alternatively, the hyperplane may consist of a </span><strong class="bold"><span class="koboSpan" id="kobo.375.1">polynomial</span></strong><span class="koboSpan" id="kobo.376.1">, allowing the model to account for non-linear features. </span><span class="koboSpan" id="kobo.376.2">Finally, and most </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.377.1">popularly, the model can use a </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">Radial Basis Function</span></strong><span class="koboSpan" id="kobo.379.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.380.1">RBF</span></strong><span class="koboSpan" id="kobo.381.1">) to determine a datapoint's membership, which is based </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.382.1">on two parameters, </span><strong class="bold"><span class="koboSpan" id="kobo.383.1">gamma</span></strong><span class="koboSpan" id="kobo.384.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">C</span></strong><span class="koboSpan" id="kobo.386.1">, which account for the decision region, how it is spread </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.387.1">out, and the penalty for a misclassification. </span><span class="koboSpan" id="kobo.387.2">With this in mind, let's now take a closer look at the idea of a hyperplane.</span></p>
			<p><span class="koboSpan" id="kobo.388.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.389.1">hyperplane</span></strong><span class="koboSpan" id="kobo.390.1"> is a function that attempts to clearly define and allow for the differentiation </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.391.1">between classes in either a linear or non-linear fashion. </span><span class="koboSpan" id="kobo.391.2">The hyperplane can be described mathematically as follows:</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<span class="koboSpan" id="kobo.392.1"><img src="image/B17761_Formula_07_012.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<span class="koboSpan" id="kobo.393.1"><img src="image/B17761_Formula_07_013.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<span class="koboSpan" id="kobo.394.1"><img src="image/B17761_Formula_07_014.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<span class="koboSpan" id="kobo.395.1"><img src="image/B17761_Formula_07_015.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.396.1">In which </span><span class="koboSpan" id="kobo.397.1"><img src="image/B17761_Formula_07_016.png" alt=""/></span><span class="koboSpan" id="kobo.398.1"> are the vectors, </span><span class="koboSpan" id="kobo.399.1"><img src="image/B17761_Formula_07_017.png" alt=""/></span><span class="koboSpan" id="kobo.400.1"> is the bias term, and </span><span class="koboSpan" id="kobo.401.1"><img src="image/B17761_Formula_07_018.png" alt=""/></span><span class="koboSpan" id="kobo.402.1"> are the variables.</span></p>
			<p><span class="koboSpan" id="kobo.403.1">Taking a quick break from the RNA dataset, let's go ahead and demonstrate the use of a linear support vector using the enrollment dataset in Python – a dataset concerning patient enrolment in which respondent data was summarized via </span><strong class="bold"><span class="koboSpan" id="kobo.404.1">PCA</span></strong><span class="koboSpan" id="kobo.405.1"> into two features. </span><span class="koboSpan" id="kobo.405.2">The three possible labels within this dataset are </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">Likely</span></strong><span class="koboSpan" id="kobo.407.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">Very Likely</span></strong><span class="koboSpan" id="kobo.409.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">Unlikely</span></strong><span class="koboSpan" id="kobo.411.1"> to enroll. </span><span class="koboSpan" id="kobo.411.2">The main objective of a </span><strong class="bold"><span class="koboSpan" id="kobo.412.1">linear SVM</span></strong><span class="koboSpan" id="kobo.413.1"> is to </span><em class="italic"><span class="koboSpan" id="kobo.414.1">draw a line</span></em><span class="koboSpan" id="kobo.415.1"> clearly separating the data based on class.</span></p>
			<p><span class="koboSpan" id="kobo.416.1">Before </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.417.1">we begin using </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.418.1">SVM, let's go ahead and import our dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.419.1">df = pd.read_csv("../datasets/dataset_enrollment_sd.csv")</span></p>
			<p><span class="koboSpan" id="kobo.420.1">For simplicity, let's eliminate the </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">Likely</span></strong><span class="koboSpan" id="kobo.422.1"> class and keep the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">Very Likely</span></strong><span class="koboSpan" id="kobo.424.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">Unlikely</span></strong><span class="koboSpan" id="kobo.426.1"> classes:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.427.1">dftmp = df[(df["enrollment_cat"] != "Likely")]</span></p>
			<p><span class="koboSpan" id="kobo.428.1">Let's go ahead and draw a line separating the data shown in the scatter plot:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.429.1">     plt.figure(figsize=(15, 6))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.430.1">     xfit = np.linspace(-90, 130)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.431.1">         sns.scatterplot(dftmp["Feature1"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.432.1">                         dftmp["Feature2"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.433.1">                         hue=dftmp["enrollment_cat"].values, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.434.1">                         s=50)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.435.1">         for m, b in [(1, -45),]:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.436.1">             plt.plot(xfit, m * xfit + b, '-k')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.437.1">         plt.xlim(-120, 150);</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.438.1">         plt.ylim(-100, 60);</span></p>
			<p><span class="koboSpan" id="kobo.439.1">Upon executing this code, this yields the following diagram:</span></p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<span class="koboSpan" id="kobo.440.1"><img src="image/B17761_07_012.jpg" alt="Figure 7.12 – Two clusters separated by an initial SVM hyperplane "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.441.1">Figure 7.12 – Two clusters separated by an initial SVM hyperplane</span></p>
			<p><span class="koboSpan" id="kobo.442.1">Notice within the plot that this linear line could be drawn in multiple ways with different </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.443.1">slopes, yet still successfully separate </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.444.1">the two classes within the dataset. </span><span class="koboSpan" id="kobo.444.2">However, as new datapoints begin to encroach toward the middle ground between the two clusters, the slope and location of the hyperplane will begin to grow in importance. </span><span class="koboSpan" id="kobo.444.3">One way to address this issue is by defining the slope and location of the plane based on the closest datapoints. </span><span class="koboSpan" id="kobo.444.4">If the line contained a margin of width </span><em class="italic"><span class="koboSpan" id="kobo.445.1">x</span></em><span class="koboSpan" id="kobo.446.1"> in relation to the closest datapoints, then a more improved </span><strong class="bold"><span class="koboSpan" id="kobo.447.1">hyperplane</span></strong><span class="koboSpan" id="kobo.448.1"> could be constructed. </span><span class="koboSpan" id="kobo.448.2">We can construct this using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">fill_between</span></strong><span class="koboSpan" id="kobo.450.1"> function, as portrayed in the following code:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.451.1">         plt.figure(figsize=(15, 6))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.452.1">         xfit = np.linspace(-110, 180)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.453.1">         sns.scatterplot(dftmp["Feature1"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.454.1">                         dftmp["Feature2"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.455.1">                         hue=dftmp["enrollment_cat"].values, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.456.1">                         s=50)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.457.1">         for m, b, d in [(1, -45, 60),]:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.458.1">      yfit = m * xfit + b</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.459.1">      plt.plot(xfit, yfit, '-k')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.460.1">         plt.fill_between(xfit, yfit - d, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.461.1">                     yfit + d, edgecolor='none',</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.462.1">                              color='#AAAAAA', alpha=0.4)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.463.1">         plt.xlim(-120, 150);</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.464.1">         plt.ylim(-100, 60);</span></p>
			<p><span class="koboSpan" id="kobo.465.1">Upon </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.466.1">executing this </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.467.1">code, this yields the following figure:</span></p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<span class="koboSpan" id="kobo.468.1"><img src="image/B17761_07_013.jpg" alt="Figure 7.13 – Two clusters separated by an initial SVM hyperplane with specified margins "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.469.1">Figure 7.13 – Two clusters separated by an initial SVM hyperplane with specified margins</span></p>
			<p><span class="koboSpan" id="kobo.470.1">The datapoints from both classes that are within the margin width of the hyperplane </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.471.1">are known as </span><strong class="bold"><span class="koboSpan" id="kobo.472.1">support vectors</span></strong><span class="koboSpan" id="kobo.473.1">. </span><span class="koboSpan" id="kobo.473.2">The main intuition is the idea that the further away the support vectors are from the hyperplane, the higher the probability that a correct class is identified for a new datapoint.</span></p>
			<p><span class="koboSpan" id="kobo.474.1">We can </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.475.1">train a new SVM </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.476.1">classifier using the SVC class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.478.1"> library. </span><span class="koboSpan" id="kobo.478.2">We begin by importing the class, splitting the data, and then training a model using the dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.479.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.480.1">X_train, X_test, y_train, y_test =  </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.481.1">                train_test_split(dftmp[["Feature1","Feature2"]], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.482.1">                                  dftmp["enrollment_cat"].values,</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.483.1">                                  test_size = 0.25)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.484.1">from sklearn.svm import SVC</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.485.1">model = SVC (kernel='linear', C=1E10, random_state = 42)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.486.1">model.fit(X_train, y_train)</span></p>
			<p><span class="koboSpan" id="kobo.487.1">With that, we have now fitted our model to the dataset. </span><span class="koboSpan" id="kobo.487.2">As a final step, we can show the scatter plot, identify the hyperplane, and also specify which datapoints were the support vectors for this particular example:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.488.1">     plt.figure(figsize=(15, 6))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.489.1">     sns.scatterplot(dftmp["Feature1"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.490.1">                     dftmp["Feature2"], </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.491.1">                     hue=dftmp["enrollment_cat"].values, s=50)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.492.1">     plot_svc_decision_function(model);</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.493.1">     for j, k in model.support_vectors_:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.494.1">         plt.plot([j], [k], lw=0, ='o', color='red', </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.495.1">                  markeredgewidth=2, markersize=20, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.496.1">                  fillstyle='none')</span></p>
			<p><span class="koboSpan" id="kobo.497.1">Upon executing this code, this yields the following figure:</span></p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<span class="koboSpan" id="kobo.498.1"><img src="image/B17761_07_014.jpg" alt="Figure 7.14 – Two clusters separated by an initial SVM hyperplane with select support vectors "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.499.1">Figure 7.14 – Two clusters separated by an initial SVM hyperplane with select support vectors</span></p>
			<p><span class="koboSpan" id="kobo.500.1">Now that </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.501.1">we have gained a </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.502.1">better understanding of how SVMs operate in relation to their hyperplanes using this basic example, let's go ahead and test out this model using the single-cell RNA classification dataset we have been working with.</span></p>
			<p><span class="koboSpan" id="kobo.503.1">Following roughly the same steps as the KNN model, we will now implement the SVM model by first importing the library, instantiating the model with a linear kernel, fitting our training data, and subsequently making predictions on the test data:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.504.1">from sklearn.svm import SVC</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.505.1">svc = SVC(kernel="linear")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.506.1">svc.fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.507.1">y_pred = svc.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.508.1">print(classification_report(y_test, y_pred))</span></p>
			<p><span class="koboSpan" id="kobo.509.1">Upon printing the report, this yields the following results:</span></p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<span class="koboSpan" id="kobo.510.1"><img src="image/B17761_07_015.jpg" alt="Figure 7.15 – Results of the SVM model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.511.1">Figure 7.15 – Results of the SVM model</span></p>
			<p><span class="koboSpan" id="kobo.512.1">We can </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.513.1">see that the model was, in fact, quite </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.514.1">robust, with our dataset yielding some high metrics and giving us a total average precision of 88%. </span><span class="koboSpan" id="kobo.514.2">SVMs are fantastic models to use with complex datasets as their main objective is to separate data via a hyperplane. </span><span class="koboSpan" id="kobo.514.3">Let's now explore a model that takes a very different approach by using decision trees to arrive at final results.</span></p>
			<h3><span class="koboSpan" id="kobo.515.1">Decision trees and random forests</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.516.1">Decision trees</span></strong><span class="koboSpan" id="kobo.517.1"> are one </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.518.1">of the most popular </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.519.1">and commonly used machine learning models when it comes </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.520.1">to structured datasets for both classification </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.521.1">and regression. </span><span class="koboSpan" id="kobo.521.2">Decision trees </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.522.1">consist of three elements: </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">nodes</span></strong><span class="koboSpan" id="kobo.524.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.525.1">edges</span></strong><span class="koboSpan" id="kobo.526.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">leaf nodes</span></strong><span class="koboSpan" id="kobo.528.1">.</span></p>
			<p><span class="koboSpan" id="kobo.529.1">Nodes generally </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.530.1">consist of a question allowing for the process to split into an arbitrary number of child nodes, shown in orange in the following diagram. </span><span class="koboSpan" id="kobo.530.2">The root node is the first node that the entire tree is referenced through. </span><span class="koboSpan" id="kobo.530.3">Edges are the connections between nodes shown in blue. </span><span class="koboSpan" id="kobo.530.4">When nodes have no children, then this final destination is called a leaf, shown in green. </span><span class="koboSpan" id="kobo.530.5">In some cases, a decision tree will have nodes containing the same parent – these are </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.531.1">called sibling nodes. </span><span class="koboSpan" id="kobo.531.2">The more nodes there are in a tree, the </span><em class="italic"><span class="koboSpan" id="kobo.532.1">deeper</span></em><span class="koboSpan" id="kobo.533.1"> the tree is said to be. </span><span class="koboSpan" id="kobo.533.2">The depth of the decision tree is a measure of </span><em class="italic"><span class="koboSpan" id="kobo.534.1">complexity</span></em><span class="koboSpan" id="kobo.535.1">. </span></p>
			<p><span class="koboSpan" id="kobo.536.1">A tree that is </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.537.1">not complex enough will not arrive </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.538.1">at an accurate result, and a tree that is too complex will be overtrained. </span><span class="koboSpan" id="kobo.538.2">Identifying a good balance is one of the primary objectives in the training process. </span><span class="koboSpan" id="kobo.538.3">Using these elements, you can construct a decision tree, allowing for processes to flow from the top to the bottom, thereby arriving at a particular destination or </span><em class="italic"><span class="koboSpan" id="kobo.539.1">decision</span></em><span class="koboSpan" id="kobo.540.1">:</span></p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<span class="koboSpan" id="kobo.541.1"><img src="image/B17761_07_016.jpg" alt="Figure 7.16 – Illustration of decision trees when it comes to nodes, leaves, and edges "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.542.1">Figure 7.16 – Illustration of decision trees when it comes to nodes, leaves, and edges</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.543.1">Decision trees</span></strong><span class="koboSpan" id="kobo.544.1"> operate in quite a genius way. </span><span class="koboSpan" id="kobo.544.2">We begin with our initial dataset in which all datapoints are labeled and ready to go. </span><span class="koboSpan" id="kobo.544.3">The first objective is to split the dataset using a decision boundary that is the most informative – which, in this case, is at </span><em class="italic"><span class="koboSpan" id="kobo.545.1">y=m.</span></em><span class="koboSpan" id="kobo.546.1"> This has successfully isolated a class from the two others; however, the two others are still not yet isolated from one another. </span><span class="koboSpan" id="kobo.546.2">The algorithm then splits the dataset again at </span><em class="italic"><span class="koboSpan" id="kobo.547.1">x = n</span></em><span class="koboSpan" id="kobo.548.1">, thus completely separating the three clusters. </span><span class="koboSpan" id="kobo.548.2">If more clusters were present, this process would iteratively and recursively continue until all classes are optimally separated. </span><span class="koboSpan" id="kobo.548.3">We can see a visual representation of this in </span><em class="italic"><span class="koboSpan" id="kobo.549.1">Figure 7.17</span></em><span class="koboSpan" id="kobo.550.1">:</span></p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<span class="koboSpan" id="kobo.551.1"><img src="image/B17761_07_017.jpg" alt="Figure 7.17 – A graphical representation of the process that decision trees take "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.552.1">Figure 7.17 – A graphical representation of the process that decision trees take</span></p>
			<p><span class="koboSpan" id="kobo.553.1">Decision trees </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.554.1">determine where and how to </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.555.1">split the data using various splitting criteria, known as attribute </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.556.1">selection measures. </span><span class="koboSpan" id="kobo.556.2">These prominent attribute selection measures include:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.557.1">Information gain</span></li>
				<li><span class="koboSpan" id="kobo.558.1">Gain ratio</span></li>
				<li><span class="koboSpan" id="kobo.559.1">Gini index</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.560.1">Let's now take a closer look at these three items.</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.561.1">Information gain</span></strong><span class="koboSpan" id="kobo.562.1"> is an attribute concerning the amount of information required to further </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.563.1">describe the tree. </span><span class="koboSpan" id="kobo.563.2">This </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.564.1">attribute minimizes the information needed for data classification while utilizing the least randomness in the partitions. </span><span class="koboSpan" id="kobo.564.2">Think of information gain of a random variable determined from the observation of a random variable, </span><em class="italic"><span class="koboSpan" id="kobo.565.1">A</span></em><span class="koboSpan" id="kobo.566.1">, as a function such that:</span></p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<span class="koboSpan" id="kobo.567.1"><img src="image/B17761_Formula_07_019.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.568.1">Broadly speaking, the information gain is the change in entropy (information entropy) </span><span class="koboSpan" id="kobo.569.1"><img src="image/B17761_Formula_07_020.png" alt=""/></span><span class="koboSpan" id="kobo.570.1">such that:</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<span class="koboSpan" id="kobo.571.1"><img src="image/B17761_Formula_07_021.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.572.1">In which </span><span class="koboSpan" id="kobo.573.1"><img src="image/B17761_Formula_07_022.png" alt=""/></span><span class="koboSpan" id="kobo.574.1"> represents the conditional entropy of </span><span class="koboSpan" id="kobo.575.1"><img src="image/B17761_Formula_07_023.png" alt=""/></span><span class="koboSpan" id="kobo.576.1"> given the attribute </span><span class="koboSpan" id="kobo.577.1"><img src="image/B17761_Formula_07_024.png" alt=""/></span><span class="koboSpan" id="kobo.578.1">. </span><span class="koboSpan" id="kobo.578.2">In summary, information gain answers the question, </span><em class="italic"><span class="koboSpan" id="kobo.579.1">How much information do we obtain from this variable given another variable?</span></em></p>
			<p><span class="koboSpan" id="kobo.580.1">On the </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.581.1">other hand, the </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">gain ratio</span></strong><span class="koboSpan" id="kobo.583.1"> is the information gain relative to </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.584.1">the intrinsic information. </span><span class="koboSpan" id="kobo.584.2">In other words, this measure is biased toward tests that result in many outcomes, thus forcing a preference in favor of features of this nature. </span><span class="koboSpan" id="kobo.584.3">The gain ratio can be represented as:</span></p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<span class="koboSpan" id="kobo.585.1"><img src="image/B17761_Formula_07_025.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.586.1">In which </span><span class="koboSpan" id="kobo.587.1"><img src="image/B17761_Formula_07_026.png" alt=""/></span><span class="koboSpan" id="kobo.588.1"> is represented as:</span></p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<span class="koboSpan" id="kobo.589.1"><img src="image/B17761_Formula_07_027.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.590.1">In summary, the gain </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.591.1">ratio penalizes </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.592.1">variables with more distinct values, which will help decide the next split at the next level.</span></p>
			<p><span class="koboSpan" id="kobo.593.1">Finally, we </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.594.1">arrive at the </span><strong class="bold"><span class="koboSpan" id="kobo.595.1">Gini index</span></strong><span class="koboSpan" id="kobo.596.1">, which is an attribute selection </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.597.1">measure representing how often a randomly selected element is incorrectly labeled. </span><span class="koboSpan" id="kobo.597.2">The Gini index can be calculated by subtracting the sum of square probabilities of each class:</span></p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<span class="koboSpan" id="kobo.598.1"><img src="image/B17761_Formula_07_028.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.599.1">This methodology in determining a split naturally favors larger partitions as opposed to information gain, which favors smaller ones. </span><span class="koboSpan" id="kobo.599.2">The objective of any data scientist is to explore different methods with your dataset and determine the best path forward.</span></p>
			<p><span class="koboSpan" id="kobo.600.1">Now that we have a much more detailed explanation of decision trees and how the model operates, let's now go ahead and implement this model using the previous single-cell RNA classification dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.601.1">from sklearn.tree import DecisionTreeClassifier</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.602.1">dtc = DecisionTreeClassifier(max_depth=4)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.603.1">dtc.fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.604.1">y_pred = dtc.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.605.1">print(classification_report(y_test, y_pred))</span></p>
			<p><span class="koboSpan" id="kobo.606.1">Upon </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.607.1">printing the report, this yields the following results:</span></p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<span class="koboSpan" id="kobo.608.1"><img src="image/B17761_07_018.jpg" alt="Figure 7.18 – Results of the decision tree classifier "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.609.1">Figure 7.18 – Results of the decision tree classifier</span></p>
			<p><span class="koboSpan" id="kobo.610.1">We can </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.611.1">see that the model, without any tuning, was able to deliver a total precision score of 77% using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.612.1">max_depth</span></strong><span class="koboSpan" id="kobo.613.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">4</span></strong><span class="koboSpan" id="kobo.615.1">. </span><span class="koboSpan" id="kobo.615.2">Using the same method as the </span><em class="italic"><span class="koboSpan" id="kobo.616.1">KNN</span></em><span class="koboSpan" id="kobo.617.1"> model, we can iterate over a range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">max_depth</span></strong><span class="koboSpan" id="kobo.619.1"> values to determine the optimal value. </span><span class="koboSpan" id="kobo.619.2">Doing so would result in an ideal </span><strong class="source-inline"><span class="koboSpan" id="kobo.620.1">max_depth</span></strong><span class="koboSpan" id="kobo.621.1"> value of 3, yielding a total precision of 82%. </span></p>
			<p><span class="koboSpan" id="kobo.622.1">As we begin to train many of our models, one of the most common issues that we will face is </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">overfitting</span></strong><span class="koboSpan" id="kobo.624.1"> our data in one way or another. </span><span class="koboSpan" id="kobo.624.2">Take, for example, a decision tree model that was very finely tuned for a specific selection of data since decision trees are built on an entire dataset using the features and variables of interest. </span><span class="koboSpan" id="kobo.624.3">In this case, the model may be prone to overfitting. </span><span class="koboSpan" id="kobo.624.4">On the other hand, another model known as a </span><strong class="bold"><span class="koboSpan" id="kobo.625.1">random forest</span></strong><span class="koboSpan" id="kobo.626.1"> is built using many different decision trees to help mitigate any potential overfitting.</span></p>
			<p><span class="koboSpan" id="kobo.627.1">Random forests </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.628.1">are robust ensemble models based on decision trees. </span><span class="koboSpan" id="kobo.628.2">Where decision trees are generally designed to develop a model using the dataset as a whole, random forests randomly select features and rows and subsequently build multiple decision trees that are then averaged in their weights. </span><span class="koboSpan" id="kobo.628.3">Random forests are powerful models given their ability to limit overfitting while avoiding a substantial increase in error due to </span><strong class="bold"><span class="koboSpan" id="kobo.629.1">bias</span></strong><span class="koboSpan" id="kobo.630.1">. </span><span class="koboSpan" id="kobo.630.2">We can see a visual representation of this in </span><em class="italic"><span class="koboSpan" id="kobo.631.1">Figure 7.19</span></em><span class="koboSpan" id="kobo.632.1">:</span></p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<span class="koboSpan" id="kobo.633.1"><img src="image/B17761_07_019.jpg" alt="Figure 7.19 – Graphical explanation of random forest models "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.634.1">Figure 7.19 – Graphical explanation of random forest models</span></p>
			<p><span class="koboSpan" id="kobo.635.1">There are two main ways in which random forests can help reduce </span><strong class="bold"><span class="koboSpan" id="kobo.636.1">variance</span></strong><span class="koboSpan" id="kobo.637.1">. </span><span class="koboSpan" id="kobo.637.2">The first method is by training on different samples of data. </span><span class="koboSpan" id="kobo.637.3">Consider the preceding example using the patient enrollment data. </span><span class="koboSpan" id="kobo.637.4">If the model was trained on samples not containing those </span><em class="italic"><span class="koboSpan" id="kobo.638.1">in between</span></em><span class="koboSpan" id="kobo.639.1"> the clusters, then determining the score on the test set will result in significantly lower accuracy. </span></p>
			<p><span class="koboSpan" id="kobo.640.1">The second method involves using a random subset of features to train on, allowing for the determination of the concept of feature importance within the model. </span><span class="koboSpan" id="kobo.640.2">Let's go ahead and take a look at this model using the single-cell RNA classification dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.641.1">from sklearn.ensemble import RandomForestClassifier</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.642.1">clf = RandomForestClassifier(n_estimators=1000)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.643.1">clf.fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.644.1">y_pred = clf.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.645.1">print(classification_report(y_test, y_pred))</span></p>
			<p><span class="koboSpan" id="kobo.646.1">Upon printing the report, this yields the following results:</span></p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<span class="koboSpan" id="kobo.647.1"><img src="image/B17761_07_020.jpg" alt="Figure 7.20 – Results of the random forest model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.648.1">Figure 7.20 – Results of the random forest model</span></p>
			<p><span class="koboSpan" id="kobo.649.1">We can immediately observe that the model has a precision of roughly 74%, slightly lower than the decision tree above, indicating that the tree may have overfitted the data slightly.</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.650.1">Random forest</span></strong><span class="koboSpan" id="kobo.651.1"> models are </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.652.1">very commonly used in the biotechnology and life sciences industries given their remarkable methods of avoiding overfitting, as well as their ability to develop predictive models with smaller datasets. </span><span class="koboSpan" id="kobo.652.2">Many applications of machine learning within the biotech space generally suffer </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.653.1">from a concept known as the </span><strong class="bold"><span class="koboSpan" id="kobo.654.1">low-N</span></strong><span class="koboSpan" id="kobo.655.1"> problem, in the sense that use cases exist, but little to no data has been collected or organized with which to develop a model. </span><span class="koboSpan" id="kobo.655.2">Random forests are commonly used for applications in this space given their ensemble nature. </span><span class="koboSpan" id="kobo.655.3">Let's now take a look at a very different model that splits data not based on decisions, but based on statistical probability instead.</span></p>
			<h3><span class="koboSpan" id="kobo.656.1">Extreme Gradient Boosting (XGBoost)</span></h3>
			<p><span class="koboSpan" id="kobo.657.1">Over the last few years, a number of robust machine learning models have begun to enter the data science space, thus changing the machine learning landscape </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.658.1">quite effectively – one of these models being the </span><strong class="bold"><span class="koboSpan" id="kobo.659.1">Extreme Gradient Boosting</span></strong><span class="koboSpan" id="kobo.660.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.661.1">XGBoost</span></strong><span class="koboSpan" id="kobo.662.1">) model. </span><span class="koboSpan" id="kobo.662.2">The </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.663.1">main idea behind this </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.664.1">model is that it is an implementation of </span><strong class="bold"><span class="koboSpan" id="kobo.665.1">Gradient Boosted Models</span></strong><span class="koboSpan" id="kobo.666.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.667.1">GBMs</span></strong><span class="koboSpan" id="kobo.668.1">), specifically, </span><em class="italic"><span class="koboSpan" id="kobo.669.1">decision trees</span></em><span class="koboSpan" id="kobo.670.1">, in which speed and performance were highly optimized. </span><span class="koboSpan" id="kobo.670.2">Because of the highly efficient and highly effective nature of this model, it began to dominate many areas of data science and eventually became the go-to algorithm for many data science competitions on </span><a href="http://Kaggle.com"><span class="koboSpan" id="kobo.671.1">Kaggle.com</span></a><span class="koboSpan" id="kobo.672.1">.</span></p>
			<p><span class="koboSpan" id="kobo.673.1">There are many reasons why GBMs are so effective with structured/tabular datasets. </span><span class="koboSpan" id="kobo.673.2">Let's go ahead and explore three of these reasons:</span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.674.1">Parallelization</span></strong><span class="koboSpan" id="kobo.675.1">: The </span><em class="italic"><span class="koboSpan" id="kobo.676.1">XGBoost</span></em><span class="koboSpan" id="kobo.677.1"> model implements a method known as parallelization. </span><span class="koboSpan" id="kobo.677.2">The </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.678.1">main idea here is that it can parallelize processes in the construction of each of the trees. </span><span class="koboSpan" id="kobo.678.2">In essence, each of the branches of a single tree is trained separately.</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.679.1">Tree-Pruning</span></strong><span class="koboSpan" id="kobo.680.1">: Pruning is the process of removing parts of a decision tree deemed </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.681.1">to be redundant or not useful to a model. </span><span class="koboSpan" id="kobo.681.2">The main idea behind </span><em class="italic"><span class="koboSpan" id="kobo.682.1">GBM</span></em><span class="koboSpan" id="kobo.683.1"> pruning, simply stated, is that the GBM model would stop splitting a given node when a negative loss in the split is encountered. </span><em class="italic"><span class="koboSpan" id="kobo.684.1">XGBoost</span></em><span class="koboSpan" id="kobo.685.1">, on the other hand, splits up to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.686.1">max_depth</span></strong><span class="koboSpan" id="kobo.687.1"> parameter, and then begins the pruning process backward and eventually removes splits after which there is no longer a positive gain.</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.688.1">Regularization</span></strong><span class="koboSpan" id="kobo.689.1">: In the context of tree-based methods overall, regularization </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.690.1">is an algorithmic method to define a minimum gain in order to prompt another split in the tree. </span><span class="koboSpan" id="kobo.690.2">In essence, regularization shrinks scores, thereby prompting the final prediction to be more conservative, which, in return, helps prevent overfitting within the model.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.691.1">Now that we have gained a much better understanding of XGBoost and some of the reasons behind its robust performance, let's go ahead and implement this model on our RNA dataset. </span><span class="koboSpan" id="kobo.691.2">We will begin by installing the model's library using </span><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">pip</span></strong><span class="koboSpan" id="kobo.693.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.694.1">pip install xgboost</span></p>
			<p><span class="koboSpan" id="kobo.695.1">With </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.696.1">the model </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.697.1">installed, let's now go ahead and import the model and then create a new instance of the model in which we specify the </span><strong class="source-inline"><span class="koboSpan" id="kobo.698.1">n_estimators</span></strong><span class="koboSpan" id="kobo.699.1"> parameter to have a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">10000</span></strong><span class="koboSpan" id="kobo.701.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.702.1">from xgboost import XGBClassifier</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.703.1">xgb = XGBClassifier(n_estimators=10000)</span></p>
			<p><span class="koboSpan" id="kobo.704.1">Similar to the previous models, we can now go ahead and fit our model with the training datasets and print the results of our predictions:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.705.1">xgb.fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.706.1">y_pred = xgb.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.707.1">print(classification_report(y_test, y_pred))</span></p>
			<p><span class="koboSpan" id="kobo.708.1">Upon printing the report, this yields the following results:</span></p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<span class="koboSpan" id="kobo.709.1"><img src="image/B17761_07_021.jpg" alt="Figure 7.21 – Results of the XGBoost model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.710.1">Figure 7.21 – Results of the XGBoost model</span></p>
			<p><span class="koboSpan" id="kobo.711.1">With that, we can see that we managed to achieve a precision of </span><strong class="source-inline"><span class="koboSpan" id="kobo.712.1">0.86</span></strong><span class="koboSpan" id="kobo.713.1">, much higher than some of the other models we tested. </span><span class="koboSpan" id="kobo.713.2">The highly optimized nature of this model allows it to be very fast and robust relative to most others.</span></p>
			<p><span class="koboSpan" id="kobo.714.1">Over the course of this section, we managed to cover quite a wide scope of </span><strong class="bold"><span class="koboSpan" id="kobo.715.1">classification</span></strong><span class="koboSpan" id="kobo.716.1"> models. </span><span class="koboSpan" id="kobo.716.2">We began with the simple </span><strong class="bold"><span class="koboSpan" id="kobo.717.1">KNN</span></strong><span class="koboSpan" id="kobo.718.1"> model, which attempts to predict the class of a new value relative to its closest neighbors. </span><span class="koboSpan" id="kobo.718.2">Next, we covered </span><strong class="bold"><span class="koboSpan" id="kobo.719.1">SVM</span></strong><span class="koboSpan" id="kobo.720.1"> models, which attempt to assign labels based on specified boundaries drawn by support vectors. </span><span class="koboSpan" id="kobo.720.2">We then covered both </span><strong class="bold"><span class="koboSpan" id="kobo.721.1">decision trees</span></strong><span class="koboSpan" id="kobo.722.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.723.1">random</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.724.1">forests</span></strong><span class="koboSpan" id="kobo.725.1">, which operate based on nodes, leaves, and splits, and then finally saw a working example of </span><strong class="bold"><span class="koboSpan" id="kobo.726.1">XGBoost</span></strong><span class="koboSpan" id="kobo.727.1">, a highly optimized model that implements many of the features we saw in other models.</span></p>
			<p><span class="koboSpan" id="kobo.728.1">As you </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.729.1">begin to dive </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.730.1">into the many different models for new datasets, you will likely investigate the idea of automating the model selection process. </span><span class="koboSpan" id="kobo.730.2">If you think about it, each of the steps we have taken above could be automated in one way or another to identify which model operates the best under a specific set of metric requirements. </span><span class="koboSpan" id="kobo.730.3">Luckily for us, a library already exists that can assist us in this space. </span><span class="koboSpan" id="kobo.730.4">Over the course of the following tutorial, we will investigate the use of these models alongside some automated machine learning capabilities on </span><strong class="bold"><span class="koboSpan" id="kobo.731.1">Google Cloud Platform</span></strong><span class="koboSpan" id="kobo.732.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.733.1">GCP</span></strong><span class="koboSpan" id="kobo.734.1">).</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.735.1">Tutorial: Classification of proteins using GCP</span></h2>
			<p><span class="koboSpan" id="kobo.736.1">During this tutorial, we will investigate a number of classification models, followed </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.737.1">by an implementation of some automated machine learning capabilities. </span><span class="koboSpan" id="kobo.737.2">Our main objective will be to automatically </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.738.1">develop a model for the classification of proteins using a dataset from the </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">Research Collaboratory for Structural Bioinformatics</span></strong><span class="koboSpan" id="kobo.740.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.741.1">RCSB</span></strong><span class="koboSpan" id="kobo.742.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.743.1">Protein Data Bank</span></strong><span class="koboSpan" id="kobo.744.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.745.1">PDB</span></strong><span class="koboSpan" id="kobo.746.1">). </span><span class="koboSpan" id="kobo.746.2">If you </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.747.1">recall, we used data from RCSB PDB in a previous chapter to plot a 3D protein structure. </span><span class="koboSpan" id="kobo.747.2">The dataset we will be working </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.748.1">with in this chapter consists of two parts—a </span><strong class="bold"><span class="koboSpan" id="kobo.749.1">structured dataset</span></strong><span class="koboSpan" id="kobo.750.1"> with rows columns, one column of which is the designated classification of each of the proteins, and a series of RNA sequences for each of the proteins. </span><span class="koboSpan" id="kobo.750.2">We will save this second set of sequence-based data for analysis in a later chapter and focus on the structured dataset for now.</span></p>
			<p><span class="koboSpan" id="kobo.751.1">In this chapter, we will use this structured dataset containing many different types of proteins in an attempt to develop a classifier. </span><span class="koboSpan" id="kobo.751.2">Given the large nature of this dataset, we will take this opportunity to move our development environment from our local installation of </span><strong class="bold"><span class="koboSpan" id="kobo.752.1">Jupyter Notebook</span></strong><span class="koboSpan" id="kobo.753.1"> to an online notebook in </span><strong class="bold"><span class="koboSpan" id="kobo.754.1">GCP</span></strong><span class="koboSpan" id="kobo.755.1">. </span><span class="koboSpan" id="kobo.755.2">Before we can do so, we will need to create a new GCP account. </span><span class="koboSpan" id="kobo.755.3">Let's go ahead and get started.</span></p>
			<h3><span class="koboSpan" id="kobo.756.1">Getting started in GCP</span></h3>
			<p><span class="koboSpan" id="kobo.757.1">Getting started in </span><strong class="bold"><span class="koboSpan" id="kobo.758.1">GCP</span></strong><span class="koboSpan" id="kobo.759.1"> is quite simple, and can be accomplished in a few simple steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.760.1">We can </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.761.1">begin by navigating to </span><a href="https://cloud.google.com/"><span class="koboSpan" id="kobo.762.1">https://cloud.google.com/</span></a><span class="koboSpan" id="kobo.763.1"> and registering for a new account. </span><span class="koboSpan" id="kobo.763.2">You will need to provide a few details, such as your name, email, and a few other items.</span></li>
				<li><span class="koboSpan" id="kobo.764.1">Once registered, you will be able to navigate to the console by clicking the </span><strong class="bold"><span class="koboSpan" id="kobo.765.1">Console</span></strong><span class="koboSpan" id="kobo.766.1"> button on the upper right-hand side of the page:</span><div id="_idContainer197" class="IMG---Figure"><span class="koboSpan" id="kobo.767.1"><img src="image/B17761_07_022.jpg" alt="Figure 7.22 – A screenshot of the Console button "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.768.1">Figure 7.22 – A screenshot of the Console button</span></p></li>
				<li><span class="koboSpan" id="kobo.769.1">Within the console page, you will be able to see all items relating to your current project, such as general information, resources used, API usage, and even billing:</span><div id="_idContainer198" class="IMG---Figure"><span class="koboSpan" id="kobo.770.1"><img src="image/B17761_07_023.jpg" alt="Figure 7.23 – An example of the console page "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.771.1">Figure 7.23 – An example of the console page</span></p></li>
				<li><span class="koboSpan" id="kobo.772.1">You will </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.773.1">likely not have any projects set up yet. </span><span class="koboSpan" id="kobo.773.2">In order to create a new project, navigate to the drop-down menu on the upper left-hand side and select the </span><strong class="bold"><span class="koboSpan" id="kobo.774.1">New Project</span></strong><span class="koboSpan" id="kobo.775.1"> option. </span><span class="koboSpan" id="kobo.775.2">Give your project a name and then click </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">CREATE</span></strong><span class="koboSpan" id="kobo.777.1">. </span><span class="koboSpan" id="kobo.777.2">You can navigate between different projects using that same drop-down menu:</span></li>
			</ol>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<span class="koboSpan" id="kobo.778.1"><img src="image/B17761_07_024.jpg" alt="Figure 7.24 – A screenshot of the project name and location pane in GCP "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.779.1">Figure 7.24 – A screenshot of the project name and location pane in GCP</span></p>
			<p><span class="koboSpan" id="kobo.780.1">With that last step completed, you are now all set up to take full advantage of the GCP platform. </span><span class="koboSpan" id="kobo.780.2">We will cover a few of the GCP capabilities in this tutorial to get us started in the data science space; however, I highly encourage new users to explore and learn the many tools and resources available here.</span></p>
			<h3><span class="koboSpan" id="kobo.781.1">Uploading data to GCP BigQuery</span></h3>
			<p><span class="koboSpan" id="kobo.782.1">There </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.783.1">are many different ways in which you can upload data within GCP; however, we will focus on one </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.784.1">particular capability unique to the GCP, known as </span><strong class="bold"><span class="koboSpan" id="kobo.785.1">BigQuery</span></strong><span class="koboSpan" id="kobo.786.1">. </span><span class="koboSpan" id="kobo.786.2">The main idea behind BigQuery is that it is a serverless data warehouse with built-in machine learning capabilities that supports the use of queries with the </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">SQL</span></strong><span class="koboSpan" id="kobo.788.1"> language. </span><span class="koboSpan" id="kobo.788.2">If you recall, we previously developed and deployed an </span><strong class="bold"><span class="koboSpan" id="kobo.789.1">AWS RDS</span></strong><span class="koboSpan" id="kobo.790.1"> to manage our data using an </span><strong class="bold"><span class="koboSpan" id="kobo.791.1">EC2</span></strong><span class="koboSpan" id="kobo.792.1"> instance as a server, whereas BigQuery, on the other hand, operates using a serverless architecture. </span><span class="koboSpan" id="kobo.792.2">We can set up BigQuery and start uploading our data in a few simple steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.793.1">Using the navigation menu on the left-hand side of the page, scroll down to the </span><strong class="bold"><span class="koboSpan" id="kobo.794.1">Products</span></strong><span class="koboSpan" id="kobo.795.1"> section, hover over the </span><strong class="bold"><span class="koboSpan" id="kobo.796.1">BigQuery</span></strong><span class="koboSpan" id="kobo.797.1"> option, and select </span><strong class="bold"><span class="koboSpan" id="kobo.798.1">SQL workspace</span></strong><span class="koboSpan" id="kobo.799.1">. </span><span class="koboSpan" id="kobo.799.2">Given that this is the first time you are using this tool, you may need to activate the API. </span><span class="koboSpan" id="kobo.799.3">This will be true for all tools that you have never used before:</span><div id="_idContainer200" class="IMG---Figure"><span class="koboSpan" id="kobo.800.1"><img src="image/B17761_07_025.jpg" alt="Figure 7.25 – A screenshot of the BigQuery menu in GCP "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.801.1">Figure 7.25 – A screenshot of the BigQuery menu in GCP</span></p></li>
				<li><span class="koboSpan" id="kobo.802.1">Within this list, you will find the project that you created in the previous section. </span><span class="koboSpan" id="kobo.802.2">Click on the options button to the right and select </span><strong class="bold"><span class="koboSpan" id="kobo.803.1">Create dataset</span></strong><span class="koboSpan" id="kobo.804.1">. </span><span class="koboSpan" id="kobo.804.2">In this menu, give the dataset a name, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.805.1">protein_structure_sequence</span></strong><span class="koboSpan" id="kobo.806.1">, leaving all the other options as their default values. </span><span class="koboSpan" id="kobo.806.2">You can then click </span><strong class="bold"><span class="koboSpan" id="kobo.807.1">Create dataset</span></strong><span class="koboSpan" id="kobo.808.1">. </span></li>
				<li><span class="koboSpan" id="kobo.809.1">On the left-hand menu, you will see the newly created dataset listed under the project name. </span><span class="koboSpan" id="kobo.809.2">If you click </span><strong class="bold"><span class="koboSpan" id="kobo.810.1">Options</span></strong><span class="koboSpan" id="kobo.811.1"> followed by </span><strong class="bold"><span class="koboSpan" id="kobo.812.1">Open</span></strong><span class="koboSpan" id="kobo.813.1">, you will be directed to the dataset's main page. </span><span class="koboSpan" id="kobo.813.2">Within this page, you will find information relating to that particular dataset. </span><span class="koboSpan" id="kobo.813.3">Let's now go ahead and create a new table here by clicking the </span><strong class="bold"><span class="koboSpan" id="kobo.814.1">Create Table</span></strong><span class="koboSpan" id="kobo.815.1"> option at the top. </span><span class="koboSpan" id="kobo.815.2">Change </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.816.1">the source to reflect the upload option and navigate to the CSV file pertaining to the protein classifications from RCSB PDB. </span><span class="koboSpan" id="kobo.816.2">Give the table a new name, and while leaving all other options as their default values, click </span><strong class="bold"><span class="koboSpan" id="kobo.817.1">Create Table</span></strong><span class="koboSpan" id="kobo.818.1">:</span><div id="_idContainer201" class="IMG---Figure"><span class="koboSpan" id="kobo.819.1"><img src="image/B17761_07_026.jpg" alt="Figure 7.26 – A screenshot of the Create table pane in GCP "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.820.1">Figure 7.26 – A screenshot of the Create table pane in GCP</span></p></li>
				<li><span class="koboSpan" id="kobo.821.1">If you navigate back to Explorer, you will see the newly created table listed under the dataset, which is listed under your project:</span></li>
			</ol>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<span class="koboSpan" id="kobo.822.1"><img src="image/B17761_07_027.jpg" alt="Figure 7.27 – An example of the table created within a dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.823.1">Figure 7.27 – An example of the table created within a dataset</span></p>
			<p><span class="koboSpan" id="kobo.824.1">If you </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.825.1">managed to follow all of these steps correctly, you should now have data available for you to use in BigQuery. </span><span class="koboSpan" id="kobo.825.2">In the following section, we will prepare a new notebook and start parsing some of our data in this dataset.</span></p>
			<h3><span class="koboSpan" id="kobo.826.1">Creating a notebook in GCP</span></h3>
			<p><span class="koboSpan" id="kobo.827.1">In this </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.828.1">section, we will create a notebook equivalent to that of the Jupyter notebooks we have been using to carry out our data science work on. </span><span class="koboSpan" id="kobo.828.2">We can get a new notebook set up in a few simple steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.829.1">In the navigation pane on the left-hand side of the screen, scroll down to the </span><strong class="bold"><span class="koboSpan" id="kobo.830.1">ARTIFICIAL INTELLIGENCE</span></strong><span class="koboSpan" id="kobo.831.1"> section, hover over </span><strong class="bold"><span class="koboSpan" id="kobo.832.1">AI Platform</span></strong><span class="koboSpan" id="kobo.833.1">, and select the </span><strong class="bold"><span class="koboSpan" id="kobo.834.1">Notebooks</span></strong><span class="koboSpan" id="kobo.835.1"> option. </span><span class="koboSpan" id="kobo.835.2">Remember that you may need to activate this API once again if you have not done so already:</span><div id="_idContainer203" class="IMG---Figure"><span class="koboSpan" id="kobo.836.1"><img src="image/B17761_07_028.jpg" alt="Figure 7.28 – A visual of the AI Platform menu "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.837.1">Figure 7.28 – A visual of the AI Platform menu</span></p></li>
				<li><span class="koboSpan" id="kobo.838.1">Next, navigate to the top of the screen and select the </span><strong class="bold"><span class="koboSpan" id="kobo.839.1">New Instance</span></strong><span class="koboSpan" id="kobo.840.1"> option. </span><span class="koboSpan" id="kobo.840.2">There are many different options available for you depending on your </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.841.1">needs. </span><span class="koboSpan" id="kobo.841.2">For the purposes of this tutorial, we can select the first option for </span><strong class="bold"><span class="koboSpan" id="kobo.842.1">Python 3</span></strong><span class="koboSpan" id="kobo.843.1">:</span><div id="_idContainer204" class="IMG---Figure"><span class="koboSpan" id="kobo.844.1"><img src="image/B17761_07_029.jpg" alt="Figure 7.29 – A screenshot of the instance options "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.845.1">Figure 7.29 – A screenshot of the instance options</span></p><p><span class="koboSpan" id="kobo.846.1">If you are familiar with notebook instances and are comfortable customizing them, I recommend creating a customized instance to suit your exact needs.</span></p></li>
				<li><span class="koboSpan" id="kobo.847.1">Once the notebook is created and the instance is online, you will be able to see it in the main </span><strong class="bold"><span class="koboSpan" id="kobo.848.1">Notebook Instances</span></strong><span class="koboSpan" id="kobo.849.1"> section. </span><span class="koboSpan" id="kobo.849.2">Go ahead and click on the </span><strong class="bold"><span class="koboSpan" id="kobo.850.1">OPEN JUPYTERLAB</span></strong><span class="koboSpan" id="kobo.851.1"> button. </span><span class="koboSpan" id="kobo.851.2">A new window will open up containing Jupyter Lab:</span><div id="_idContainer205" class="IMG---Figure"><span class="koboSpan" id="kobo.852.1"><img src="image/B17761_07_030.jpg" alt="Figure 7.30 – A screenshot of the instance menu "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.853.1">Figure 7.30 – A screenshot of the instance menu</span></p></li>
				<li><span class="koboSpan" id="kobo.854.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.855.1">home</span></strong><span class="koboSpan" id="kobo.856.1"> directory, create a new directory called </span><strong class="source-inline"><span class="koboSpan" id="kobo.857.1">biotech-machine-learning</span></strong><span class="koboSpan" id="kobo.858.1"> for us to save our notebooks in. </span><span class="koboSpan" id="kobo.858.2">Open the directory and create a new </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.859.1">notebook by clicking the </span><strong class="bold"><span class="koboSpan" id="kobo.860.1">Python 3</span></strong><span class="koboSpan" id="kobo.861.1"> notebook option on the right:</span></li>
			</ol>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<span class="koboSpan" id="kobo.862.1"><img src="image/B17761_07_031.jpg" alt="Figure 7.31 – A screenshot of Jupyter Lab on GCP "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.863.1">Figure 7.31 – A screenshot of Jupyter Lab on GCP</span></p>
			<p><span class="koboSpan" id="kobo.864.1">With the instance now provisioned and the notebook created, you are now all set to run all of your data science models on GCP. </span><span class="koboSpan" id="kobo.864.2">Let's now take a closer look at the data and begin training a few machine learning models.</span></p>
			<h3><span class="koboSpan" id="kobo.865.1">Using auto-sklearn in GCP Notebooks</span></h3>
			<p><span class="koboSpan" id="kobo.866.1">If you open your newly created notebook, you see the very familiar environment of Jupyter </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.867.1">Lab that we have been working with all along. </span><span class="koboSpan" id="kobo.867.2">The two main benefits here are that we now have the ability </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.868.1">to manage our datasets within this same environment and can provision larger resources to process our data relative to the few </span><strong class="bold"><span class="koboSpan" id="kobo.869.1">CPUs</span></strong><span class="koboSpan" id="kobo.870.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.871.1">GPUs</span></strong><span class="koboSpan" id="kobo.872.1"> we have on our local machines.</span></p>
			<p><span class="koboSpan" id="kobo.873.1">Recall that our main objective for getting to this state is to be able to develop a classification model to correctly classify proteins based on some input features. </span><span class="koboSpan" id="kobo.873.2">The dataset we are working with is known as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">real-world</span></strong><span class="koboSpan" id="kobo.875.1"> dataset in the sense that it is not well organized, has missing values, may contain too much data, and will need some preprocessing prior to the development of any model.</span></p>
			<p><span class="koboSpan" id="kobo.876.1">Let's go ahead and start by importing a few necessary libraries:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.877.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.878.1">import numpy as np</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.879.1">from google.cloud import bigquery</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.880.1">import missingno as msno</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.881.1">from sklearn.metrics import classification_report</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.882.1">import ast</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.883.1">import autosklearn.classification</span></p>
			<p><span class="koboSpan" id="kobo.884.1">Next, let's now import our dataset from BigQuery. </span><span class="koboSpan" id="kobo.884.2">We can do that directly here in the notebook by instantiating a client using the BigQuery class of the Google Cloud library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.885.1">client = bigquery.Client(location="US")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.886.1">print("Client creating using default project: {}".format(client.project))</span></p>
			<p><span class="koboSpan" id="kobo.887.1">Next, we can go ahead and query our data using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.888.1">SELECT</span></strong><span class="koboSpan" id="kobo.889.1"> command in the </span><strong class="bold"><span class="koboSpan" id="kobo.890.1">SQL</span></strong><span class="koboSpan" id="kobo.891.1"> language. </span><span class="koboSpan" id="kobo.891.2">We can simply begin by querying all the data in our dataset. </span><span class="koboSpan" id="kobo.891.3">In the following code snippet, we will query the data using SQL, and convert the results to a dataframe:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.892.1">query = """</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.893.1">    SELECT *</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.894.1">    FROM `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups`</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.895.1">"""</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.896.1">query_job = client.query(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.897.1">    query,</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.898.1">    location="US",</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.899.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.900.1">df = query_job.to_dataframe()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.901.1">print(df.shape)</span></p>
			<p><span class="koboSpan" id="kobo.902.1">Once converted to a more manageable dataframe, we can see that the dataset we are </span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.903.1">working with is quite extensive, with nearly 140,000 rows and 14 columns of data. </span><span class="koboSpan" id="kobo.903.2">Immediately, we notice </span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.904.1">that one of the columns is called </span><strong class="source-inline"><span class="koboSpan" id="kobo.905.1">classification</span></strong><span class="koboSpan" id="kobo.906.1">. </span><span class="koboSpan" id="kobo.906.2">Let's take a look at the unique number of classes in this dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.907.1">n_unique()</span></strong><span class="koboSpan" id="kobo.908.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.909.1">df.classification.nunique()</span></p>
			<p><span class="koboSpan" id="kobo.910.1">We notice that there are 5,050 different classes! </span><span class="koboSpan" id="kobo.910.2">That is a lot for a dataset this size, indicating that we may need to reduce this quite heavily before any analysis. </span><span class="koboSpan" id="kobo.910.3">Before proceeding any further, let's go ahead and drop any and all potential duplicates:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.911.1">dfx = df.drop_duplicates(["structureId"])</span></p>
			<p><span class="koboSpan" id="kobo.912.1">Let's now take a closer look at the top 10 classes in our dataset by count:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.913.1">dfx.classification.value_counts()[:10].sort_values().plot(kind = 'barh')</span></p>
			<p><span class="koboSpan" id="kobo.914.1">The following figure is yielded from the code, showing us the top 10 classes from this dataset:</span></p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<span class="koboSpan" id="kobo.915.1"><img src="image/B17761_07_032.jpg" alt="Figure 7.32 – The top 10 most frequent labels in the dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.916.1">Figure 7.32 – The top 10 most frequent labels in the dataset</span></p>
			<p><span class="koboSpan" id="kobo.917.1">Immediately, we notice that there are two or three classes or proteins that account </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.918.1">for the vast majority of this </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.919.1">data: hydrolase, transferase, and oxidoreductase. </span><span class="koboSpan" id="kobo.919.2">This will be problematic for two reasons:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.920.1">Data should always be </span><strong class="bold"><span class="koboSpan" id="kobo.921.1">balanced</span></strong><span class="koboSpan" id="kobo.922.1"> in the sense that each of the classes should have a roughly equal number of rows.</span></li>
				<li><span class="koboSpan" id="kobo.923.1">As a general rule of thumb, the ratio of classes to observations should be around 50:1, meaning that with 5,050 classes, we would require around 252,500 observations, which we do not currently have.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.924.1">Given these two constraints, we can account for both by simply focusing on developing a model using the first three classes. </span><span class="koboSpan" id="kobo.924.2">For now, we notice that there are quite a few features available to us regardless of the classes at hand. </span><span class="koboSpan" id="kobo.924.3">We can go ahead and take a closer look at the completeness of our features of interest using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">msno</span></strong><span class="koboSpan" id="kobo.926.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.927.1">dfx = dfx[["classification", "residueCount", "resolution", "resolution", "crystallizationTempK", "densityMatthews", "densityPercentSol", "phValue"]]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.928.1">msno.matrix(dfx)</span></p>
			<p><span class="koboSpan" id="kobo.929.1">The following screenshot, representing the completeness of the dataset, is then generated. </span><span class="koboSpan" id="kobo.929.2">Notice that a substantial number of rows for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.930.1">crystallizationTempK</span></strong><span class="koboSpan" id="kobo.931.1"> feature are missing:</span></p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<span class="koboSpan" id="kobo.932.1"><img src="image/B17761_07_033.jpg" alt="Figure 7.33 – A graphical representation showing the completeness of the dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.933.1">Figure 7.33 – A graphical representation showing the completeness of the dataset</span></p>
			<p><span class="koboSpan" id="kobo.934.1">So far within this dataset, we have noted the fact that we will need to reduce the number </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.935.1">of classes to the top two classes </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.936.1">to avoid an imbalanced dataset, and we will also need to address the many rows of data we are missing. </span><span class="koboSpan" id="kobo.936.2">Let's go ahead and prepare our dataset for the development of a few classification models based on our observations. </span><span class="koboSpan" id="kobo.936.3">First, we can go ahead and reduce the dataset using a simple </span><strong class="source-inline"><span class="koboSpan" id="kobo.937.1">groupby</span></strong><span class="koboSpan" id="kobo.938.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.939.1">df2 = dfx.groupby("classification").filter(lambda x: len(x) &gt; 14000)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.940.1">df2.classification.value_counts()</span></p>
			<p><span class="koboSpan" id="kobo.941.1">If we run a quick check on the dataframe using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.942.1">value_counts()</span></strong><span class="koboSpan" id="kobo.943.1"> function, we notice that we were able to reduce it down to the top two labels.</span></p>
			<p><span class="koboSpan" id="kobo.944.1">Alternatively, we can run this same command in </span><strong class="bold"><span class="koboSpan" id="kobo.945.1">SQL</span></strong><span class="koboSpan" id="kobo.946.1"> using a few clever joins. </span><span class="koboSpan" id="kobo.946.2">We can begin with our inner query in which we </span><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">SELECT</span></strong><span class="koboSpan" id="kobo.948.1"> the classification and </span><strong class="source-inline"><span class="koboSpan" id="kobo.949.1">COUNT</span></strong><span class="koboSpan" id="kobo.950.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.951.1">residueCount</span></strong><span class="koboSpan" id="kobo.952.1"> feature and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">GROUP BY</span></strong><span class="koboSpan" id="kobo.954.1"> classification. </span><span class="koboSpan" id="kobo.954.2">Next, we </span><strong class="source-inline"><span class="koboSpan" id="kobo.955.1">INNER JOIN</span></strong><span class="koboSpan" id="kobo.956.1"> that </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.957.1">query with the original </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.958.1">table setting, classification against classification, but filtering using our </span><strong class="source-inline"><span class="koboSpan" id="kobo.959.1">WHERE</span></strong><span class="koboSpan" id="kobo.960.1"> clause:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.961.1">query = """</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.962.1">    SELECT DISTINCT</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.963.1">        dups.*</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.964.1">    FROM (</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.965.1">        SELECT classification, count(residueCount) AS classCount</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.966.1">        FROM `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups`</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.967.1">        GROUP BY classification</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.968.1">    ) AS sub</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.969.1">    INNER JOIN `biotech-project-321515.protein_structure_sequence.dataset_pdb_no_dups` AS dups</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.970.1">        ON sub.classification = dups.classification</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.971.1">    WHERE sub.classCount &gt; 14000</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.972.1">"""</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.973.1">query_job = client.query(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.974.1">    query,</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.975.1">    location="US",</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.976.1">)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.977.1">df2 = query_job.to_dataframe()</span></p>
			<p><span class="koboSpan" id="kobo.978.1">Next, we can go ahead and remove the rows of data with missing values using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.979.1">dropna()</span></strong><span class="koboSpan" id="kobo.980.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.981.1">df2 = df2.dropna()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.982.1">df2.shape</span></p>
			<p><span class="koboSpan" id="kobo.983.1">Immediately, we observe that the size of the dataset has been reduced down to 24,179 observations. </span><span class="koboSpan" id="kobo.983.2">This will be a sufficient dataset to work with when developing our models. </span><span class="koboSpan" id="kobo.983.3">In order to avoid having to process it again, we can write the contents of the dataframe to a new table in the same BigQuery dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.984.1">import pandas_gbq</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.985.1">pandas_gbq.to_gbq(df2, 'protein_structure_sequence.dataset_pdb_no_dups_cleaned', project_id ='biotech-project-321515', if_exists='replace')</span></p>
			<p><span class="koboSpan" id="kobo.986.1">With the </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.987.1">data now prepared, let's go ahead </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.988.1">and develop a model. </span><span class="koboSpan" id="kobo.988.2">We can go ahead and split the input and output data, scale the data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.990.1"> class, and split the data into test and training sets:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.991.1">X = df2.drop(columns=["classification"])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.992.1">y = df2.classification.values.ravel()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.993.1">from sklearn.preprocessing import StandardScaler</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.994.1">scaler = StandardScaler()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.995.1">X_scaled = scaler.fit_transform(X)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.996.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.997.1">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)</span></p>
			<p><span class="koboSpan" id="kobo.998.1">For the automation section, we will use a library called </span><strong class="source-inline"><span class="koboSpan" id="kobo.999.1">autosklearn</span></strong><span class="koboSpan" id="kobo.1000.1">, which can be installed using the command line via </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">pip</span></strong><span class="koboSpan" id="kobo.1002.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1003.1">pip install autosklearn</span></p>
			<p><span class="koboSpan" id="kobo.1004.1">With the library installed, we can go ahead and import the library and instantiate a new instance of that model. </span><span class="koboSpan" id="kobo.1004.2">We will then set a few parameters relating to the time we wish to dedicate to this process and give the model a temporary directory to operate in:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1005.1">import autosklearn.classification</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1006.1">automl = autosklearn.classification.AutoSklearnClassifier(</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1007.1">    time_left_for_this_task=120,</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1008.1">    per_run_time_limit=30,</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1009.1">    tmp_folder='/tmp/autosklearn_protein_tmp5',</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1010.1">)</span></p>
			<p><span class="koboSpan" id="kobo.1011.1">Finally, we can </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.1012.1">go ahead and fit the model </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.1013.1">on our data. </span><span class="koboSpan" id="kobo.1013.2">This process will take a few minutes to run:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1014.1">automl.fit(X_train, y_train, dataset_name='dataset_pdb_no_dups')</span></p>
			<p><span class="koboSpan" id="kobo.1015.1">When the model is complete, we can take a look at the results by printing the leader board:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1016.1">print(automl.leaderboard())</span></p>
			<p><span class="koboSpan" id="kobo.1017.1">Upon printing the leaderboard, we retrieve the following results:</span></p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1018.1"><img src="image/B17761_07_034.jpg" alt="Figure 7.34 – Results of the auto-sklearn model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1019.1">Figure 7.34 – Results of the auto-sklearn model</span></p>
			<p><span class="koboSpan" id="kobo.1020.1">We can also take a look at the top-performing </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">random_forest</span></strong><span class="koboSpan" id="kobo.1022.1"> model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">get_models_with_weights()</span></strong><span class="koboSpan" id="kobo.1024.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1025.1">automl.get_models_with_weights()[0]</span></p>
			<p><span class="koboSpan" id="kobo.1026.1">We can also go ahead and get a few more metrics by making some predictions using the model and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">classification_report()</span></strong><span class="koboSpan" id="kobo.1028.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1029.1">predictions = automl.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1030.1">print("classification_report:", classification_report(y_test, predictions))</span></p>
			<p><span class="koboSpan" id="kobo.1031.1">Upon printing the report, this yields the following results:</span></p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1032.1"><img src="image/B17761_07_035.jpg" alt="Figure 7. 35 – Results of the top-performing auto-sklearn model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1033.1">Figure 7. </span><span class="koboSpan" id="kobo.1033.2">35 – Results of the top-performing auto-sklearn model</span></p>
			<p><span class="koboSpan" id="kobo.1034.1">With that, we managed to develop a machine learning model successfully and automatically </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.1035.1">for our dataset. </span><span class="koboSpan" id="kobo.1035.2">However, the model has not yet been fined-tuned or optimized for this task. </span><span class="koboSpan" id="kobo.1035.3">One challenge </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.1036.1">that I recommend you complete is the process of tuning the various parameters in this model in an attempt to increase our metrics. </span><span class="koboSpan" id="kobo.1036.2">In addition, another challenge would be to try and explore some of the other models we learned about and see whether any of them can beat </span><strong class="source-inline"><span class="koboSpan" id="kobo.1037.1">autosklearn</span></strong><span class="koboSpan" id="kobo.1038.1">. </span><span class="koboSpan" id="kobo.1038.2">Hint: </span><strong class="bold"><span class="koboSpan" id="kobo.1039.1">XGBoost</span></strong><span class="koboSpan" id="kobo.1040.1"> has always been a great model for structured datasets.</span></p>
			<h3><span class="koboSpan" id="kobo.1041.1">Using the AutoML application in GCP</span></h3>
			<p><span class="koboSpan" id="kobo.1042.1">In the </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.1043.1">previous section, we </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.1044.1">used an open source library called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1045.1">auto-sklearn</span></strong><span class="koboSpan" id="kobo.1046.1"> that automates the process in which models can be selected using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">sklearn</span></strong><span class="koboSpan" id="kobo.1048.1"> library. </span><span class="koboSpan" id="kobo.1048.2">However, as we have seen with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1049.1">XGBoost</span></strong><span class="koboSpan" id="kobo.1050.1"> library, there are many other models out there outside of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1051.1">sklearn</span></strong><span class="koboSpan" id="kobo.1052.1"> API. </span><span class="koboSpan" id="kobo.1052.2">GCP offers a robust tool, similar to that of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">auto-sklearn</span></strong><span class="koboSpan" id="kobo.1054.1">, that iterates over a large selection of models </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.1055.1">and methods </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.1056.1">to find the most optimal model for a given dataset. </span><span class="koboSpan" id="kobo.1056.2">Let's go ahead and give this a try:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1057.1">In the navigation menu in GCP, scroll down to the </span><strong class="bold"><span class="koboSpan" id="kobo.1058.1">ARTIFICIAL INTELLIGENCE</span></strong><span class="koboSpan" id="kobo.1059.1"> section, hover over </span><strong class="bold"><span class="koboSpan" id="kobo.1060.1">Tables</span></strong><span class="koboSpan" id="kobo.1061.1">, and select </span><strong class="bold"><span class="koboSpan" id="kobo.1062.1">Datasets</span></strong><span class="koboSpan" id="kobo.1063.1">:</span><div id="_idContainer211" class="IMG---Figure"><span class="koboSpan" id="kobo.1064.1"><img src="image/B17761_07_036.jpg" alt="Figure 7.36 – Selecting Datasets from the ARTIFICIAL INTELLIGENCE menu "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1065.1">Figure 7.36 – Selecting Datasets from the ARTIFICIAL INTELLIGENCE menu</span></p></li>
				<li><span class="koboSpan" id="kobo.1066.1">At the top of the page, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1067.1">New Dataset</span></strong><span class="koboSpan" id="kobo.1068.1"> option. </span><span class="koboSpan" id="kobo.1068.2">At the time this book was written, the beta implementation of the model was available. </span><span class="koboSpan" id="kobo.1068.3">Some of the steps will likely have changed in future implementations:</span><div id="_idContainer212" class="IMG---Figure"><span class="koboSpan" id="kobo.1069.1"><img src="image/B17761_07_037.jpg" alt="Figure 7.37 – A screenshot of the button to create a new dataset "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1070.1">Figure 7.37 – A screenshot of the button to create a new dataset</span></p></li>
				<li><span class="koboSpan" id="kobo.1071.1">Go ahead and give the dataset a name and region and click </span><strong class="bold"><span class="koboSpan" id="kobo.1072.1">Create Dataset</span></strong><span class="koboSpan" id="kobo.1073.1">.</span></li>
				<li><span class="koboSpan" id="kobo.1074.1">We have the option to import our dataset of interest either as a raw CSV file or using BigQuery. </span><span class="koboSpan" id="kobo.1074.2">Go ahead and import our cleaned version of the proteins dataset by specifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.1075.1">projectID</span></strong><span class="koboSpan" id="kobo.1076.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1077.1">datasetID</span></strong><span class="koboSpan" id="kobo.1078.1">, and the table name, and then click </span><strong class="bold"><span class="koboSpan" id="kobo.1079.1">Import</span></strong><span class="koboSpan" id="kobo.1080.1">.</span></li>
				<li><span class="koboSpan" id="kobo.1081.1">In </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.1082.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.1083.1">TRAIN</span></strong><span class="koboSpan" id="kobo.1084.1"> section, you will have the ability to see the tables within this dataset. </span><span class="koboSpan" id="kobo.1084.2">Go </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.1085.1">ahead and specify the </span><strong class="bold"><span class="koboSpan" id="kobo.1086.1">Classification</span></strong><span class="koboSpan" id="kobo.1087.1"> column as the target column and then click </span><strong class="bold"><span class="koboSpan" id="kobo.1088.1">TRAIN MODEL</span></strong><span class="koboSpan" id="kobo.1089.1">:</span><div id="_idContainer213" class="IMG---Figure"><span class="koboSpan" id="kobo.1090.1"><img src="image/B17761_07_038.jpg" alt="Figure 7.38 – An example of the training menu "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1091.1">Figure 7.38 – An example of the training menu</span></p></li>
				<li><span class="koboSpan" id="kobo.1092.1">The model selection process will take some time to complete. </span><span class="koboSpan" id="kobo.1092.2">Upon completion, you will be able to see the results of the model under the </span><strong class="bold"><span class="koboSpan" id="kobo.1093.1">Evaluate</span></strong><span class="koboSpan" id="kobo.1094.1"> tab. </span><span class="koboSpan" id="kobo.1094.2">Here you will get a sense of the classification metrics we have been working with, as well as a few others.</span></li>
			</ol>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1095.1"><img src="image/B17761_07_039.jpg" alt="Figure 7.39 – Results of the trained model showing the metrics "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1096.1">Figure 7.39 – Results of the trained model showing the metrics</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1097.1">GCP</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.1098.1">AutoML</span></strong><span class="koboSpan" id="kobo.1099.1"> is a powerful </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.1100.1">tool that you can use to your advantage when handling a difficult dataset. </span><span class="koboSpan" id="kobo.1100.2">You will find that the implementation of the model is quite robust and generally comprehensive relative to the many options that we, as data scientists, can explore. </span><span class="koboSpan" id="kobo.1100.3">One of the downsides of </span><strong class="bold"><span class="koboSpan" id="kobo.1101.1">AutoML</span></strong><span class="koboSpan" id="kobo.1102.1"> is the fact that the final model is not shared with the user; however, the user does have the ability to test new data and use the model later on. </span><span class="koboSpan" id="kobo.1102.2">We will </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.1103.1">explore another option similar to </span><strong class="bold"><span class="koboSpan" id="kobo.1104.1">AutoML</span></strong><span class="koboSpan" id="kobo.1105.1"> in the following section known as </span><strong class="bold"><span class="koboSpan" id="kobo.1106.1">AutoPilot</span></strong><span class="koboSpan" id="kobo.1107.1"> in </span><strong class="bold"><span class="koboSpan" id="kobo.1108.1">AWS</span></strong><span class="koboSpan" id="kobo.1109.1">. </span><span class="koboSpan" id="kobo.1109.2">Now that we have explored quite a few different models and methods relating to classification, let's go and explore their respective counterparts when it comes to regression.</span></p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.1110.1">Understanding regression in supervised machine learning</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1111.1">Regressions</span></strong><span class="koboSpan" id="kobo.1112.1"> are models </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.1113.1">generally used to determine the relationship or </span><strong class="bold"><span class="koboSpan" id="kobo.1114.1">correlation</span></strong><span class="koboSpan" id="kobo.1115.1"> between </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.1116.1">dependent and independent variables. </span><span class="koboSpan" id="kobo.1116.2">Within the context </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.1117.1">of machine learning, we define regressions as supervised machine learning models that allow for the identification of correlations between two or more variables in order to </span><strong class="bold"><span class="koboSpan" id="kobo.1118.1">generalize</span></strong><span class="koboSpan" id="kobo.1119.1"> or learn from historical data to make predictions on new observations. </span></p>
			<p><span class="koboSpan" id="kobo.1120.1">Within the </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.1121.1">confines of the </span><strong class="bold"><span class="koboSpan" id="kobo.1122.1">biotechnology</span></strong><span class="koboSpan" id="kobo.1123.1"> space, we use regression models to predict values in many different areas.</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1124.1">Predicting the LCAP of a compound ahead of time</span></li>
				<li><span class="koboSpan" id="kobo.1125.1">Predicting titer results further upstream</span></li>
				<li><span class="koboSpan" id="kobo.1126.1">Predicting the isoelectric point of a monoclonal antibody</span></li>
				<li><span class="koboSpan" id="kobo.1127.1">Predicting the decomposition percentages of compounds</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1128.1">Correlations </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.1129.1">are generally </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.1130.1">established between two columns. </span><span class="koboSpan" id="kobo.1130.2">Two columns within a dataset are said to have a strong </span><strong class="bold"><span class="koboSpan" id="kobo.1131.1">correlation</span></strong><span class="koboSpan" id="kobo.1132.1"> when a dependence is observed. </span><span class="koboSpan" id="kobo.1132.2">The specific relationship can be better understood using a linear regression model such that:</span></p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1133.1"><img src="image/B17761_Formula_07_029.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1134.1">In which </span><span class="koboSpan" id="kobo.1135.1"><img src="image/B17761_Formula_07_030.png" alt=""/></span><span class="koboSpan" id="kobo.1136.1"> is the first feature, </span><span class="koboSpan" id="kobo.1137.1"><img src="image/B17761_Formula_07_031.png" alt=""/></span><span class="koboSpan" id="kobo.1138.1"> is the second feature, </span><span class="koboSpan" id="kobo.1139.1"><img src="image/B17761_Formula_07_032.png" alt=""/></span><span class="koboSpan" id="kobo.1140.1"> is a small error term, with </span><span class="koboSpan" id="kobo.1141.1"><img src="image/B17761_Formula_07_033.png" alt=""/></span><span class="koboSpan" id="kobo.1142.1"> and </span><span class="koboSpan" id="kobo.1143.1"><img src="image/B17761_Formula_07_034.png" alt=""/></span><span class="koboSpan" id="kobo.1144.1"> as constants. </span><span class="koboSpan" id="kobo.1144.2">Using this simple equation, we can understand our data more effectively, and calculate any correlation. </span><span class="koboSpan" id="kobo.1144.3">For example, recall earlier we observed a correlation in the toxicity dataset, specifically between the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1145.1">MolWt</span></strong><span class="koboSpan" id="kobo.1146.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1147.1">HeavyAtoms</span></strong><span class="koboSpan" id="kobo.1148.1"> features. </span></p>
			<p><span class="koboSpan" id="kobo.1149.1">The main idea behind any given regression model, unlike its classification counterparts, is to output a continuous value rather than a class or category. </span><span class="koboSpan" id="kobo.1149.2">For example, we could use a number of columns in the toxicity dataset in an attempt to predict other columns in the same dataset. </span></p>
			<p><span class="koboSpan" id="kobo.1150.1">There are many different types of regression models commonly used in the data science </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.1151.1">space. </span><span class="koboSpan" id="kobo.1151.2">There </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.1152.1">are linear regressions that focus on </span><strong class="bold"><span class="koboSpan" id="kobo.1153.1">optimizing</span></strong><span class="koboSpan" id="kobo.1154.1"> a linear relationship between a set of variables, logistic regression that acts more as a binary classifier rather than regressors, and ensemble models that combine the predictive power of several base estimators, among many others. </span><span class="koboSpan" id="kobo.1154.2">We can see some examples in </span><em class="italic"><span class="koboSpan" id="kobo.1155.1">Figure 7.40</span></em><span class="koboSpan" id="kobo.1156.1">:</span></p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1157.1"><img src="image/B17761_07_040.jpg" alt="Figure 7.40 – Different types of regression models "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1158.1">Figure 7.40 – Different types of regression models</span></p>
			<p><span class="koboSpan" id="kobo.1159.1">Over the course of this section, we will explore a few of these models as we investigate the application of a few regression models using the toxicity dataset. </span><span class="koboSpan" id="kobo.1159.2">Let's go ahead and prepare our data.</span></p>
			<p><span class="koboSpan" id="kobo.1160.1">We can begin by importing our libraries of interest:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1161.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1162.1">import numpy as np</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1163.1">from sklearn.metrics import r2_score</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1164.1">from sklearn.metrics import mean_squared_error</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1165.1">import matplotlib.pyplot as plt</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1166.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1167.1">sns.set_theme(color_codes=True)</span></p>
			<p><span class="koboSpan" id="kobo.1168.1">Next, we </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.1169.1">can go ahead and import </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.1170.1">our dataset and drop the missing rows. </span><span class="koboSpan" id="kobo.1170.2">For practice, I suggest you upload this dataset to </span><strong class="bold"><span class="koboSpan" id="kobo.1171.1">BigQuery</span></strong><span class="koboSpan" id="kobo.1172.1">, just as we did in the previous section, and query your data using </span><strong class="bold"><span class="koboSpan" id="kobo.1173.1">SQL</span></strong><span class="koboSpan" id="kobo.1174.1"> via a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1175.1">SELECT</span></strong><span class="koboSpan" id="kobo.1176.1"> statement:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1177.1">df = pd.read_csv("../../datasets/dataset_toxicity_sd.csv")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1178.1">df = df.dropna()</span></p>
			<p><span class="koboSpan" id="kobo.1179.1">Next, we can split our data into input and output values, and scale our data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1180.1">MinMaxScaler()</span></strong><span class="koboSpan" id="kobo.1181.1"> class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1182.1">sklearn</span></strong><span class="koboSpan" id="kobo.1183.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1184.1">X = df[["Heteroatoms", "MolWt", "HeavyAtoms", "NHOH", "HAcceptors", "HDonors"]]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1185.1">y = df.TPSA.values.ravel()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1186.1">from sklearn.preprocessing import MinMaxScaler</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1187.1">scaler = MinMaxScaler()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1188.1">X_scaled = scaler.fit_transform(X)</span></p>
			<p><span class="koboSpan" id="kobo.1189.1">Finally, we can split the dataset into training and test data:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1190.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1191.1">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)</span></p>
			<p><span class="koboSpan" id="kobo.1192.1">With our dataset prepared, we are now ready to go ahead and start exploring a few regression models.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.1193.1">Exploring different regression models</span></h2>
			<p><span class="koboSpan" id="kobo.1194.1">There are many types of regression methods that can be used with a given dataset. </span><span class="koboSpan" id="kobo.1194.2">We can </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.1195.1">think of regression as falling into four main categories: linear regressions, logistic regressions, ensemble regressions, and finally, boosted regressions. </span><span class="koboSpan" id="kobo.1195.2">Throughout the next section, we will be exploring examples from each of these categories, starting with linear regression.</span></p>
			<h3><span class="koboSpan" id="kobo.1196.1">Single and multiple linear regression</span></h3>
			<p><span class="koboSpan" id="kobo.1197.1">In many of the datasets you will likely encounter in your career, you oftentimes find that </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.1198.1">some of the features </span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.1199.1">exhibit some type of correlation vis-à-vis one </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.1200.1">another. </span><span class="koboSpan" id="kobo.1200.2">Earlier in this chapter, we discussed the idea of a correlation between two features as a dependence </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.1201.1">of one feature upon another, which can be calculated using the Pearson correlation metric known as </span><strong class="bold"><span class="koboSpan" id="kobo.1202.1">R2</span></strong><span class="koboSpan" id="kobo.1203.1">. </span><span class="koboSpan" id="kobo.1203.2">Over the last few chapters, we have looked at the idea of correlations in a few different ways, including heats maps and pairplots.</span></p>
			<p><span class="koboSpan" id="kobo.1204.1">Using the dataset we just prepared, we can take a look at a few correlations using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1205.1">seaborn</span></strong><span class="koboSpan" id="kobo.1206.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1207.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1208.1">fig = sns.pairplot(data=df[["Heteroatoms", "MolWt", "HeavyAtoms"]])</span></p>
			<p><span class="koboSpan" id="kobo.1209.1">This yields the following figure:</span></p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1210.1"><img src="image/B17761_07_041.jpg" alt="Figure 7.41 – Results of the pairplot function using the toxicity dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1211.1">Figure 7.41 – Results of the pairplot function using the toxicity dataset</span></p>
			<p><span class="koboSpan" id="kobo.1212.1">We can </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.1213.1">see that there are a few </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.1214.1">correlations in our dataset already. </span><span class="koboSpan" id="kobo.1214.2">Using </span><strong class="bold"><span class="koboSpan" id="kobo.1215.1">simple linear regression</span></strong><span class="koboSpan" id="kobo.1216.1">, we can take advantage of this correlation in the sense that we </span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.1217.1">can use one variable to predict what the </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.1218.1">other will most likely be. </span><span class="koboSpan" id="kobo.1218.2">For example, if X was the independent variable, and Y was the dependent variable, we can define the linear relationship between the two as:</span></p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1219.1"><img src="image/B17761_Formula_07_035.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1220.1">In which </span><em class="italic"><span class="koboSpan" id="kobo.1221.1">m</span></em><span class="koboSpan" id="kobo.1222.1"> is the slope and </span><em class="italic"><span class="koboSpan" id="kobo.1223.1">c</span></em><span class="koboSpan" id="kobo.1224.1"> is the </span><em class="italic"><span class="koboSpan" id="kobo.1225.1">y</span></em><span class="koboSpan" id="kobo.1226.1"> intercept. </span><span class="koboSpan" id="kobo.1226.2">This equation should be familiar to you based on some of the earlier content in this book, as well as your math classes in high school. </span><span class="koboSpan" id="kobo.1226.3">Using this relationship, our objective will be to optimize this line </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.1227.1">relative to our data in order </span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.1228.1">to determine the values for </span><em class="italic"><span class="koboSpan" id="kobo.1229.1">m</span></em><span class="koboSpan" id="kobo.1230.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1231.1">c</span></em><span class="koboSpan" id="kobo.1232.1"> using a </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.1233.1">method known as the least </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.1234.1">squares method.</span></p>
			<p><span class="koboSpan" id="kobo.1235.1">Before we can </span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.1236.1">discuss the </span><strong class="bold"><span class="koboSpan" id="kobo.1237.1">least squares method</span></strong><span class="koboSpan" id="kobo.1238.1">, let's first discuss the idea of a </span><strong class="bold"><span class="koboSpan" id="kobo.1239.1">loss function</span></strong><span class="koboSpan" id="kobo.1240.1">. </span><span class="koboSpan" id="kobo.1240.2">A loss function within the context of machine </span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.1241.1">learning is a measure of the difference between our calculated </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.1242.1">and expected values. </span><span class="koboSpan" id="kobo.1242.2">For example, let's examine the </span><strong class="bold"><span class="koboSpan" id="kobo.1243.1">quadratic loss function</span></strong><span class="koboSpan" id="kobo.1244.1">, commonly used to calculate loss within a regression model, which we can define as:</span></p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1245.1"><img src="image/B17761_Formula_07_036.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1246.1">By now, I would hope you recognize this function from our discussions in the </span><em class="italic"><span class="koboSpan" id="kobo.1247.1">Measuring success</span></em><span class="koboSpan" id="kobo.1248.1"> section. </span><span class="koboSpan" id="kobo.1248.2">Can you tell me where we last used this?</span></p>
			<p><span class="koboSpan" id="kobo.1249.1">Now that we have discussed the idea of loss functions, let's take a closer look at the </span><strong class="bold"><span class="koboSpan" id="kobo.1250.1">least squares method</span></strong><span class="koboSpan" id="kobo.1251.1">. </span><span class="koboSpan" id="kobo.1251.2">The main idea behind this mathematical method is to determine a line of best fit for a given set of data, as demonstrated by the correlations we just saw, by minimizing the loss. </span><span class="koboSpan" id="kobo.1251.3">To fully explain the concepts behind this equation we would need to discuss partial derivatives and what not. </span><span class="koboSpan" id="kobo.1251.4">For the purposes of simplicity, we will simply define the least squares method as a process for </span><strong class="bold"><span class="koboSpan" id="kobo.1252.1">minimizing</span></strong><span class="koboSpan" id="kobo.1253.1"> loss in order to determine a line of best fit for a given dataset.</span></p>
			<p><span class="koboSpan" id="kobo.1254.1">We can divide linear regressions into two main categories: </span><strong class="bold"><span class="koboSpan" id="kobo.1255.1">simple linear regression</span></strong><span class="koboSpan" id="kobo.1256.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1257.1">multiple linear regression</span></strong><span class="koboSpan" id="kobo.1258.1">. </span><span class="koboSpan" id="kobo.1258.2">The main idea here concerns the number of features we will be training the model with. </span><span class="koboSpan" id="kobo.1258.3">If we are simply training the model based on one feature, we will be developing a simple linear regression. </span><span class="koboSpan" id="kobo.1258.4">On the other hand, if we train the model using multiple features, we would be training a multiple regression model.</span></p>
			<p><span class="koboSpan" id="kobo.1259.1">Whether you are training a simple or multiple regression model, the process and desired outcomes are generally the same. </span><span class="koboSpan" id="kobo.1259.2">Ideally, the output of our models when </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.1260.1">plotted against the actual </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.1261.1">values should result in a linear line showing </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.1262.1">a strong correlation in our data, as depicted in </span><em class="italic"><span class="koboSpan" id="kobo.1263.1">Figure 7.42</span></em><span class="koboSpan" id="kobo.1264.1">:</span></p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1265.1"><img src="image/B17761_07_042.jpg" alt="Figure 7.42 – A simple linear line showing the ideal correlation "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1266.1">Figure 7.42 – A simple linear line showing the ideal correlation</span></p>
			<p><span class="koboSpan" id="kobo.1267.1">Let's go </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.1268.1">ahead and explore the development of a multiple linear regression model. </span><span class="koboSpan" id="kobo.1268.2">With the data imported in the previous section, we can import the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1269.1">LinearRegression</span></strong><span class="koboSpan" id="kobo.1270.1"> class for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1271.1">sklearn</span></strong><span class="koboSpan" id="kobo.1272.1">, fit it with our training data, and make a prediction using the test dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1273.1">from sklearn.linear_model import LinearRegression</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1274.1">reg = LinearRegression().fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1275.1">y_pred = reg.predict(X_test)</span></p>
			<p><span class="koboSpan" id="kobo.1276.1">Next, we can go ahead and use the actual and predicted values to both calculate the </span><strong class="bold"><span class="koboSpan" id="kobo.1277.1">R2</span></strong><span class="koboSpan" id="kobo.1278.1"> value and visualize on the graph. </span><span class="koboSpan" id="kobo.1278.2">In addition, we will also capture the </span><strong class="bold"><span class="koboSpan" id="kobo.1279.1">MSE</span></strong><span class="koboSpan" id="kobo.1280.1"> metric:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1281.1">p = sns.jointplot(x=y_test, y=y_pred, kind="reg")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1282.1">p.fig.suptitle(f"Linear Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1283.1">p.fig.subplots_adjust(top=0.90)</span></p>
			<p><span class="koboSpan" id="kobo.1284.1">The code will then yield the following figure:</span></p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1285.1"><img src="image/B17761_07_043.jpg" alt="Figure 7.43 – Results of the linear regression model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1286.1">Figure 7.43 – Results of the linear regression model</span></p>
			<p><span class="koboSpan" id="kobo.1287.1">Immediately, we </span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.1288.1">notice that this </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.1289.1">simple linear regression model was quite effective </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.1290.1">in making predictions </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.1291.1">on our dataset. </span><span class="koboSpan" id="kobo.1291.2">Notice that this model not only used one feature, but used all of the features to make its prediction. </span><span class="koboSpan" id="kobo.1291.3">We notice from the graph that the vast majority of our data is localized at the bottom left. </span><span class="koboSpan" id="kobo.1291.4">This is not ideal for a regression model as we would prefer the values to be equally dispersed across all bounds; however, it is important to remember that in the biotechnology space, you will almost always encounter real-world data in which you will observe items such as these. </span></p>
			<p><span class="koboSpan" id="kobo.1292.1">If you are following along in </span><strong class="bold"><span class="koboSpan" id="kobo.1293.1">Jupyter Notebooks</span></strong><span class="koboSpan" id="kobo.1294.1">, go ahead and reduce the dataset </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.1295.1">to only one input feature, scale and split the data, train a simple linear regression, and compare the results to the multiple linear regression. </span><span class="koboSpan" id="kobo.1295.2">Does the correlation of our predictions and actual values increase or decrease?</span></p>
			<h3><span class="koboSpan" id="kobo.1296.1">Logistic regression</span></h3>
			<p><span class="koboSpan" id="kobo.1297.1">Recall that in the linear regression section, we discussed the methodology </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.1298.1">in which a single linear </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.1299.1">line can be used to predict a value based on a correlated feature as input. </span><span class="koboSpan" id="kobo.1299.2">We outlined the linear equation as follows:</span></p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1300.1"><img src="image/B17761_Formula_07_037.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1301.1">In some instances, the relationship between the data and the desired output may not be best represented by a linear model, but rather a non-linear one:</span></p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1302.1"><img src="image/B17761_07_044.jpg" alt="Figure 7.44 – A simple sigmoid curve "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1303.1">Figure 7.44 – A simple sigmoid curve</span></p>
			<p><span class="koboSpan" id="kobo.1304.1">Although known as </span><strong class="bold"><span class="koboSpan" id="kobo.1305.1">logistic regression</span></strong><span class="koboSpan" id="kobo.1306.1">, this regression </span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.1307.1">is mostly used as a </span><strong class="bold"><span class="koboSpan" id="kobo.1308.1">binary classification</span></strong><span class="koboSpan" id="kobo.1309.1"> algorithm. </span><span class="koboSpan" id="kobo.1309.2">However, the main focus here is that the word </span><em class="italic"><span class="koboSpan" id="kobo.1310.1">logistic</span></em><span class="koboSpan" id="kobo.1311.1"> is referring </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.1312.1">to the </span><strong class="bold"><span class="koboSpan" id="kobo.1313.1">logistic function</span></strong><span class="koboSpan" id="kobo.1314.1">, also known as the </span><strong class="bold"><span class="koboSpan" id="kobo.1315.1">Sigmoid</span></strong><span class="koboSpan" id="kobo.1316.1"> function, represented </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.1317.1">as:</span></p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1318.1"><img src="image/B17761_Formula_07_038.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1319.1">With </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.1320.1">this in mind, we will want to use </span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.1321.1">this function to make predictions in our dataset. </span><span class="koboSpan" id="kobo.1321.2">If we wanted to determine whether or not a compound was toxic given a specific input value, we could calculate the weighted sum of inputs such that:</span></p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1322.1"><img src="image/B17761_Formula_07_039.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1323.1">This would allow us to calculate the probability of toxicity such that:</span></p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1324.1"><img src="image/B17761_Formula_07_040.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.1325.1">Using this probability</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.1326.1">, a final prediction can be made, and an output value assigned. </span><span class="koboSpan" id="kobo.1326.2">Go ahead and implement this model using the protein classification dataset in the previous section and compare the results that you find to those of the other classifiers.</span></p>
			<h3><span class="koboSpan" id="kobo.1327.1">Decision trees and random forest regression</span></h3>
			<p><span class="koboSpan" id="kobo.1328.1">Similar </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.1329.1">to its classification counterpart, </span><strong class="bold"><span class="koboSpan" id="kobo.1330.1">Decision Tree Regressions</span></strong><span class="koboSpan" id="kobo.1331.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1332.1">DTRs</span></strong><span class="koboSpan" id="kobo.1333.1">) are </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.1334.1">commonly used machine learning models implementing nearly the same internal mechanisms that decision tree classifiers use. </span><span class="koboSpan" id="kobo.1334.2">The only difference </span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.1335.1">between the </span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.1336.1">models is the fact that regressors output continuous numerical values, whereas classifiers output discrete classes. </span></p>
			<p><span class="koboSpan" id="kobo.1337.1">Similarly, another model known as </span><strong class="bold"><span class="koboSpan" id="kobo.1338.1">Random Forest Regressors</span></strong><span class="koboSpan" id="kobo.1339.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1340.1">RFRs</span></strong><span class="koboSpan" id="kobo.1341.1">) also exists and operates similarly to its classification counterparts. </span><span class="koboSpan" id="kobo.1341.2">This model is also an ensemble method in which each tree is trained as a separate model and subsequently averaged.</span></p>
			<p><span class="koboSpan" id="kobo.1342.1">Let's go ahead and implement an RFR using this dataset. </span><span class="koboSpan" id="kobo.1342.2">Just as we have previously done, we will first create an instance of the model, fit it to our data, make a prediction, and visualize the results:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1343.1">from sklearn.ensemble import RandomForestRegressor</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1344.1">reg = RandomForestRegressor().fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1345.1">y_pred = reg.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1346.1">p = sns.jointplot(x=y_test, y=y_pred, kind="reg")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1347.1">p.fig.suptitle(f"RandomForestRegressor Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1348.1"># p.ax_joint.collections[0].set_alpha(0)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1349.1"># p.fig.tight_layout()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1350.1">p.fig.subplots_adjust(top=0.90)</span></p>
			<p><span class="koboSpan" id="kobo.1351.1">With the model developed, we can visualize the results using the following plot:</span></p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1352.1"><img src="image/B17761_07_045.jpg" alt="Figure 7.45 – Results of the random forest regression model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1353.1">Figure 7.45 – Results of the random forest regression model</span></p>
			<p><span class="koboSpan" id="kobo.1354.1">Notice </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.1355.1">that while the </span><strong class="bold"><span class="koboSpan" id="kobo.1356.1">R2</span></strong><span class="koboSpan" id="kobo.1357.1"> remained relatively </span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.1358.1">the same, the </span><strong class="bold"><span class="koboSpan" id="kobo.1359.1">MSE</span></strong><span class="koboSpan" id="kobo.1360.1"> did decrease substantially relative to the first linear model. </span><span class="koboSpan" id="kobo.1360.2">We can improve </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.1361.1">these scores by </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.1362.1">tuning the model's parameters. </span><span class="koboSpan" id="kobo.1362.2">We can demonstrate this using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1363.1">max_depth</span></strong><span class="koboSpan" id="kobo.1364.1"> parameter:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1365.1">for i in range(1,10):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1366.1">    reg = RandomForestRegressor(max_depth=i)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1367.1">                       .fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1368.1">    y_pred = reg.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1369.1">    print("depth =", i, </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1370.1">          "score=", r2_score(y_test, y_pred), </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1371.1">          "mse = ", mean_squared_error(y_test, y_pred))</span></p>
			<p><span class="koboSpan" id="kobo.1372.1">The output for this code is shown below, indicating that a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1373.1">max_depth</span></strong><span class="koboSpan" id="kobo.1374.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1375.1">8</span></strong><span class="koboSpan" id="kobo.1376.1"> would likely be optimal given that it results in an </span><strong class="bold"><span class="koboSpan" id="kobo.1377.1">R2</span></strong><span class="koboSpan" id="kobo.1378.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1379.1">0.967</span></strong><span class="koboSpan" id="kobo.1380.1"> and an </span><strong class="bold"><span class="koboSpan" id="kobo.1381.1">MSE</span></strong><span class="koboSpan" id="kobo.1382.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1383.1">248.133</span></strong><span class="koboSpan" id="kobo.1384.1">:</span></p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1385.1"><img src="image/B17761_07_046.jpg" alt="Figure 7.46 – Results of the random forest regression model with differing max_depth "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1386.1">Figure 7.46 – Results of the random forest regression model with differing max_depth</span></p>
			<p><span class="koboSpan" id="kobo.1387.1">Similar </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.1388.1">to classification, decision trees for </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.1389.1">regression tend to be </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.1390.1">excellent methods </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.1391.1">for allowing you to develop models while trying to avoid overfitting your data. </span><span class="koboSpan" id="kobo.1391.2">Another great benefit of </span><strong class="bold"><span class="koboSpan" id="kobo.1392.1">DTR</span></strong><span class="koboSpan" id="kobo.1393.1"> models, when using the </span><strong class="bold"><span class="koboSpan" id="kobo.1394.1">sklearn</span></strong><span class="koboSpan" id="kobo.1395.1"> API, is gaining insights into feature importance directly from the model. </span><span class="koboSpan" id="kobo.1395.2">Let's go ahead and demonstrate this:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1396.1">features = X.columns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1397.1">importances = reg.feature_importances_</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1398.1">indices = np.argsort(importances)[-9:]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1399.1">plt.title('Feature Importances')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1400.1">plt.barh(range(len(indices)), importances[indices],  </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1401.1">                           color='royalblue', </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1402.1">                           align='center')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1403.1">plt.yticks(range(len(indices)),[features[i] for i in indices])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1404.1">plt.xlabel('Relative Importance')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1405.1">plt.show()</span></p>
			<p><span class="koboSpan" id="kobo.1406.1">With that complete, this yields the following diagram:</span></p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1407.1"><img src="image/B17761_07_047.jpg" alt="Figure 7.47 – Feature importance of the random forest regression model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1408.1">Figure 7.47 – Feature importance of the random forest regression model</span></p>
			<p><span class="koboSpan" id="kobo.1409.1">Looking at this figure, we can see that the top three features that had the biggest impact </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.1410.1">on the development of this model </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.1411.1">were </span><strong class="source-inline"><span class="koboSpan" id="kobo.1412.1">HDonors</span></strong><span class="koboSpan" id="kobo.1413.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1414.1">Heteroatoms</span></strong><span class="koboSpan" id="kobo.1415.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1416.1">HAcceptors</span></strong><span class="koboSpan" id="kobo.1417.1">. </span><span class="koboSpan" id="kobo.1417.2">Although this example of feature importance </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.1418.1">was developed </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.1419.1">using the RFR model, we can theoretically use this with many other models. </span><span class="koboSpan" id="kobo.1419.2">One library in particular that has gained a great deal of importance in the field concerning the idea of feature importance is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1420.1">SHAP</span></strong><span class="koboSpan" id="kobo.1421.1"> library. </span><span class="koboSpan" id="kobo.1421.2">It is highly recommended that you take a glance at this library and the many wonderful features (no pun intended) that it has to offer.</span></p>
			<h3><span class="koboSpan" id="kobo.1422.1">XGBoost regression</span></h3>
			<p><span class="koboSpan" id="kobo.1423.1">Similar </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.1424.1">to the </span><strong class="bold"><span class="koboSpan" id="kobo.1425.1">XGBoost</span></strong><span class="koboSpan" id="kobo.1426.1"> classification model we investigated </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.1427.1">in the previous section, we also have regression implementation, which allows us to predict a value rather than a category. </span><span class="koboSpan" id="kobo.1427.2">We can go ahead and implement this easily, just as we did in the previous section:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1428.1">import xgboost as xg</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1429.1">reg = xg.XGBRegressor(objective ='reg:linear',n_estimators = 1000).fit(X_train, y_train)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1430.1">y_pred = reg.predict(X_test)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1431.1">p = sns.jointplot(x=y_test, y=y_pred, kind="reg")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1432.1">p.fig.suptitle(f"xgboost Regression, R2 = {round(r2_score(y_test, y_pred), 3)}, MSE = {round(mean_squared_error(y_test, y_pred), 2)}")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1433.1">p.fig.subplots_adjust(top=0.90)</span></p>
			<p><span class="koboSpan" id="kobo.1434.1">With </span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.1435.1">the code completed, this yields </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.1436.1">the following figure:</span></p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1437.1"><img src="image/B17761_07_048.jpg" alt="Figure 7.48 – Results of the XGBoost regression model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1438.1">Figure 7.48 – Results of the XGBoost regression model</span></p>
			<p><span class="koboSpan" id="kobo.1439.1">You will notice that this implementation of the model gave us a very respectable </span><strong class="bold"><span class="koboSpan" id="kobo.1440.1">R2</span></strong><span class="koboSpan" id="kobo.1441.1"> value in the context of our actual and predicted values and managed to yield an </span><strong class="bold"><span class="koboSpan" id="kobo.1442.1">MSE</span></strong><span class="koboSpan" id="kobo.1443.1"> of 282.79, which is slightly less than some of the other models we've attempted in this chapter. </span><span class="koboSpan" id="kobo.1443.2">With the model completed, let's now move on to see how we could use some of the automated machine learning functionality provided in AWS in the following tutorial.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.1444.1">Tutorial: Regression for property prediction</span></h2>
			<p><span class="koboSpan" id="kobo.1445.1">Over the course of this chapter, we have reviewed some of the most common (and popular) regression models as they relate to the prediction of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1446.1">TPSA</span></strong><span class="koboSpan" id="kobo.1447.1"> feature using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1448.1">toxicity</span></strong><span class="koboSpan" id="kobo.1449.1"> dataset. </span><span class="koboSpan" id="kobo.1449.2">In the previous section pertaining to classification, we created a GCP instance and used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1450.1">auto-sklearn</span></strong><span class="koboSpan" id="kobo.1451.1"> library to automatically identify one of the top machine </span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.1452.1">learning models for a given dataset. </span><span class="koboSpan" id="kobo.1452.2">In this tutorial, we will create an </span><strong class="bold"><span class="koboSpan" id="kobo.1453.1">AWS SageMaker</span></strong><span class="koboSpan" id="kobo.1454.1"> notebook, query our dataset from RDS, and run the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1455.1">auto-sklearn</span></strong><span class="koboSpan" id="kobo.1456.1"> library in a similar manner. </span><span class="koboSpan" id="kobo.1456.2">In addition, we will also explore an even more powerful </span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.1457.1">automated machine learning method using </span><strong class="bold"><span class="koboSpan" id="kobo.1458.1">AWS Autopilot</span></strong><span class="koboSpan" id="kobo.1459.1">. </span><span class="koboSpan" id="kobo.1459.2">In one of the earlier chapters, we used </span><strong class="bold"><span class="koboSpan" id="kobo.1460.1">AWS RDS</span></strong><span class="koboSpan" id="kobo.1461.1"> to launch </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.1462.1">a relation database to host our toxicity dataset. </span><span class="koboSpan" id="kobo.1462.2">Using that same </span><strong class="bold"><span class="koboSpan" id="kobo.1463.1">AWS</span></strong><span class="koboSpan" id="kobo.1464.1"> account, we will now go ahead and get started.</span></p>
			<h3><span class="koboSpan" id="kobo.1465.1">Creating a SageMaker notebook in AWS</span></h3>
			<p><span class="koboSpan" id="kobo.1466.1">Similar </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.1467.1">to the creation of a notebook in </span><strong class="bold"><span class="koboSpan" id="kobo.1468.1">GCP</span></strong><span class="koboSpan" id="kobo.1469.1">, we can create a </span><strong class="bold"><span class="koboSpan" id="kobo.1470.1">SageMaker</span></strong><span class="koboSpan" id="kobo.1471.1"> notebook in </span><strong class="bold"><span class="koboSpan" id="kobo.1472.1">AWS</span></strong><span class="koboSpan" id="kobo.1473.1"> in a few simple steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1474.1">Navigate to the AWS Management Console on the front page. </span><span class="koboSpan" id="kobo.1474.2">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1475.1">Services</span></strong><span class="koboSpan" id="kobo.1476.1"> drop-down menu and select </span><strong class="bold"><span class="koboSpan" id="kobo.1477.1">Amazon SageMaker</span></strong><span class="koboSpan" id="kobo.1478.1"> under the </span><strong class="bold"><span class="koboSpan" id="kobo.1479.1">Machine Learning</span></strong><span class="koboSpan" id="kobo.1480.1"> section:</span><div id="_idContainer236" class="IMG---Figure"><span class="koboSpan" id="kobo.1481.1"><img src="image/B17761_07_049.jpg" alt="Figure 7.49 – The list of services provided by AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1482.1">Figure 7.49 – The list of services provided by AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1483.1">On </span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.1484.1">the left-hand side of the page, click the </span><strong class="bold"><span class="koboSpan" id="kobo.1485.1">Notebook</span></strong><span class="koboSpan" id="kobo.1486.1"> drop-down menu and then select the </span><strong class="bold"><span class="koboSpan" id="kobo.1487.1">Notebook instances</span></strong><span class="koboSpan" id="kobo.1488.1"> button:</span><div id="_idContainer237" class="IMG---Figure"><span class="koboSpan" id="kobo.1489.1"><img src="image/B17761_07_050.jpg" alt="Figure 7.50 – A screenshot of the notebook menu "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1490.1">Figure 7.50 – A screenshot of the notebook menu</span></p></li>
				<li><span class="koboSpan" id="kobo.1491.1">Within the notebook instances menu, click on the orange button called </span><strong class="bold"><span class="koboSpan" id="kobo.1492.1">Create notebook instance</span></strong><span class="koboSpan" id="kobo.1493.1">:</span><div id="_idContainer238" class="IMG---Figure"><span class="koboSpan" id="kobo.1494.1"><img src="image/B17761_07_051.jpg" alt="Figure 7.51 – A screenshot of the Create notebook instance button "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1495.1">Figure 7.51 – A screenshot of the Create notebook instance button</span></p></li>
				<li><span class="koboSpan" id="kobo.1496.1">Let's now go ahead and give the notebook instance a name, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1497.1">biotech-machine-learning</span></strong><span class="koboSpan" id="kobo.1498.1">. </span><span class="koboSpan" id="kobo.1498.2">We can leave the instance type as the default selection of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1499.1">ml.t2.medium</span></strong><span class="koboSpan" id="kobo.1500.1">. </span><span class="koboSpan" id="kobo.1500.2">This is a medium-tier instance and is more than </span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.1501.1">enough for the purposes of our demo today:</span><div id="_idContainer239" class="IMG---Figure"><span class="koboSpan" id="kobo.1502.1"><img src="image/B17761_07_052.jpg" alt="Figure 7.52 – A screenshot of the notebook instance settings "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1503.1">Figure 7.52 – A screenshot of the notebook instance settings</span></p></li>
				<li><span class="koboSpan" id="kobo.1504.1">Under the </span><strong class="bold"><span class="koboSpan" id="kobo.1505.1">Permissions and encryption</span></strong><span class="koboSpan" id="kobo.1506.1"> section, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1507.1">Create a new role</span></strong><span class="koboSpan" id="kobo.1508.1"> option for the IAM role section. </span><span class="koboSpan" id="kobo.1508.2">The main idea behind IAM roles is the concept of granting certain users or roles the ability to interact with specific AWS resources. </span><span class="koboSpan" id="kobo.1508.3">For example, we could allow this role to also have access to some but not all S3 buckets. </span><span class="koboSpan" id="kobo.1508.4">For the purposes of this tutorial, let's go ahead and grant this role access to any S3 bucket. </span><span class="koboSpan" id="kobo.1508.5">Go ahead and click on </span><strong class="bold"><span class="koboSpan" id="kobo.1509.1">Create role</span></strong><span class="koboSpan" id="kobo.1510.1">:</span><div id="_idContainer240" class="IMG---Figure"><span class="koboSpan" id="kobo.1511.1"><img src="image/B17761_07_053.jpg" alt="Figure 7.53 – Creating an IAM role in AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1512.1">Figure 7.53 – Creating an IAM role in AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1513.1">Leaving </span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.1514.1">all the other options as they are, go ahead and click on </span><strong class="bold"><span class="koboSpan" id="kobo.1515.1">Create notebook instance</span></strong><span class="koboSpan" id="kobo.1516.1">. </span><span class="koboSpan" id="kobo.1516.2">You will be redirected back to the </span><strong class="bold"><span class="koboSpan" id="kobo.1517.1">Notebook instance</span></strong><span class="koboSpan" id="kobo.1518.1"> menu where you will see your newly created instance in a </span><strong class="bold"><span class="koboSpan" id="kobo.1519.1">Pending</span></strong><span class="koboSpan" id="kobo.1520.1"> state. </span><span class="koboSpan" id="kobo.1520.2">Within a few moments, you will notice that status change to </span><strong class="bold"><span class="koboSpan" id="kobo.1521.1">InService</span></strong><span class="koboSpan" id="kobo.1522.1">. </span><span class="koboSpan" id="kobo.1522.2">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1523.1">Open JupyterLab</span></strong><span class="koboSpan" id="kobo.1524.1"> button to the right of the status:</span><div id="_idContainer241" class="IMG---Figure"><span class="koboSpan" id="kobo.1525.1"><img src="image/B17761_07_054.jpg" alt="Figure 7.54 – The options to open a Jupyter notebook or Jupyer lab in AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1526.1">Figure 7.54 – The options to open a Jupyter notebook or Jupyer lab in AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1527.1">Once again, you will find yourself in the familiar Jupyter environment you have been working in. </span><p><strong class="bold"><span class="koboSpan" id="kobo.1528.1">AWS SageMaker</span></strong><span class="koboSpan" id="kobo.1529.1"> is a great resource for you to use at a very low cost. </span><span class="koboSpan" id="kobo.1529.2">Within this space, you will be able to run all of the Python commands and libraries you have learned throughout this book. </span><span class="koboSpan" id="kobo.1529.3">You can create directories to organize your files and scripts and access them anywhere in the world without having to have your laptop with you. </span><span class="koboSpan" id="kobo.1529.4">In addition, you will also have access to almost 100 SageMaker sample notebooks and starter code for you to use.</span></p></li>
			</ol>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1530.1"><img src="image/B17761_07_055.jpg" alt="Figure 7.55 – An example list of the SageMaker starter notebooks "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1531.1">Figure 7.55 – An example list of the SageMaker starter notebooks</span></p>
			<p><span class="koboSpan" id="kobo.1532.1">With this final step complete, we now have a fully working notebook instance. </span><span class="koboSpan" id="kobo.1532.2">In the following section, we will use SageMaker to import data and start running our models. </span></p>
			<h3><span class="koboSpan" id="kobo.1533.1">Creating a notebook and importing data in AWS</span></h3>
			<p><span class="koboSpan" id="kobo.1534.1">Given </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.1535.1">that we are once again working </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.1536.1">in our familiar Jupyter space, we can easily create a notebook by selecting one of the many options on the right-hand side and start running our code. </span><span class="koboSpan" id="kobo.1536.2">Let's go ahead and get started:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1537.1">We can begin by selecting the </span><strong class="bold"><span class="koboSpan" id="kobo.1538.1">conda_python3</span></strong><span class="koboSpan" id="kobo.1539.1"> option on the right-hand side, creating a new notebook for us in our current directory:</span><p class="figure-caption"><span class="koboSpan" id="kobo.1540.1">    </span></p><div id="_idContainer243" class="IMG---Figure"><span class="koboSpan" id="kobo.1541.1"><img src="image/B17761_07_056.jpg" alt="Figure 7.56 – A screenshot of conda_python3 "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1542.1">Figure 7.56 – A screenshot of conda_python3</span></p></li>
				<li><span class="koboSpan" id="kobo.1543.1">With </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.1544.1">the notebook prepared, we </span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.1545.1">will need to install a few libraries to get started. </span><span class="koboSpan" id="kobo.1545.2">Go ahead and install </span><strong class="source-inline"><span class="koboSpan" id="kobo.1546.1">mysql-connector</span></strong><span class="koboSpan" id="kobo.1547.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1548.1">pymysql</span></strong><span class="koboSpan" id="kobo.1549.1"> using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1550.1">pip</span></strong><span class="koboSpan" id="kobo.1551.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.1552.1">pip install mysql-connector pymysql</span></p></li>
				<li><span class="koboSpan" id="kobo.1553.1">Next, we will need to import a few things:</span><p class="source-code"><span class="koboSpan" id="kobo.1554.1">import pandas as pd</span></p><p class="source-code"><span class="koboSpan" id="kobo.1555.1">import mysql.connector</span></p><p class="source-code"><span class="koboSpan" id="kobo.1556.1">from sqlalchemy import create_engine</span></p><p class="source-code"><span class="koboSpan" id="kobo.1557.1">import sys</span></p><p class="source-code"><span class="koboSpan" id="kobo.1558.1">import seaborn as sns</span></p></li>
				<li><span class="koboSpan" id="kobo.1559.1">Now, we can define some of the items we will need to query our data, as we did previously in </span><a href="B17761_03_Final_JM_ePub.xhtml#_idTextAnchor050"><em class="italic"><span class="koboSpan" id="kobo.1560.1">Chapter 3</span></em></a><span class="koboSpan" id="kobo.1561.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1562.1">Getting Started with SQL and Relational Databases</span></em><span class="koboSpan" id="kobo.1563.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.1564.1">ENDPOINT="toxicitydataset.xxxxxx.us-east-2.rds.amazonaws.com"</span></p><p class="source-code"><span class="koboSpan" id="kobo.1565.1">PORT="3306"</span></p><p class="source-code"><span class="koboSpan" id="kobo.1566.1">USR="admin"</span></p><p class="source-code"><span class="koboSpan" id="kobo.1567.1">DBNAME="toxicity_db_tutorial"</span></p><p class="source-code"><span class="koboSpan" id="kobo.1568.1">PASSWORD = "xxxxxxxxxxxxxxxxxx"</span></p></li>
				<li><span class="koboSpan" id="kobo.1569.1">Next, we can go ahead and create a connection to our </span><strong class="bold"><span class="koboSpan" id="kobo.1570.1">RDS</span></strong><span class="koboSpan" id="kobo.1571.1"> instance:</span><p class="source-code"><span class="koboSpan" id="kobo.1572.1">db_connection_str = 'mysql+pymysql://{USR}:{PASSWORD}@{ENDPOINT}:{PORT}/{DBNAME}'.format(USR=USR, PASSWORD=PASSWORD, ENDPOINT=ENDPOINT, PORT=PORT, DBNAME=DBNAME)</span></p><p class="source-code"><span class="koboSpan" id="kobo.1573.1">db_connection = create_engine(db_connection_str)</span></p></li>
				<li><span class="koboSpan" id="kobo.1574.1">Finally, we can go ahead and query our data using a basic </span><strong class="bold"><span class="koboSpan" id="kobo.1575.1">SQL</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.1576.1">SELECT</span></strong><span class="koboSpan" id="kobo.1577.1"> statement:</span><p class="source-code"><span class="koboSpan" id="kobo.1578.1">df = pd.read_sql('SELECT * FROM dataset_toxicity_sd', con=db_connection)</span></p></li>
			</ol>
			<p><span class="koboSpan" id="kobo.1579.1">With </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.1580.1">that complete, we are now </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.1581.1">able to query our data directly from </span><strong class="bold"><span class="koboSpan" id="kobo.1582.1">AWS RDS</span></strong><span class="koboSpan" id="kobo.1583.1">. </span><span class="koboSpan" id="kobo.1583.2">As you begin to explore new models in the realm of data science, you will need a place to store and organize your data. </span><span class="koboSpan" id="kobo.1583.3">Selecting a platform such as </span><strong class="bold"><span class="koboSpan" id="kobo.1584.1">AWS RDS</span></strong><span class="koboSpan" id="kobo.1585.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1586.1">AWS S3</span></strong><span class="koboSpan" id="kobo.1587.1">, or even </span><strong class="bold"><span class="koboSpan" id="kobo.1588.1">GCP</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.1589.1">BigQuery</span></strong><span class="koboSpan" id="kobo.1590.1"> will help you organize your data and studies.</span></p>
			<h3><span class="koboSpan" id="kobo.1591.1">Running auto-sklearn using the toxicity dataset</span></h3>
			<p><span class="koboSpan" id="kobo.1592.1">Now </span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.1593.1">that we have our data in a working notebook, let's go ahead and use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1594.1">auto-sklean</span></strong><span class="koboSpan" id="kobo.1595.1"> library to identify a model best suited for our given dataset:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1596.1">We can begin by installing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1597.1">auto-sklearn</span></strong><span class="koboSpan" id="kobo.1598.1"> library in our </span><strong class="bold"><span class="koboSpan" id="kobo.1599.1">SageMaker</span></strong><span class="koboSpan" id="kobo.1600.1"> instance:</span><p class="source-code"><span class="koboSpan" id="kobo.1601.1">pip install auto-sklearn</span></p></li>
				<li><span class="koboSpan" id="kobo.1602.1">Next, we can isolate our input features and output values and scale them accordingly:</span><p class="source-code"><span class="koboSpan" id="kobo.1603.1">X = df[["Heteroatoms", "MolWt", "HeavyAtoms", "NHOH", "HAcceptors", "HDonors"]]</span></p><p class="source-code"><span class="koboSpan" id="kobo.1604.1">y = df.TPSA.values.ravel()</span></p><p class="source-code"><span class="koboSpan" id="kobo.1605.1">from sklearn.preprocessing import MinMaxScaler</span></p><p class="source-code"><span class="koboSpan" id="kobo.1606.1">scaler = MinMaxScaler()</span></p><p class="source-code"><span class="koboSpan" id="kobo.1607.1">X_scaled = scaler.fit_transform(X)</span></p></li>
				<li><span class="koboSpan" id="kobo.1608.1">With </span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.1609.1">the data scaled, we can now go ahead and separate our training and test datasets:</span><p class="source-code"><span class="koboSpan" id="kobo.1610.1">from sklearn.model_selection import train_test_split</span></p><p class="source-code"><span class="koboSpan" id="kobo.1611.1">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)</span></p></li>
				<li><span class="koboSpan" id="kobo.1612.1">Finally, we can import the regression implementation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1613.1">sklearn</span></strong><span class="koboSpan" id="kobo.1614.1">, adjust the parameters, and fit the model to our dataset:</span><p class="source-code"><span class="koboSpan" id="kobo.1615.1">import autosklearn.regression</span></p><p class="source-code"><span class="koboSpan" id="kobo.1616.1">automl = autosklearn.regression.AutoSklearnRegressor(</span></p><p class="source-code"><span class="koboSpan" id="kobo.1617.1">    time_left_for_this_task=120,</span></p><p class="source-code"><span class="koboSpan" id="kobo.1618.1">    per_run_time_limit=30,</span></p><p class="source-code"><span class="koboSpan" id="kobo.1619.1">    tmp_folder='/tmp/autosklearn_regression_example_tmp2')</span></p><p class="source-code"><span class="koboSpan" id="kobo.1620.1">automl.fit(X_train, y_train, dataset_name='dataset_toxicity')</span></p></li>
				<li><span class="koboSpan" id="kobo.1621.1">Once the model is done, we can take a look at the top-performing candidate model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1622.1">get_models_with_weights()</span></strong><span class="koboSpan" id="kobo.1623.1"> function:</span><p class="source-code"><span class="koboSpan" id="kobo.1624.1">automl.get_models_with_weights()[0]</span></p></li>
				<li><span class="koboSpan" id="kobo.1625.1">Lastly, we can go ahead and get a sense of the model's performance using the </span><strong class="bold"><span class="koboSpan" id="kobo.1626.1">R2</span></strong><span class="koboSpan" id="kobo.1627.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1628.1">MSE</span></strong><span class="koboSpan" id="kobo.1629.1"> metrics, as we have done previously:</span><p class="source-code"><span class="koboSpan" id="kobo.1630.1">from sklearn.metrics import r2_score, mean_squared_error</span></p><p class="source-code"><span class="koboSpan" id="kobo.1631.1">predictions = automl.predict(X_test)</span></p><p class="source-code"><span class="koboSpan" id="kobo.1632.1">p = sns.jointplot(x=y_test, y=predictions, kind="reg")</span></p><p class="source-code"><span class="koboSpan" id="kobo.1633.1">p.fig.suptitle(f"automl, R2 = {round(r2_score(y_test, predictions), 3)}, MSE = {round(mean_squared_error(y_test, predictions), 2)}")</span></p><p class="source-code"><span class="koboSpan" id="kobo.1634.1"># p.ax_joint.collections[0].set_alpha(0)</span></p><p class="source-code"><span class="koboSpan" id="kobo.1635.1"># p.fig.tight_layout()</span></p><p class="source-code"><span class="koboSpan" id="kobo.1636.1">p.fig.subplots_adjust(top=0.90)</span></p><p><span class="koboSpan" id="kobo.1637.1">Upon </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.1638.1">plotting the output, this yields the following results:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1639.1"><img src="image/B17761_07_057.jpg" alt="Figure 7.57 – Results of the AutoML model showing the correlation of 0.97 "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1640.1">Figure 7.57 – Results of the AutoML model showing the correlation of 0.97</span></p>
			<p><span class="koboSpan" id="kobo.1641.1">We can see from the figure above that the actual results and predicted results line </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.1642.1">up quite nicely, giving us an R2 value of approximately 0.97, showing a strong correlation. </span><span class="koboSpan" id="kobo.1642.2">In the following section, we will explore the process of automating parts of the model development process using AWS Autopilot.</span></p>
			<h3><span class="koboSpan" id="kobo.1643.1">Automated regression using AWS Autopilot</span></h3>
			<p><span class="koboSpan" id="kobo.1644.1">Many </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.1645.1">different tools and applications </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.1646.1">can be found in the </span><strong class="bold"><span class="koboSpan" id="kobo.1647.1">AWS Management Console</span></strong><span class="koboSpan" id="kobo.1648.1">, offering solutions to many data science and computer </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.1649.1">science problems any developer will likely encounter. </span><span class="koboSpan" id="kobo.1649.2">There is one tool in particular that has stood out and begun to grow in </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.1650.1">popularity among the data science community known as </span><strong class="bold"><span class="koboSpan" id="kobo.1651.1">AWS Autopilot</span></strong><span class="koboSpan" id="kobo.1652.1">. </span><span class="koboSpan" id="kobo.1652.2">The purpose of </span><strong class="bold"><span class="koboSpan" id="kobo.1653.1">AWS Autopilot</span></strong><span class="koboSpan" id="kobo.1654.1"> is to help automate some of the tasks generally undertaken in any given data science project. </span><span class="koboSpan" id="kobo.1654.2">We can see a visual representation of this in </span><em class="italic"><span class="koboSpan" id="kobo.1655.1">Figure 7.58</span></em><span class="koboSpan" id="kobo.1656.1">:</span></p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1657.1"><img src="image/B17761_07_058.jpg" alt="Figure 7.58 – The Autopilot pipeline "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1658.1">Figure 7.58 – The Autopilot pipeline</span></p>
			<p><span class="koboSpan" id="kobo.1659.1">Users are able to load in their datasets, identify a few parameters, and let the model take it from there as it identifies the top-performing models, optimizes a few parameters, and even generates sample code for the user to take and optimize even further. </span><span class="koboSpan" id="kobo.1659.2">Let's go ahead and demonstrate the use of this model using the same dataset:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1660.1">We can begin by creating a SageMaker Studio instance by navigating to the SageMaker console and selecting the </span><strong class="bold"><span class="koboSpan" id="kobo.1661.1">Open SageMaker Studio</span></strong><span class="koboSpan" id="kobo.1662.1"> button on the right. </span><span class="koboSpan" id="kobo.1662.2">Using the quick start option, the default settings, and a new </span><strong class="bold"><span class="koboSpan" id="kobo.1663.1">IAM role</span></strong><span class="koboSpan" id="kobo.1664.1">, click the </span><strong class="bold"><span class="koboSpan" id="kobo.1665.1">Submit</span></strong><span class="koboSpan" id="kobo.1666.1"> button. </span><span class="koboSpan" id="kobo.1666.2">After a few moments, the instance will provision. </span><span class="koboSpan" id="kobo.1666.3">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1667.1">Open Studio</span></strong><span class="koboSpan" id="kobo.1668.1"> button:</span><div id="_idContainer246" class="IMG---Figure"><span class="koboSpan" id="kobo.1669.1"><img src="image/B17761_07_059.jpg" alt="Figure 7.59 – The Open Studio option in AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1670.1">Figure 7.59 – The Open Studio option in AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1671.1">While </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.1672.1">the instance is </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.1673.1">provisioning, let's upload our dataset to </span><strong class="bold"><span class="koboSpan" id="kobo.1674.1">S3</span></strong><span class="koboSpan" id="kobo.1675.1">. </span><span class="koboSpan" id="kobo.1675.2">Using the </span><strong class="bold"><span class="koboSpan" id="kobo.1676.1">AWS Management Console</span></strong><span class="koboSpan" id="kobo.1677.1">, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1678.1">S3</span></strong><span class="koboSpan" id="kobo.1679.1"> option under the storage section. </span><span class="koboSpan" id="kobo.1679.2">Here, you can create buckets, which act as online storage spaces. </span><span class="koboSpan" id="kobo.1679.3">Create a new bucket called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1680.1">biotech-machine-learning</span></strong><span class="koboSpan" id="kobo.1681.1"> while keeping all the other options at their default values.</span><div id="_idContainer247" class="IMG---Figure"><span class="koboSpan" id="kobo.1682.1"><img src="image/B17761_07_060.jpg" alt="Figure 7.60 – Creating a bucket in AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1683.1">Figure 7.60 – Creating a bucket in AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1684.1">Once created, open the bucket and click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1685.1">Upload</span></strong><span class="koboSpan" id="kobo.1686.1"> button. </span><span class="koboSpan" id="kobo.1686.2">Then, upload the CSV file of the reduced and cleaned proteins dataset.</span><div id="_idContainer248" class="IMG---Figure"><span class="koboSpan" id="kobo.1687.1"><img src="image/B17761_07_061.jpg" alt="Figure 7.61 – Uploading files in AWS "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1688.1">Figure 7.61 – Uploading files in AWS</span></p></li>
				<li><span class="koboSpan" id="kobo.1689.1">With the dataset uploaded, let's now head back to SageMaker. </span><span class="koboSpan" id="kobo.1689.2">Using the navigation pane on the left, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1690.1">SageMaker Components and Registries</span></strong><span class="koboSpan" id="kobo.1691.1"> tab. </span><span class="koboSpan" id="kobo.1691.2">Using the drop-down menu, select </span><strong class="bold"><span class="koboSpan" id="kobo.1692.1">Experiments and trials</span></strong><span class="koboSpan" id="kobo.1693.1">, and then click the </span><strong class="bold"><span class="koboSpan" id="kobo.1694.1">Create Autopilot Experiment</span></strong><span class="koboSpan" id="kobo.1695.1"> button:</span><div id="_idContainer249" class="IMG---Figure"><span class="koboSpan" id="kobo.1696.1"><img src="image/B17761_07_062.jpg" alt="Figure 7.62 – Creating SageMaker resources in AWS SageMaker Studio "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1697.1">Figure 7.62 – Creating SageMaker resources in AWS SageMaker Studio</span></p></li>
				<li><span class="koboSpan" id="kobo.1698.1">Let's move </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.1699.1">on and give </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.1700.1">the experiment a name, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1701.1">dataset-pdb-nodups-cleaned</span></strong><span class="koboSpan" id="kobo.1702.1">.</span></li>
				<li><span class="koboSpan" id="kobo.1703.1">In the </span><strong class="bold"><span class="koboSpan" id="kobo.1704.1">CONNECT YOUR DATA</span></strong><span class="koboSpan" id="kobo.1705.1"> section, select the S3 bucket name you created earlier, as well as the dataset filename:</span><div id="_idContainer250" class="IMG---Figure"><span class="koboSpan" id="kobo.1706.1"><img src="image/B17761_07_063.jpg" alt="Figure 7.63 – Connecting data to the experiment  "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1707.1">Figure 7.63 – Connecting data to the experiment </span></p></li>
				<li><span class="koboSpan" id="kobo.1708.1">Next, select </span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.1709.1">the target </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.1710.1">column, which in our case, is the classification column:</span><div id="_idContainer251" class="IMG---Figure"><span class="koboSpan" id="kobo.1711.1"><img src="image/B17761_07_064.jpg" alt="Figure 7.64 – Selecting a target column for the model training process "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1712.1">Figure 7.64 – Selecting a target column for the model training process</span></p></li>
				<li><span class="koboSpan" id="kobo.1713.1">Finally, you can now go ahead and disable the </span><strong class="bold"><span class="koboSpan" id="kobo.1714.1">Auto deploy</span></strong><span class="koboSpan" id="kobo.1715.1"> option and click </span><strong class="bold"><span class="koboSpan" id="kobo.1716.1">Create Experiment</span></strong><span class="koboSpan" id="kobo.1717.1">. </span><span class="koboSpan" id="kobo.1717.2">Similar to GCP's </span><strong class="bold"><span class="koboSpan" id="kobo.1718.1">AutoML</span></strong><span class="koboSpan" id="kobo.1719.1">, the application will identify a set of models deemed to be the best fit for your given dataset. </span><span class="koboSpan" id="kobo.1719.2">You have the option to select between </span><strong class="bold"><span class="koboSpan" id="kobo.1720.1">Pilot</span></strong><span class="koboSpan" id="kobo.1721.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.1722.1">Complete</span></strong><span class="koboSpan" id="kobo.1723.1">.</span><p><span class="koboSpan" id="kobo.1724.1">A complete experiment will train and tune the model while allowing users to view the details and statistics in real time. </span><span class="koboSpan" id="kobo.1724.2">It will go through various phases, such as preprocessing, candidate definition generation, feature engineering, model tuning, and report generation.</span></p></li>
				<li><span class="koboSpan" id="kobo.1725.1">Upon completing the process, a dashboard with all the trained models and their associated metrics will be presented, as depicted in </span><em class="italic"><span class="koboSpan" id="kobo.1726.1">Figure 7.65</span></em><span class="koboSpan" id="kobo.1727.1">. </span><span class="koboSpan" id="kobo.1727.2">Users can explore the models and deploy them in a few simple clicks.</span></li>
			</ol>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1728.1"><img src="image/B17761_07_065.jpg" alt="Figure 7.65 – Results of the Autopilot model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1729.1">Figure 7.65 – Results of the Autopilot model</span></p>
			<p><span class="koboSpan" id="kobo.1730.1">AWS </span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.1731.1">Autopilot is a robust and useful </span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.1732.1">tool that every data scientist can utilize when facing a difficult dataset. </span><span class="koboSpan" id="kobo.1732.2">It not only assists in identifying the best model for a given dataset, but can also help preprocess the data, tune the model, and provide sample code for users to use.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.1733.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1734.1">Congratulations! </span><span class="koboSpan" id="kobo.1734.2">We finally made it to the end of a very dense, yet very informative chapter. </span><span class="koboSpan" id="kobo.1734.3">In this chapter, we learned quite a few different things. </span><span class="koboSpan" id="kobo.1734.4">In the first half of this chapter, we explored the realm of classification and demonstrated the application of a number of models using the single-cell RNA dataset – a classical application in the field of biotechnology and life sciences. </span><span class="koboSpan" id="kobo.1734.5">We learned about a number of different models, including KNNs, SVMs, decision trees, random forests, and XGBoost. </span><span class="koboSpan" id="kobo.1734.6">We then moved our data and code to GCP, where we stored our data in BigQuery, and provisioned a notebook instance to run our code in. </span><span class="koboSpan" id="kobo.1734.7">In addition, we learned how to automate some of the manual and labor-intensive parts of the model development process as it pertains to the protein classification dataset using auto-sklearn. </span><span class="koboSpan" id="kobo.1734.8">Finally, we took advantage of GCP's AutoML application to develop a classification model for our dataset.</span></p>
			<p><span class="koboSpan" id="kobo.1735.1">In the second half of this chapter, we explored the realm of regression as it pertains to the toxicity dataset. </span><span class="koboSpan" id="kobo.1735.2">We explored the idea of correlation within data and learned a few important regression models too. </span><span class="koboSpan" id="kobo.1735.3">Some of the models we looked at included simple and multiple linear regression, logistic regression, decision tree regressors, and an XGBoost regressor as well. </span><span class="koboSpan" id="kobo.1735.4">We then moved our code to AWS's SageMaker platform and used the previously provisioned RDS to query our data and run auto-sklearn for regression as well. </span><span class="koboSpan" id="kobo.1735.5">Finally, we implemented AWS Autopilot's automated machine learning model for the toxicity dataset.</span></p>
			<p><span class="koboSpan" id="kobo.1736.1">So far, we have spent much of our time developing machine learning models using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1737.1">sklearn</span></strong><span class="koboSpan" id="kobo.1738.1"> library. </span><span class="koboSpan" id="kobo.1738.2">However, not every dataset can be classified or regressed using machine learning – sometimes, a more powerful set of models will be needed. </span><span class="koboSpan" id="kobo.1738.3">For datasets such as those, we can turn to the field of deep learning, which will be our focus for the next chapter.</span></p>
		</div>
	</body></html>