<html><head></head><body>
		<div id="_idContainer137">
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/>Chapter 7: Feast Alternatives and ML Best Practices</h1>
			<p>In the last chapter, we discussed how to use Amazon Managed Workflows with Apache Airflow for orchestration and productionizing online and batch models with <strong class="bold">Feast</strong>. So far in this book, we have been discussing one feature store – Feast. However, there are a bunch of feature stores available on the market today. In this chapter, we will look at a few of them and discuss how they are different from Feast and the advantages or disadvantages of using them over Feast. </p>
			<p>In this chapter, we will try out one other feature store, specifically Amazon SageMaker. We will take the same feature set that we generated while building the customer <strong class="bold">lifetime value (LTV)</strong> model and ingest it into SageMaker Feature Store and also run a couple of queries. The reason for choosing AWS over other feature stores such as Tecton, Hopworks, and H2O.ai is the easy access to the trial version. However, choosing the right feature store for you depends on the tools and infrastructure that you already have and more, which we will discuss in this chapter.  </p>
			<p>The aim of this chapter is to give you the gist of what is available on the market and how it differs from self-managed feature stores such as Feast. We will also discuss the similarities and differences between these feature stores. The other aspect that I want to discuss in this chapter is the best practices in ML development. Irrespective of the tools/software we use for ML development, there are a few things that can be universally adopted by all of us to improve ML engineering.</p>
			<p>In this chapter, we will discuss the following topics: </p>
			<ul>
				<li>The available feature stores on the market</li>
				<li>Feature management with SageMaker Feature Store </li>
				<li>ML best practices</li>
			</ul>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor114"/>Technical requirements</h1>
			<p>To run through the examples and get a better understanding of this chapter, the topics covered in previous chapters will be useful but not required. To follow the code examples in the chapter, you need familiarity with a notebook environment, which could be a local setup such as Jupyter or an online notebook environment such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account with full access to SageMaker and the AWS Glue console. You can create a new account and use all the services for free during the trial period. You can find the code examples of the book using the following GitHub link:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter07</a></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor115"/>The available feature stores on the market</h1>
			<p>In this section, we will briefly discuss some of the available feature stores on the market and how they compare with Feast, as well as some commonalities and differences between these feature stores. </p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor116"/>The Tecton Feature Store</h2>
			<p><em class="italic">Tecton is an enterprise feature store, built by the creators of Uber's machine learning platform Michelangelo</em> (https://eng.uber.com/michelangelo-machine-learning-platform/). Tecton<a id="_idIndexMarker448"/> is also one of the major contributors to Feast. Hence, when you<a id="_idIndexMarker449"/> look at Tecton's <a id="_idIndexMarker450"/>documentation (<a href="https://docs.tecton.ai/index.html">https://docs.tecton.ai/index.html</a>), you <a id="_idIndexMarker451"/>will see a lot of similarities in the APIs and terminology. However, there are a lot of functionalities in Tecton that don't exist in Feast. Also, Tecton is a managed feature store, which means that you don't need to build and manage the infrastructure; it will be managed for you. </p>
			<p>As with most feature stores, Tecton uses online and offline stores for low latency and historical storage respectively. However, there are fewer options for online and offline stores compared to Feast, and it is currently supported only on AWS. If you prefer Azure or GCP, you don't have any other option but to wait for now. I believe multiple cloud providers and data stores will be eventually supported. Tecton uses a <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>) deployment <a id="_idIndexMarker452"/>model and separates deployment into data and control planes. You can find their deployment model at the following link: <a href="https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html">https://docs.tecton.ai/setting-up-tecton/07a-deployment_saas.html</a>. The <a id="_idIndexMarker453"/>best part is that data never leaves the customer's AWS account, and only the metadata required for the control panel to work is accessed by the Tecton-owned AWS account; also, the UI will be hosted in their account. However, if you want to expose online data through a REST/gRPC API endpoint, the service will be hosted in Tecton's AWS account. The online feature request and response will be routed through their account.</p>
			<p>Once Tecton<a id="_idIndexMarker454"/> is deployed into your AWS account, you <a id="_idIndexMarker455"/>can interact with it using the Python SDK. The CLI commands are similar to Feast commands; however, there are options such as being able to manage versions of your feature definitions and downgrading to a previous version of the definitions. As well as the common workflows that you can do with a feature store such as ingesting, querying at low latency, and performing point-in-time joins, with Tecton, you can define transformation as part of the feature store. This is one of my favorite features of Tecton. Here is the link to the feature views and transformation page in the feature store: <a href="https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html">https://docs.tecton.ai/overviews/framework/feature_views/feature_views.html</a>. What this means is that you can define a raw data source configuration for a data warehouse (Snowflake), database, Kinesis, or Kafka, and define a PySpark, Spark SQL, or pandas transformation to generate features. Tecton orchestrates these jobs on a defined schedule and generates features, and ingests them into online and offline stores. This can help in tracking data lineage. </p>
			<p>The following is an example code snippet on how to define feature views and transformation:</p>
			<pre class="source-code"># Feature View type</pre>
			<pre class="source-code">@batch_feature_view(</pre>
			<pre class="source-code">    # Pipeline attributes</pre>
			<pre class="source-code">    inputs=...</pre>
			<pre class="source-code">    mode=...</pre>
			<pre class="source-code">    # Entities</pre>
			<pre class="source-code">    entities=...</pre>
			<pre class="source-code">    # Materialization and serving configuration</pre>
			<pre class="source-code">    online=...</pre>
			<pre class="source-code">    offline=...</pre>
			<pre class="source-code">    batch_schedule=...</pre>
			<pre class="source-code">    feature_start_time=...</pre>
			<pre class="source-code">    ttl=...</pre>
			<pre class="source-code">    backfill_config=...</pre>
			<pre class="source-code">    # Metadata</pre>
			<pre class="source-code">    owner=...</pre>
			<pre class="source-code">    description=...</pre>
			<pre class="source-code">    tags=...</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code"># Feature View name</pre>
			<pre class="source-code">def my_feature_view(input_data):</pre>
			<pre class="source-code">    intermediate_data = my_transformation(input_data)</pre>
			<pre class="source-code">    output_data = my_transformation_two(intermediate_data)</pre>
			<pre class="source-code">    return output_data</pre>
			<p>You may <a id="_idIndexMarker456"/>recognize some of the parameters that <a id="_idIndexMarker457"/>you see in the preceding code block. Here, the annotation says it's a batch transformation on which you can define parameters such as which entities to use, what the schedule is, and whether it should ingest data into online and offline stores. In the method definition, input data will be injected based on whatever is assigned to the <strong class="source-inline">input</strong> parameter in the annotation definition (you can assume it to be a DataFrame from a raw data source). On the DataFrame, you add your transformation and return the output DataFrame, which will be features. These features will be ingested into the online and offline stores on the defined schedule. Once you define the preceding transformation, you will have to run <strong class="source-inline">tecton apply</strong>, which is similar to the <strong class="source-inline">feast apply</strong> command, to register this transformation. The other functionalities are similar to what other feature stores offer; hence, I will skip over them and let you explore their documentation. </p>
			<p>What is <a id="_idIndexMarker458"/>worth keeping in mind though is that the <a id="_idIndexMarker459"/>Tecton deployments are single-tenant at the time of writing, which means that if there are teams that cannot share data, you might need multiple deployments. There is a set of roles that needs to be created that will allow Tecton to install and create required resources using cross-account roles, which involves a one-time initial setup from you. </p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/>Databricks Feature Store</h2>
			<p>Databricks Feature Store is <a id="_idIndexMarker460"/>another option that is available out there for users. It makes sense if you are already using Databricks as your <a id="_idIndexMarker461"/>notebook environment and for data processing jobs. It comes with the Databricks workspaces, so you can't have just the feature store. However, you can get a workspace and not use anything else except the feature store. It can be hosted on AWS, GCP, or Azure. So, if you are on any of the major cloud providers, this could be an option.</p>
			<p>The concepts are similar to other feature stores, such as feature tables, timestamp versioning on the rows, the ability to do point-in-time joins, and online and offline stores. It uses a delta lake for its offline store and uses one of the key-value stores, available on a cloud based on which cloud provider you are on. The best part about Databricks Feature Store is that it integrates well with all the other aspects and components of Databricks, such as Spark DataFrame ingestion, retrieval, out-of-the-box integration with the MLflow model repository, access control, and tracking the lineage of notebooks that are used to generate a particular feature table. It also has a nice UI where you can browse and search for features. The next best part is there is no setup required if you already have the Databricks workspace. Here is a link to the notebook, which features examples of feature creation, ingestion, retrieval, training, and model scoring: <a href="https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html">https://docs.databricks.com/_static/notebooks/machine-learning/feature-store-taxi-example.html</a>.</p>
			<p>However, there are a few things to keep in mind. Databricks Feature Store doesn't have a concept of projects; hence, feature tables are the highest level of abstraction, and access control is at the feature table level. Additionally, Databricks' online model hosting is still in public preview (although no doubt it will eventually become a standard offering). This means that if you use Databricks Feature Store for an online model that is hosted outside of Databricks, it might have to connect to the online store using the direct client. For example, if <a id="_idIndexMarker462"/>you use DynamoDB as an online store (Databricks offers multiple choices, depending on the cloud provider) and <a id="_idIndexMarker463"/>host the model in Amazon <strong class="bold">Elastic Container Service (ECS)</strong>, you<a id="_idIndexMarker464"/> may have to implement the logic query for DynamoDB directly using the <strong class="source-inline">boto3</strong> client for features during prediction. Also, sharing features across the workspace might need additional configuration for either access tokens or using a central workspace for the feature store. Here is the link to the <a id="_idIndexMarker465"/>Databricks Feature Store documentation for more details: <a href="https://docs.databricks.com/applications/machine-learning/feature-store/index.html">https://docs.databricks.com/applications/machine-learning/feature-store/index.html</a>.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor118"/>Google's Vertex AI Feature Store</h2>
			<p>Google's Vertex AI is <a id="_idIndexMarker466"/>a <strong class="bold">Platform as a Service</strong> (<strong class="bold">PaaS</strong>) offering from <a id="_idIndexMarker467"/>Google for ML and AI. Vertex AI <a id="_idIndexMarker468"/>aims at offering an end-to-end ML platform that provides a set of tools for ML development, training, orchestration, model deployment, monitoring, and more. The tool that we are most interested in is Vertex AI Feature Store. If you are already using GCP for your services, it should be an automatic pick.</p>
			<p>The concepts and terminology are very similar to that of Feast. The highest level of abstraction in Vertex AI is called a <em class="italic">featurestore</em>, similar to a <em class="italic">project</em> in Feast, and a <em class="italic">featurestore</em> can have <em class="italic">entities</em>, and <em class="italic">features</em> should belong to <em class="italic">entities</em>. It supports online and batch serving, just like all the other feature stores. However, unlike Feast and Tecton, there are no options available for online and historical stores. Since it is a managed infrastructure, users don't need to worry about installation and choosing online and offline stores – probably just the pricing. Here is a link to its prices: <a href="https://cloud.google.com/vertex-ai/pricing#featurestore">https://cloud.google.com/vertex-ai/pricing#featurestore</a>. It uses <strong class="bold">IAM</strong> (short for <strong class="bold">Identity and Access Management</strong>) for <a id="_idIndexMarker469"/>authentication and authorization, and you also get a UI to search and browse features.</p>
			<p>The best part of Vertex AI is its integration with other components of GCP and the Vertex AI service itself for feature generation, pipeline management, and data lineage tracking. One of my favorite features is drift monitoring. You can set up a feature monitoring configuration on the feature tables, which can generate data distribution reports for you without requiring any additional work. </p>
			<p>Again, there <a id="_idIndexMarker470"/>are a few things to keep in mind. For online serving, you <a id="_idIndexMarker471"/>need to do capacity sizing and set up the number of nodes required to handle your traffic. The autoscaling option for online serving is still in public preview (although it's just a matter of time before it becomes a standard offering), but capacity planning should be a major problem to solve. A few load test simulations should help you figure that out easily. Also, there are quotas and limits on the number of online serving nodes you can have for a feature store, the length of data retention, and the number of features per entity. Some <a id="_idIndexMarker472"/>of these can be increased on request whereas others can't. Here is a link to the list of quotas and limits on a feature store: https://cloud.google.com/vertex-ai/docs/quotas#featurestore.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor119"/>The Hopsworks Feature Store</h2>
			<p>Hopsworks is <a id="_idIndexMarker473"/>another open source feature store under the AGPL-V3 license that can be run on-premises, on AWS or Azure. It also has an <a id="_idIndexMarker474"/>enterprise version of the feature store that supports GCP as well as any Kubernetes environment. Similar to other ML platform services, it also offers multiple components, such as model management and compute environment management. </p>
			<p>The concepts are similar to that of other feature stores; however, the terminology is different. It doesn't have a concept of entities, and <em class="italic">featuregroups</em> in <em class="italic">Hopsworks</em> are analogous to <em class="italic">featureviews</em> in <em class="italic">Feast</em>. Just like other feature stores, Hopsworks supports online and offline serving. It uses Apache Hive with Apache Hudi as an offline store and MySQL Cluster as an online store. Again, there are no options for online or offline stores. However, there are different storage connectors developed by Hopsworks that can be used to create on-demand external feature groups, such as <em class="italic">RedShiftSource</em>, which we defined in <em class="italic">Feast</em> in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. But there is a limitation on external feature groups, meaning there is no time travel, online serving, and so on.</p>
			<p>There are a lot of features in the Hopsworks Feature Store that are fancy and very interesting. Some of the best ones are as follows:</p>
			<ul>
				<li><strong class="bold">Project-level multi-tenancy</strong>: Each <a id="_idIndexMarker475"/>project has an owner and can share resources with other members in the team and across teams.</li>
				<li><strong class="bold">Feature group versioning</strong>: Hopsworks <a id="_idIndexMarker476"/>supports feature group versioning, which is not currently supported by any other feature stores on the market.</li>
				<li><strong class="bold">Statistics on feature groups</strong>: It <a id="_idIndexMarker477"/>provides a few out-of-the-box statistics on feature groups, such as feature co-relation computation, a frequency histogram on features, and uniqueness. The following is an example feature group:<p class="source-code">store_fg_meta = fs.create_feature_group(</p><p class="source-code">    name="store_fg",</p><p class="source-code">    version=1,</p><p class="source-code">    primary_key=["store"],</p><p class="source-code">    description="Store related features",</p><p class="source-code">    statistics_config={"enabled": True, </p><p class="source-code">                         "histograms": True, </p><p class="source-code">                         "correlations": True})</p></li>
				<li><strong class="bold">Feature validation</strong>: This<a id="_idIndexMarker478"/> is another fancy feature that is available out of the box. This is a set of predefined validation rules that exist on feature groups such as the minimum and maximum values of a feature, a uniqueness count of features, the entropy of a feature, and the maximum length of features. It has enough rule types that you won't have a use case where you need to customize a validation rule. The following are a couple of example rules:<p class="source-code">#the minimum value of the feature needs to be between 0 and 10</p><p class="source-code">rules=[Rule(name="HAS_MIN", level="WARNING", </p><p class="source-code">             min=0, max=10)] </p><p class="source-code">#Exactly 10% of all instances of the feature need to be contained in the legal_values list</p><p class="source-code">rules=[Rule(name="IS_CONTAINED_IN", level="ERROR", </p><p class="source-code">             legal_values=["a", "b"], min=0.1, </p><p class="source-code">             max=0.1)] </p></li>
				<li><strong class="bold">Transformation functions</strong>: Similar <a id="_idIndexMarker479"/>to Tecton transformations for feature views, in Hopsworks, you can define or use built-in transformation on a training dataset (Hopsworks has a concept of training data where you can pick features from different feature groups and create a training dataset definition on top of them– a concept similar to database views). </li>
			</ul>
			<p>There are some <a id="_idIndexMarker480"/>things to keep in mind though. If you choose the open source version, you may not have several features, and infrastructure will have to be self-managed. Conversely, for the enterprise version, you will have to collaborate with a Hopsworks engineer and create a few resources and roles required for the installation of Hopsworks on the cloud provider. Here is a link to all the <a id="_idIndexMarker481"/>documentation: https://docs.hopsworks.ai/feature-store-api/2.5.8/. I recommend having a look at the features even if you don't use them; this might give an idea of some of the features you might want to build or have in your feature store.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/>SageMaker Feature Store</h2>
			<p>SageMaker<a id="_idIndexMarker482"/> is an <a id="_idIndexMarker483"/>end-to-end ML platform offered by AWS. Just like Vertex AI, it has a notebook environment, AutoML, processing jobs and model management, a feature store, and so on. If you are an AWS-focused company, this must be a natural pick over the others. </p>
			<p>The concepts are close to that of other feature stores, although some of the terms are different. For example, SageMaker Feature Store also doesn't have the concept of entities, and <em class="italic">featureviews</em> in Feast are analogous to <em class="italic">featuregroups</em> in SageMaker. It has all the basic features, such as online and offline stores and serving. However, you don't <a id="_idIndexMarker484"/>have options to pick from. It uses S3 as an offline store and one of the key-value stores as an online store (AWS doesn't say what is used for an online store in its documentation). AWS uses IAM for authentication and authorization. To access feature store currently, you need full access to SageMaker and the AWS Glue console. If you compare <a id="_idIndexMarker485"/>SageMaker to Feast, both use/support S3 as an offline store, a key-value store as an online store, and a Glue catalog for managing the schema. Apart from SageMaker being a managed feature store, another difference is that Feast uses Redshift for querying offline data, whereas SageMaker uses Amazon Athena (serverless) for querying. You can add this functionality to Feast if you are a fan of serverless technologies. </p>
			<p>One of my favorite things about SageMaker Feature Store is that there is no infrastructure management. Apart from creating an IAM role to access the feature store, you don't need to manage anything. All the resources for any given load are managed by AWS. All you need to worry about is just developing and ingesting features. SageMaker Feature Store also supports ingestion using Spark on EMR or Glue jobs (serverless). Along with the features, it also adds metadata, such as <strong class="source-inline">write_time</strong> and <strong class="source-inline">api_invocation_time</strong>, that can be used in queries. The best part is that you can query offline data using Amazon Athena SQL queries.</p>
			<p>There are a few things to keep in mind though. The current implementation doesn't yet have granular access management. Right now, you need full access to SageMaker to use Feature Store, although I believe that it's only a matter of time before AWS starts offering granular access. Point-in-time joins are not available out of the box; however, these can be achieved using SQL queries or Spark.</p>
			<p>So far, we have looked at a few of the available options on the market; you can find other feature stores that are available at this link: <a href="https://www.featurestore.org/">https://www.featurestore.org/</a>. However, picking the right feature store for your project or team can be tricky. The following are a few things to keep in mind while picking a feature store:</p>
			<ul>
				<li>Your primary cloud provider makes a huge difference. If you are GCP-focused, it doesn't make sense to use SageMaker Feature Store and vice versa. If you are multi-cloud, then you will have more options.</li>
				<li>The data processing framework is also another key factor that decides what feature store to use. For example, if you use SageMaker as your ML platform, trying out SageMaker Feature Store before others makes more sense. </li>
				<li>Integration <a id="_idIndexMarker486"/>with other components in your ecosystem is also key – for instance, answering questions such as how well it integrates with your processing platform, the orchestration framework, the model management service, data validation frameworks, and your ML development process can really help in picking the right feature store.</li>
				<li>The required<a id="_idIndexMarker487"/> functionalities and your team structure make a big difference. If you are a small team who wants to just concentrate on ML, then a managed offering of a feature store makes sense, whereas if you have a platform team to manage the infrastructure, you may look into open source offerings and also evaluate the build versus buy options. If you have a platform team, they might look for additional features such as multi-tenancy, granular access control, and SaaS/PaaS.</li>
			</ul>
			<p>In conclusion, a lot of factors influence the choice of a feature store other than the functionalities it offers, as it must integrate well with a broader ecosystem.</p>
			<p>Next, let's look at how a managed feature store works.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor121"/>Feature management with SageMaker Feature Store</h1>
			<p>In this section, we <a id="_idIndexMarker488"/>will look into what action we might <a id="_idIndexMarker489"/>have to take if we were to use a managed feature store instead of Feast in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">All managed feature stores have a similar workflow; some may be API-based and some work through a CLI. But irrespective of this, the amount of work involved in using the feature store would be similar to what we will discuss in this section. The only reason I am going through SageMaker is familiarity and ease of access to it, using the free trial as a featured product in AWS. </p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor122"/>Resources to use SageMaker</h2>
			<p>In <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, before we started using the feature store, we <a id="_idIndexMarker490"/>created a bunch of resources on AWS, such as an S3 bucket, a Redshift cluster, an IAM role, and a Glue catalog table. Conversely, for a managed feature store such as SageMaker, all you need to have is an IAM role that has full access to SageMaker and you are all set. Let's try that out now. </p>
			<p>We need some IAM user credentials and an IAM role that SageMaker Feature Store can assume. Creating an IAM user is similar to what we have done before. Follow the same steps and create an IAM user, and assign <strong class="bold">AmazonS3FullAccess</strong> and <strong class="bold">AmazonSageMakerFullAccess</strong> permissions. IAM role creation is the same as we have done before; however, we need to allow the SageMaker service to assume the role. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">As mentioned many times before, it is never a good idea to assign full access; permissions should always be restrictive based on resources. </p>
			<p>Let's create an IAM role:</p>
			<ol>
				<li>Log in to your AWS account and navigate to the IAM role page, using the search bar; alternatively, visit the following URL: https://us-east-1.console.aws.amazon.com/iamv2/home#/roles. The following page will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B18024_07_001.jpg" alt="Figure 7.1 – The IAM role home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – The IAM role home page</p>
			<ol>
				<li value="2">On the <a id="_idIndexMarker491"/>displayed web page, click on <strong class="bold">Create role</strong> to navigate to the following screen:</li>
			</ol>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B18024_07_002.jpg" alt="Figure 7.2 – The IAM role creation page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – The IAM role creation page</p>
			<ol>
				<li value="3">On the screen displayed in <em class="italic">Figure 7.2</em>, in the <strong class="bold">Use cases for other AWS services</strong> dropdown, select <strong class="bold">SageMaker</strong> and then click the <strong class="bold">SageMaker - Execution</strong> radio button. Scroll down and click on <strong class="bold">Next</strong>, leaving everything as default on<a id="_idIndexMarker492"/> the <strong class="bold">Add Permissions</strong> page, and then click on <strong class="bold">Next</strong>. The following page will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B18024_07_003.jpg" alt="Figure 7.3 – The Name, review and create page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – The Name, review and create page</p>
			<ol>
				<li value="4">On the displayed web page, fill in the <strong class="bold">Role name</strong> field. We'll set the role name as <strong class="source-inline">sagemaker-iam-role</strong>. Scroll all the way down and click on <strong class="bold">Create role</strong>. Once the role is created, navigate to the IAM role details page and add the <strong class="bold">AmazonS3FullAccess</strong> permission, similar<a id="_idIndexMarker493"/> to how we did before. Also, copy the role <strong class="bold">ARN</strong> (short for <strong class="bold">Amazon Resource Name</strong>), which will be of the following format: <strong class="source-inline">arn:aws:iam::&lt;account_number&gt;:role/sagemaker-iam-role</strong>. </li>
			</ol>
			<p>That's all we need to access SageMaker Feature Store. Let's create the feature definitions next.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor123"/>Generating features</h2>
			<p>To define <a id="_idIndexMarker494"/>the feature group, since we are trying to compare how it differs from Feast, we will take the same feature set. You can download the previously ingested features from an S3 bucket or download it from the GitHub link: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter07/rfm_features.parquet</a>. After downloading the Parquet file, copy it to a location that can be accessed from the notebook. The next step is to create a new notebook, which I am calling <strong class="source-inline">ch7-sagemaker-feature-store.ipynb</strong>:</p>
			<ol>
				<li value="1">Let's install the required libraries first: <p class="source-code">!pip install sagemaker pandas</p></li>
				<li>After installing the libraries, let's generate the features. Here, we will be just reading the copied file from the location and making minor modifications to the dataset:<p class="source-code">import pandas as pd</p><p class="source-code">import time</p><p class="source-code">df = pd.read_parquet(path="/content/rfm_features.parquet")</p><p class="source-code">df = df.drop(columns=["created_timestamp"])</p><p class="source-code">df["event_timestamp"] = float(round(time.time()))</p><p class="source-code">df["customerid"] = df['customerid'].astype(float)</p><p class="source-code">df.head()</p></li>
			</ol>
			<p>The preceding code block reads the file and drops the <strong class="source-inline">created_timestamp</strong> column, as it is not required by SageMaker. We are also updating the <strong class="source-inline">event_timestamp</strong> column to the latest time and changing the type to <strong class="source-inline">float</strong> instead of <strong class="source-inline">datetime</strong>. The reason for this is that SageMaker only supports the <strong class="source-inline">int</strong>, <strong class="source-inline">float</strong>, and <strong class="source-inline">string</strong> features at the time of writing, and <strong class="source-inline">datetime</strong> files can either be a <strong class="source-inline">float</strong> or <strong class="source-inline">string</strong> object in the <strong class="source-inline">datetime</strong> ISO format. </p>
			<p>The code block produces the following output:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B18024_07_004.jpg" alt="Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Recency, Frequency, and Monetary value (RFM) features</p>
			<p>Now that we <a id="_idIndexMarker495"/>have RFM features, the next step is to define the feature group. If you recall correctly from <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, after generating the features, we created the feature definitions and applied them to the feature store.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor124"/>Defining the feature group</h2>
			<p>To define <a id="_idIndexMarker496"/>the feature group, since it's a one-time activity, it should be done in a separate notebook rather than by feature engineering. For this exercise, let's continue in the same notebook and define the feature group:</p>
			<ol>
				<li value="1">The following code block defines a few imports and creates the SageMaker session:<p class="source-code">import sagemaker</p><p class="source-code">import sys</p><p class="source-code">import boto3</p><p class="source-code">from sagemaker.session import Session</p><p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">import os</p><p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] ="&lt;aws_secret_id&gt;"</p><p class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</p><p class="source-code">prefix = 'sagemaker-featurestore-introduction'</p><p class="source-code">role = "arn:aws:iam::&lt;account_number&gt;:role/sagemaker-iam-role"</p><p class="source-code">sagemaker_session = sagemaker.Session()</p><p class="source-code">region = sagemaker_session.boto_region_name</p><p class="source-code">s3_bucket_name = "feast-demo-mar-2022"</p></li>
			</ol>
			<p>In the code block, replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret_id&gt;</strong> with the key and secret of the IAM user created earlier. Also, assign <strong class="source-inline">role</strong> with your IAM role ARN.</p>
			<ol>
				<li value="2">The <a id="_idIndexMarker497"/>following code block creates the feature group object and loads the feature definitions from the input DataFrame:<p class="source-code">from sagemaker.feature_store.feature_group import \</p><p class="source-code">  FeatureGroup</p><p class="source-code">customers_feature_group = FeatureGroup(</p><p class="source-code">    name="customer-rfm-features", </p><p class="source-code">    sagemaker_session=sagemaker_session</p><p class="source-code">)</p><p class="source-code">customers_feature_group.load_feature_definitions(df)</p></li>
			</ol>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B18024_07_005.jpg" alt="Figure 7.5 – The load feature definitions call&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – The load feature definitions call</p>
			<p>As <a id="_idIndexMarker498"/>you can see in <em class="italic">Figure 7.5</em>, the <strong class="source-inline">load_feature_definitions</strong> call reads the input DataFrame and loads the feature definition automatically. </p>
			<ol>
				<li value="3">The next step is to create the feature group. The following code block creates the feature group in SageMaker:<p class="source-code">customers_feature_group.create(</p><p class="source-code">    s3_uri=f"s3://{s3_bucket_name}/{prefix}",</p><p class="source-code">    record_identifier_name="customerid",</p><p class="source-code">    event_time_feature_name="event_timestamp",</p><p class="source-code">    role_arn=role,</p><p class="source-code">    enable_online_store=True</p><p class="source-code">)</p></li>
			</ol>
			<p>The preceding code block invokes the create API by passing the following parameters:</p>
			<ul>
				<li><strong class="source-inline">s3_uri</strong>: The location where the feature data will be stored</li>
				<li><strong class="source-inline">record_identifier_name</strong>: The name of the <strong class="source-inline">id</strong> column (the same as the entity column in Feast)</li>
				<li><strong class="source-inline">event_time_feature_name</strong>: The timestamp column that will be used for time travel</li>
				<li><strong class="source-inline">role_arn</strong>: The role that SageMaker Feature Store can assume</li>
				<li><strong class="source-inline">enable_online_store</strong>: Whether to enable online serving or not for this feature group</li>
			</ul>
			<p>The <a id="_idIndexMarker499"/>code block produces the following output on the successful creation of the feature group:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B18024_07_006.jpg" alt="Figure 7.6 – Feature group creation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Feature group creation</p>
			<p>That's all – our feature group is ready to use. Let's ingest the features next.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/>Feature ingestion</h2>
			<p>Feature ingestion in SageMaker Feature Store is simple. It is a simple API call, as shown in the following <a id="_idIndexMarker500"/>code block:</p>
			<pre class="source-code">ingestion_manager = customers_feature_group.ingest(df))</pre>
			<pre class="source-code">ingestion_manager.wait()</pre>
			<pre class="source-code">ingestion_manager.failed_rows</pre>
			<p>The preceding code block will ingest the features and print the failed row numbers if there are any.</p>
			<p>One thing to keep in mind here is that, like Feast, you don't need to do anything extra to materialize the latest features from an offline to an online store. If the online store is enabled, the data will be ingested to both online and offline stores, and the latest data will be available<a id="_idIndexMarker501"/> in the online store for querying right away.</p>
			<p>Let's query the online store next.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/>Getting records from an online store</h2>
			<p>Like Feast, querying <a id="_idIndexMarker502"/>from an online store is simple. All you need is the record ID and the feature group name. The following code block gets the record from the online store:</p>
			<pre class="source-code">customer_id = 12747.0</pre>
			<pre class="source-code">sg_runtime_client = sagemaker_session.boto_session.client(</pre>
			<pre class="source-code">    'sagemaker-featurestore-runtime', </pre>
			<pre class="source-code">    region_name=region)</pre>
			<pre class="source-code">record = sg_runtime_client.get_record(</pre>
			<pre class="source-code">    FeatureGroupName="customer-rfm-features", </pre>
			<pre class="source-code">    RecordIdentifierValueAsString=str(customer_id))</pre>
			<pre class="source-code">print(record)</pre>
			<p>The preceding code block gets all the features for the customer with the <strong class="source-inline">12747.0</strong> ID from the online store. The query should return the results within milliseconds. The output will be similar to the following code block:</p>
			<pre class="source-code">{'ResponseMetadata': {'RequestId': '55342bbc-c69b-49ca-bbd8-xxxx', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '55342bbc-c69b-49ca-bbd8-xxx, 'content-type': 'application/json', 'content-length': '729', 'date': 'Mon, 02 May 2022 01:36:27 GMT'}, 'RetryAttempts': 0}, </pre>
			<pre class="source-code">'Record': [{'FeatureName': 'customerid', 'ValueAsString': '12747.0'}, {'FeatureName': 'recency', 'ValueAsString': '7'}, {'FeatureName': 'frequency', 'ValueAsString': '35'}, {'FeatureName': 'monetaryvalue', 'ValueAsString': '1082.09'}, {'FeatureName': 'r', 'ValueAsString': '3'}, {'FeatureName': 'f', 'ValueAsString': '2'}, {'FeatureName': 'm', 'ValueAsString': '3'}, {'FeatureName': 'rfmscore', 'ValueAsString': '8'}, {'FeatureName': 'revenue6m', 'ValueAsString': '1666.1100000000001'}, {'FeatureName': 'ltvcluster', 'ValueAsString': '1'}, {'FeatureName': 'segmenthighvalue', 'ValueAsString': '1'}, {'FeatureName': 'segmentlowValue', 'ValueAsString': '0'}, {'FeatureName': 'segmentmidvalue', 'ValueAsString': '0'}, {'FeatureName': 'event_timestamp', 'ValueAsString': '1651455004.0'}]}</pre>
			<p>As you can <a id="_idIndexMarker503"/>see, the output contains all the features and corresponding values. </p>
			<p>Now that we have looked at querying an online store, let's check out how to generate the training dataset and query historical data next.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>Querying historical data with Amazon Athena</h2>
			<p>As mentioned <a id="_idIndexMarker504"/>previously, SageMaker Feature Store offers the ability to run SQL queries on a historical store using Amazon Athena. </p>
			<p>The following code block generates the latest snapshot of all customers and their features:</p>
			<pre class="source-code">get_latest_snapshot_query = customers_feature_group.athena_query()</pre>
			<pre class="source-code">query = f"""SELECT *</pre>
			<pre class="source-code">FROM</pre>
			<pre class="source-code">    (SELECT *,</pre>
			<pre class="source-code">         row_number()</pre>
			<pre class="source-code">        OVER (PARTITION BY customerid</pre>
			<pre class="source-code">    ORDER BY  event_timestamp desc, Api_Invocation_Time DESC, write_time DESC) AS row_num</pre>
			<pre class="source-code">    FROM "{get_latest_snapshot_query.table_name}")</pre>
			<pre class="source-code">WHERE row_num = 1 and </pre>
			<pre class="source-code">NOT is_deleted;"""</pre>
			<pre class="source-code">get_latest_snapshot_query.run(query_string=query, output_location=f"s3://{s3_bucket_name}/output")</pre>
			<pre class="source-code">get_latest_snapshot_query.get_query_execution()</pre>
			<p>The code <a id="_idIndexMarker505"/>block uses a nested SQL query, where the inner query gets all customers and their features, in descending order, from the <strong class="source-inline">event_time</strong>, <strong class="source-inline">Api_Invocation_Time</strong>, and <strong class="source-inline">write_time</strong> columns. The outer query selects the first occurrence of every customer from the results of the inner query. On successful execution of the query, the code block outputs the location of the query results along with additional details.</p>
			<p>The results can be loaded as a DataFrame, as shown in the following code block:</p>
			<pre class="source-code">latest_df = get_latest_snapshot_query.as_dataframe()</pre>
			<pre class="source-code">latest_df.head()</pre>
			<p>The preceding code block output the following:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B18024_07_007.jpg" alt="Figure 7.7 – Athena query results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Athena query results</p>
			<p>Feel free to try out other Athena queries on Feature Store. Here is the documentation of the Amazon Athena query: <a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a>.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Cleaning up a SageMaker feature group</h2>
			<p>Let's clean up the<a id="_idIndexMarker506"/> SageMaker resources to save costs before we move forward. The cleanup is pretty easy; it is just another API call to delete the feature group. The following code block performs that:</p>
			<pre class="source-code">customers_feature_group.delete()</pre>
			<p>That's all. After successful execution, it deletes the feature group but leaves behind the data in S3 and the Glue catalog, which can still be queried with Amazon Athena (using the <strong class="source-inline">boto3</strong> client) if required. Just to make sure everything is cleaned up, run the following code block<a id="_idIndexMarker507"/> in the same notebook. It should return an empty list of feature groups:</p>
			<pre class="source-code">sagemaker_client = sagemaker_session.boto_session.client(</pre>
			<pre class="source-code">    "sagemaker", region_name=region</pre>
			<pre class="source-code">) </pre>
			<pre class="source-code">sagemaker_client.list_feature_groups()</pre>
			<p>Now that we have looked at the SageMaker feature group, let's look into ML best practices next.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>ML best practices</h1>
			<p>So far in the <a id="_idIndexMarker508"/>book, we have discussed feature stores, how to use them for ML development and production, and what the available options are when choosing a feature store. Though a feature store is one of the major components/aspects of ML, there are other aspects of ML that we haven't concentrated on much in this book. In this section, let's briefly talk through some of the other aspects and best practices in ML.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Data validation at source</h2>
			<p>Irrespective of the <a id="_idIndexMarker509"/>technologies, algorithms, and infrastructure we use for building an ML model, if there are errors and anomalies in data, model performance will be severely impacted. Data should be treated as a first-class citizen of any ML system. Hence, it is very important to detect errors and anomalies in the data before it enters the ML pipeline. </p>
			<p>To run validation on raw data sources, we need a component to create and orchestrate the validation rules against the data. Users of the data should be able to write any custom rules in SQL queries, Python scripts, or Spark SQL. Any failures in the rule should be notified to the data consumers who, in turn, should be able to make a decision on whether to stop the pipeline execution, retrain the model, or take no action. </p>
			<p>Some of the common<a id="_idIndexMarker510"/> rules include descriptive analytics of the dataset on schedule, which can provide insights into data drift. More advanced statistics <a id="_idIndexMarker511"/>such as <strong class="bold">Kullback–Leibler</strong> (<strong class="bold">KL</strong>) divergence and <a id="_idIndexMarker512"/>the <strong class="bold">Population Stability Index</strong> (<strong class="bold">PSI</strong>) are good to have. Having simple data validation rules such as data freshness, unique values, string field length, patterns, and value range thresholds can be very beneficial. Schema validation is another important aspect of data validation. Any changes in the validation can affect all the consumers and pipelines. The better data validation we have at source, the healthier and more performant our models and pipeline will be.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Breaking down ML pipeline and orchestration</h2>
			<p>One bad practice is <a id="_idIndexMarker513"/>to develop everything in a single notebook, from data validation and feature engineering to model prediction. This is not a scalable or reusable approach. Most of the time is spent cleaning up unwanted code and productionizing the model. Hence, it is always a good idea to break down the ML pipeline into multiple smaller steps, such as data validation, cleaning, transformation, feature engineering, model training, and model prediction. The smaller the transformation steps, the more readable, reusable, and easy it will be to debug code for errors. This is one of the reasons that Feature Views and transformation in Tecton, and storage connectors and transformation functions in Hopsworks are great features. Similar features are also <a id="_idIndexMarker514"/>offered by many <strong class="bold">Extract, Transform and Load</strong> (<strong class="bold">ETL</strong>) frameworks. </p>
			<p>Apart from breaking <a id="_idIndexMarker515"/>down the ML pipeline, orchestration is another important part of ML platforms. Every cloud provider has one, and there are many open source offerings as well. Developing pipeline steps that can be orchestrated without much work is key. Nowadays, there are a lot of tools for orchestration, and as long as the steps are small and meaningful, it should be easy to orchestrate with any of the existing frameworks. </p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>Tracking data lineage and versioning</h2>
			<p>If you recall <a href="B18024_06_ePub.xhtml#_idTextAnchor096"><em class="italic">Chapter 6</em></a>, <em class="italic">Model to Production and Beyond</em>, we discussed debugging <a id="_idIndexMarker516"/>prediction issues. In that example, we discussed generating the same feature set that produced the anomaly in prediction; however, many times it won't be enough to figure out what went wrong in the system and whether it was caused by code or the dataset. Hence, along with that, being able to track the data lineage of that feature set all the way to the data source can be very helpful in debugging the issue.</p>
			<p>For every run of the <a id="_idIndexMarker517"/>pipeline, saving the input and output of every step with the timestamp version is the key here. With this, we can track the anomaly in prediction all the way back to its source, which is data. For example, instead of just having the features that generated a bad recommendation for a customer on a website, it would be better to also have the ability to trace the features all the way back to their interactions at the time of the event and the different transformation that was generated in the ML pipeline when this event occurred. </p>
			<p>The following is some of the pipeline information that can help in better lineage tracking:</p>
			<ul>
				<li>Versions of all the libraries used in the steps</li>
				<li>Versions of code that was run in the pipeline, including the pipeline version itself</li>
				<li>Input arguments and artifacts produced by every step of the pipeline, such as the raw data, the dataset, and models </li>
			</ul>
			<p>Next, let's look at the feature repository.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor133"/>The feature repository</h2>
			<p>Having a <a id="_idIndexMarker518"/>feature repository can be very beneficial for ML development. Though there are a few gray areas in respect of updates to feature table schema, the benefits of a feature store, such as reusability, browsable features, the readiness of online serving, time travel, and point-in-time joins, are very useful in model development. As we observed in the previous chapter, the features developed during the development of the customer lifetime value model were useful in the Next Purchase Day model. Similarly, as the feature repository grows in size, more and more features become available for use, and there is less duplication of work for data scientists and engineers to do, thereby accelerating the development of the model.</p>
			<p>The following screenshot depicts the cost of developing an ML model versus the number of curated features in the feature store:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B18024_07_008.jpg" alt="Figure 7.8 – The average cost of the model versus the number of curated features in the feature store&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – The average cost of the model versus the number of curated features in the feature store</p>
			<p>As shown in <em class="italic">Figure 7.8</em>, the cost of developing and productionizing the model goes down as the feature repository grows. Going by the reuse and add new if not available, all the features available in the feature repository are either production-ready or serving production models. We will be just adding delta features for each new model. This means that the only additional cost on the infrastructure would be to run these additional feature engineering transformations and new feature tables, and the rest is assumed to auto-scale for the production load if we are using a managed feature store. Hence, the cost involved in the development and production of the new model should decrease over time and flatten once the feature repository is saturated. </p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Experiment tracking, model versioning, and the model repository</h2>
			<p>Experiment tracking <a id="_idIndexMarker519"/>and the model repository are other important aspects of ML development. When developing a model, we run different experiments – it could be different algorithms, different implementations such <a id="_idIndexMarker520"/>as TensorFlow versus PyTorch, hyperparameter tuning, a different set of features for the model, a different training dataset, and <a id="_idIndexMarker521"/>also different transformations on the training dataset. Keeping track of these experiments is not easy, as some of these experiments can go on for days or weeks. Hence, using experiment-tracking software that comes out of the box with many of the notebook environments is very important. </p>
			<p>Every run should log the following:</p>
			<ul>
				<li>The version of model training notebooks or scripts.</li>
				<li>Some parameters about the dataset that can be used to reproduce the same training and evaluation dataset. If you are using a feature store, then it could be the timestamps and entities used; if not, you can also save the training dataset to a file and log the location of the dataset.</li>
				<li>All the parameters that are used in the training algorithm.</li>
				<li>The performance metrics of each run.</li>
				<li>Any visualization of the results can also be very useful.</li>
			</ul>
			<p>The logged metrics for each run can be used for comparing the performance metrics of the models for different runs. These metrics will be critical in making a decision on which run of the model is better performing and should be moved to new stages, such as stage deployment, and AB testing. In addition, each run also helps you browse through the history of the experiments if you or anybody else on the team ever need to look back and reproduce some specific run. </p>
			<p>Similarly, a model repository can help in keeping track of all the different versions of the model. The model registry/repository stores the information required to load and run the model – for instance, an MLflow model repository stores information such as the conda environment, the model's <strong class="source-inline">pickle</strong> file, and any other additional <em class="italic">dependencies</em> of the model. If you have a central repository of the model, it can be useful for consumers to browse and search, and also for the life cycle management of models, such as moving models to different stages – development, staging, production, and archived. Model repositories can also be used for scanning any vulnerabilities in code and any packages used in the model. Hence, the model repository plays a key role in ML development. </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor135"/>Feature and model monitoring</h2>
			<p>As we discussed<a id="_idIndexMarker522"/> in the previous chapter, feature monitoring is another important aspect. An important counterpart of the feature repository is monitoring for changes and anomalies. The feature monitoring rules will be similar to that of data monitoring. Some of the useful rules of features are feature freshness, minimum and maximum rules, monitoring for outliers, descriptive statistics of the latest features, and metrics such as KL divergence and PSI. The Hopsworks monitoring rules should be a good starting point for the list of rules that you may have on features. Here is a link to the documentation: <a href="https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/">https://docs.hopsworks.ai/feature-store-api/2.5.8/generated/feature_validation/</a>. </p>
			<p>Model monitoring is <a id="_idIndexMarker523"/>another important aspect. After moving a model to production, it tends to decay in performance over time. This happens as user behaviors change; hence the data profiles. It is important to keep track of how the model is performing in production. These performance reports should be generated on schedule, if not in real time, and appropriate actions must be taken, such as model retraining with the new data or starting a new iteration altogether.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor136"/>Miscellaneous</h2>
			<p>A few other <a id="_idIndexMarker524"/>things to keep in mind during ML development include keeping track of runtime environments, library upgrades, and depreciations. It is better to proactively act on these. For instance, if you use tools that are strictly tied to a specific environment, such as a Python or Spark version, once a specific runtime is deprecated and removed from production support, the jobs might start failing and the production system may be hampered. Another example could be that Databricks has runtimes that are tied to specific Python and Spark versions. If you are running jobs on a deprecated version, once it goes out of support, the jobs might start failing if there are breaking changes in the new version. Hence, it is better to upgrade proactively.</p>
			<p>With that, let's summarize what we have learned in this chapter before looking at an end-to-end use case in the next chapter.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Summary </h1>
			<p>In this chapter, we took a look at some of the available feature stores on the market. We discussed five of them, namely Tecton, Databricks, Vertex AI, Hopsworks, and SageMaker Feature Store. We also did a deep dive into SageMaker Feature Store to get a feel of using a managed feature store instead of Feast and how it differs when it comes to resource creation, feature ingestion, and querying. In the last section, we briefly discussed a set of best practices for ML development. </p>
			<p>In the next chapter, we'll go through an end-to-end use case on a managed ML platform.</p>
		</div>
	</body></html>