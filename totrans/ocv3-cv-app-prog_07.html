<html><head></head><body><div class="calibre1" title="Chapter&#xA0;7.&#xA0;Extracting Lines, Contours, and Components"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title"><a id="ch07" class="calibre6"/>Chapter 7. Extracting Lines, Contours, and Components</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem">Detecting image contours with the Canny operator</li><li class="listitem">Detecting lines in images with the Hough transform</li><li class="listitem">Fitting a line to a set of points</li><li class="listitem">Extracting connected components</li><li class="listitem">Computing components' shape descriptors</li></ul></div><div class="calibre1" title="Introduction"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec46" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">In order to perform content-based analysis of an image, it is necessary to extract meaningful features from the collection of pixels that constitute the image. Contours, lines, blobs, and so on, are fundamental image primitives that can be used to describe the elements contained in an image. This chapter will teach you how to extract some of these image primitives.</p></div></div>
<div class="calibre1" title="Detecting image contours with the Canny operator"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec47" class="calibre6"/>Detecting image contours with the Canny operator</h1></div></div></div><p class="calibre8">In the previous chapter, we learned how it is possible to detect the edges of an image. In particular, we showed you that by applying a threshold to the gradient magnitude, a binary map of the main edges of an image can be obtained. Edges carry important visual information since they delineate the image elements. For this reason, they can be used, for example, in object recognition. However, simple binary edge maps suffer from two main drawbacks. First, the edges that are detected are unnecessarily thick; this makes the object's limits more difficult to identify. Second, and more importantly, it is often impossible to find a threshold that is sufficiently low in order to detect all important edges of an image and is, at the same time, sufficiently high in order to not include too many insignificant edges. This is a trade-off problem that the <span><strong class="calibre15">Canny</strong></span> algorithm tries to solve.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec137" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The Canny algorithm is implemented in OpenCV by the <code class="literal">cv::Canny</code> function. As will be explained, this algorithm requires the specification of two thresholds. The call to the function is, therefore, as follows:</p><pre class="programlisting">    //Apply Canny algorithm 
    cv::Mat contours; 
    cv::Canny(image,     // gray-level image 
              contours,  // output contours 
              125,       // low threshold 
              350);      // high threshold 
</pre><p class="calibre8">Let's consider the following image:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">When the algorithm is applied on the preceding image, the result is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_002.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that here we have inverted the contour representation since the normal result represents contours by non-zero pixels. The displayed image is simply <code class="literal">255-contours</code>.</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec138" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The Canny operator is generally based on the Sobel operator that was presented in 
<a href="ch06.html" title="Chapter 6. Filtering the Images">Chapter 6</a>
, <span><em class="calibre16">Filtering the Images</em></span>, although other gradient operators can also be used. The key idea here is to use two different thresholds in order to determine which point should belong to a contour: a low and a high threshold.</p><p class="calibre8">The low threshold should be chosen in a way that it includes all edge pixels that are considered to belong to a significant image contour. For example, using the low-threshold value specified in the example of the preceding section and applying it on the result of a Sobel operator, the following edge map is obtained:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_003.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As can be seen, the edges that delineate the road are very well defined. However, because a permissive threshold was used, more edges than what is ideally needed are also detected. The role of the second threshold, then, is to define the edges that belong to all important contours. It should exclude all edges considered as outliers. For example, the Sobel edge map that corresponds to the high threshold used in our example is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_004.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We now have an image that contains broken edges, but the ones that are visible certainly belong to the significant contours of the scene. The Canny algorithm combines these two edge maps in order to produce an optimal map of contours. It operates by keeping only the edge points of the low-threshold edge map for which a continuous path of edges exists, linking those edge points to an edge that belongs to the high-threshold edge map. Consequently, all edge points of the high-threshold map are kept, while all isolated chains of edge points in the low-threshold map are removed. The solution that is obtained constitutes a good compromise, allowing good quality contours to be obtained as long as appropriate threshold values are specified. This strategy, based on the use of two thresholds to obtain a binary map, is called <span><strong class="calibre15">hysteresis thresholding</strong></span>, and can be used in any context where a binary map needs to be obtained from a thresholding operation. However, this is done at the cost of higher computational complexity.</p><p class="calibre8">In addition, the Canny algorithm uses an extra strategy to improve the quality of the edge map. Prior to the application of the hysteresis thresholding, all edge points for which the gradient magnitude is not a maximum in the gradient direction are removed (recall that the gradient orientation is always perpendicular to the edge). Therefore, the local maximum of the gradient in this direction corresponds to the point of maximum strength of the contour. This is a contour thinning operation that creates edges having a width of 1 pixel. This explains why thin edges are obtained in the Canny contour maps.</p></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec139" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The classic article by <span><em class="calibre16">J. Canny</em></span>, <span><em class="calibre16">A computational approach to edge detection, IEEE Transactions on Pattern Analysis and Image Understanding, vol. 18, issue 6, 1986</em></span></li></ul></div></div></div>
<div class="calibre1" title="Detecting lines in images with the Hough transform"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec48" class="calibre6"/>Detecting lines in images with the Hough transform</h1></div></div></div><p class="calibre8">In our human-made world, planar and linear structures abound. As a result, straight lines are frequently visible in images. These are meaningful features that play an important role in object recognition and image understanding. The <span><strong class="calibre15">Hough transform</strong></span> is a classic algorithm that is often used to detect these particular features in images. It was initially developed to detect lines in images and, as we will see, it can also be extended to detect other simple image structures.</p><div class="calibre1" title="Getting ready"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec140" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">With the Hough transform, lines are represented using the following equation:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/image_07_005-2-300x54.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The <code class="literal">ρ</code> parameter is the distance between the line and the image origin (the upper-left corner), and <code class="literal">θ</code> is the angle of the perpendicular to the line. In this representation, the lines visible in an image have a <code class="literal">θ</code> angle between <code class="literal">0</code> and <code class="literal">π</code> radians, while the <code class="literal">ρ</code> radius can have a maximum value that equals the length of the image diagonal. Consider, for example, the following set of lines:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Getting ready" src="graphics/image_07_006.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">A vertical line such as line <span><strong class="calibre15">1</strong></span> has a <code class="literal">θ</code> angle value equal to zero, while a horizontal line (for example, line <span><strong class="calibre15">5</strong></span>) has its <code class="literal">θ </code>value equal to <code class="literal">π/2</code>. Therefore, line <span><strong class="calibre15">3</strong></span> has an angle <code class="literal">θ</code> equal to <code class="literal">π/4</code>, and line <span><strong class="calibre15">4</strong></span> is at <code class="literal">0.7π</code> approximately. In order to be able to represent all possible lines with <code class="literal">θ</code> in the <code class="literal">[0, π]</code> interval, the radius value can be made negative. This is the case with line <span><strong class="calibre15">2</strong></span>, which has a <code class="literal">θ</code> value equal to <code class="literal">0.8π</code> with a negative value for <code class="literal">ρ</code>.</p></div><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec141" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">OpenCV offers two implementations of the Hough transform for line detection. The basic version is <code class="literal">cv::HoughLines</code>. Its input is a binary map that contains a set of points (represented by non-zero pixels), some of which are aligned to form lines. Usually, this is an edge map obtained, for example, from the Canny operator. The output of the <code class="literal">cv::HoughLines</code> function is a vector of <code class="literal">cv::Vec2f</code> elements, each of them being a pair of floating point values, representing the parameters of a detected line, <code class="literal">(ρ,θ)</code>. The following is an example of using this function where we first apply the Canny operator to obtain the image contours and then detect the lines using the Hough transform:</p><pre class="programlisting">    // Apply Canny algorithm 
    cv::Mat contours; 
    cv::Canny(image,contours,125,350); 
    // Hough transform for line detection 
    std::vector&lt;cv::Vec2f&gt; lines; 
    cv::HoughLines(test, lines, 1,  
                   PI/180,  // step size 
                   60);     // minimum number of votes 
</pre><p class="calibre8">Parameters 3 and 4 correspond to the step size for the line search. In our example, the function will search for lines of all possible radii by steps of <code class="literal">1</code> and all possible angles by steps of <code class="literal">π/180</code>. The role of the last parameter will be explained in the next section. With this particular choice of parameter values, several lines are detected on the road image of the preceding recipe. In order to visualize the result of the detection, it is interesting to draw these lines on the original image. However, it is important to note that this algorithm detects lines in an image and not line segments, since the endpoints of each line are not given. Consequently, we will draw lines that traverse the entire image. To do this, for a vertically oriented line, we calculate its intersection with the horizontal limits of the image (that is, the first and last rows) and draw a line between these two points. We proceed similarly with horizontally-oriented lines but using the first and last columns. Lines are drawn using the <code class="literal">cv::line </code>function. Note that this function works well even with point coordinates outside the image limits. Therefore, there is no need to check whether the computed intersection points fall within the image. Lines are then drawn by iterating over the line vector as follows:</p><pre class="programlisting">    std::vector&lt;cv::Vec2f&gt;::const_iterator it= lines.begin(); 
    while (it!=lines.end()) { 
 
      float rho= (*it)[0];   // first element is distance rho 
      float theta= (*it)[1]; // second element is angle theta 
 
      if (theta &lt; PI/4.|| theta &gt; 3.*PI/4.) { //~vertical line 
 
        // point of intersection of the line with first row 
        cv::Point pt1(rho/cos(theta),0); 
        // point of intersection of the line with last row 
        cv::Point pt2((rho-result.rows*sin(theta))/ 
                       cos(theta),result.rows); 
        //draw a white line 
         cv::line( image, pt1, pt2, cv::Scalar(255), 1); 
    
      } else { // ~horizontal line 
 
        // point of intersection of the 
        // line with first column 
        cv::Point pt1(0,rho/sin(theta)); 
        //point of intersection of the line with last column 
        cv::Point pt2(result.cols,
                      (rho-result.cols*cos(theta))/sin(theta)); 
        // draw a white line 
        cv::line(image, pt1, pt2, cv::Scalar(255), 1); 
      } 
      ++it; 
    } 
</pre><p class="calibre8">The following result is obtained:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_007.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As can be seen, the Hough transform simply looks for an alignment of edge pixels across the image. This can potentially create some false detections due to incidental pixel alignments or multiple detections when several lines with slightly different parameter values pass through the same alignment of pixels.</p><p class="calibre8">To overcome some of these problems, and to allow line segments to be detected (that is, with endpoints), a variant of the transform has been proposed. This is the Probabilistic Hough transform, and it is implemented in OpenCV as the <code class="literal">cv::HoughLinesP</code> function. We use it here to create our <code class="literal">LineFinder</code> class, which encapsulates the function parameters:</p><pre class="programlisting">    class LineFinder { 
 
      private: 
 
      // original image 
      cv::Mat img; 
 
      // vector containing the endpoints of the detected lines 
      std::vector&lt;cv::Vec4i&gt; lines; 
 
      // accumulator resolution parameters 
      double deltaRho; 
      double deltaTheta; 
 
      // minimum number of votes that a line   
      // must receive before being considered 
      int minVote; 
 
     //min length for a line 
     double minLength; 
 
     //max allowed gap along the line 
     double maxGap; 
 
     public: 
   
      // Default accumulator resolution is 1 pixel by 1 degree 
      // no gap, no minimum length 
      LineFinder() : deltaRho(1), deltaTheta(PI/180),              
                     minVote(10), minLength(0.), maxGap(0.) {} 
</pre><p class="calibre8">Take a look at the corresponding setter methods:</p><pre class="programlisting">    // Set the resolution of the accumulator 
    void setAccResolution(double dRho, double dTheta) { 
 
      deltaRho= dRho; 
      deltaTheta= dTheta; 
    } 
 
    // Set the minimum number of votes 
    void setMinVote(int minv) { 
 
      minVote= minv; 
    } 
 
    // Set line length and gap 
    void setLineLengthAndGap(double length, double gap) { 
 
      minLength= length; 
      maxGap= gap; 
    } 
</pre><p class="calibre8">With the preceding method, the method that performs Hough line segment detection is as follows:</p><pre class="programlisting">    // Apply probabilistic Hough Transform 
    std::vector&lt;cv::Vec4i&gt; findLines(cv::Mat&amp; binary) { 
 
      lines.clear(); 
      cv::HoughLinesP(binary,lines,
                      deltaRho, deltaTheta, minVote,
                      minLength, maxGap); 
 
      return lines; 
    } 
</pre><p class="calibre8">This method returns a vector of <code class="literal">cv::Vec4i</code>, which contains the start and endpoint coordinates of each detected segment. The detected lines can then be drawn on an image with the following method:</p><pre class="programlisting">    // Draw the detected lines on an image 
    void drawDetectedLines(cv::Mat &amp;image,               
                           cv::Scalar color=cv::Scalar(255,255,255)) { 
 
      // Draw the lines 
      std::vector&lt;cv::Vec4i&gt;::const_iterator it2= lines.begin(); 
 
      while (it2!=lines.end()) { 
 
        cv::Point pt1((*it2)[0],(*it2)[1]); 
        cv::Point pt2((*it2)[2],(*it2)[3]); 
 
        cv::line( image, pt1, pt2, color); 
 
        ++it2; 
      } 
    } 
</pre><p class="calibre8">Now, using the same input image, lines can be detected with the following sequence:</p><pre class="programlisting">    // Create LineFinder instance 
    LineFinder finder; 
 
    // Set probabilistic Hough parameters 
    finder.setLineLengthAndGap(100,20); 
    finder.setMinVote(60); 
 
    // Detect lines and draw them on the image 
    std::vector&lt;cv::Vec4i&gt; lines= finder.findLines(contours); 
    finder.drawDetectedLines(image); 
</pre><p class="calibre8">The preceding code gives the following result:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_008.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec142" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The objective of the Hough transform is to find all lines in a binary image that pass through a sufficient number of points. It proceeds by considering each individual pixel point in the input binary map and identifying all possible lines that pass through it. When the same line passes through many points, it means that this line is significant enough to be considered.</p><p class="calibre8">The Hough transform uses a two-dimensional accumulator in order to count how many times a given line is identified. The size of this accumulator is defined by the specified step sizes (as mentioned in the preceding section) of the <code class="literal">(ρ,θ)</code> parameters of the adopted line representation. To illustrate the functioning of the transform, let's create a <code class="literal">180</code> by <code class="literal">200</code> matrix (corresponding to a step size of <code class="literal">π/180</code> for <code class="literal">θ</code> and <code class="literal">1</code> for <code class="literal">ρ</code>):</p><pre class="programlisting">    // Create a Hough accumulator 
    // here a uchar image; in practice should be ints 
    cv::Mat acc(200,180,CV_8U,cv::Scalar(0)); 
</pre><p class="calibre8">This accumulator is a mapping of different <code class="literal">(ρ,θ)</code> values. Therefore, each entry of this matrix corresponds to one particular line. Now, if we consider one point, let's say one at <code class="literal">(50,30)</code>, then it is possible to identify all lines that pass through this point by looping over all possible <code class="literal">θ</code> angles (with a step size of <code class="literal">π/180</code>) and computing the corresponding (rounded) <code class="literal">ρ</code> value:</p><pre class="programlisting">    // Choose a point 
    int x=50, y=30; 
    // loop over all angles 
    for (int i=0; i&lt;180; i++) { 
 
      double theta= i*PI/180.; 
 
      // find corresponding rho value  
      double rho= x*std::cos(theta)+y*std::sin(theta); 
      // j corresponds to rho from -100 to 100 
      int j= static_cast&lt;int&gt;(rho+100.5); 
 
      std::cout &lt;&lt; i &lt;&lt; "," &lt;&lt; j &lt;&lt; std::endl; 
 
      // increment accumulator 
      acc.at&lt;uchar&gt;(j,i)++; 
    } 
</pre><p class="calibre8">The entries of the accumulator corresponding to the computed <code class="literal">(ρ,θ)</code> pairs are then incremented, signifying that all of these lines pass through one point of the image (or, to say it another way, each point votes for a set of possible candidate lines). If we display the accumulator as an image (inverted and multiplied by <code class="literal">100</code> to make the count of <code class="literal">1</code> visible), we obtain the following:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_009.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The preceding curve represents the set of all lines that pass through the specified point. Now, if we repeat the same exercise with, let's say, point <code class="literal">(30,10)</code>, we now have the following accumulator:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8"> 
As can be seen, the two resulting curves intersect at one point: the point that corresponds to the line that passes through these two points. The corresponding entry of the accumulator receives two votes, indicating that two points pass through this line.</p><p class="calibre8">If the same process is repeated for all points of a binary map, then points aligned along a given line will increase a common entry of the accumulator many times. At the end, you just need to identify the local maxima in this accumulator that receives a significant number of votes in order to detect the lines (that is, point alignments) in the image. The last parameter specified in the <code class="literal">cv::HoughLines</code> function corresponds to the minimum number of votes that a line must receive to be considered as detected. This means that the lower this minimum number of votes is, then the higher the number of detected lines will be.</p><p class="calibre8">For example, if we lower this value to <code class="literal">50</code> in the case of our road example, then the following lines are now detected:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_011.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The Probabilistic Hough transform adds a few modifications to the basic algorithm. First, instead of systematically scanning the image row-by-row, points are chosen in random order in the binary map. Whenever an entry of the accumulator reaches the specified minimum value, the image is scanned along the corresponding line and all points that pass through it are removed (even if they have not voted yet). This scanning also determines the length of the segments that will be accepted. For this, the algorithm defines two additional parameters. One is the minimum length for a segment to be accepted, and the other is the maximum pixel gap that is permitted to form a continuous segment. This additional step increases the complexity of the algorithm, but this is partly compensated by the fact that fewer points will be involved in the voting process, as some of them are eliminated by the line-scanning process.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec143" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The Hough transform can also be used to detect other geometrical entities. In fact, any entity that can be represented by a parametric equation is a good candidate for the Hough transform. There is also a Generalized Hough transform that can detect objects of any shape.</p><div class="calibre1" title="Detecting circles"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch07lvl3sec31" class="calibre6"/>Detecting circles</h3></div></div></div><p class="calibre8">In the case of circles, the corresponding parametric equation is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Detecting circles" src="graphics/B05388_07_22.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This equation includes three parameters (the circle radius and center coordinates), which means that a three-dimensional accumulator would be required. However, it is generally found that the Hough transform becomes more complex and less reliable as the dimensionality of its accumulator increases. Indeed, in this case, a large number of entries of the accumulator will be incremented for each point and, as a consequence, the accurate localization of local peaks becomes more difficult. Different strategies have been proposed in order to overcome this problem. The strategy used in the OpenCV implementation of the Hough circle detection uses two passes. During the first pass, a two-dimensional accumulator is used to find candidate circle locations. Since the gradient of points on the circumference of a circle should point in the direction of the radius, for each point, only the entries in the accumulator along the gradient direction are incremented (based on predefined minimum and maximum radius values). Once a possible circle center is detected (that is, has received a predefined number of votes), a 1D histogram of a possible radius is built during the second pass. The peak value in this histogram corresponds to the radius of the detected circles.</p><p class="calibre8">The <code class="literal">cv::HoughCircles</code> function that implements the preceding strategy integrates both the Canny detection and the Hough transform. It is called as follows:</p><pre class="programlisting">    cv::GaussianBlur(image,image,cv::Size(5,5),1.5); 
    std::vector&lt;cv::Vec3f&gt; circles; 
       cv::HoughCircles(image, circles, cv::HOUGH_GRADIENT,  
                   2,    //accumulator resolution (size of the image/2)  
                   50,   // minimum distance between two circles 
                   200,  // Canny high threshold  
                   100,  // minimum number of votes  
                   25,
                   100); // min and max radius 
</pre><p class="calibre8">Note that it is always recommended that you smooth the image before calling the <code class="literal">cv::HoughCircles</code> function in order to reduce the image noise that could cause several false circle detections. The result of the detection is given in a vector of <code class="literal">cv::Vec3f</code> instances. The first two values are the circle center coordinates and the third is the radius.</p><p class="calibre8">The <code class="literal">cv::HOUGH_GRADIENT</code> argument was the only option available at the time of writing. It corresponds to the two-pass circle detection method. The fourth parameter defines the accumulator resolution. It is a divider factor; specifying a value of <code class="literal">2</code>, for example, makes the accumulator half the size of the image. The next parameter is the minimum distance in pixels between two detected circles. The other parameter corresponds to the high threshold of the Canny edge detector. The low-threshold value is always set at half this value. The seventh parameter is the minimum number of votes that a center location must receive during the first pass to be considered as a candidate circle for the second pass. Finally, the last two parameters are the minimum and maximum radius values for the circles to be detected. As can be seen, the function includes many parameters that make it difficult to tune.</p><p class="calibre8">Once the vector of detected circles is obtained, these circles can be drawn on the image by iterating over the vector and calling the <code class="literal">cv::circle</code> drawing function with the obtained parameters:</p><pre class="programlisting">    std::vector&lt;cv::Vec3f&gt;::const_iterator itc= circles.begin(); 
 
    while (itc!=circles.end()) { 
 
      cv::circle(image,   
                 cv::Point((*itc)[0], (*itc)[1]), // circle centre 
                 (*itc)[2],       // circle radius 
                 cv::Scalar(255), // color 
                 2);              // thickness 
   
      ++itc;    
    } 
</pre><p class="calibre8">The following is the result obtained on a test image with the chosen arguments:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Detecting circles" src="graphics/image_07_013.jpg" class="calibre17"/></div><p class="calibre8">
</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec144" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The following article, <span><em class="calibre16">Gradient-based Progressive Probabilistic Hough Transform</em></span> by <span><em class="calibre16">C. Galambos</em></span>, <span><em class="calibre16">J. Kittler</em></span>, and<span><em class="calibre16"> J. Matas, IEE Vision Image and Signal Processing</em></span>, <span><em class="calibre16">vol. 148 no 3, pp. 158-165, 2002</em></span>, is one of the numerous references on the Hough transform and describes the probabilistic algorithm implemented in OpenCV.</li><li class="listitem">The following article, <span><em class="calibre16">Comparative Study of Hough Transform Methods for Circle Finding, Image and Vision Computing</em></span>, <span><em class="calibre16">vol. 8 no 1, pp. 71-77, 1990</em></span>, by <span><em class="calibre16">H.K. Yuen</em></span>, <span><em class="calibre16">J. Princen</em></span>, <span><em class="calibre16">J. Illingworth</em></span>, and<span><em class="calibre16"> J Kittler</em></span>, describes different strategies for circle detection using the Hough transform.</li></ul></div></div></div>
<div class="calibre1" title="Fitting a line to a set of points"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec49" class="calibre6"/>Fitting a line to a set of points</h1></div></div></div><p class="calibre8">In some applications, it could be important to not only detect lines in an image, but also to obtain an accurate estimate of the line's position and orientation. This recipe will show you how to estimate the exact line that best fits a given set of points.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec145" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The first thing to do is to identify points in an image that seem to be aligned along a straight line. Let's use one of the lines we detected in the preceding recipe. The lines detected using <code class="literal">cv::HoughLinesP</code> are contained in <code class="literal">std::vector&lt;cv::Vec4i&gt;</code> called <code class="literal">lines</code>. To extract the set of points that seem to belong to, let's say, the first of these lines, we can proceed as follows. We draw a white line on a black image and intersect it with the Canny image of <code class="literal">contours</code> used to detect our lines. This is simply achieved by the following statements:</p><pre class="programlisting">    int n=0;         // we select line 0 
    // black image 
    cv::Mat oneline(contours.size(),CV_8U,cv::Scalar(0)); 
    // white line 
    cv::line(oneline, cv::Point(lines[n][0],lines[n][1]),
             cv::Point(lines[n] [2],
             lines[n][3]), cv::Scalar(255),  
             3);      // line width 
    // contours AND white line 
    cv::bitwise_and(contours,oneline,oneline); 
</pre><p class="calibre8">The result is an image that contains points that could be associated with the specified line. In order to introduce some tolerance, we draw a line of a certain thickness (here, <code class="literal">3</code>). All points inside the defined neighborhood are, therefore, accepted.</p><p class="calibre8">The following is the image that is obtained (inverted for better viewing):</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The coordinates of the points in this set can then be inserted in a <code class="literal">std::vector</code> of <code class="literal">cv::Point</code> objects (floating point coordinates, that is, <code class="literal">cv::Point2f</code>, can also be used) with the following double loop:</p><pre class="programlisting">    std::vector&lt;cv::Point&gt; points; 
 
    // Iterate over the pixels to obtain all point positions 
    for( int y = 0; y &lt; oneline.rows; y++ ) { 
      // row y 
 
      uchar* rowPtr = oneline.ptr&lt;uchar&gt;(y); 
 
      for( int x = 0; x &lt; oneline.cols; x++ ) { 
        // column x  
 
        // if on a contour 
        if (rowPtr[x]) { 
 
          points.push_back(cv::Point(x,y)); 
        } 
      } 
    } 
</pre><p class="calibre8">We now have a list of points and we want to fit a line passing through these points. This best fitting line is easily found by calling the <code class="literal">cv::fitLine</code> OpenCV function:</p><pre class="programlisting">    cv::Vec4f line; 
    cv::fitLine(points,line, 
                cv::DIST_L2, //distance type 
                0,           //not used with L2 distance 
                0.01,0.01);  //accuracy 
</pre><p class="calibre8">The preceding code gives us the parameters of the line equation in the form of a unit-directional vector (the first two values of <code class="literal">cv::Vec4f</code>) and the coordinates of one point on the line (the last two values of <code class="literal">cv::Vec4f</code>). The last two parameters specify the requested accuracy for the line parameters.</p><p class="calibre8">In general, the line equation will be used in the calculation of some properties (calibration is a good example where precise parametric representation is required). As an illustration, and to make sure we calculated the right line, let's draw the estimated line on the image. Here, we simply draw an arbitrary black segment that has a length of <code class="literal">100</code> pixels and a thickness of <code class="literal">2</code> pixels (to make it visible):</p><pre class="programlisting">    int x0= line[2];        // a point on the line 
    int y0= line[3]; 
    int x1= x0+100*line[0]; // add a vector of length 100 
    int y1= y0+100*line[1]; // using the unit vector 
    // draw the line 
    cv::line(image,cv::Point(x0,y0),cv::Point(x1,y1),  
             0,2);          // color and thickness 
</pre><p class="calibre8">The following image shows this line well aligned with one of the road's sides:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_015.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec146" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Fitting lines to a set of points is a classic problem in mathematics. The OpenCV implementation proceeds by minimizing the sum of the distances from each point to the line. Several distance functions are proposed, and the fastest option is to use the Euclidean distance, which is specified by <code class="literal">cv::DIST_L2</code>. This choice corresponds to the standard least-squares line fitting. When outliers (that is, points that don't belong on the line) are included in the point set, other distance functions that give less influence to far points can be selected. The minimization is based on the M-estimator technique, which iteratively solves a weighted least-squares problem with weights that are inversely proportional to the distance from the line.</p><p class="calibre8">Using this function, it is also possible to fit a line to a 3D point set. The input is, in this case, a set of <code class="literal">cv::Point3i</code> or <code class="literal">cv::Point3f</code> objects, and the output is a <code class="literal">std::Vec6f</code> instance.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec147" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The <code class="literal">cv::fitEllipse</code> function fits an ellipse to a set of 2D points. This returns a rotated rectangle (a <code class="literal">cv::RotatedRect</code> instance), inside which the ellipse is inscribed. In this case, you would write the following:</p><pre class="programlisting">    cv::RotatedRect rrect= cv::fitEllipse(cv::Mat(points)); 
    cv::ellipse(image,rrect,cv::Scalar(0)); 
</pre><p class="calibre8">The <code class="literal">cv::ellipse</code> function is the one you would use to draw the computed ellipse.</p></div></div>
<div class="calibre1" title="Extracting connected components"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec50" class="calibre6"/>Extracting connected components</h1></div></div></div><p class="calibre8">Images generally contain representations of objects. One of the goals of image analysis is to identify and extract these objects. In object detection/recognition applications, the first step is often to produce a binary image that shows you where certain objects of interest could be located. No matter how this binary map is obtained (for example, from the histogram back projection we performed in 
<a href="ch04.html" title="Chapter 4. Counting the Pixels with Histograms">Chapter 4</a>
, <span><em class="calibre16">Counting the Pixels with Histograms</em></span>, or from motion analysis as we will learn in <a href="ch12.html" title="Chapter 12. Processing Video Sequences">
Chapter 12
</a>, <span><em class="calibre16">Processing Video Sequences</em></span>), the next step is to extract the objects that are contained in this collection of 1s and 0s.</p><p class="calibre8">Consider, for example, the image of buffaloes in a binary form that we manipulated in 
<a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>
, <span><em class="calibre16">Transforming Images with Morphological Operations</em></span>, as shown in the following figure:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Extracting connected components" src="graphics/image_07_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We obtained this image from a simple thresholding operation followed by the application of morphological filters. This recipe will show you how to extract the objects of such images. More specifically, we will extract the connected components, that is, shapes made of a set of connected pixels in a binary image.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec148" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">OpenCV offers a simple function that extracts the contours of the connected components of an image. This is the <code class="literal">cv::findContours</code> function:</p><pre class="programlisting">    // the vector that will contain the contours 
    std::vector&lt;std::vector&lt;cv::Point&gt;&gt; contours; 
    cv::findContours(image,     
                 contours,              // a vector of contours 
                 cv::RETR_EXTERNAL,     // retrieve the external contours 
                 cv::CHAIN_APPROX_NONE);// all pixels of each contours 
</pre><p class="calibre8">The input is obviously the binary image. The output is a vector of contours, each contour being represented by a vector of <code class="literal">cv::Point</code> objects. This explains why the output parameter is defined as a <code class="literal">std::vector</code> instance of the <code class="literal">std::vector</code> instances. In addition, two flags are specified. The first one indicates that only the external contours are required, that is, holes in an object will be ignored (the <span><em class="calibre16">There's more...</em></span> section will discuss the other options).</p><p class="calibre8">The second flag is there to specify the format of the contour. With the current option, the vector will list all of the points in the contour. With the <code class="literal">cv::CHAIN_APPROX_SIMPLE</code> flag, only the endpoints for horizontal, vertical, or diagonal contours will be included. Other flags would give a more sophisticated chain approximation of the contours in order to obtain a more compact representation. With the preceding image, nine connected components are obtained as given by <code class="literal">contours.size()</code>.</p><p class="calibre8">Fortunately, there is a very convenient function that can draw the contours of those components on an image (here, a white image):</p><pre class="programlisting">    //draw black contours on a white image 
    cv::Mat result(image.size(),CV_8U,cv::Scalar(255)); 
    cv::drawContours(result,contours,
                     -1, // draw all contours 
                     0,  // in black 
                     2); // with a thickness of 2 
</pre><p class="calibre8">If the third parameter of this function is a negative value, then all contours are drawn. Otherwise, it is possible to specify the index of the contour to be drawn. The result is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_017.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec149" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The contours are extracted by a simple algorithm that consists of systematically scanning the image until a component is hit. From this starting point on the component, its contour is followed, marking the pixels on its border. When the contour is completed, the scanning resumes at the last position until a new component is found.</p><p class="calibre8">The identified connected components can then be individually analyzed. For example, if some prior knowledge is available about the expected size of the objects of interest, it becomes possible to eliminate some of the components. Let's then use a minimum and a maximum value for the perimeter of the components. This is done by iterating over the vector of contours and eliminating the invalid components:</p><pre class="programlisting">    // Eliminate too short or too long contours 
    int cmin= 50;   // minimum contour length 
    int cmax= 1000; // maximum contour length 
    std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::
               iterator itc= contours.begin(); 
    // for all contours 
    while (itc!=contours.end()) { 
    
      // verify contour size 
      if (itc-&gt;size() &lt; cmin || itc-&gt;size() &gt; cmax) 
        itc= contours.erase(itc); 
      else 
        ++itc; 
    } 
</pre><p class="calibre8">Note that this loop could have been made more efficient since each erasing operation in a <code class="literal">std::vector</code> instance is O(N). However, considering the small size of this vector, the overall cost is not too high.</p><p class="calibre8">This time, we draw the remaining contours on the original image and obtain the following result:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_07_018.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">We were lucky enough to find a simple criterion that allowed us to identify all objects of interest in this image. In more complex situations, a more refined analysis of the components' properties is required. This is the object of the next recipe, <span><em class="calibre16">Computing components' shape descriptors</em></span>.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec150" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">With the <code class="literal">cv::findContours</code> function, it is also possible to include all closed contours in the binary map, including the ones formed by holes in the components. This is done by specifying another flag in the function call:</p><pre class="programlisting">       cv::findContours(image,  
                        contours,               // a vector of contours  
                        cv::RETR_LIST,          // retrieve all contours 
                        cv::CHAIN_APPROX_NONE); // all pixels 
</pre><p class="calibre8">With this call, the following contours are obtained:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="There's more..." src="graphics/image_07_019.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Notice the extra contours that were added in the background forest. It is also possible to have these contours organized into a hierarchy. The main component is the parent, holes in it are its children, and if there are components inside these holes, they become the children of the previous children, and so on. This hierarchy is obtained by using the <code class="literal">cv::RETR_TREE</code> flag, as follows:</p><pre class="programlisting">    std::vector&lt;cv::Vec4i&gt; hierarchy; 
    cv::findContours(image, contours, // a vector of contours 
            hierarchy,                // hierarchical representation  
            cv::RETR_TREE,            // contours in tree format 
            cv::CHAIN_APPROX_NONE);   //all pixels of each contours 
</pre><p class="calibre8">In this case, each contour has a corresponding hierarchy element at the same index, made of four integers. The first two integers give you the index of the next and the previous contours of the same level, and the next two integers give you the index of the first child and the parent of this contour. A negative index indicates the end of a contour list. The <code class="literal">cv::RETR_CCOMP</code> flag is similar but limits the hierarchy at two levels.</p></div></div>
<div class="calibre1" title="Computing components' shape descriptors"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch07lvl1sec51" class="calibre6"/>Computing components' shape descriptors</h1></div></div></div><p class="calibre8">A connected component often corresponds to the image of an object in a pictured scene. To identify this object, or to compare it with other image elements, it can be useful to perform some measurements on the component in order to extract some of its characteristics. In this recipe, we will look at some of the shape descriptors available in OpenCV that can be used to describe the shape of a connected component.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec151" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Many OpenCV functions are available when it comes to shape description. We will apply some of them on the components that we have extracted in the preceding recipe. In particular, we will use our vector of four contours corresponding to the four buffaloes we previously identified. In the following code snippets, we compute a shape descriptor on the contours (<code class="literal">contours[0]</code> to <code class="literal">contours[3]</code>) and draw the result (with a thickness of <code class="literal">2</code>) over the image of the contours (with a thickness of <code class="literal">1</code>). This image is shown at the end of this section.</p><p class="calibre8">The first one is the bounding box, which is applied to the bottom-right component:</p><pre class="programlisting">    // testing the bounding box  
    cv::Rect r0= cv::boundingRect(contours[0]); 
    // draw the rectangle 
    cv::rectangle(result,r0, 0, 2); 
</pre><p class="calibre8">The minimum enclosing circle is similar. It is applied to the upper-right component:</p><pre class="programlisting">    // testing the enclosing circle  
    float radius; 
    cv::Point2f center; 
    cv::minEnclosingCircle(contours[1],center,radius); 
    // draw the circle 
    cv::circle(result,center,static_cast&lt;int&gt;(radius), 
               cv::Scalar(0),2); 
</pre><p class="calibre8">The polygonal approximation of a component's contour is computed as follows (on the left-hand component):</p><pre class="programlisting">    // testing the approximate polygon 
    std::vector&lt;cv::Point&gt; poly; 
    cv::approxPolyDP(contours[2],poly,5,true); 
    // draw the polygon 
    cv::polylines(result, poly, true, 0, 2); 
</pre><p class="calibre8">Notice the polygon drawing function, <code class="literal">cv::polylines</code>. This operates similarly to the other drawing functions. The third Boolean parameter is used to indicate whether the contour is closed or not (if yes, the last point is linked to the first one).</p><p class="calibre8">The convex hull is another form of polygonal approximation (on the second component from the left):</p><pre class="programlisting">    // testing the convex hull 
    std::vector&lt;cv::Point&gt; hull; 
    cv::convexHull(contours[3],hull); 
    // draw the polygon 
    cv::polylines(result, hull, true, 0, 2); 
</pre><p class="calibre8">Finally, the computation of the moments is another powerful descriptor (the center of mass is drawn inside all components):</p><pre class="programlisting">    // testing the moments 
    // iterate over all contours 
    itc= contours.begin(); 
    while (itc!=contours.end()) { 
 
      // compute all moments 
      cv::Moments mom= cv::moments(cv::Mat(*itc++)); 
   
      // draw mass center 
      cv::circle(result, 
                 // position of mass center converted to integer 
                 cv::Point(mom.m10/mom.m00,mom.m01/mom.m00), 
                 2, cv::Scalar(0),2); // draw black dot 
    } 
</pre><p class="calibre8">The resulting image is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_07_020.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec152" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The bounding box of a component is probably the most compact way to represent and localize a component in an image. It is defined as the upright rectangle of minimum size that completely contains the shape. Comparing the height and width of the box gives you an indication about the vertical or horizontal dimensions of the object (for example, one could use a height-to-width ratio in order to distinguish an image of a car from one of a pedestrian). The minimum enclosing circle is generally used when only the approximate component size and location is required.</p><p class="calibre8">The polygonal approximation of a component is useful when one wants to manipulate a more compact representation that resembles the component's shape. It is created by specifying an accuracy parameter, giving you the maximal acceptable distance between a shape and its simplified polygon. It is the fourth parameter in the <code class="literal">cv::approxPolyDP</code> function. The result is a vector of <code class="literal">cv::Point</code>, which corresponds to the vertices of the polygon. To draw this polygon, we need to iterate over the vector and link each point with the next one by drawing a line between them.</p><p class="calibre8">The convex hull, or convex envelope, of a shape is the minimal convex polygon that encompasses a shape. It can be visualized as the shape that an elastic band would take if placed around the component. As can be seen, the convex hull contour will deviate from the original one at the concave locations of the shape contour.</p><p class="calibre8">These locations are often designated as convexity defects, and a special OpenCV function is available to identify them: the <code class="literal">cv::convexityDefects</code> function. It is called as follows:</p><pre class="programlisting">    std::vector&lt;cv::Vec4i&gt; defects; 
    cv::convexityDefects(contour, hull, defects); 
</pre><p class="calibre8">The <code class="literal">contour</code> and <code class="literal">hull</code> arguments are, respectively, the original and the convex hull contours (both represented with <code class="literal">std::vector&lt;cv::Point&gt;</code> instances). The output is a vector of four integer elements. The first two integers are the indices of the points on the contour, delimiting the defect; the third integer corresponds to the farthest point inside the concavity, and finally, the last integer corresponds to the distance between this farthest point and the convex hull.</p><p class="calibre8">Moments are commonly used mathematical entities in the structural analysis of shapes. OpenCV has defined a data structure that encapsulates all computed moments of a shape. It is the object returned by the <code class="literal">cv::moments</code> function. Together, the moments represent a compact description of the shape of an object. They are commonly used, for example, in character recognition. We simply use this structure to obtain the mass center of each component that is computed from the first three spatial moments here.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch07lvl2sec153" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Other structural properties can be computed using the available OpenCV functions. The <code class="literal">cv::minAreaRect</code> function computes the minimum enclosed rotated rectangle (this was used in 
<a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>
, <span><em class="calibre16">Transforming Images with Morphological Operations</em></span>, in the <span><em class="calibre16">Extracting distinctive regions using MSER</em></span> recipe). The <code class="literal">cv::contourArea</code> function estimates the area of (the number of pixels inside) a contour. The <code class="literal">cv::pointPolygonTest</code> function determines whether a point is inside or outside a contour, and <code class="literal">cv::matchShapes</code> measures the resemblance between two contours. All these property measures can be advantageously combined in order to perform more advanced structural analysis.</p><div class="calibre1" title="Quadrilateral detection"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch07lvl3sec32" class="calibre6"/>Quadrilateral detection</h3></div></div></div><p class="calibre8">The MSER features presented in 
<a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>
, <span><em class="calibre16">Transforming Images with Morphological Operations</em></span>, constitutes an efficient tool to extract shapes in an image. Considering the MSER result obtained in the preceding chapter, we will now build an algorithm to detect quadrilateral components in an image. In the case of the current image, this detection will allow us to identify the building's windows. A binary version of the MSER image is easily obtained, as follows:</p><pre class="programlisting">    // create a binary version 
    components= components==255; 
    // open the image (white background) 
    cv::morphologyEx(components,components,  
                     cv::MORPH_OPEN,cv::Mat(),
                     cv::Point(-1,-1),3); 
</pre><p class="calibre8">In addition, we cleaned the image with a morphological filter. The image is then as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Quadrilateral detection" src="graphics/image_07_021.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The next step is to obtain the contours:</p><pre class="programlisting">    //invert image (background must be black) 
    cv::Mat componentsInv= 255-components; 
    //Get the contours of the connected components 
    cv::findContours(componentsInv, 
                     contours,          // a vector of contours 
                     cv::RETR_EXTERNAL, // retrieve the external contours 
                     cv::CHAIN_APPROX_NONE); 
</pre><p class="calibre8">Finally, we go over all the contours and roughly approximate them with a polygon:</p><pre class="programlisting">    // white image 
    cv::Mat quadri(components.size(),CV_8U,255); 
 
    // for all contours 
    std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::iterator it= contours.begin(); 
    while (it!= contours.end()) { 
      poly.clear(); 
      // approximate contour by polygon 
      cv::approxPolyDP(*it,poly,10,true); 
   
       // do we have a quadrilateral? 
      if (poly.size()==4) { 
        //draw it 
        cv::polylines(quadri, poly, true, 0, 2); 
      } 
      ++it; 
    } 
</pre><p class="calibre8">The quadrilaterals are those polygons that have four edges. The detected ones are the following:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="Quadrilateral detection" src="graphics/image_07_022.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">To detect rectangles, you can simply measure the angles between adjacent edges and reject the quadrilaterals that have angles that deviate too much from 90 degrees.</p></div></div></div></body></html>