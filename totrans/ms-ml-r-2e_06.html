<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Classification and Regression Trees</h1>
            </header>

            <article>
                
<div class="packt_quote">"The classifiers most likely to be the best are the random forest (RF) versions, the best of which (implemented in R and accessed via caret), achieves 94.1 percent of the maximum accuracy overcoming 90 percent in the 84.3 percent of the data sets."<br/>
                                                                                           - Fern<em>á</em>ndez-Delgado et al. (2014)</div>
<p>This quote from <em>Fernández-Delgado et al</em>. in the <em>Journal of Machine Learning Research</em> is meant to demonstrate that the techniques in this chapter are quite powerful, particularly when used for classification problems. Certainly, they don't always offer the best solution but they do provide a good starting point.</p>
<p>In the previous chapters, we examined the techniques used to predict either a quantity or a label classification. Here, we will apply them to both types of problems. We will also approach the business problem differently than in the previous chapters. Instead of defining a new problem, we will apply the techniques to some of the issues that we already tackled, with an eye to see if we can improve our predictive power. For all intents and purposes, the business case in this chapter is to see if we can improve on the models that we selected before.</p>
<p>The first item of discussion is the basic decision tree, which is both simple to build and to understand. However, the single decision tree method does not perform as well as the other methods that you learned, for example, the support vector machines, or as the ones that we will learn, such as the neural networks. Therefore, we will discuss the creation of multiple, sometimes hundreds, of different trees with their individual results combined, leading to a single overall prediction.<br/>
These methods, as the paper referenced at the beginning of this chapter states, perform as well as, or better than, any technique in this book. These methods are known as <strong>random forests</strong> and <strong>gradient boosted trees</strong>. Additionally, we will take a break from a business case and show how employing the random forest method on a dataset can assist in feature elimination/selection. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of the techniques</h1>
            </header>

            <article>
                
<p>We will now get to an overview of the techniques, covering the regression and classification trees, random forests, and gradient boosting. This will set the stage for the practical business cases.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Understanding the regression trees</h1>
            </header>

            <article>
                
<p>To establish an understanding of tree-based methods, it is probably easier to start with a quantitative outcome and then move on to how it works in a classification problem. The essence of a tree is that the features are partitioned, starting with the first split that improves the RSS the most. These binary splits continue until the termination of the tree. Each subsequent split/partition is not done on the entire dataset but only on the portion of the prior split that it falls under. This top-down process is referred to as recursive partitioning. It is also a process that is <strong>greedy</strong>, a term you may stumble upon in reading about the machine learning methods. Greedy means that, during each split in the process, the algorithm looks for the greatest reduction in the RSS without any regard to how well it will perform on the latter partitions. The result is that you may end up with a full tree of unnecessary branches leading to a low bias but a high variance. To control this effect, you need to appropriately prune the tree to an optimal size after building a full tree.</p>
<p><em>Figure 6.1</em>, provides a visual of this technique in action. The data is hypothetical with 30 observations, a response ranging from 1 to 10, and two predictor features, both ranging in value from 0 to 10 named <strong>X1</strong> and <strong>X2</strong>. The tree has three splits leading to four terminal nodes. Each split is basically an <kbd>if...then</kbd> statement or uses an R syntax <kbd>ifelse()</kbd>. The first split is--if <strong>X1</strong> is less than <strong>3.5</strong>, then the response is split into four observations with an average value of <strong>2.4</strong> and the remaining 26 observations. This left branch of four observations is a terminal node as any further splits would not substantially improve the RSS. The predicted value for these four observations in that partition of the tree becomes the average. The next split is at <strong>X2 &lt; 4</strong> and finally, <strong>X1 &lt; 7.5</strong>.<br/>
An advantage of this method is that it can handle highly nonlinear relationships; however, can you see a couple of potential problems? The first issue is that an observation is given the average of the terminal node under which it falls. This can hurt the overall predictive performance (high bias). Conversely, if you keep partitioning the data further and further so as to achieve a low bias, a high variance can become an issue. As with the other methods, you can use cross-validation to select the appropriate tree depth size:</p>
<div class="CDPAlignCenter CDPAlign"><img height="185" width="296" class="image-border" src="assets/B06473_06_01.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 6.1: Regression Tree with 3 splits and 4 terminal nodes and the corresponding node average and number of observations</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification trees</h1>
            </header>

            <article>
                
<p>Classification trees operate under the same principle as regression trees, except that the splits are not determined by the RSS but an error rate. The error rate used is not what you would expect where the calculation is simply the misclassified observations divided by the total observations. As it turns out, when it comes to tree-splitting, a misclassification rate, by itself, may lead to a situation where you can gain information with a further split but not improve the misclassification rate. Let's look at an example.</p>
<p>Suppose we have a node, let's call it <strong>N0</strong>, where you have seven observations labeled <kbd>No</kbd> and three observations labeled <kbd>Yes</kbd>, and we can say that the misclassified rate is 30 percent. With this in mind, let's calculate a common alternative error measure called the <strong>Gini index</strong>. The formula for a single node Gini index is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="31" width="480" src="assets/image_06_01b.png"/></div>
<p>Then, for <kbd>N0</kbd>, the Gini is <em>1 - (.7)<sup>2</sup> - (.3)<sup>2</sup></em>, which is equal to <em>0.42</em>, versus the misclassification rate of 30 per cent.</p>
<p>Taking this example further, we will now create node <em>N1</em> with three observations from <kbd>Class 1</kbd> and none from <kbd>Class 2</kbd>, along with <em>N2</em>, which has four observations from <kbd>Class 1</kbd> and three from <kbd>Class 2</kbd>. Now, the overall misclassification rate for this branch of the tree is still 30 per cent, but look at how the overall Gini index has improved:</p>
<ul>
<li><em>Gini(N1) = 1 - (3/3)<sup>2</sup> - (0/3)<sup>2</sup> = 0</em></li>
<li><em>Gini(N2) = 1 - (4/7)<sup>2</sup> - (3/7)<sup>2</sup> = 0.49</em></li>
<li><em>New Gini index = (proportion of N1 x Gini(N1)) + (proportion of N2 x Gini(N2))</em>, which is equal to <em>(.3 x 0) + (.7 x 0.49)</em> or <em>0.343</em></li>
</ul>
<p>By doing a split on a surrogate error rate, we actually improved our model impurity, reducing it from <em>0.42</em> to <em>0.343</em>, whereas the misclassification rate did not change. This is the methodology that is used by the <kbd>rpart()</kbd> package, which we will be using in this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forest</h1>
            </header>

            <article>
                
<p>To greatly improve our model's predictive ability, we can produce numerous trees and combine the results. The random forest technique does this by applying two different tricks in model development. The first is the use of <strong>bootstrap aggregation</strong> or <strong>bagging</strong>, as it is called.</p>
<p>In bagging, an individual tree is built on a random sample of the dataset, roughly two-thirds of the total observations (note that the remaining one-third is referred to as <strong>out-of-bag</strong> (<strong>oob</strong>)). This is repeated dozens or hundreds of times and the results are averaged. Each of these trees is grown and not pruned based on any error measure, and this means that the variance of each of these individual trees is high. However, by averaging the results, you can reduce the variance without increasing the bias.</p>
<p>The next thing that random forest brings to the table is that concurrently with the random sample of the data--that is, bagging--it also takes a random sample of the input features at each split. In the <kbd>randomForest</kbd> package, we will use the default random number of the predictors that are sampled, which, for classification problems, is the square root of the total predictors and for regression, it is the total number of the predictors divided by three. The number of predictors the algorithm randomly chooses at each split can be changed via the model tuning process.<br/>
By doing this random sample of the features at each split and incorporating it into the methodology, you can mitigate the effect of a highly correlated predictor becoming the main driver in all of your bootstrapped trees, preventing you from reducing the variance that you hoped to achieve with bagging. The subsequent averaging of the trees that are less correlated to each other is more generalizable and robust to outliers than if you only performed bagging.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Gradient boosting</h1>
            </header>

            <article>
                
<p>Boosting methods can become extremely complicated to learn and understand, but you should keep in mind what is fundamentally happening behind the curtain. The main idea is to build an initial model of some kind (linear, spline, tree, and so on) called the base learner, examine the residuals, and fit a model based on these residuals around the so-called <strong>loss function</strong>. A loss function is merely the function that measures the discrepancy between the model and desired prediction, for example, a squared error for regression or the logistic function for classification. The process continues until it reaches some specified stopping criterion. This is sort of like the student who takes a practice exam and gets 30 out of 100 questions wrong and, as a result, studies only these 30 questions that were missed. In the next practice exam, they get 10 out of those 30 wrong and so only focus on those 10 questions, and so on. If you would like to explore the theory behind this further, a great resource for you is available in Frontiers in Neurorobotics, <em>Gradient boosting machines, a tutorial</em>, Natekin A., Knoll A. (2013), at <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/"><span class="URLPACKT">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/</span></a>.</p>
<p>As just mentioned, boosting can be applied to many different base learners, but here we will only focus on the specifics of <strong>tree-based learning</strong>. Each tree iteration is small and we will determine how small with one of the tuning parameters referred to as interaction depth. In fact, it may be as small as one split, which is referred to as a stump.</p>
<p>Trees are sequentially fit to the residuals, according to the loss function, up to the number of trees that we specified (our stopping criterion).</p>
<p>There are a number of parameters that require tuning in the model-building process using the <kbd>Xgboost</kbd> package, which stands for <strong>eXtreme Gradient Boosting</strong>. This package has become quite popular for online data contests because of its winning performance. There is excellent background material on boosting trees and on Xgboost on the following website:</p>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html">http://xgboost.readthedocs.io/en/latest/model.html</a><br/>
What we will do in the business case is show how to begin to optimize the hyperparameters and produce meaningful output and predictions. These parameters can interact with each other, and if you just tinker with one without considering the other, your model may worsen the performance. The <kbd>caret</kbd> package will help us in the tuning endeavor.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business case</h1>
            </header>

            <article>
                
<p>The overall business objective in this situation is to see whether we can improve the predictive ability for some of the cases that we already worked on in the previous chapters. For regression, we will revisit the prostate cancer dataset from <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em>. The baseline mean squared error to improve on is 0.444.</p>
<p>For classification purposes, we will utilize both the breast cancer biopsy data from <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Logistic Regression and Discriminant Analysis</em> and the Pima Indian Diabetes data from <a href="a7511867-5362-4215-a7dd-bbdc162740d1.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>More Classification Techniques - K-Nearest Neighbors and Support Vector Machines</em>. In the breast cancer data, we achieved 97.6 per cent predictive accuracy. For the diabetes data, we are seeking to improve on the 79.6 per cent accuracy rate.</p>
<p>Both random forests and boosting will be applied to all three datasets. The simple tree method will only be used on the breast and prostate cancer sets from <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>To perform the modeling process, we will need to load seven different R packages. Then, we will go through each of the techniques and compare how well they perform on the data analyzed with the prior methods in the previous chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regression tree</h1>
            </header>

            <article>
                
<p>We will jump right into the <kbd>prostate</kbd> dataset, but let's first load the necessary R packages. As always, please ensure that you have the libraries installed prior to loading the packages:</p>
<pre>
  <strong>&gt; library(rpart) #classification and regression trees</strong><br/>  <strong>&gt; library(partykit) #treeplots</strong><br/>  <strong>&gt; library(MASS) #breast and pima indian data</strong><br/>  <strong>&gt; library(ElemStatLearn) #prostate data</strong><br/>  <strong>&gt; library(randomForest) #random forests</strong><br/>  <strong>&gt; library(xgboost) #gradient boosting</strong><br/>  <strong>&gt; library(caret) #tune hyper-parameters</strong>
</pre>
<p>We will first do regression with the <kbd>prostate</kbd> data and prepare it, as we did in <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em>. This involves calling the dataset, coding the <kbd>gleason</kbd> score as an indicator variable using the <kbd>ifelse()</kbd> function, and creating the <kbd>test</kbd> and <kbd>train</kbd> sets. The <kbd>train</kbd> set will be <kbd>pros.train</kbd> and the <kbd>test</kbd> set will be <kbd>pros.test</kbd>, as follows:</p>
<pre>
  <strong>&gt; data(prostate)</strong><br/>  <strong>&gt; prostate$gleason &lt;- ifelse(prostate$gleason == 6, 0, 1)</strong><br/>  <strong>&gt; pros.train &lt;- subset(prostate, train == TRUE)[, 1:9]</strong><br/>  <strong>&gt; pros.test &lt;- subset(prostate, train == FALSE)[, 1:9]</strong>
</pre>
<p>To build a regression tree on the <kbd>train</kbd> data, we will use the <kbd>rpart()</kbd> function from R's <kbd>party</kbd> package. The syntax is quite similar to what we used in the other modeling techniques:</p>
<pre>
  <strong>&gt; tree.pros &lt;- rpart(lpsa ~ ., data = pros.train)</strong>
</pre>
<p>We can call this object and examine the error per number of splits in order to determine the optimal number of splits in the tree:</p>
<pre>
  <strong>&gt; print(tree.pros$cptable)</strong><br/>  <strong>   CP nsplit rel error  xerror   xstd</strong><br/>  <strong>1 0.35852251   0 1.0000000 1.0364016 0.1822698</strong><br/>  <strong>2 0.12295687   1 0.6414775 0.8395071 0.1214181</strong><br/>  <strong>3 0.11639953   2 0.5185206 0.7255295 0.1015424</strong><br/>  <strong>4 0.05350873   3 0.4021211 0.7608289 0.1109777</strong><br/>  <strong>5 0.01032838   4 0.3486124 0.6911426 0.1061507</strong><br/>  <strong>6 0.01000000   5 0.3382840 0.7102030 0.1093327</strong>
</pre>
<p>This is a very important table to analyze. The first column labeled <kbd>CP</kbd> is the cost complexity parameter. The second column, <kbd>nsplit</kbd>, is the number of splits in the tree. The <kbd>rel error</kbd> column stands for relative error and is the RSS for the number of splits divided by the RSS for no splits <em>RSS(k)/RSS(0)</em>. Both <kbd>xerror</kbd> and <kbd>xstd</kbd> are based on the ten-fold cross-validation with <kbd>xerror</kbd> being the average error and <kbd>xstd</kbd> the standard deviation of the cross-validation process. We can see that while five splits produced the lowest error on the <kbd>full</kbd> dataset, four splits produced a slightly less error using cross-validation. You can examine this using <kbd>plotcp()</kbd>:</p>
<pre>
  <strong>&gt; plotcp(tree.pros)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="279" width="430" class="image-border" src="assets/image_06_02.png"/></div>
<p>The plot shows us the relative error by the tree size with the corresponding error bars. The horizontal line on the plot is the upper limit of the lowest standard error. Selecting a tree size, <kbd>5</kbd>, which is four splits, we can build a new tree object where <kbd>xerror</kbd> is minimized by pruning our tree by first creating an object for <kbd>cp</kbd> associated with the pruned tree from the table. Then the <kbd>prune()</kbd> function handles the rest:</p>
<pre>
  <strong>&gt; cp &lt;- min(tree.pros$cptable[5, ])</strong><br/>  <strong>&gt; prune.tree.pros &lt;- prune(tree.pros, cp = cp)</strong>
</pre>
<p>With this done, you can plot and compare the full and pruned trees. The tree plots produced by the <kbd>partykit</kbd> package are much better than those produced by the <kbd>party</kbd> package. You can simply use the <kbd>as.party()</kbd> function as a wrapper in <kbd>plot()</kbd>:</p>
<pre>
  <strong>&gt; plot(as.party(tree.pros))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="269" width="530" class="image-border" src="assets/image_06_03.png"/></div>
<p>Now we will use the <kbd>as.party()</kbd> function for the pruned tree:</p>
<pre>
  <strong>&gt; plot(as.party(prune.tree.pros))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="262" width="516" class="image-border" src="assets/image_06_04.png"/></div>
<p>Note that the splits are exactly the same in the two trees with the exception of the last split, which includes the variable <kbd>age</kbd> for the full tree. Interestingly, both the first and second splits in the tree are related to the log of cancer volume (<kbd>lcavol</kbd>). These plots are quite informative as they show the splits, nodes, observations per node, and boxplots of the outcome that we are trying to predict.</p>
<p>Let's see how well the pruned tree performs on the <kbd>test</kbd> data. What we will do is create an object of the predicted values using the <kbd>predict()</kbd> function and incorporate the <kbd>test</kbd> data. Then, calculate the errors (the predicted values minus the actual values) and finally, the mean of the squared errors:</p>
<pre>
  <strong>&gt; party.pros.test &lt;- predict(prune.tree.pros, newdata = pros.test)</strong><br/>  <strong>&gt; rpart.resid &lt;- party.pros.test - pros.test$lpsa </strong><br/>  <strong>&gt; mean(rpart.resid^2) #caluclate MSE</strong><br/>  <strong>[1] 0.5267748</strong>
</pre>
<p>We have not improved on the predictive value from our work in <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em> where the baseline MSE was <kbd>0.44</kbd>. However, the technique is not without value. One can look at the tree plots that we produced and easily explain what the primary drivers behind the response are. As mentioned in the introduction, the trees are easy to interpret and explain, which may be more important than accuracy in many cases.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification tree</h1>
            </header>

            <article>
                
<p>For the classification problem, we will prepare the breast cancer data in the same fashion as we did in <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Logistic Regression and Discriminant Analysis</em>. After loading the data, you will delete the patient ID, rename the features, eliminate the few missing values, and then create the <kbd>train</kbd>/<kbd>test</kbd> datasets in the following way:</p>
<pre>
  <strong>&gt; data(biopsy)</strong><br/>  <strong>&gt; biopsy &lt;- biopsy[, -1] #delete ID</strong><br/>  <strong>&gt; names(biopsy) &lt;- c("thick", "u.size", "u.shape", "adhsn", <br/>    "s.size", "nucl",<br/>  "chrom", "n.nuc", "mit", "class") #change the feature names</strong><br/>  <strong>&gt; biopsy.v2 &lt;- na.omit(biopsy) #delete the observations with <br/>    missing values</strong><br/>  <strong>&gt; set.seed(123) #random number generator</strong><br/>  <strong>&gt; ind &lt;- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, <br/>    0.3))</strong><br/>  <strong>&gt; biop.train &lt;- biopsy.v2[ind == 1, ] #the training data set</strong><br/>  <strong>&gt; biop.test &lt;- biopsy.v2[ind == 2, ] #the test data set</strong>
</pre>
<p>With the data set up appropriately, we will use the same syntax style for a classification problem as we did previously for a regression problem, but before creating a classification tree, we will need to ensure that the outcome is <kbd>Factor</kbd>, which can be done using the <kbd>str()</kbd> function:</p>
<pre>
  <strong>&gt; str(biop.test[, 10])</strong><br/>  <strong> Factor w/ 2 levels "benign","malignant": 1 1 1 1 1 2 1 2 1 1 ...</strong>
</pre>
<p>First, create the tree and then examine the table for the optimal number of splits:</p>
<pre>
  <strong>&gt; set.seed(123)</strong><br/>  <strong>&gt; tree.biop &lt;- rpart(class ~ ., data = biop.train)</strong><br/>  <strong>&gt; tree.biop$cptable</strong><br/>  <strong>  CP nsplit rel error  xerror    xstd</strong><br/>  <strong>1 0.79651163   0 1.0000000 1.0000000 0.06086254</strong><br/>  <strong>2 0.07558140   1 0.2034884 0.2674419 0.03746996</strong><br/>  <strong>3 0.01162791   2 0.1279070 0.1453488 0.02829278</strong><br/>  <strong>4 0.01000000   3 0.1162791 0.1744186 0.03082013</strong>
</pre>
<p>The cross-validation error is at a minimum with only two splits (row <kbd>3</kbd>). We can now prune the tree, plot the pruned tree, and see how it performs on the <kbd>test</kbd> set:</p>
<pre>
  <strong>&gt; cp &lt;- min(tree.biop$cptable[3, ])</strong><br/>  <strong>&gt; prune.tree.biop &lt;- prune(tree.biop, cp = cp)</strong><br/>  <strong>&gt; plot(as.party(prune.tree.biop))</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="258" width="477" class="image-border" src="assets/image_06_05.png"/></div>
<p>An examination of the tree plot shows that the uniformity of the cell size is the first split, then nuclei. The full tree had an additional split at the cell thickness. We can predict the <kbd>test</kbd> observations using <kbd>type="class"</kbd> in the <kbd>predict()</kbd> function, as follows:</p>
<pre>
  <strong>&gt; rparty.test &lt;- predict(prune.tree.biop, newdata = biop.test, type <br/>   ="class")</strong>  <br/>  <strong>&gt; table(rparty.test, biop.test$class)</strong><br/>  <strong>rparty.test benign malignant</strong><br/>  <strong> benign    136     3</strong><br/>  <strong> malignant   6    64</strong><br/>  <strong>&gt; (136+64)/209</strong><br/>  <strong>[1] 0.9569378</strong>
</pre>
<p>The basic tree with just two splits gets us almost 96 percent accuracy. This still falls short of 97.6 percent with logistic regression but should encourage us to believe that we can improve on this with the upcoming methods, starting with random forests.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forest regression</h1>
            </header>

            <article>
                
<p>In this section, we will start by focusing on the <kbd>prostate</kbd> data again. Before moving on to the breast cancer and Pima Indian sets. We will use the <kbd>randomForest</kbd> package. The general syntax to create a <kbd>random forest</kbd> object is to use the <kbd>randomForest()</kbd> function and specify the formula and dataset as the two primary arguments. Recall that for regression, the default variable sample per tree iteration is p/3, and for classification, it is the square root of p, where p is equal to the number of predictor variables in the data frame. For larger datasets, in terms of p, you can tune the <kbd>mtry</kbd> parameter, which will determine the number of p sampled at each iteration. If p is less than 10 in these examples, we will forgo this procedure. When you want to optimize <kbd>mtry</kbd> for larger p datasets, you can utilize the <kbd>caret</kbd> package or use the <kbd>tuneRF()</kbd> function in <kbd>randomForest</kbd>. With this, let's build our forest and examine the results, as follows:</p>
<pre>
  <strong>&gt; set.seed(123)</strong><br/>  <strong>&gt; rf.pros &lt;- randomForest(lpsa ~ ., data = pros.train)</strong><br/>  <strong>&gt; rf.pros</strong><br/>  <strong>Call:</strong><br/><strong>  randomForest(formula = lpsa ~ ., data = pros.train) </strong><br/><strong>  Type of random forest: regression</strong><br/><strong>  Number of trees: 500</strong><br/><strong>  No. of variables tried at each split: 2</strong><br/><strong>  Mean of squared residuals: 0.6792314</strong><br/><strong>  % Var explained: 52.73<br/></strong>
</pre>
<p>The call of the <kbd>rf.pros</kbd> object shows us that the random forest generated <kbd>500</kbd> different trees (the default) and sampled two variables at each split. The result is an MSE of <kbd>0.68</kbd> and nearly 53 percent of the variance explained. Let's see if we can improve on the default number of trees. Too many trees can lead to overfitting; naturally, how many is too many depends on the data. Two things can help out, the first one is a plot of <kbd>rf.pros</kbd> and the other is to ask for the minimum MSE:</p>
<pre>
  <strong>&gt; plot(rf.pros)</strong>
</pre>
<p><span>The output of the preceding command is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="255" width="410" class="image-border" src="assets/image_06_06.png"/></div>
<p>This plot shows the MSE by the number of trees in the model. You can see that as the trees are added, significant improvement in MSE occurs early on and then flatlines just before <kbd>100</kbd> trees are built in the forest.</p>
<p>We can identify the specific and optimal tree with the <kbd>which.min()</kbd> function, as follows:</p>
<pre>
  <strong>&gt; which.min(rf.pros$mse)</strong><br/>  <strong>[1] 75</strong>
</pre>
<p>We can try <kbd>75</kbd> trees in the random forest by just specifying <kbd>ntree=75</kbd> in the model syntax:</p>
<pre>
  <strong>&gt; set.seed(123)</strong><br/>  <strong>&gt; rf.pros.2 &lt;- randomForest(lpsa ~ ., data = pros.train, ntree <br/>   =75)</strong><br/>  <strong>&gt; rf.pros.2</strong><br/>  <strong>Call:</strong><br/><strong>  randomForest(formula = lpsa ~ ., data = pros.train, ntree = 75) </strong><br/><strong>  Type of random forest: regression</strong><br/><strong>  Number of trees: 75</strong><br/><strong>  No. of variables tried at each split: 2</strong><br/><strong>  Mean of squared residuals: 0.6632513</strong><br/><strong>  % Var explained: 53.85<br/></strong>
</pre>
<p>You can see that the MSE and variance explained have both improved slightly. Let's see another plot before testing the model. If we are combining the results of <kbd>75</kbd> different trees that are built using bootstrapped samples and only two random predictors, we will need a way to determine the drivers of the outcome. One tree alone cannot be used to paint this picture, but you can produce a variable importance plot and corresponding list. The y-axis is a list of variables in descending order of importance and the x-axis is the percentage of improvement in MSE. Note that for the classification problems, this will be an improvement in the Gini index. The function is <kbd>varImpPlot()</kbd>:</p>
<pre>
  <strong>&gt; varImpPlot(rf.pros.2, scale = T, <br/>   main = "Variable Importance Plot - PSA Score")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="266" width="427" class="image-border" src="assets/image_06_07.png"/></div>
<p>Consistent with the single tree, <kbd>lcavol</kbd> is the most important variable and <kbd>lweight</kbd> is the second-most important variable. If you want to examine the raw numbers, use the <kbd>importance()</kbd> function, as follows:</p>
<pre>
  <strong>&gt; importance(rf.pros.2)</strong><br/>  <strong>    IncNodePurity</strong><br/>    <strong>lcavol  24.108641</strong><br/><strong>   lweight  15.721079</strong><br/><strong>     age  6.363778</strong><br/><strong>     lbph  8.842343</strong><br/><strong>     svi  9.501436</strong><br/><strong>     lcp  9.900339</strong><br/><strong>   gleason  0.000000</strong><br/><strong>    pgg45  8.088635<br/></strong>
</pre>
<p>Now, it is time to see how it did on the <kbd>test</kbd> data:</p>
<pre>
  <strong>&gt; rf.pros.test &lt;- predict(rf.pros.2, newdata = pros.test)</strong><br/>  <strong>&gt; rf.resid = rf.pros.test - pros.test$lpsa #calculate residual</strong><br/>  <strong>&gt; mean(rf.resid^2)</strong><br/>  <strong>[1] 0.5136894</strong>
</pre>
<p>The MSE is still higher than our <kbd>0.44</kbd> that we achieved in <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em> with LASSO and no better than just a single tree.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forest classification</h1>
            </header>

            <article>
                
<p>Perhaps you are disappointed with the performance of the random forest regression model, but the true power of the technique is in the classification problems. Let's get started with the breast cancer diagnosis data. The procedure is nearly the same as we did with the regression problem:</p>
<pre>
  <strong>&gt; set.seed(123)</strong> <br/>  <strong>&gt; rf.biop &lt;- randomForest(class ~. , data = biop.train)</strong><br/>  <strong>&gt; rf.biop</strong><br/>  <strong>Call:</strong><br/>  <strong> randomForest(formula = class ~ ., data = biop.train)</strong><br/>  <strong>        Type of random forest: classification</strong><br/>  <strong>           Number of trees: 500</strong><br/>  <strong>No. of variables tried at each split: 3</strong><br/>  <strong>    OOB estimate of error rate: 3.16%</strong><br/>  <strong>Confusion matrix:</strong><br/>  <strong>     benign malignant class.error</strong><br/>  <strong>benign    294     8 0.02649007</strong><br/>  <strong>malignant   7    165 0.04069767</strong>
</pre>
<p>The <kbd>OOB</kbd> error rate is <kbd>3.16%</kbd>. Again, this is with all the <strong>500</strong> trees factored into the analysis. Let's plot the <strong>Error</strong> by <strong>trees</strong>:</p>
<pre>
  <strong>&gt; plot(rf.biop)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="260" width="466" class="image-border" src="assets/image_06_08.png"/></div>
<p>The plot shows that the minimum error and standard error is the lowest with quite a few trees. Let's now pull the exact number using <kbd>which.min()</kbd> again. The one difference from before is that we need to specify column <kbd>1</kbd> to get the error rate. This is the overall error rate and there will be additional columns for each error rate by the class label. We will not need them in this example. Also, <kbd>mse</kbd> is no longer available but rather <kbd>err.rate</kbd> is used instead, as follows:</p>
<pre>
  <strong>&gt; which.min(rf.biop$err.rate[, 1])</strong><br/>  <strong>[1] 19</strong>
</pre>
<p>Only 19 trees are needed to optimize the model accuracy. Let's try this and see how it performs:</p>
<pre>
<strong>  &gt; set.seed(123)</strong><br/><strong>  &gt; rf.biop.2 &lt;- randomForest(class~ ., data = biop.train, ntree = <br/>   19)</strong><br/>  <strong>&gt; print(rf.biop.2)</strong> <br/>  <strong>Call:</strong><br/>  <strong> randomForest(formula = class ~ ., data = biop.train, ntree = 19)</strong><br/>  <strong>        Type of random forest: classification</strong><br/>  <strong>           Number of trees: 19</strong><br/>  <strong>No. of variables tried at each split: 3</strong><br/>  <strong>    OOB estimate of error rate: 2.95%</strong><br/>  <strong>Confusion matrix:</strong><br/>  <strong>     benign malignant class.error</strong><br/>  <strong>benign    294     8 0.02649007</strong><br/>  <strong>malignant   6    166 0.03488372</strong><br/>  <strong>&gt; rf.biop.test &lt;- predict(rf.biop.2, newdata = biop.test, type = <br/>   "response")</strong><br/>  <strong>&gt; table(rf.biop.test, biop.test$class)</strong><br/>  <strong>rf.biop.test benign malignant</strong><br/>  <strong>  benign    139     0</strong><br/>  <strong>  malignant   3    67</strong><br/>  <strong>&gt; (139 + 67) / 209</strong><br/>  <strong>[1] 0.9856459</strong>
</pre>
<p>Well, how about that? The <kbd>train</kbd> set error is below 3 percent, and the model even performs better on the <kbd>test</kbd> set where we had only three observations misclassified out of <kbd>209</kbd> and none were false positives. Recall that the best so far was with logistic regression with 97.6 percent accuracy. So this seems to be our best performer yet on the breast cancer data. Before moving on, let's have a look at the variable importance plot:</p>
<pre>
  <strong>&gt; varImpPlot(rf.biop.2)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="488" class="image-border" src="assets/image_06_09.png"/></div>
<p>The importance in the preceding plot is in each variable's contribution to the mean decrease in the Gini index. This is rather different from the splits of the single tree. Remember that the full tree had splits at the size (consistent with random forest), then nuclei, and then thickness. This shows how potentially powerful a technique building random forests can be, not only in the predictive ability, but also in feature selection.</p>
<p>Moving on to the tougher challenge of the Pima Indian diabetes model, we will first need to prepare the data in the following way:</p>
<pre>
  <strong>&gt; data(Pima.tr)</strong><br/>  <strong>&gt; data(Pima.te)</strong><br/>  <strong>&gt; pima &lt;- rbind(Pima.tr, Pima.te)</strong><br/>  <strong>&gt; set.seed(502)</strong><br/>  <strong>&gt; ind &lt;- sample(2, nrow(pima), replace = TRUE, prob = c(0.7, 0.3))</strong><br/>  <strong>&gt; pima.train &lt;- pima[ind == 1, ]</strong><br/>  <strong>&gt; pima.test &lt;- pima[ind == 2, ]</strong>
</pre>
<p>Now we will move on to the building of the model, as follows:</p>
<pre>
  <strong>&gt; set.seed(321)</strong> <br/>  <strong>&gt; rf.pima = randomForest(type~., data=pima.train)</strong><br/>  <strong>&gt; rf.pima</strong><br/>  <strong>Call:</strong><br/>  <strong> randomForest(formula = type ~ ., data = pima.train)</strong><br/>  <strong>        Type of random forest: classification</strong><br/>  <strong>           Number of trees: 500</strong><br/>  <strong>No. of variables tried at each split: 2</strong><br/>  <strong>    OOB estimate of error rate: 20%</strong><br/>  <strong>Confusion matrix:</strong><br/>  <strong>   No Yes class.error</strong><br/>  <strong>No 233 29  0.1106870</strong><br/>  <strong>Yes 48 75  0.3902439</strong>
</pre>
<p>We get a <kbd>20</kbd> per cent misclassification rate error, which is no better than what we've done before on the <kbd>train</kbd> set. Let's see if optimizing the tree size can improve things dramatically:</p>
<pre>
  <strong>&gt; which.min(rf.pima$err.rate[, 1])</strong><br/>  <strong>[1] 80</strong><br/>  <strong>&gt; set.seed(321)</strong><br/>  <strong>&gt; rf.pima.2 = randomForest(type~., data=pima.train, ntree=80)</strong><br/>  <strong>&gt; print(rf.pima.2)</strong><br/>  <strong>Call:</strong><br/>  <strong> randomForest(formula = type ~ ., data = pima.train, ntree = 80)</strong><br/>  <strong>        Type of random forest: classification</strong><br/>  <strong>           Number of trees: 80</strong><br/>  <strong>No. of variables tried at each split: 2</strong><br/>  <strong>    OOB estimate of error rate: 19.48%</strong><br/>  <strong>Confusion matrix:</strong><br/>  <strong>   No Yes class.error</strong><br/>  <strong>No 230 32  0.1221374</strong><br/>  <strong>Yes 43 80  0.3495935</strong>
</pre>
<p>At <kbd>80</kbd> trees in the forest, there is minimal improvement in the <kbd>OOB</kbd> error. Can random forest live up to the hype on the <kbd>test</kbd> data? We will see in the following way:</p>
<pre>
  <strong>&gt; rf.pima.test &lt;- predict(rf.pima.2, newdata= pima.test, <br/>   type = "response")</strong><br/>  <strong>&gt; table(rf.pima.test, pima.test$type)</strong><br/>  <strong>rf.pima.test No Yes</strong><br/>  <strong>     No 75 21</strong><br/>  <strong>     Yes 18 33</strong><br/>  <strong>&gt; (75+33)/147</strong><br/>  <strong>[1] 0.7346939</strong>
</pre>
<p>Well, we get only 73 percent accuracy on the <kbd>test</kbd> data, which is inferior to what we achieved using the SVM.</p>
<p>While random forest disappointed on the diabetes data, it proved to be the best classifier so far for the breast cancer diagnosis. Finally, we will move on to gradient boosting.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extreme gradient boosting - classification</h1>
            </header>

            <article>
                
<p>As mentioned previously, we will be using the <kbd>xgboost</kbd> package in this section, which we have already loaded. Given the method's well-earned reputation, let's try it on the diabetes data.</p>
<p>As stated in the boosting overview, we will be tuning a number of parameters:</p>
<ul>
<li><kbd>nrounds</kbd>: The maximum number of iterations (number of trees in final model).</li>
<li><kbd>colsample_bytree</kbd>: The number of features, expressed as a ratio, to sample when building a tree. Default is 1 (100% of the features).</li>
<li><kbd>min_child_weight</kbd>: The minimum weight in the trees being boosted. Default is 1.</li>
<li><kbd>eta</kbd>: Learning rate, which is the contribution of each tree to the solution. Default is 0.3.</li>
<li><kbd>gamma</kbd>: Minimum loss reduction required to make another leaf partition in a tree.</li>
<li><kbd>subsample</kbd>: Ratio of data observations. Default is 1 (100%).</li>
<li><kbd>max_depth</kbd>: Maximum depth of the individual trees.</li>
</ul>
<div class="packt_infobox">Using the <kbd>expand.grid()</kbd> function, we will build our experimental grid to run through the training process of the <kbd>caret</kbd> package. If you do not specify values for all of the preceding parameters, even if it is just a default, you will receive an error message when you execute the function. The following values are based on a number of training iterations I've done previously. I encourage you to try your own tuning values.</div>
<p>Let's build the grid as follows:</p>
<pre>
<strong>  &gt; grid = expand.grid(</strong><br/><strong>   nrounds = c(75, 100),</strong><br/><strong>   colsample_bytree = 1,</strong><br/><strong>   min_child_weight = 1,</strong><br/><strong>   eta = c(0.01, 0.1, 0.3), #0.3 is default,</strong><br/><strong>   gamma = c(0.5, 0.25),</strong><br/><strong>   subsample = 0.5,</strong><br/><strong>   max_depth = c(2, 3)</strong><br/><strong>   )<br/></strong>
</pre>
<p>This creates a grid of 24 different models that the <kbd>caret</kbd> package will run so as to determine the best tuning parameters. A note of caution is in order. On a dataset of the size that we will be working with, this process takes only a few seconds. However, in large datasets, this can take hours. As such, you must apply your judgment and experiment with smaller samples of the data in order to identify the tuning parameters, in case the time is of the essence, or you are constrained by the size of your hard drive.</p>
<p>Before using the <kbd>train()</kbd> function from the <kbd>caret</kbd> package, I would like to specify the <kbd>trainControl</kbd> argument by creating an object called <kbd>control</kbd>. This object will store the method that we want so as to train the tuning parameters. We will use the <kbd>5</kbd> fold cross-validation, as follows:</p>
<pre>
<strong>  &gt; cntrl = trainControl(<br/>   method = "cv",<br/>   number = 5,<br/>   verboseIter = TRUE,<br/>   returnData = FALSE,<br/>   returnResamp = "final" <br/>   )</strong><strong><br/></strong>
</pre>
<p>To utilize the <kbd>train.xgb()</kbd> function, just specify the formula as we did with the other models: the <kbd>train</kbd> dataset inputs, labels, method, train control, and experimental grid. Remember to set the random seed:</p>
<pre>
<strong>  &gt; set.seed(1)</strong><br/><strong>  &gt; train.xgb = train(</strong><br/><strong>   x = pima.train[, 1:7],</strong><br/><strong>   y = ,pima.train[, 8],</strong><br/><strong>   trControl = cntrl,</strong><br/><strong>   tuneGrid = grid,</strong><br/><strong>   method = "xgbTree"</strong><br/><strong>   ) </strong>
</pre>
<p>Since in <kbd>trControl</kbd> I set <kbd>verboseIter</kbd> to <kbd>TRUE</kbd>, you should have seen each training iteration within each k-fold.</p>
<p>Calling the object gives us the optimal parameters and the results of each of the parameter settings, as follows (abbreviated for simplicity):</p>
<pre>
<strong>  &gt; train.xgb</strong><br/><strong>  eXtreme Gradient Boosting </strong><br/><strong>  No pre-processing</strong><br/><strong>  Resampling: Cross-Validated (5 fold) </strong><br/><strong>  Summary of sample sizes: 308, 308, 309, 308, 307 </strong><br/><strong>  Resampling results across tuning parameters:</strong><br/><strong>  eta max_depth gamma nrounds Accuracy  Kappa </strong><br/><strong>  0.01 2     0.25  75    0.7924286 0.4857249</strong><br/><strong>  0.01 2     0.25  100    0.7898321 0.4837457</strong><br/><strong>  0.01 2     0.50  75    0.7976243 0.5005362</strong><br/><strong>  ...................................................</strong><br/><strong>  0.30 3     0.50  75    0.7870664 0.4949317</strong><br/><strong>  0.30 3     0.50  100    0.7481703 0.3936924</strong><br/><strong>  Tuning parameter 'colsample_bytree' was held constant at a <br/>  value of </strong><strong>1</strong><br/><strong>  Tuning parameter 'min_child_weight' was held constant at a <br/>  value of </strong><strong>1</strong><br/><strong>  Tuning parameter 'subsample' was held constant at a value of 0.5</strong><br/><strong>  Accuracy was used to select the optimal model using the largest <br/>   value.</strong><br/><strong>  The final values used for the model were nrounds = 75, max_depth = <br/>   2,</strong><br/><strong>  eta = 0.1, gamma = 0.5, colsample_bytree = 1, min_child_weight = 1</strong><br/><strong>  and subsample = 0.5.</strong>
</pre>
<p>This gives us the best combination of parameters to build a model. The accuracy in the training data was 81% with a Kappa of 0.55. Now it gets a little tricky, but this is what I've seen as best practice. First, create a list of parameters that will be used by the <kbd>xgboost</kbd> training function, <kbd>xgb.train()</kbd>. Then, turn the dataframe into a matrix of input features and a list of labeled numeric outcomes (0s and 1s). Then further, turn the features and labels into the input required, as <kbd>xgb.Dmatrix</kbd>. Try this:</p>
<pre>
<strong>  &gt; param &lt;- list( objective = "binary:logistic", <br/>   booster = "gbtree",<br/>   eval_metric = "error",<br/>   eta = 0.1, <br/>   max_depth = 2, <br/>   subsample = 0.5,<br/>   colsample_bytree = 1,<br/>   gamma = 0.5<br/>   )  <br/>  &gt; x &lt;- as.matrix(pima.train[, 1:7])</strong><br/><strong>  &gt; y &lt;- ifelse(pima.train$type == "Yes", 1, 0)</strong><br/><strong>  &gt; train.mat &lt;- xgb.DMatrix(data = x, </strong><strong>label = y)<br/></strong>
</pre>
<p>With all of that prepared, just create the model:</p>
<pre>
<strong>  &gt; set.seed(1)</strong><br/><strong>  &gt; xgb.fit &lt;- xgb.train(params = param, data = train.mat, nrounds = <br/>   75)<br/></strong>
</pre>
<p>Before seeing how it does on the test set, let's check the variable importance and also plot it. You can examine three items: <strong>gain</strong>, <strong>cover</strong>, and<strong> frequency</strong>. <strong>Gain</strong> is the improvement in accuracy that feature brings to the branches it is on. <strong>Cover</strong> is the relative number of total observations related to this feature. <strong>Frequency</strong> is the per cent of times that feature occurs in all of the trees. The following code produces the desired output:</p>
<pre>
<strong>  &gt; impMatrix &lt;- xgb.importance(feature_names = dimnames(x)[[2]], <br/>   model = xgb.fit)</strong><strong><br/>  &gt; impMatrix</strong><br/><strong>    Feature    Gain   Cover Frequency</strong><br/><strong>  1:   glu 0.40000548 0.31701688 0.24509804</strong><br/><strong>  2:   age 0.16177609 0.15685050 0.17156863</strong><br/><strong>  3:   bmi 0.12074049 0.14691325 0.14705882</strong><br/><strong>  4:   ped 0.11717238 0.15400331 0.16666667</strong><br/><strong>  5:  npreg 0.07642333 0.05920868 0.06862745</strong><br/><strong>  6:  skin 0.06389969 0.08682105 0.10294118</strong><br/><strong>  7:   bp 0.05998254 0.07918634 0.09803922</strong><strong><br/>  &gt; xgb.plot.importance(impMatrix, main = "Gain by Feature") <br/></strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="239" width="402" class="image-border" src="assets/image_06_10.png"/></div>
<p>How does the feature importance compare to other methods? </p>
<p>Here is how we see it performed on the test set, which like the training data must be in a matrix. Let's also bring in the tools from the <kbd>InformationValue</kbd> package to help our efforts. This code loads the library and produces some output to analyze model performance:</p>
<pre>
<strong>  &gt; library(InformationValue)<br/>  &gt; pred &lt;- predict(xgb.fit, x)<br/>  &gt; optimalCutoff(y, pred)<br/>  [1] 0.3899574</strong><br/><strong>  &gt; pima.testMat &lt;- as.matrix(pima.test[, 1:7])</strong><br/><strong>  &gt; xgb.pima.test &lt;- predict(xgb.fit, pima.testMat)</strong><br/><strong>  &gt; y.test &lt;- ifelse(pima.test$type == "Yes", 1, 0)</strong><strong><br/>  &gt; confusionMatrix(y.test, xgb.pima.test, threshold = 0.39)</strong><br/><strong>    0 1</strong><br/><strong>  0 72 16</strong><br/><strong>  1 20 39</strong><br/><strong>  &gt; 1 - misClassError(y.test, xgb.pima.test, threshold = 0.39)</strong><br/><strong>  [1] 0.7551</strong>
</pre>
<p>Did you notice what I did there with <kbd>optimalCutoff()</kbd>? Well, that function from <kbd>InformationValue</kbd> provides the optimal probability threshold to minimize error. By the way, the model error is around 25%. It's still not superior to our SVM model. As an aside, we see the ROC curve and the achievement of an AUC above 0.8. The following code produces the ROC curve:</p>
<pre>
<strong>  &gt; plotROC(y.test, xgb.pima.test)</strong>
</pre>
<p>The output of the code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="203" width="342" class="image-border" src="assets/image_06_11.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Model selection</h1>
            </header>

            <article>
                
<p>Recall that our primary objective in this chapter was to use the tree-based methods to improve the predictive ability of the work done in the prior chapters. What did we learn? First, on the <kbd>prostate</kbd> data with a quantitative response, we were not able to improve on the linear models that we produced in <a href="04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Advanced Feature Selection in Linear Models</em>. Second, the random forest outperformed logistic regression on the Wisconsin Breast Cancer data of <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Logistic Regression and Discriminant Analysis</em>. Finally, and I must say disappointingly, we were not able to improve on the SVM model on the Pima Indian diabetes data with boosted trees.</p>
<p>As a result, we can feel comfortable that we have good models for the prostate and breast cancer problems. We will try one more time to improve the model for diabetes in <a href="73c55dff-b99c-4ce7-ab3d-9a47ff2f6f2f.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Neural Networks and Deep Learning</em>. Before we bring this chapter to a close, I want to introduce the powerful method of feature elimination using random forest techniques.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Feature Selection with random forests</h1>
            </header>

            <article>
                
<p>So far, we've looked at several feature selection techniques, such as regularization, best subsets, and recursive feature elimination. I now want to introduce an effective feature selection method for classification problems with Random Forests using the <kbd>Boruta</kbd> package. A paper is available that provides details on how it works in providing all relevant features:</p>
<p>Kursa M., Rudnicki W. (2010), <em>Feature Selection with the Boruta Package</em>, <em>Journal of Statistical Software,</em> 36(11), 1 - 13</p>
<p>What I will do here is provide an overview of the algorithm and then apply it to a wide dataset. This will not serve as a separate business case but as a template to apply the methodology. I have found it to be highly effective, but be advised it can be computationally intensive. That may seem to defeat the purpose, but it effectively eliminates unimportant features, allowing you to focus on building a simpler, more efficient, and more insightful model. It is time well spent.</p>
<p>At a high level, the algorithm creates <strong>shadow</strong> <strong>attributes</strong> by copying all the inputs and shuffling the order of their observations to decorrelate them. Then, a random forest model is built on all the inputs and a Z-score of the mean accuracy loss for each feature, including the shadow ones. Features with significantly higher Z-scores or significantly lower Z-scores than the shadow attributes are deemed <strong>important</strong> and <strong>unimportant</strong> respectively. The shadow attributes and those features with known importance are removed and the process repeats itself until all features are assigned an importance value. You can also specify the maximum number of random forest iterations. After completion of the algorithm, each of the original features will be labeled as <strong>confirmed</strong>, <strong>tentative</strong>, or r<strong>ejected</strong>. You must decide on whether or not to include the tentative features for further modeling. Depending on your situation, you have some options:</p>
<ul>
<li>Change the random seed and rerun the methodology multiple (k) times and select only those features that are confirmed in all the k runs</li>
<li>Divide your data (training data) into k folds, run separate iterations on each fold, and select those features which are confirmed for all the k folds</li>
</ul>
<p>Note that all of this can be done with just a few lines of code. Let's have a look at the code, applying it to the <kbd>Sonar</kbd> data from the <kbd>mlbench</kbd> package. It consists of 208 observations, 60 numerical input features, and one vector of labels for classification. The labels are factors where, <kbd>R</kbd> if the <kbd>sonar</kbd> object is a rock and <kbd>M</kbd> if it is a mine. The first thing to do is load the data and do a quick, very quick, data exploration:</p>
<pre>
<strong>  &gt; data(Sonar, package="mlbench")</strong><br/><strong>  &gt; dim(Sonar)</strong><br/><strong>  [1] 208 61</strong><br/><strong>  &gt; table(Sonar$Class)</strong><br/><strong>   M R </strong><br/><strong>  111 97</strong>
</pre>
<p>To run the algorithm, you just need to load the <kbd>Boruta</kbd> package and create a formula in the <kbd>boruta()</kbd> function. Keep in mind that the labels must be and as a factor, or the algorithm will not work. If you want to track the progress of the algorithm, specify <kbd>doTrace = 1</kbd>. Also, don't forget to set the random seed:</p>
<pre>
<strong>  &gt; library(Boruta)</strong><br/><strong>  &gt; set.seed(1)</strong><br/><strong>  &gt; feature.selection &lt;- Boruta(Class ~ ., data = Sonar, doTrace = 1)</strong>
</pre>
<p>As mentioned in the previous section, this can be computationally intensive. Here is how long it took on my old-fashioned laptop:</p>
<pre>
<strong>  &gt; feature.selection$timeTaken</strong><br/><strong>  Time difference of 25.78468 secs</strong>
</pre>
<p>A simple table will provide the count of the final importance decision. We see that we could safely eliminate half of the features:</p>
<pre>
<strong>  &gt; table(feature.selection$finalDecision)</strong><br/><strong>  Tentative Confirmed Rejected </strong><br/><strong>      12    31    17</strong>
</pre>
<p>Using these results, it is simple to create a new dataframe with our selected features. We start out using the <kbd>getSelectedAttributes()</kbd> function to capture the feature names. In this example, let's only select those that are confirmed. If we wanted to include confirmed and tentative, we just specify <kbd>withTentative = TRUE</kbd> in the function:</p>
<pre>
<strong>  &gt; fNames &lt;- getSelectedAttributes(feature.selection) # withTentative = TRUE</strong><br/><strong>  &gt; fNames</strong><br/><strong>  [1] "V1" "V4" "V5" "V9" "V10" "V11" "V12" "V13" "V15" "V16"</strong><br/><strong>  [11] "V17" "V18" "V19" "V20" "V21" "V22" "V23" "V27" "V28" "V31"</strong><br/><strong>  [21] "V35" "V36" "V37" "V44" "V45" "V46" "V47" "V48" "V49" "V51"</strong><br/><strong>  [31] "V52"</strong>
</pre>
<p>Using the feature names, we create our subset of the Sonar data:</p>
<pre>
<strong>  &gt; Sonar.features &lt;- Sonar[, fNames]</strong><br/><strong>  &gt; dim(Sonar.features)</strong><br/><strong>  [1] 208 31</strong>
</pre>
<p>There you have it! The <kbd>Sonar.features</kbd> dataframe includes all the confirmed features from the boruta algorithm. It can now be subjected to further meaningful data exploration and analysis. A few lines of code and some patience as the algorithm does its job can significantly improve your modeling efforts and insight generation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, you learned both the power and limitations of tree-based learning methods for both classification and regression problems. Single trees, while easy to build and interpret, may not have the necessary predictive power for many of the problems that we are trying to solve. To improve on the predictive ability, we have the tools of random forest and gradient-boosted trees at our disposal. With random forest, hundreds or even thousands of trees are built and the results aggregated for an overall prediction. Each tree of the random forest is built using a sample of the data called bootstrapping as well as a sample of the predictive variables. As for gradient boosting, an initial, and a relatively small, tree is produced. After this initial tree is built, subsequent trees are produced based on the residuals/misclassifications. The intended result of such a technique is to build a series of trees that can improve on the weakness of the prior tree in the process, resulting in decreased bias and variance. We also saw that in R, one can utilize random forests as a feature selection method.</p>
<p>While these methods are extremely powerful, they are not some sort of nostrum in the world of machine learning. Different datasets require judgment on the part of the analyst as to which techniques are applicable. The techniques to be applied to the analysis and the selection of the tuning parameters are equally important. This fine tuning can make all the difference between a good predictive model and a great predictive model.</p>
<p>In the next chapter, we turn our attention to using R to build neural networks and deep learning models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>