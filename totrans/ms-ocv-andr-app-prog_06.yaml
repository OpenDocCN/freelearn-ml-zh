- en: Chapter 6. Working with Image Alignment and Stitching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One limitation of cameras is the limited field of view, often shortened to FOV.
    Field-of-view is the parameter that defines how much information can be captured
    in one frame obtained by the camera. So, to capture an image that requires a larger
    field-of-view, we use image stitching. Image stitching is a method of joining
    multiple images to form a bigger image that represents the information that is
    consistent with the original images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Image stitching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image alignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video stabilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stereo vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image stitching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There has been a lot of work in image stitching over the years, but we will
    take a look at the algorithm OpenCV implements internally. Most of it was proposed
    by Michael Brown and David Lowe. Image stitching is done in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Find suitable features and match them reliably across the set of images to obtain
    the relative positioning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop the geometry to choose reliable features that are invariant to rotation,
    scale, and illumination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Match images using the RANSAC algorithm and a probabilistic model for verification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Align the matched images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the results to obtain a panoramic image. We use automatic straightening,
    gain compensation, and multi-band blending to achieve a seamlessly stitched panoramic
    image, as shown here:![Image stitching](img/B02052_06_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature detection and matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we find and match SIFT features between all the images. By doing this,
    we get the scale and orientation associated with each feature point. With these
    details, we can form a similarity invariant matrix, where we can make appropriate
    measurements for calculations. We accumulate local gradients in the orientation
    histograms to obtain such a frame. By implementing such an algorithm, edges can
    shift slightly without modifying the descriptor values, thereby providing small
    levels of affine and shift invariances. The algorithm also proposes to achieve
    the illumination invariance using gradients to eliminate bias and normalizes the
    descriptor vector to eliminate the gain.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm also makes the assumption that a camera only rotates about its
    optical center. Due to this assumption, we can define rotations along the three
    primary axes, *x*, *y*, and *z*, as ![Feature detection and matching](img/B02052_06_11.jpg),
    and ![Feature detection and matching](img/B02052_06_12.jpg), respectively. We
    define a vector *θ*, as ![Feature detection and matching](img/B02052_06_13.jpg).
    We also use the focal length, *f* as a parameter. Thus, we get the pairwise homographies
    as ![Feature detection and matching](img/B02052_06_14.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature detection and matching](img/B02052_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Feature detection and matching](img/B02052_06_16.jpg) and ![Feature
    detection and matching](img/B02052_06_17.jpg) are the homographic image positions.
    ![Feature detection and matching](img/B02052_06_18.jpg) is the image position
    in a 2-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature detection and matching](img/B02052_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The values of ![Feature detection and matching](img/B02052_06_20.jpg) and ![Feature
    detection and matching](img/B02052_06_21.jpg) are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature detection and matching](img/B02052_06_22.jpg)![Feature detection
    and matching](img/B02052_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, this representation of R is consistent with the exponential
    form of depicting rotations. We have included provisions to allow small changes
    in positions. Hence, we have the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature detection and matching](img/B02052_06_24.jpg)![Feature detection
    and matching](img/B02052_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Feature detection and matching](img/B02052_06_26.jpg) represents the affine
    transformation of an image obtained by calculating linear homography of ![Feature
    detection and matching](img/B02052_06_27.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: After detecting features in all the images, we need to match them to find their
    relative arrangements. For this, we match the overlapping features using the k-nearest
    neighbors (with *k = 4*) in the feature space to obtain overlapping features.
    This method is employed to take into consideration the fact that each feature
    may overlap in more than one image.
  prefs: []
  type: TYPE_NORMAL
- en: Image matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, we have obtained the features and have the matches between features.
    Now we need to obtain the matching images to form the panorama. To form a panorama,
    we need a small number of images to match any image, so as to find adjacent images.
    The algorithm suggests the use of six matching images to the current image. This
    section is performed in two parts. First, we estimate the homography with which
    the two frames are compatible and we find a set of inliers for the same. For this,
    we use the RANSAC algorithm. Then we use a probabilistic model to verify the match
    between the images.
  prefs: []
  type: TYPE_NORMAL
- en: Homography estimation using RANSAC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The RANSAC algorithm, short for Random Sample Consensus, is an algorithm that
    uses a small set of randomly chosen matches in images to estimate the image transformation
    parameters. For image stitching, we use four feature matches to compute the homography
    between them. For this, the algorithm proposes the use of the direct linear transformation
    method described by R. Hartley and A. Zisserman. This is performed for 500 iterations
    and ultimately, the solution with the maximum number of *inliers* is chosen. Inliers
    are those features whose linear projections are consistent with the homography,
    H, up to a specified tolerance value for pixels. By performing probability calculations,
    it was found that the probability of finding a match is very high. For example,
    if inliers between images match with a probability of 0.5, the probability of
    not finding the homography is ![Homography estimation using RANSAC](img/B02052_06_28.jpg).
    Hence, RANSAC is quite successful at estimating H. This method is called the maximum
    likelihood estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Verification of image matches using a probabilistic model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By the model obtained till now, we have a set of feature matches within the
    overlap region (inliers), and some features within the area of overlap that do
    not match (outliers). Using a probabilistic model, we will verify that the obtained
    set of inliers and outliers produces a valid image match. The algorithm makes
    the assumption that the probability of the ![Verification of image matches using
    a probabilistic model](img/B02052_06_29.jpg) feature matching is an independent
    Bernoulli trial. The two equations that are obtained from this are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verification of image matches using a probabilistic model](img/B02052_06_30.jpg)![Verification
    of image matches using a probabilistic model](img/B02052_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Verification of image matches using a probabilistic model](img/B02052_06_32.jpg)
    represents the total number of features present in the overlap area. ![Verification
    of image matches using a probabilistic model](img/B02052_06_33.jpg) represents
    the total number of inliers. *m* specifies whether the two images have been matched
    correctly or not. ![Verification of image matches using a probabilistic model](img/B02052_06_35.jpg)
    is the probability of the feature being an inlier, given a correct image match.
    ![Verification of image matches using a probabilistic model](img/B02052_06_36.jpg)
    is the probability that the feature is not an inlier, given a correct image match.
    ![Verification of image matches using a probabilistic model](img/B02052_06_37.jpg)
    represents the set of feature matches ![Verification of image matches using a
    probabilistic model](img/B02052_06_38.jpg). *B* represents the binomial distribution,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verification of image matches using a probabilistic model](img/B02052_06_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the purpose of this algorithm, the values of ![Verification of image matches
    using a probabilistic model](img/B02052_06_40.jpg) and ![Verification of image
    matches using a probabilistic model](img/B02052_06_41.jpg) are set to 0.6 and
    0.1, respectively. Using Bayes rule, we can calculate the probability of an image
    match being valid as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verification of image matches using a probabilistic model](img/B02052_06_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An image match is considered to be valid if the value of the preceding expression
    is greater than a pre-chosen minimum probability. The algorithm suggests the use
    of ![Verification of image matches using a probabilistic model](img/B02052_06_43.jpg)
    and ![Verification of image matches using a probabilistic model](img/B02052_06_44.jpg).
    The match is accepted if the following equation is satisfied, and rejected otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verification of image matches using a probabilistic model](img/B02052_06_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A condition that arises from the assumption made earlier is that for a valid
    image match the following equation must be satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Verification of image matches using a probabilistic model](img/B02052_06_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the original paper, the authors also proposed a method by which the parameters
    can be learnt from the images rather than assigning fixed values to them.
  prefs: []
  type: TYPE_NORMAL
- en: Bundle adjustment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown and Lowe's algorithm proposes the use of bundle adjustment to obtain all
    the camera parameters, jointly for a given set of matches between the images.
    For this, images are added to a bundle adjuster in decreasing order of the number
    of feature matches. Each time, the new image is initialized with the rotation
    and focal length of the image to which it matched. Then we use the Levenberg-Marquadt
    algorithm to update the camera parameters. The Levenberg-Marquadt algorithm is
    generally used to solve non-linear least squares problems in curve fitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm tries to minimize the sum of the squared projection errors.
    For this, each feature is projected on to every other image to which the original
    image matches, and then the sum of the squared distances is minimized with respect
    to the camera parameters. If the ![Bundle adjustment](img/B02052_06_47.jpg) feature
    in one image matches the ![Bundle adjustment](img/B02052_06_48.jpg) feature in
    another, we obtain the residual for the projection as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bundle adjustment](img/B02052_06_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Bundle adjustment](img/B02052_06_50.jpg) represents the ![Bundle adjustment](img/B02052_06_51.jpg)
    feature in the ![Bundle adjustment](img/B02052_06_52.jpg) image, ![Bundle adjustment](img/B02052_06_53.jpg)
    is the residual after the projection of the ![Bundle adjustment](img/B02052_06_54.jpg)
    feature from ![Bundle adjustment](img/B02052_06_55.jpg) image on the ![Bundle
    adjustment](img/B02052_06_56.jpg) image, and ![Bundle adjustment](img/B02052_06_57.jpg)
    is the projection of ![Bundle adjustment](img/B02052_06_58.jpg) from the ![Bundle
    adjustment](img/B02052_06_59.jpg) image on the ![Bundle adjustment](img/B02052_06_60.jpg)
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the error function is calculated by summing up all the robustified residual
    field errors, over all the features, spanning all the images. For this robustification,
    the Huber robust error function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bundle adjustment](img/B02052_06_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On solving this, we get a non-linear equation, which is solved using the Levenberg-Marquadt
    algorithm, to estimate the values of the camera parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic panoramic straightening
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, the algorithm has been able to successfully find matches between images
    and able to stitch them together. However, there still exists an unknown 3D rotation
    component, which causes the panorama to be formed in a wave-like output, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic panoramic straightening](img/B02052_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This arises mainly due to the fact that the camera would not have been perfectly
    level while clicking the multiple images.
  prefs: []
  type: TYPE_NORMAL
- en: This is solved by taking a heuristic into consideration regarding the way people
    click panoramic images. It is assumed that it is highly unlikely for a user to
    rotate the camera while clicking the image, so the camera vectors generally lie
    on the same plane. So, we try to find the null vector of the covariance matrix
    of the camera vectors and the vector normal to the plane of the center and horizon.
    This way, we can then apply the rotation on the images to effectively remove the
    wavy effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic panoramic straightening](img/B02052_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gain compensation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gain is the camera parameter that describes the sensitivity of the image to
    light. Different images could have been clicked at different levels of gain. To
    overcome this situation, we make use of gain compensation, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gain compensation](img/B02052_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Gain compensation refers to the normalization of the gain in images to facilitate
    a seamlessly stitched image. The method used is similar to the one used to compute
    the camera parameters. The error function used here is the sum of the errors in
    gain-normalized intensities for all the overlapping pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gain compensation](img/B02052_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multi-band blending
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even after gain compensation, the stitching doesn't appear to be seamless. We
    need to apply a good blending algorithm to join the images without it being noticeable
    that the image has been stitched from multiple images.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we apply a good blending strategy. We choose a blending algorithm
    in which we assign a weight function to each image. This weight function varies
    linearly with weight = 1 at the center and weight = 0 at the edges. This weight
    function is also extended to a spherical coordinate system. A simple weighted
    sum of the intensities along each ray can be calculated using these weight functions,
    but this would cause high frequency areas to be blurred out.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, we need to implement multi-band blending. The multi-band blending
    blends low frequency regions over a large area, where it blends high frequency
    regions over a relatively smaller area. We assign weights to each image, using
    ![Multi-band blending](img/B02052_06_62.jpg), such that the value of ![Multi-band
    blending](img/B02052_06_63.jpg) is 1 where there is maximum weight in the image
    and 0 where the maximum weight for the region is from some other image. We then
    successively blur out these weight graphs to ultimately get the blending weights
    for each band.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we linearly combine the overlapping images for each band with respect
    to the blend weights. The amount of blurring depends on the frequency of the band.
    This results in the high frequency bands being blended over short regions, while
    the low frequency bands get blended over large regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-band blending](img/B02052_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Image stitching using OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the image stitching pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image stitching using OpenCV](img/B02052_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will now see how to implement image stitching.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will set up our project in the same way as we did for all the previous
    chapters. We will use the package name `com.packtpub.masteringopencvandroid.chapter6`
    for this project. First, we will edit our manifest file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will add all the required permissions to this project. We require the permissions
    to access the camera, and also to read and write to the external storage. So,
    add the following code to your manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then we will declare our activities. We only require one activity for this project.
    We will call it the `StitchingActivity`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Android NDK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We require NDK for this project as the stitching module is unavailable in OpenCV''s
    Java SDK. So, we will write the C++ code and compile it using Android NDK in order
    for it to be used as a part of our project. To do this, first download NDK from
    [http://developer.android.com/tools/sdk/ndk](http://developer.android.com/tools/sdk/ndk)
    and extract it to a location on your computer. Then go to your `local.properties`
    file and add the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, go to your `build.gradle` file that is located in the main module of
    your project. In this file, inside the `defaultConfig` tag, add the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the name of the module, which will contain our functions, where our
    computations will be performed. Now, under the `android` tag, after `defaultConfig`
    ends, add the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This defines where our compiled libraries will be located. After this, we need
    to set up the NDK part of our project. In the `src` folder, add a folder called
    `jni`. In this folder, we need to create two files. The first one is `Android.mk`.
    This contains information about the files in the project. Copy the following lines
    to this file. Remember to replace `OpenCV4AndroidSDK` with the location on your
    computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create another file named `Application.mk`. This defines the architectures
    for which the code has to be compiled. Copy the following lines to this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we are all set to use NDK code in our project.
  prefs: []
  type: TYPE_NORMAL
- en: The layout and Java code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next we will draw our layout. For this project, we only need one layout with
    one `ImageView` tag to display the stitched image and two `Buttons`. One of the
    buttons is used to click more images and one is used to signify that there are
    no more images to be clicked. We will also put all the items in a `ScrollView`
    tag to be able to see the full image if its size exceeds the screen size. Our
    `activity_stitching.xml` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have to write our Java code. In the `StitchingActivity.java` file, in
    your OpenCV `BaseLoaderCallback` object, edit the `onManagerConnected` function
    by adding the following line in `case LoaderCallbackInterface.SUCCESS`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this is the same name that we gave our module in our `Android.mk`
    file. In our Java code, we will first declare and initialize all the variables
    that we will need. We have a button called `bClickImage`, which, on clicking,
    calls Android''s camera intent and requests the system''s camera app to click
    a picture and sends it to the app. We will convert this `Bitmap` image into an
    OpenCV `Mat` and store it in an `ArrayList`. We will stitch all the images together
    in the end, when the user clicks on the `bDone` button. The `onClickListener`
    for both the buttons is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `onActivityResult` function is called when the camera intent returns from
    the camera app. We need to check whether an image has been clicked and add it
    to the `ArrayList`, if required. We will use OpenCV''s `BitmapToMat` function
    to convert the image from an Android Bitmap to an OpenCV Mat. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In `onClickListener` for `bDone`, we called a `createPanorama` function. In
    this function, we will execute an `AsyncTask`, as this is a computationally intensive
    task. In `AsyncTask`, we will call upon our NDK to perform the actual computation.
    This is what our `doInBackground` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to declare the `StitchPanorama` function as a native function
    so that Android knows where to look for it when executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After this, in `onPostExecute`, we just need to set the returned `Bitmap` as
    the source for `ImageView`. This completes our Java code for this project, and
    all the major stitching is done using the C++ code.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In your `jni` folder, create the `stitcher.cpp` file. Notice that this is the
    same name as set in the `Android.mk` file. First, we need to include some libraries
    that we will require. We will also declare some namespaces that we will be using
    and some global variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to declare our function and write our code in it. To declare the
    function, write the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The ellipses are placeholders for where our code will go. Notice the variables
    and their orders compared to the variables declared in our Java code. First, we
    will initialize some variables and also convert the Mat object that we sent from
    Java to a C++ Mat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have used the address of the Mat object and type-casted it to a C++
    Mat pointer. Next we need to convert the Mat array sent from Java to a C++ vector.
    We will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We need to manually delete the local objects as the C++ code doesn't automatically
    call the garbage collector, and being on mobile, it is highly important to optimize
    the memory use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use OpenCV''s stitcher module to stitch our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We have used the default setup for stitching the images; however, the stitcher
    module allows the modification of the pipeline by giving more control to the developer.
    Check out the available options at [http://docs.opencv.org/modules/stitching/doc/introduction.html](http://docs.opencv.org/modules/stitching/doc/introduction.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we just need to build our C++ code file to generate the object files that
    our Java code will use to make function calls to C++ functions. For this, you
    will need to open the terminal/command prompt, and then use the `cd` command to
    change the active directory to `<project_dir>/app/src/main/jni`. Now we need to
    build our files. For this, you need to use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This will generate our object files and place them in the `obj` and `libs` folders.
  prefs: []
  type: TYPE_NORMAL
- en: This completes our project on image stitching using OpenCV on Android. You can
    see the stitched results in the following images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the first sample image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The C++ code](img/B02052_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the second sample image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The C++ code](img/B02052_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the result of applying image stitching over these two sample
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The C++ code](img/B02052_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are chances that your code might crash due to the high memory requirements
    of the stitcher module. This is a limitation of the mobile ecosystem and can be
    overcome by including a server in the middle to perform the computations instead.
    You can modify the source of the app to send the images to the server, which in
    turn performs the stitching and returns the stitched result, which can be displayed
    in the app.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how panoramic images are stitched. We took a look at
    image alignment by finding homography, using RANSAC, and image stitching as a
    whole. We also saw how it can be implemented in Android using OpenCV. These image
    alignment techniques can also be used for video stabilization. In the next chapter,
    we will take a look at how we can make use of machine learning algorithms to automate
    some of the complex tasks that generally require a human to be present.
  prefs: []
  type: TYPE_NORMAL
