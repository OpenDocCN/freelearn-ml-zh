<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Intuitive Deep Learning in C# .NET</h1>
                </header>
            
            <article>
                
<p>Our goal in this chapter is to expose you to some of the powerful functionality that is available with Kelp.Net.</p>
<p>In this chapter, you will learn:</p>
<ul>
<li>How to use Kelp.Net to perform your own testing</li>
<li>How to write tests</li>
<li>How to do benchmarks of functions</li>
<li>How to extend Kelp.Net</li>
</ul>
<p>Kelp.Net<sub>4</sub> is a deep learning library written in C# and .NET. With the ability to chain functions into a function stack, it provides an incredible amount of power in a very flexible and intuitive platform. It also takes heavy advantage of the OpenCL language platform to enable seamless operation on both CPU-and GPU-enabled devices. Deep learning is an incredibly powerful tool, and native support for Caffe and Chainer model loading makes this platform even more powerful. As you will see, you can create a 1 million hidden layer deep learning network in just a few lines of code.</p>
<p>Kelp.Net also makes it very easy to save and load models to and from disk storage. This is a very powerful feature, allowing you to perform your training, save the model, and then load and test as required. It also makes it much easier to productionize code and truly separate the training and the test phases.</p>
<p>Among other things, Kelp.Net is an incredibly powerful tool for you to be able to learn and understand better various types of functions, their interactions, and performance. For instance, you can run tests against the same network with different optimizers and see the results by changing a single line of code. Also, you can design your tests easily to see the difference in using various batch sizes, number of hidden layers, epochs, and more. There really is no deep learning workbench for .NET that offers the power and flexibility found in Kelp.Net.</p>
<p>Let's begin by talking a little bit about deep learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is deep learning?</h1>
                </header>
            
            <article>
                
<p>To discuss deep learning, we need to go back in time, not so long ago, to when big data arrived in front of all our faces. The term was, and still is, everywhere. It was a skill that everyone just had to have, a buzzword-compliant checklist item. But what exactly did that term really mean? Well, it just meant that rather than siloed SQL databases and files being FTP'ed to use, we had this explosion of digital data from social media, internet search engines, e-commerce sites, and much more. And of course, this data came in various forms and formats. More formally, we were suddenly dealing with unstructured data. Not only did we have this explosion of data due to applications such as Facebook, Twitter, Google, and more, but also the explosion continues. More and more people get and stay connected to each other, sharing vast amounts of information about themselves that they wouldn't dare provide to someone if asked via a telephone call, right? And we have little to no control over the format and quality of that data. This will become an important point as we proceed.</p>
<p>Now, this vast amount of data is great, but humans can barely absorb what they are exposed to daily, let alone this explosion of data. So, along the way, people realized that machine learning and artificial intelligence could be adapted to just such a task. From a simple machine learning algorithm, all the way up to multilayered networks, artificial intelligence and deep learning were born (at least the corporate world likes to believe it happened that way!).</p>
<p>Deep learning, which is a branch of machine learning and artificial intelligence, uses many levels of neural network layers (hierarchical, if you like) to perform its job. In many cases, these networks are built to mirror what we think we know about the human brain, with neurons connecting layers together like an intricately layered web. This allows data processing to occur in a nonlinear fashion. Each layer processes data from the previous layer (with an exception being the first layer, of course), passing its information on to the next layer. With any luck, each layer improves the model, and in the end, we achieve our goal and solve our problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenCL</h1>
                </header>
            
            <article>
                
<p>Kelp.Net makes heavy usage of the open computing language, or OpenCL. According to Wikipedia:</p>
<div class="packt_quote">"OpenCL views a computing system as consisting of a number of compute devices, which might be central processing units (CPUs), or accelerators such as graphics processing units (GPUs), attached to a host processor (a CPU). Functions executed on an OpenCL device are called kernels. A single compute device typically consists of several compute units, which in turn comprise multiple processing elements (PS). A single kernel execution can run on all or many of the PEs in parallel."</div>
<p>In OpenCL, tasks are scheduled on command queues. There is at least one command queue for each device. The OpenCL runtime breaks the scheduled data-parallel tasks into pieces and sends the tasks to the device processing element.</p>
<p>OpenCL defines a memory hierarchy:</p>
<ul>
<li><strong>Global</strong>: Shared by all processing elements, and has high latency</li>
<li><strong>Read-only</strong>: Smaller, of lower latency, and writable by the host CPU but not compute devices</li>
<li><strong>Local</strong>: Shared by a process element group</li>
<li><strong>Per-element</strong>: Private memory</li>
</ul>
<p>OpenCL also provides an API designed more towards math. This can be seen in the exposure of fixed-length vector types such as float4 (four vector of single-precision floats), available in lengths of 2, 3, 4, 8 and 16. As you gain more exposure to Kelp.Net and start to create your own functions, you will encounter OpenCL programming. For now, it's enough to know that it exists and is being used under the hood extensively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenCL hierarchy</h1>
                </header>
            
            <article>
                
<p>In Kelp.Net, the hierarchy for various OpenCL resources is as shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/637115a5-7fd0-4e75-9e81-73caec8cee8c.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c519fb66-bcbc-4634-b380-1b6c1fdf9bec.png" style=""/></div>
<p>Let's describe these in more detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute kernel</h1>
                </header>
            
            <article>
                
<p>A kernel object encapsulates a specific kernel function declared in a program and the argument values to be used when executing this kernel function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute program</h1>
                </header>
            
            <article>
                
<p>An OpenCL program consisting of a set of kernels. Programs may also contain auxiliary functions called by the kernel functions and constant data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute sampler</h1>
                </header>
            
            <article>
                
<p>An object that describes how to sample an image when the image is read in the kernel. The image read functions take a sampler as an argument. The sampler specifies the image addressing mode (meaning how out-of-range coordinates are handled), the filtering mode, and whether the input image coordinate is a normalized or unnormalized value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute device</h1>
                </header>
            
            <article>
                
<p>A compute device is a collection of compute units. A command queue is used to queue commands to a device. Examples of commands include executing kernels or reading/writing memory objects. OpenCL devices typically correspond to a GPU, a multi-core CPU, and other processors such as <strong>Digital Signal Processor</strong> (<strong>DSP</strong>) and the cell/B.E. processor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute resource</h1>
                </header>
            
            <article>
                
<p>An OpenCL resource that can be created and deleted by the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute object</h1>
                </header>
            
            <article>
                
<p>An object identified by its handle in the OpenCL environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute context</h1>
                </header>
            
            <article>
                
<p>A compute context is the actual environment within which the kernels execute and the domain in which synchronization and memory management are defined.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute command queue</h1>
                </header>
            
            <article>
                
<p>A command queue is an object that holds commands that will be executed on a specific device. The command queue is created on a specific device within a context. Commands to a queue are queued in order but may be executed either in order or out of order.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute buffer</h1>
                </header>
            
            <article>
                
<p>A memory object that stores a linear collection of bytes. Buffer objects are accessible using a pointer in a kernel executing on a device.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute event</h1>
                </header>
            
            <article>
                
<p>An event encapsulates the status of an operation such as a command. It can be used to synchronize operations in a context.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute image</h1>
                </header>
            
            <article>
                
<p>A memory object that stores a 2D or 3D structured array. Image data can only be accessed with read and write functions. Read functions use a sampler.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute platform</h1>
                </header>
            
            <article>
                
<p>The host plus a collection of devices managed by the OpenCL framework that allow an application to share resources and execute kernels on devices on the platform.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compute user event</h1>
                </header>
            
            <article>
                
<p>This represents a user-created event.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Kelp.Net Framework</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Functions</h1>
                </header>
            
            <article>
                
<p>Functions are the basic building blocks of a Kelp.Net neural network. Single functions are chained together within function stacks to create powerful and possibly complex network chains. There are four primary types of functions you need to know about, and their purposes, should be self-explanatory:</p>
<ul>
<li>Single-input functions</li>
<li>Dual-input functions</li>
<li>Multi-input functions</li>
<li>Multi-output functions</li>
</ul>
<p>Functions are also chained together when networks are loaded from disks.</p>
<p>Each function has a forward and backward method that you will be implementing when you create functions of your own:</p>
<pre>public abstract NdArray[] Forward(params NdArray[] xs);<br/>public virtual void Backward([CanBeNull] params NdArray[] ys){}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Function stacks</h1>
                </header>
            
            <article>
                
<p>Function stacks are layers of functions that are executed simultaneously in one forward, backward, or update pass. Function stacks are created <span>when you</span><span> </span><span>either create a test or load a model from disk. Here are some examples of function stacks.</span></p>
<p>They can be small and simple:</p>
<pre>FunctionStack nn = new FunctionStack(<br/>                 new Linear(2, 2, name: "l1 Linear"),<br/>                 new Sigmoid(name: "l1 Sigmoid"),<br/>                 new Linear(2, 2, name: "l2 Linear"));</pre>
<p>They can be a little bit bigger:</p>
<pre>FunctionStack nn = new FunctionStack(<br/>                 new Convolution2D(1, 2, 3, name: "conv1", gpuEnable: true),// Do not forget the GPU flag if necessary<br/>                 new ReLU(),<br/>                 new MaxPooling(2, 2),<br/>                 new Convolution2D(2, 2, 2, name: "conv2", gpuEnable: true),<br/>                 new ReLU(),<br/>                 new MaxPooling(2, 2),<br/>                 new Linear(8, 2, name: "fl3"),<br/>                 new ReLU(),<br/>                 new Linear(2, 2, name: "fl4")<br/>             );</pre>
<p>Or, they can be very large:</p>
<pre>FunctionStack nn = new FunctionStack(<br/>                 new Linear(neuronCount * neuronCount, N, name: "l1 Linear"), // L1<br/>                 new BatchNormalization(N, name: "l1 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l1 LeakyReLU"),<br/>                 new Linear(N, N, name: "l2 Linear"), // L2<br/>                 new BatchNormalization(N, name: "l2 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l2 LeakyReLU"),<br/>                 new Linear(N, N, name: "l3 Linear"), // L3<br/>                 new BatchNormalization(N, name: "l3 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l3 LeakyReLU"),<br/>                 new Linear(N, N, name: "l4 Linear"), // L4<br/>                 new BatchNormalization(N, name: "l4 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l4 LeakyReLU"),<br/>                 new Linear(N, N, name: "l5 Linear"), // L5<br/>                 new BatchNormalization(N, name: "l5 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l5 LeakyReLU"),<br/>                 new Linear(N, N, name: "l6 Linear"), // L6<br/>                 new BatchNormalization(N, name: "l6 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l6 LeakyReLU"),<br/>                 new Linear(N, N, name: "l7 Linear"), // L7<br/>                 new BatchNormalization(N, name: "l7 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l7 ReLU"),<br/>                 new Linear(N, N, name: "l8 Linear"), // L8<br/>                 new BatchNormalization(N, name: "l8 BatchNorm"),<br/>                 new LeakyReLU(slope: 0.000001, name: "l8 LeakyReLU"),<br/>                 new Linear(N, N, name: "l9 Linear"), // L9<br/>                 new BatchNormalization(N, name: "l9 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l9 PolynomialApproximantSteep"),<br/>                 new Linear(N, N, name: "l10 Linear"), // L10<br/>                 new BatchNormalization(N, name: "l10 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l10 PolynomialApproximantSteep"),<br/>                 new Linear(N, N, name: "l11 Linear"), // L11<br/>                 new BatchNormalization(N, name: "l11 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l11 PolynomialApproximantSteep"),<br/>                 new Linear(N, N, name: "l12 Linear"), // L12<br/>                 new BatchNormalization(N, name: "l12 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l12 PolynomialApproximantSteep"),<br/>                 new Linear(N, N, name: "l13 Linear"), // L13<br/>                 new BatchNormalization(N, name: "l13 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l13 PolynomialApproximantSteep"),<br/>                 new Linear(N, N, name: "l14 Linear"), // L14<br/>                 new BatchNormalization(N, name: "l14 BatchNorm"),<br/>                 new PolynomialApproximantSteep(slope: 0.000001, name: "l14 PolynomialApproximantSteep"),<br/>                 new Linear(N, 10, name: "l15 Linear") // L15<br/>             );</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Function dictionaries</h1>
                </header>
            
            <article>
                
<p>A function dictionary is a serializable dictionary of functions (described previously). When a network model is loaded from disk, a function dictionary will be returned and can be operated on exactly as if you had just created the function stack itself in code. The function dictionary is used primarily with the Caffe data model loader.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caffe1</h1>
                </header>
            
            <article>
                
<p>Kelp.Net was developed strongly around the Caffe style of development and supports many of its features.</p>
<p>Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approximately 2 ms per image). By separating model representation and actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment, from prototyping machines to cloud environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chainer</h1>
                </header>
            
            <article>
                
<p>According to the Chainer documentation<sub>2</sub>:</p>
<div class="packt_quote">"Chainer is a flexible framework for neural networks. One major goal is flexibility, so it must enable us to write complex architectures simply and intuitively."</div>
<p>Chainer adopts a define-by-run scheme, that is, the network is defined dynamically via the actual forward computation. More precisely, Chainer stores the history of computation instead of programming logic. For example, Chainer does not need any magic to introduce conditionals and loops into the network definitions. The define-by-run scheme is the core concept of Chainer. This strategy also makes it easy to write multi-GPU parallelization, since logic comes closer to network manipulation.</p>
<p>Kelp.Net can load a Chainer model directly from disk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss</h1>
                </header>
            
            <article>
                
<p>Kelp.Net is comprised of a single abstract <kbd>LossFunction</kbd> class, designed to be implemented for your specific instance in determining how you evaluate loss.</p>
<p><span class="y0nh2b">In machine learning, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively, representing some cost associated with the event. Kelp.Net offers two out-of-the-box loss functions:</span> mean squared error and softmax cross entropy. You can easily extend these to meet yo<span class="y0nh2b">ur needs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model saving and loading</h1>
                </header>
            
            <article>
                
<p>Kelp.Net makes it very easy to save and load models with a call to one simple class. The <kbd>ModelIO</kbd> class exposes both a <kbd>Save</kbd> and a <kbd>Load</kbd> method for easy saving and loading to disk. Here is a very simple example of saving a model after training, reloading, and then performing testing on that model:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4618e810-510c-49f1-8847-ec05afaf293e.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizers</h1>
                </header>
            
            <article>
                
<p>Optimization algorithms minimize or maximize an error function depending on the model's parameters. Examples of parameters would be weights and biases. They help compute the output value and update the model towards the position of optimal solution by minimizing loss. Extending Kelp.Net to add your own optimization algorithms is a simple process, although adding the OpenCL and resource side of things is a coordinated effort.</p>
<p>Kelp.Net comes with many predefined optimizers, such as:</p>
<ul>
<li>AdaDelta</li>
<li>AdaGrad</li>
<li>Adam</li>
<li>GradientClipping</li>
<li>MomentumSGD</li>
<li>RMSprop</li>
<li>SGD</li>
</ul>
<p>These are all based on the abstract optimizer class.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datasets</h1>
                </header>
            
            <article>
                
<p>Kelp.Net natively supports the following datasets:</p>
<ul>
<li>CIFAR</li>
<li>MNIST</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CIFAR</h1>
                </header>
            
            <article>
                
<p>The CIFAR datasets come in two flavors, CIFAR-10 and CIFAR 100, with the difference being the number of classes within each. Let's briefly discuss both.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CIFAR-10</h1>
                </header>
            
            <article>
                
<p>The CIFAR-10 dataset consists of 60,000 32 x 32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1,000 randomly selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5,000 images from each class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CIFAR-100</h1>
                </header>
            
            <article>
                
<p>The CIFAR-100 dataset is just like CIFAR-10, except that it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in CIFAR-100 are grouped into 20 superclasses. Each image comes with a fine label (the class to which it belongs) and a coarse label (the superclass to which it belongs). Here is the list of classes in CIFAR-100:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Superclass</strong></p>
</td>
<td>
<p><strong>Classes</strong></p>
</td>
</tr>
<tr>
<td>
<p>Aquatic mammals</p>
</td>
<td>
<p>Beaver, dolphin, otter, seal, and whale</p>
</td>
</tr>
<tr>
<td>
<p>Fish</p>
</td>
<td>
<p>Aquarium fish, flatfish, ray, shark, and trout</p>
</td>
</tr>
<tr>
<td>
<p>Flowers</p>
</td>
<td>
<p>Orchids, poppies, roses, sunflowers, and tulips</p>
</td>
</tr>
<tr>
<td>
<p>Food containers</p>
</td>
<td>
<p>Bottles, bowls, cans, cups, and plates</p>
</td>
</tr>
<tr>
<td>
<p>Fruit and vegetables</p>
</td>
<td>
<p>Apples, mushrooms, oranges, pears, and sweet peppers</p>
</td>
</tr>
<tr>
<td>
<p>Household electrical devices</p>
</td>
<td>
<p>Clock, computer keyboard, lamp, telephone, and television</p>
</td>
</tr>
<tr>
<td>
<p>Household furniture</p>
</td>
<td>
<p>Bed, chair, couch, table, and wardrobe</p>
</td>
</tr>
<tr>
<td>
<p>Insects</p>
</td>
<td>
<p>Bee, beetle, butterfly, caterpillar, and cockroach</p>
</td>
</tr>
<tr>
<td>
<p>Large carnivores</p>
</td>
<td>
<p>Bear, leopard, lion, tiger, and wolf</p>
</td>
</tr>
<tr>
<td>
<p>Large man-made outdoor things</p>
</td>
<td>
<p>Bridge, castle, house, road, and skyscraper</p>
</td>
</tr>
<tr>
<td>
<p>Large natural outdoor scenes</p>
</td>
<td>
<p>Cloud, forest, mountain, plain, and sea</p>
</td>
</tr>
<tr>
<td>
<p>Large omnivores and herbivores</p>
</td>
<td>
<p>Camel, cattle, chimpanzee, elephant, and kangaroo</p>
</td>
</tr>
<tr>
<td>
<p>Medium-sized mammals</p>
</td>
<td>
<p>Fox, porcupine, possum, raccoon, and skunk</p>
</td>
</tr>
<tr>
<td>
<p>Non-insect invertebrates</p>
</td>
<td>
<p>Crab, lobster, snail, spider, and worm</p>
</td>
</tr>
<tr>
<td>
<p>People</p>
</td>
<td>
<p>Baby, boy, girl, man, and woman</p>
</td>
</tr>
<tr>
<td>
<p>Reptiles</p>
</td>
<td>
<p>Crocodile, dinosaur, lizard, snake, and turtle</p>
</td>
</tr>
<tr>
<td>
<p>Small mammals</p>
</td>
<td>
<p>Hamster, mouse, rabbit, shrew, and squirrel</p>
</td>
</tr>
<tr>
<td>
<p>Trees</p>
</td>
<td>
<p>Maple, oak, palm, pine, and willow</p>
</td>
</tr>
<tr>
<td>
<p>Vehicles 1</p>
</td>
<td>
<p>Bicycle, bus, motorcycle, pickup truck, and train</p>
</td>
</tr>
<tr>
<td>
<p>Vehicles 2</p>
</td>
<td>
<p>Lawn-mower, rocket, streetcar, tank, and tractor</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MNIST</h1>
                </header>
            
            <article>
                
<p>The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It has a training set of 60,000 examples and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image, making it the standard of choice for people wanting to try various learning techniques without requiring the effort of preprocessing and formatting:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/19ca2c60-4ff0-483f-8711-a7b0c4500ff3.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign">MNIST Example</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tests</h1>
                </header>
            
            <article>
                
<p>Tests are actual execution events, small programs if you will. Because of the usage of OpenCL, these programs are compiled at runtime. To create a test, you only need to provide a single static <kbd>Run</kbd> function that encapsulates your code. Kelp.Net comes with a preconfigured tester, which makes it very simple to add your own tests. We will explore this in detail in our section on writing tests, for now, here is an example of a simple XOR test program:</p>
<pre>public static void Run()<br/>         {<br/>             const int learningCount = 10000;<br/> <br/>             Real[][] trainData =<br/>             {<br/>                 new Real[] { 0, 0 },<br/>                 new Real[] { 1, 0 },<br/>                 new Real[] { 0, 1 },<br/>                 new Real[] { 1, 1 }<br/>             };<br/> <br/>             Real[][] trainLabel =<br/>             {<br/>                 new Real[] { 0 },<br/>                 new Real[] { 1 },<br/>                 new Real[] { 1 },<br/>                 new Real[] { 0 }<br/>             };<br/> <br/>             FunctionStack nn = new FunctionStack(<br/>                 new Linear(2, 2, name: "l1 Linear"),<br/>                 new ReLU(name: "l1 ReLU"),<br/>                 new Linear(2, 1, name: "l2 Linear"));<br/> <br/>             nn.SetOptimizer(new AdaGrad());<br/> <br/>             RILogManager.Default?.SendDebug("Training...");<br/>             for (int i = 0; i &lt; learningCount; i++)<br/>             {</pre>
<pre>                 //use MeanSquaredError for loss function<br/>                 Trainer.Train(nn, trainData[0], trainLabel[0], new MeanSquaredError(), false);<br/>                 Trainer.Train(nn, trainData[1], trainLabel[1], new MeanSquaredError(), false);<br/>                 Trainer.Train(nn, trainData[2], trainLabel[2], new MeanSquaredError(), false);<br/>                 Trainer.Train(nn, trainData[3], trainLabel[3], new MeanSquaredError(), false);<br/> <br/>                 //If you do not update every time after training, you can update it as a mini batch<br/>                 nn.Update();<br/>             }<br/> <br/>             RILogManager.Default?.SendDebug("Test Start...");<br/>             foreach (Real[] val in trainData)<br/>             {<br/>                 NdArray result = nn.Predict(val)[0];<br/>                 RILogManager.Default?.SendDebug($"{val[0]} xor {val[1]} = {(result.Data[0] &gt; 0.5 ? 1 : 0)} {result}");<br/>             }<br/>         }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring Kelp.Net</h1>
                </header>
            
            <article>
                
<p>ReflectInsight from ReflectSoftware is hands down the best real-time logging framework for both logging and rich visualization available today. Kelp.Net has native support for this framework, thereby making it very easy for you to see what is going on inside your tests.</p>
<p>Here is what the main screen of ReflectInsight looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cdbc8870-d14b-43c1-820e-324905bcf3bb.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign">An example of the main screen of Reflect Insight</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Watches</h1>
                </header>
            
            <article>
                
<p>Watches allow you to keep an eye on specific data elements throughout the execution of your tests. When it comes to machine learning, understanding and seeing exactly what is going on inside your algorithms is incredibly important, and the watch panel is exactly the place to accomplish that<span><span>:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1c6900a8-53fa-44a2-b40c-de21a72a6e8c.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Messages</h1>
                </header>
            
            <article>
                
<p>The message panel is where each message is displayed during test execution. The information available is totally up to you. The image displayed to the immediate left of the message text is based upon the type of message that you send (Information, Debug, Warning, Error, etc.):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/429ba129-b82f-404e-9542-a409769840fb.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Properties</h1>
                </header>
            
            <article>
                
<p>Each message has predefined properties that can be viewed via the <span class="packt_screen">Properties</span> panel. There are standard properties, such as that shown below, which are available for every message. Then there are customize message properties that can be applied:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/58da7859-9e0b-422a-860d-7dc692adda5b.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign">An example of per message properties</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weaver</h1>
                </header>
            
            <article>
                
<p>The weaver is a critical component of Kelp.Net and is the first object call you will make when running a test. This object houses a variety of OpenCL objects such as:</p>
<ul>
<li>ComputeContext</li>
<li>An array of ComputeDevices</li>
<li>ComputeCommandQueue</li>
<li>A Boolean flag indicating if the GPU is enabled</li>
<li>ComputePlatform</li>
<li>A dictionary of KernelSources</li>
</ul>
<p>The weaver is the place to tell your program whether you will be using a CPU or GPU, and which device (if your system is capable of multiple devices) you will be using. You only need to make a single call to the weaver, at the beginning of your program, similar to what you see here:</p>
<pre>Weaver.Initialize(ComputeDeviceTypes.Gpu);</pre>
<p>You also can avoid using the initialization call of the weaver and allow it to determine what needs to happen automatically.</p>
<p>Here are the basic contents of the weaver. Its purpose is to build (compile dynamically at runtime) the program that will be executed:</p>
<pre> /// &lt;summary&gt;   The context. &lt;/summary&gt;<br/>         internal static ComputeContext Context;<br/>         /// &lt;summary&gt;   The devices. &lt;/summary&gt;<br/>         private static ComputeDevice[] Devices;<br/>         /// &lt;summary&gt;   Queue of commands. &lt;/summary&gt;<br/>         internal static ComputeCommandQueue CommandQueue;<br/>         /// &lt;summary&gt;   Zero-based index of the device. &lt;/summary&gt;<br/>         private static int DeviceIndex;<br/>         /// &lt;summary&gt;   True to enable, false to disable. &lt;/summary&gt;<br/>         internal static bool Enable;<br/>         /// &lt;summary&gt;   The platform. &lt;/summary&gt;<br/>         private static ComputePlatform Platform;<br/>         /// &lt;summary&gt;   The kernel sources. &lt;/summary&gt;<br/>         private static readonly Dictionary&lt;string, string&gt; KernelSources = new Dictionary&lt;string, string&gt;();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing tests</h1>
                </header>
            
            <article>
                
<p>Creating tests for Kelp.Net is incredibly simple. Each test that you author needs only a single <kbd>Run</kbd> function exposed. The rest is your logic as to how you want your network to operate. The general guidelines for your <kbd>Run</kbd> function would be:</p>
<ul>
<li>Load data (real or simulated):</li>
</ul>
<pre style="padding-left: 60px">Real[][] trainData = new Real[N][];<br/>             Real[][] trainLabel = new Real[N][];<br/> <br/>             for (int i = 0; i &lt; N; i++)<br/>             {<br/>                 //Prepare Sin wave for one cycle<br/>                 Real radian = -Math.PI + Math.PI * 2.0 * i / (N - 1);<br/>                 trainData[i] = new[] { radian };<br/>                 trainLabel[i] = new Real[] { Math.Sin(radian) };<br/>             }</pre>
<ul>
<li>Create your function stack:</li>
</ul>
<pre style="padding-left: 60px">FunctionStack nn = new FunctionStack(<br/>                 new Linear(1, 4, name: "l1 Linear"),<br/>                 new Tanh(name: "l1 Tanh"),<br/>                 new Linear(4, 1, name: "l2 Linear")<br/>             );</pre>
<ul>
<li>Select your optimizer:</li>
</ul>
<pre style="padding-left: 60px"> nn.SetOptimizer(new SGD());</pre>
<ul>
<li>Train your data:</li>
</ul>
<pre style="padding-left: 60px">for (int i = 0; i &lt; EPOCH; i++)<br/>            {<br/>                Real loss = 0;<br/>                for (int j = 0; j &lt; N; j++)<br/>                {<br/>                    //When training is executed in the network, an error is returned to the return value<br/>                    loss += Trainer.Train(nn, trainData[j], trainLabel[j], new MeanSquaredError());<br/>                }<br/>                if (i % (EPOCH / 10) == 0)<br/>                {<br/>                    RILogManager.Default?.SendDebug("loss:" + loss / N);<br/>                    RILogManager.Default?.SendDebug("");<br/>                }<br/>            }</pre>
<ul>
<li>Test your data:</li>
</ul>
<pre style="padding-left: 60px">    <strong>        </strong>RILogManager.Default?.SendDebug("Test Start...");<br/>            foreach (Real[] val in trainData)<br/>            {<br/>                RILogManager.Default?.SendDebug(val[0] + ":" + nn.Predict(val)[0].Data[0]);<br/>            }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benchmarking functions</h1>
                </header>
            
            <article>
                
<p>The <kbd>SingleBenchmark</kbd> class within the <kbd>KelpNetTester</kbd> class allows for simple benchmarking of various activation, noise, and other functions. If a function has a GPU capability, that is benchmarked, and so are CPU capabilities. The timing is at the microsecond level, as ReLU forward will usually always be below 1 ms in granularity.</p>
<div class="CDPAlignCenter CDPAlign"><strong>With CPU enabled</strong></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e2b3619a-941f-49cf-8c3f-116833511bf8.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><strong>With GPU enabled</strong></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/03550d87-07a5-4a7e-86b1-e3a64b026410.png" style=""/></div>
<p>Now let's talk about how we run a single benchmark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running a Single Benchmark</h1>
                </header>
            
            <article>
                
<p>When you run the <kbd>SingleBenchmark</kbd> class, the functions you see in the upcoming images will be timed. Forward and backward CPU and GPU timing will be provided (GPU when applicable). Here is a collapsed view of the benchmark tests:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e9befcb-0529-4b8e-8f39-6a46752776e6.png" style=""/></div>
<p>And here is an expanded view of the benchmarks:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78e325aa-8eda-4da2-b0aa-5471ce3d223e.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we welcomed you to the world of intuitive deep learning. We showed you how you could use Kelp.Net to be your research platform to test virtually any hypothesis. We also showed you the power and flexibility of Kelp.Net. In our next chapter, we will enter the world of quantum computing and show you a little bit of the future of computing. Hold on to your hats, this one is different!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol>
<li><em>Convolutional Architecture for fast feature embedding</em>, Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, proceedings of the 22<sup>nd</sup> ACM international conference, 2014</li>
<li>Chainer at <a href="https://docs.chainer.org/en/stable/index.html">https://docs.chainer.org/en/stable/index.html</a></li>
<li><em>Learning</em> <em>Multiple Layers of Features from Tiny Images</em> at <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>, Alex Krizhevsky, 2009</li>
<li>Original Kelp.Net at <a href="https://github.com/harujoh">https://github.com/harujoh</a></li>
<li>(Filice '15) Simone Filice, Giuseppe Castellucci, Danilo Croce, Roberto Basili, <em>Kelp: a kernel-based Learning Platform for Natural Language Processing</em>, proceedings of ACL: system demonstrations, Beijing, China (July 2015)</li>
</ol>


            </article>

            
        </section>
    </body></html>