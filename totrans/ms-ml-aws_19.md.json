["```py\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml import Pipeline\n\nlinear = LinearRegression(featuresCol=\"features\", labelCol=\"medv\")\nparam_grid = ParamGridBuilder() \\\n  .addGrid(linear.elasticNetParam, [0.01, 0.02, 0.05]) \\\n  .addGrid(linear.solver, ['normal', 'l-bfgs']) \\\n  .addGrid(linear.regParam, [0.4, 0.5, 0.6]).build()\n\npipeline = Pipeline(stages=[vector_assembler, linear])\ncrossval = CrossValidator(estimator=pipeline,\n                        estimatorParamMaps=param_grid,\n                        evaluator=evaluator,\n                        numFolds=10)\noptimized_model = crossval.fit(housing_df)\n```", "```py\nevaluator = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"r2\")\n```", "```py\n[(k.name, v) for (k, v) in optimized_model.bestModel.stages[1].extractParamMap().items()]\n```", "```py\n[('epsilon', 1.35),\n('featuresCol', 'features'),\n('predictionCol', 'prediction'),\n('loss', 'squaredError'),\n('elasticNetParam', 0.02),\n('regParam', 0.6),\n('maxIter', 100),\n('labelCol', 'medv'),\n('tol', 1e-06),\n('standardization', True),\n('aggregationDepth', 2),\n('fitIntercept', True),\n('solver', 'l-bfgs')]\n```", "```py\n_, test_df = housing_df.randomSplit([0.8, 0.2], seed=17)\nevaluator.evaluate(optimized_model.transform(test_df))\n```", "```py\nimport boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsess = sagemaker.Session()\nrole = get_execution_role()\ncontainer = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', \"xgboost\", \"latest\")\n\ns3_validation_data = 's3://mastering-ml-aws/chapter4/test-vector-csv/'\ns3_train_data = 's3://mastering-ml-aws/chapter4/training-vector-csv/'\ns3_output_location = 's3://mastering-ml-aws/chapter14/output/'\n\n```", "```py\nsagemaker_model = sagemaker.estimator.Estimator(container,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.c4.4xlarge',\n                                                train_volume_size=30,\n                                                train_max_run=360000,\n                                                input_mode='File',\n                                                output_path=s3_output_location,\n                                                sagemaker_session=sess)\n\nsagemaker_model.set_hyperparameters(objective='binary:logistic',\n                                    max_depth=5,\n                                    eta=0.2,\n                                    gamma=4,\n                                    min_child_weight=6,\n                                    subsample=0.7,\n                                    silent=0,\n                                    num_round=50)\n\n```", "```py\ntrain_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated',\n                                        content_type='text/csv', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated',\n                                             content_type='text/csv', s3_data_type='S3Prefix')\n\ndata_channels = {'train': train_data, 'validation': validation_data}\n```", "```py\nfrom sagemaker.tuner import HyperparameterTuner, ContinuousParameter,IntegerParameter\n\ntree_tuner = HyperparameterTuner\n(estimator=sagemaker_model, \n                              objective_metric_name='validation:auc',\nmax_jobs=10,\nmax_parallel_jobs=3,\nhyperparameter_ranges={'lambda': \nContinuousParameter(0, 1000),\n                                                               'max_depth': IntegerParameter(3,7),\n                                                       'eta':ContinuousParameter(0.1, 0.5)})\n\ntree_tuner.fit(inputs=data_channels, logs=True)\n```", "```py\ntree_tuner.best_training_job()\n```", "```py\nsess.sagemaker_client.describe_training_job(TrainingJobName=tree_tuner.best_training_job())\n```", "```py\n{'TrainingJobName': 'xgboost-190407-1532-005-0e830ada',\n 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:095585830284:training-job/xgboost-190407-1532-005-0e830ada',\n 'TuningJobArn': 'arn:aws:sagemaker:us-east-1:095585830284:hyper-parameter-tuning-job/xgboost-190407-1532',\n 'ModelArtifacts': {'S3ModelArtifacts': 's3://mastering-ml-aws/chapter14/output/xgboost-190407-1532-005-0e830ada/output/model.tar.gz'},\n 'TrainingJobStatus': 'Completed',\n 'SecondaryStatus': 'Completed',\n 'HyperParameters': {'_tuning_objective_metric': 'validation:auc',\n 'eta': '0.4630125855085939',\n 'gamma': '4',\n 'lambda': '29.566673825272677',\n 'max_depth': '7',\n 'min_child_weight': '6',\n 'num_round': '50',\n 'objective': 'binary:logistic',\n 'silent': '0',\n 'subsample': '0.7'},....}\n```", "```py\nsess.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName='xgboost-190407-1532')\n```", "```py\n{'HyperParameterTuningJobName': 'xgboost-190407-1532',\n 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:095585830284:hyper-parameter-tuning-job/xgboost-190407-1532',\n 'HyperParameterTuningJobConfig': {'Strategy': 'Bayesian',\n 'HyperParameterTuningJobObjective': {'Type': 'Maximize',\n 'MetricName': 'validation:auc'},\n 'ResourceLimits': {'MaxNumberOfTrainingJobs': 10,\n 'MaxParallelTrainingJobs': 3},\n ....\n 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:auc',\n 'Value': 0.6545940041542053},\n '\n ...}\n```"]