<html><head></head><body>
<div id="_idContainer039" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">3</p>
</div>
<div id="_idContainer040" class="Content">
<h1 id="_idParaDest-62"><a id="_idTextAnchor066"></a>
 Regression</h1>
</div>
<div id="_idContainer041" class="Content">
<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="bullets">Describe the mathematical logic involved in regression</li>
<li class="bullets">Illustrate the use of the NumPy library for Regression</li>
<li class="bullets">Identify linear regression with one variable and with multiple variables</li>
<li class="bullets">Use polynomial regression</li>
</ul>
<p>This chapter covers the fundamentals of linear and polynomial regression.</p>
</div>
<div id="_idContainer064" class="Content">
<h2 id="_idParaDest-63"><a id="_idTextAnchor067"></a>
 Introduction</h2>
<p>Regression is a broad topic that connects mathematical statistics, data science, machine learning, and artificial intelligence. As the basics of regression are rooted in mathematics, we will start by exploring the mathematical fundamentals.</p>
<p>Most of this topic will deal with different forms of linear regression, including linear regression with one variable, linear regression with multiple variables, polynomial regression with one variable, and polynomial regression with multiple variables. Python provides a lot of support for performing regression operations.</p>
<p>We will also use alternative regression models while comparing and contrasting support vector Regression with forms of Linear Regression. Throughout this chapter, we will use stock price data loaded from an online service provider. The models in this chapter are not intended to provide trading or investment advice.</p>
<h4>Note</h4>
<p class="callout">Although it is not suggested to use the models in this chapter to provide trading or investment advice, it is a very exciting and interesting journey that explains the fundamentals of regression.</p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor068"></a>
 Linear Regression with One Variable</h2>
<p>A general regression problem can be defined as follows. Suppose we have a set of data points. We need to figure out a best fit curve to approximately fit the given data points. This curve will describe the relationship between our input variable x, which is the data points, and output variable y, which is the curve.</p>
<p class="_idGenParaOverride-1">In real life, we often have multiple input variables determining one output variable. Regression helps us understand how the output variable changes when we keep all but one input variable fixed, and we change the remaining input variable.</p>
<div></div>
<h3 id="_idParaDest-65"><a id="_idTextAnchor069"></a>
 What Is Regression?</h3>
<p>In this chapter, we will work with regression on the two-dimensional plane. This means that our data points are two-dimensional, and we are looking for a curve to approximate how to calculate one variable from another.</p>
<p>We will learn about the following types of regression:</p>
<ul>
<li>
<strong class="keyword _idGenCharOverride-1">Linear regression with one variable using a polynomial of degree 1</strong>
 : This is the most basic form of regression, where a straight line approximates the trajectory of future datasets.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Linear regression with multiple variables using a polynomial of degree 1</strong>
 : We will be using equations of degree 1, but we will now allow multiple input variables, also known as features.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Polynomial regression with one variable</strong>
 : This is a generic form of linear regression of one variable. As the polynomial used to approximate the relationship between the input and the output is of an arbitrary degree, we can create curves that fit the data points better than a straight line. The regression is still linear – not because the polynomial is linear, but because the regression problem can be modeled using linear algebra.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Polynomial regression with multiple variables</strong>
 : This is the most generic regression problem using higher degree polynomials and multiple features to predict the future.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Support vector regression</strong>
 : This form of regression uses support vector machines to predict data points. This type of regression is included to compare its usage to the other four regression types.</li>
</ul>
<p>In this topic, we will deal with the first type of linear regression: we will use one variable, and the polynomial of the regression will describe a straight line.</p>
<p class="_idGenParaOverride-1">On the two-dimensional plane, we will use the Déscartes coordinate system, more commonly known as the Cartesian coordinate system. We have an <em class="italics _idGenCharOverride-2">X</em>
 and a <em class="italics _idGenCharOverride-2">Y</em>
 axis, and the intersection of these two axes is the origin. We denote points by their X and Y coordinates.</p>
<div></div>
<p>For instance, the point (2, 1) corresponds to the orange point on the following coordinate system:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer042" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00020.jpg" alt="" />
</div>
</div>
<h6>Figure 3.1: Representation of point (2,1) on the coordinate system</h6>
<p class="_idGenParaOverride-1">A straight line can be described with the equation <strong class="inline _idGenCharOverride-3">y = a*x + b</strong>
 , where a is the slope of the equation, determining how steeply the equation climbs up, and b is a constant determining where the line intersects the Y axis</p>
<div></div>
<p>In the following diagram, you can see three equations:</p>
<ul>
<li>The blue line is described with the y = 2*x + 1 equation.</li>
<li>The orange line is described with the y = x + 1 equation.</li>
<li>The purple line is described with the y = 0.5*x + 1 equation.</li>
</ul>
<p>You can see that all three equations intersect the y-axis at 1, and their slope is determined by the factor by which we multiply x.</p>
<p>If you know x, you can figure out y. If you know y, you can figure out x:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer043" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00021.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 3.2: Representation of the equations y = 2*x + 1, y = x + 1, and y = 0.5*x + 1 on the coordinate system</h6>
<div></div>
<p>We can describe more complex curves with equations too:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer044" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00022.jpg" alt="" />
</div>
</div>
<h6>Figure 3.3: Image showing a complex curve</h6>
<h4>Note</h4>
<p class="callout">If you would like to experiment more with the Cartesian coordinate system, you can use the following plotter: <a href="https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html">https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html</a>
 .</p>
<h3 id="_idParaDest-66"><a id="_idTextAnchor070"></a>
 Features and Labels</h3>
<p>In machine learning, we differentiate between features and labels. Features are considered our input variables, and labels are our output variables.</p>
<p>When talking about regression, the possible values of labels is a continuous set of rational numbers.</p>
<p>Think about features as values on the X-axis, and labels as the value on the Y-axis.</p>
<p>The task of regression is to predict label values based on feature values. We often create a label by shifting values of a feature forward. For instance, if we would like to predict stock prices in 1 month, and we create the label by shifting the stock price feature 1 month to the future, then:</p>
<ul>
<li>For each stock price feature value that's at least 1 month old, training data is available that shows the predicted stock price data 1 month in the future</li>
<li>For the last month, prediction data is not available, so these values are all NaN (not a number)</li>
</ul>
<p class="_idGenParaOverride-1">We must drop the last month, because we cannot use these values for the prediction.</p>
<div></div>
<h3 id="_idParaDest-67"><a id="_idTextAnchor071"></a>
 Feature Scaling</h3>
<p>At times, we have multiple features that may have values within completely different ranges. Imagine comparing micrometers on a map to kilometers in the real world. They won't be easy to handle because of the magnitudinal difference of nine zeros.</p>
<p>A less dramatic difference is the difference between imperial and metric data. Pounds and kilograms, and centimeters and inches, don't compare that well.</p>
<p>Therefore, we often scale our features to normalized values that are easier to handle, as we can compare values of this range more easily. We scale training and testing data together. Ranges are typically scaled within [-1;1].</p>
<p>We will demonstrate two types of scaling:</p>
<ul>
<li>Min-Max normalization</li>
<li>Mean normalization</li>
</ul>
<p>Min-max scaling is calculated as follows:</p>
<p class="snippet">x_scaled[n] = (x[n] - min(x)) / (max(x)-min(x))</p>
<p>Mean normalization is calculated as follows:</p>
<p class="snippet">avg = sum(x) / len(x)</p>
<p class="snippet">x_scaled[n] = (x[n] – avg) / (max(x)-min(x))</p>
<p class="snippet">[(float(i)-avg)/(max(fibonacci)-min(fibonacci)) for i in fibonacci]</p>
<p>Here's an example of Min-Max and min-max:</p>
<p class="snippet">fibonacci = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]</p>
<p class="snippet"># Min-Max scaling:</p>
<p class="snippet">[(float(i)-min(fibonacci))/(max(fibonacci)-min(fibonacci)) for i in fibonacci]</p>
<p class="snippet">[0.0,</p>
<p class="snippet">0.006944444444444444,</p>
<p class="snippet">0.006944444444444444,</p>
<p class="snippet">0.013888888888888888,</p>
<p class="snippet">0.020833333333333332,</p>
<p class="snippet">0.034722222222222224,</p>
<p class="snippet">0.05555555555555555,</p>
<p class="snippet">0.09027777777777778,</p>
<p class="snippet">0.14583333333333334,</p>
<p class="snippet">0.2361111111111111,</p>
<p class="snippet">0.3819444444444444,</p>
<p class="snippet">0.6180555555555556,</p>
<p class="snippet">1.0]</p>
<p class="snippet"># Mean normalization:</p>
<p class="snippet">avg = sum(fibonacci) / len(fibonacci)</p>
<p class="snippet"># 28.923076923076923</p>
<p class="snippet">[(float(i)-avg)/(max(fibonacci)-min(fibonacci)) for i in fibonacci]</p>
<p class="snippet">[-0.20085470085470086,</p>
<p class="snippet">-0.19391025641025642,</p>
<p class="snippet">-0.19391025641025642,</p>
<p class="snippet">-0.18696581196581197,</p>
<p class="snippet">-0.18002136752136752,</p>
<p class="snippet">-0.16613247863247863,</p>
<p class="snippet">-0.1452991452991453,</p>
<p class="snippet">-0.11057692307692307,</p>
<p class="snippet">-0.05502136752136752,</p>
<p class="snippet">0.035256410256410256,</p>
<p class="snippet">0.18108974358974358,</p>
<p class="snippet">0.4172008547008547,</p>
<p class="snippet">0.7991452991452992]</p>
<p>Scaling could add to the processing time, but often it is a sensible step to add.</p>
<p>In the scikit-learn library, we have access to a function that scales NumPy arrays:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">preprocessing.scale(fibonacci)</p>
<p class="snippet">array([-0.6925069 , -0.66856384, -0.66856384, -0.64462079, -0.62067773,</p>
<p class="snippet">       -0.57279161, -0.50096244, -0.38124715, -0.18970269, 0.12155706,</p>
<p class="snippet">        0.62436127, 1.43842524, 2.75529341])</p>
<p class="_idGenParaOverride-1">The scale method performs mean normalization. Notice that the result is a NumPy array.</p>
<div></div>
<h3 id="_idParaDest-68"><a id="_idTextAnchor072"></a>
 Cross-Validation with Training and Test Data</h3>
<p>Cross-validation measures the predictive performance of a statistical model. The better the cross-validation result, the more you can trust that your model can be used to predict the future.</p>
<p>During cross-validation, we test our model's ability to predict the future on real <strong class="bold _idGenCharOverride-1">test data</strong>
 . Test data is not used in the prediction process.</p>
<p>
<strong class="bold _idGenCharOverride-1">Training data</strong>
 is used to construct the model that predicts our results.</p>
<p>Once we load data from a data source, we typically separate data into a larger chunk of training data, and a smaller chunk of test data. This separation shuffles the entries of training and test data randomly. Then, it gives you an array of training features, their corresponding training labels, testing features, and their corresponding testing labels.</p>
<p>We can do the training-testing split using the <strong class="inline _idGenCharOverride-3">model_selection</strong>
 library of scikit-learn.</p>
<p>Suppose in our dummy example that we have scaled Fibonacci data and its indices as labels:</p>
<p class="snippet">features = preprocessing.scale(fibonacci)</p>
<p class="snippet">label = np.array(range(13))</p>
<p>Let's use 10% of the data as test data.</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">(x_train, x_test, y_train, y_test) =</p>
<p class="snippet">model_selection.train_test_split(features, label, test_size=0.1)</p>
<p class="snippet">x_train</p>
<p class="snippet">array([-0.66856384, 0.12155706, -0.18970269, -0.64462079, 1.43842524,</p>
<p class="snippet">        2.75529341, -0.6925069 , -0.38124715, -0.57279161, -0.62067773,</p>
<p class="snippet">       -0.66856384])</p>
<p class="snippet">x_test</p>
<p class="snippet">array([-0.50096244, 0.62436127])</p>
<p class="snippet">y_train</p>
<p class="snippet">array([1, 9, 8, 3, 11, 12, 0, 7, 5, 4, 2])</p>
<p class="snippet">y_test</p>
<p class="snippet">array([6, 10])</p>
<p>With training and testing, if we get the ratios wrong, we run the risk of overfitting or underfitting the model.</p>
<p>Overfitting occurs when we train the model too well, and it fits the training dataset too well. The model will be very accurate on the training data, but it will not be usable in real life, because its accuracy decreases when used on any other data. The model adjusts to the random noise in the training data and assumes patterns on this noise that yield false predictions. Underfitting occurs when the model does not fit the training data well enough to recognize important characteristics of the data. As a result, it cannot make the necessary predictions on new data. One example for this is when we attempt to do linear regression on data that is not linear. For instance, Fibonacci numbers are not linear, therefore, a model on a Fibonacci-like sequence cannot be linear either.</p>
<h4>Note</h4>
<p class="callout">If you remember the Cartesian coordinate system, you know that the horizontal axis is the X axis, and that the vertical axis is the Y axis. Our features are on the X axis, while our labels are on the Y axis. Therefore, we use features and X as synonyms, while labels are often denoted by Y. Therefore, x_test denotes feature test data, x_train denotes feature training data, y_test denotes label test data, and y_train denotes label training data.</p>
<h3 id="_idParaDest-69"><a id="_idTextAnchor073"></a>
 Fitting a Model on Data with scikit-learn</h3>
<p>We are illustrating the process of regression on a dummy example, where we only have one feature and very limited data.</p>
<p class="_idGenParaOverride-1">As we only have one feature, we have to format <strong class="inline _idGenCharOverride-3">x_train</strong>
 by reshaping it with <strong class="inline _idGenCharOverride-3">x_train.reshape (-1,1)</strong>
 to a NumPy array containing one feature.</p>
<div></div>
<p>Therefore, before executing the code on fitting the best line, execute the following code:</p>
<p>
<strong class="inline _idGenCharOverride-3">x_train = x_train.reshape(-1, 1)</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3">x_test = x_test.reshape(-1, 1)</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3"># array([a, b, c]).reshape(-1, 1) becomes:</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3"># array([[a, b, c]])</strong>
</p>
<p>Suppose we have train and test data for our features and labels.</p>
<p>We can fit a model on this data for performing prediction. We will now use linear regression for this purpose:</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">linear_regression = linear_model.LinearRegression()</p>
<p class="snippet">model = linear_regression.fit(x_train, y_train)</p>
<p class="snippet">model.predict(x_test)</p>
<p class="snippet">array([4.16199119, 7.54977143])</p>
<p>We can also calculate the score associated with the model:</p>
<p class="snippet">model.score(x_test, y_test)</p>
<p class="snippet">-0.17273705326696565</p>
<p>This score is the mean square error and represents the accuracy of the model. It represents how well we can predict features from labels.</p>
<p>This number indicates a very bad model. The best possible score is 1.0. A score of 0.0 can be achieved if we constantly predict the labels by ignoring the features. We will omit the mathematical background of this score in this book.</p>
<p>Our model does not perform well for two reasons:</p>
<ul>
<li>11 training data and 2 testing data are simply not enough to perform proper predictive analysis.</li>
<li>Even if we ignore the number of points, the Fibonacci <strong class="inline _idGenCharOverride-3">x -&gt; y</strong>
 function does not describe a linear relationship between x and y. Approximating a non-linear function with a line is only useful if we are very close to the training interval.</li>
</ul>
<p class="_idGenParaOverride-1">We will see a lot of more accurate models in the future, and we may even reach model scores of 0.9.</p>
<div></div>
<h3 id="_idParaDest-70"><a id="_idTextAnchor074"></a>
 Linear Regression Using NumPy Arrays</h3>
<p>One reason why NumPy arrays are handier than Python lists is that they can be treated as vectors. There are a few operations defined on vectors that can simplify our calculations. We can perform operations on vectors of similar lengths. The sum and the (vectorial) product of two vectors equals a vector, where each coordinate is the sum or (vectorial) product of the corresponding coordinates.</p>
<p>For example:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">v1 = np.array([1,2,3])</p>
<p class="snippet">v2 = np.array([2,0,2])</p>
<p class="snippet">v1 + v2 # array([3, 2, 5])</p>
<p class="snippet">v1 * v2 # array([2, 0, 6])</p>
<p>The product of a vector and a scalar is a vector, where each coordinate is multiplied by the scalar:</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-3">v1 * 2 # array([2, 4, 6])</strong>
</p>
<p>The second power of a vector equals the vectorial product of the vector with itself. The double asterisk denotes the power operator:</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-3">v1 ** 2 # array([1, 4, 9], dtype=int32)</strong>
</p>
<p>Suppose we have a set of points in the plane. Our job is to find the best fit line.</p>
<p>Let's see two examples.</p>
<p>Our first example contains 13 values that seem linear in nature. We are plotting the following data:</p>
<p class="snippet _idGenParaOverride-1">[2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62]</p>
<div></div>
<p>If you wanted to draw a line that is the closest to these dots, your educated guess would be quite close to reality:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer045" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00023.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 3.4: Plotted graph of values [2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62]</h6>
<div></div>
<p>Our second example is the first 13 values of the Fibonacci sequence, after scaling. Although we can define a line that fits these points the closest, we can see from the distribution of the points that our model will not be too useful:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer046" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00024.jpg" alt="" />
</div>
</div>
<h6>Figure 3.5: Plotted graph of Fibonacci values</h6>
<p>We have already learned what the equation of a straight line is: <strong class="inline _idGenCharOverride-3">y = a * x + b</strong>
</p>
<p>In this equation, <strong class="inline _idGenCharOverride-3">a</strong>
 is the slope, and <strong class="inline _idGenCharOverride-3">b</strong>
 is the <strong class="inline _idGenCharOverride-3">y</strong>
 -intercept. To find the line of best fit, we have to find the co-efficients <strong class="inline _idGenCharOverride-3">a</strong>
 and <strong class="inline _idGenCharOverride-3">b</strong>
 .</p>
<p>Our job is to minimize the sum of distances from the line of best fit.</p>
<p class="_idGenParaOverride-1">In this book, we will save the thought process behind calculating the coefficients <strong class="inline _idGenCharOverride-3">a</strong>
 and <strong class="inline _idGenCharOverride-3">b</strong>
 , because you will find little practical use for it. We would rather utilize the mean as the arithmetic mean of the values in a list. We can use the mean function provided by NumPy for this.</p>
<div></div>
<p>Let's find the line of best fit for these two examples:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from numpy import mean</p>
<p class="snippet">x = np.array(range(1, 14))</p>
<p class="snippet">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet">a = (mean(x)*mean(y) - mean(x*y)) / (mean(x) ** 2 - mean( x ** 2 ))</p>
<p class="snippet">4.857142857142859</p>
<p class="snippet">b = mean(y) - a*mean(x)</p>
<p class="snippet">-2.7692307692307843</p>
<p>Once we plot the line y = a*x + b with the preceding coefficients, we get the following graph:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer047" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00025.jpg" alt="" />
</div>
</div>
<h6>Figure 3.6: Plotted graph of array values [2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62] and the line y=a*x+b</h6>
<h4>Note</h4>
<p class="callout _idGenParaOverride-1">You can find a linear regression calculator at <a href="http://www.endmemo.com/statistics/lr.php">http://www.endmemo.com/statistics/lr.php</a>
 . You can also check the calculator to get an idea of what lines of best fit look like on a given dataset.</p>
<div></div>
<p>Regarding the scaled Fibonacci values, the line of best fit looks as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer048" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00026.jpg" alt="" />
</div>
</div>
<h6>Figure 3.7: Plotted graph showing Fibonacci values and the line y=a*x+b</h6>
<p>The best fit line of the second dataset clearly appears more off from anywhere outside the trained interval.</p>
<h4>Note</h4>
<p class="callout _idGenParaOverride-1">We don't have to use this method to perform linear regression. Many libraries, including scikit-learn, will help us automatize this process. Once we perform linear regression with multiple variables, we are better off using a library to perform regression for us.</p>
<div></div>
<h3 id="_idParaDest-71"><a id="_idTextAnchor075"></a>
 Fitting a Model Using NumPy Polyfit</h3>
<p>NumPy Polyfit can also be used to create a line of best fit for linear regression with one variable.</p>
<p>Recall the calculation for the line of best fit:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from numpy import mean</p>
<p class="snippet">x = np.array(range(1, 14))</p>
<p class="snippet">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet">a = (mean(x)*mean(y) - mean(x*y)) / (mean(x) ** 2 - mean( x ** 2 ))</p>
<p class="snippet">4.857142857142859</p>
<p class="snippet">b = mean(y) - a*mean(x)</p>
<p class="snippet">-2.7692307692307843</p>
<p>The equation for finding the coefficients a and b is quite long. Fortunately, <strong class="inline _idGenCharOverride-3">numpy.polyfit</strong>
 performs these calculations to find the coefficients of the line of best fit. The <strong class="inline _idGenCharOverride-3">polyfit</strong>
 function accepts three arguments: the array of <strong class="inline _idGenCharOverride-3">x</strong>
 values, the array of <strong class="inline _idGenCharOverride-3">y</strong>
 values, and the degree of polynomial to look for. As we are looking for a straight line, the highest power of <strong class="inline _idGenCharOverride-3">x</strong>
 is 1 in the polynomial:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">x = np.array(range(1, 14))</p>
<p class="snippet">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet">[a, b] = np.polyfit(x, y, 1)</p>
<p class="snippet">[4.857142857142858, -2.769230769230769]</p>
<p>Plotting the Results in Python</p>
<p>Suppose you have a set of data points and a regression line. Our task is to plot the points and the line together so that we can see the results with our own eyes.</p>
<p>We will use the <strong class="inline _idGenCharOverride-3">matplotlib.pyplot</strong>
 library for this. This library has two important functions:</p>
<ul>
<li>
<strong class="keyword _idGenCharOverride-1">Scatter:</strong>
 This displays scattered points on the plane, defined by a list of x coordinates and a list of y coordinates.</li>
<li class="_idGenParaOverride-1">
<strong class="keyword _idGenCharOverride-1">Plot:</strong>
 Along with two arguments, this function plots a segment defined by two points, or a sequence of segments defined by multiple points. Plot is like scatter, except that instead of displaying the points, they are connected by lines.</li>
<li style="list-style: none; display: inline"><div></div>
</li>
</ul>
<p>A plot with three arguments plots a segment and/or two points formatted according to the third argument</p>
<p>A segment is defined by two points. As <strong class="inline _idGenCharOverride-3">x</strong>
 ranges between 1 and 14, it makes sense to display a segment between 0 and 15. We must substitute the value of <strong class="inline _idGenCharOverride-3">x</strong>
 in the equation <strong class="inline _idGenCharOverride-3">a*x+b</strong>
 to get the corresponding <strong class="inline _idGenCharOverride-3">y</strong>
 values:</p>
<p class="snippet">import matplotlib.pyplot as plot</p>
<p class="snippet">x = np.array(range(1, 14))</p>
<p class="snippet">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet">a = (mean(x)*mean(y) - mean(x*y)) / (mean(x) ** 2 - mean(x ** 2))</p>
<p class="snippet">4.857142857142859</p>
<p class="snippet">b = mean(y) - a*mean(x)</p>
<p class="snippet">-2.7692307692307843</p>
<p class="snippet"># Plotting the points</p>
<p class="snippet">plot.scatter(x, y)</p>
<p class="snippet"># Plotting the line</p>
<p class="snippet">plot.plot([0, 15], [b, 15*a+b])</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer049" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00027.jpg" alt="" />
</div>
</div>
<h6>Figure 3.8: Graph displaying how data points fit a regression line</h6>
<p>You might have to call <strong class="inline _idGenCharOverride-3">plot.show()</strong>
 to display the preceding graph. In the IPython console, the coordinate system shows up automatically.</p>
<p>The segment and the scattered data points are displayed as expected.</p>
<p>Plot has an advanced signature. You can use one call of plot to draw scattered dots, lines, and any curves on this diagram. These variables are interpreted in groups of three:</p>
<ul>
<li>X values</li>
<li>Y values</li>
<li class="_idGenParaOverride-1">Formatting options in the form of a string</li>
<li style="list-style: none; display: inline"><div></div>
</li>
</ul>
<p>Let's create a function for deriving an array of approximated y values from an array of approximated x values:</p>
<p class="snippet">def fitY( arr ):</p>
<p class="snippet">    return [4.857142857142859 * x - 2.7692307692307843 for x in arr]</p>
<p>We will use the <strong class="inline _idGenCharOverride-3">fit</strong>
 function to plot values:</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    x, y, 'go',</p>
<p class="snippet">    x, fitY(x), 'r--o'</p>
<p class="snippet">)</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer050" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00028.jpg" alt="" />
</div>
</div>
<h6>Figure 3.9: Graph for the plot function using the fit function</h6>
<p class="_idGenParaOverride-1">The Python plotter library offers a simple solution for most of your graphing problems. You can draw as many lines, dots, and curves as you want on this graph.</p>
<div></div>
<p>Every third variable is responsible for formatting. The letter g stands for green, while the letter r stands for red. You could have used b for blue, y for yellow, and so on. In the absence of a color, each triple will be displayed using a different color. The o character symbolizes that we want to display a dot where each data point lies. Therefore, 'go' has nothing to do with movement – it requests the plotter to plot green dots. The '-' characters are responsible for displaying a dashed line. If you just use one minus, a straight line appears instead of the dashed line.</p>
<p>If we simplify this formatting, we can specify that we only want dots of an arbitrary color, and straight lines of another arbitrary color. By doing this, we can simply write the following plot call:</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    x, y, 'o',</p>
<p class="snippet">    x, fitY(x), '-'</p>
<p class="snippet">)</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer051" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00029.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 3.10: Graph for plot function with dashed line</h6>
<div></div>
<p>When displaying curves, the plotter connects the dots with segments. Also, keep in mind that even a complex sequence of curves is an approximation that connects the dots. For instance, if you execute the code from <a href="https://gist.github.com/traeblain/1487795">https://gist.github.com/traeblain/1487795</a>
 , you will recognize the segments of the batman function as connected lines:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer052" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00030.jpg" alt="" />
</div>
</div>
<h6>Figure 3.11: Graph for the batman function</h6>
<p>There is a wide variety of ways to plot curves. We have seen that the <strong class="inline _idGenCharOverride-3">polyfit</strong>
 method of the NumPy library returns an array of coefficients to describe a linear equation:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">x = np.array(range(1, 14))</p>
<p class="snippet">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet">np.polyfit(x, y, 1)</p>
<p class="snippet"># array([ 4.85714286, -2.76923077])</p>
<p>This array describes the equation <strong class="inline _idGenCharOverride-3">4.85714286 * x - 2.76923077</strong>
 .</p>
<p>Suppose we now want to plot a curve, <strong class="inline _idGenCharOverride-3">y = -x**2 + 3*x - 2</strong>
 . This quadratic equation is described by the coefficient array <strong class="inline _idGenCharOverride-3">[-1, 3, -2].</strong>
 We could write our own function to calculate the y values belonging to x values. However, the NumPy library already has a feature to do this work for us: <strong class="inline _idGenCharOverride-3">np.poly1d</strong>
 :</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">x = np.array(range( -10, 10, 0.2 ))</p>
<p class="snippet _idGenParaOverride-1">f = np.poly1d([-1,3,-2])</p>
<div></div>
<p>The f function that's created by the poly1d call not only works with single values, but also with lists or NumPy arrays:</p>
<p class="snippet">f(5)</p>
<p class="snippet"># -12</p>
<p class="snippet">f(x)</p>
<p class="snippet"># array([-132, -110, -90, -72, -56, -42, -30, -20, -12, -6, -2, 0, 0, -2, -6, -12, -20, -30, -42, -56])</p>
<p>We can now use these values to plot a non-linear curve:</p>
<p class="snippet">import matplotlib.pyplot as plot</p>
<p class="snippet">plot.plot(x, f(x))</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer053" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00031.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 3.12: Graph for pyplot function</h6>
<div></div>
<h3 id="_idParaDest-72"><a id="_idTextAnchor076"></a>
 Predicting Values with Linear Regression</h3>
<p>Suppose we are interested in the <strong class="inline _idGenCharOverride-3">y</strong>
 value belonging to the <strong class="inline _idGenCharOverride-3">x</strong>
 coordinate <strong class="inline _idGenCharOverride-3">20</strong>
 . Based on the linear regression model, all we need to do is substitute the value of <strong class="inline _idGenCharOverride-3">20</strong>
 in the place of <strong class="inline _idGenCharOverride-3">x</strong>
 :</p>
<p class="snippet"># Plotting the points</p>
<p class="snippet">plot.scatter(x, y)</p>
<p class="snippet"># Plotting the prediction belonging to x = 20</p>
<p class="snippet">plot.scatter(20, a * 20 + b, color='red')</p>
<p class="snippet"># Plotting the line</p>
<p class="snippet">plot.plot([0, 25], [b, 25*a+b])</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer054" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00032.jpg" alt="" />
</div>
</div>
<h6>Figure 3.13: Graph showing the predicted value using Linear Regression</h6>
<p class="_idGenParaOverride-1">Here, we denoted the predicted value with red. This red point is on the best fit line.</p>
<div></div>
<h3 id="_idParaDest-73"><a id="_idTextAnchor077"></a>
 Activity 5: Predicting Population</h3>
<p>You are working at the government office of Metropolis, trying to forecast the need for elementary school capacity. Your task is to figure out a 2025 and 2030 prediction for the number of children starting elementary school. The past data is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer055" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00033.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 3.14: A table representing number of kids starting elementary school from 2001 to 2018</h6>
<div></div>
<p>Plot these tendencies on a two-dimensional chart. To do this, you must:</p>
<ol>
<li value="1">Use linear regression. Features are the years ranging from 2001 to 2018. For simplicity, we can indicate 2001 as year 1, and 2018 as year 18.</li>
<li value="2">Use <strong class="inline _idGenCharOverride-3">np.polyfit</strong>
 to determine the coefficients of the regression line.</li>
<li value="3">Plot the results using <strong class="inline _idGenCharOverride-3">matplotlib.pyplot</strong>
 to determine future tendencies.<h4 class="_idGenParaOverride-2">Note</h4>
<p class="callout _idGenParaOverride-2">The solution to this activity is available at page 269.</p>
</li>
</ol>
<h2 id="_idParaDest-74"><a id="_idTextAnchor078"></a>
 Linear Regression with Multiple Variables</h2>
<p>In the previous topic, we dealt with linear regression with one variable. Now we will learn an extended version of linear regression, where we will use multiple input variables to predict the output.</p>
<p>We will rely on examples where we will load and predict stock prices. Therefore, we will experiment with the main libraries used for loading stock prices.</p>
<h3 id="_idParaDest-75"><a id="_idTextAnchor079"></a>
 Multiple Linear Regression</h3>
<p>If you recall the formula for the line of best fit in linear regression, it was defined as <strong class="inline _idGenCharOverride-3">y = a*x + b</strong>
 , where <strong class="inline _idGenCharOverride-3">a</strong>
 is the slope of the line, <strong class="inline _idGenCharOverride-3">b</strong>
 is the y-intercept of the line, <strong class="inline _idGenCharOverride-3">x</strong>
 is the feature value, and <strong class="inline _idGenCharOverride-3">y</strong>
 is the calculated label value.</p>
<p>In multiple regression, we have multiple features and one label. Assuming that we have three features, <strong class="inline _idGenCharOverride-3">x1</strong>
 , <strong class="inline _idGenCharOverride-3">x2</strong>
 , and <strong class="inline _idGenCharOverride-3">x3</strong>
 , our model changes as follows:</p>
<p class="snippet">y = a1 * x1 + a2 * x2 + a3 * x3 + b</p>
<p>In NumPy array format, we can write this equation as follows:</p>
<p class="snippet">y = np.dot(np.array([a1, a2, a3]), np.array([x1, x2, x3])) + b</p>
<p>For convenience, it makes sense to define the whole equation in a vector multiplication form. The coefficient of <strong class="inline _idGenCharOverride-3">b</strong>
 is going to be <strong class="inline _idGenCharOverride-3">1</strong>
 :</p>
<p class="snippet _idGenParaOverride-1">y = np.dot(np.array([b, a1, a2, a3]) * np.array([1, x1, x2, x3]))</p>
<div></div>
<p>Multiple linear regression is a simple scalar product of two vectors, where the coefficients <strong class="inline _idGenCharOverride-3">b</strong>
 , <strong class="inline _idGenCharOverride-3">a1</strong>
 , <strong class="inline _idGenCharOverride-3">a2</strong>
 , and <strong class="inline _idGenCharOverride-3">a3</strong>
 determine the best fit equation in four-dimensional space.</p>
<p>To understand the formula of multiple linear regression, you will need the scalar product of two vectors. As the other name for a scalar product is dot product, the NumPy function performing this operation is called dot:</p>
<p>
<strong class="inline _idGenCharOverride-3">import numpy as np</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3">v1 = [1, 2, 3]</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3">v2 = [4, 5, 6]</strong>
</p>
<p>
<strong class="inline _idGenCharOverride-3">np.dot( v1, v2 ) = 1 * 4 + 2 * 5 + 3 * 6 = 32</strong>
</p>
<p>We simply sum the product of each respective coordinate.</p>
<p>We can determine these coefficients by minimizing the error between data points and the nearest points described by the equation. For simplicity, we will omit the mathematical solution of the best fit equation, and use scikit-learn instead.</p>
<h4>Note</h4>
<p class="callout">In n-dimensional spaces, where n is greater than 3, the number of dimensions determines the different variables that are in our model. In the preceding example, we have three features and one label. This yields four dimensions. If you want to imagine a four-dimensional space, you can imagine three-dimensional space and time for simplification. A five-dimensional space can be imagined as a four-dimensional space, where each point in time has a temperature. Dimensions are just features (and labels); they do not necessarily correlate with our concept of three-dimensional space.</p>
<h3 id="_idParaDest-76"><a id="_idTextAnchor080"></a>
 The Process of Linear Regression</h3>
<p>We will follow the following simple steps to solve Linear Regression problems:</p>
<ol>
<li class="ParaOverride-1" value="1">Load data from data sources.</li>
<li value="2">Prepare data for prediction (normalize, format, filter).</li>
<li class="_idGenParaOverride-1" value="3">Compute the parameters of the regression line. Regardless of whether we use linear regression with one variable or with multiple variables, we will follow these steps.<div></div>
</li>
</ol>
<h3 id="_idParaDest-77"><a id="_idTextAnchor081"></a>
 Importing Data from Data Sources</h3>
<p>There are multiple libraries that can provide us with access to data sources. As we will be working with stock data, let's cover two examples that are geared toward retrieving financial data, Quandl and Yahoo Finance:</p>
<ul>
<li>scikit-learn comes with a few datasets that can be used for practicing your skills.</li>
<li>
<a href="http://Quandl.com">Quandl.com</a>
 provides you with free and paid financial datasets.</li>
<li>
<a href="http://pandas.io">pandas.io</a>
 helps you load any .csv, .excel, .json, or SQL data.</li>
<li>Yahoo Finance provides you with financial datasets.</li>
</ul>
<h3 id="_idParaDest-78"><a id="_idTextAnchor082"></a>
 Loading Stock Prices with Yahoo Finance</h3>
<p>The process of loading stock data with Yahoo Finance is straightforward. All you need to do is install the fix_yahoo_finance package using the following command in the CLI::</p>
<p class="snippet">pip install fix_yahoo_finance</p>
<p>We will download a dataset that has open, high, low, close, adjusted close, and volume values of the S&amp;P 500 index, starting from 2015:</p>
<p class="snippet">import fix_yahoo_finance as yahoo</p>
<p class="snippet">spx_data_frame = yahoo.download("^GSPC", "2015-01-01")</p>
<p>That's all you need to do. The data frame containing the S&amp;P 500 index is ready.</p>
<p>You can plot the index prices with the plot method:</p>
<p class="snippet _idGenParaOverride-1">spx_data_frame.Close.plot()</p>
<div></div>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer056" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00034.jpg" alt="" />
</div>
</div>
<h6>Figure 3.15: Graph showing stock prices for Yahoo Finance</h6>
<p>It is also possible to save data to a CSV file using the following command:</p>
<p class="snippet">spx.to_csv("spx.csv")</p>
<h3 id="_idParaDest-79"><a id="_idTextAnchor083"></a>
 Loading Files with pandas</h3>
<p>Suppose a CSV file containing stock data is given. We will now use pandas to load data from this file:</p>
<p class="snippet">import pandas as pd</p>
<p class="snippet">spx_second_frame = pd.read_csv("spx.csv", index_col="Date", header=0, parse_dates=True)</p>
<p>To properly parse data, we must set the index column name, specify the absence of headers, and make sure that dates are parsed as dates.</p>
<h3 id="_idParaDest-80"><a id="_idTextAnchor084"></a>
 Loading Stock Prices with Quandl</h3>
<p class="_idGenParaOverride-1">Quandl.com is a reliable source of financial and economic datasets.</p>
<div></div>
<h3 id="_idParaDest-81"><a id="_idTextAnchor085"></a>
 Exercise 8: Using Quandl to Load Stock Prices</h3>
<ol>
<li class="ParaOverride-1" value="1">Open the Anaconda prompt and install Quandl using the following command:<p class="snippet _idGenParaOverride-3">pip install quandl</p>
</li>
<li value="2">Go to <a href="https://www.quandl.com/">https://www.quandl.com/</a>
 .</li>
<li value="3">Click on the Financial data.</li>
<li value="4">Among the filters, click the checkbox next to the <strong class="bold _idGenCharOverride-1">Free</strong>
 label. If you have a Quandl subscription, you can use that to download stock data.</li>
<li value="5">Select a stock or index you would like to use. For our example, we will use the S&amp;P Composite index data that was collected by the Yale Department of Economics. The link for this is <a href="https://www.quandl.com/data/YALE/SPCOMP-S-P-Composite">https://www.quandl.com/data/YALE/SPCOMP-S-P-Composite</a>
 .</li>
<li value="6">Find the Quandl ticker belonging to your instrument you would like to load. Our Quandl code for the S&amp;P 500 data is "<strong class="inline _idGenCharOverride-3">YALE/SPCOMP</strong>
 ".</li>
<li value="7">Load the data from the Jupyter QtConsole:<p class="snippet _idGenParaOverride-3">import quandl</p>
<p class="snippet _idGenParaOverride-3">data_frame = quandl.get("YALE/SPCOMP")</p>
</li>
<li value="8">All the columns of the imported values are features:<p class="snippet _idGenParaOverride-3">data_frame.head()</p>
</li>
<li value="9">The output is as follows:<p class="snippet _idGenParaOverride-3">            S&amp;P Composite Dividend Earnings        CPI Long Interest Rate \</p>
<p class="snippet _idGenParaOverride-3">Year                                                                        </p>
<p class="snippet _idGenParaOverride-3">1871-01-31         4.44     0.26     0.4 12.464061            5.320000</p>
<p class="snippet _idGenParaOverride-3">1871-02-28         4.50     0.26     0.4 12.844641            5.323333</p>
<p class="snippet _idGenParaOverride-3">1871-03-31         4.61     0.26     0.4 13.034972            5.326667</p>
<p class="snippet _idGenParaOverride-3">1871-04-30         4.74     0.26     0.4 12.559226            5.330000</p>
<p class="snippet _idGenParaOverride-3">1871-05-31         4.86     0.26     0.4 12.273812            5.333333</p>
<p class="snippet _idGenParaOverride-3">            Real Price Real Dividend Real Earnings \</p>
<p class="snippet _idGenParaOverride-3">Year                                                </p>
<p class="snippet _idGenParaOverride-3">1871-01-31 89.900119     5.264421     8.099110</p>
<p class="snippet _idGenParaOverride-3">1871-02-28 88.415295     5.108439     7.859137</p>
<p class="snippet _idGenParaOverride-3">1871-03-31 89.254001     5.033848     7.744382</p>
<p class="snippet _idGenParaOverride-3">1871-04-30 95.247222     5.224531     8.037740</p>
<p class="snippet _idGenParaOverride-3">1871-05-31 99.929493     5.346022     8.224650</p>
<p class="snippet _idGenParaOverride-3">            Cyclically Adjusted PE Ratio</p>
<p class="snippet _idGenParaOverride-3">Year                                    </p>
<p class="snippet _idGenParaOverride-3">1871-01-31                         NaN</p>
<p class="snippet _idGenParaOverride-3">1871-02-28                         NaN</p>
<p class="snippet _idGenParaOverride-3">1871-03-31                         NaN</p>
<p class="snippet _idGenParaOverride-3">1871-04-30                         NaN</p>
<p class="snippet _idGenParaOverride-3">1871-05-31                         NaN</p>
<p class="snippet _idGenParaOverride-3">                                 ...</p>
<p class="snippet _idGenParaOverride-3">2016-02-29                     24.002607</p>
<p class="snippet _idGenParaOverride-3">2016-03-31                     25.372299</p>
</li>
</ol>
<h3 id="_idParaDest-82"><a id="_idTextAnchor086"></a>
 Preparing Data for Prediction</h3>
<p>Before we perform regression, we must choose the features we are interested in, and we also have to figure out the data range on which we do the regression.</p>
<p>Preparing the data for prediction is the second step in the regression process. This step also has several sub-steps. We will go through these sub-steps in the following order:</p>
<ol>
<li class="ParaOverride-1" value="1">Suppose a data frame is given with preloaded data.</li>
<li value="2">Select the columns from the dataset you are interested in.</li>
<li value="3">Replace NaN values with a numeric value to avoid getting rid of data.</li>
<li value="4">Determine the forecast interval T, determining the amount of time or number of data rows you wish to look into in the future.</li>
<li value="5">Create a label column out of the value you wish to forecast. For row i of the data frame, the value of the label should belong to the time instant, i+T.</li>
<li value="6">For the last T rows, the label value is NaN. Drop these rows from the data frame.</li>
<li value="7">Create NumPy arrays from the features and the label.</li>
<li value="8">Scale the features array.</li>
<li value="9">Separate the training and testing data.</li>
</ol>
<p>A few features highly correlate to each other. For instance, the Real Dividend column proportionally grows with Real Price. The ratio between them is not always similar, but they do correlate.</p>
<p>As regression is not about detecting correlation between features, we would rather get rid of a few attributes we know are redundant and perform regression on the features that are non-correlated.</p>
<p>If you have gone through the Loading stock prices with Quandl section, you already have a data frame containing historical data on the S&amp;P 500 Index. We will keep the Long Interest Rate, Real Price, and Real Dividend columns:</p>
<p class="snippet">data_frame[['Long Interest Rate', 'Real Price', 'Real Dividend', 'Cyclically Adjusted PE Ratio']]</p>
<p>As you cannot work with NaN data, you can replace it by filling in numbers in place of NaNs. In general, you have two choices:</p>
<ul>
<li>Get rid of the data</li>
<li>Replace the data with a default value that makes sense</li>
</ul>
<p class="snippet">data_frame.fillna(-100, inplace=True)</p>
<p>We can check the length of the data frame by using the <strong class="inline _idGenCharOverride-3">len</strong>
 function, as shown in the following code:</p>
<p class="snippet">len(data_frame)</p>
<p class="snippet">1771</p>
<p>The length of our data frame is 1771.</p>
<p>If we want to predict the Real Price for the upcoming 20 years, we will have to predict 240 values. This is approximately 15% of the length of the data frame, which makes perfect sense.</p>
<p>We will therefore create a Real Price Label by shifting the Real Price values up by 240 units:</p>
<p class="snippet">data_frame['Real Price Label'] = data_frame['Real Price'].shift( -240 )</p>
<p>This way, each Real Price Label value will be the Real Price value in 20 years.</p>
<p>The side effect of shifting these values is that NaN values appear in the last 240 values:</p>
<p class="snippet _idGenParaOverride-1">data_frame.tail()</p>
<div></div>
<p>The output is as follows:</p>
<p class="snippet"></p>
<p class="snippet">            S&amp;P Composite Dividend Earnings     CPI Long Interest Rate \</p>
<p class="snippet">Year                                                                        </p>
<p class="snippet">2018-03-31        2702.77     50.00     NaN 249.5540             2.840</p>
<p class="snippet">2018-04-30        2653.63     50.33     NaN 250.5460             2.870</p>
<p class="snippet">2018-05-31        2701.49     50.66     NaN 251.5880             2.976</p>
<p class="snippet">2018-06-30        2754.35     50.99     NaN 252.1090             2.910</p>
<p class="snippet">2018-07-31        2736.61     NaN     NaN 252.3695             2.830</p>
<p class="snippet">             Real Price Real Dividend Real Earnings \</p>
<p class="snippet">Year                                                    </p>
<p class="snippet">2018-03-31 2733.262995     50.564106            NaN</p>
<p class="snippet">2018-04-30 2672.943397     50.696307            NaN</p>
<p class="snippet">2018-05-31 2709.881555     50.817364            NaN</p>
<p class="snippet">2018-06-30 2757.196024     51.042687            NaN</p>
<p class="snippet">2018-07-31 2736.610000            NaN            NaN</p>
<p class="snippet">            Cyclically Adjusted PE Ratio Real Price Label</p>
<p class="snippet">Year                                                        </p>
<p class="snippet">2018-03-31                     31.988336             NaN</p>
<p class="snippet">2018-04-30                     31.238428             NaN</p>
<p class="snippet">2018-05-31                     31.612305             NaN</p>
<p class="snippet">2018-06-30                     32.091415             NaN</p>
<p class="snippet">2018-07-31                     31.765318             NaN</p>
<p>We can get rid of them by executing dropna on the data frame:</p>
<p class="snippet _idGenParaOverride-1">data_frame.dropna(inplace=True)</p>
<div></div>
<p>This way, we have data up to 1998 July, and we have the future values up to 2018 in the Real Price Label column:</p>
<p class="snippet">data_frame.tail()</p>
<p>The output is as follows:</p>
<p class="snippet"></p>
<p class="snippet">            S&amp;P Composite Dividend Earnings    CPI Long Interest Rate \</p>
<p class="snippet">Year                                                                    </p>
<p class="snippet">1998-03-31        1076.83 15.6400 39.5400 162.2                5.65</p>
<p class="snippet">1998-04-30        1112.20 15.7500 39.3500 162.5                5.64</p>
<p class="snippet">1998-05-31        1108.42 15.8500 39.1600 162.8                5.65</p>
<p class="snippet">1998-06-30        1108.39 15.9500 38.9700 163.0                5.50</p>
<p class="snippet">1998-07-31        1156.58 16.0167 38.6767 163.2                5.46</p>
<p class="snippet">             Real Price Real Dividend Real Earnings \</p>
<p class="snippet">Year                                                    </p>
<p class="snippet">1998-03-31 1675.456527     24.334519     61.520900</p>
<p class="snippet">1998-04-30 1727.294510     24.460428     61.112245</p>
<p class="snippet">1998-05-31 1718.251850     24.570372     60.705096</p>
<p class="snippet">1998-06-30 1716.097117     24.695052     60.336438</p>
<p class="snippet">1998-07-31 1788.514193     24.767932     59.808943</p>
<p class="snippet">            Cyclically Adjusted PE Ratio Real Price Label</p>
<p class="snippet">Year                                                        </p>
<p class="snippet">1998-03-31                     36.296928     2733.262995</p>
<p class="snippet">1998-04-30                     37.276934     2672.943397</p>
<p class="snippet">1998-05-31                     36.956599     2709.881555</p>
<p class="snippet">1998-06-30                     36.802293     2757.196024</p>
<p class="snippet _idGenParaOverride-1">1998-07-31                     38.259645     2736.610000</p>
<div></div>
<p>Let's prepare our features and labels for regression.</p>
<p>For the features, we will use the drop method of the data frame. The drop method returns a new data frame that doesn't contain the column that was dropped:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">features = np.array(data_frame.drop('Real Price Label', 1))</p>
<p class="snippet">label = np.array(data_frame['Real Price Label'])</p>
<p>The 1 in the second argument specifies that we are dropping columns. As the original data frame was not modified, the label can be directly extracted from it.</p>
<p>It is now time to scale the features with the preprocessing module of Scikit Learn:</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">scaled_features = preprocessing.scale(features)</p>
<p class="snippet">features</p>
<p class="snippet">array([[6.19000000e+00, 2.65000000e-01, 4.85800000e-01, ...,</p>
<p class="snippet">        7.10000389e+00, 1.30157807e+01, 1.84739523e+01],</p>
<p class="snippet">       [6.17000000e+00, 2.70000000e-01, 4.81700000e-01, ...,</p>
<p class="snippet">        7.16161179e+00, 1.27768459e+01, 1.81472582e+01],</p>
<p class="snippet">       [6.24000000e+00, 2.75000000e-01, 4.77500000e-01, ...,</p>
<p class="snippet">        7.29423423e+00, 1.26654431e+01, 1.82701191e+01],</p>
<p class="snippet">       ...,</p>
<p class="snippet">       [1.10842000e+03, 1.58500000e+01, 3.91600000e+01, ...,</p>
<p class="snippet">        2.45703721e+01, 6.07050959e+01, 3.69565985e+01],</p>
<p class="snippet">       [1.10839000e+03, 1.59500000e+01, 3.89700000e+01, ...,</p>
<p class="snippet">        2.46950523e+01, 6.03364381e+01, 3.68022935e+01],</p>
<p class="snippet">       [1.15658000e+03, 1.60167000e+01, 3.86767000e+01, ...,</p>
<p class="snippet">        2.47679324e+01, 5.98089427e+01, 3.82596451e+01]])</p>
<p class="snippet">scaled_features</p>
<p class="snippet">array([[-0.47564285, -0.62408514, -0.57496262, ..., -1.23976862,</p>
<p class="snippet">        -0.84099698, 0.6398416 ],</p>
<p class="snippet">       [-0.47577027, -0.62273749, -0.5754623 , ..., -1.22764677,</p>
<p class="snippet">        -0.85903686, 0.57633607],</p>
<p class="snippet">       [-0.47532429, -0.62138984, -0.57597417, ..., -1.20155224,</p>
<p class="snippet">        -0.86744792, 0.60021881],</p>
<p class="snippet">       ...,</p>
<p class="snippet">       [ 6.54690076, 3.57654404, 4.13838295, ..., 2.19766676,</p>
<p class="snippet">         2.75960615, 4.23265262],</p>
<p class="snippet">       [ 6.54670962, 3.60349707, 4.11522706, ..., 2.22219859,</p>
<p class="snippet">         2.73177202, 4.20265751],</p>
<p class="snippet">       [ 6.85373845, 3.62147473, 4.07948167, ..., 2.23653834,</p>
<p class="snippet">         2.69194545, 4.48594968]])</p>
<p>As you can see, the scaled features are easier to read and interpret. While scaling data, we must scale all data together, including training and testing data.</p>
<h3 id="_idParaDest-83"><a id="_idTextAnchor087"></a>
 Performing and Validating Linear Regression</h3>
<p>Now that scaling is done, our next task is to separate the training and testing data from each other. We will be using 90% of the data as training data, and the rest (10%) will be used as test data:</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">(features_train, features_test, label_train, label_test) =</p>
<p class="snippet">    model_ selection.train_test_split(</p>
<p class="snippet">        scaled_features, label, test_size=0.1</p>
<p class="snippet">    )</p>
<p>The train_test_split function shuffles the lines of our data, keeping the correspondence, and puts approximately 10% of all data in the test variables, keeping 90% for the training variables. This will help us evaluate how good our model is.</p>
<p>We can now create the linear regression model based on the training data:</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p>Once the model is ready, we can use it to predict the labels belonging to the test feature values:</p>
<p class="snippet _idGenParaOverride-1">label_predicted = model.predict(features_test)</p>
<div></div>
<p>If you are interested in the relationship between the predicted feature values and the accurate test feature values, you can plot them using a Python two-dimensional graph plotter utility:</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet">plot.scatter(label_test, label_predicted)</p>
<p>This gives you an image of a graph where the test data is compared to the results of the prediction. The closer these values are to the <strong class="inline _idGenCharOverride-3">y = x</strong>
 line, the better.</p>
<p>You can see from the following graph that the predictions do center around the <strong class="inline _idGenCharOverride-3">y=x</strong>
 line with a degree of error. This error is obvious, as otherwise, we would be able to make a lot of money with such a simple prediction, and everyone would pursue predicting stock prices instead of working in their own field of expertise:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer057" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00035.jpg" alt="" />
</div>
</div>
<h6>Figure 3.16: Graph for the plot scatter function</h6>
<p>We can conclude that there is a degree of error in the model. The question is, how can we quantify this error? The answer is simple: we can score the model using a built-in utility that calculates the mean square error of the model:</p>
<p class="snippet">model.score(features_test, label_test)</p>
<p class="snippet _idGenParaOverride-1">0.9051697119010782</p>
<div></div>
<p>We can conclude that the model is very accurate. This is not a surprise, because every single financial advisor scammer tends to tell us that the market grows at around 6-7% a year. This is a linear growth, and the model essentially predicts that the markets will continue growing at a linear rate. Making a conclusion that markets tend to go up in the long run is not rocket science.</p>
<h3 id="_idParaDest-84"><a id="_idTextAnchor088"></a>
 Predicting the Future</h3>
<p>We have already used prediction on test data. Now, it's time to use the actual data to see into the future.</p>
<p class="snippet">import quandl</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">data_frame = quandl.get("YALE/SPCOMP")</p>
<p class="snippet">data_frame[['Long Interest Rate', 'Real Price', 'Real Dividend', 'Cyclically Adjusted PE Ratio']]</p>
<p class="snippet">data_frame.fillna(-100, inplace=True)</p>
<p class="snippet">data_frame['Real Price Label'] = data_frame['Real Price'].shift(-240)</p>
<p class="snippet">data_frame.dropna(inplace=True)</p>
<p class="snippet">features = np.array(data_frame.drop('Real Price Label', 1))</p>
<p class="snippet">label = np.array(data_frame['Real Price Label'])</p>
<p class="snippet">scaled_features = preprocessing.scale(features)</p>
<p class="snippet">(features_train, features_test, label_train, label_test) =</p>
<p class="snippet">    model_ selection.train_test_split(</p>
<p class="snippet">        scaled_features, label, test_size=0.1</p>
<p class="snippet">    )</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p class="snippet">label_predicted = model.predict(features_test)</p>
<p>The trick to predicting the future is that we have to save the values belonging to the values we dropped when building the model. We built our stock price model based on historical data from 20 years ago. Now, we have to keep this data, and we also have to include this data in scaling:</p>
<p class="snippet">import quandl</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">data_frame = quandl.get("YALE/SPCOMP")</p>
<p class="snippet">data_frame[['Long Interest Rate', 'Real Price', 'Real Dividend', 'Cyclically Adjusted PE Ratio']]</p>
<p class="snippet">data_frame.fillna(-100, inplace=True)</p>
<p class="snippet"># We shift the price data to be predicted 20 years forward</p>
<p class="snippet">data_frame[ 'Real Price Label'] = data_frame['Real Price'].shift(-240)</p>
<p class="snippet"># Then exclude the label column from the features</p>
<p class="snippet">features = np.array(data_frame.drop('Real Price Label', 1))</p>
<p class="snippet"># We scale before dropping the last 240 rows from the</p>
<p class="snippet"># features</p>
<p class="snippet">scaled_features = preprocessing.scale(features)</p>
<p class="snippet"># Save the last 240 rows before dropping them</p>
<p class="snippet">scaled_features_latest_240 = scaled_features[-240:]</p>
<p class="snippet"># Exclude the last 240 rows from the data used for model</p>
<p class="snippet"># building</p>
<p class="snippet">scaled_features = scaled_features[:-240]</p>
<p class="snippet"># Now we can drop the last 240 rows from the data frame</p>
<p class="snippet">data_frame.dropna(inplace=True)</p>
<p class="snippet"># Then build the labels from the remaining data</p>
<p class="snippet">label = np.array(data_frame['Real Price Label'])</p>
<p class="snippet"># The rest of the model building stays</p>
<p class="snippet">(features_train, features_test, label_train, label_test) =</p>
<p class="snippet">    model_ selection.train_test_split(</p>
<p class="snippet">        scaled_features, label, test_size=0.1</p>
<p class="snippet">    )</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p>Now that we have access to the scaled values of the features from the last 20 years, we can now predict the index prices of the next 20 years using the following code:</p>
<p class="snippet">label_predicted = model.predict(scaled_features_latest_240)</p>
<p class="_idGenParaOverride-1">This sounds great in theory, but in practice, using this model for making money by betting on the forecast is by no means better than gambling in a casino. This is just an example model to illustrate prediction; it is definitely not sufficient to be used for short-term or long-term speculation on market prices.</p>
<div></div>
<p>If you look at the values, you can see why this prediction may easily backfire. First, there are a few negative values, which are impossible for indices. Then, due to a few major market crashes, linear regression made a doomsday forecast a point in the future, where the index will drop from more than 3,000 to literally zero within a year. Linear regression is not a perfect tool to look ahead 20 years based on limited data. Also, note that stock prices are meant to be close to time invariant systems. This means that the past does not imply any patterns in the future.</p>
<p>Let's output the prediction belonging to the first ten years:</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet">plot.plot(list(range(1,241)), label_predicted[:240])</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer058" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00036.jpg" alt="" />
</div>
</div>
<h6>Figure 3.17: Graph for the plot function with a range of 1 to 241 and the label predicted as 240</h6>
<p>The graph is hard to read near the end due to extreme values. Let's draw our conclusions by omitting the last five years and just plotting the first 180 months out of the predictions:</p>
<p class="snippet _idGenParaOverride-1">plot.plot(list(range(1,181)), label_predicted[:180])</p>
<div></div>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer059" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00037.jpg" alt="" />
</div>
</div>
<h6>Figure 3.18: Graph for the plot function with a range of 1 to 181 and the label predicted as 180</h6>
<p>This is a scary future for the American economy. According to this model, the S&amp;P 500 has a bull run for about 2.5-3 years and doesn't recover for a long time. Also, notice that our model does not know that index values cannot be negative.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor089"></a>
 Polynomial and Support Vector Regression</h2>
<p>When performing polynomial regression, the relationship between x and y, or using their other names, features and labels, is not a linear equation, but a polynomial equation. This means that instead of the <strong class="inline _idGenCharOverride-3">y = a*x+b</strong>
 equation, we can have multiple coefficients and multiple powers of x in the equation.</p>
<p>To make matters even more complicated, we can perform polynomial regression using multiple variables, where each feature may have coefficients multiplying different powers of the feature.</p>
<p class="_idGenParaOverride-1">Our task is to find a curve that best fits our dataset. Once polynomial regression is extended to multiple variables, we will learn the Support Vector Machines model to perform polynomial regression.</p>
<div></div>
<h3 id="_idParaDest-86"><a id="_idTextAnchor090"></a>
 Polynomial Regression with One Variable</h3>
<p>As a recap, we have performed two types of regression so far:</p>
<ul>
<li>Simple linear regression: <strong class="inline _idGenCharOverride-3">y = a*x + b</strong>
</li>
<li>Multiple linear regression: <strong class="inline _idGenCharOverride-3">y = b + a1 * x1 + a2 * x2 + … + an * xn</strong>
</li>
</ul>
<p>We will now learn how to do polynomial linear regression with one variable. The equation for polynomial linear regression is as follows:</p>
<p class="snippet">
<strong class="inline _idGenCharOverride-3">y = b + a1*x + a2*(x ** 2) + a3*(x ** 3) + … + an * (x ** n)</strong>
</p>
<p>have a vector of coefficients <strong class="inline _idGenCharOverride-3">(b, a1, a2, …, an)</strong>
 multiplying a vector of degrees of x in the polynomial, <strong class="inline _idGenCharOverride-3">(1, x**1, x**2, …, x**n)</strong>
 .</p>
<p>At times, polynomial regression works better than linear regression. If the relationship between labels and features can be described using a linear equation, then using linear equation makes perfect sense. If we have a non-linear growth, polynomial regression tends to approximate the relationship between features and labels better.</p>
<p>The simplest implementation of linear regression with one variable was the <strong class="inline _idGenCharOverride-3">polyfit</strong>
 method of the NumPy library. In the next exercise, we will perform multiple polynomial linear regression with degrees of 2 and 3.</p>
<h4>Note</h4>
<p class="callout _idGenParaOverride-1">Even though our polynomial regression has an equation containing coefficients of x ** n, this equation is still referred to as polynomial linear regression in the literature. Regression is made linear not because we restrict the usage of higher powers of x in the equation, but because the coefficients a1, a2, …, and so on are linear in the equation. This means that we use the toolset of linear algebra, and work with matrices and vectors to find the missing coefficients that minimize the error of the approximation.</p>
<div></div>
<h3 id="_idParaDest-87"><a id="_idTextAnchor091"></a>
 Exercise 9: 1st, 2nd, and 3rd Degree Polynomial Regression</h3>
<p>Perform a 1st, 2nd, and 3rd degree polynomial regression on the following two datasets:</p>
<p class="snippet">Declare the two datasets</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet"># First dataset:</p>
<p class="snippet">x1 = np.array(range(1, 14))</p>
<p class="snippet">y1 = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
<p class="snippet"># Second dataset:</p>
<p class="snippet">x2 = np.array(range(1, 14))</p>
<p class="snippet">y2 = np.array([0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144])</p>
<p>Then plot your results on the graph:</p>
<p>Let's start with plotting the first example:</p>
<p class="snippet">import matplotlib.pyplot as plot</p>
<p class="snippet">deg1 = np.polyfit(x1, y1, 1)</p>
<p class="snippet"># array([ 4.85714286, -2.76923077])</p>
<p class="snippet">f1 = np.poly1d(deg1)</p>
<p class="snippet">deg2 = np.polyfit(x1, y1, 2)</p>
<p class="snippet"># array([-0.03196803, 5.3046953 , -3.88811189])</p>
<p class="snippet">f2 = np.poly1d(deg2)</p>
<p class="snippet">deg3 = np.polyfit(x1, y1, 3)</p>
<p class="snippet"># array([-0.01136364, 0.20666833, 3.91833167, -1.97902098])</p>
<p class="snippet">f3 = np.poly1d(deg3)</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    x1, y1, 'o',</p>
<p class="snippet">    x1, f1(x1),</p>
<p class="snippet">    x1, f2(x1),</p>
<p class="snippet">    x1, f3(x1)</p>
<p class="snippet"> )</p>
<p class="snippet _idGenParaOverride-1">plot.show()</p>
<div></div>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer060" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00038.jpg" alt="" />
</div>
</div>
<h6>Figure 3.19: Graph showing the first dataset with linear curves</h6>
<p>As the coefficients are enumerated from left to right in order of decreasing degree, we can see that the higher degree coefficients stay close to negligible. In other words, the three curves are almost on top of each other, and we can only detect a divergence near the right edge. This is because we are working on a dataset that can be very well approximated with a linear model.</p>
<p>In fact, the first dataset was created out of a linear function. Any nonzero coefficients for x**2 and x**3 are the results of overfitting the model based on the available data. The linear model is better for predicting values outside the range of the training data than any higher degree polynomial.</p>
<p>Let's contrast this behavior with the second example. We know that the Fibonacci sequence is non-linear. So, using a linear equation to approximate it is a clear case for underfitting. Here, we expect a higher degree polynomic to perform better:</p>
<p class="snippet">deg1 = np.polyfit(x2, y2, 1)</p>
<p class="snippet"># array([ 9.12087912, -34.92307692])</p>
<p class="snippet">f1 = np.poly1d(deg1)</p>
<p class="snippet">deg2 = np.polyfit(x2, y2, 2)</p>
<p class="snippet"># array([ 1.75024975, -15.38261738, 26.33566434])</p>
<p class="snippet">f2 = np.poly1d(deg2)</p>
<p class="snippet">deg3 = np.polyfit(x2, y2, 3)</p>
<p class="snippet"># array([0.2465035, -3.42632368, 14.69080919,</p>
<p class="snippet"># -15.07692308])</p>
<p class="snippet">f3 = np.poly1d(deg3)</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    x2, y1, 'o',# blue dots</p>
<p class="snippet">    x2, f1(x2), # orange</p>
<p class="snippet">    x2, f2(x2), # green</p>
<p class="snippet">    x2, f3(x2) # red</p>
<p class="snippet">)</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer061" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00039.jpg" alt="Figure 3.19: Graph showing second dataset points and three polynomial curves " />
</div>
</div>
<h6>Figure 3.20: Graph showing second dataset points and three polynomial curves</h6>
<p class="_idGenParaOverride-1">The difference is clear. The quadratic curve fits the points a lot better than the linear one. The cubic curve is even better.</p>
<div></div>
<p>If you research Binet's formula, you will find out that the Fibonacci function is an exponential function, as the xth Fibonacci number is calculated as the xth power of a constant. Therefore, the higher degree polynomial we use, the more accurate our approximation will be.</p>
<h3 id="_idParaDest-88"><a id="_idTextAnchor092"></a>
 Polynomial Regression with Multiple Variables</h3>
<p>When we have one variable of degree n, we have n+1 coefficients in the equation:</p>
<p class="snippet">y = b + a1*x + a2*(x ** 2) + a3*(x ** 3) + … + an * (x ** n)</p>
<p>Once we deal with multiple features x1, x2, …, xm, and their powers of up to the nth degree, we get an m * (n+1) matrix of coefficients. The math will become quite lengthy once we start exploring the details and prove how a polynomial model works. We will also lose the nice visualizations of two-dimensional curves.</p>
<p>Therefore, we will apply the chapters learned in previous section on polynomial regression with one variable and omit the math. When training and testing a Linear Regression model, we can calculate the mean square error to see how good an approximation a model is.</p>
<p>In scikit-learn, the degree of the polynomials used in the approximation is a simple parameter in the model.</p>
<p>As polynomial regression is a form of linear regression, we can perform polynomial regression without changing the regression model. All we need to do is transform the input and keep the linear regression model.</p>
<p>The transformation of the input is performed by the <strong class="inline _idGenCharOverride-3">fit_transform</strong>
 method of the <strong class="inline _idGenCharOverride-3">PolynomialFeatures</strong>
 package:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">import quandl</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet">from sklearn.preprocessing import PolynomialFeatures</p>
<p class="snippet">data_frame = quandl.get("YALE/SPCOMP")</p>
<p class="snippet">data_frame.fillna(-100, inplace=True)</p>
<p class="snippet"># We shift the price data to be predicted 20 years forward</p>
<p class="snippet">data_frame['Real Price Label'] = data_frame['Real Price'].shift(-240)</p>
<p class="snippet"># Then exclude the label column from the features</p>
<p class="snippet">features = np.array(data_frame.drop('Real Price Label', 1))</p>
<p class="snippet"># We scale before dropping the last 240 rows from the features</p>
<p class="snippet">scaled_features = preprocessing.scale(features)</p>
<p class="snippet"># Save the last 240 rows before dropping them</p>
<p class="snippet">scaled_features_latest_240 = scaled_features[-240:]</p>
<p class="snippet"># Exclude the last 240 rows from the data used for model building</p>
<p class="snippet">scaled_features = scaled_features[:-240]</p>
<p class="snippet"># Now we can drop the last 240 rows from the data frame</p>
<p class="snippet">data_frame.dropna(inplace=True)</p>
<p class="snippet"># Then build the labels from the remaining data</p>
<p class="snippet">label = np.array(data_frame['Real Price Label'])</p>
<p class="snippet"># Create a polynomial regressor model and fit it to the training data</p>
<p class="snippet">poly_regressor = PolynomialFeatures(degree=3)</p>
<p class="snippet">poly_scaled_features = poly_regressor.fit_transform(scaled_features)</p>
<p class="snippet"># Split to training and testing data</p>
<p class="snippet _idGenParaOverride-1">(</p>
<div></div>
<p class="snippet">    poly_features_train, poly_features_test,</p>
<p class="snippet">    poly_label_train, poly_label_test</p>
<p class="snippet">) = model_selection.train_test_split(</p>
<p class="snippet">    poly_scaled_ features,</p>
<p class="snippet">    label, test_size=0.1</p>
<p class="snippet">)</p>
<p class="snippet"># Apply linear regression</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(poly_features_train, poly_label_train)</p>
<p class="snippet"># Model score</p>
<p class="snippet">print('Score: ', model.score(poly_features_test, poly_label_test))</p>
<p class="snippet"># Prediction</p>
<p class="snippet">poly_label_predicted = model.predict(poly_features_test)</p>
<p class="snippet">plot.scatter(poly_label_test, poly_label_predicted)</p>
<p>The model scores too well. Chances are, the polynomial model is overfitting the dataset.</p>
<p>There is another model in scikit-learn that performs polynomial regression called the SVM model, which stands for Support Vector Machines.</p>
<h3 id="_idParaDest-89"><a id="_idTextAnchor093"></a>
 Support Vector Regression</h3>
<p>Support Vector Machines are binary classifiers defined on a vector space. Vector Machines divide the state space with a surface. An SVM classifier takes classified data and tries to predict where unclassified data belongs. Once the classification of a data point is determined, it gets labeled.</p>
<p class="_idGenParaOverride-1">Support Vector Machines can also be used for regression. Instead of labeling data, we can predict future values in a series. The Support Vector Regression model uses the space between our data as a margin of error. Based on the margin of error, it makes predictions regarding future values.</p>
<div></div>
<p>If the margin of error is too small, we risk overfitting the existing dataset. If the margin of error is too big, we risk underfitting the existing dataset.</p>
<p>A kernel describes the surface dividing the state space in the case of a classifier. A kernel is also used to measure the margin of error in the case of a regressor. This kernel can use a linear model, a polynomial model, or many other possible models. The default kernel is <strong class="bold _idGenCharOverride-1">RBF</strong>
 , which stands for <strong class="bold _idGenCharOverride-1">Radial Basis Function</strong>
 .</p>
<p>Support Vector Regression is an advanced topic, which is outside the scope of this book. Therefore, we will only stick to a walkthrough of how easy it is to try out another regression model on our test data.</p>
<p>Suppose we have our features and labels in two separate NumPy arrays. Let's recall how we performed linear regression on them:</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">(features_train, features_test, label_train,</p>
<p class="snippet"> label_test) = model_selection.train_test_split(scaled_features, label, test_size=0.1)</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p>We can perform regression with Support Vector Machines by changing the linear model to a support vector model:</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import svm</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet"># The rest of the model building stays the same</p>
<p class="snippet">(features_train, features_test, label_train,</p>
<p class="snippet"> label_test) = model_selection.train_test_split(scaled_features, label, test_size=0.1)</p>
<p class="snippet">model = svm.SVR()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p class="snippet">label_predicted = model.predict(features_test)</p>
<p class="snippet">print('Score: ', model.score(features_test, label_test))</p>
<p class="snippet">plot.plot(label_test, label_predicted, 'o')</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer062" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00040.jpg" alt="" />
</div>
</div>
<h6>Figure 3.21: Graph showing Support Vector regression with a linear model</h6>
<p>The output is as follows:</p>
<p class="snippet"> -0.19365084431020874</p>
<p>The model score is quite low, and the points don't align on the <strong class="inline _idGenCharOverride-3">y=x</strong>
 line. Prediction with the default values is quite low.</p>
<p>The output of the model describes the parameters of the Support Vector Machine:</p>
<p class="snippet">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)</p>
<p class="_idGenParaOverride-1">We could fiddle with these parameters to increase the accuracy of the prediction by creating a better algorithm.</p>
<div></div>
<h3 id="_idParaDest-90"><a id="_idTextAnchor094"></a>
 Support Vector Machines with a 3 Degree Polynomial Kernel</h3>
<p>Let's switch the kernel of the Support Vector Machine to poly. The default degree of the polynomial is 3:</p>
<p class="snippet">model = svm.SVR(kernel='poly')</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p class="snippet">label_predicted = model.predict(features_test)</p>
<p class="snippet">plot.plot(label_test, label_predicted, 'o')</p>
<p>The output is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer063" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00041.jpg" alt="" />
</div>
</div>
<h6>Figure 3.22: Graph showing Support Vector regression with a polynomial kernel of degree 3</h6>
<p class="snippet">model.score(features_test, label_test)</p>
<p>The output is as follows:</p>
<p class="snippet">0.06388628722032952</p>
<p class="_idGenParaOverride-1">With Support Vector Machines, we often end up with points concentrated in small areas. We could change the margin of error to separate the points a bit more.</p>
<div></div>
<h3 id="_idParaDest-91"><a id="_idTextAnchor095"></a>
 Activity 6: Stock Price Prediction with Quadratic and Cubic Linear Polynomial Regression with Multiple Variables</h3>
<p>In this section, we will discuss how to perform linear, polynomial, and support vector regression with scikit-learn. We will also learn how to find the best fit model for a given task. We will be assuming that you are a software engineer at a financial institution and your employer wants to know whether linear regression or support vector regression is a better fit for predicting stock prices. You will have to load all of the data of the S&amp;P 500 from a data source. Then, you will need to build a regressor using linear regression, cubic polynomial linear regression, and a support vector regression with a polynomial kernel of degree 3 before separating the training and test data. Plot the test labels and the prediction results and compare them with the y=x line. Finally, compare how well the three models score.</p>
<ol>
<li class="ParaOverride-1" value="1">Load the S&amp;P 500 index data using Quandl, and then prepare the data for prediction.</li>
<li value="2">Use a polynomial of degree 1 for the evaluation of the model and for the prediction.<p class="_idGenParaOverride-4">The closer the dots are to the y=x line, the less error the model works with.</p>
<p class="_idGenParaOverride-4">Perform a linear multiple regression with quadratic polynomials. The only change is in the Linear Regression model.</p>
</li>
<li value="3">Perform Support Vector regression with a polynomial kernel of degree 3.</li>
</ol>
<p>The model does not look efficient at all. For some reason, this model clearly prefers lower values for the S&amp;P 500 that are completely unrealistic, assuming that the stock market does not lose 80% of its value within a day.</p>
<h4>Note</h4>
<p class="callout">The solution to this activity is available at page 271.</p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor096"></a>
 Summary</h2>
<p>In this chapter, we have learned the fundamentals of Linear Regression.</p>
<p>After going through some basic mathematics, we dived into the mathematics of linear regression using one variable and multiple variables.</p>
<p>Challenges occurring with regression include loading data from external sources such as a .csv file, Yahoo Finance, or Quandl were dealt with. After loading the data, we learned how to identify the features and labels, how to scale data, and how to format data to perform regression.</p>
<p>We learned how to train and test a linear regression engine, and how to predict the future. Our results were visualized by an easy-to-use Python graph plotting library called <strong class="inline _idGenCharOverride-3">pyplot</strong>
 .</p>
<p>A more complex form of linear regression is a linear polynomial regression of arbitrary degree. We learned how to define these regression problems on multiple variables. We compared their performance to each other on stock price prediction problems. As an alternative to polynomial regression, we also introduced Support Vector Machines as a regression model and experimented with two kernels.</p>
<p>You will soon learn about another field inside machine learning. The setup and code structure of this machine learning method will be very similar to regression, while the problem domain is somewhat different. In the next chapter, you will learn the ins and outs of classification.</p>
</div>
</body></html>