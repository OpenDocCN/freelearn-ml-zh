<html><head></head><body>
		<div id="_idContainer004">
			<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Preface</h1>
			<p>Hello and welcome! <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) enables computers to learn from data using algorithms to make informed decisions, automate tasks, and extract valuable insights. One particular aspect that often garners attention is imbalanced data, where certain classes may have considerably fewer samples <span class="No-Break">than others.</span></p>
			<p>This book provides an in-depth guide to understanding and navigating the intricacies of skewed data. You will gain insights into best practices for managing imbalanced datasets in <span class="No-Break">ML contexts.</span></p>
			<p><strong class="bold">While imbalanced data can present challenges, it’s important to understand that the techniques to address this imbalance are not universally applicable. Their relevance and necessity depend on various factors such as the domain, the data distribution, the performance metrics you’re optimizing, and the business objectives. Before adopting any techniques, it’s essential to establish a baseline. Even if you don’t currently face issues with imbalanced data, it can be beneficial to be aware of the challenges and solutions discussed in this book. Familiarizing yourself with these techniques will provide you with a comprehensive toolkit, preparing you for scenarios that you may not yet know you’ll encounter. If you do find that model performance is lacking, especially for underrepresented (minority) classes, the insights and strategies covered in the book can be instrumental in guiding </strong><span class="No-Break"><strong class="bold">effective improvements.</strong></span></p>
			<p>As the domains of ML and artificial intelligence continue to grow, there will be an increasing demand for professionals who can adeptly handle various data challenges, including imbalance. This book aims to equip you with the knowledge and tools to be one of those <span class="No-Break">sought-after experts.</span></p>
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Who this book is for</h1>
			<p>This comprehensive book is thoughtfully tailored to meet the needs of a variety of professionals, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">ML researchers, ML scientists, ML engineers, and students</strong>: Professionals and learners in the fields of ML and deep learning who seek to gain valuable insights and practical knowledge for tackling the challenges posed by data imbalance</li>
				<li><strong class="bold">Data scientists and analysts</strong>: Experienced data experts eager to expand their knowledge of handling skewed data with practical, real-world solutions</li>
				<li><strong class="bold">Software engineers</strong>: Software engineers who want to effectively integrate ML and deep learning solutions into their applications when dealing with imbalanced data</li>
				<li><strong class="bold">Practical insight seekers</strong>: Professionals and enthusiasts from various backgrounds who want to use hands-on, industry-relevant approaches for efficiently dealing with data imbalance in ML and deep learning, enabling them to excel in their respective roles</li>
			</ul>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>What this book covers</h1>
			<p><a href="B17259_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Data Imbalance in Machine Learning</em>, serves as an exploration of data imbalance within the context of ML. This chapter elucidates the nature of imbalanced data, distinguishing it from other dataset types. It also provides a comprehensive introduction to the essential components of ML and model performance metrics most relevant for cases when there is a data imbalance. The chapter looks into the issues and concerns involved in dealing with imbalanced data, explaining when it can occur and why it can sometimes be a challenge. More importantly, we will go over when not to worry about data imbalance at all or when it may not be worth worrying about. Furthermore, it introduces the <strong class="source-inline">imbalanced-learn</strong> library, offering invaluable insights and general guidelines to navigate the intricacies of dealing with imbalanced <span class="No-Break">datasets effectively.</span></p>
			<p><a href="B17259_02.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Oversampling Methods</em>, introduces the concept of oversampling, outlining when to employ it and when not to, and various techniques to augment imbalanced datasets. It guides you through the practical application of these techniques using the <strong class="source-inline">imbalanced-learn</strong> library and compares their performance across classical ML models. Practical advice on the effectiveness of these techniques in real-world scenarios concludes <span class="No-Break">the chapter.</span></p>
			<p><a href="B17259_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Undersampling Methods</em>, presents the concept of undersampling as an effective approach for data balancing when standard oversampling isn’t an option. This chapter covers strategies to effectively remove examples from imbalanced data, different ways of addressing noisy observations, and procedures for handling easily categorized instances. We will also discuss when to avoid undersampling of the <span class="No-Break">majority class.</span></p>
			<p><a href="B17259_04.xhtml#_idTextAnchor120"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ensemble Methods</em>, explores the application of ensemble techniques, including bagging and boosting, to enhance the performance of ML models. Moreover, it tackles the challenge of imbalanced datasets, where traditional ensemble methods may be ineffective, by combining the ensemble methods with the techniques introduced in <span class="No-Break">previous chapters.</span></p>
			<p><a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, explores some alternatives to sampling techniques, including oversampling and undersampling. This chapter highlights the significance of cost-sensitive learning as an effective strategy to overcome the problem of imbalanced datasets. We also discuss <strong class="bold">threshold-tuning techniques</strong>, which can be very relevant in the context of <span class="No-Break">data imbalance.</span></p>
			<p><a href="B17259_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Data Imbalance in Deep Learning</em>, presents the core concepts of deep learning and walks through the issues posed by imbalanced datasets. You will investigate typical types of imbalanced data challenges in various deep learning applications and develop an understanding of <span class="No-Break">their impact.</span></p>
			<p><a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep Learning Methods</em>, marks a transition from classical ML to deep learning, exploring the adaptation of familiar data-level sampling techniques and unveiling opportunities for enhancing these methods in the context of deep learning models. It dives into combining deep learning with oversampling and undersampling techniques, covering dynamic sampling and data augmentation for images and text. It emphasizes the fundamental differences between deep learning and classical ML, particularly the nature of the data they handle, whereas deep learning deals with unstructured data such as images, text, audio, and video. The chapter also explores techniques to address class imbalance in computer vision and their applicability to <strong class="bold">Natural Language Processing</strong> (<span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">) problems.</span></p>
			<p><a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep Learning Techniques</em>, expands on the concepts from <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, and applies them to deep learning models. We adapt deep learning models through loss function modifications using the PyTorch deep learning framework, ultimately enhancing model performance and enabling more <span class="No-Break">effective predictions.</span></p>
			<p><a href="B17259_09.xhtml#_idTextAnchor256"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Hybrid Deep Learning Methods</em>, explores innovative techniques that bridge the gap between data-level and algorithm-level methods from the previous two chapters. This chapter introduces the concept of graph ML and employs a real-world Facebook social network dataset to provide valuable insights and practical applications for addressing data imbalance in deep learning. We will also introduce the concept of hard mining loss and build upon it to explore a specialized technique called <strong class="bold">minority class incremental rectification</strong>, which combines hard mining with <span class="No-Break">cross-entropy loss.</span></p>
			<p><a href="B17259_10.xhtml#_idTextAnchor279"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Model Calibration</em>, takes a different angle of addressing data imbalance. Rather than focusing on data preprocessing or model building, this chapter highlights the post-processing of prediction scores obtained from trained models. Such post-processing can be valuable for both real-time predictions and offline model evaluation. The chapter offers insights into measuring the calibration of a model and explains why this aspect can be indispensable when dealing with imbalanced data. This is particularly important since data balancing techniques can often lead to <span class="No-Break">model miscalibration.</span></p>
			<p><em class="italic">Appendix</em>, <em class="italic">Machine Learning Pipeline in Production</em>, offers a foundational guide to constructing ML pipelines in production environments that encounter imbalanced data. This appendix provides a brief roadmap, going over the sequence and stage at which techniques for addressing data imbalance should <span class="No-Break">be integrated.</span></p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>📌 Usage of techniques – In production tips</h1>
			<p>Throughout this book, you will come across “In production” tip boxes like the following one, highlighting real-world applications of the <span class="No-Break">techniques discussed:</span></p>
			<p class="callout-heading">🚀 Class reweighting in production at OpenAI</p>
			<p class="callout">OpenAI was trying to solve the problem of bias in training data of the image generation model DALL-E 2 [1]. DALL-E 2 is trained on a massive dataset of images from the internet, which can contain biases. For example, the dataset may contain more images of men than women or more images of people from certain racial or ethnic groups than others.</p>
			<p>These snippets offer insights into how well-known companies grappled with data imbalance and what strategies they adopted to effectively navigate these challenges. For instance, the tip on OpenAI’s approach with DALL-E 2 sheds light on the intricate balance between filtering training data and inadvertently amplifying biases. Such examples underscore the importance of being both strategic and cautious when dealing with imbalanced data. To delve deeper into the specifics and understand the nitty-gritty of these implementations, you are encouraged to follow the company blog or paper links provided. These insights can provide a clearer understanding of how to adapt and apply techniques in varied real-world <span class="No-Break">scenarios effectively.</span></p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>To get the most out of this book</h1>
			<p>This book assumes some foundational knowledge of ML, deep learning, and Python programming. Some basic working knowledge of <strong class="source-inline">scikit-learn</strong> and <strong class="source-inline">PyTorch</strong> can be helpful, although they can be learned on <span class="No-Break">the go.</span></p>
			<table id="table001" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Software/hardware covered in </strong><span class="No-Break"><strong class="bold">the book</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Operating </strong><span class="No-Break"><strong class="bold">system requirements</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Google Colab</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Windows, macOS, <span class="No-Break">or Linux</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>For the software requirements, you have two options to execute the code provided in this book. You can choose to either run the code within Google Colab online at <a href="https://colab.research.google.com/">https://colab.research.google.com/</a> or download the code to your local computer and execute it there. Google Colab provides a hassle-free option as it comes with all the necessary libraries pre-installed, so you don’t need to install anything on your local machine. All you need is a web browser to access Google Colab and a Google account. If you prefer to work locally, ensure that you have Python (3.6 or higher) installed, as well as the specified libraries such as PyTorch, torchvision, NumPy, and scikit-learn. A list of required libraries can be found in the GitHub repository of the book. These libraries are compatible with Windows, macOS, and Linux operating systems. A modern GPU can speed up the code execution for the deep learning chapters that appear later in the book; however, it’s <span class="No-Break">not mandatory.</span></p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting </strong><span class="No-Break"><strong class="bold">of code.</strong></span></p>
			<p>Regarding references, we use numbered references such as “[6],” where you can go to the <em class="italic">References</em> section at the end of that chapter and download the corresponding reference (paper/blog/article) either using the link (if mentioned) or searching for that reference on Google <span class="No-Break">Scholar (</span><a href="https://scholar.google.com/"><span class="No-Break">https://scholar.google.com/</span></a><span class="No-Break">).</span></p>
			<p>At the conclusion of each chapter, you will find a set of questions designed to test your comprehension of the material covered. We strongly encourage you to engage with these questions to reinforce your learning. Solutions or answers to selected questions can be found in Assessments towards the end of <span class="No-Break">this book.</span></p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Download the example code files</h1>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data</a>. If there’s an update to the code, it will be updated in the <span class="No-Break">GitHub repository.</span></p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check <span class="No-Break">them out!</span></p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout <span class="No-Break">this book.</span></p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “Since it’s possible to provide a base estimator to <strong class="source-inline">BaggingClassifier</strong>, let’s use <strong class="source-inline">DecisionTreeClassifier</strong> with the maximum depth of the trees <span class="No-Break">being </span><span class="No-Break"><strong class="source-inline">6</strong></span><span class="No-Break">.”</span></p>
			<p>A block of code is set <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from collections import Counter X, y = make_data(sep=2)print(y.value_counts()) sns.scatterplot(data=X, x="feature_1", y="feature_2")plt.title('Separation: {}'.format(separation))plt.show()</pre>			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: “<strong class="bold">True Negative Rate</strong> (<strong class="bold">TNR</strong>): TNR measures the proportion of actual negatives that are correctly identified <span class="No-Break">as such.”</span></p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Get in touch</h1>
			<p>Feedback from our readers is <span class="No-Break">always welcome.</span></p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a> and mention the book title in the subject of <span class="No-Break">your message.</span></p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill in <span class="No-Break">the form.</span></p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to <span class="No-Break">the material.</span></p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please <span class="No-Break">visit </span><a href="http://authors.packtpub.com"><span class="No-Break">authors.packtpub.com</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Share Your Thoughts</h1>
			<p>Once you’ve read <em class="italic">Machine Learning for Imbalanced Data</em>, we’d love to hear your thoughts! Please <a href="https://www.packtpub.com/">click here to go straight to the Amazon review page</a> for this book and share <span class="No-Break">your feedback.</span></p>
			<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent <span class="No-Break">quality content.</span></p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Download a free PDF copy of this book</h1>
			<p>Thanks for purchasing <span class="No-Break">this book!</span></p>
			<p>Do you like to read on the go but are unable to carry your print <span class="No-Break">books everywhere?</span></p>
			<p>Is your eBook purchase not compatible with the device of <span class="No-Break">your choice?</span></p>
			<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at <span class="No-Break">no cost.</span></p>
			<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application. </p>
			<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your <span class="No-Break">inbox daily</span></p>
			<p>Follow these simple steps to get <span class="No-Break">the benefits:</span></p>
			<ol>
				<li>Scan the QR code or visit the <span class="No-Break">link below</span></li>
			</ol>
			<div>
				<div id="_idContainer003" class="IMG---Figure">
					<img src="image/B17259_QR_Free_PDF.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/9781801070836">https://packt.link/free-ebook/9781801070836</a></p>
			<ol>
				<li value="2">Submit your proof <span class="No-Break">of purchase</span></li>
				<li>That’s it! We’ll send your free PDF and other benefits to your <span class="No-Break">email directly</span></li>
			</ol>
		</div>
	</body></html>