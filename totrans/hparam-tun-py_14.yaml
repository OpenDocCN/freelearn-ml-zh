- en: '*Chapter 11*: Understanding the Hyperparameters of Popular Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*: 理解流行算法的超参数'
- en: Most **machine learning** (**ML**) algorithms have their own hyperparameters.
    Knowing how to implement a lot of fancy hyperparameter tuning methods without
    understanding the hyperparameters of the model is the same as a doctor writing
    a prescription before diagnosing the patient.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数**机器学习**（**ML**）算法都有自己的超参数。在不了解模型超参数的情况下，知道如何实施许多花哨的超参数调整方法，就像医生在诊断病人之前就开处方一样。
- en: In this chapter, we’ll learn about the hyperparameters of several popular ML
    algorithms. There will be a broad explanation for each of the algorithms, including
    (but not limited to) the definition of each hyperparameter, what will be impacted
    when the value of each hyperparameter is changed, and the priority list of hyperparameters
    based on the impact.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习几种流行机器学习算法的超参数。对于每个算法，都会有广泛的解释，包括但不限于每个超参数的定义、当每个超参数的值发生变化时将产生什么影响，以及基于影响的超参数优先级列表。
- en: By the end of this chapter, you will understand the important hyperparameters
    of several popular ML algorithms. Understanding the hyperparameters of ML algorithms
    is crucial since not all hyperparameters are equally significant when it comes
    to impacting the model’s performance. We do not have to perform hyperparameter
    tuning on all of the hyperparameters of a model; we just need to focus on the
    more critical hyperparameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解几种流行机器学习算法的重要超参数。理解机器学习算法的超参数至关重要，因为并非所有超参数在影响模型性能时都同等重要。我们不必对模型的所有超参数进行超参数调整；我们只需要关注更关键的超参数。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Exploring Random Forest hyperparameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索随机森林超参数
- en: Exploring XGBoost hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索XGBoost超参数
- en: Exploring LightGBM hyperparameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LightGBM超参数
- en: Exploring CatBoost hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索CatBoost超参数
- en: Exploring SVM hyperparameters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索SVM超参数
- en: Exploring artificial neural network hyperparameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索人工神经网络超参数
- en: Exploring Random Forest hyperparameters
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索随机森林超参数
- en: '**Random Forest** is a tree-based model that is built using a collection of
    **decision trees**. It is a very powerful ensemble ML model that can be utilized
    for both classification and regression tasks. The way Random Forest utilizes the
    collection of decision trees is by performing an ensemble method called **bootstrap
    aggregation** (**bagging**) with some modifications. To understand how each of
    the Random Forest’s hyperparameters can impact the model’s performance, we need
    to understand how the model works in the first place.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是一种基于树的模型，它使用一系列**决策树**构建而成。它是一个非常强大的集成机器学习模型，可以用于分类和回归任务。随机森林利用决策树集合的方式是通过执行一种称为**自助聚集**（**bagging**）的集成方法，并进行一些修改。为了理解随机森林的每个超参数如何影响模型性能，我们首先需要了解模型是如何工作的。'
- en: Before discussing how Random Forest ensembles a collection of decision trees,
    let’s discuss how a decision tree works at a high level. A decision tree can be
    utilized to perform a classification or regression task by constructing a series
    of decisions (in the form of rules and splitting points) that can be visualized
    in the form of a tree. These decisions are made by looking through all of the
    features and the feature values of the given training data. The goal of a decision
    tree is to have high homogeneity for each of the leaf nodes. Several methods can
    be used to measure homogeneity; the two most popular methods for classification
    tasks are to calculate the **Gini** or **Entropy** values, while the most popular
    method for regression tasks is to calculate the **Mean Squared Error** value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论随机森林如何集成一系列决策树之前，让我们先讨论一下决策树在高级别是如何工作的。决策树可以通过构建一系列决策（以规则和分割点形式）来执行分类或回归任务，这些决策可以以树的形式可视化。这些决策是通过查看给定训练数据中的所有特征和特征值来做出的。决策树的目标是使每个叶节点具有高度的纯度。可以使用几种方法来衡量纯度；对于分类任务，最流行的方法是计算**基尼**或**熵**值，而对于回归任务，最流行的方法是计算**均方误差**值。
- en: Random Forest utilizes the bagging method to ensemble the collection of decision
    trees. Bagging is an ensemble method that works by combining predictions from
    multiple ML models with the hope of generating a more accurate and robust prediction.
    In this case, Random Forest combines the prediction outputs from several decision
    trees so that we are not too focused on the prediction from a single tree. This
    is because a decision tree is very likely to overfit the training data. However,
    Random Forest does not just utilize the vanilla bagging ensemble method – it also
    ensures that it only utilizes prediction outputs from the collection of decision
    trees that are not highly correlated with each other. How is Random Forest able
    to do that? Instead of asking each decision tree to look through all the features
    and their values when choosing the splitting points, Random Forest customizes
    this procedure so that each decision tree only looks at a random sample of features.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林利用袋装法来集成决策树集合。袋装法是一种集成方法，通过结合多个机器学习模型的预测，希望生成更准确和鲁棒的预测。在这种情况下，随机森林结合了多个决策树的预测输出，这样我们就不太关注单个树的预测。这是因为决策树很可能对训练数据进行过拟合。然而，随机森林不仅仅利用了传统的袋装集成方法，它还确保只利用那些彼此之间高度不相关的决策树集合的预测输出。随机森林是如何做到这一点的呢？它不是要求每个决策树在选择分割点时查看所有特征及其值，而是随机森林定制了这个过程，使得每个决策树只查看特征的一个随机样本。
- en: 'The most popular and well-maintained implementation of Random Forest in Python
    can be found in the scikit-learn package. It includes implementations for both
    regression (`RandomForestRegressor`) and classification (`RandomForestClassifier`)
    tasks. Both implementations have very similar hyperparameters with only a few
    small differences. The following are the most important hyperparameters, starting
    with the most important to the least based on the impact on model performance.
    Note that this priority list is subjective, based on our experience of developing
    Random Forest models in the past:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，最受欢迎且维护得最好的随机森林实现可以在scikit-learn包中找到。它包括回归（`RandomForestRegressor`）和分类（`RandomForestClassifier`）任务的实现。这两个实现具有非常相似的超参数，只有少数小的差异。以下是最重要的超参数，按照对模型性能影响从大到小排序。请注意，这个优先级列表是主观的，基于我们过去开发随机森林模型的经验：
- en: '`n_estimators`: This specifies the number of decision trees to be utilized
    to build the Random Forest. In general, the larger the number of trees, the better
    the model’s performance will be, with a trade-off of having longer computational
    time. However, there is a threshold beyond which adding more trees will not have
    much additional impact on the model’s performance. It could even have a negative
    impact due to the problem of overfitting.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这指定了用于构建随机森林的决策树数量。一般来说，树的数量越多，模型的性能越好，但这也意味着计算时间会更长。然而，存在一个阈值，超过这个阈值添加更多的树对模型性能的提升影响不大。甚至可能由于过拟合问题而产生负面影响。'
- en: '`max_features`: This specifies the number of randomly sampled features that
    are used by Random Forest to choose the best splitting point in each of the decision
    trees. The higher the value, the lower the reduction in variance, and hence the
    lower the increase in bias. A higher value also leads to having a longer computational
    time. scikit-learn, by default, will use all of the features for regression tasks
    and use only `sqrt(n_features)` number of features for classification tasks.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_features`：这指定了随机森林用于在每个决策树中选择最佳分割点的随机采样特征的数量。值越高，方差减少越少，因此偏差增加越少。更高的值也会导致计算时间更长。scikit-learn默认情况下，对于回归任务将使用所有特征，而对于分类任务则只使用`sqrt(n_features)`数量的特征。'
- en: '`criterion`: This is used to measure the homogeneity of each decision tree.
    scikit-learn implemented several methods for both regression and classification
    tasks. There’s `squared_error`, `absolute_error`, and `poisson` for regression
    tasks, while there’s `gini`, `entropy`, and `log_loss` for classification tasks.
    Different methods will have different impacts on model performance; there is no
    clear rule of thumb for this hyperparameter.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`criterion`：这用于衡量每个决策树的同质性。scikit-learn为回归和分类任务实现了几种方法。对于回归任务有`squared_error`、`absolute_error`和`poisson`，而对于分类任务有`gini`、`entropy`和`log_loss`。不同的方法将对模型性能产生不同的影响；对于这个超参数没有明确的经验法则。'
- en: '`max_depth`: This specifies the maximum depth of each decision tree. The default
    value of this hyperparameter is `None`, meaning that the nodes of each tree will
    keep branching until we have pure leaf nodes or until all the leaves contain less
    than `min_samples_split` number of samples. The lower the value, the better, since
    this prevents overfitting. However, a value that is too low can lead to an underfitting
    problem. One thing is for sure – a higher value implies a longer computational
    time.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_depth`: 这指定了每个决策树的最大深度。此超参数的默认值为`None`，意味着每个树的节点将继续分支，直到我们得到纯叶节点或直到所有叶子节点包含的样本数少于`min_samples_split`。值越低越好，因为这可以防止过拟合。然而，一个过低的值可能导致欠拟合问题。有一点可以肯定——值越高意味着计算时间越长。'
- en: '`min_samples_split`: This specifies the minimum number of samples required
    for a tree to be able to further split an internal node (a node that can be split
    into child nodes). The higher the value, the easier it is to prevent overfitting.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`min_samples_split`: 这指定了树能够进一步分裂内部节点（可以分裂成子节点的节点）所需的最小样本数。值越高，越容易防止过拟合。'
- en: '`min_samples_leaf`: This specifies the minimum number of samples required in
    the leaf nodes. A higher value can help us prevent overfitting.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`: 这指定了叶节点中所需的最小样本数。较高的值可以帮助我们防止过拟合。'
- en: Random Forest Hyperparameters in scikit-learn
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的随机森林超参数
- en: For more information about each of the hyperparameters of the Random Forest
    implementation in scikit-learn, please visit the official documentation pages
    at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
    and [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解scikit-learn中随机森林实现的每个超参数的更多信息，请访问官方文档页面：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
    和 [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)。
- en: 'Other useful boilerplate parameters can be found across different scikit-learn
    estimator implementations. The following are several important parameters that
    you need to be aware of that can help you while training a scikit-learn estimator:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其他有用的模板参数可以在不同的scikit-learn估计器实现中找到。以下是一些您在训练scikit-learn估计器时需要了解的重要参数，这些参数可以帮助您：
- en: '`class_weight`: This specifies the weights for each class that exists in the
    training data. This is only available for classification tasks. This parameter
    is very important when you face an imbalanced class problem. We need to give higher
    weights to classes that have fewer samples.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`class_weight`: 这指定了训练数据中每个类别的权重。此参数仅适用于分类任务。当您面临类别不平衡问题时，此参数非常重要。我们需要给样本较少的类别赋予更高的权重。'
- en: '`n_jobs`: This specifies the number of parallel processes to be utilized when
    training the estimator. scikit-learn utilizes the `joblib` package in the backend.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_jobs`: 这指定了在训练估计器时使用的并行进程数。scikit-learn在后台使用`joblib`包。'
- en: '`random_state`: This specifies the random seed number to ensure the code is
    reproducible.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`random_state`: 这指定了随机种子数，以确保代码的可重复性。'
- en: '`verbose`: This parameter is used to control any logging activities. Setting
    `verbose` to an integer greater than zero enables us to see what happens when
    training an estimator.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`verbose`: 此参数用于控制任何日志活动。将`verbose`设置为大于零的整数可以让我们看到在训练估计器时发生了什么。'
- en: In this section, we learned how Random Forest works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    the model’s performance. We also looked at the main hyperparameters. Furthermore,
    we learned about several useful parameters in scikit-learn that can ease the training
    process. In the next section, we will discuss the XGBoost algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了随机森林在高级别上的工作原理，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还了解了主要超参数。此外，我们还了解了scikit-learn中的一些有用参数，这些参数可以简化训练过程。在下一节中，我们将讨论XGBoost算法。
- en: Exploring XGBoost hyperparameters
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索XGBoost超参数
- en: '**Extreme Gradient Boosting** (**XGBoost**) is also a tree-based model that
    is built using a collection of decision trees, similar to a Random Forest. It
    can also be utilized for both classification and regression tasks. The difference
    between XGBoost and Random Forest is in how they perform the ensemble. Unlike
    Random Forest, which uses the bagging ensemble method, XGBoost utilizes another
    ensemble method called **boosting**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**极端梯度提升**（**XGBoost**）也是一个基于树的模型，它通过一系列决策树的集合构建，类似于随机森林。它也可以用于分类和回归任务。XGBoost与随机森林之间的区别在于它们如何进行集成。与使用袋装集成方法的随机森林不同，XGBoost使用另一种称为**提升**的集成方法。'
- en: 'Boosting is an ensemble algorithm whose goal is to achieve higher performance
    through a sequence of individually weak models by overcoming the weaknesses of
    the predecessor models (see *Figure 11.1*). It is not a specific model; it’s just
    a generic ensemble algorithm. The definition of weakness may vary across different
    types of boosting ensemble implementation. In XGBoost, it is defined based on
    the error of the gradient from the previous decision tree model. Take a look at
    the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是一种集成算法，其目标是通过一系列单独的弱模型，通过克服先前模型的弱点（参见*图11.1*）来提高性能。它不是一个特定的模型；它只是一个通用的集成算法。弱度的定义可能因不同的提升集成实现类型而异。在XGBoost中，它是基于先前决策树模型的梯度误差来定义的。请看以下图表：
- en: '![Figure 11.1 – Boosting ensemble algorithm'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.1 – Boosting ensemble algorithm]'
- en: '](img/B18753_11_001.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_11_001.jpg]'
- en: Figure 11.1 – Boosting ensemble algorithm
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 提升集成算法
- en: XGBoost is a very popular and well-adopted ML model that is built using the
    boosting ensemble algorithm and a collection of decision trees. Each of the decision
    trees is added one at a time and is fitted to the prediction errors from the previous
    tree to correct those errors. It is worth noting that since XGBoost is part of
    the gradient boosting algorithm, all of the weak models (decision trees) need
    to be fitted using a differentiable loss function and the gradient descent optimization
    method.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一个非常流行且广泛采用的机器学习模型，它使用提升集成算法和一系列决策树构建。每个决策树都是逐个添加的，并拟合到前一个树的预测误差，以纠正这些误差。值得注意的是，由于XGBoost是梯度提升算法的一部分，所有弱模型（决策树）都需要使用可微分的损失函数和梯度下降优化方法进行拟合。
- en: 'XGBoost has its own package and can be utilized not only in Python but also
    in other programming languages, such as R and JVM. In Python, you can install
    XGBoost via `pip install xgboost`. This package also implements the scikit-learn
    wrappers for both regression (`XGBRegressor`) and classification (`XGBClassifier`)
    tasks. Numerous hyperparameters are provided by the package, but not all of them
    are very important in affecting the model’s performance. The following are the
    most important hyperparameters, starting with the most important to the least
    based on their impact on model performance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost有自己的包，不仅可以在Python中使用，还可以在其他编程语言中使用，如R和JVM。在Python中，您可以通过`pip install
    xgboost`安装XGBoost。此包还实现了scikit-learn包装器，用于回归（`XGBRegressor`）和分类（`XGBClassifier`）任务。该包提供了许多超参数，但并非所有参数都对模型性能有重大影响。以下是最重要的超参数，按其对模型性能影响的重要性从高到低排序：
- en: '`n_estimators`: This specifies the number of decision trees to be utilized
    to build the XGBoost model. It can also be interpreted as the number of boosting
    rounds, which is similar to the concept of epochs in a neural network. In general,
    the higher the value, the better the model’s performance will be, with the trade-off
    of having a longer computation time. However, we need to be careful with a value
    that’s too high since it can lead us to the overfitting problem.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这指定了用于构建XGBoost模型要使用的决策树数量。它也可以解释为提升轮数，这与神经网络中的epoch概念相似。一般来说，值越高，模型的性能越好，但代价是计算时间会更长。然而，我们需要小心过高的值，因为它可能导致过拟合问题。'
- en: '`learning_rate`: This is the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding the
    optimum solution, with a trade-off of having a longer computational time. You
    can increase the value of this hyperparameter if there no sign of overfitting
    is found on the last iterations of training; you can decrease it if there is overfitting.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate`：这是梯度下降优化算法的学习率。值越低，模型找到最优解的机会越高，但代价是计算时间更长。如果你在训练的最后几次迭代中没有发现过拟合的迹象，你可以增加这个超参数的值；如果有过拟合，你可以减少它。'
- en: '`max_depth`: This is the maximum depth of each decision tree. A lower value
    can help us prevent overfitting. However, a too-low value can lead to an underfitting
    problem. One thing is for sure – a higher value leads to a longer computational
    time.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_depth`：这是每个决策树的最大深度。较低的值可以帮助我们防止过拟合。然而，过低的值可能导致欠拟合问题。有一点可以肯定——较高的值会导致更长的计算时间。'
- en: '`min_child_weight`: This is the minimum sum of instance weight, calculated
    using the Hessian, that’s needed in a child. This hyperparameter acts as a regularizer
    to ensure that each tree will stop trying to split the node once a certain degree
    of purity is reached. In other words, it is a regularization parameter that works
    by limiting the depth of the tree so that the overfitting problem can be prevented.
    A higher value can help us prevent overfitting. However, a too-high value can
    lead to an underfitting problem.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`min_child_weight`：这是在子节点中需要的、使用 Hessian 计算的实例权重最小总和。这个超参数作为正则化器，确保每个树在达到一定程度的纯度后停止尝试分裂节点。换句话说，它是一个通过限制树深度来防止过拟合的正则化参数。较高的值可以帮助我们防止过拟合。然而，过高的值可能导致欠拟合问题。'
- en: '`gamma`: This is a pseudo-regularization parameter that is calculated based
    on a reduction in the loss value. The value of this hyperparameter specifies the
    minimum loss reduction required to make a further partition on a leaf node of
    the tree. You can put a high value on this hyperparameter to prevent the overfitting
    problem. However, please be careful and don’t use a value that’s too high; it
    can lead to an underfitting problem.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gamma`：这是一个基于损失值减少的伪正则化参数。这个超参数的值指定了在树的叶子节点上进行进一步划分所需的最小损失减少量。你可以给这个超参数一个较高的值来防止过拟合问题。然而，请小心，不要使用过高的值；它可能导致欠拟合问题。'
- en: '`colsample_bytree`: This is the fraction version of the `max_features` hyperparameter
    in the scikit-learn implementation of Random Forest. This hyperparameter is responsible
    for telling XGBoost how many randomly sampled features are needed to choose the
    best splitting point in each of the decision trees. A low value can help us prevent
    overfitting and lowers the computational time. However, a value that’s too low
    can lead to an underfitting problem.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：这是 scikit-learn 实现的随机森林中 `max_features` 超参数的分数版本。这个超参数负责告诉
    XGBoost 在每个决策树中选择最佳分裂点需要多少随机采样的特征。较低的值可以帮助我们防止过拟合并降低计算时间。然而，过低的值可能导致欠拟合问题。'
- en: '`subsample`: This is the observation’s version of the `colsample_bytree` hyperparameter.
    This hyperparameter is responsible for telling XGBoost how many training samples
    need to be used while training each tree. This hyperparameter can be useful to
    prevent the overfitting problem. However, it can also lead us to an underfitting
    problem if we use a value that’s too low.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`subsample`：这是 `colsample_bytree` 超参数的观察版本。这个超参数负责告诉 XGBoost 在训练每个树时需要使用多少训练样本。这个超参数可以用来防止过拟合问题。然而，如果我们使用过低的值，它也可能导致欠拟合问题。'
- en: Complete List of XGBoost Hyperparameters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 超参数完整列表
- en: 'For more information about other XGBoost’s hyperparameters, please visit the
    official documentation page: https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解 XGBoost 其他超参数的更多信息，请访问官方文档页面：https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn。
- en: In this section, we discussed how XGBoost works at a high level and looked at
    several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the LightGBM algorithm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了 XGBoost 的高层次工作原理，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还看了主要超参数。在下一节中，我们将讨论
    LightGBM 算法。
- en: Exploring LightGBM hyperparameters
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 LightGBM 超参数
- en: '**Light Gradient Boosting Machine** (**LightGBM**) is also a boosting algorithm
    built on top of a collection of decision trees, similar to XGBoost. It can also
    be utilized both for classification and regression tasks. However, it differs
    from XGBoost in the way the trees are grown. In LightGBM, trees are grown in a
    leaf-wise manner, while XGBoost grows trees in a level-wise manner (see *Figure
    11.2*). By leaf-wise, we mean that LightGBM grows trees by prioritizing nodes
    whose split leads to the highest increase of homogeneity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**轻量梯度提升机**（**LightGBM**）也是一种基于决策树集合的增强算法，类似于XGBoost。它既可以用于分类任务，也可以用于回归任务。然而，它在树的生长方式上与XGBoost不同。在LightGBM中，树是以叶节点的顺序生长的，而XGBoost是以层级的顺序生长的（见*图11.2*）。我们所说的“叶节点优先”是指LightGBM通过优先增长那些分裂导致同质性增加最大的节点来生长树：'
- en: '![Figure 11.2 – Level-wise versus leaf-wise tree growth'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.2 – Level-wise versus leaf-wise tree growth'
- en: '](img/B18753_11_002.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_11_002.jpg]'
- en: Figure 11.2 – Level-wise versus leaf-wise tree growth
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – Level-wise versus leaf-wise tree growth
- en: Besides the difference in how XGBoost and LightGBM grow the trees, they also
    have different ways of handling categorical features. In XGBoost, we need to encode
    the categorical features before passing them to the model. This is usually done
    using the one-hot encoding or integer encoding methods. In LightGBM, we can just
    tell which features are categorical and it will handle those features automatically
    by performing equality splitting. There are several other differences between
    XGBoost and LightGBM in terms of the way they perform optimization in distributed
    learning. In general, LightGBM has a much faster computation time compared to
    XGBoost.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了XGBoost和LightGBM在树的生长方式上的不同之外，它们在处理分类特征方面也有不同的方法。在XGBoost中，我们需要在将特征传递给模型之前对分类特征进行编码。这通常是通过使用独热编码或整数编码方法来完成的。在LightGBM中，我们只需告诉哪些特征是分类的，它就会通过执行等值分裂自动处理这些特征。在分布式学习中进行优化时，XGBoost和LightGBM在执行优化方面还有其他几个不同之处。总的来说，与XGBoost相比，LightGBM的计算时间要快得多。
- en: 'Similar to XGBoost, LightGBM also has its own package and can be utilized not
    only in Python but also in the R language. In Python, you can install LightGBM
    via `pip install lightgbm`. This package also implements the scikit-learn wrappers
    for both regression (`LGBMRegressor`) and classification (`LGBMClassifier`) tasks.
    The following are the most important hyperparameters for LightGBM, starting with
    the most important to the least based on the impact on model performance:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与XGBoost类似，LightGBM也有自己的包，不仅可以在Python中使用，也可以在R语言中使用。在Python中，您可以通过`pip install
    lightgbm`来安装LightGBM。此包还实现了scikit-learn包装器，用于回归（`LGBMRegressor`）和分类（`LGBMClassifier`）任务。以下是LightGBM最重要的超参数，从对模型性能影响最大到最小排序：
- en: '`max_depth`: This specifies the maximum depth of each decision tree. A lower
    value can help us prevent overfitting. However, a value that’s too low can lead
    to an underfitting problem. One thing is for sure – a higher value implies a longer
    computational time.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_depth`：这指定了每个决策树的最大深度。较低的值可以帮助我们防止过拟合。然而，过低的值可能导致欠拟合问题。有一点可以肯定——较高的值意味着更长的计算时间。'
- en: '`num_leaves`: This specifies the maximum number of leaves in each tree. It
    should have a value lower than two to the power of `max_depth` since a leaf-wise
    tree is much deeper than a depth-wise tree for a set number of leaves. In general,
    the higher the value, the better the model’s performance will be, with a trade-off
    of having a longer computational time. However, there is a threshold where the
    impact of adding more leaves will not have much additional impact on the model’s
    performance or even have a negative impact due to overfitting.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`num_leaves`：这指定了每棵树的最大叶节点数。它的值应该小于`max_depth`的2的幂，因为对于固定数量的叶节点，叶节点优先的树比深度优先的树要深得多。一般来说，值越高，模型的性能越好，但这也意味着更长的计算时间。然而，存在一个阈值，增加更多叶节点对模型性能的影响不会很大，甚至可能由于过拟合而产生负面影响。'
- en: '`Learning_rate`: This specifies the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding a
    more optimum solution, with a trade-off of having a longer computational time.
    You can increase the value of this hyperparameter if no sign of overfitting is
    found on the last iterations of training and vice versa.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Learning_rate`：这指定了梯度下降优化算法的学习率。值越低，模型找到更优解的可能性越高，但这也意味着更长的计算时间。如果在训练的最后几次迭代中没有发现过拟合的迹象，则可以增加此超参数的值，反之亦然。'
- en: '`min_child_samples`: This specifies the minimum number of samples required
    in the leaf nodes. A higher value can help us prevent overfitting. However, a
    value that’s too high can lead to an underfitting problem.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`min_child_samples`：这指定了叶节点中所需的最小样本数。较高的值可以帮助我们防止过拟合。然而，一个过高的值可能导致欠拟合问题。'
- en: '`Feature_fraction`: This is similar to `colsample_bytree` in XGBoost. This
    hyperparameter tells LightGBM how many randomly sampled features need to be used
    to choose the best splitting point in each of the decision trees. This hyperparameter
    can be useful for preventing overfitting. However, it can also lead to an underfitting
    problem if we use a value that is too low.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`特征分数`：这与XGBoost中的`colsample_bytree`类似。这个超参数告诉LightGBM在每棵决策树中选择最佳分割点时需要使用多少随机采样的特征。这个超参数对于防止过拟合可能很有用。然而，如果我们使用一个过低的值，也可能导致欠拟合问题。'
- en: '`bagging_fraction`: This is the observation’s version of the `feature_fraction`
    hyperparameter. This hyperparameter is responsible for telling LightGBM how many
    training samples need to be used during the training of each tree. Lower values
    can help us prevent overfitting and lower the computational time. However, a value
    that is too low can lead to an underfitting problem.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bagging_fraction`：这是`feature_fraction`超参数的观测版本。这个超参数负责告诉LightGBM在每棵树的训练过程中需要使用多少训练样本。较低的值可以帮助我们防止过拟合并降低计算时间。然而，一个过低的值可能导致欠拟合问题。'
- en: Complete List of LightGBM Hyperparameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM超参数完整列表
- en: 'For more information about other LightGBM hyperparameters, please visit the
    official documentation page: [https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于其他LightGBM超参数的信息，请访问官方文档页面：[https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api)。
- en: In this section, we discussed how LightGBM works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the CatBoost algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了LightGBM在高级别的工作方式，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还看了主要超参数。在下一节中，我们将讨论CatBoost算法。
- en: Exploring CatBoost hyperparameters
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索CatBoost超参数
- en: '**Categorical Boosting** (**CatBoost**) is another boosting algorithm built
    on top of a collection of decision trees, similar to XGBoost and LightGBM. It
    can also be utilized both for classification and regression tasks. The main difference
    between CatBoost and XGBoost or LightGBM is how it grows the trees. In XGBoost
    and LightGBM, trees are grown asymmetrically, while in CatBoost, trees are grown
    symmetrically so that all of the trees are balanced. This balanced tree characteristic
    provides several benefits, including the ability to control overfitting problems,
    lower inference time, and efficient implementation in CPUs. CatBoost does this
    by using the same condition in every split in the nodes, as shown in the following
    diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类提升**（**CatBoost**）是另一种基于决策树集合的增强算法，类似于XGBoost和LightGBM。它也可以用于分类和回归任务。CatBoost与XGBoost或LightGBM的主要区别在于其生长树的方式。在XGBoost和LightGBM中，树是非对称生长的，而在CatBoost中，树是对称生长的，使得所有树都是平衡的。这种平衡树特性提供了几个好处，包括控制过拟合问题的能力、降低推理时间和在CPU上的高效实现。CatBoost通过在每个节点的每个分割中使用相同的条件来实现这一点，如下面的图所示：'
- en: '![Figure 11.3 – Asymmetric versus symmetric tree'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.3 – 非对称与对称树'
- en: '](img/B18753_11_003.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_11_003.jpg](img/B18753_11_003.jpg)'
- en: Figure 11.3 – Asymmetric versus symmetric tree
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 非对称与对称树
- en: 'The main selling point of CatBoost is its ability to handle numerous types
    of features automatically, including numerical, categorical, and text, especially
    for categorical features. We just need to tell CatBoost which features are categorical
    features via the `cat_features` parameter and it will handle those features automatically.
    By default, CatBoost will perform one-hot encoding for categorical features that
    only have two classes. For higher cardinality features, it will perform target
    encoding and combine several categorical features or even categorical and numerical
    features. For more information on how CatBoost handles categorical features, please
    refer to the official documentation page: [https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost的主要卖点是其自动处理多种类型特征的能力，包括数值、分类和文本，特别是对于分类特征。我们只需通过`cat_features`参数告诉CatBoost哪些特征是分类特征，它就会自动处理这些特征。默认情况下，CatBoost会对只有两个类别的分类特征执行独热编码。对于更高基数特征，它将执行目标编码，并组合几个分类特征，甚至分类和数值特征。有关CatBoost如何处理分类特征的更多信息，请参阅官方文档页面：[https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)。
- en: 'Similar to XGBoost and LightGBM, CatBoost also has its own package and can
    be utilized not only in Python but also in the R language. In Python, you can
    install CatBoost via `pip install catboost`. You can utilize the implemented scikit-learn-friendly
    classes for both regression (`CatBoostRegressor`) and classification (`CatBoostClassifier`)
    tasks. The following is a list of CatBoost’s most important hyperparameters, sorted
    in descending order based on the importance of each hyperparameter regarding model
    performance:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与XGBoost和LightGBM类似，CatBoost也有自己的包，不仅可以在Python中使用，也可以在R语言中使用。在Python中，你可以通过`pip
    install catboost`安装CatBoost。你可以利用实现的scikit-learn兼容类来处理回归（`CatBoostRegressor`）和分类（`CatBoostClassifier`）任务。以下是根据每个超参数对模型性能的重要性按降序排列的CatBoost最重要的超参数列表：
- en: '`iterations`: This specifies the number of decision trees to be utilized to
    build the CatBoost model. It can also be interpreted as the number of boosting
    rounds, similar to the concept of epochs in a neural network. In general, the
    higher the value, the better the model’s performance will be, with a trade-off
    of having a longer computational time. However, there is a threshold where the
    impact of adding more trees will not have much additional impact on the model’s
    performance or even have a negative impact due to overfitting.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`iterations`: 这指定了用于构建CatBoost模型的决定树的数量。它也可以解释为提升轮数，类似于神经网络中的epoch概念。一般来说，值越高，模型的性能越好，但代价是计算时间更长。然而，存在一个阈值，增加更多树对模型性能的影响不会太大，甚至可能由于过拟合而产生负面影响。'
- en: '`depth`: This specifies the maximum depth of each decision tree. A lower value
    can help us prevent overfitting. However, a value that’s too low can lead to an
    underfitting problem. One thing is for sure – a higher value implies a longer
    computational time.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`depth`: 这指定了每个决策树的最大深度。一个较低的值可以帮助我们防止过拟合。然而，一个过低的值可能导致欠拟合问题。有一点可以肯定——较高的值意味着更长的计算时间。'
- en: '`learning_rate`: This specifies the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding a
    more optimum solution, with a trade-off of having a longer computational time.
    You can increase the value of this hyperparameter if no sign of overfitting is
    found on the last iterations of training and vice versa.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`learning_rate`: 这指定了梯度下降优化算法的学习率。值越低，模型找到更优解的机会越高，但代价是计算时间更长。如果在训练的最后几次迭代中没有发现过拟合的迹象，你可以增加这个超参数的值，反之亦然。'
- en: '`l2_leaf_reg`: This is the regularization parameter on the cost function. This
    hyperparameter can prevent the overfitting problem. However, it can also lead
    to an underfitting problem if we use a value that’s too high.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`l2_leaf_reg`: 这是在成本函数上的正则化参数。这个超参数可以防止过拟合问题。然而，如果我们使用一个过高的值，它也可能导致欠拟合问题。'
- en: '`one_hot_max_size`: This is the threshold that tells CatBoost when to perform
    one-hot encoding on the categorical features. Any categorical features that have
    cardinality lower than or equal to the given value will be transformed into numerical
    values via the one-hot encoding method.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete List of CatBoost Hyperparameters
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: For more information about other CatBoost hyperparameters, please visit the
    official documentation page ([https://catboost.ai/en/docs/concepts/parameter-tuning](https://catboost.ai/en/docs/concepts/parameter-tuning)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how CatBoost works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the SVM algorithm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Exploring SVM hyperparameters
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machine** (**SVM**) is an ML model that utilizes lines or
    hyperplanes, along with some linear algebra transformations, to perform a classification
    or regression task. All the algorithms discussed in the previous sections can
    be classified as tree-based algorithms, while SVM is not part of the *tree-based*
    group of ML algorithms. It is part of the *distance-based* group of algorithms.
    We usually called the linear algebra transformation in SVM a **kernel**. This
    is responsible for transforming any problem into a linear problem.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular and well-maintained implementation of SVM in Python can be
    found in the scikit-learn package. It includes implementations for both regression
    (`SVR`) and classification (`SVC`) tasks. Both of them have very similar hyperparameters
    with only a few small differences. The following are the most important hyperparameters
    for SVM, starting with the most important to the least based on their impact on
    model performance:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel`: This is the linear algebra transformation, whose goal is to convert
    the given problem into a linear problem. There are five kernels that we can choose
    from, including linear (`linear`), polynomial (`poly`), radial basis function
    (`rbf`), and sigmoid (`sigmoid`) kernels. Different kernels will have different
    impacts on model performance and there is no clear rule of thumb for this hyperparameter.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`C`: This is the regularization parameter that controls overfitting. The lower
    the value, the stronger the impact that regularization will have on the model,
    and hence a higher chance of preventing overfitting.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`degree`: This hyperparameter is specific to the polynomial kernel function.
    The value of this hyperparameter corresponds to the degree of the polynomial function
    that’s used by the model.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gamma`: This is the coefficient for the radial basis, polynomial, and sigmoid
    kernel functions. There are two options that scikit-learn provides, namely `scale`
    and `auto`.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SVM Hyperparameters in scikit-learn
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: For more information about how each of the hyperparameters in SVM are implemented
    in scikit-learn, you can visit the official documentation pages at https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    and [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how SVM works at a high level and looked at several
    important hyperparameters, along with an explanation of how they impact model
    performance. We also looked at the main hyperparameters. In the next section,
    we will discuss artificial neural networks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Exploring artificial neural network hyperparameters
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An **artificial neural network**, also known as **deep learning**, is a kind
    of ML algorithm that mimics how human brains work. Deep learning can be utilized
    for both regression and classification tasks. One of the main selling points of
    this model is its ability to perform feature engineering and selection automatically
    from the raw data. In general, to ensure this algorithm works decently, we need
    a large amount of training data to be fed to the model. The simplest form of a
    neural network is called a **perceptron** (see *Figure 11.4*). A perceptron is
    just a linear combination that is applied on top of all of the features, with
    bias added at the end of the calculation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Perceptron'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_11_004.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – Perceptron
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: If the output from the perceptron is passed to a non-linear function, which
    is usually called an **activation function**, and then passed to another perceptron,
    then we can call this a **multi-layer perceptron** (**MLP**) with one layer. The
    training process for a neural network consists of two big procedures, namely **forward
    propagation** and **backward propagation**. In forward propagation, we just let
    the neural network perform calculations on top of the given inputs based on the
    defined architecture. In backward propagation, the model will update the weights
    and bias parameters based on the defined loss function using a gradient-based
    optimization procedure.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: There are other variants of neural networks besides MLP, such as **convolutional
    neural networks** (**CNNs**), **long short-term memory networks** (**LSTMs**),
    **recurrent neural networks** (**RNNs**), and **transformers**. CNN is usually
    adopted when we work with image data, but we can also use a one-dimensional CNN
    when working with text data. RNNs and LSTMs are usually adopted when working with
    time series or natural language data. Transformers are mainly used for text-related
    projects, but recently, they have been adopted for image and voice data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Several packages provide implementations of neural networks in Python, including
    PyTorch, TensorFlow, and Scikit Learn. The following are the most important hyperparameters,
    sorted in descending order based on the importance of each hyperparameter regarding
    model performance. Note that this priority list is subjective based on our experience
    of developing Random Forest models in the past. Since the naming of the hyperparameters
    may differ across different packages, we will only use the general names of the
    hyperparameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer**: This is the gradient-based optimization algorithm to be used.
    There are several optimizers for us to choose from. However, perhaps the most
    popular and widely adopted optimizer is **Adam**. There are other options, including
    (but not limited to) SGD and RMSProp. Different optimizers may have different
    impacts on model performance and there is no clear rule of thumb for choosing
    which one is the best. It is worth noting that each optimizer has its own hyperparameter
    as well.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Learning Rate**: This hyperparameter controls how big the step will be for
    the optimizer to “learn” from the given training data during the optimization
    process. It is important to choose the best range of learning rates first before
    tuning other hyperparameters. The lower the value, the higher the chances of the
    model finding a more optimum solution, with a trade-off of having a longer computational
    time. You can increase the value of this hyperparameter if no sign of overfitting
    is found on the last iterations of training and vice versa.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch Size**: This specifies the number of training samples that will be
    passed to the neural network within each training step. In general, the higher
    the value, the better the model’s performance will be. However, a batch size that’s
    too high will usually be constrained by the device’s memory.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_estimators` in XGBoost and `iterations` in CatBoost, a high value can lead
    to better model performance, with a trade-off of having a longer computational
    time. However, we need to be careful when using a value that’s too high since
    it can lead to overfitting.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of Layers**: The higher the value, the higher the complexity of the
    model, hence the higher the chance of overfitting. Usually, one or two layers
    is more than enough to build a good model.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of Nodes**: The number of units or nodes within each of the layers.
    The higher the value, the higher the complexity of the model, hence a higher chance
    of overfitting.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation Function**: The non-linear transformation function. There are
    many activation functions to choose from. Some of the most well-adopted activation
    functions in practice are **Rectified Linear Activation Function** (**ReLU**),
    **Exponential Linear Unit** (**ELU**), **Sigmoid**, **Softmax**, and **Tanh**.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dropout Rate**: The rate for the dropout layer. The dropout layer is a special
    layer in a neural network that acts as a regularizer by randomly setting the unit
    value to zero. This hyperparameter controls how many units are set to zero. A
    higher value can help us prevent overfitting. However, a value that’s too high
    can lead to an underfitting problem.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dropout 率**：dropout 层的比率。dropout 层是神经网络中的一个特殊层，通过随机将单元值设为零来充当正则化器。这个超参数控制了多少个单元被设为零。较高的值可以帮助我们防止过拟合。然而，过高的值可能导致欠拟合问题。'
- en: '**L1/L2 Regularization**: These are the regularization parameters that are
    applied to the loss function. This hyperparameter can help prevent overfitting.
    However, it can also lead to an underfitting problem if its value is too high.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L1/L2 正则化**：这些是应用于损失函数的正则化参数。这个超参数可以帮助防止过拟合。然而，如果其值过高，也可能导致欠拟合问题。'
- en: In this section, we have discussed how neural network works at a high level,
    the variants of neural networks, and looked at several important hyperparameters,
    along with an explanation of how they impact model performance. We also looked
    at the main hyperparameters. Now, let’s summarize this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了神经网络在高级层面上的工作原理，神经网络的变体，并查看了一些重要的超参数，以及它们如何影响模型性能的解释。我们还探讨了主要超参数。现在，让我们总结本章内容。
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how several popular algorithms work at a high
    level, explained their important hyperparameters and how they impact performance,
    and provided priority lists of the hyperparameters, sorted in descending order
    based on their impact on performance. At this point, you should be able to design
    your hyperparameter tuning experiments more effectively by focusing on the most
    important hyperparameters. You should also understand what impact each of the
    important hyperparameters has on the performance of the model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了几个流行的算法在高级层面上的工作原理，解释了它们的重要超参数以及它们如何影响性能，并提供了按性能影响降序排列的超参数优先级列表。此时，你应该能够通过关注最重要的超参数来更有效地设计你的超参数调整实验。你也应该了解每个重要超参数对模型性能的影响。
- en: In the next chapter, we’ll summarize the hyperparameter tuning methods we’ve
    discussed here into a simple decision map that can help you choose which method
    is the most suitable for your problem. Furthermore, we will cover several study
    cases that show how to utilize this decision map in practice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把这里讨论的超参数调整方法总结成一个简单的决策图，帮助你选择最适合你问题的方法。此外，我们还将涵盖几个案例研究，展示如何在实际中利用这个决策图。
