- en: '*Chapter 11*: Understanding the Hyperparameters of Popular Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most **machine learning** (**ML**) algorithms have their own hyperparameters.
    Knowing how to implement a lot of fancy hyperparameter tuning methods without
    understanding the hyperparameters of the model is the same as a doctor writing
    a prescription before diagnosing the patient.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn about the hyperparameters of several popular ML
    algorithms. There will be a broad explanation for each of the algorithms, including
    (but not limited to) the definition of each hyperparameter, what will be impacted
    when the value of each hyperparameter is changed, and the priority list of hyperparameters
    based on the impact.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the important hyperparameters
    of several popular ML algorithms. Understanding the hyperparameters of ML algorithms
    is crucial since not all hyperparameters are equally significant when it comes
    to impacting the model’s performance. We do not have to perform hyperparameter
    tuning on all of the hyperparameters of a model; we just need to focus on the
    more critical hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Random Forest hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring XGBoost hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring LightGBM hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring CatBoost hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring SVM hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring artificial neural network hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Random Forest hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Random Forest** is a tree-based model that is built using a collection of
    **decision trees**. It is a very powerful ensemble ML model that can be utilized
    for both classification and regression tasks. The way Random Forest utilizes the
    collection of decision trees is by performing an ensemble method called **bootstrap
    aggregation** (**bagging**) with some modifications. To understand how each of
    the Random Forest’s hyperparameters can impact the model’s performance, we need
    to understand how the model works in the first place.'
  prefs: []
  type: TYPE_NORMAL
- en: Before discussing how Random Forest ensembles a collection of decision trees,
    let’s discuss how a decision tree works at a high level. A decision tree can be
    utilized to perform a classification or regression task by constructing a series
    of decisions (in the form of rules and splitting points) that can be visualized
    in the form of a tree. These decisions are made by looking through all of the
    features and the feature values of the given training data. The goal of a decision
    tree is to have high homogeneity for each of the leaf nodes. Several methods can
    be used to measure homogeneity; the two most popular methods for classification
    tasks are to calculate the **Gini** or **Entropy** values, while the most popular
    method for regression tasks is to calculate the **Mean Squared Error** value.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest utilizes the bagging method to ensemble the collection of decision
    trees. Bagging is an ensemble method that works by combining predictions from
    multiple ML models with the hope of generating a more accurate and robust prediction.
    In this case, Random Forest combines the prediction outputs from several decision
    trees so that we are not too focused on the prediction from a single tree. This
    is because a decision tree is very likely to overfit the training data. However,
    Random Forest does not just utilize the vanilla bagging ensemble method – it also
    ensures that it only utilizes prediction outputs from the collection of decision
    trees that are not highly correlated with each other. How is Random Forest able
    to do that? Instead of asking each decision tree to look through all the features
    and their values when choosing the splitting points, Random Forest customizes
    this procedure so that each decision tree only looks at a random sample of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular and well-maintained implementation of Random Forest in Python
    can be found in the scikit-learn package. It includes implementations for both
    regression (`RandomForestRegressor`) and classification (`RandomForestClassifier`)
    tasks. Both implementations have very similar hyperparameters with only a few
    small differences. The following are the most important hyperparameters, starting
    with the most important to the least based on the impact on model performance.
    Note that this priority list is subjective, based on our experience of developing
    Random Forest models in the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This specifies the number of decision trees to be utilized
    to build the Random Forest. In general, the larger the number of trees, the better
    the model’s performance will be, with a trade-off of having longer computational
    time. However, there is a threshold beyond which adding more trees will not have
    much additional impact on the model’s performance. It could even have a negative
    impact due to the problem of overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max_features`: This specifies the number of randomly sampled features that
    are used by Random Forest to choose the best splitting point in each of the decision
    trees. The higher the value, the lower the reduction in variance, and hence the
    lower the increase in bias. A higher value also leads to having a longer computational
    time. scikit-learn, by default, will use all of the features for regression tasks
    and use only `sqrt(n_features)` number of features for classification tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`criterion`: This is used to measure the homogeneity of each decision tree.
    scikit-learn implemented several methods for both regression and classification
    tasks. There’s `squared_error`, `absolute_error`, and `poisson` for regression
    tasks, while there’s `gini`, `entropy`, and `log_loss` for classification tasks.
    Different methods will have different impacts on model performance; there is no
    clear rule of thumb for this hyperparameter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max_depth`: This specifies the maximum depth of each decision tree. The default
    value of this hyperparameter is `None`, meaning that the nodes of each tree will
    keep branching until we have pure leaf nodes or until all the leaves contain less
    than `min_samples_split` number of samples. The lower the value, the better, since
    this prevents overfitting. However, a value that is too low can lead to an underfitting
    problem. One thing is for sure – a higher value implies a longer computational
    time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`min_samples_split`: This specifies the minimum number of samples required
    for a tree to be able to further split an internal node (a node that can be split
    into child nodes). The higher the value, the easier it is to prevent overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: This specifies the minimum number of samples required in
    the leaf nodes. A higher value can help us prevent overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Forest Hyperparameters in scikit-learn
  prefs: []
  type: TYPE_NORMAL
- en: For more information about each of the hyperparameters of the Random Forest
    implementation in scikit-learn, please visit the official documentation pages
    at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
    and [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Other useful boilerplate parameters can be found across different scikit-learn
    estimator implementations. The following are several important parameters that
    you need to be aware of that can help you while training a scikit-learn estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight`: This specifies the weights for each class that exists in the
    training data. This is only available for classification tasks. This parameter
    is very important when you face an imbalanced class problem. We need to give higher
    weights to classes that have fewer samples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_jobs`: This specifies the number of parallel processes to be utilized when
    training the estimator. scikit-learn utilizes the `joblib` package in the backend.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`random_state`: This specifies the random seed number to ensure the code is
    reproducible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`verbose`: This parameter is used to control any logging activities. Setting
    `verbose` to an integer greater than zero enables us to see what happens when
    training an estimator.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we learned how Random Forest works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    the model’s performance. We also looked at the main hyperparameters. Furthermore,
    we learned about several useful parameters in scikit-learn that can ease the training
    process. In the next section, we will discuss the XGBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring XGBoost hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Extreme Gradient Boosting** (**XGBoost**) is also a tree-based model that
    is built using a collection of decision trees, similar to a Random Forest. It
    can also be utilized for both classification and regression tasks. The difference
    between XGBoost and Random Forest is in how they perform the ensemble. Unlike
    Random Forest, which uses the bagging ensemble method, XGBoost utilizes another
    ensemble method called **boosting**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting is an ensemble algorithm whose goal is to achieve higher performance
    through a sequence of individually weak models by overcoming the weaknesses of
    the predecessor models (see *Figure 11.1*). It is not a specific model; it’s just
    a generic ensemble algorithm. The definition of weakness may vary across different
    types of boosting ensemble implementation. In XGBoost, it is defined based on
    the error of the gradient from the previous decision tree model. Take a look at
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Boosting ensemble algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_11_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Boosting ensemble algorithm
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is a very popular and well-adopted ML model that is built using the
    boosting ensemble algorithm and a collection of decision trees. Each of the decision
    trees is added one at a time and is fitted to the prediction errors from the previous
    tree to correct those errors. It is worth noting that since XGBoost is part of
    the gradient boosting algorithm, all of the weak models (decision trees) need
    to be fitted using a differentiable loss function and the gradient descent optimization
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has its own package and can be utilized not only in Python but also
    in other programming languages, such as R and JVM. In Python, you can install
    XGBoost via `pip install xgboost`. This package also implements the scikit-learn
    wrappers for both regression (`XGBRegressor`) and classification (`XGBClassifier`)
    tasks. Numerous hyperparameters are provided by the package, but not all of them
    are very important in affecting the model’s performance. The following are the
    most important hyperparameters, starting with the most important to the least
    based on their impact on model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This specifies the number of decision trees to be utilized
    to build the XGBoost model. It can also be interpreted as the number of boosting
    rounds, which is similar to the concept of epochs in a neural network. In general,
    the higher the value, the better the model’s performance will be, with the trade-off
    of having a longer computation time. However, we need to be careful with a value
    that’s too high since it can lead us to the overfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`learning_rate`: This is the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding the
    optimum solution, with a trade-off of having a longer computational time. You
    can increase the value of this hyperparameter if there no sign of overfitting
    is found on the last iterations of training; you can decrease it if there is overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max_depth`: This is the maximum depth of each decision tree. A lower value
    can help us prevent overfitting. However, a too-low value can lead to an underfitting
    problem. One thing is for sure – a higher value leads to a longer computational
    time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`min_child_weight`: This is the minimum sum of instance weight, calculated
    using the Hessian, that’s needed in a child. This hyperparameter acts as a regularizer
    to ensure that each tree will stop trying to split the node once a certain degree
    of purity is reached. In other words, it is a regularization parameter that works
    by limiting the depth of the tree so that the overfitting problem can be prevented.
    A higher value can help us prevent overfitting. However, a too-high value can
    lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gamma`: This is a pseudo-regularization parameter that is calculated based
    on a reduction in the loss value. The value of this hyperparameter specifies the
    minimum loss reduction required to make a further partition on a leaf node of
    the tree. You can put a high value on this hyperparameter to prevent the overfitting
    problem. However, please be careful and don’t use a value that’s too high; it
    can lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`colsample_bytree`: This is the fraction version of the `max_features` hyperparameter
    in the scikit-learn implementation of Random Forest. This hyperparameter is responsible
    for telling XGBoost how many randomly sampled features are needed to choose the
    best splitting point in each of the decision trees. A low value can help us prevent
    overfitting and lowers the computational time. However, a value that’s too low
    can lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`subsample`: This is the observation’s version of the `colsample_bytree` hyperparameter.
    This hyperparameter is responsible for telling XGBoost how many training samples
    need to be used while training each tree. This hyperparameter can be useful to
    prevent the overfitting problem. However, it can also lead us to an underfitting
    problem if we use a value that’s too low.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete List of XGBoost Hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about other XGBoost’s hyperparameters, please visit the
    official documentation page: https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how XGBoost works at a high level and looked at
    several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the LightGBM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LightGBM hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Light Gradient Boosting Machine** (**LightGBM**) is also a boosting algorithm
    built on top of a collection of decision trees, similar to XGBoost. It can also
    be utilized both for classification and regression tasks. However, it differs
    from XGBoost in the way the trees are grown. In LightGBM, trees are grown in a
    leaf-wise manner, while XGBoost grows trees in a level-wise manner (see *Figure
    11.2*). By leaf-wise, we mean that LightGBM grows trees by prioritizing nodes
    whose split leads to the highest increase of homogeneity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Level-wise versus leaf-wise tree growth'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_11_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – Level-wise versus leaf-wise tree growth
  prefs: []
  type: TYPE_NORMAL
- en: Besides the difference in how XGBoost and LightGBM grow the trees, they also
    have different ways of handling categorical features. In XGBoost, we need to encode
    the categorical features before passing them to the model. This is usually done
    using the one-hot encoding or integer encoding methods. In LightGBM, we can just
    tell which features are categorical and it will handle those features automatically
    by performing equality splitting. There are several other differences between
    XGBoost and LightGBM in terms of the way they perform optimization in distributed
    learning. In general, LightGBM has a much faster computation time compared to
    XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to XGBoost, LightGBM also has its own package and can be utilized not
    only in Python but also in the R language. In Python, you can install LightGBM
    via `pip install lightgbm`. This package also implements the scikit-learn wrappers
    for both regression (`LGBMRegressor`) and classification (`LGBMClassifier`) tasks.
    The following are the most important hyperparameters for LightGBM, starting with
    the most important to the least based on the impact on model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: This specifies the maximum depth of each decision tree. A lower
    value can help us prevent overfitting. However, a value that’s too low can lead
    to an underfitting problem. One thing is for sure – a higher value implies a longer
    computational time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`num_leaves`: This specifies the maximum number of leaves in each tree. It
    should have a value lower than two to the power of `max_depth` since a leaf-wise
    tree is much deeper than a depth-wise tree for a set number of leaves. In general,
    the higher the value, the better the model’s performance will be, with a trade-off
    of having a longer computational time. However, there is a threshold where the
    impact of adding more leaves will not have much additional impact on the model’s
    performance or even have a negative impact due to overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Learning_rate`: This specifies the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding a
    more optimum solution, with a trade-off of having a longer computational time.
    You can increase the value of this hyperparameter if no sign of overfitting is
    found on the last iterations of training and vice versa.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`min_child_samples`: This specifies the minimum number of samples required
    in the leaf nodes. A higher value can help us prevent overfitting. However, a
    value that’s too high can lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Feature_fraction`: This is similar to `colsample_bytree` in XGBoost. This
    hyperparameter tells LightGBM how many randomly sampled features need to be used
    to choose the best splitting point in each of the decision trees. This hyperparameter
    can be useful for preventing overfitting. However, it can also lead to an underfitting
    problem if we use a value that is too low.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bagging_fraction`: This is the observation’s version of the `feature_fraction`
    hyperparameter. This hyperparameter is responsible for telling LightGBM how many
    training samples need to be used during the training of each tree. Lower values
    can help us prevent overfitting and lower the computational time. However, a value
    that is too low can lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete List of LightGBM Hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about other LightGBM hyperparameters, please visit the
    official documentation page: [https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how LightGBM works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the CatBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring CatBoost hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Categorical Boosting** (**CatBoost**) is another boosting algorithm built
    on top of a collection of decision trees, similar to XGBoost and LightGBM. It
    can also be utilized both for classification and regression tasks. The main difference
    between CatBoost and XGBoost or LightGBM is how it grows the trees. In XGBoost
    and LightGBM, trees are grown asymmetrically, while in CatBoost, trees are grown
    symmetrically so that all of the trees are balanced. This balanced tree characteristic
    provides several benefits, including the ability to control overfitting problems,
    lower inference time, and efficient implementation in CPUs. CatBoost does this
    by using the same condition in every split in the nodes, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Asymmetric versus symmetric tree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_11_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – Asymmetric versus symmetric tree
  prefs: []
  type: TYPE_NORMAL
- en: 'The main selling point of CatBoost is its ability to handle numerous types
    of features automatically, including numerical, categorical, and text, especially
    for categorical features. We just need to tell CatBoost which features are categorical
    features via the `cat_features` parameter and it will handle those features automatically.
    By default, CatBoost will perform one-hot encoding for categorical features that
    only have two classes. For higher cardinality features, it will perform target
    encoding and combine several categorical features or even categorical and numerical
    features. For more information on how CatBoost handles categorical features, please
    refer to the official documentation page: [https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to XGBoost and LightGBM, CatBoost also has its own package and can
    be utilized not only in Python but also in the R language. In Python, you can
    install CatBoost via `pip install catboost`. You can utilize the implemented scikit-learn-friendly
    classes for both regression (`CatBoostRegressor`) and classification (`CatBoostClassifier`)
    tasks. The following is a list of CatBoost’s most important hyperparameters, sorted
    in descending order based on the importance of each hyperparameter regarding model
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iterations`: This specifies the number of decision trees to be utilized to
    build the CatBoost model. It can also be interpreted as the number of boosting
    rounds, similar to the concept of epochs in a neural network. In general, the
    higher the value, the better the model’s performance will be, with a trade-off
    of having a longer computational time. However, there is a threshold where the
    impact of adding more trees will not have much additional impact on the model’s
    performance or even have a negative impact due to overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`depth`: This specifies the maximum depth of each decision tree. A lower value
    can help us prevent overfitting. However, a value that’s too low can lead to an
    underfitting problem. One thing is for sure – a higher value implies a longer
    computational time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`learning_rate`: This specifies the learning rate of the gradient descent optimization
    algorithm. The lower the value, the higher the chances of the model finding a
    more optimum solution, with a trade-off of having a longer computational time.
    You can increase the value of this hyperparameter if no sign of overfitting is
    found on the last iterations of training and vice versa.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`l2_leaf_reg`: This is the regularization parameter on the cost function. This
    hyperparameter can prevent the overfitting problem. However, it can also lead
    to an underfitting problem if we use a value that’s too high.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`one_hot_max_size`: This is the threshold that tells CatBoost when to perform
    one-hot encoding on the categorical features. Any categorical features that have
    cardinality lower than or equal to the given value will be transformed into numerical
    values via the one-hot encoding method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete List of CatBoost Hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: For more information about other CatBoost hyperparameters, please visit the
    official documentation page ([https://catboost.ai/en/docs/concepts/parameter-tuning](https://catboost.ai/en/docs/concepts/parameter-tuning)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how CatBoost works at a high level and looked
    at several important hyperparameters, along with an explanation of how they impact
    model performance. We also looked at the main hyperparameters. In the next section,
    we will discuss the SVM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring SVM hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machine** (**SVM**) is an ML model that utilizes lines or
    hyperplanes, along with some linear algebra transformations, to perform a classification
    or regression task. All the algorithms discussed in the previous sections can
    be classified as tree-based algorithms, while SVM is not part of the *tree-based*
    group of ML algorithms. It is part of the *distance-based* group of algorithms.
    We usually called the linear algebra transformation in SVM a **kernel**. This
    is responsible for transforming any problem into a linear problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular and well-maintained implementation of SVM in Python can be
    found in the scikit-learn package. It includes implementations for both regression
    (`SVR`) and classification (`SVC`) tasks. Both of them have very similar hyperparameters
    with only a few small differences. The following are the most important hyperparameters
    for SVM, starting with the most important to the least based on their impact on
    model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel`: This is the linear algebra transformation, whose goal is to convert
    the given problem into a linear problem. There are five kernels that we can choose
    from, including linear (`linear`), polynomial (`poly`), radial basis function
    (`rbf`), and sigmoid (`sigmoid`) kernels. Different kernels will have different
    impacts on model performance and there is no clear rule of thumb for this hyperparameter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`C`: This is the regularization parameter that controls overfitting. The lower
    the value, the stronger the impact that regularization will have on the model,
    and hence a higher chance of preventing overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`degree`: This hyperparameter is specific to the polynomial kernel function.
    The value of this hyperparameter corresponds to the degree of the polynomial function
    that’s used by the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gamma`: This is the coefficient for the radial basis, polynomial, and sigmoid
    kernel functions. There are two options that scikit-learn provides, namely `scale`
    and `auto`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SVM Hyperparameters in scikit-learn
  prefs: []
  type: TYPE_NORMAL
- en: For more information about how each of the hyperparameters in SVM are implemented
    in scikit-learn, you can visit the official documentation pages at https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    and [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how SVM works at a high level and looked at several
    important hyperparameters, along with an explanation of how they impact model
    performance. We also looked at the main hyperparameters. In the next section,
    we will discuss artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring artificial neural network hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An **artificial neural network**, also known as **deep learning**, is a kind
    of ML algorithm that mimics how human brains work. Deep learning can be utilized
    for both regression and classification tasks. One of the main selling points of
    this model is its ability to perform feature engineering and selection automatically
    from the raw data. In general, to ensure this algorithm works decently, we need
    a large amount of training data to be fed to the model. The simplest form of a
    neural network is called a **perceptron** (see *Figure 11.4*). A perceptron is
    just a linear combination that is applied on top of all of the features, with
    bias added at the end of the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_11_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – Perceptron
  prefs: []
  type: TYPE_NORMAL
- en: If the output from the perceptron is passed to a non-linear function, which
    is usually called an **activation function**, and then passed to another perceptron,
    then we can call this a **multi-layer perceptron** (**MLP**) with one layer. The
    training process for a neural network consists of two big procedures, namely **forward
    propagation** and **backward propagation**. In forward propagation, we just let
    the neural network perform calculations on top of the given inputs based on the
    defined architecture. In backward propagation, the model will update the weights
    and bias parameters based on the defined loss function using a gradient-based
    optimization procedure.
  prefs: []
  type: TYPE_NORMAL
- en: There are other variants of neural networks besides MLP, such as **convolutional
    neural networks** (**CNNs**), **long short-term memory networks** (**LSTMs**),
    **recurrent neural networks** (**RNNs**), and **transformers**. CNN is usually
    adopted when we work with image data, but we can also use a one-dimensional CNN
    when working with text data. RNNs and LSTMs are usually adopted when working with
    time series or natural language data. Transformers are mainly used for text-related
    projects, but recently, they have been adopted for image and voice data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several packages provide implementations of neural networks in Python, including
    PyTorch, TensorFlow, and Scikit Learn. The following are the most important hyperparameters,
    sorted in descending order based on the importance of each hyperparameter regarding
    model performance. Note that this priority list is subjective based on our experience
    of developing Random Forest models in the past. Since the naming of the hyperparameters
    may differ across different packages, we will only use the general names of the
    hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer**: This is the gradient-based optimization algorithm to be used.
    There are several optimizers for us to choose from. However, perhaps the most
    popular and widely adopted optimizer is **Adam**. There are other options, including
    (but not limited to) SGD and RMSProp. Different optimizers may have different
    impacts on model performance and there is no clear rule of thumb for choosing
    which one is the best. It is worth noting that each optimizer has its own hyperparameter
    as well.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Learning Rate**: This hyperparameter controls how big the step will be for
    the optimizer to “learn” from the given training data during the optimization
    process. It is important to choose the best range of learning rates first before
    tuning other hyperparameters. The lower the value, the higher the chances of the
    model finding a more optimum solution, with a trade-off of having a longer computational
    time. You can increase the value of this hyperparameter if no sign of overfitting
    is found on the last iterations of training and vice versa.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch Size**: This specifies the number of training samples that will be
    passed to the neural network within each training step. In general, the higher
    the value, the better the model’s performance will be. However, a batch size that’s
    too high will usually be constrained by the device’s memory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_estimators` in XGBoost and `iterations` in CatBoost, a high value can lead
    to better model performance, with a trade-off of having a longer computational
    time. However, we need to be careful when using a value that’s too high since
    it can lead to overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of Layers**: The higher the value, the higher the complexity of the
    model, hence the higher the chance of overfitting. Usually, one or two layers
    is more than enough to build a good model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of Nodes**: The number of units or nodes within each of the layers.
    The higher the value, the higher the complexity of the model, hence a higher chance
    of overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation Function**: The non-linear transformation function. There are
    many activation functions to choose from. Some of the most well-adopted activation
    functions in practice are **Rectified Linear Activation Function** (**ReLU**),
    **Exponential Linear Unit** (**ELU**), **Sigmoid**, **Softmax**, and **Tanh**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dropout Rate**: The rate for the dropout layer. The dropout layer is a special
    layer in a neural network that acts as a regularizer by randomly setting the unit
    value to zero. This hyperparameter controls how many units are set to zero. A
    higher value can help us prevent overfitting. However, a value that’s too high
    can lead to an underfitting problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**L1/L2 Regularization**: These are the regularization parameters that are
    applied to the loss function. This hyperparameter can help prevent overfitting.
    However, it can also lead to an underfitting problem if its value is too high.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we have discussed how neural network works at a high level,
    the variants of neural networks, and looked at several important hyperparameters,
    along with an explanation of how they impact model performance. We also looked
    at the main hyperparameters. Now, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how several popular algorithms work at a high
    level, explained their important hyperparameters and how they impact performance,
    and provided priority lists of the hyperparameters, sorted in descending order
    based on their impact on performance. At this point, you should be able to design
    your hyperparameter tuning experiments more effectively by focusing on the most
    important hyperparameters. You should also understand what impact each of the
    important hyperparameters has on the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll summarize the hyperparameter tuning methods we’ve
    discussed here into a simple decision map that can help you choose which method
    is the most suitable for your problem. Furthermore, we will cover several study
    cases that show how to utilize this decision map in practice.
  prefs: []
  type: TYPE_NORMAL
