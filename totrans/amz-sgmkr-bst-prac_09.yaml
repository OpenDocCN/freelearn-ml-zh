- en: 'Chapter 7: Profile Training Jobs with Amazon SageMaker Debugger'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training **machine learning** (**ML**) models involves experimenting with multiple
    algorithms, with their hyperparameters typically crunching through large volumes
    of data. Training a model that yields optimal results is both a time- and compute-intensive
    task. Improved training time yields improved productivity and reduces overall
    training costs.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training, as we discussed in [*Chapter 6*](B17249_06_Final_JM_ePub.xhtml#_idTextAnchor117),
    *Training and Tuning at Scale*, goes a long way in achieving improved training
    times by using a scalable compute cluster. However, monitoring training infrastructure
    to identify and debug resource bottlenecks is not trivial. Once a training job
    has been launched, the process becomes non-transparent, and you don't have much
    visibility into the model training process. Equally non-trivial is real-time monitoring
    to detect sub-optimal training jobs and stop them early to avoid wasting training
    time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Debugger provides visibility into training jobs and the infrastructure
    a training job is executing on. Real-time training metrics such as **learning
    gradients** and **network weights** captured by SageMaker Debugger provide visibility
    into a training job in progress, so you can act on conditions such as **vanishing
    gradients** and **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Debugger also monitors and provides reports about the system's resources such
    as CPU, GPU, and memory, providing you with insights into resource utilization
    and bottlenecks. Additionally, if you use TensorFlow or PyTorch for your deep
    learning training jobs, Debugger provides you with a view into framework metrics
    that can be used to speed up your training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to use the capabilities of Amazon
    SageMaker Debugger and apply best practices to address challenges typical to debugging
    ML training. These challenges include identifying and reacting to sub-optimal
    training, gaining visibility into the resource utilization of the training infrastructure,
    and optimizing training framework parameters. You will also learn how to improve
    the training time and costs by applying detailed recommendations provided by SageMaker
    Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Debugger essentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time monitoring of training jobs using built-in and custom rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain insight into the training infrastructure and training framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment for this book yet, please refer
    to [*Chapter 2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039), *Data Science
    Environments*, which will walk you through the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter07](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter07).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Debugger essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about the basic terminology and capabilities
    of Amazon SageMaker Debugger. Using Debugger with your training jobs involves
    three high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Configuring* the training job to use SageMaker Debugger.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Analyzing* the collected tensors and metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Taking* action.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding points are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Amazon SageMaker Debugger overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Amazon SageMaker Debugger overview
  prefs: []
  type: TYPE_NORMAL
- en: As we dive into each one of these steps, we will introduce the necessary terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a training job to use SageMaker Debugger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to configure training jobs to use Amazon SageMaker Debugger.
    By now, you are familiar with using the `Estimator` object from SageMaker SDK
    to launch training jobs. To use Amazon SageMaker Debugger, you must enhance `Estimator`
    with three additional configuration parameters: `DebuggerHookConfig`, `Rules`,
    and `ProfilerConfig`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `DebuggerHookConfig`, you can specify which debugging metrics to collect
    and where to store them, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`s3_output_path` is the location where all the collected data is persisted.
    If this location is not specified, Debugger uses the default path, `s3://<output_path>/debug-output/`,
    where `<output_path>` is the output path of the SageMaker training job. The `CollectionConfig`
    list allows you to organize the debug data or tensors into collections for easier
    analysis. A tensor represents the state of a training network at a specific time
    during the training process. Data is collected at intervals, as specified by `save_interval`,
    which is the number of steps in a training run.'
  prefs: []
  type: TYPE_NORMAL
- en: How do you know which tensors to collect? SageMaker Debugger comes with a set
    of built-in collections to capture common training metrics such as `weights`,
    `layers`, and `outputs`. You can choose to collect all of the available tensors
    or a subset of them. In the preceding code sample, Debugger is gathering the `metrics`
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For a complete list of built-in collections, refer to [https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a custom collection of metrics to collect. In the following
    code block, Debugger captures all the metrics with `relu`, `tanh`, or `weight`
    in their names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While it may be tempting to collect all the tensors, this leads to collecting
    a lot of data, which increases training time, training costs, and storage costs.
    In this case, using a `ReductionConfig` allows you to save reduced tensors instead
    of saving the full tensor ([https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection)).
  prefs: []
  type: TYPE_NORMAL
- en: 'While `DebuggerHookConfig` allows you to configure and save tensors, a rule
    analyzes the tensors that are captured during the training for specific conditions
    such as **loss not decreasing**. SageMaker Debugger supports two different types
    of rules: **built-in** and **custom**. SageMaker Debugger comes with a set of
    built-in rules in Python that can detect and report common training problems such
    as overfitting, underfitting, and vanishing gradients. With custom rules, you
    write your own rules in Python for SageMaker Debugger to evaluate against the
    collected tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the following code block, Debugger collects tensors related
    to the `metrics` collection and evaluates the tensors to detect whether the training
    loss is reduced throughout the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `ProfilerConfig` allows you to collect system metrics such as CPU,
    GPU, Memory, I/O, and framework metrics specific to the framework being used in
    your training job. For the system metrics, you must specify the time interval
    for which you want to collect metrics, while for framework metrics, you specify
    the starting step and the number of steps, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table summarizes the tensors and metrics that are collected by
    SageMaker. It shows the different types of metrics, examples of each type, and
    how to collect and use them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Tensors and metrics collected by SageMaker Debugger'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Tensors and metrics collected by SageMaker Debugger
  prefs: []
  type: TYPE_NORMAL
- en: Using these configuration parameters, SageMaker Debugger collects quite a lot
    of information about your training jobs. But how do you ensure that the data that's
    been collected is secure?
  prefs: []
  type: TYPE_NORMAL
- en: A best practice is to encrypt all the data in an S3 bucket, either with a key
    provided by AWS or your own key with **customer-managed key** (**CMK**). Additionally,
    the rules that have been configured are executed on isolated Debugger rule containers.
    The rule containers also execute in the same VPC as the training job and use the
    IAM role that's used by the training job.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are satisfied with your Debugger configuration, kick off training using
    `estimator.fit()`. Next, we will analyze the information that's collected by the
    Debugger during the training job.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the collected tensors and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All tensors and metrics that are collected during training are persisted in
    S3\. SageMaker Debugger uses a `trial` object to represent a single training run.
    A trial object consists of multiple steps, where each step represents a single
    batch of training data. At each step, a collected tensor has a specific value.
  prefs: []
  type: TYPE_NORMAL
- en: To access the tensor values, you get the path to the tensors from the estimator,
    create a trial, get the list of tensors, find out the steps where you have data
    for a specific tensor you are interested in, and view the values of the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'By following this path from the trial to the individual tensor values, you
    can manually query the tensor values, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can visualize the tensor values that have been collected even further by
    using custom plot code in the notebook. The following diagram shows a visualization
    of the **train-rmse** and **validation-rmse** training metrics, which were collected
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Training and validation errors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Training and validation errors
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can also view the visualizations in SageMaker Studio. Additionally,
    if you have rules configured, Debugger automatically analyses the tensors to evaluate
    training conditions and trigger cloud watch alerts. Similarly, when you set the
    `ProfileConfig` parameter, a detailed profiler report is generated and saved in
    S3\. Next, let's take a look at how to act on the rule results.
  prefs: []
  type: TYPE_NORMAL
- en: Taking action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rules evaluate the collected tensor data. As the rule evaluation's status changes
    during training, a CloudWatch Event is triggered. You can configure a CloudWatch
    rule to be triggered for the CloudWatch Event to automate actions in response
    to the issues found by the rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you can use Debugger''s built-in actions to automate the responses.
    The following code block shows how to use a combination of Debugger''s built-in
    rules and actions to stop a training job if the loss is not continuously reduced
    during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: On the other hand, when you have the `ProfilerConfig` parameter configured,
    a profiler report with a detailed analysis of system metrics and framework metrics
    is generated and persisted in S3\. You can download, review, and apply recommendations
    to the profiler report.
  prefs: []
  type: TYPE_NORMAL
- en: In the next two sections, you will learn how to automate responses to rule evaluations
    and implement recommendations from the profiler report.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time monitoring of training jobs using built-in and custom rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will use Debugger capabilities to monitor a job with built-in
    and custom rules to detect sub-optimal training conditions such as `LossNotDecreasing`
    and `ExplodingGradients`.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker provides a set of built-in rules to identify common training issues
    such as `class_imbalance`, `loss_no_decreasing`, and `overfitting`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete list of SageMaker built-in rules can be accessed here: [https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to configure `built_in` rules with SageMaker
    Debugger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After calling fit, SageMaker starts one training job and one processing job
    for each configured built-in rule. The rule evaluation status is visible in the
    training logs in CloudWatch at regular intervals. You can also view the results
    of the rule execution programmatically using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results from the built-in rules that have been configured should be similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Summary of built-in rule execution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Summary of built-in rule execution
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the rule summary, you can see that the `LossNotDecreasing` rule
    is triggered, as indicated by `RuleEvaluationStatus` – `IssuesFound`. Since the
    action that's been configured is used to stop the training job, you will notice
    that the training job is stopped before all epochs are executed. You can also
    see that the other built-in rules – `Overfit`, `Overtraining`, and `StalledTrainingRule`
    – were not triggered during training.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in rules are managed by AWS, freeing you from having to manage updates
    to rules. You simply plug them into the estimator. However, you may want to monitor
    a metric that is not included in the built-in rules, in which case you must configure
    a custom rule. A bit more work is involved with custom rules. For example, let's
    say you want to track if the gradients are becoming too large during training.
    To create a custom rule for this, you must extend the `Rule` interface provided
    by SageMaker Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker provides two sets of Docker images for rules: one set for evaluating
    built-in rules and one set for evaluating custom rules. The **Elastic container
    registry** (**ECR**) URLs for these Docker images are available at [https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-docker-images-rules.html](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-docker-images-rules.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the custom rule will work with the tensors that were
    collected using the `gradients` collection. The `invoke_at_step` method provides
    the logic to be executed. At each step, the mean value of the gradient is compared
    against a threshold. If the gradient value is greater than the threshold, the
    rule is triggered, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define the custom rule, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the custom rule in the estimator and call the `fit` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After calling `fit`, Amazon SageMaker starts one training job and one processing
    job for each configured customer rule. The rule evaluation status is visible in
    the training logs in CloudWatch at regular intervals. Similar to the rule summary
    for `built_in` rules, you can view the custom rule summary using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using a combination of built-in and custom rules, you can gain insight into
    the training process and proactively stop the training jobs, without having to
    run an ineffective training job to completion.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walkthrough of using SageMaker
    Debugger''s built-in and custom rules is provided in the following GitHub repository:
    [https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/debugger/weather-prediction-debugger-rules.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/debugger/weather-prediction-debugger-rules.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you got an inside look at the training process and improved
    the training job based on issues that have been detected by built-in and custom
    rules. In the next section, you will learn how to gain insight into the infrastructure
    and framework that's used for training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Gaining insight into the training infrastructure and training framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to gain visibility into the resource utilization
    of the training infrastructure and the training framework. You will also learn
    how to analyze and implement recommendations provided by the deep profiler capability
    of SageMaker Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: Debugger profiler provides you with visibility into the utilization of the infrastructure
    running ML training jobs on SageMaker. Debugger automatically monitors system
    resources such as CPU, GPU, network, I/O, and memory. Additionally, Debugger collects
    metrics specific to the training framework such as step duration, data loading,
    preprocessing, and operator runtime on CPU and GPU. You can decide to profile
    the training job in its entirety or just portions of it to collect the necessary
    framework metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to collecting the system and framework metrics, behind the scenes,
    Debugger correlates these metrics automatically, which makes it easy for you to
    identify possible resource bottlenecks and perform root cause analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore this in detail with our example use case – predicting weather
    using PyTorch. Here, we will explore the system metrics, the framework metrics
    that are generated by the profiler, and look at implementing recommendations made
    by the profiler. This kind of deep profiling of training jobs includes the following
    high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a PyTorch model for weather prediction with Debugger enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing and visualizing the system and framework metrics generated by the
    profiler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing the profiler report generated by SageMaker Debugger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reviewing and implementing recommendations from the profiler report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparing the training jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at each of these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Training a PyTorch model for weather prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will train a deep learning model using the PyTorch framework. Because
    of the large volumes of data and the deep learning framework, we''ll train on
    GPU instances. We will train on two `ml.p3.2xlarge` instances. Our infrastructure
    configuration will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define `ProfilerConfig` so that it can collect system and framework
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must configure the PyTorch estimator by using the infrastructure and
    profiler configuration as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s start the training job with the `fit()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, you will analyze and visualize the metrics generated by
    Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and visualizing the system and framework metrics generated by the
    profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the training job starts, Debugger starts collecting system and framework
    metrics. In this section, you will learn how to query, analyze, and visualize
    the collected metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at how to analyze the collected metrics manually. The following
    code block shows how to query for system metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block results in the following output, which shows the GPU
    of one of the training instances at a particular time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The value of `0.0` indicates that this GPU is not being utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the system metrics, you can review framework metrics as well. The
    following code block shows how to query for framework metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block results in the following, showing one of the framework
    metrics at a particular time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Once the metrics have been collected, you can visualize them using a heat map
    or custom plots in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more colorful visualization of the heat map and a more in-depth analysis
    of system and framework metrics, take a look at the following notebook: [https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the profiler report generated by SageMaker Debugger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will download and review the profiler report that was generated
    by Debugger. SageMaker Debugger creates a detailed profiler report and saves it
    in an S3 bucket at `s3://<your bucket> /<job-name>/profiler-output/`. You can
    download the report directly from S3\. In the following list, we will review a
    few sections of the downloaded report:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training job summary**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section of the report provides a detailed summary of the training job,
    including the start and end time of the job and the time that was spent on various
    phases of training. The following screenshot shows a sample of the training job''s
    summary:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Training job summary of the profiler report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Training job summary of the profiler report
  prefs: []
  type: TYPE_NORMAL
- en: '**System metrics summary**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section of the report shows the resource utilization of the training nodes.
    The following screenshot shows CPU, GPU, memory utilization, I/O wait time, and
    the amount of data that was sent and received:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.6 – System metrics summary of the profiler report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – System metrics summary of the profiler report
  prefs: []
  type: TYPE_NORMAL
- en: '**Framework metrics summary**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section of the report starts by showing how much time the training job
    spent in the training and validation phases, as well as the time it spent waiting:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Framework metrics summary of the profiler report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Framework metrics summary of the profiler report
  prefs: []
  type: TYPE_NORMAL
- en: '**Rules summary**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As the training job is running, Debugger executes a set of rules to profile
    the training process. This section of the profiler report summarizes all the debugger
    rules that have been evaluated, the description of the rule, the number of times
    each rule was triggered during training, the analysis, and recommendations for
    improving the training job. The following screenshot shows the rule summary in
    table format:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Rules summary of the profiler report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Rules summary of the profiler report
  prefs: []
  type: TYPE_NORMAL
- en: In addition to directly querying and visualizing the metrics, as well as downloading
    the profiler report in your notebook, you can use SageMaker Studio, which provides
    built-in visualizations for analyzing profiling insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access Debugger in Studio, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the navigation pane, choose **Components and registries**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Experiments and trails**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose your training job (right-click).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Debugger Insights** from the Debugger tab that opens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Debugger** tab, you will see multiple sections. One of these sections
    is called **Training job summary**, as shown in the following screenshot. This
    built-in visualization shows training job details, such as the start time, end
    time, duration, and time spent in individual phases of training. The pie chart
    visualization shows the relative time spent by the training job in the initialization,
    training, and finalization phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Debugger visualization in SageMaker Studio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Debugger visualization in SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed a few sections of the downloaded profiler report
    at a high level. To explore the profiler report in more detail, please run through
    the notebook in our Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and implementing recommendations from the profiler report
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have recommendations from the profiler, let's analyze and implement
    a recommendation to see if it leads to an improved training job.
  prefs: []
  type: TYPE_NORMAL
- en: From the rules summary table in the preceding section, we can see that the rule
    triggered a maximum number of times during our training is `LowGPUUtilization`.
    This rule indicates that there is a possibility of bottlenecks occurring due to
    blocking calls and recommends changing the distributed training strategy or increasing
    the batch size. The next rule that was triggered the most times was `BatchSize`,
    which indicates that the GPU utilization could be low because of the smaller batch
    size.
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation from the profiler, based on this rule's execution, is to
    consider running on a smaller instance type and to increase the batch size. Let's
    combine the profiler recommendations from these two most triggered rules, run
    two new training jobs with different settings, and check the profiler reports
    for the new training jobs to see if there is any improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run the first training job with the same infrastructure, `()`, but
    with an increased batch size, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For the next training job, we will use smaller training instances, `()`, and
    increase the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Using these two different configurations, run two different training jobs using
    `estimator.fit()`. Once the training jobs are complete, download and analyze the
    two profiler reports.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two training jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we have a total of three completed training jobs with different
    configurations. In this section, we''ll compare the original training job to the
    two new training jobs we configured based on the recommendations from the profiler.
    When comparing these jobs, we will focus on the training time and the resulting
    training costs. The following table shows the initial and revised training job
    configurations, along with the training time, resource utilization, and cost comparisons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Comparison of training jobs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Comparison of training jobs
  prefs: []
  type: TYPE_NORMAL
- en: First, let's compare the original training job with the training job that uses
    the first revised training configuration. In the revised training configuration,
    the batch size is increased from `64` to `1024`. This configuration change decreased
    the training time by `17637` seconds; that is, from `18262` seconds to `895` seconds.
    Assuming that the training jobs were run in the us-west-2 region, the cost of
    `p3.2xlarge` is $3.825 at the time of writing. This leads to a cost saving of
    26.67%.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you compare the second revised training configuration, where we
    updated both the batch size and instance type to the original, the training time
    increased but the overall training cost improved by 65.36%. If you can tolerate
    a slight increase in the training time, you can save on training costs by implementing
    recommendations from the profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'An example notebook that provides a complete walkthrough of using the SageMaker
    Debugger profiler is provided in the following GitHub repository: [https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb](https://gitlab.com/randydefauw/packt_book/-/blob/master/CH06/weather-prediction-debugger-profiler.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: The results that were discussed in this section are from using the full dataset
    for PyTorch training. In the notebook, you will have the chance to explore the
    same functionality but with a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we implemented a couple of recommendations from the profiler
    and saw considerable training improvements. There are still more recommendations
    that you can experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in this section, we focused on how to kick off an estimator with
    Debugger enabled. You can also attach a profiler to a running training job using
    `estimator.enable_default_profiling()`. Similarly, to enable Debugger's built-in
    rules, system monitoring, and framework profiling with customizable configuration
    parameters, use `estimator.update_profiler()`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use the capabilities of Amazon SageMaker
    Debugger to gain visibility of the training process, training infrastructure,
    and training framework. This visibility allows you to react to typical training
    issues such as overfitting, training loss, and stopping the training jobs from
    running to completion, only to result in sub-optimal models. Using recommendations
    from the deep profiler capabilities of Amazon SageMaker, you learned how to improve
    training jobs with respect to training time and costs.
  prefs: []
  type: TYPE_NORMAL
- en: Using the debugger capabilities discussed in this chapter, you can continuously
    improve your training jobs by tweaking the underlying ML framework parameters
    and the training infrastructure configurations for faster and cost-effective ML
    training. In the next chapter, you will learn how to manage trained models at
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional reading material, please review these references:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify bottlenecks, improve resource utilization, and reduce ML training
    costs with the deep profiling feature in Amazon SageMaker Debugger:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/identify-bottlenecks-improve-resource-utilization-and-reduce-ml-training-costs-with-the-new-profiling-feature-in-amazon-sagemaker-debugger/](https://aws.amazon.com/blogs/machine-learning/identify-bottlenecks-improve-resource-utilization-and-reduce-ml-training-costs-with-the-new-profiling-feature-in-amazon-sagemaker-debugger/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ML Explainability with Amazon SageMaker Debugger:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/](https://aws.amazon.com/blogs/machine-learning/ml-explainability-with-amazon-sagemaker-debugger/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
